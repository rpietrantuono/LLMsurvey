Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"78882875","Why can't non-finite values be detected in forward pass of Pytorch Lightning model?","2024-08-17 17:32:31","","0","18","<python><pytorch><nan><transformer-model><pytorch-lightning>","<p>I'm working on a Vision Transformer using the Pytorch Lightning framework. I ran into an issue where the gradient and predictions would end up as nan after a few iterations of the training step.</p>
<p>After much trial and error, I discovered that adding these lines of preprocessing code (before the data is passed to the model) to set non-finite values to 0 fixed the issue:</p>
<pre><code>non_fin = ~torch.isfinite(x)

x[non_fin] = 0
</code></pre>
<p>The model now trains and runs as expected.</p>
<p>However, what still confuses me is that, earlier in my debugging process, I had added these lines to the forward pass of the model class to try to check for the exact same issue:</p>
<pre><code>if not torch.all(torch.isfinite(x)) :
    print(&quot;THERE ARE EITHER NANS OR INF IN THE INPUT ITSELF&quot;)
    raise Exception(&quot;THERE ARE NANS OR INFS IN THE INPUT X&quot;)
</code></pre>
<p>When doing this (and without the preprocessing step), the gradient and predictions still end up as nan, but nothing is printed and no errors are thrown. This confuses me because logically, I'm thinking that</p>
<p>preprocessing step solves the issue -&gt;
at least some values in x are being replaced with 0 -&gt;
the if statement should trigger in the forward pass (when the preprocessing isn't added)</p>
<p>Am I missing something? I can provide more details if necessary.</p>
","transformer-model"
"78870919","Pandapower Incorrect Transformer Loading Calculation and Impedance Problems","2024-08-14 12:27:22","","0","24","<model><transformer-model><pandapower>","<p>I’m working on a Python-based power flow analysis using the Pandapower library, but I’ve run into some issues related to the diagnostic tool and transformer loading calculations that seem to be linked to potential bugs or misinterpretations within the Pandapower framework.</p>
<p><strong>Project</strong>: I’m using Pandapower to model a small distribution network with 16 buses, several lines, and a 0.25 MVA transformer connecting the network to the medium-voltage grid.</p>
<p><strong>Objective</strong>: I need to accurately simulate power flow and transformer loading under different load conditions.</p>
<p><strong>Encountered Issues</strong>:</p>
<h3><strong>A.</strong>   -&gt; SOLVED</h3>
<p>The network includes a transformer rated at 0.25 MVA (20/0.4 kV). I’ve set up loads across multiple buses and expected the transformer loading to vary with changes in load.</p>
<p>Regardless of significant changes in load (e.g., increasing each house connection load from 0.01 MW to 0.1 MW), the transformer loading percentage remains nearly constant and unrealistically low.</p>
<p><a href=""https://i.sstatic.net/Z4Yb6zOm.png"" rel=""nofollow noreferrer"">trafo loading results</a></p>
<p><strong>steps I tried:</strong></p>
<ul>
<li>Changing the load while having constant transformer: loading changes in the sixth/seventh value after the comma</li>
<li>Changing the trafo while having constant load: different loading value, but not realistically higher/lower in relation to the trafo rated power. Also only 3-4 percent points changing</li>
</ul>
<p>-&gt; Solved: in the Slack-Sheet of the input-excel the wrong bus was connected (has to be the medium voltage grid bus aka the external grid representative)</p>
<h3>**B. ** -&gt; OPEN</h3>
<pre><code>pandapower_converter.py:712: RuntimeWarning: divide by zero encountered in divide
  rx_mag = mag_g / np.sqrt(i_no_load * i_no_load * 1e-4 - mag_g * mag_g)
</code></pre>
<p><strong>steps I tried:</strong></p>
<p>I tried to set up a new pandapower file with a small test grid from the pandapower website and then the transformer loading values are changing realistically, but the rx_mag error is still on. I printed i_no_load and mag_g and those are 0.32 and 0.0032 which are 0 when connected through the formula. From the pandapower_converter.py file I know that the rx_mag is &quot;only&quot; used to check the transformer values, and not in further calculations, but it is unnerving either way.</p>
<p>Here the code how the value is used:</p>
<pre><code># Calculate rx ratio of magnetising branch
        mag_g = np.divide(pfe, sn_mva * 1000, where=valid)
        mag_g[np.logical_not(valid)] = np.nan


        rx_mag = mag_g / np.sqrt(i_no_load * i_no_load * 1e-4 - mag_g * mag_g)
        # positive and zero sequence magnetising impedance must be equal.
        # mag0_percent = z0mag / z0.
        checks = {
            &quot;vk0_percent&quot;: np.allclose(vk_percent, vk0_percent) or np.isnan(vk0_percent).all(),
            &quot;vkr0_percent&quot;: np.allclose(vkr_percent, vkr0_percent) or np.isnan(vkr0_percent).all(),
            &quot;mag0_percent&quot;: np.allclose(i_no_load * 1e-2, 1e4 / (vk0_percent * mag0_percent))
            or np.isnan(mag0_percent).all(),
            &quot;mag0_rx&quot;: np.allclose(rx_mag, mag0_rx) or np.isnan(mag0_rx).all(),
            &quot;si0_hv_partial&quot;: np.isnan(
                self._get_pp_attr(&quot;trafo&quot;, &quot;si0_hv_partial&quot;, expected_type=&quot;f8&quot;, default=np.nan)
            ).all(),
        }
        if not all(checks.values()):
            failed_checks = &quot;, &quot;.join([key for key, value in checks.items() if not value])
            logger.warning(f&quot;Zero sequence parameters given in trafo shall be ignored:{failed_checks}&quot;)
</code></pre>
<h3><strong>C.</strong>   -&gt; OPEN</h3>
<p>I manually input impedance values for the lines using the following parameters:</p>
<pre><code>import pandapower as pp

# Define impedance values
r_ohm_per_km = 0.322097
x_ohm_per_km = 0.073199

# Example line creation
pp.create_line_from_parameters(net, from_bus, to_bus, length_km=0.1, r_ohm_per_km=r_ohm_per_km, x_ohm_per_km=x_ohm_per_km, c_nf_per_km=210, max_i_ka=0.24)
</code></pre>
<p>Despite these correct values, Pandapower’s diagnostic tool flags several lines as having impedance values close to zero (<code>r_ohm &lt;= 0.001 or x_ohm &lt;= 0.001</code>). This occurs after running the power flow calculation, although the values remain unchanged in the network model.</p>
<h3><strong>steps I tried:</strong></h3>
<p>I’ve checked the impedance values immediately after importing them, and they are correctly set in the Pandapower model. The issue only appears after running the diagnostic tool. Additionally, after running the power flow calculation, the values remain correct, but the diagnostic tool still reports the same problem. I found out that when I change the line length the impedance problem doesn't occur, so it has to be some calculation in the background that gets to small, but 15m is not that unusually short. I wonder which code changes could help that.</p>
<ul>
<li>I’ve used both the old (<code>pp.runpp()</code>) and new (<code>pp.run.runpp_pgm()</code>) power flow calculation functions in Pandapower, with similar results.</li>
</ul>
<h3><strong>request for help:</strong></h3>
<ul>
<li><p>Is there more options for log files in pandapower?</p>
</li>
<li><p>I am quite new to python programming, is there another option to search for errors?</p>
<p>thank you!</p>
</li>
</ul>
","transformer-model"
"78870789","Pytorch TransformerEncoder: How to determine most important input features?","2024-08-14 11:59:51","","0","13","<pytorch><transformer-model><feature-engineering>","<p>This is the model I use to determine the likelihood of a specific event based on input data:</p>
<pre><code>import torch
import torch.nn as nn
class Trnsfrmr(nn.Module):
    def __init__(
        self,
        d_model=500,
        nhead=10,
        dim_feedforward=2048,
        num_layers=6,
        dropout=0.1
    ):

        super().__init__()

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers,
        )
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Linear( d_model // 2, 1 ),
            nn.Sigmoid()
        )
        self.d_model = d_model

    def forward(self, x):
        x = self.transformer_encoder( x )
        x = self.classifier(x)

        return x
</code></pre>
<p>Now I am wondering how I can determine the most important input features, In this case, I have 570 input features, that lead to the output decision? For example, if the output probability is close to 1 - how do I know which input features led the network the make this prediction?</p>
","transformer-model"
"78868403","tensorflow.keras.layers.MultiHeadAttention warning that query layer is destroying mask","2024-08-13 21:42:06","","0","12","<tensorflow><keras><deep-learning><transformer-model><multihead-attention>","<p>I am building a transformer model using <code>tensorflow==2.16.1</code> and one of the layers is a tensorflow.keras.layers.MultiHeadAttention layer.</p>
<p>I implement the attention layer in the TransformerBlock below:</p>
<pre><code># Import TensorFlow and Keras for building and training neural network models
import tensorflow as tf
from tensorflow.keras.layers import (
    Dense,
    LayerNormalization,
    MultiHeadAttention,
    Dropout,
)

class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):

        super(TransformerBlock, self).__init__(**kwargs)

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.ff_dim = ff_dim
        self.rate = rate

        
        self.att = None  
        self.ffn = None  

        self.layernorm1 = None 
        self.layernorm2 = None 

        self.dropout1 = None
        self.dropout2 = None

    def build(self, input_shape):

        self.att = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim)
        
        self.ffn = tf.keras.Sequential(
            [Dense(self.ff_dim, activation=&quot;relu&quot;), Dense(self.embed_dim)]
        )

        self.layernorm1 = LayerNormalization(epsilon=1e-6)
        self.layernorm2 = LayerNormalization(epsilon=1e-6)

        self.dropout1 = Dropout(self.rate)
        self.dropout2 = Dropout(self.rate)

        super(TransformerBlock, self).build(input_shape)

    def call(self, inputs, training, padding_mask=None, causal_mask=True, qa=False):

        mask = None

        seq_len = tf.shape(inputs)[1]
        batch_size = tf.shape(inputs)[0]

        if padding_mask is not None:
            padding_mask_reshaped = tf.cast(
                tf.reshape(padding_mask, (batch_size, 1, seq_len)), dtype=tf.float32
            )
            mask = tf.broadcast_to(
                padding_mask_reshaped, (batch_size, seq_len, seq_len)
            )


        attn_output = self.att(
            inputs, inputs, attention_mask=mask, use_causal_mask=True
        )

        attn_output = self.dropout1(attn_output, training=training)

        out1 = self.layernorm1(inputs + attn_output)

        ffn_output = self.ffn(out1)

        ffn_output = self.dropout2(ffn_output, training=training)

        out2 = self.layernorm2(out1 + ffn_output)

        return out2
        

</code></pre>
<p>Whenever I implement this TransformerBlock I receive a warning.</p>
<p><code>lib/python3.11/site-packages/keras/src/layers/layer.py:877: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.</code></p>
<p>However, when I pass a <code>padding_mask</code> and <code>use_causal_mask=True</code>, it changes the model performance. For example, if I pass <code>use_causal_mask=False</code> the model performs unrealistically well, as predicted if there is no causal mask, which implies to me that the causal mask is working. This same behavior is observed if I create and merge the  causal_mask with the padding_mask and pass it through the <code>attention_mask</code> arg.</p>
<p>When I search the internet to see why I am getting this warning there is very little information on it. Does anyone here know why I can't stop getting this warning and what it means?</p>
","transformer-model"
"78859987","Using BART, T5, mBART, and mT5 for translation of a new language","2024-08-12 04:33:49","","-1","14","<transformer-model><fine-tuning><bart>","<p>I am working on developing a transliteration model to convert Romanized Tamil (Tamil written in the Latin alphabet) text into its native Tamil script. I’ve come across several projects that use transformer-based models like T5, BART, mT5, and mBART for translation tasks. I understand that mT5 and mBART models have been trained on Tamil language data. Romanized Tamil can be consider as another language known as “Tanglish”.</p>
<p>My question is: How can I use one of these models to translate Romanized Tamil (Tamil written in the Latin alphabet) into the native Tamil script? Note that, in this translation task, the input and output are expected to have a similar number of words. Any guidance on how to approach this problem would be greatly appreciated. Tamil is an Indic language and I used this language here as an example.</p>
","transformer-model"
"78856532","How to make huggingface transformer for translation return n translation inferences?","2024-08-10 16:37:03","78856815","1","32","<python><huggingface-transformers><transformer-model>","<p>So I am trying to use this transformer from huggingface <a href=""https://huggingface.co/docs/transformers/en/tasks/translation"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/en/tasks/translation</a>. The issue is that I want n translations returned and not just one. How can I  do that? I mean, I want to have ordered translations, that means the translation with index 0 would have the highest confidence, this is important for my use case, which is about translating natural language to commands language (about 40 commands without subcommands).</p>
<p>The github repo and exact model is this one <a href=""https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py"" rel=""nofollow noreferrer"">https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py</a></p>
<p>This is the HuggingFace API:</p>
<pre><code>translator = pipeline(&quot;translation_xx_to_yy&quot;, model=&quot;my_awesome_opus_books_model&quot;)
translator(text)
</code></pre>
<p>But I am intending to use the model directly from the google search github repo, so it seems some tweaking should be done here:</p>
<pre><code>predictions = []
    for batch in dataset:
      predicted_tokens = self._model.generate(
          input_ids=self.to_tensor(batch[&quot;inputs&quot;]), **generate_kwargs
      )
      predicted_tokens = predicted_tokens.cpu().numpy().tolist()
      predictions.extend(
          [vocabs[&quot;targets&quot;].decode(p) for p in predicted_tokens]
      )

    for inp, pred in zip(inputs, predictions):
      logging.info(&quot;%s\n  -&gt; %s&quot;, inp, pred)

    if output_file is not None:
      utils.write_lines_to_file(predictions, output_file)
</code></pre>
<p>Also any suggestion on some other model option to solve this natural language to cmd is welcomed!</p>
","transformer-model"
"78855385","How do I pass varying length sequence input to decoder for look ahead mask?","2024-08-10 06:34:56","","0","19","<python><translation><large-language-model><embedding><transformer-model>","<p>I am working on a translation model by transformer from scratch for the first time. And I am encountering problems while training decoders with varying sequence lengths with look ahead pads. I am unable to understand how to integrate varying length input with batch size and pass it to a custom training loop. How can I solve this?</p>
<pre class=""lang-py prettyprint-override""><code>def create_padding_mask(seq):
    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
    return seq[:, tf.newaxis, tf.newaxis, :]

def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask

def CreateMask(encoderInput, decoderInput):
    encPadMask = create_padding_mask(encoderInput)
    decPadMask = create_padding_mask(decoderInput)
    LAMask = create_look_ahead_mask(tf.shape(decoderInput)[1])
    # decTargetPadMask = create_padding_mask(decoderInput)
    LAMask = tf.maximum(decPadMask, LAMask)
    return encPadMask, LAMask, decPadMask
</code></pre>
","transformer-model"
"78855135","sentence-transformers: combined parallelization for custom chunking function and encode_multi_process()","2024-08-10 03:16:07","","1","61","<python><machine-learning><python-multiprocessing><transformer-model><sentence-transformers>","<p>I am working in Python 3.10, using a sentence-transformers model to encode/embed a list of text strings. I want to use sentence-transformer's <code>encode_multi_process</code> method to exploit my GPU. This is a very specific function that takes in a string, or a list of strings, and produces a numeric vector (or list of vectors). The function distributes the work among system CPUs and GPUs.</p>
<p>I also want to parallelize my custom chunking function <code>create_chunks</code>, which splits a raw text string into chunks that are small enough to fit into the model's constraints. So, for any given text input, it has to go through <code>create_chunks</code> before going through <code>encode_multi_process</code>. I'm pretty sure that using multiple CPU cores to parallelize this step is the way to go.</p>
<p>Right now I am considering using <code>multiprocessing</code> to apply <code>create_chunks</code> to my dataset, and then <code>encode_multi_process</code>, but this seems inefficient: the chunks that come out of <code>create_chunks</code> have to wait until the whole dataset is finished before moving on to <code>encode_multi_process</code>. Are there more efficient Python alternatives? I have to build my solution around <code>encode_multi_process</code>, which is the main difficulty.</p>
<p>I wish I could use Dask, but the language model is too big to fit into a Dask task graph.</p>
","transformer-model"
"78854431","Error Loading Hugging Face Model Locally: 'FloatProgress' Object has No Attribute 'Style'","2024-08-09 19:53:32","","0","14","<deep-learning><artificial-intelligence><google-colaboratory><huggingface-transformers><transformer-model>","<p>I’m encountering an issue when trying to load Hugging Face models locally using the transformers library. The code below:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(&quot;meta-llama/Meta-Llama-3-8B&quot;) 
model = AutoModelForCausalLM.from_pretrained(&quot;meta-llama/Meta-Llama-3-8B&quot;)
</code></pre>
<p>results in the following error:</p>
<pre><code>tokenizer_config.json

Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.


AttributeError: 'FloatProgress' object has no attribute 'style'
</code></pre>
<p>This issue arises during the download of tokenizer configuration files and seems to be related to the tqdm progress bar used in the Hugging Face library.</p>
<p>Interestingly, this same model and code run perfectly on Google Colab without any issues, which suggests that the problem might be specific to my local setup.</p>
<p>Steps Taken:</p>
<ul>
<li><p>Checked Environment: The issue persists across different models, not just the one mentioned.</p>
</li>
<li><p>Library Versions: I have the latest versions of transformers, huggingface_hub, and tqdm installed.</p>
</li>
<li><p>Reinstalled Libraries: Reinstalled tqdm and huggingface_hub but the issue remains.</p>
</li>
</ul>
<p>I am not sure why this is happening. Because , same code works with colab but why it is not working with local machine I don't know. I tried multiple ways like using huggingface_hub, git of that model. But in every way, I found this error.</p>
<p>How to handle this error?</p>
","transformer-model"
"78852192","Choose available GPU devices with device_map","2024-08-09 10:04:34","","0","30","<machine-learning><transformer-model>","<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map=&quot;cuda:3&quot;,
)
</code></pre>
<p>There are many GPUs on the server, but I can only use two of them. How should I configure device_map (or other parameters) so that the model runs on both GPUs?</p>
","transformer-model"
"78851068","How to get the standard result of test dataset of ImageNet2012?","2024-08-09 03:28:46","","0","4","<transformer-model><imagenet>","<p>I have implemented a hardware accecleartor of ViT by using some linear-approximatelly method, and I want to test the accuracy on test dataset of ImageNet.
When I have downloaded the dataset(there are 10000 pictures in test dataset and 50000 val dataset), there is only the standard result txt for val dataset.</p>
<p>How can I get the standard label txt for teste dataset?</p>
","transformer-model"
"78847838","Pytorch Dependencies","2024-08-08 10:18:48","","-1","18","<python><deep-learning><pytorch><nlp><transformer-model>","<p>CT_USER/Documents/Translator/transformer.py
Traceback (most recent call last):
File &quot;c:\Users\CT_USER\Documents\Translator\transformer.py&quot;, line 2, in 
import torch
File &quot;C:\Users\CT_USER\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch_<em>init</em>_.py&quot;, line 148, in 
raise err
OSError: [WinError 126] The specified module could not be found. Error loading &quot;C:\Users\CT_USER\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\lib\fbgemm.dll&quot; or one of its dependencies.</p>
<p>I have tried all the things in internet like</p>
<p>pip uninstall torch
pip install torch torchvision torchaudio
pip install --upgrade torch</p>
<p>and also <a href=""https://pytorch.org/get-started/locally/"" rel=""nofollow noreferrer"">https://pytorch.org/get-started/locally/</a>
manually from pytorch documentation</p>
","transformer-model"
"78846449","Informer: Is there something wrong with loss calculation?","2024-08-08 03:48:57","","-1","21","<deep-learning><pytorch><time-series><loss-function><transformer-model>","<p>I think there is some calculation error in the source code of Informer's paper. (<a href=""https://github.com/zhouhaoyi/Informer2020"" rel=""nofollow noreferrer"">https://github.com/zhouhaoyi/Informer2020</a>)</p>
<p>When training the model, the source code for calculating the loss is shown below.</p>
<pre><code>def _select_criterion(self):
    criterion =  nn.MSELoss()
    return criterion

def train(self, setting):

        (Omitted...)

        criterion =  self._select_criterion()

        (Omitted...)

            for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(train_loader):
                iter_count += 1
                
                model_optim.zero_grad()
                pred, true = self._process_one_batch(
                    train_data, batch_x, batch_y, batch_x_mark, batch_y_mark)
                loss = criterion(pred, true)
                train_loss.append(loss.item())
                
        (Omitted...)

        return self.model

def _process_one_batch(self, dataset_object, batch_x, batch_y, batch_x_mark, batch_y_mark):

    (Omitted...)

    # encoder - decoder
    if self.args.use_amp:
        with torch.cuda.amp.autocast():
            if self.args.output_attention:
                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]
            else:
                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
    else:
        if self.args.output_attention:
            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]
        else:
            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)
    if self.args.inverse:
        outputs = dataset_object.inverse_transform(outputs)
    f_dim = -1 if self.args.features=='MS' else 0
    batch_y = batch_y[:,-self.args.pred_len:,f_dim:].to(self.device)

    return outputs, batch_y

</code></pre>
<p>In def train, the shape of the pred and the true must be the same,</p>
<p>but the true uses all or only the last vector, depending on the situation, and the pred uses all vectors.</p>
<p>If self.args.features=='MS', is an error in the loss calculation?</p>
","transformer-model"
"78841733","Spacy Transformer Model Not Able to Unload From GPU When Run in Loop","2024-08-07 03:38:10","","0","16","<nlp><gpu><transformer-model><spacy-3><spacy-transformers>","<p>Why is the SpaCy model not able to unload from GPU when run in loop? I want to unload the model at different interval of time. Also their is CPU memory leak issue with this model. How to resolve this issue.</p>
<pre><code>import spacy
import torch
import time
spacy.require_gpu()

for _ in range(10):
    print(&quot;The GPU occupied before model load: &quot;, torch.cuda.memory_allocated())
    ner_model = spacy.load(&quot;en_core_web_trf&quot;, disable=[&quot;tagger&quot;, &quot;parser&quot;, &quot;attribute_ruler&quot;, &quot;lemmatizer&quot;, &quot;morphologizer&quot;, &quot;senter&quot;, &quot;textcat&quot;])
    print(&quot;The GPU occupied after model load: &quot;, torch.cuda.memory_allocated())
    time.sleep(10)
    del ner_model
    time.sleep(10)
</code></pre>
<p>I deleted the model object but still not able to unload the model. Why?</p>
<p>I want to unload the model periodically. I want the model should unload whenever i need.</p>
","transformer-model"
"78840548","How to optimize Vision Transformer (ViT) model training for medical image analysis?","2024-08-06 18:28:12","","0","21","<python><tensorflow><transformer-model><medical-imaging><vision-transformer>","<p>I'm currently working on a research project involving the use of Vision Transformer (ViT) models for lung cancer detection from chest X-ray and CT scan images. While I have some experience with traditional CNNs, I'm relatively new to ViTs and am encountering several challenges. Here are some specifics:</p>
<p><strong>Data Preprocessing</strong>: What are the best practices for preprocessing medical images for ViT models? Should I use any specific augmentations or normalization techniques?</p>
<p><strong>Training Stability</strong>: I'm experiencing instability during training, such as fluctuating loss and accuracy. What are some recommended strategies to stabilize training for ViT models?</p>
<p><strong>Hyperparameter Tuning</strong>: Are there any specific hyperparameters (e.g., learning rate, batch size) that I should focus on adjusting for optimal performance with ViTs?</p>
<p><strong>Transfer Learning</strong>: Is transfer learning effective with ViT models in the medical imaging domain? If so, which pre-trained models or datasets would you recommend starting with?</p>
<p><strong>Computational Resources</strong>: ViTs require a lot of computational power. Are there any techniques or tricks to reduce the computational load without significantly impacting performance?</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Example data generator setup
datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

train_generator = datagen.flow_from_directory(
    'data/train',
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

# Example ViT model setup
vit_model = tf.keras.applications.VisionTransformer(
    input_shape=(224, 224, 3),
    include_top=True,
    weights=None,
    classes=2
)

vit_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history = vit_model.fit(train_generator, epochs=10)
</code></pre>
","transformer-model"
"78838655","relative_attention_max_distance and relative_attention_num_buckets?","2024-08-06 11:02:17","","-2","16","<machine-learning><deep-learning><nlp><huggingface-transformers><transformer-model>","<p>In T5 what is relative_attention_max_distance and relative_attention_num_buckets this two parameter suggests??</p>
<p>In my use case I want to give 512 conditional tokens (encoder) and generate 384 tokens (decoder).</p>
<p>what should be the best paramter for this use case. I want to long range dependency too. In ideal scenario task want to be absolute positional embedding.</p>
","transformer-model"
"78837737","Grounding Dino inference speed-up","2024-08-06 07:27:47","","0","38","<python><pytorch><transformer-model><zeroshot-classification>","<p>Grounding Dino is a very powerful zero-shot learning, but inference time is too slow.
With my GPU Nvdia GTX 950 it takes about 3 seconds for image (1100x840).</p>
<p>There are some parameter I can use to speed-up the inference time (and not loss too much in quality)?</p>
<p>In particular, I have in loop the same text, so how can I save the text encode time, doing it just for the first time?</p>
","transformer-model"
"78833164","Issue with PyTorch's transformer Model repeating last token during inference","2024-08-05 07:10:28","","0","31","<python><pytorch><nlp><autoencoder><transformer-model>","<p>I’ve been trying to implement PyTorch’s nn.TransformerEncoder and nn.TransformerDecoder solutions into a simple model, but I’m running into an issue that I’m unable to resolve where during inference the model only produces the last token fed into it.</p>
<p>For example lets say I have a tensor [1,2,3,4,5] the model will continue the sequence with [1,2,3,4,5,5,5,5,5,5,…] or if I had [5,2,8,3] it would continue to produce [5,2,8,3,3,3,3,3,3,3,…] even when using training data as input although when using a new randomly initialized model it will produce diverse output although since not trained is useless.</p>
<p>Although it produces the above results, the loss continues to decrease as I train it further indicating that its managing to learn the dataset. Due to this I initially thought this was just a problem with the dataset where the target was the same as the input which would cause it to produce the same tokens, but after further testing I’m sure that the targets are definitely the next token in the sequence, for example the input would be [1,2,3,4] and the target would be [2,3,4,5].</p>
<p>This lead me to my current standing theory that there is something wrong with the seq2seq implementation but after much research and trying different implementations of the common components such as positional encoding, adjusting hyper-parameters and removing / adding masks to the encoder and decoder, but regardless still weeks later and I’m still zero progress towards identifying the issue.</p>
<p>For reference here is the model and training step I’m using:</p>
<pre><code>class TextEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embed_dim: int, padding_index: int):
        super(TextEmbedding, self).__init__()
        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=padding_index)

    def forward(self, x):
        return self.embedding(x)

class TextTransformer(nn.Module):
    def __init__(self, vocab_size, embed_dim = 512, nhead = 8, num_encoder_layers = 6, num_decoder_layers = 6, max_length = 5000, padding_index = 0):
        super(TextTransformer, self).__init__()
        self.vocab_size = vocab_size
        self.max_length = max_length

        self.text_embedding = TextEmbedding(vocab_size, embed_dim, padding_index)
        self.positional_encoding = nn.Parameter(torch.zeros(1, max_length, embed_dim))

        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=2048)
        self.encoder = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_encoder_layers)

        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=nhead, dim_feedforward=2048)
        self.decoder = nn.TransformerDecoder(decoder_layer=decoder_layer, num_layers=num_decoder_layers)

        self.fc = nn.Sequential(
            nn.Linear(embed_dim, vocab_size)
        )

    def forward(self, src, tgt, src_mask, tgt_mask):
        #Embedding + Positional Encoding
        src_embedding = self.text_embedding(src) + self.positional_encoding[:, :src.size(1), :]
        tgt_embedding = self.text_embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]

        tgt_square_mask = create_square_mask(tgt.size(1)).to(src.device)

        #Encoder
        memory = self.encoder(src_embedding.permute(1, 0, 2), src_key_padding_mask=src_mask)

        #Decoder
        decoder_out = self.decoder(tgt_embedding.permute(1, 0, 2), memory, tgt_mask=tgt_square_mask, tgt_key_padding_mask=tgt_mask)
        decoder_out = decoder_out.permute(1, 0, 2)

        #FC output
        output = self.fc(decoder_out)

        return output

    def seq2seq(self, src, src_mask, stop_token, max_length = 500):
        src_embedding = self.text_embedding(src) + self.positional_encoding[:, :src.size(1), :]

        memory = self.encoder(src_embedding.permute(1, 0, 2), src_key_padding_mask=src_mask)
        sequence = src
        stop = False

        while sequence.shape[1] &lt; min(self.max_length, max_length) and not stop:
            tgt_embedding = self.text_embedding(sequence) + self.positional_encoding[:, :sequence.size(1), :]

            tgt_square_mask = create_square_mask(sequence.size(1)).to(src.device)
            dec_output = self.decoder(tgt_embedding.permute(1, 0, 2), memory, tgt_mask=tgt_square_mask)
            dec_output = dec_output.permute(1, 0, 2)

            out = self.fc(dec_output)[:, -1, :]
            predicted = out.argmax(dim=1)
            
            if predicted.item() == stop_token:
                stop = True

            sequence = torch.cat((sequence, predicted.unsqueeze(dim=0)),dim=1)

        return sequence

    def create_square_mask(size):
        mask = torch.triu(torch.ones(size, size), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf')).masked_fill(mask == 0, float(0.0))
        return mask
</code></pre>
<pre><code>def train_step(model, dataloader, criterion, optimizer, device):
    avg_loss = 0
    model.train()
    for batch, (text_data, text_pad_mask) in enumerate(dataloader):
        text_data, text_pad_mask = text_data.to(device), text_pad_mask.to(device)

        #shift data so that the in_text is the initial tokens and that tgt_text is the next predicted token in the sequence
        in_text = text_data[:, :-1]
        in_mask = text_pad_mask[:, :-1]
        tgt_text = text_data[:, 1:]
        tgt_mask = text_pad_mask[:, 1:]


        out = model(in_text, tgt_text, in_mask, tgt_mask)

        outputs = out[:, :].reshape(-1, model.vocab_size)# Reshape to [batch_size * steps, vocab_size]
        targets = tgt_text[:, :].reshape(-1)# Reshape to [batch_size * steps]

        loss = criterion(outputs, targets)
        avg_loss += loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
    return avg_loss / len(dataloader)
</code></pre>
<p>The loss function is CrossEntropyLoss, the optimizer is AdamW and the dataloader returns tokenized texts in the shape of (batch, sequence). I think this is all that is necessary to try diagnose the issue as I’m 100% sure the tokenizer and data loader is working perfectly as I’ve done a lot of testing on them and don’t want to flood this post with too much code but I can provide the code for them upon request if it helps at all.</p>
","transformer-model"
"78825076","“ValueError: Unrecognized model type” when loading my trained custom transformer model","2024-08-02 10:48:06","","0","41","<python><huggingface-transformers><transformer-model>","<p>I have trained a transformer model as a sequence-to-sequence model, did not finetune an LLM.</p>
<p>However, my way of saving the model into huggingface or even locally seems to be incorrect, meaning that checkpoints, weights and other important model configurations may have not been saved properly.</p>
<p>because when I try to load it from huggingface, it gives me this error:</p>
<pre><code>ValueError: Unrecognized model in Lu...... Should have a model_type key in its config.json, or contain one of the following strings in its name: albert, align, altclip......
</code></pre>
<p>Please find the layout of my work in this notebook:
<a href=""https://colab.research.google.com/drive/1qwT4mqaTPoVW7HwlWNRKheR7FAzIdR-L#scrollTo=DtU3whBevE31"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1qwT4mqaTPoVW7HwlWNRKheR7FAzIdR-L#scrollTo=DtU3whBevE31</a></p>
","transformer-model"
"78820055","Why is attn_mask in PyTorch' MultiheadAttention specified for each head separately?","2024-08-01 09:07:23","","0","34","<python><pytorch><large-language-model><transformer-model><multihead-attention>","<p>PyTorch <a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention"" rel=""nofollow noreferrer"">MultiheadAttention</a> allows to specify the attention mask, either as 2D or as 3D. The former will be broadcasted over all N batches the latter allows one to specify specific masks for each example in the batch. All seems to make sense. However, from the documentation, the 3D mask is defined as follows:</p>
<blockquote>
<p>(N⋅num_heads,L,S), where N is the batch size, L is the target sequence
length, and S is the source sequence length.</p>
</blockquote>
<p>Two questions araise:</p>
<ol>
<li>Why would anyone want a different mask for different heads?</li>
<li>If we have to do it this way anyway, how are they ordered? i.e. is it (Example1, Head1), (Example1, Head2),... etc OR is it (Example1, Head1), (Example2, Head1),... ? This is also asked in the comments at this <a href=""https://stackoverflow.com/questions/62629644/what-the-difference-between-att-mask-and-key-padding-mask-in-multiheadattnetion"">question</a>.</li>
</ol>
","transformer-model"
"78807395","Error when converting forecast_it to list in GluonTS Transformer Predictor: ""Setting an array element with a sequence""","2024-07-29 13:35:11","","0","10","<prediction><transformer-model><gluonts>","<p>I'm using GluonTS to make time series predictions with the TransformerPredictor. However, when I try to convert the forecast_it iterator to a list, I encounter the following error:</p>
<p>Setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (15, 11) + inhomogeneous part. During handling of the above exception, another exception occurred: source array must be array like object</p>
<pre class=""lang-py prettyprint-override""><code>train_data = ListDataset(
    [{
        'start': df.index[0],
        'target': df['target'].values,
        'feat_dynamic_real': df['feat_dynamic_real'].values.T,
        'feat_static_cat': df['feat_static_cat'].values if 'feat_static_cat' in df.columns else None
    }],
    freq='D'
)

predictor = TransformerPredictor(
    input_size=1,
    prediction_length=30,
    freq='D',
    context_length=30,
    trainer=Trainer(epochs=10)
)

predictor.train(train_data)

forecast_it, ts_it = make_evaluation_predictions(dataset=train_data, predictor=predictor)

forecasts = list(forecast_it)  # This line raises the error
</code></pre>
<p>Tried ensuring all the time series are of equal lengths. Did not resolve the issue.</p>
<p>Let me know if you need any additional information—this is my first time asking a question on stack overflow</p>
","transformer-model"
"78790838","How to visualize attention for long sequences (e.g., amino acids of length 1000) in Transformer models?","2024-07-24 22:32:39","","0","8","<visualization><transformer-model><multihead-attention>","<p>I am working with Transformer models and I have a specific use case where I need to visualize the attention mechanism for long sequences. Specifically, I am dealing with amino acid sequences of length 1000 along with their related codons.</p>
<p>Given the length of these sequences, I'm finding it challenging to effectively visualize the attention weights. What are the best practices or techniques for visualizing attention in such long sequences?</p>
<p>Here are some specifics of my situation:</p>
<ul>
<li><p>The sequence length is 1000.</p>
</li>
<li><p>I have multiple attention heads and possibly multiple layers.</p>
</li>
<li><p>I would like to visualize the attention weights in a way that is both informative and interpretable.</p>
</li>
</ul>
<h3>Key Questions:</h3>
<ol>
<li><p><strong>What type of plot is best suited for visualizing attention weights in long sequences?</strong></p>
</li>
<li><p><strong>Are there any techniques to aggregate or simplify the attention visualization for long sequences?</strong></p>
</li>
<li><p><strong>Can interactive plots be beneficial for this purpose, and if so, which libraries or tools should I use?</strong></p>
</li>
</ol>
<h3>What I Tried:</h3>
<p>I have tried using heatmaps to visualize the attention weights, but with a sequence length of 1000, the plots become too dense and difficult to interpret. I have also considered aggregating the attention weights over smaller windows of tokens, which helps, but I'm looking for additional insights or best practices.</p>
<h3>Expected Outcome:</h3>
<p>I am looking for effective ways to visualize these attention weights so that the important patterns and relationships can be clearly seen, despite the long sequence length.</p>
","transformer-model"
"78789348","Keras Transformer regression model not predicting values beyond a threshold","2024-07-24 15:44:17","","0","18","<keras><deep-learning><regression><tf.keras><transformer-model>","<p>I am currently working on a keras transformer model for regression and I am getting prediction values which are cut off to some specific threshold.</p>
<p>Code:</p>
<pre><code>def transformer_block(self,inputs, embed_dim, num_heads, ff_dim, dropout_rate):
        
    x = inputs
    x = MultiHeadAttention(key_dim=embed_dim,num_heads=num_heads,dropout=dropout_rate)(x, x)
    #x = Dropout(dropout_rate)(x)
    res = x + inputs

    # Feed Forward Part
    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation=&quot;relu&quot;)(x)
    x = Dropout(dropout_rate)(x)
    x = Dense(inputs.shape[-1])(x)
    return x + res
# Build the model
def create_model(self, embed_dim = 64, num_heads = 6,
        ff_dim = 128, num_transformer_blocks = 3,
        dropout_rate = 0.1):
        
    vocab_size,input_length = self.vocab_size,self.input_length,
    inputs = tf.keras.Input(shape=(input_length,), dtype=tf.int32)
    embedding_layer = keras.layers.Embedding(input_dim=vocab_size,
    output_dim=embed_dim)(inputs)

    x = embedding_layer
    
    for _ in range(num_transformer_blocks):
        x = self.transformer_block(x, embed_dim, num_heads, ff_dim, dropout_rate)
        x = LayerNormalization(epsilon=1e-6)(x) 
        x = tf.keras.layers.GlobalAveragePooling1D()(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
        
    f = ff_dim
        
    while(f//2 &gt; 1):                
        x = tf.keras.layers.Dense(f//2, activation='relu')(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
        f = f//2
            
    outputs = tf.keras.layers.Dense(1, activation = 'linear')(x)  # Regression output
    
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model
</code></pre>
<p><a href=""https://i.sstatic.net/TpiuyTMJ.png"" rel=""nofollow noreferrer"">True vs Predicted graph:</a></p>
<p>A previous question mentioned not to use tanh or sigmoid activation functions but I am only using relu (I also tried gelu and leaky relu). I tried adding more transformer layers but still get this problem. Can some one explain why I'm getting this and how to mitigate it?</p>
","transformer-model"
"78788521","How to Track Attention Weights of a Transformer Model with Comet?","2024-07-24 13:01:02","","0","17","<python><pytorch><transformer-model><comet>","<p>I am working on a Transformer model for a translation task and want to track attention weights using Comet. My model consists of 2 layers with 2 attention heads each. I am interested in understanding how to log attention weights effectively during and after training.</p>
<p><strong>Details:</strong></p>
<ol>
<li><p><strong>Model Setup:</strong></p>
<ul>
<li><p>I am using PyTorch with a Transformer architecture.</p>
</li>
<li><p>The model has 2 layers and each layer has 2 attention heads.</p>
</li>
</ul>
</li>
<li><p><strong>Tracking Attention:</strong></p>
<ul>
<li><p><strong>During Training:</strong> I want to track attention weights periodically to monitor how they evolve.</p>
</li>
<li><p><strong>At the End of Training:</strong> I also want to log attention weights from the final model for analysis.</p>
</li>
</ul>
</li>
<li><p><strong>Data:</strong></p>
<ul>
<li>let's assume my dataset consists of 10 instances with a batch size of 2, so there are 5 batches.</li>
</ul>
</li>
<li><p><strong>Questions:</strong></p>
<ul>
<li><p><strong>Batch/Instance Selection:</strong> How should I select which batch or instance to track? Should I use a specific batch, a random batch, or a representative batch?</p>
</li>
<li><p><strong>Tracking Attention Weights:</strong></p>
<ul>
<li><p>How can I modify my model to return attention weights during the forward pass?</p>
</li>
<li><p>What is the best way to log attention weights using Comet? Should I log them at regular intervals during training and also at the end of training?</p>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>I attempted to track attention weights from a Transformer model during training and at the end using Comet. I modified the model to return attention weights and implemented code to log these weights periodically and at the final stage.</p>
<p><strong>What I Expected:</strong></p>
<p>I expected to successfully log attention weights for a specific batch or instance, both during training at specified intervals and at the end of the training process, and view these logged weights in the Comet dashboard.</p>
<p><strong>What Actually Resulted:</strong></p>
<p>I encountered issues in selecting the correct batch or instance to track and experienced difficulties in logging attention weights in a way that is viewable in the Comet dashboard. I need guidance on the best practices for selecting batches/instances and logging attention weights</p>
","transformer-model"
"78779077","Transformer models for contextual word embedding in large datasets","2024-07-22 13:48:51","","0","28","<python><nlp><transformer-model><word-embedding>","<p>I'm interested in using contextual word embeddings generated by a transformer-based model to explore the similarity of certain words in a large dataset.</p>
<p>Most transformer models only allow up to 512 tokens of input. So presumably I would need to break the dataset down into individual sentences (&lt;512t) &amp; feed them into the model. That would give me a list of word embeddings per sentence. I'm struggling to understand how I could best translate this list into meaningful embeddings per word across the whole dataset.</p>
<p>The immediately obvious approach would be to find the average embedding for each word. However, the point of contextual embedding is that it can identify different uses/meanings for the same word. 'Bank' as a noun and 'bank' as an adjective may have very different embeddings and therefore I'm not sure that the average would have a great deal of meaning. Is this a genuine concern? Is there a better approach?</p>
<p>In such a use case is there any value in using a contextual transformer model over a static one?</p>
","transformer-model"
"78775009","Transformer Model Repeating Same Codon During Inference Despite High Training Accuracy","2024-07-21 11:06:50","","0","23","<python><machine-learning><pytorch><nlp><transformer-model>","<p>I'm working on a transformer-based model to translate amino acids to codons. During training and validation, my model achieves 95-98% accuracy. However, during inference, I encounter an issue where the output sequence consists of the same codon repeated over and over again, like this:</p>
<p>gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc gcc ...</p>
<p>Here is my inference code:</p>
<h1>Define the inference function</h1>
<pre><code>def infer(model, path_infer, start_token_id=2, output_level='DNA'): 
    if output_level == 'DNA':
        tokenizer = Tokenizer.from_file(tokenizer_DNAfile)
    else:
        tokenizer = Tokenizer.from_file(tokenizer_RNAfile)
    model.eval()
    infer_data = s.fasta_to_list(path_infer, seq_to_codon=False, separate_aa=True, sos_eos=False)
    fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_AAfile)
    src = fast_tokenizer.encode(infer_data[0], padding='max_length', return_tensors=&quot;pt&quot;, max_length=config['input_length'])
    max_length = torch.count_nonzero(src).item()
    
    with torch.no_grad():
        src_embed = model.src_embedding(src)
        memory = model.encoder(src_embed)
        tgt_input = torch.LongTensor([[start_token_id]])
        output_sequence = [start_token_id]
        
        for _ in range(max_length):
            tgt_embed = model.tgt_embedding(tgt_input)
            decoder_output = model.decoder(memory, tgt_embed)
            output_logits = model.fc_out(decoder_output[:, -1, :])
            next_token = output_logits.argmax(dim=-1).item()
            output_sequence.append(next_token)
            tgt_input = torch.cat((tgt_input, torch.LongTensor([[next_token]])), dim=1)
    
    return tokenizer.decode(output_sequence[1:], skip_special_tokens=True)

loading_path = os.path.join(checkpoint_dir, config['model_name'])
model.load_checkpoint(loading_path, model, optimizer)
z = infer(model, path_infer)
print(z)
</code></pre>
<h3>Details:</h3>
<ul>
<li><p><strong>Framework</strong>: PyTorch</p>
</li>
<li><p><strong>Model</strong>: Transformer</p>
</li>
<li><p><strong>Tokenizer</strong>: PreTrainedTokenizerFast for amino acids, custom tokenizers for DNA/RNA</p>
</li>
<li><p><strong>Training Accuracy</strong>: 95-98%</p>
</li>
<li><p><strong>Inference Output</strong>: Repetitive sequences (e.g., &quot;gcc gcc gcc...&quot;)</p>
</li>
</ul>
<h3>Steps I've Taken:</h3>
<ol>
<li><p><strong>Checked the training and validation stages</strong>: The model performs well with high accuracy.</p>
</li>
<li><p><strong>Examined the inference code</strong>: Ensure it follows standard transformer inference practices.</p>
</li>
</ol>
<h3>Problem:</h3>
<p>During inference, the model outputs repetitive codons, which is not expected given the high accuracy during training and validation.</p>
<h3>Questions:</h3>
<ol>
<li><p>What could be causing the model to generate repetitive sequences during inference?</p>
</li>
<li><p>Are there any common pitfalls in transformer inference that might lead to this behavior?</p>
</li>
<li><p>How can I modify my inference function to address this issue?</p>
</li>
</ol>
<h3>Additional Information:</h3>
<ul>
<li><p>Using <code>argmax</code> for next token selection.</p>
</li>
<li><p>The model is loaded correctly and is set to evaluation mode.</p>
</li>
</ul>
<p>Any insights or suggestions would be greatly appreciated!</p>
","transformer-model"
"78772141","How to Estimate GPU Memory for training and inference, Data Requirements, and Training Time for Large Language Models?","2024-07-20 07:32:20","","-1","32","<deep-learning><nlp><nvidia><large-language-model><transformer-model>","<p><strong>This is a very concrete and well-defined computer engineering question. I don't understand why someone would want to close it.</strong></p>
<p>Today, I faced this question during an interview for an ML Engineer position. I didn't answer it perfectly at the time. How should I answer it ideally?</p>
<p>Assume we have models like Transformer, BERT, and GPT, each with <code>x</code> billion parameters, and the parameters are in FP32 precision.</p>
<ol>
<li><p><strong>GPU Memory Requirements</strong>:</p>
<ul>
<li>If using Adam as the optimizer, how much GPU memory is needed during <strong>training</strong> (considering activations, optimizer states like momentum, etc.) and <strong>inference</strong> (considering KV Cache, etc.)?</li>
<li>Provide specific values for <code>x = 7B</code> and <code>x = 70B</code>. How many H100 GPUs at least are required to fit the memory? (H100 80GB Memory)</li>
</ul>
</li>
<li><p><strong>Data Requirements</strong>:</p>
<ul>
<li>If your manager assigns you this task, how much training data of text (in <strong>TB</strong>) would you request to ensure the <code>x</code> billion parameter model converges?</li>
<li>Provide specific value for <code>x = 7B</code> and <code>x = 70B</code>.</li>
</ul>
</li>
<li><p><strong>Training Time</strong>:</p>
<ul>
<li>If your manager wants the training to be completed in one month (30 days) by using above amount of training data, how many H100 GPUs would you request?</li>
<li>Consider the H100 specs (Tensor Core FP32 989 TFLOPS with sparsity (true dense TFLOPS is just half: 495 TFLOPS), GPU memory bandwidth ~ 3TB/s, Interconnect 900 GB/s) and parallel efficiency (speedup) for multiple GPUs.</li>
</ul>
</li>
</ol>
","transformer-model"
"78760862","How does the transformer model's attention mechanism deal with differing sequence lengths?","2024-07-17 17:29:38","","1","21","<huggingface-transformers><large-language-model><transformer-model>","<p>I am going through the architecture of the transformer and its attention mechanism. The thing I don't get about this mechanism is how it handles sequences of different lengths. For example:</p>
<p>How does the attention mechanism handle shorter sequences compared to longer ones?
Any special preprocessing necessary for sequences of unequal lengths?
How does padding impact the attention calculations, and is there a standard way to handle padding with transformer models?
All explanations or references to examples would be very useful!</p>
<p>references:
<a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1706.03762</a>
<a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">https://jalammar.github.io/illustrated-transformer/</a></p>
","transformer-model"
"78759049","Using Sparse Categorical CrossEntropy, the loss becomes negative (tensorflow/keras)","2024-07-17 10:40:58","","0","29","<python><tensorflow><keras><transformer-model>","<p>I am doing the Tensorflow TF tutorial (<a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a>) but with my own data. My data is not related to text, but it is sequences of tokens anyway, with a start token, and an end token. All the tokens go from 0 to 30 (start token is 31, end is 32). The lentgh of the sequence is 64 (in total 66 with the start and end tokens). A sequence looks like:</p>
<pre><code>tf.Tensor(
[31 10 10 10 10 18 10 19 27 22  5 19 10 10 10 10 10 19 10 19 10  1  1 20
 22 15 12 26 14 22 17  3 10 14 22  9 25 25 20  7 19 28  4  7 15 14 13 25
 21 15 15 17 14 18 14 14 14 27 14 19 25 19  5  3 17 32], shape=(66,), dtype=int32)
</code></pre>
<p>My code is very similar to the one from the tutorial, with very small changes:</p>
<pre><code>import pickle, os
 
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import layers
 
from src.utils.callbacks import VQTransCallback
from src.models.layers import GlobalSelfAttention, CrossAttention, CausalSelfAttention
from src.models.layers import TransformerFeedForward as FeedForward
 
 
 
class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.d_model = d_model
        self.embedding = layers.Embedding(vocab_size, d_model, mask_zero=True) 
        self.pos_encoding = self.positional_encoding(length=66, depth=d_model)
 
    def compute_mask(self, *args, **kwargs):
        return self.embedding.compute_mask(*args, **kwargs)
 
    def positional_encoding(self, length, depth):
        depth = depth/2
        positions = np.arange(length)[:, np.newaxis]     # (seq, 1)
        depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)
        angle_rates = 1 / (10000**depths)         # (1, depth)
        angle_rads = positions * angle_rates      # (pos, depth)
        pos_encoding = np.concatenate(
            [np.sin(angle_rads), np.cos(angle_rads)],
            axis=-1) 
        return tf.cast(pos_encoding, dtype=tf.float32)
 
    def call(self, x):
        length = tf.shape(x)[1]
        x = self.embedding(x)
        # This factor sets the relative scale of the embedding and positonal_encoding.
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x = x + self.pos_encoding[tf.newaxis, :length, :]
        return x
 
 
class EncoderLayer(tf.keras.layers.Layer):
  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):
    super().__init__()
 
    self.self_attention = GlobalSelfAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)
 
    self.ffn = FeedForward(d_model, dff)
    self.supports_masking = True
 
  def call(self, x):
    x = self.self_attention(x)
    x = self.ffn(x)
    return x
  
 
class Encoder(tf.keras.layers.Layer):
  def __init__(self, *, num_layers, d_model, num_heads,
               dff, vocab_size, dropout_rate=0.1):
    super().__init__()
 
    self.d_model = d_model
    self.num_layers = num_layers
    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)
 
    self.enc_layers = [
        EncoderLayer(d_model=d_model,
                     num_heads=num_heads,
                     dff=dff,
                     dropout_rate=dropout_rate)
        for _ in range(num_layers)]
    self.dropout = tf.keras.layers.Dropout(dropout_rate)
    self.supports_masking = True
 
  def call(self, x):
    x = self.pos_embedding(x)  # Shape `(batch_size, d_model)`.
    x = self.dropout(x)
    for i in range(self.num_layers):
      x = self.enc_layers[i](x)
    return x  # Shape `(batch_size, seq_len, d_model)`.
 
 
class DecoderLayer(tf.keras.layers.Layer):
  def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):
    super(DecoderLayer, self).__init__()
 
    self.causal_self_attention = CausalSelfAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)
    self.cross_attention = CrossAttention(
        num_heads=num_heads,
        key_dim=d_model,
        dropout=dropout_rate)
    self.ffn = FeedForward(d_model, dff)
    self.supports_masking = True
 
  def call(self, x, context):
    x = self.causal_self_attention(x=x)
    x = self.cross_attention(x=x, context=context)
    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.
    return x
  
 
class Decoder(tf.keras.layers.Layer):
  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,
               dropout_rate=0.1):
    super(Decoder, self).__init__()
 
    self.d_model = d_model
    self.num_layers = num_layers
    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)
    self.dropout = tf.keras.layers.Dropout(dropout_rate)
    self.dec_layers = [
        DecoderLayer(d_model=d_model, num_heads=num_heads,
                     dff=dff, dropout_rate=dropout_rate)
        for _ in range(num_layers)]
    self.supports_masking = True
 
  def call(self, x, context):
    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)
    x = self.dropout(x)
    for i in range(self.num_layers):
        x  = self.dec_layers[i](x, context)
    return x
 
 
class VQVAE2Transformer(tf.keras.Model):
    def __init__(self, *, enc_num_layers, dec_num_layers, d_model, num_heads, dff,
                 vocab_size, codebook_length, dropout_rate=0.1):
        super().__init__()
        self.encoder = Encoder(num_layers=enc_num_layers, d_model=d_model,
                                num_heads=num_heads, dff=dff, vocab_size=vocab_size,
                                dropout_rate=dropout_rate)
        self.decoder = Decoder(num_layers=dec_num_layers, d_model=d_model,
                                num_heads=num_heads, dff=dff, vocab_size=vocab_size,
                                dropout_rate=dropout_rate)
        
        self.enc_num_layers = enc_num_layers
        self.dec_num_layers = dec_num_layers
        self.d_model = d_model
        self.num_heads = num_heads
        self.dff = dff
        self.vocab_size = vocab_size + 3 + 1 # +3 start, end and mask
        self.codebook_length = codebook_length + 2 # +2 start and end tokens
 
        self.start_token = vocab_size+1
        self.end_token = vocab_size+2
 
        self.final_layer = tf.keras.layers.Dense(self.vocab_size)
        self.supports_masking = True
 
 
    def call(self, inputs):
        enc_in, dec_in = inputs[0], inputs[1]
        
        context = self.encoder(enc_in)  # (batch_size, context_len, d_model)
        x = self.decoder(dec_in, context)  # (batch_size, target_len, d_model)
        # Final linear layer output.
        logits = self.final_layer(x)  # (batch_size, target_len, latent_size)
 
        try:
            # Drop the keras mask, so it doesn't scale the losses/metrics.
            # b/250038731
            del logits._keras_mask
        except AttributeError:
            pass
 
        # Return the final output and the attention weights.
        return logits
 
 
    def accuracy(self, label, pred):
        pred = tf.argmax(pred, axis=2)
        label = tf.cast(label, pred.dtype)
        match = label == pred
        match = tf.cast(match, dtype=tf.float32)
        return tf.reduce_sum(match) / self.codebook_length
 
 
    def compile_model(self):
        optimizer = keras.optimizers.AdamW()
        scce = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=True)
 
        self.compile(optimizer=optimizer,
                     loss=scce,
                     metrics=[&quot;accuracy&quot;])
 
 
    def save_build(self, folder):
        &quot;&quot;&quot;Saves the config before the training starts. The model itself will be saved
        later on using keras checkpoints.
 
        Args:
            folder: Where to save the config parameters
        &quot;&quot;&quot;
        if not os.path.exists(folder):
            os.makedirs(folder)
            os.makedirs(os.path.join(folder, 'weights'))
 
        with open(os.path.join(folder, 'params.pkl'), 'wb') as f:
            pickle.dump([
                self.enc_num_layers,
                self.dec_num_layers,
                self.d_model,
                self.num_heads,
                self.dff,
                self.vocab_size,
            ], f)
 
 
    def train_step(self, batch):
        &quot;&quot;&quot;Processes one batch inside model.fit().&quot;&quot;&quot;
 
        enc_in, dec_in = batch[0][:,:66], batch[0][:,66:]
        dec_input = dec_in[:, :-1]
        dec_target = dec_in[:, 1:]
        with tf.GradientTape() as tape:
            preds = self([enc_in, dec_input])
            loss = self.compute_loss(None, dec_target, preds)
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
 
        # Update the metrics
        self.compiled_metrics.update_state(dec_target, preds)
        return {m.name: m.result() for m in self.metrics}
 
 
    def test_step(self, batch):
        enc_in, dec_in = batch[0][:,:66], batch[0][:,66:]
        dec_input = dec_in[:, :-1]
        dec_target = dec_in[:, 1:]
        preds = self([enc_in, dec_input])
        self.compiled_metrics.update_state(dec_target, preds)
        return {m.name: m.result() for m in self.metrics}
 
 
    def train(
        self, train_dataset, valid_dataset, epochs, run_folder, initial_epoch=0, 
        print_every_n_epochs=5
    ):
        
        test_batch = next(valid_dataset.dataset_iter)
        display_cb = VQTransCallback(test_batch, run_folder)
 
 
        checkpoint_filepath = os.path.join(
                run_folder, &quot;weights/{epoch:03d}-{loss:.5f}-{val_loss:.5f}.weights.h5&quot;)
        checkpoint1 = keras.callbacks.ModelCheckpoint(
            checkpoint_filepath, save_weights_only=True, save_best_only=True)
        checkpoint2 = keras.callbacks.ModelCheckpoint(
            os.path.join(run_folder, 'weights/last.weights.h5'),
            save_weights_only=True, save_best_only=True)
 
        callbacks_list = [checkpoint1, checkpoint2, display_cb]
 
        self.fit(
            train_dataset.dataset, validation_data=valid_dataset.dataset,
            epochs=epochs, initial_epoch=initial_epoch, callbacks=callbacks_list,
            #steps_per_epoch=1000, validation_steps=1000
            )
 
    
    def generate(self, enc_in, dec_in, startid=0):
        &quot;&quot;&quot;Performs inference over one batch of inputs.&quot;&quot;&quot;
        bs = tf.shape(enc_in)[0]
 
        # scce = tf.keras.losses.SparseCategoricalCrossentropy(
        #     from_logits=True)
        # logits = self([enc_in, dec_in[:,:-1]])
        # print(&quot;SCCE LOSS&quot;)
        # print(scce(dec_in[:,1:],logits))
 
        if startid == 0:
            dec_in = tf.ones((bs, 1), dtype=tf.int32) * self.start_token
        else:
            dec_in = tf.cast(dec_in[:,:startid], dtype=tf.int32)
 
        for _ in range(self.codebook_length - startid):
            logits = self([enc_in, dec_in])
            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)
            last_logit = tf.expand_dims(logits[:, -1], axis=-1)
            dec_in = tf.concat([dec_in, last_logit], axis=-1)
 
        return dec_in
</code></pre>
<p>I am using sparse cross entropy as in the tutorial.</p>
<p>I have a problem where the loss that is displayed as the training goes becomes negative. For example right now it says -2.0593.</p>
<p>I am using Sparse Cross Entropy. When I monitor  with a callback the loss of some test batches, the value returned is never negative, but usually some value between 1.5 and 2.</p>
<p>As you can see my last layer in the Transformer is a Dense layer without activation function (as in the tutorial) and therefore in the loss I set &quot;from_logit=True&quot;. I have tried using a softmax activation in that last layer, and then setting &quot;from_logit=False&quot;, but this does not seem to train, and it gets stuck in around a loss of 0.274, and it never moves.</p>
<p>I have no idea why the loss becomes negative, when the loss function always seems to output positive numbers when I test it.</p>
<p>Overall the results of the training are also not good.</p>
","transformer-model"
"78743055","“Bus Error and Resource Tracker Warning When Training PyTorch Model on GPU with MPS”","2024-07-13 06:23:56","","0","46","<pytorch><nlp><huggingface-transformers><large-language-model><transformer-model>","<p>I’ve built a vanilla Transformer using PyTorch for machine translation and am encountering issues while trying to train it on an Apple Mac M3 with a 12-core CPU and an 18-core GPU (18GB RAM) environment. Below are the details and issues I’m facing:</p>
<p>1.Device Selection Issue:</p>
<p>I’m setting up the device with the following code:</p>
<pre><code>device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;mps&quot; if torch.has_mps or torch.backends.mps.is_available() else &quot;cpu&quot;
print(&quot;Using device:&quot;, device)
</code></pre>
<p>Although MPS is detected and selected, training crashes immediately after starting, with the following error:</p>
<pre><code>/opt/anaconda3/envs/mynewenv/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
warnings.warn('resource_tracker: There appear to be %d ').
</code></pre>
<p>2.CPU Training:</p>
<p>When I switch to CPU training on the same machine, it runs without any issues using the same batch size of 8.</p>
<p>3.Google Colab Training:</p>
<p>There are no issues when running the same code on Google Colab.</p>
<p>I’m looking for insights into what might be causing these issues on MPS and how I could resolve them. Specifically, I’d like to understand the semaphore leak and bus error that seems to occur only when using MPS. If needed, I can provide specific code snippets or further details.</p>
<pre><code>from model import build_transformer
from dataset import BilingualDataset, causal_mask
from config import get_config, get_weights_file_path

import torchtext.datasets as datasets
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from torch.optim.lr_scheduler import LambdaLR

import warnings
from tqdm import tqdm
import os
from pathlib import Path

# Huggingface datasets and tokenizers
from datasets import load_dataset
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace

import wandb

import torchmetrics

def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):
    sos_idx = tokenizer_tgt.token_to_id('[SOS]')
    eos_idx = tokenizer_tgt.token_to_id('[EOS]')

    # Precompute the encoder output and reuse it for every step
    encoder_output = model.encode(source, source_mask)
    # Initialize the decoder input with the sos token
    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)
    while True:
        if decoder_input.size(1) == max_len:
            break

        # build mask for target
        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)

        # calculate output
        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)

        # get next token
        prob = model.project(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        decoder_input = torch.cat(
            [decoder_input, torch.empty(1, 1).type_as(source).fill_(next_word.item()).to(device)], dim=1
        )

        if next_word == eos_idx:
            break

    return decoder_input.squeeze(0)


def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, num_examples=2):
    model.eval()
    count = 0

    source_texts = []
    expected = []
    predicted = []

    try:
        # get the console window width
        with os.popen('stty size', 'r') as console:
            _, console_width = console.read().split()
            console_width = int(console_width)
    except:
        # If we can't get the console width, use 80 as default
        console_width = 80

    with torch.no_grad():
        for batch in validation_ds:
            count += 1
            encoder_input = batch[&quot;encoder_input&quot;].to(device) # (b, seq_len)
            encoder_mask = batch[&quot;encoder_mask&quot;].to(device) # (b, 1, 1, seq_len)

            # check that the batch size is 1
            assert encoder_input.size(
                0) == 1, &quot;Batch size must be 1 for validation&quot;

            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)

            source_text = batch[&quot;src_text&quot;][0]
            target_text = batch[&quot;tgt_text&quot;][0]
            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())

            source_texts.append(source_text)
            expected.append(target_text)
            predicted.append(model_out_text)
            
            # Print the source, target and model output
            print_msg('-'*console_width)
            print_msg(f&quot;{f'SOURCE: ':&gt;12}{source_text}&quot;)
            print_msg(f&quot;{f'TARGET: ':&gt;12}{target_text}&quot;)
            print_msg(f&quot;{f'PREDICTED: ':&gt;12}{model_out_text}&quot;)

            if count == num_examples:
                print_msg('-'*console_width)
                break
    
    
    # Evaluate the character error rate
    # Compute the char error rate 
    metric = torchmetrics.CharErrorRate()
    cer = metric(predicted, expected)
    wandb.log({'validation/cer': cer, 'global_step': global_step})

    # Compute the word error rate
    metric = torchmetrics.WordErrorRate()
    wer = metric(predicted, expected)
    wandb.log({'validation/wer': wer, 'global_step': global_step})

    # Compute the BLEU metric
    metric = torchmetrics.BLEUScore()
    bleu = metric(predicted, expected)
    wandb.log({'validation/BLEU': bleu, 'global_step': global_step})

def get_all_sentences(ds, lang):
    for item in ds:
        yield item['translation'][lang]

def get_or_build_tokenizer(config, ds, lang):
    tokenizer_path = Path(config['tokenizer_file'].format(lang))
    if not Path.exists(tokenizer_path):
        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour
        tokenizer = Tokenizer(WordLevel(unk_token=&quot;[UNK]&quot;))
        tokenizer.pre_tokenizer = Whitespace()
        trainer = WordLevelTrainer(special_tokens=[&quot;[UNK]&quot;, &quot;[PAD]&quot;, &quot;[SOS]&quot;, &quot;[EOS]&quot;], min_frequency=2)
        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)
        tokenizer.save(str(tokenizer_path))
    else:
        tokenizer = Tokenizer.from_file(str(tokenizer_path))
    return tokenizer

def get_ds(config):
    # It only has the train split, so we divide it overselves
    ds_raw = load_dataset('opus_books', f&quot;{config['lang_src']}-{config['lang_tgt']}&quot;, split='train')

    # Build tokenizers
    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])
    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])

    # Keep 90% for training, 10% for validation
    train_ds_size = int(0.9 * len(ds_raw))
    val_ds_size = len(ds_raw) - train_ds_size
    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])

    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])
    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])

    # Find the maximum length of each sentence in the source and target sentence
    max_len_src = 0
    max_len_tgt = 0

    for item in ds_raw:
        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids
        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids
        max_len_src = max(max_len_src, len(src_ids))
        max_len_tgt = max(max_len_tgt, len(tgt_ids))

    print(f'Max length of source sentence: {max_len_src}')
    print(f'Max length of target sentence: {max_len_tgt}')
    

    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)
    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)

    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt

def get_model(config, vocab_src_len, vocab_tgt_len):
    model = build_transformer(vocab_src_len, vocab_tgt_len, config[&quot;seq_len&quot;], config['seq_len'], d_model=config['d_model'])
    return model

def train_model(config):
    # Define the device
    # device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else  &quot;cpu&quot;)

    # Define the device
    device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;mps&quot; if torch.has_mps or torch.backends.mps.is_available() else &quot;cpu&quot;
    print(&quot;Using device:&quot;, device)

    # if device == 'cuda':
    # # CUDA device information
    #     cuda_device = torch.cuda.current_device()
    #     print(f&quot;Device name: {torch.cuda.get_device_name(cuda_device)}&quot;)
    #     print(f&quot;Device memory: {torch.cuda.get_device_properties(cuda_device).total_memory / 1024 ** 3} GB&quot;)
    # elif device == 'mps':
    #     # MPS device information
    #     print(&quot;Device name: &lt;MPS&gt;&quot;)  # Adjust this based on specific MPS details if available
    # else:
    # # CPU device information or fallback message
    #     print(&quot;NOTE: If you have a GPU, consider using it for training.&quot;)
    #     print(&quot;      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc&quot;)
    #     print(&quot;      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu&quot;)

    # Set device for torch tensors
    device = torch.device(device)

    # Make sure the weights folder exists
    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)

    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)
    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)

    # If the user specified a model to preload before training, load it
    initial_epoch = 0
    global_step = 0
    if config['preload']:
        model_filename = get_weights_file_path(config, config['preload'])
        print(f'Preloading model {model_filename}')
        state = torch.load(model_filename)
        model.load_state_dict(state['model_state_dict'])
        initial_epoch = state['epoch'] + 1
        optimizer.load_state_dict(state['optimizer_state_dict'])
        global_step = state['global_step']
        del state

    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id('[PAD]'), label_smoothing=0.1).to(device)

    # define our custom x axis metric
    wandb.define_metric(&quot;global_step&quot;)
    # define which metrics will be plotted against it
    wandb.define_metric(&quot;validation/*&quot;, step_metric=&quot;global_step&quot;)
    wandb.define_metric(&quot;train/*&quot;, step_metric=&quot;global_step&quot;)

    for epoch in range(initial_epoch, config['num_epochs']):
        torch.cuda.empty_cache()
        model.train()
        batch_iterator = tqdm(train_dataloader, desc=f&quot;Processing Epoch {epoch:02d}&quot;)
        for batch in batch_iterator:

            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)
            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)
            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)
            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)

            # Run the tensors through the encoder, decoder and the projection layer
            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)
            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)
            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)

            # Compare the output with the label
            label = batch['label'].to(device) # (B, seq_len)

            # Compute the loss using a simple cross entropy
            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))
            batch_iterator.set_postfix({&quot;loss&quot;: f&quot;{loss.item():6.3f}&quot;})

            # Log the loss
            wandb.log({'train/loss': loss.item(), 'global_step': global_step})

            # Backpropagate the loss
            loss.backward()

            # Update the weights
            optimizer.step()
            optimizer.zero_grad(set_to_none=True)

            global_step += 1

        # Run validation at the end of every epoch
        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step)

        # Save the model at the end of every epoch
        model_filename = get_weights_file_path(config, f&quot;{epoch:02d}&quot;)
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'global_step': global_step
        }, model_filename)


if __name__ == '__main__':
    warnings.filterwarnings(&quot;ignore&quot;)
    config = get_config()
    config['num_epochs'] = 30
    config['preload'] = None

    wandb.init(
        # set the wandb project where this run will be logged
        project=&quot;pytorch-transformer&quot;,
        
        # track hyperparameters and run metadata
        config=config
    )
    
    train_model(config)

</code></pre>
","transformer-model"
"78738444","Text summarizations of comments and replace the duplicates with the first occurrence if the meaning is comment is same","2024-07-12 03:18:09","","1","22","<nlp><transformer-model><summarization>","<p>Context - Doing an NLP project to analyze comments column in a data frame. I want to replace the duplicates with the first occurrence if the meaning of the comments are same.</p>
<p>I wants to compare all the sentences and consider the sentences which have similar meaning.</p>
<p>I have tried transformer for summarization and cosine to find similarity between sentences. but it is not giving me the desired results.</p>
<p>Any help will be deeply appreciated.</p>
<p>code is here -</p>
<pre><code>#functions to lemmatize and remove punctuation
def remove_punctuation(text):
    text = text.lower()
    text=text.strip()
    text = text.translate(str.maketrans('','',string.punctuation))
    text = sent_tokenize(text)
    text_list=[t.strip() for t in text]
    return text_list


def lemmatize_text(text):
    lemmatizer = WordNetLemmatizer()
    words = word_tokenize(text.lower())
    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]
    return &quot; &quot;.join(lemmatized_words)

model=SentenceTransformer('paraphrase-MiniLM-L6-v2')

#this will have unique values of other comments column. values after summarizations will be replaced in other comments column to bring uniformity
Other_comments_unique=df1[&quot;DUPLICATE - Other comments&quot;].apply(lambda x:x.strip().lower().translate(str.maketrans('','',string.punctuation))).unique()
df1['DUPLICATE - Other comments']=df1['DUPLICATE - Other comments'].apply(remove_punctuation)
df1['DUPLICATE - Other comments'].head()

#summarization - meaning extracting the core message of the sentence. done in this cell
changed_values={}
for i in range(0,len(df1)):
    items = [n for n in df1['test'][i]]
    for u in Other_comments_unique:
        if u!=&quot;nothing more&quot;:
            for a in items:
                embeddings1=model.encode(u)
                embeddings2=model.encode(a)
                cosine_sim = util.cos_sim(embeddings1,embeddings2)
                if cosine_sim.item()&gt;0.5 and cosine_sim.item()&lt;.99:
                changed_values.update({u:a})
        items=[]
</code></pre>
","transformer-model"
"78728147","Why am I seeing unused parameters in position embeddings when using relative_key in BertModel?","2024-07-10 00:10:08","","1","33","<pytorch><huggingface-transformers><bert-language-model><transformer-model>","<p>I am training a BERT model using pytorch and HuggingFace's BertModel. The sequences of tokens can vary in length from 1 (just a CLS token) to 128. The model trains fine when using absolute position embeddings, but when I switch to using relative position embeddings (specifically setting <code>position_embedding_type=&quot;relative_key&quot;</code>), training fails because of unused parameters. When I investigate further (adding print statements as proposed in <a href=""https://discuss.pytorch.org/t/how-to-find-the-unused-parameters-in-network/63948/5"" rel=""nofollow noreferrer"">this thread</a>), I find that the unused parameter is <code>module.bert_model.embeddings.position_embeddings.weight</code>.</p>
<p>I am aware that I can avoid this error by setting <code>find_unused_parameters=True</code> in <code>DDP</code>, and that runs fine. But I'd like to understand why this is happening, to make sure there isn't a problem.</p>
<p>I tried padding all sequences to be the maximum length, and that did not help.
I tried adjusting the attention mask to be all ones, and that did not help.
I would expect the model to train and to use the position_embeddings.weight parameter, but it is not using it.</p>
<p>Why would switching to relative position embeddings cause the position_embeddings.weight parameter to be unused?</p>
","transformer-model"
"78722064","Positional encoding for explicitly timestamped transformer input data","2024-07-08 16:48:58","","0","74","<time-series><transformer-model>","<p>I understand the concept of positional encoding for e.g text data when embedding a sequence for input to a transformer model - the transformer's attention mechanism processes the entire sequence at once, and therefore some method is needed to preserve information about the positional relationships between elements in the original input sequence.</p>
<p>That said, when inputting numerical data - for example a sequence of measurements of a robot's position over time - it would make sense to me to simply include the timestamp in the feature vector representing each measurement. This seems like a very obvious and easy-to-learn representation. However when researching this, it seems as though preference is still to include the traditional sin/cos method of positional encoding.</p>
<p>This seems counterintuitive to me. If I receive my measurements out of order, then adding positional encoding is just going to tell the transformer the order I received them in, rather than a more useful chronological order. Alternatively, I could first time-order my set of measurements and then add the positional encoding, but to time-order them in the first place I'm just going to sort on the timestamp, so adding positional encoding on top of this seems redundant. And if my measurements aren't recorded at regular intervals, isn't it more useful to learn temporal information from an exact timestamp than a generic sin/cos sequence?</p>
<p>Why is it not enough to simply include an explicit timestamp? And is the traditional sin/cos method still optimal if successive measurements aren't equally spaced in time?</p>
","transformer-model"
"78715749","How to fix this error: KeyError: 'model.embed_tokens.weight'","2024-07-06 19:41:44","","0","155","<large-language-model><transformer-model><pre-trained-model><llama><checkpoint>","<p>This is the detailed error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/home/cyq/zxc/SmartEdit/train/DS_MLLMSD11_train.py&quot;, line 769, in &lt;module&gt;
    train()
  File &quot;/home/cyq/zxc/SmartEdit/train/DS_MLLMSD11_train.py&quot;, line 540, in train
    model_.load_pretrain_MLLM_alignment(SD_QFormer_conversation_33tokens=SD_QFormer_conversation_33tokens, LLaVA_00002=LLaVA_00002)
  File &quot;/home/cyq/zxc/SmartEdit/model/DS_MLLMSD11_model.py&quot;, line 227, in load_pretrain_MLLM_alignment
    LLaMA_word2vec = weights.pop('model.embed_tokens.weight')
KeyError: 'model.embed_tokens.weight'
</code></pre>
<p>In this code, I first initialized the qformer, and I want to load the pre-trained weights of qformer from this path: '/SmartEdit-7B/embeddings_qformer/checkpoint-15000_embeddings_qformer.bin'. However, I encountered this problem when loading the weights. The loading code is as follows:</p>
<pre><code> def load_pretrain_MLLM_alignment(self, SD_QFormer_conversation_33tokens, LLaVA_00002):
        weights = torch.load(SD_QFormer_conversation_33tokens, map_location=&quot;cpu&quot;)
        LLaVA_00002_weights = torch.load(LLaVA_00002, map_location=&quot;cpu&quot;)
        print('mm_projector weight:', weights['mm_projector.weight'] == LLaVA_00002_weights['model.mm_projector.weight'])
        print('mm_projector bias:', weights['mm_projector.bias'] == LLaVA_00002_weights['model.mm_projector.bias'])

        # 1. vec2word: Linear(in_features=4096, out_features=32035, bias=False)
        LLaMA_lm_haed = weights.pop('lm_head.weight')
        LLaMA_lm_haed = LLaMA_lm_haed[-self.config.num_new_tokens:]
        self.lm_head.weight.data[-self.config.num_new_tokens:] = LLaMA_lm_haed
        original_LLaMA_lm_head = self.original_lm_head_value
        self.lm_head.weight.data[:-self.config.num_new_tokens] = original_LLaMA_lm_head
        print('Matching language model head:', self.lm_head.weight.data[0] == self.original_LLM_language_model_head_0)

        # 2. word2vec: Embedding(32035, 4096)
        LLaMA_word2vec = weights.pop('model.embed_tokens.weight')
        LLaMA_word2vec = LLaMA_word2vec[-self.config.num_new_tokens:]
        self.model.embed_tokens.weight.data[-self.config.num_new_tokens:] = LLaMA_word2vec
        original_LLaMA_embed_tokens = self.origin_inp_embedding
        self.model.embed_tokens.weight.data[:-self.config.num_new_tokens] = original_LLaMA_embed_tokens
        print('Matching word embedding:', self.model.embed_tokens.weight.data[0] == self.original_LLM_word_embedding_0)

        # 3. mm_projector
        mm_projector_param = {'weight': weights.pop('mm_projector.weight'), 'bias': weights.pop('mm_projector.bias')}
        self.mm_projector.load_state_dict(mm_projector_param, strict=True)

        # 4. SD_Query and SD_Qformer -&gt; remove 'sd_qformer.'
        self.sd_query_tokens.data = weights.pop('sd_query_tokens').float()
        self.sd_qformer.load_state_dict({k[len('sd_qformer.'):]: v for k, v in weights.items()})
        print('Loading embeddings for Qformer checkpoint:', self.sd_qformer.load_state_dict({k[len('sd_qformer.'):]: v for k, v in weights.items()}, strict=True))
</code></pre>
<p>Could you help me solving this problem? Thanks a lot!</p>
","transformer-model"
"78712626","Pytorch, use loss that don't return gradient","2024-07-05 17:05:32","","0","35","<audio><pytorch><gradient-descent><transformer-model><custom-training>","<p>I'm trying to develop a model that improves the quality of a given audio. For this task I use <a href=""https://github.com/descriptinc/descript-audio-codec"" rel=""nofollow noreferrer"">DAC</a> for the latent space and I run a transformer model to change the value of the latent space to improve the audio. Then I use the decoder of the DAC model to decode the audio and generate the waveform. So what I do is: noise audio &gt; Encode audio &gt; latent space A &gt; transformer model (that I train) &gt; latent space B &gt; Decode audio &gt; denoised audio</p>
<p>I want to test the approach of training my transformer model on the raw audio itself (not on the latent space B) so I need to decode the audio generated during the training and then use the L1 loss between the noised audio and the denoised generated audio. The problem is that I can't keep the gradient of my transformer during the decoding process (too much RAM used and I don't want to train the decoder). I know that PyTorch uses the chain rule to compute the gradient but why does it need to know the gradient of the decoder to change the weights of my model? The decoding process is only useful to compute the loss so I can imagine that the chain rule stops at the end of my transformer model. It can compute the gradient on the result of my L1 loss and then change the weights of the transformer model only with the result of the loss.</p>
<p>Currently I have the error:</p>
<pre><code>loss.backward()
File &quot;/home/jourdelune/Bureau/dev/WaveAI/AudioEnhancer/venv/lib/python3.10/site-packages/torch/_tensor.py&quot;, line 492, in backward
    torch.autograd.backward(
File &quot;/home/jourdelune/Bureau/dev/WaveAI/AudioEnhancer/venv/lib/python3.10/site-packages/torch/autograd/__init__.py&quot;, line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>I have tried some approaches, but what I expect is to train my model like this:</p>
<pre class=""lang-py prettyprint-override""><code>y_hat = model(x)  # prediction of the model
y_hat = rearrange(y_hat, &quot;b (t c) d -&gt; b c d t&quot;, c=c, d=d)  # transform the data to have the latent space dim
loss = 0
for i in range(y_hat.shape[0]):  # for each value in the batch, DAC doesn't support decoding of batches
    with torch.no_grad():  # disable gradient because too much RAM
        z_q, _, _, _ = dataset.autoencoder.quantizer(y_hat[i].float(), None)
        decoded = dataset.autoencoder.decode(z_q)  # RAW audio
    out = l1_loss(decoded, base_waveform[i][:, :, : decoded.shape[-1]])  # compute the distance between the base waveform (target) and the decoded audio (pred)
    loss += out
loss.backward()  # backpropagation
</code></pre>
<p>I have tried to do:</p>
<pre class=""lang-py prettyprint-override""><code>loss = torch.tensor(loss, requires_grad=True)
loss.backward()
</code></pre>
<p>but the model doesn't progress at all. So my question is, is it possible to do it this way?</p>
","transformer-model"
"78707719","How do I train a transformer for pointwise inference of time series data?","2024-07-04 14:43:16","","0","18","<time-series><regression><loss-function><transformer-model>","<p>My dataset is composed of different particle trajectories over time [x(t), y(t)]. Each trajectory has a different length from another and my goal is to perform pointwise regression, i.e. estimate a continuous value a(t) for each time step. Thus if my input is of dimension [batch_size, sequence_length, trajectory_coordinates] I expect my output to be something like [batch_size, sequence_length, 1].</p>
<p>So far, I have trained an architecture composed of:</p>
<ul>
<li>Convolutional module</li>
<li>Self-attention module</li>
<li>Feedforward module</li>
</ul>
<p>Because of the different lengths of the inputs, I apply a zero-padding on it and I feed the correponding masks altogether with the inputs. As loss, I mainly adopted MAE and MSE.</p>
<p>When training, the model clearly tends to learn the median/mean value of all the dataset ( all the particles and all the values for each time). Here an example of the performance of the algorithm:</p>
<p><a href=""https://i.sstatic.net/tI2Ue7yf.png"" rel=""nofollow noreferrer"">The red-dashed line is the ground truth of a(t). The black line is the output of the model for a(t). The blue line is the average over all particles and time steps of the ground truth available.</a></p>
<p>As it can be noticed, the model is completely incapable of detecting neither a correct value or a change in the property of interest.</p>
<p>I think that there might be some mistake either on the choice of the loss or in implementing it in the code.</p>
<p>In the script, I first declare the loss (in this case MAE) without any form of reduction since I have some nan values that come from the masking.</p>
<pre><code>criterion = nn.L1Loss(reduction='none')
</code></pre>
<p>Then, in the training cycle, I calculate the loss. Recall that output (and labels) are objects of size [batch_size, sequence_length, 1]. Then I sum all the non-nan values and average by the number of elements that are not nan.</p>
<pre><code>loss = criterion(output, labels)
final_loss = torch.nansum(loss).float() / key_masks.count_nonzero()
</code></pre>
<p>I believe that I am making a mistake by implicitly averaging over the time steps.
Do you have any solution to recommend?</p>
","transformer-model"
"78704181","How can a Transformer model predict output by a Python loop?","2024-07-03 20:57:25","","0","40","<transformer-model>","<p>I understand how a transformer model combines input and output to produce a shifted output, but I don't understand how it hires a loop to generate word by word when the training is done?<br>
I guess it assumes that the output has only one start token at the first and then it feeds the input and assumed output into the model and catches the last index of the prediction and by this edits next index of output and feed them again into the model.
Something like this:<br><br>
<em>The goal is to convert 'I am Student' to 'Ich bin Student'<br>
English tokens: {0: PAD, 1: START, 2: END, 3: Am, 4: Student, 5: I}<br>
German tokens: {0: PAD, 1: START, 2: END, 3: Bin, 4: Ich, 5: Student}</em><br><br>
<strong>Loop 1.</strong><br>
input: [1, 5, 3, 4, 2, 0]<br>
output: [1, 0, 0, 0, 0, 0]<br>
predict: transformer((input, output)) = [4, 0, 0, 0, 0, 0]<br>
next-token = 4<br><br>
<strong>Loop 2.</strong><br>
input: [1, 5, 3, 4, 2, 0]<br>
output: [1, 4, 0, 0, 0, 0]<br>
predict: transformer((input, output)) = [4, 3, 0, 0, 0, 0]<br>
next-token = 3<br><br>
<strong>Loop 3.</strong><br>
input: [1, 5, 3, 4, 2, 0]<br>
output: [1, 4, 3, 0, 0, 0]<br>
predict: transformer((input, output)) = [4, 3, 5, 0, 0, 0]<br>
next-token = 5<br><br>
<strong>Loop 4.</strong><br>
input: [1, 5, 3, 4, 2, 0]<br>
output: [1, 4, 3, 5, 0, 0]<br>
predict: transformer((input, output)) = [4, 3, 5, 2, 0, 0]<br>
next-token = 2<br><br>
<strong>Loop 5.</strong><br>
next-token is 2: END so the loop ends here.<br>
result: [1, 4, 3, 5, 2, 0]<br><br>
Am I thinking right?</p>
","transformer-model"
"78691833","Tensorflow/Keras Transformer struggles to predict the last position in a sequence, but does well in all the other positions","2024-07-01 10:59:52","","0","37","<tensorflow><keras><time-series><transformer-model>","<p>I am using a Transformer for next frame prediction. Every frame has been previously encoded into a 1D latent vector using a VAE (the encoding-decoding from the VAE is very good).</p>
<p>I have divided the videos into chunks of 24 frames, and using the VAE I have created sequences like 24,100 (where 24 is the sequence length, and 100 the latent vectors from the VAE). These sequences are inputted into a small Transformer (GPT-like, only the decoder with casual attention). Because it is batched, the actual input is 64,24,100. Let's call one of those &quot;source&quot;. I input to the transformer &quot;source[:,:-1]&quot; and the target is &quot;source[:,1:]&quot;. The loss function is MSE. When I check the outputs from the transformer, it does an excellent prediction in all the sequence positions (0,23), except in the last one (24). The last one is OK'ish. Not random, but not amazing. As an example, all the other positions get a MSE loss of around 0.006, while the last one gets an MSE loss of 0.15.</p>
<p>These would all indicate that I am not using casual masking - since it does well in all the sequence entries where it can see the future, but it struggles in the last position where it can't. But I am using casual attention, so none of the entries should see the future:</p>
<pre class=""lang-py prettyprint-override""><code>class BaseAttention(tf.keras.layers.Layer):
  def __init__(self, **kwargs):
    super().__init__()
    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
    self.layernorm = tf.keras.layers.LayerNormalization()
    self.add = tf.keras.layers.Add()

class CausalSelfAttention(BaseAttention):
  def call(self, x):
    attn_output = self.mha(
        query=x,
        value=x,
        key=x,
        use_causal_mask = True)
    x = self.add([x, attn_output])
    x = self.layernorm(x)
    return x
</code></pre>
<p>And the transformer blocks using this:</p>
<pre class=""lang-py prettyprint-override""><code>class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.3):
        super(TransformerBlock, self).__init__()
        self.att = CausalSelfAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.1)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation=&quot;relu&quot;),
             layers.Dense(embed_dim), ]
        )
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout2 = layers.Dropout(rate)


    def call(self, inputs):
        attention_output = self.att(inputs)
        ffn_output = self.ffn(attention_output)
        ffn_output = self.dropout2(ffn_output)
        return self.layernorm2(attention_output + ffn_output)
</code></pre>
<p>Previous to this I was calculating the masks myself and sending them when creating the MHA instead of using &quot;use_casual_mask=True&quot;. but the results were the same.</p>
<p>Here you can also see my train_step, where you dan see I swift the window for inputs and predictions:</p>
<pre class=""lang-py prettyprint-override""><code>    def train_step(self, batch):
        &quot;&quot;&quot;Processes one batch inside model.fit().&quot;&quot;&quot;
        source = batch[0]/4.  # [0] are the sequences of latent vectors
        source_input = source[:, :-1]
        source_target = source[:, 1:]
        with tf.GradientTape() as tape:
            preds = self(source_input)
            loss = self.compiled_loss(source_target, preds)*4.
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        self.loss_metric.update_state(loss)

        return {&quot;loss&quot;: self.loss_metric.result()}
</code></pre>
<p>Here you can see my embedding and positional embedding:</p>
<pre class=""lang-py prettyprint-override""><code>class PositionEmbedding(layers.Layer):
    def __init__(self, maxlen, embed_dim):
        super(PositionEmbedding, self).__init__()
        # self.latent_emb = layers.Dense(embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)
        self.latent_emb = keras.Sequential(
            [layers.Conv2D(1, (5,5), padding=&quot;same&quot;, activation=&quot;relu&quot;),
             layers.Reshape([maxlen, 100]),
             layers.Conv1D(embed_dim, 3, padding=&quot;same&quot;), ]
        )

    def call(self, x):

        maxlen = tf.shape(x)[1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)

        x = self.latent_emb(tf.expand_dims(x,-1))
        # x = self.latent_emb(x)
        return x + positions
</code></pre>
<p>As you can see I have tried two things (if you check the comments). First I tried using a Dense layer as embedding, and that's it. Then I tried using Conv2d and Conv1d (as the one uncommented now). The results were the same - using Convs it was a bit better, but not by much. The difference in predictions between [0,-1] and [-1] remains.</p>
<p>I am confused why there's such a big difference, because using Casual Masks, it shouldn't be like this. I have also check to see if the problem is with the padding doing the Conv, but the first prediction (row) seems to be unaffected. First and last columns also seem to be OK. The only problem is the last row (the new prediction). I tried different padding techniques, but the problem remained. If the Conv padding was the problem you would expect it all around the data, not just in the last row.</p>
<p>Overall I am confused, because it all seems to indicate it is ignoring the casual mask, but to the best of my knowledge it is using it.</p>
","transformer-model"
"78691646","Custom Transformer-Based Chatbot Model Not Generating Valid Responses","2024-07-01 10:22:29","","0","49","<python><artificial-intelligence><chatbot><transformer-model>","<p>I'm currently working on building a custom chatbot using a Transformer-based model for a personal project. Despite trying different hyperparameters, adjusting epochs, and increasing my dataset size, I'm encountering issues where the model fails to generate any valid responses based on my datasets.</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
import pandas as pd
import math
from transformers import BertTokenizer

# Define the tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Load the conversational data from a CSV file
def load_conversational_data(file_path):
    data = pd.read_csv(file_path)
    print(&quot;Loaded data:&quot;)
    print(data.head())
    return data['input'].tolist(), data['response'].tolist()

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=50):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0
        self.d_head = d_model // num_heads
        self.num_heads = num_heads
        self.linear_q = nn.Linear(d_model, d_model)
        self.linear_k = nn.Linear(d_model, d_model)
        self.linear_v = nn.Linear(d_model, d_model)
        self.linear_out = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        seq_length = query.size(1)
        q = self.linear_q(query)
        k = self.linear_k(key)
        v = self.linear_v(value)
        q = q.view(batch_size, seq_length, self.num_heads, self.d_head).transpose(1, 2)
        k = k.view(batch_size, seq_length, self.num_heads, self.d_head).transpose(1, 2)
        v = v.view(batch_size, seq_length, self.num_heads, self.d_head).transpose(1, 2)
        scores = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.d_head)
        
        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(1)
            scores = scores.masked_fill(mask == 0, float('-inf'))
        
        attn_weights = torch.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        context = torch.matmul(attn_weights, v)
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_length, self.num_heads * self.d_head)
        output = self.linear_out(context)
        return output

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(FeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        x = nn.functional.relu(self.linear1(x))
        x = self.dropout(x)
        x = self.linear2(x)
        return x

class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerEncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        src2 = self.norm1(src)
        src2 = self.self_attn(src2, src2, src2, src_mask)
        src = src + self.dropout1(src2)
        
        src2 = self.norm2(src)
        src2 = self.ffn(src2)
        src = src + self.dropout2(src2)
        return src

class TransformerEncoder(nn.Module):
    def __init__(self, num_layers, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerEncoder, self).__init__()
        self.layers = nn.ModuleList(
            [TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]
        )
        
    def forward(self, src, src_mask=None):
        output = src
        for layer in self.layers:
            output = layer(output, src_mask)
        return output

class CustomTransformerModel(nn.Module):
    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=6, d_ff=256, dropout=0.1):
        super(CustomTransformerModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        self.transformer_encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff, dropout)
        self.fc = nn.Linear(d_model, vocab_size)
        self.init_weights()

    def init_weights(self):
        for p in self.parameters():
            if p.dim() &gt; 1:
                nn.init.xavier_uniform_(p)

    def forward(self, src, src_mask=None):
        x = self.embedding(src)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x, src_mask)
        x = self.fc(x)
        return x

# Custom Dataset class
class ConversationDataset(Dataset):
    def __init__(self, inputs, responses, tokenizer, max_length=50):
        self.inputs = inputs
        self.responses = responses
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        input_text = self.inputs[idx]
        response_text = self.responses[idx]
        input_tokens = self.tokenizer.encode(input_text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding='max_length')
        response_tokens = self.tokenizer.encode(response_text, add_special_tokens=True, max_length=self.max_length, truncation=True, padding='max_length')
        return torch.tensor(input_tokens), torch.tensor(response_tokens)

# Load and preprocess your real-world dataset here
inputs, responses = load_conversational_data('conversations.csv')
dataset = ConversationDataset(inputs, responses, tokenizer)
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# Data loaders
batch_size = 14
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Instantiate the model
vocab_size = tokenizer.vocab_size
model = CustomTransformerModel(vocab_size)

# Loss and optimizer
loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
optimizer = optim.AdamW(model.parameters(), lr=0.001)

# Function to generate responses
def generate_response(model, tokenizer, input_text, max_length=50):
    model.eval()
    try:
        # Tokenize the input text
        tokens = tokenizer.encode(input_text, add_special_tokens=True, max_length=max_length, truncation=True, padding='max_length')
        input_data = torch.tensor([tokens])
        input_mask = (input_data != tokenizer.pad_token_id).long()
        
        # Log the tokenized input
        print(f&quot;Tokenized input: {tokens}&quot;)
        
        with torch.no_grad():
            # Forward pass through the model
            output = model(input_data)
            
            # Check for NaNs in the output
            if torch.sum(torch.isnan(output)) &gt; 0:
                print(&quot;Error: Model output contains NaNs.&quot;)
                return &quot;I'm sorry, there was an error in generating the response.&quot;
            
            # Get the predicted tokens
            output_tokens = torch.argmax(output, dim=-1).squeeze().tolist()
            print(f&quot;Output tokens: {output_tokens}&quot;)
            
            # Decode the tokens into a string
            response = tokenizer.decode(output_tokens, skip_special_tokens=True)
            print(f&quot;Generated response: {response}&quot;)
            
            if response.strip() == &quot;&quot;:
                print(&quot;Error: Generated response is empty.&quot;)
                return &quot;I'm sorry, I couldn't generate a valid response.&quot;
            
            return response
        
    except Exception as e:
        print(f&quot;Exception during response generation: {str(e)}&quot;)
        return &quot;I'm sorry, an unexpected error occurred.&quot;


# Training and validation loop
num_epochs = 10
for epoch in range(num_epochs):
    # Training
    model.train()
    train_loss = 0.0
    for input_data, target in train_loader:
        optimizer.zero_grad()
        input_mask = (input_data != tokenizer.pad_token_id).long()
        
        output = model(input_data, input_mask)
        loss = loss_fn(output.view(-1, vocab_size), target.view(-1))
        loss.backward()
        
        optimizer.step()
        
        train_loss += loss.item()
    
    avg_train_loss = train_loss / len(train_loader)
    
    # Validation
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for input_data, target in val_loader:
            input_mask = (input_data != tokenizer.pad_token_id).long()
            
            output = model(input_data, input_mask)
            loss = loss_fn(output.view(-1, vocab_size), target.view(-1))
            val_loss += loss.item()
    
    avg_val_loss = val_loss / len(val_loader)
    
    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')
    
    # Log a sample response from the model
    sample_input = &quot;Hello, how are you?&quot;
    sample_response = generate_response(model, tokenizer, sample_input)
    print(f&quot;Sample input: {sample_input}\nSample response: {sample_response}&quot;)

# Test the model interactively
while True:
    test_input = input(&quot;you: &quot;)
    if test_input.lower() == 'exit':
        break
    response = generate_response(model, tokenizer, test_input)
    print(f&quot;Input: {test_input}\nResponse: {response}&quot;)


</code></pre>
<p>I've experimented with various hyperparameters such as learning rates, batch sizes, and different numbers of training epochs. My expectation was that these adjustments would help improve the quality of responses generated by my Transformer-based model. However, despite increasing the dataset size and refining these parameters, the model continues to produce either repeated tokens or completely empty responses during training and inference.</p>
<p>here is output:</p>
<pre><code>python ai.py
Loaded data:
                                   input                                      response
0                    Hello, how are you?                          I'm good, thank you!
1              What are you up to today?         Just relaxing at home. How about you?
2  Have you seen any good movies lately?   Yes, I watched a great thriller last night.
3      Can you recommend any good books?  Sure! Have you read 'To Kill a Mockingbird'?
4            Tell me about your hobbies.                  I enjoy painting and hiking.
Epoch [1/10], Train Loss: 9.8999, Val Loss: 8.5966
Tokenized input: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output tokens: [1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045]
Generated response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Sample input: Hello, how are you?
Sample response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Epoch [2/10], Train Loss: 6.4885, Val Loss: 8.5841
Tokenized input: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output tokens: [1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045]
Generated response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Sample input: Hello, how are you?
Sample response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Epoch [3/10], Train Loss: 5.1883, Val Loss: 10.1303
Tokenized input: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output tokens: [1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012]
Generated response: ..................................................
Sample input: Hello, how are you?
Sample response: ..................................................
Epoch [4/10], Train Loss: 4.8297, Val Loss: 10.9283
Tokenized input: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output tokens: [1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045]
Generated response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Sample input: Hello, how are you?
Sample response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Epoch [5/10], Train Loss: 4.8646, Val Loss: 11.6196
Tokenized input: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output tokens: [1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045]
Generated response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Sample input: Hello, how are you?
Sample response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Epoch [6/10], Train Loss: 4.8166, Val Loss: 11.6514
Tokenized input: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output tokens: [1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012]
Generated response: ..................................................
Sample input: Hello, how are you?
Sample response: ..................................................
Epoch [7/10], Train Loss: 4.7412, Val Loss: 11.3170
Tokenized input: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output tokens: [1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012, 1012]
Generated response: ..................................................
Sample input: Hello, how are you?
Sample response: ..................................................
Epoch [8/10], Train Loss: 4.7109, Val Loss: 10.7475
Tokenized input: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output tokens: [102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102]
Generated response: 
Error: Generated response is empty.
Sample input: Hello, how are you?
Sample response: I'm sorry, I couldn't generate a valid response.
Epoch [9/10], Train Loss: 4.6831, Val Loss: 10.1369
Tokenized input: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output tokens: [1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045]
Generated response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Sample input: Hello, how are you?
Sample response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Epoch [10/10], Train Loss: 4.6578, Val Loss: 9.8423
Tokenized input: [101, 7592, 1010, 2129, 2024, 2017, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Output tokens: [1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045, 1045]
Generated response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
Sample input: Hello, how are you?
Sample response: i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i i
you: 
</code></pre>
","transformer-model"
"78683742","Regression Head for bi-GRU + Transformer Encoder for time series regre","2024-06-28 17:21:42","","0","21","<deep-learning><pytorch><recurrent-neural-network><transformer-model><encoder>","<p>I'm working on a deep learning model that involves a bidirectional GRU (bi-GRU) followed by an Encoder Transformer. My input time series has the shape (batch_size, seq_len, num_features), where num_features is 3. The bi-GRU processes this input, and its output is fed into the Encoder Transformer.</p>
<p>The challenge I'm facing is how to design a suitable regression head after the Encoder Transformer. This regression head should take the transformer's output and generate a 2D vector of 4 values as predictions.</p>
<p>Specifically, I'm looking for guidance on the following:</p>
<p>Architecture: What would be an effective architecture for this regression head?
Implementation: How can I implement this regression head in a way that seamlessly integrates with my existing bi-GRU and Encoder Transformer components?</p>
<p>I have implemented the bi-GRU and Encoder Transformer components of my model. The bi-GRU successfully processes the input time series, and the Encoder Transformer further refines the representations.</p>
<p>I've tried two different approaches for the regression head:</p>
<p>Linear Layer:  I added a simple linear layer with four output units to the output of the Encoder Transformer. The expectation was that this layer would learn to map the transformer's high-level features to the four target values.</p>
<p>Multi-Layer Perceptron (MLP):  I replaced the linear layer with a small MLP (e.g., two hidden layers with ReLU activation) to increase the complexity of the regression head and potentially improve its capacity to model non-linear relationships.</p>
<p>However, I'm unsure whether these approaches are appropriate or if there are better alternatives. I'm hoping to get feedback on the suitability of these regression heads and any potential improvements.</p>
<p>Those two implementations are my regression heads:</p>
<pre><code>self.linear_relu_stack = nn.Sequential(
                nn.Linear(self.bidirectional * self.hidden_size, 256),
                nn.ReLU(),
                nn.Linear(256, 64),
                nn.ReLU(),
                nn.Linear(64, self.num_features)
            )
</code></pre>
<p>The second implementation is the following (which I don't really understand):</p>
<pre><code>self.head_nf = self.seq_len * d_model
        if custom_head is not None: 
            if isinstance(custom_head, nn.Module): self.head = custom_head
            else: self.head = custom_head(d_model, self.num_features, self.seq_len)
        else:
            self.head = self.create_head(self.head_nf, self.num_features, act=act, fc_dropout=fc_dropout, y_range=y_range)

def create_head(self, nf, num_features, act=&quot;gelu&quot;, fc_dropout=0., y_range=None):
        layers = [get_activation_fn(act), nn.Flatten()]
        if fc_dropout: layers += [nn.Dropout(fc_dropout)]
        layers += [nn.Linear(nf, num_features)]
        if y_range: layers += [nn.SigmoidRange(*y_range)]
        return nn.Sequential(*layers)
</code></pre>
","transformer-model"
"78665183","Audio Spectrogram Transformer (AST) handles different lengths of audio data","2024-06-25 02:38:15","","1","32","<deep-learning><speech-recognition><transformer-model>","<p>In the context of the Audio Spectrogram Transformer (AST), how should variable-length data be handled? Specifically, within a dataset, each batch of data is padded to have the same length, but different batches have varying lengths. During actual training, is it necessary to reinitialize the model based on the length of the data within each batch?</p>
<pre><code>if __name__ == '__main__':
    input_tdim = 100
    ast_mdl = ASTModel(input_tdim=input_tdim)
    # input a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins
    test_input = torch.rand([10, input_tdim, 128])
    test_output = ast_mdl(test_input)
    # output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes.
    print(test_output.shape)

    input_tdim = 256
    ast_mdl = ASTModel(input_tdim=input_tdim,label_dim=50, audioset_pretrain=True)
    # input a batch of 10 spectrogram, each with 512 time frames and 128 frequency bins
    test_input = torch.rand([10, input_tdim, 128])
    test_output = ast_mdl(test_input)
    # output should be in shape [10, 50], i.e., 10 samples, each with prediction of 50 classes.
    print(test_output.shape)
</code></pre>
<p>Given the code in the AST project on GitHub, it appears that different lengths of data are handled by starting the model twice for each length. This raises the question of whether the AST model needs to be reinitialized for each batch with different lengths during training.</p>
","transformer-model"
"78663466","Optimizing Video Processing with OpenCV and DETR","2024-06-24 15:46:23","","0","21","<machine-learning><deep-learning><pytorch><neural-network><transformer-model>","<p>I'm working on a Python script using OpenCV to process a video file, annotate detected objects using a pretrained DETR model, and save the annotated video. The problem is the processing part which takes 5 minutes even for a 4 second video. This is the solution I came with:</p>
<pre class=""lang-py prettyprint-override""><code>DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
CHECKPOINT = 'facebook/detr-resnet-50'
CONFIDENCE_THRESHOLD = 0.5
BATCH_SIZE = 4
image_processor = DetrImageProcessor.from_pretrained(CHECKPOINT)

id2label = {
    0: 'potholes',
    1: 'pothole'
}

model = DetrForObjectDetection.from_pretrained(&quot;Models/potholes-model&quot;)
model.to(DEVICE)
model.eval()
box_annotator = sv.BoxAnnotator()

def process_video(video_path, output_path):
    cap = cv2.VideoCapture(video_path)

    if not cap.isOpened():
        print(&quot;Error: Could not open video.&quot;)
        return

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = int(cap.get(cv2.CAP_PROP_FPS))

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(os.path.join(output_path, &quot;results.mp4&quot;), fourcc, fps, (width, height))

    if not out.isOpened():
        print(&quot;Error: Could not open VideoWriter.&quot;)
        return

    frames = []
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        frames.append(frame)
        if len(frames) == BATCH_SIZE:
            process_and_write_batch(frames, out)
            frames = []

    if frames:
        process_and_write_batch(frames, out)

    cap.release()
    out.release()
    cv2.destroyAllWindows()
</code></pre>
<p>I tried to process a batch of frames instead of one at the time:</p>
<pre class=""lang-py prettyprint-override""><code>def process_and_write_batch(frames, out):
    with torch.no_grad():
        inputs = image_processor(images=frames, return_tensors='pt').to(DEVICE)
        outputs = model(**inputs)

        target_sizes = torch.tensor([frame.shape[:2] for frame in frames]).to(DEVICE)
        results = image_processor.post_process_object_detection(
            outputs=outputs,
            threshold=CONFIDENCE_THRESHOLD,
            target_sizes=target_sizes
        )

    for frame, result in zip(frames, results):
        try:
            detections = sv.Detections.from_transformers(transformers_results=result).with_nms(threshold=0.5)
            labels = [f&quot;{id2label[class_id]} {confidence:.2f}&quot; for _, confidence, class_id, _ in detections]
            annotated_frame = box_annotator.annotate(scene=frame.copy(), detections=detections, labels=labels)
            out.write(annotated_frame)
        except:
            out.write(frame)
</code></pre>
<p>The process still takes a very long time, even for short videos. What can I do to optimize the process?</p>
","transformer-model"
"78658266","Custom Transformer model issue","2024-06-23 09:15:26","","0","58","<python><pytorch><transformer-model>","<pre><code>import math
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau
from tqdm import tqdm
import sentencepiece as spm

# Define PositionalEncoding, MultiHeadAttention, PositionwiseFeedforward,
# TransformerEncoderLayer, TransformerDecoderLayer, and Transformer classes
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % num_heads == 0
        self.d_model = d_model
        self.num_heads = num_heads
        self.depth = d_model // num_heads

        self.query = nn.Linear(d_model, d_model)
        self.key = nn.Linear(d_model, d_model)
        self.value = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, mask=None):
        batch_size = query.size(0)
        seq_len = query.size(1)

        query = self.query(query).view(batch_size, seq_len, self.num_heads, self.depth).transpose(1, 2)
        key = self.key(key).view(batch_size, seq_len, self.num_heads, self.depth).transpose(1, 2)
        value = self.value(value).view(batch_size, seq_len, self.num_heads, self.depth).transpose(1, 2)

        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.depth)

        if mask is not None:
            mask = mask.unsqueeze(1).unsqueeze(1)  # Ensure mask is broadcastable
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attention_weights = F.softmax(scores, dim=-1)
        attention_weights = self.dropout(attention_weights)
        context = torch.matmul(attention_weights, value)
        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)
        return context, attention_weights


class PositionwiseFeedforward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedforward, self).__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.dropout(F.relu(self.fc1(x)))
        x = self.fc2(x)
        return x

class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerEncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = PositionwiseFeedforward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        src2, _ = self.self_attn(src, src, src, src_mask)
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.ffn(src)
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src

class TransformerDecoderLayer(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super(TransformerDecoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.src_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ffn = PositionwiseFeedforward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):
        tgt2, self_attention_weights = self.self_attn(tgt, tgt, tgt, tgt_mask)
        tgt = tgt + self.dropout1(tgt2)
        tgt = self.norm1(tgt)
        tgt2, src_attention_weights = self.src_attn(tgt, memory, memory, memory_mask)
        tgt = tgt + self.dropout2(tgt2)
        tgt = self.norm2(tgt)
        tgt2 = self.ffn(tgt)
        tgt = tgt + self.dropout3(tgt2)
        tgt = self.norm3(tgt)
        return tgt, self_attention_weights, src_attention_weights

class Transformer(nn.Module):
    def __init__(self, num_encoder_layers, num_decoder_layers, vocab_size, d_model, num_heads, d_ff, dropout=0.1):
        super(Transformer, self).__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_encoder_layers)])
        self.decoder_layers = nn.ModuleList([TransformerDecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_decoder_layers)])
        self.fc_out = nn.Linear(d_model, vocab_size)
        self.d_model = d_model

    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        src = self.embedding(src) * math.sqrt(self.d_model)
        src = self.pos_encoder(src)
        tgt = self.embedding(tgt) * math.sqrt(self.d_model)
        tgt = self.pos_encoder(tgt)

        for layer in self.encoder_layers:
            src = layer(src, src_mask)
        
        memory = src

        for layer in self.decoder_layers:
            tgt, _, _ = layer(tgt, memory, tgt_mask, src_mask)

        output = self.fc_out(tgt)
        return output

class ConversationDataset(Dataset):
    def __init__(self, conversations, sp_model):
        self.conversations = conversations
        self.sp_model = sp_model

    def __len__(self):
        return len(self.conversations)

    def __getitem__(self, idx):
        input_text, target_text = self.conversations[idx]
        input_tensor = self.sp_model.encode(input_text, out_type=int)
        target_tensor = self.sp_model.encode(target_text, out_type=int)
        return input_tensor, target_tensor

def pad_sequence(seq, max_len, pad_value):
    return seq + [pad_value] * (max_len - len(seq))

def collate_fn(batch, pad_token=0):
    input_seqs, target_seqs = zip(*batch)
    max_input_len = max(len(seq) for seq in input_seqs)
    max_target_len = max(len(seq) for seq in target_seqs)
    input_seqs = [pad_sequence(seq, max_input_len, pad_token) for seq in input_seqs]
    target_seqs = [pad_sequence(seq, max_target_len, pad_token) for seq in target_seqs]
    input_seqs = torch.tensor(input_seqs, dtype=torch.long)
    target_seqs = torch.tensor(target_seqs, dtype=torch.long)
    return input_seqs, target_seqs

def train_model(model, dataloader, num_epochs, learning_rate, vocab_size):
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    criterion = nn.CrossEntropyLoss(ignore_index=0)
    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, verbose=True)

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        pbar = tqdm(dataloader, desc=f&quot;Epoch {epoch + 1}/{num_epochs}&quot;)
        for src, tgt in pbar:
            optimizer.zero_grad()

            src = src.transpose(0, 1)
            tgt_input = tgt[:, :-1].transpose(0, 1)
            tgt_output = tgt[:, 1:].transpose(0, 1)

            src_mask = generate_square_subsequent_mask(src.size(0)).to(src.device)
            tgt_mask = generate_square_subsequent_mask(tgt_input.size(0)).to(tgt_input.device)

            src_mask = src_mask.unsqueeze(0)
            tgt_mask = tgt_mask.unsqueeze(0)

            output = model(src, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask)
            output = output.transpose(0, 1).contiguous().view(-1, vocab_size)
            tgt_output = tgt_output.contiguous().view(-1)

            loss = criterion(output, tgt_output)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            pbar.set_postfix({'loss': total_loss / len(pbar)})
        
        scheduler.step(total_loss / len(dataloader))


def generate_square_subsequent_mask(sz):
    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask


def generate_response(model, sp_model, input_text, max_len=20):
    model.eval()

    with torch.no_grad():
        src_tokens = sp_model.encode(input_text, out_type=int)
        src_tensor = torch.tensor([src_tokens], dtype=torch.long)
        src_mask = (src_tensor != 0).unsqueeze(-2)

        memory = model.encoder(src_tensor, src_mask)
        
        # Start decoding with &quot;&lt;s&gt;&quot; token
        tgt_token = [sp_model.bos_id()]

        for i in range(max_len):
            tgt_tensor = torch.tensor([tgt_token], dtype=torch.long)
            tgt_mask = (tgt_tensor != 0).unsqueeze(-2)
            
            output = model.decoder(tgt_tensor, memory, tgt_mask, src_mask)
            output = torch.argmax(output, dim=-1)
            token = output[0, -1].item()
            
            if token == sp_model.eos_id():
                break
            
            tgt_token.append(token)

        output_text = sp_model.decode_ids(tgt_token)
        return output_text

# Load SentencePiece model
sp_model = spm.SentencePieceProcessor(model_file='m.model')

# Example conversation dataset
conversations = [
    (&quot;hello how are you&quot;, &quot;i am fine&quot;),
    (&quot;what is your name&quot;, &quot;my name is bot&quot;),
    (&quot;how old are you&quot;, &quot;i am 2 years old&quot;)
]

# Create dataset and dataloader
dataset = ConversationDataset(conversations, sp_model)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)

# Model parameters
num_encoder_layers = 6
num_decoder_layers = 6
vocab_size = sp_model.get_piece_size()  # Get actual vocab size from SentencePiece model
d_model = 512
num_heads = 8
d_ff = 2048
dropout = 0.1
learning_rate = 0.0001
num_epochs = 10

# Initialize and train the model
model = Transformer(num_encoder_layers, num_decoder_layers, vocab_size, d_model, num_heads, d_ff, dropout)
train_model(model, dataloader, num_epochs, learning_rate, vocab_size)

# Test inference
input_text = &quot;hello how are you&quot;
response = generate_response(model, sp_model, input_text)
print(f&quot;Input: {input_text}\nResponse: {response}&quot;)
</code></pre>
<pre><code>&gt;&gt;&gt; %Run ai.py
/home/shaykhul/.local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn(&quot;The verbose parameter is deprecated. Please use get_last_lr() &quot;
Epoch 1/10:   0%|          | 0/2 [00:00&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;/home/shaykhul/Desktop/ai.py&quot;, line 272, in &lt;module&gt;
    train_model(model, dataloader, num_epochs, learning_rate, vocab_size)
  File &quot;/home/shaykhul/Desktop/ai.py&quot;, line 196, in train_model
    output = model(src, tgt_input, src_mask=src_mask, tgt_mask=tgt_mask)
  File &quot;/home/shaykhul/.local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/home/shaykhul/.local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/home/shaykhul/Desktop/ai.py&quot;, line 137, in forward
    src = layer(src, src_mask)
  File &quot;/home/shaykhul/.local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/home/shaykhul/.local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/home/shaykhul/Desktop/ai.py&quot;, line 87, in forward
    src2, _ = self.self_attn(src, src, src, src_mask)
  File &quot;/home/shaykhul/.local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/home/shaykhul/.local/lib/python3.11/site-packages/torch/nn/modules/module.py&quot;, line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/home/shaykhul/Desktop/ai.py&quot;, line 55, in forward
    scores = scores.masked_fill(mask == 0, float('-inf'))
RuntimeError: The size of tensor a (13) must match the size of tensor b (2) at non-singleton dimension 4
</code></pre>
<p>I can't fix this error. I tried different approaches and even ChatGP, but it was not fixed. The error occurred due to MultiHeadAttention dimension begin mismatched. I want to build a simple personal chat-bot with an AI girlfriend. So please help me to fix the error. I want to make my own custom transformer model, and train it with my own custom data sets.</p>
","transformer-model"
"78653170","InvalidArgumentError: Graph execution error Incompatible shapes: [32,800,64] vs. [32,125,64] in PatchEncoder in ViT","2024-06-21 15:17:08","","0","26","<machine-learning><neural-network><tf.keras><transformer-model><vision-transformer>","<p>I am building a 3D ViT for classification</p>
<pre class=""lang-py prettyprint-override""><code>class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim, **kwargs):
        super(PatchEncoder, self).__init__(**kwargs)
        self.num_patches = num_patches
        self.projection_dim = projection_dim
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = self.add_weight(
            name=&quot;position_embedding&quot;,
            shape=(num_patches, projection_dim),  # Remove the first dimension
            initializer=tf.keras.initializers.RandomNormal(stddev=0.02),
            trainable=True
        )

    def call(self, patches):
        batch_size = tf.shape(patches)[0]
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        positions_embedding = tf.nn.embedding_lookup(self.position_embedding, positions)
        positions_embedding = tf.tile(tf.expand_dims(positions_embedding, axis=0), [batch_size, 1, 1])

        projected_patches = self.projection(patches)
        encoded_patches = projected_patches + positions_embedding

        print(f&quot;PatchEncoder: patches shape={patches.shape}, encoded_patches shape={encoded_patches.shape}&quot;)
        
        return encoded_patches


class Patches3D(layers.Layer):
    def __init__(self, patch_size, **kwargs):
        super().__init__(**kwargs)
        self.patch_size = patch_size

    def call(self, volumes):
        batch_size = tf.shape(volumes)[0]
        depth = volumes.shape[1]
        height = volumes.shape[2]
        width = volumes.shape[3]
        channels = volumes.shape[4]

        patches = tf.image.extract_patches(
            images=tf.reshape(volumes, [-1, height, width, channels]),
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding='VALID'
        )

        patch_depth = depth // self.patch_size
        patch_height = height // self.patch_size
        patch_width = width // self.patch_size

        patches = tf.reshape(patches, [batch_size, patch_depth, patch_height, patch_width, -1])
        patches = tf.transpose(patches, [0, 1, 2, 3, 4])
        patches = tf.reshape(patches, [batch_size, -1, self.patch_size * self.patch_size * channels])

        print(f&quot;Patches3D: volumes shape={volumes.shape}, patches shape={patches.shape}&quot;)

        return patches

# Assuming x_train has the shape (1265, 32, 32, 32, 1)
x_train_expanded = tf.expand_dims(x_train, axis=-1)

# Now create the ViT classifier
def create_vit_classifier(input_shape=(32, 32, 32, 1), num_classes=1):
    patch_size = 6
    num_patches = (input_shape[0] // patch_size) * (input_shape[1] // patch_size) * (input_shape[2] // patch_size)
    projection_dim = 64
    transformer_layers = 8

    inputs = keras.Input(shape=input_shape)
    # Augment data.
    augmented = data_augmentation(inputs)
    # Create patches.
    patches = Patches3D(patch_size)(augmented)
    # Encode patches.
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

    print(f&quot;create_vit_classifier: input shape={input_shape}, patches shape={patches.shape}, encoded_patches shape={encoded_patches.shape}&quot;)

    for _ in range(transformer_layers):
        encoded_patches = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
        attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=projection_dim, dropout=0.1)(encoded_patches, encoded_patches)
        attention_output = layers.Dropout(0.1)(attention_output)
        encoded_patches = layers.Add()([encoded_patches, attention_output])

        feedforward_output = layers.Dense(projection_dim, activation=tf.nn.gelu)(encoded_patches)
        feedforward_output = layers.Dropout(0.1)(feedforward_output)
        encoded_patches = layers.Add()([encoded_patches, feedforward_output])

    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = layers.Flatten()(representation)
    representation = layers.Dropout(0.1)(representation)
    outputs = layers.Dense(units=num_classes, activation='sigmoid')(representation)
    
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

vit_classifier = create_vit_classifier(input_shape=(32, 32, 32, 1))
vit_classifier.summary()
from tensorflow.keras.optimizers.legacy import Adam
vit_classifier.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=tf.keras.losses.BinaryCrossentropy(),  # Adjust loss function as needed
    metrics=[tf.keras.metrics.BinaryAccuracy()]  # Adjust metrics as needed
)

# Train the model
history = vit_classifier.fit(
    x_train, y_train,
    batch_size=32,
    epochs=10,
    validation_split=0.2
)
</code></pre>
<p>And I get following error:</p>
<pre><code>InvalidArgumentError                      Traceback (most recent call last)
Cell In[143], line 108
    101 vit_classifier.compile(
    102     optimizer=tf.keras.optimizers.Adam(),
    103     loss=tf.keras.losses.BinaryCrossentropy(),  # Adjust loss function as needed
    104     metrics=[tf.keras.metrics.BinaryAccuracy()]  # Adjust metrics as needed
    105 )
    107 # Train the model
--&gt; 108 history = vit_classifier.fit(
    109     x_train, y_train,
    110     batch_size=32,
    111     epochs=10,
    112     validation_split=0.2
    113 )

File ~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--&gt; 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     51 try:
     52   ctx.ensure_initialized()
---&gt; 53   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     54                                       inputs, attrs, num_outputs)
     55 except core._NotOkStatusException as e:
     56   if name is not None:

InvalidArgumentError: Graph execution error:

Detected at node gradient_tape/functional_107_1/patch_encoder_61_1/add/BroadcastGradientArgs defined at (most recent call last):
  File &quot;&lt;frozen runpy&gt;&quot;, line 198, in _run_module_as_main


in train_step

Incompatible shapes: [32,800,64] vs. [32,125,64]
     [[{{node gradient_tape/functional_107_1/patch_encoder_61_1/add/BroadcastGradientArgs}}]] [Op:__inference_one_step_on_iterator_834044]
</code></pre>
<p>I understand that there is some issue when applying the gradient and I have a mismatch, which I think comes from the PatchEncoder. But I cant pinpoint the issue:</p>
<p>What causes this error?</p>
<p>Thank you for reading, Luca</p>
<p>I tried fixing the Encoder but somehow that didn't help. It even goes through the first epoch, which is weird in my eyes. I also thought that it could be in pulling out the patches but it doesn't seem to be an issue.</p>
","transformer-model"
"78648954","Robust Application for Analyzing Plots with Primary y-axis Bars and Secondary y-axis Scatter","2024-06-20 17:46:33","","0","14","<huggingface-transformers><large-language-model><transformer-model><huggingface>","<p>I am looking for a robust application to analyze plots that typically have a primary y-axis with bars (or stacked bars) and a secondary y-axis with scatter points.</p>
<p>Here are some examples of common plots. The application should be robust enough to capture data from a high variability of similar plots automatically and accurately.</p>
<p><a href=""https://i.sstatic.net/cWyn5osg.png"" rel=""nofollow noreferrer"">plot_1</a></p>
<p><a href=""https://i.sstatic.net/pB1Y0awf.png"" rel=""nofollow noreferrer"">plot_2</a></p>
<p>Does anyone know of an application or method that performs well for this task?</p>
<p>I tried using <a href=""https://huggingface.co/google/deplot"" rel=""nofollow noreferrer"">Google Deplot</a>, a Visual Question Answering model, but the performance was very poor. Additionally, I had difficulty converting the output to a pandas DataFrame efficiently.</p>
","transformer-model"
"78640689","Unable to run code on Multiple GPUs in PyTorch - Usage shows only 1 GPU is being utilized","2024-06-19 05:45:12","","0","57","<pytorch><distributed-computing><transformer-model><multi-gpu>","<p>I am training a Transformer Encoder-Decoder based model for Text summarization. The code works without any errors but uses only 1 GPU when checked with nvidia-smi. However, I want to run it on all the available GPUs (I can access as many as I want). I wrapped my model in Dataparallel.
<strong>Here’s how I have wrapped the model:</strong></p>
<pre><code>if torch.cuda.device_count() &gt; 1:
        print(f&quot;Using {torch.cuda.device_count()} GPUs&quot;)
        model = nn.DataParallel(model)

    model.to(device)
</code></pre>
<p><strong>This is how I am calling the functions from my model:</strong></p>
<pre><code>if torch.cuda.device_count() &gt; 1:
                encoder_output = model.module.encode(
                    encoder_input, encoder_mask
                )  # (B, input_len, d_model)
                decoder_output = model.module.decode(
                    encoder_output, encoder_mask, decoder_input, decoder_mask
                )  # (B, seq_len, d_model)
                proj_output = model.module.project(
                    decoder_output
                )
</code></pre>
<p><strong>I am using Python 3.12 and Torch 2.3.0.</strong>
MWE is available on <a href=""https://github.com/abidmeeraj/MWE-problem-with-MultipleGPUs"" rel=""nofollow noreferrer"">GitHub</a>.</p>
<p>I have also checked if GPUs are configured correctly. Tried this example from <a href=""https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html"" rel=""nofollow noreferrer"">PyTorch Documentation</a> to test my GPUs configuration and working.</p>
","transformer-model"
"78639577","Understanding the results of Transformers Learn In Context with Gradient Descent","2024-06-18 20:43:45","78645188","-1","54","<machine-learning><nlp><large-language-model><transformer-model><meta-learning>","<p>I'm trying to implement this paper:
<a href=""https://arxiv.org/pdf/2212.07677"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2212.07677</a></p>
<p>(Here's their code):
<a href=""https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd"" rel=""nofollow noreferrer"">https://github.com/google-research/self-organising-systems/tree/master/transformers_learn_icl_by_gd</a></p>
<p>I'm struggling to match their experimental results. Specifically, on their simplest GD model (a single layer with a single head and no softmax), they obtain a constant low loss of roughly 0.20 on their test data. I don't quite understand why this is the case, conceptually.</p>
<p>As I understand it, this model only does a single iteration of gradient descent on the data, so why would it reach such a low loss? And why would the loss be constant/near constant over training steps? Aren't we training the learning rate in the GD model?</p>
","transformer-model"
"78631255","How to extract image hidden states in LLaVa's transformers (Huggingface) implementation?","2024-06-17 07:04:49","78664084","1","129","<huggingface-transformers><transformer-model><multimodal>","<p>I am using the transformers library (Huggingface) to extract all hidden units of LLaVa 1.5. On the <a href=""https://huggingface.co/docs/transformers/v4.40.2/en/model_doc/llava#transformers.LlavaForConditionalGeneration"" rel=""nofollow noreferrer"">huggingface documentation</a>, it shows that it is possible to extract image hidden states from the vision component.</p>
<p>Unfortunately, the <code>outputs</code> object has only these following keys available in the output dictionary:
<code>odict_keys(['sequences', 'attentions', 'hidden_states', 'past_key_values'])</code></p>
<p>How do I also extract the <code>image_hidden_states</code> from this LLaVa implementation alongwith the exisiting outputs?</p>
<p>I have implemented the follow code in the hopes to do so.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig, AutoProcessor, LlavaProcessor
from PIL import Image
import requests
from torchinfo import summary

device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;
model_id = 'llava-hf/llava-1.5-7b-hf'

# Initializing a CLIP-vision config
vision_config = CLIPVisionConfig(output_hidden_states=True, output_attentions=True, return_dict=True)

# Initializing a Llama config
text_config = LlamaConfig(output_hidden_states=True, output_attentions=True, return_dict=True)

# Initializing a Llava llava-1.5-7b style configuration
configuration = LlavaConfig(vision_config, text_config, output_hidden_states=True, output_attentions=True, return_dict=True)
cfg=LlavaConfig(vision_config, text_config, output_hidden_states=True, output_attentions=True, return_dict=True)

# Initializing a model from the llava-1.5-7b style configuration
model = LlavaForConditionalGeneration(configuration).from_pretrained(model_id, output_hidden_states=True, output_attentions=True, return_dict=True)

# Accessing the model configuration
configuration = model.config

model=model.to(device)
print(summary(model))

processor = LlavaProcessor.from_pretrained(&quot;llava-hf/llava-1.5-7b-hf&quot;, output_hidden_states=True, output_attentions=True, return_dict=True)
prompt = &quot;USER: &lt;image&gt;\nIs there sun in the image? ASSISTANT:&quot;
url = &quot;https://www.ilankelman.org/stopsigns/australia.jpg&quot;
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=prompt, images=image, return_tensors=&quot;pt&quot;)
inputs=inputs.to(device)

with torch.no_grad():
    outputs = model.generate(**inputs, 
                             output_hidden_states=True, 
                             return_dict_in_generate=True, 
                             max_new_tokens=1, 
                             min_new_tokens=1,
                            return_dict=True)

print(outputs.keys())

</code></pre>
","transformer-model"
"78628411","Why do these two methods of implementing Positional Encoding in the Transformer model output different results？","2024-06-16 06:18:52","","0","20","<python><deep-learning><transformer-model>","<p>The first heatmap is the result of method <code>_positional_encoding</code>, and the second one is the result of method <code>_positional_encoding_vector</code>:</p>
<p><img src=""https://i.sstatic.net/9QFRjIIK.png"" alt=""figure 1"" /></p>
<p><img src=""https://i.sstatic.net/z19yVOL5.png"" alt=""figure 2"" /></p>
<p>I think they are all implement of Positional Encoding in standard Transformer, why the result is different? Please give me a hand, thank you so much!</p>
<pre><code>class PositionalEncoding:
    def __init__(self, embed_size: int, vector: bool = True):
        self.embed_size = embed_size
        self.vector = vector

    def __call__(self, input_x: torch.tensor) -&gt; torch.Any:
        return self._positional_encoding_vector(input_x, self.embed_size) if self.vector else self._positional_encoding(input_x, self.embed_size)

    def _positional_encoding_vector(self, input_x: torch.tensor, embed_size: int):
        batch_size, seq_len = input_x.shape[0], input_x.shape[1]
        pos_encoding = torch.zeros(batch_size, seq_len, embed_size)
        pos_encoding_element = torch.arange(0, seq_len)[..., None].tile(
            (1, embed_size)
        ) / (
            10000
            ** ((torch.arange(embed_size) // 2) / embed_size)[None, ...].tile(seq_len, 1)
        )
        pos_encoding[..., 0::2] = torch.sin(pos_encoding_element[..., 0::2])
        pos_encoding[..., 1::2] = torch.cos(pos_encoding_element[..., 1::2])

        return pos_encoding

    def _positional_encoding(self, input_x: torch.tensor, embed_size: int):
        batch_size, seq_len = input_x.shape[0], input_x.shape[1]
        pos_encoding = torch.zeros(batch_size, seq_len, embed_size)
        for i in range(embed_size):
            if i % 2 == 0:
                pos_encoding[:, :, i] = torch.sin(
                    torch.arange(0, seq_len) / 10000 ** (i / embed_size)
                )
            else:
                pos_encoding[:, :, i] = torch.cos(
                    torch.arange(0, seq_len) / 10000 ** ((i - 1) / embed_size)
                )
        return pos_encoding
</code></pre>
","transformer-model"
"78624391","Fine-tuned Phi-2 model did not work correctly, when save it as pytorch or Pickle","2024-06-14 18:06:19","","0","39","<python><pytorch><large-language-model><transformer-model><fine-tuning>","<p>I have a problem here I did fine tune Phi-2 model with LoRA, and I saved the model as a safe-tensors , and here is what is inside my folder</p>
<pre><code>phi-2-sxd\adapter_config.json
phi-2-sxd\adapter_model.safetensors
phi-2-sxd\README.md
</code></pre>
<p>the safe-tensors model is working very well: here is how I make predictions</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline, PhiForCausalLM, AutoTokenizer

# Load the fine-tuned model and tokenizer
model = PhiForCausalLM.from_pretrained(&quot;phi-2-sxd/&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/phi-2&quot;, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

# Create the text generation pipeline
pipe = pipeline(task=&quot;text-generation&quot;, model=model, tokenizer=tokenizer, max_length=300)

# Generate text based on the provided prompt
prompt = &quot;I have been experiencing joint pain in my fingers, wrists, and knees. The pain is often achy and throbbing, and it gets worse when I move my joints&quot;
result = pipe(f&quot;[answer]: {prompt}&quot;)
print(result[0]['generated_text'])
</code></pre>
<p>but I want to convert that to pth (Pytorch) or Pickle (pkl) but in the end the output file was so big like 12GB.
and that was not the only problem, I tried many times, to save it in pth ,pkl and h5, but in the end I got these long error</p>
<pre><code>RuntimeError: Error(s) in loading state_dict for PhiForCausalLM: Missing key(s) in state_dict: HERE_IS_LIST_OF_MISSING_LAYERS
</code></pre>
<h2>here are my many attempts to save it:</h2>
<pre class=""lang-py prettyprint-override""><code>
# Save it in Pytorch
from transformers import PhiForCausalLM, AutoTokenizer
import torch

model = PhiForCausalLM.from_pretrained(&quot;phi-2-sxd/&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/phi-2&quot;, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

# Save the model's tensors to a PyTorch file
torch.save(model.state_dict(), &quot;Phi-2-SXD.pth&quot;)


# save the model using H5 format
from transformers import PhiForCausalLM, AutoTokenizer
import h5py

model = PhiForCausalLM.from_pretrained(&quot;phi-2-sxd/&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/phi-2&quot;, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

# Save the model to H5 format
with h5py.File(&quot;Phi-2-SXD.h5&quot;, &quot;w&quot;) as f:
    for name, param in model.state_dict().items():
        f.create_dataset(name, data=param.detach().numpy())

# save the model using H5 format with compression
import h5py
from transformers import PhiForCausalLM,AutoTokenizer

model = PhiForCausalLM.from_pretrained(&quot;phi-2-sxd/&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/phi-2&quot;, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

# Save the model to H5 format with compression
with h5py.File(&quot;Phi-2-SXD.h5&quot;, &quot;w&quot;, libver=&quot;latest&quot;) as f:
    for name, param in model.state_dict().items():
        f.create_dataset(name, data=param.detach().numpy(), compression=&quot;gzip&quot;, compression_opts=9) 


# Save it in Pytorch with some tricks to reduce the size
import torch
from transformers import PhiForCausalLM, AutoTokenizer
from transformers.modeling_utils import LoRAModel
from torch.nn.utils import prune

model = PhiForCausalLM.from_pretrained(&quot;phi-2-sxd/&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/phi-2&quot;, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

# Prune the model
for name, module in model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name=&quot;weight&quot;, amount=0.5)
    elif isinstance(module, LoRAModel):
        prune.l1_unstructured(module.lora_A, name=&quot;weight&quot;, amount=0.5)
        prune.l1_unstructured(module.lora_B, name=&quot;weight&quot;, amount=0.5)

# Save the pruned model
torch.save(model.state_dict(), &quot;Phi-2-SXD-pruned.pth&quot;)




# pytorch with quantize

import torch
from transformers import PhiForCausalLM, AutoTokenizer
from torch.quantization import quantize, quantize_dynamic

model = PhiForCausalLM.from_pretrained(&quot;phi-2-sxd/&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/phi-2&quot;, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;
# Dynamically quantize the model
quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)

# Save the quantized model
torch.save(quantized_model.state_dict(), &quot;Phi-2-SXD-quantized.pth&quot;)

# Both Technik
import torch
from transformers import PhiForCausalLM, AutoTokenizer
from torch.nn.utils import prune
from torch.quantization import quantize, quantize_dynamic

model = PhiForCausalLM.from_pretrained(&quot;phi-2-sxd/&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/phi-2&quot;, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

# Dynamically quantize the model
quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)

# Prune the quantized model
for name, module in quantized_model.named_modules():
    if isinstance(module, torch.nn.Linear):
        prune.l1_unstructured(module, name=&quot;weight&quot;, amount=0.5)

# Save the pruned and quantized model
torch.save(quantized_model.state_dict(), &quot;Phi-2-SXD-quantized-pruned.pth&quot;)
</code></pre>
<p>What should I do to get my Pth, or pkl working just like safe-tensors?</p>
<p>I have even try to disable the strict mode:</p>
<pre class=""lang-py prettyprint-override""><code>model.load_state_dict(torch.load(&quot;Phi-2-SXD.pth&quot;, map_location=device), strict=False) 
</code></pre>
<p>and even Check if the model is wrapped in DataParallel</p>
<pre class=""lang-py prettyprint-override""><code>if isinstance(quantized_model, torch.nn.DataParallel):
    quantized_model = quantized_model.module
</code></pre>
<p>It works, but the model prediction is making no sense at all, just garbage.</p>
<p>Thanks a lot in advance.</p>
<p>I am trying to save my safe-tensors as Pth or pkl files, to be load faster, and easier.</p>
","transformer-model"
"78618353","cannot back propagate on multi head attention tensorflowjs","2024-06-13 14:02:19","","1","22","<tensorflow><tensorflow.js><transformer-model><self-attention><multihead-attention>","<p>I am trying to create a multi-head attention using <code>tensorflowjs</code>. When trying to train the model, an error kept popping up that the gradient shape was inconsistent with the input shape.</p>
<p>reproducable bug.</p>
<pre class=""lang-js prettyprint-override""><code>const EMBEDDING_DIMENSION = 16;
const NUMBER_OF_HEADS = 2;
const BATCH_SIZE = 8;
const SEQUENCES_LENGTH = 4;
const EPOCHS = 10;

tf.setBackend(&quot;cpu&quot;);
tf.enableDebugMode();

function dotProductAttention(query, key, value) {
  return tf.tidy(() =&gt; {
    const matMul = query.matMul(key, false, true);
    const dk = tf.scalar(query.shape[query.shape.length - 1], 'float32');
    const scaled = matMul.div(tf.sqrt(dk));
    const attentionWeights = scaled.softmax(-1);
    return attentionWeights.matMul(value);
  })
}

function muliHeadAttention(input, numberOfHeads, qkvWeights, outputWeights) {
  return tf.tidy(() =&gt; {
    const batchSize = input.shape[0];
    const seuquencesLength = input.shape[1];
    const embeddingDimension = qkvWeights.shape[0];
    const headDimension = embeddingDimension / numberOfHeads;

    // calculate all query, key, and value by doing one big mat multipication.
    const QKV = input.matMul(qkvWeights);

    // reshap the QKV to include number of heads
    const QKVReshaped = QKV
      .reshape([batchSize, seuquencesLength, numberOfHeads, 3 * headDimension])
      .transpose([0, 2, 1, 3]);

    // split the mega QKV into 3 chunk each query, key, and value matrix
    const [query, key, value] = QKVReshaped.split(3, -1);

    // calculate the attention of query, key, and value
    const attention = dotProductAttention(query, key, value);

    // concatenate the attention values
    const concatenated = attention
      .transpose([0, 2, 1, 3])
      .reshape([batchSize, seuquencesLength, embeddingDimension]);

    // matrix multiply with output weights
    const output = concatenated.matMul(outputWeights);

    // finally, return it [batchSize, seuquencesLength, embeddingDimension]
    return output;
  })
}

function initializeQKVWeights(embeddingDimension) {
  return tf.variable(tf.randomNormal([embeddingDimension, 3 * embeddingDimension]));
}

function initializeOutputWeights(embeddingDimension) {
  return tf.variable(tf.randomNormal([embeddingDimension, embeddingDimension]));
}

// query, key, and value weights combined for better performance.
const QKVWeights = initializeQKVWeights(EMBEDDING_DIMENSION);

// final weight of muti head attention 
const outputWeights = initializeOutputWeights(EMBEDDING_DIMENSION);

// the predict function
const f = (x) =&gt; muliHeadAttention(x, NUMBER_OF_HEADS, QKVWeights, outputWeights);

// mean squared error
const loss = (predict, real) =&gt; predict.sub(real).square().mean();

// the optimizer
const optimizer = tf.train.sgd(0.01);

// temporary generated train datas
const xTrain = tf.randomNormal([BATCH_SIZE, SEQUENCES_LENGTH, EMBEDDING_DIMENSION]);
const yTrain = tf.randomNormal([BATCH_SIZE, SEQUENCES_LENGTH, EMBEDDING_DIMENSION]);

// training the model
for (let i = 0; i &lt; EPOCHS; i++) {
  tf.tidy(() =&gt; {
    const lossValue = optimizer.minimize(() =&gt; loss(f(xTrain), yTrain), [QKVWeights, outputWeights]);

    // Logging loss every epoch
    lossValue.data().then(lossValueArray =&gt; {
      console.log(`Epoch ${i + 1}, Loss: ${lossValueArray[0]}`);
    });
  });
}

f(xTrain).array().then(pred =&gt; {
  console.log(&quot;Predictions:&quot;);
  console.log(pred);
  xTrain.dispose();
});

// Disposing tensors
yTrain.dispose();;
</code></pre>
<p>At first, I thought the minimise function used random tensors to backpropagate. therefore, I freed most of the intermediate tensors (which were created during operations) and explicitly set which variable to modify (aka. the weights). However, the error persisted. Next, I believed that the shape of my tensors were wrong and manually compiled the shapes (one by one) with my pencil and enabled debugging mode to verify it. it looked perfectly fine. and to strengthen my correct operations, I only tried forward propagation, which worked marvellously. So, where did this error come from? the error only emerges when trying to train the model or, specifically, plugging the loss function into the minimize function of the optimizer. I don't know why it throws an error. I believe that there is a fundamental flaw in the optimizer function or I am bad at creating multi-head attention. If you know anything about this, please kindly share it.</p>
","transformer-model"
"78613599","Onnxruntime Test Error after Successfully Converting Model to ONNX","2024-06-12 15:23:06","","0","280","<pytorch><export><transformer-model><onnxruntime>","<p>I have a simple PyTorch model that I'm attempting to convert to ONNX format. The <code>forward()</code> function consists of a call to <code>nn.transformer.encoder()</code>. The conversion to ONNX using <code>torch.onnx.export()</code> completes successfully. However, when I test the model using <code>onnxruntime_test</code>, it fails, except for specific input cases.</p>
<p>I suspect the problem is related to dynamic axes and reshaping during runtime, but I'm unsure of the exact cause.</p>
<p>I've provided a minimal example below where I create the model, run it with two different tensors, export it to ONNX, and attempt to replicate the process using ONNX.</p>
<p>Any insights into why this issue might be occurring would be greatly appreciated. Thank you!</p>
<pre><code>#!/usr/bin/env python3

import torch.nn as nn
from torch import Tensor, rand
import torch.onnx
import onnx

onnx_model = 'MicroTest.onnx'

class Test_trans(nn.Module):
    def __init__( self, emb_size=100):
        super(Test_trans, self).__init__()
        self.transformer = nn.Transformer(emb_size, 2, 2, 2, 512, 0.1)

    def forward(self, src: Tensor):
        return self.transformer.encoder(src)

def process_one_torch(session, ten):
    print('Tensor In size:', ten.size(), end='\t')
    memory = session(ten)
    print('Mem size:', memory.size());

def process_one_onnx(session, npa):
    ortvalue = onnxruntime.OrtValue.ortvalue_from_numpy(npa)
    print('In ortvalue.shape:', ortvalue.shape(), end='\t')
    memory = session.run(None, {session.get_inputs()[0].name: ortvalue})
    print('ONNX mem.shape:', memory[0].shape)

mini = Test_trans()

c_tensor_12 = rand((12,1,100))
c_tensor_10 = rand((10,1,100))

print('################################# Torch ###################################')
process_one_torch(mini, c_tensor_12)
process_one_torch(mini, c_tensor_10)

torch.onnx.export(mini, # model being run
    c_tensor_12,        # model input (or a tuple for multiple inputs)
    onnx_model,         # where to save the model (can be a file or file-like object)
    export_params=True, # store the trained parameter weights inside the model file
    opset_version=17,   # the ONNX version to export the model to
    do_constant_folding=False,
    input_names = ['input'],   # the model's input names
    output_names = ['output'], # the model's output names
    dynamic_axes = {'input' : {0: 'max_len'}})

print('################################# ONNX RT #################################')
import onnxruntime

session = onnxruntime.InferenceSession(onnx_model, providers=[&quot;CPUExecutionProvider&quot;])
print('Session inputs:', session.get_inputs()[0])

process_one_onnx(session, c_tensor_12.numpy())
process_one_onnx(session, c_tensor_10.numpy()) #This one crashes
 
</code></pre>
<p>Running the onnxruntime_test with the created onnx model I got:
` onnxruntime_test MicroTest.onnx
2024-06-12 17:11:22.877402346 [E:onnxruntime:, sequential_executor.cc:514 ExecuteKernel] Non-zero status code returned while running Reshape node. Name:'/encoder/layers.0/self_attn/Reshape_4' Status Message: /croot/onnxruntime_1711063034809/work/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:44 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&amp;, onnxruntime::TensorShapeVector&amp;, bool) input_shape_size == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{1,1,100}, requested shape:{12,2,50}</p>
<p>Traceback (most recent call last):
File &quot;/home/if/miniconda3/envs/cpu/bin/onnxruntime_test&quot;, line 11, in 
sys.exit(main())
^^^^^^
File &quot;/home/if/miniconda3/envs/cpu/lib/python3.11/site-packages/onnxruntime/tools/onnxruntime_test.py&quot;, line 159, in main
exit_code, _, _ = run_model(args.model_path, args.num_iters, args.debug, args.profile, args.symbolic_dims)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File &quot;/home/if/miniconda3/envs/cpu/lib/python3.11/site-packages/onnxruntime/tools/onnxruntime_test.py&quot;, line 118, in run_model
outputs = sess.run([], feeds)  # fetch all outputs
^^^^^^^^^^^^^^^^^^^
File &quot;/home/if/miniconda3/envs/cpu/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py&quot;, line 220, in run
return self._sess.run(output_names, input_feed, run_options)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
onnxruntime.capi.onnxruntime_pybind11_state.RuntimeException: [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Non-zero status code returned while running Reshape node. Name:'/encoder/layers.0/self_attn/Reshape_4' Status Message: /croot/onnxruntime_1711063034809/work/onnxruntime/core/providers/cpu/tensor/reshape_helper.h:44 onnxruntime::ReshapeHelper::ReshapeHelper(const onnxruntime::TensorShape&amp;, onnxruntime::TensorShapeVector&amp;, bool) input_shape_size == size was false. The input tensor cannot be reshaped to the requested shape. Input shape:{1,1,100}, requested shape:{12,2,50}
`
All I understand is that inside is reshaped something differently then expected.</p>
","transformer-model"
"78606702","What do these CAN_BUS values represent and why are they used as positional encodings for BEVFormer?","2024-06-11 09:48:19","","0","35","<can-bus><transformer-model><openmmlab>","<p>BEVFormer is a transformer-based architecture that is heavily used as a component of many other architectures. It learns bird-eye-view representations of scenes for self-driving cars. When studying its source code to understand how it works I came across these lines:</p>
<pre><code>can_bus = bev_queries.new_tensor(
    [each['can_bus'] for each in kwargs['img_metas']])  # [:, :]
can_bus = self.can_bus_mlp(can_bus)[None, :, :]
bev_queries = bev_queries + can_bus * self.use_can_bus
</code></pre>
<p>source: <a href=""https://github.com/fundamentalvision/BEVFormer/blob/66b65f3a1f58caf0507cb2a971b9c0e7f842376c/projects/mmdet3d_plugin/bevformer/modules/transformer.py#L159"" rel=""nofollow noreferrer"">https://github.com/fundamentalvision/BEVFormer/blob/66b65f3a1f58caf0507cb2a971b9c0e7f842376c/projects/mmdet3d_plugin/bevformer/modules/transformer.py#L159</a></p>
<p>The can_bus contains 18 values, as shown by the input to can_bus_mlp. However, the dataset preparation code seems to suggest that can_bus only contains 8 values. These 8 values consist of ego2global rotations, translations, and patch angles.</p>
<p>source : <a href=""https://github.com/fundamentalvision/BEVFormer/blob/66b65f3a1f58caf0507cb2a971b9c0e7f842376c/projects/mmdet3d_plugin/datasets/nuscenes_dataset.py#L158"" rel=""nofollow noreferrer"">https://github.com/fundamentalvision/BEVFormer/blob/66b65f3a1f58caf0507cb2a971b9c0e7f842376c/projects/mmdet3d_plugin/datasets/nuscenes_dataset.py#L158</a></p>
<p>I have the feeling that the can_bus values are used as positional encodings to give the model information about how the pose of the ego vehicle relates to the global scene (similar to how in NLP positional encodings are used to give the model information about the relative positions of words in sentences). However it is a bit hard for me to wrap my head around how this idea came into practice. Or if my intuition is even correct in the first place.</p>
<p>My questions are the following:</p>
<ol>
<li>What are the 18 values in the can_bus?</li>
<li>Where do they come from?</li>
<li>Why are these values used as positional encodings?</li>
</ol>
","transformer-model"
"78604979","how to train equivariant transformer on atomic coordinate data","2024-06-11 00:47:20","","0","25","<pytorch><transformer-model>","<p>i'm new to using transformers. i'm trying to use the equiformer model <a href=""https://github.com/lucidrains/equiformer-pytorch"" rel=""nofollow noreferrer"">docs here</a> to train a score function on atomic coordinate data. the atomic coordinate data is an Nx3 array, where N is the number of atoms and 3 because of 3 spatial dimensions. likewise, i also have an Nx118 array of one-hot encoded labels for each atom type (118 because of 118 atoms in periodic table).</p>
<p>however, i am confused on how to train this model. i would normally provide some code for this but ive tried so many things that it seems pretty frivolous to include anything.</p>
<p>its a very simple short script that im looking for, and would like to ask anyone of you who might be experienced with this type of model.</p>
<p>just assume the loss function is an empty return statement.</p>
<p>thanks</p>
<p>i tried inputting my Nx118 atom labels into the &quot;feats&quot; parameter but it gave me a ton of random errors, notably that my dim must be equal to 2.</p>
","transformer-model"
"78603401","How to handle None values for condition_input in a CrossAttention block in PyTorch?","2024-06-10 16:08:38","","0","39","<python><deep-learning><pytorch><transformer-model><self-attention>","<p>I am working on implementing a cross-attention block in a transformer using PyTorch. My code works fine when <code>condition_input</code> is provided, but it raises an error when <code>condition_input</code> is <code>None</code>. What are the best techniques to handle this situation? For example, <code>x</code> can be image data, and <code>condition_input</code> can be text. If possible, please provide reference.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class CrossAttention(nn.Module):
    def __init__(self, embed_dim=512, block_size=16, n_head=8, drop_out_rate=0.1):
        super().__init__()
        assert embed_dim % 8 == 0
        # key, query, value projections for all heads
        self.key = nn.Linear(embed_dim, embed_dim)
        self.query = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)

        self.attn_drop = nn.Dropout(drop_out_rate)
        self.resid_drop = nn.Dropout(drop_out_rate)

        self.proj = nn.Linear(embed_dim, embed_dim)
        # causal mask to ensure that attention is only applied to the left in the input sequence
        self.register_buffer(&quot;mask&quot;, torch.tril(torch.ones(block_size, 77)).view(1, 1, block_size, 77))
        self.n_head = n_head

    def forward(self, x, condition_input=None):
        B, T, C = x.size()
                
        B, N, D = condition_input.size()
        
        # Calculate query, key, values for all heads in batch and move head forward to be the batch dim
        k = self.key(condition_input).view(B, N, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, N, hs)
        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)       # (B, nh, T, hs)
        v = self.value(condition_input).view(B, N, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, N, hs)
        
        # Causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, N) -&gt; (B, nh, T, N)
        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))
        att = F.softmax(att, dim=-1)
        att = self.attn_drop(att)
        y = att @ v  # (B, nh, T, N) x (B, nh, N, hs) -&gt; (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C)  # Re-assemble all head outputs side by side

        # Output projection
        y = self.resid_drop(self.proj(y))
        return y


# Example usage:
cross_attention = CrossAttention()
x = torch.randn(4, 10, 512)  # Example input tensor
output = cross_attention(x)  # Pass x without 'condition_input'
print(output.shape)
</code></pre>
<p><strong>Current Error</strong>
When condition_input is None, the code produces an error</p>
<p><strong>Desired Behavior</strong>
I would like to modify the CrossAttention block so that it can still run when condition_input is None. How can I achieve this?</p>
","transformer-model"
"78591008","I am implementing transformers from scratch in pytorch and getting some error in addition in positional encoding part in output layer","2024-06-07 09:22:25","","0","49","<deep-learning><pytorch><nlp><transformer-model><machine-translation>","<p>I am implementing transformer in pytorch and getting an error when the Positional encoding is applied in the decoder layer that is in the  <code>op_positional_encoding = self.positional_encoding(op_embed)</code> part:
<strong>RuntimeError: The size of tensor a (19) must match the size of tensor b (20) at non-singleton dimension 1</strong>  in the part <strong>return x + self.pe[:x.shape[1]]</strong></p>
<p>I am attaching all my codes:
**
Data Generation code :**</p>
<pre><code>en_tokenizer = spacy.load(&quot;en_core_web_sm&quot;)
fr_tokenizer = spacy.load(&quot;fr_core_news_sm&quot;)

def data_process(sentence_en,sentence_fr):
  data = []
  for (en_sen,fr_sen) in zip(sentence_en,sentence_fr):
    data.append((en_sen,fr_sen))
  return data
train_data = data_process(train_en,train_fr)
valid_data = data_process(valid_en,valid_fr)

UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3
special_symbols = ['&lt;unk&gt;', '&lt;pad&gt;', '&lt;bos&gt; ', ' &lt;eos&gt;']

def yield_tokens_en(data_iter):
    for (en_batch, fr_batch) in data_iter:
        for en_sent in en_batch:
            my_string = ' '.join(en_sent)
            en_tokens = my_string.split()  # Split English sentence into tokens
            yield en_tokens

def yield_tokens_fr(data_iter):
    for (en_batch, fr_batch) in data_iter:
        for fr_sent in fr_batch:
            my_string = ' '.join(fr_sent)
            fr_tokens = my_string.split()  # Split English sentence into tokens
            yield fr_tokens
en_vocab = build_vocab_from_iterator(yield_tokens_en(train_iter),
                                  min_freq=1,
                                  specials=special_symbols,
                                  special_first=True)
fr_vocab = build_vocab_from_iterator(yield_tokens_fr(train_iter),
                                  min_freq=1,
                                  specials=special_symbols,
                                  special_first=True)
en_vocab.set_default_index(UNK_IDX)
fr_vocab.set_default_index(UNK_IDX)

def pad_sequences_to_length(batch, max_length=20, padding_value=0):
    padded_sequences = []
    for seq in batch:
        if len(seq) &lt; max_length:
            # Pad sequence to max_length
            padded_seq = torch.cat([seq, torch.full((max_length - len(seq),), padding_value)])
        else:
            # Truncate sequence if longer than max_length
            padded_seq = seq[:max_length]
        padded_sequences.append(padded_seq)
    return torch.stack(padded_sequences)

def collate_fn(data_batch):
  en_batch = []
  fr_batch = []
  for (en_item,fr_item) in data_batch:
    en_ids = torch.tensor([en_vocab[token] for token in en_item],dtype = torch.long)
    fr_ids = torch.tensor([fr_vocab[token] for token in fr_item],dtype = torch.long)
    en_batch.append(torch.cat([torch.tensor([en_vocab['&lt;bos&gt;']]), en_ids ,torch.tensor([en_vocab['&lt;eos&gt;']])], dim=0))
    fr_batch.append(torch.cat([torch.tensor([fr_vocab['&lt;bos&gt;']]), fr_ids ,torch.tensor([fr_vocab['&lt;eos&gt;']])], dim=0))
  en_batch = pad_sequences_to_length(en_batch,padding_value = PAD_IDX)
  fr_batch = pad_sequences_to_length(fr_batch,padding_value = PAD_IDX)
  return en_batch,fr_batch

train_data_token_iter = DataLoader(train_data, batch_size=32,
                        shuffle=True, collate_fn=collate_fn)
valid_data_token_iter = DataLoader(valid_data, batch_size=32,
                        shuffle=True, collate_fn=collate_fn)

</code></pre>
<p>**positional coding part : **</p>
<pre><code>class PositionalEncoding(nn.Module):
    def __init__(self, embed_dim, max_seq_length):
        super(PositionalEncoding, self).__init__()

        pe = torch.zeros(max_seq_length, embed_dim)
        position = torch.arange(0, max_seq_length).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        print(x.size())
        print(self.pe.size())
        return x + self.pe[:x.shape[1]]
</code></pre>
<p>**MultiHead Attention Part : **</p>
<pre><code>class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim=512, n_heads=8):
        super(MultiHeadAttention, self).__init__()
        assert embed_dim % n_heads == 0, &quot;Embedding dimension must be divisible by the number of heads&quot;
        # Initialize dimensions
        self.embed_dim = embed_dim
        self.n_heads = n_heads
        self.single_head_dim = embed_dim // n_heads

        # Linear layers for transforming inputs
        self.W_q = nn.Linear(embed_dim, embed_dim)
        self.W_k = nn.Linear(embed_dim, embed_dim)
        self.W_v = nn.Linear(embed_dim, embed_dim)
        self.W_o = nn.Linear(embed_dim, embed_dim)

    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.single_head_dim)

        # Apply mask if provided (useful for preventing attention to certain parts like padding)
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)

        attn_probs = torch.softmax(attn_scores, dim=-1)

        # Multiply by values to obtain the final output
        output = torch.matmul(attn_probs, V)
        return output

    def split_heads(self, x):
        # Reshape the input to have num_heads for multi-head attention
        batch_size, seq_length, embed_dim = x.size()
        return x.view(batch_size, seq_length, self.n_heads, self.single_head_dim).transpose(1, 2)

    def combine_heads(self, x):
        # Combine the multiple heads back to original shape
        batch_size, _, seq_length, single_head_dim = x.size()
        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)

    def forward(self, Q, K, V, mask=None):
        # Apply linear transformations and split heads
        Q = self.split_heads(self.W_q(Q))
        K = self.split_heads(self.W_k(K))
        V = self.split_heads(self.W_v(V))

        # Perform scaled dot-product attention
        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)

        # Combine heads and apply output transformation
        output = self.W_o(self.combine_heads(attn_output))
        return output
</code></pre>
<p><strong>Encoder and Decoder Block</strong></p>
<pre><code>class EncoderBlock(nn.Module):
  def __init__(self,embed_dim,n_heads=8,expansion_factor=4):
    super(EncoderBlock,self).__init__()
    self.attention = MultiHeadAttention(embed_dim,n_heads)
    self.feed_forward = FeedForwardNetwork(embed_dim,expansion_factor*embed_dim)
    self.norm_1 = nn.LayerNorm(embed_dim)
    self.norm_2 = nn.LayerNorm(embed_dim)
    self.dropout = nn.Dropout(0.2)

  def forward(self,x,mask):
    attention_output = self.attention(x,x,x,mask)
    x = self.norm_1(x + self.dropout(attention_output))
    ff_output = self.feed_forward(x)
    x = self.norm_2(x + self.dropout(ff_output))
    return x

class DecoderBlock(nn.Module):
  def __init__(self,embed_dim,n_heads=8,expansion_factor=4):
    super(DecoderBlock,self).__init__()
    self.masked_attention = MultiHeadAttention(embed_dim,n_heads)
    self.norm1 = nn.LayerNorm(embed_dim)
    self.attention = MultiHeadAttention(embed_dim,n_heads)
    self.norm2 = nn.LayerNorm(embed_dim)
    self.feed_forward = FeedForwardNetwork(embed_dim,expansion_factor*embed_dim)
    self.norm3 = nn.LayerNorm(embed_dim)
    self.dropout = nn.Dropout(0.2)

  def forward(self,x,encoder_output,source_mask,target_mask):
    masked_output = self.masked_attention(x,x,x,target_mask)
    x = self.norm1(x + self.dropout(masked_output))
    attention_output = self.attention(x,encoder_output,encoder_output,source_mask)
    x = self.norm2(x + self.dropout(attention_output))
    ff_output = self.feed_forward(x)
    x = self.norm3(x + self.dropout(ff_output))
    return x
</code></pre>
<p><strong>Transformer Block</strong></p>
<pre><code>class Transformer(nn.Module):
  def __init__(self,input_vocab_size,output_vocab_size,device,embed_dim=512,n_heads=8,expansion_factor=4,max_sentence_len=50,num_layers=1):
    super(Transformer,self).__init__()
    self.max_len = max_sentence_len
    # self.tokenizer_en = en_tokenizer
    # self.tokenizer_fr = fr_tokenizer
    self.input_embedding = nn.Embedding(input_vocab_size,embed_dim)
    self.positional_encoding = PositionalEncoding(embed_dim,max_sentence_len)
    self.output_embedding = nn.Embedding(output_vocab_size,embed_dim)
    self.encoder_layer = nn.ModuleList([EncoderBlock(embed_dim,n_heads,expansion_factor)])
    self.decoder_layer = nn.ModuleList([DecoderBlock(embed_dim,n_heads,expansion_factor)])
    self.fc = nn.Linear(embed_dim,output_vocab_size)
    self.dropout = nn.Dropout(0.2)
  def generate_mask(self,source,target):
    source_mask = (source!=0).unsqueeze(1).unsqueeze(2)
    target_mask = (target!=0).unsqueeze(1).unsqueeze(3)
    seq_length = target.size(1)
    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length,device = device) ,diagonal=1)).bool()
    target_mask = target_mask&amp;nopeak_mask
    return source_mask,target_mask
  def forward(self,source,target):
    source_mask,target_mask = self.generate_mask(source,target)
    ip_embed = self.input_embedding(source)
    ip_positional_encoding = self.positional_encoding(ip_embed)
    ip_embedded = self.dropout(ip_positional_encoding)
    encoder_output = ip_embedded
    for enc_layer in self.encoder_layer:
      encoder_output = enc_layer(encoder_output,source_mask)
    op_embed = self.output_embedding(target)
    op_positional_encoding = self.positional_encoding(op_embed)
    op_embedded = self.dropout(op_positional_encoding)
    decoder_output = op_embedded
    for dec_layer in self.decoder_layer:
      decoder_output = dec_layer(decoder_output,encoder_output,source_mask,target_mask)
    output = self.fc(decoder_output)
    return output
</code></pre>
<p><strong>Training Script :</strong></p>
<pre><code>def train(model,data_iterator,optimizer,criterion,clip,target_vocab_size):

  model.train()

  epoch_loss = 0
  epoch_total = 0
  epoch_corrects = 0

  for _,(source,target) in enumerate(data_iterator):
    # print(source)
    # print(f&quot;Source Shape:{source.size()}&quot;)
    print(type(source))
    source,target = source.to(device),target.to(device)
    print(f&quot;Target before : {target.size()}&quot;)
    optimizer.zero_grad()

    output = model(source,target[:,:-1])
    print(f&quot;Output before : {output.size()}&quot;)
    output_reshape = output.contiguous().view(-1, output.shape[-1])
    target = target[:, 1:].contiguous().view(-1)
    print(f&quot;Output after : {output.size()}&quot;)
    print(f&quot;Target after : {target.size()}&quot;)
    loss = criterion(output_reshape, target)
    _, predicted = torch.max(output, 1)
    corrects = torch.sum(predicted == target).item()
    total = target.size(0)
    accuracy = corrects / total
    loss.backward()

    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
    optimizer.step()

    epoch_loss+=loss.item()
    epoch_corrects+=corrects
    epoch_total+=total
  return epoch_loss/len(data_iterator),epoch_corrects/epoch_total


source_vocab_size = len(en_vocab)
target_vocab_size = len(fr_vocab)
embed_dim = 256
n_heads = 8
num_layers = 6
expansion_factor = 4
max_sentence_length = 20
transformer_model = Transformer(source_vocab_size,target_vocab_size,device,embed_dim,n_heads,expansion_factor,max_sentence_length)
# transformer_model = Transformer(embed_dim,n_heads,expansion_factor,source_vocab_size,target_vocab_size,max_sentence_length,num_layers)
count_parameters(transformer_model)

N_EPOCHS = 5
CLIP = 1
loss_f = nn.CrossEntropyLoss(ignore_index=PAD_IDX)
optimizer = optim.Adam(transformer_model.parameters(),lr = 0.0001,betas = (0.9,0.98),eps = 1e-9)
best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):
  start_time = time.time()
  train_loss,train_accuracy = train(transformer_model,train_data_token_iter,optimizer,loss_f,CLIP,target_vocab_size)
  end_time = time.time()
  epoch_min,epoch_sec = epoch_time(start_time,end_time)

  print(f&quot;Epoch {epoch+1:02} completed | Time : {epoch_min}m and {epoch_sec}s&quot;)
</code></pre>
<p>also the size of target is <strong><code>torch.Size([32, 20])</code></strong>
and x.size : <strong><code>torch.Size([32, 19, 256])</code></strong>
pe.size : <strong><code>torch.Size([1, 20, 256])</code></strong></p>
<p>when I changed the output as output = model(source, target) then another error : <strong><code>Expected input batch_size (640) to match target batch_size (608)</code>.</strong></p>
","transformer-model"
"78586455","Loss is zero while training ViTPose (Vision Transformer) with custom dataset","2024-06-06 12:08:11","","0","49","<machine-learning><deep-learning><neural-network><loss-function><transformer-model>","<p>I am trying to fine tuning <a href=""https://github.com/ViTAE-Transformer/ViTPose/blob/main/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/ViTPose_base_coco_256x192.py"" rel=""nofollow noreferrer"">ViTPose</a> Base trained on COCO 256x192. At the beginning of the training my losses are already zero.</p>
<p><code>2024-06-05 17:09:38,939 - mmpose - INFO - Epoch [1][1/18]       lr: 2.376e-10, eta: 14 days, 5:07:40, time: 682.635, data_time: 2.816, heatmap_loss: 0.0000, acc_pose: 0.0000, loss: 0.0000, grad_norm: 0.0000</code></p>
<p>Debugging I've seen that the target tensor is composed of all zeros.
<code>target. Any() = False</code> and the losses object is <code>{'heatmap_loss': tensor(0., grad_fn=&lt;MulBackward0&gt;), 'acc_pose': 0.0}</code>.</p>
<p>Does anyone know why is that?</p>
<p>My <code>train.json</code> and <code>val.json</code> follow this format (as seen in the documentation):</p>
<pre><code>[ {     &quot;image_file&quot;: &quot;100-0.png&quot;,
        &quot;image_size&quot;: [ ... ],
        &quot;bbox&quot;: [ ... ],
        &quot;keypoints&quot;: [ ... ] ,
 ... } ]
</code></pre>
<p>The images are all in <code>images</code> folder.</p>
<p>The <code>train.py</code> class is as follows:</p>
<pre><code>from config_test import get_config
from mmpose.datasets import build_dataset
from mmpose.models import build_posenet
from mmpose.apis import train_model
import mmcv

# config
cfg = get_config()

# build dataset
datasets = [build_dataset(cfg.data.train)]

# build model
model = build_posenet(cfg.model)

# create work_dir
mmcv.mkdir_or_exist(cfg.work_dir)

# train model
train_model(model, datasets, cfg, distributed=False, validate=True, meta=dict())
</code></pre>
<p>The config are taken from <code>get_config()</code> method in the config class.</p>
<pre><code>from mmengine.config import Config
def get_config():
    cfg = Config.fromfile('ViTPose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/ViTPose_base_coco_256x192.py')

    # set basic configs
    cfg.data_root = 'path'
    cfg.work_dir = 'path'
    cfg.gpu_ids = range(1)
    cfg.seed = 0

    # set log interval
    cfg.log_config.interval = 1

    # set evaluation configs
    cfg.evaluation.interval = 10
    cfg.evaluation.metric = 'PCK'
    cfg.evaluation.save_best = 'PCK'

    # set batch size
    cfg.data.samples_per_gpu = 16
    cfg.data.val_dataloader = dict(samples_per_gpu=16)
    cfg.data.test_dataloader = dict(samples_per_gpu=16)

    # set dataset configs
    cfg.data.train.type = 'Custom Dataset Name'
    cfg.data.train.ann_file = 'path'
    cfg.data.train.img_prefix = 'path'

    cfg.data.val.type = 'Custom Dataset Name'
    cfg.data.val.ann_file = 'path'
    cfg.data.val.img_prefix = 'path'

    cfg.data.test.type = 'Custom Dataset Name'
    cfg.data.test.ann_file = 'path'
    cfg.data.test.img_prefix = 'path'
    return cfg
</code></pre>
<p>Most of the config though are taken from <code>ViTPose_base_coco_256x192.py</code> class.</p>
<pre><code>_base_ = [
    '../../../../_base_/default_runtime.py',
    '../../../../_base_/datasets/my_custom_dataset.py'
]
evaluation = dict(interval=10, metric='mAP', save_best='AP')

optimizer = dict(type='AdamW', lr=1e-5, betas=(0.9, 0.999), weight_decay=0.1,
                 constructor='LayerDecayOptimizerConstructor', 
                 paramwise_cfg=dict(
                                    num_layers=12, 
                                    layer_decay_rate=0.75,
                                    custom_keys={
                                            'bias': dict(decay_multi=0.),
                                            'pos_embed': dict(decay_mult=0.),
                                            'relative_position_bias_table': dict(decay_mult=0.),
                                            'norm': dict(decay_mult=0.)
                                            }
                                    )
                )

optimizer_config = dict(grad_clip=dict(max_norm=1., norm_type=2))

# learning policy
lr_config = dict(
    policy='step',
    warmup='linear',
    warmup_iters=10,
    warmup_ratio=0.001,
    step=[17, 35])
total_epochs = 100
target_type = 'GaussianHeatmap'
channel_cfg = dict(
    num_output_channels=12, # this value has been changed since I only have 12 keypoints in my custom dataset
    dataset_joints=12, # this value has been changed since I only have 12 keypoints in my custom dataset
    dataset_channel=[
       list(range(12)) # this value has been changed since I only have 12 keypoints in my custom dataset
    ],
    inference_channel=list(range(12))) # this value has been changed since I only have 12 keypoints in my custom dataset

# model settings
model = dict(
    type='TopDown',
    pretrained='path',
    backbone=dict(
        type='ViT',
        img_size=(256, 192),
        patch_size=16,
        embed_dim=768,
        depth=12,
        num_heads=12,
        ratio=1,
        use_checkpoint=True,
        mlp_ratio=4,
        qkv_bias=True,
        drop_path_rate=0.3,
    ),
    keypoint_head=dict(
        type='TopdownHeatmapSimpleHead',
        in_channels=768,
        num_deconv_layers=2,
        num_deconv_filters=(256, 256),
        num_deconv_kernels=(4, 4),
        extra=dict(final_conv_kernel=1, ),
        out_channels=channel_cfg['num_output_channels'],
        loss_keypoint=dict(type='JointsMSELoss', use_target_weight=True)),
    train_cfg=dict(),
    test_cfg=dict(
        flip_test=False,
        post_process='default',
        shift_heatmap=False,
        target_type=target_type,
        modulate_kernel=11,
        use_udp=True))

data_cfg = dict(
    image_size=[192, 256],
    heatmap_size=[48, 64],
    num_output_channels=channel_cfg['num_output_channels'],
    num_joints=channel_cfg['dataset_joints'],
    dataset_channel=channel_cfg['dataset_channel'],
    inference_channel=channel_cfg['inference_channel'],
    soft_nms=False,
    nms_thr=1.0,
    oks_thr=0.9,
    vis_thr=0.2,
    use_gt_bbox=False,
    det_bbox_thr=0.0,
    bbox_file='path',
)

train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(
        type='TopDownHalfBodyTransform',
        num_joints_half_body=8,
        prob_half_body=0.3),
    dict(
        type='TopDownGetRandomScaleRotation', rot_factor=40, scale_factor=0.5),
    dict(type='TopDownAffine', use_udp=True),
    dict(type='ToTensor'),
    dict(
        type='NormalizeTensor',
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]),
    dict(
        type='TopDownGenerateTarget',
        sigma=2,
        encoding='UDP',
        target_type=target_type),
    dict(
        type='Collect',
        keys=['img', 'target', 'target_weight'],
        meta_keys=[
            'image_file', 'joints_3d', 'joints_3d_visible', 'center', 'scale',
            'rotation', 'bbox_score'
        ]),
]

val_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='TopDownAffine', use_udp=True),
    dict(type='ToTensor'),
    dict(type='NormalizeTensor',
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]),
    dict(type='Collect', keys=['img'], meta_keys=['image_file', 'center', 'scale', 'rotation', 'bbox_score']),
]

test_pipeline = val_pipeline

data_root = 'path'
data = dict(
    samples_per_gpu=64,
    workers_per_gpu=4,
    val_dataloader=dict(samples_per_gpu=32),
    test_dataloader=dict(samples_per_gpu=32),
    train=dict(
        type='Custom Dataset Name',
        ann_file='path',
        img_prefix='path',
        data_cfg=data_cfg,
        pipeline=train_pipeline,
        dataset_info={{_base_.dataset_info}}),
    val=dict(
        type='Custom Dataset Name',
        ann_file='path',
        img_prefix='path',
        data_cfg=data_cfg,
        pipeline=val_pipeline,
        dataset_info={{_base_.dataset_info}}),
    test=dict(
        type='Custom Dataset Name',
        ann_file='path',
        img_prefix='path',
        data_cfg=data_cfg,
        pipeline=test_pipeline,
        dataset_info={{_base_.dataset_info}}),
)
</code></pre>
","transformer-model"
"78581025","ViTHybrid cant add positional embeddings and embeddings","2024-06-05 12:35:10","","0","18","<machine-learning><deep-learning><conv-neural-network><huggingface-transformers><transformer-model>","<p>When I create a new model and give it random size data as input [1, 3, 224, 224], then I get the embeddings and positional_embeddings dimension error</p>
<pre><code>model = ViTHybridModel(ViTHybridConfig(backbone_config = {
    &quot;depths&quot;: [3, 4, 16, 3],
    &quot;hidden_sizes&quot;: [128, 256, 512, 1024],
    &quot;layer_type&quot;: &quot;bottleneck&quot;
}, image_size=224)
</code></pre>
<pre><code>torch.Size([1, 3, 224, 224])
Traceback (most recent call last):
  File &quot;D:\sddif\itestingvit.py&quot;, line 17, in &lt;module&gt;
    outputs = model(inputs[&quot;pixel_values&quot;])
  File &quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py&quot;, line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py&quot;, line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\models\vit_hybrid\modeling_vit_hybrid.py&quot;, line 588, in forward
    embedding_output = self.embeddings(
  File &quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py&quot;, line 1532, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\module.py&quot;, line 1541, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;C:\Users\ermak\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\models\vit_hybrid\modeling_vit_hybrid.py&quot;, line 128, in forward
    embeddings = embeddings + self.position_embeddings
RuntimeError: The size of tensor a (50) must match the size of tensor b (577) at non-singleton dimension 1

</code></pre>
","transformer-model"
"78580988","TransformerEncoderLayer.forward() got an unexpected keyword argument 'is_causal'","2024-06-05 12:28:00","","0","76","<transformer-model>","<p>I have tried to learn TrainAD, but I am not learning because of the error Transformer EncoderLayer.forward() got an unexpected keyword argument 'is_causal'.</p>
<pre><code>class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=16, dropout=0):
        super(TransformerEncoderLayer, self).__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.activation = nn.LeakyReLU(True)

    def forward(self, src,src_mask=None, src_key_padding_mask=None, **kwargs):
        src2 = self.self_attn(src, src, src)[0]
        src = src + self.dropout1(src2)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        return src
</code></pre>
<p>What should I fix here?</p>
<p>def forward(self, src,src_mask=None, src_key_padding_mask=None, **kwargs)
Said I could put **kwargs in but it didn't work for me.</p>
","transformer-model"
"78576005","Target and input tensor size mismatch during classifier training","2024-06-04 14:21:44","","0","22","<python><huggingface-transformers><torch><transformer-model><roberta>","<p>I want to classify text paragraphs into one of 3 categories through a RobBERT-v2-dutch-base classification model. Labels are defined as float64's and are either 0, 1, or 2. I'm getting a ValueError when i call the .train() function on my Trainer object.</p>
<p>The data is within a DatasetDict, with a &quot;train&quot;, &quot;validation&quot; and &quot;test&quot; dataset. First, I tokenize by calling:</p>
<pre><code>def tokenize(batch):
    return tokenizer(batch[&quot;text&quot;], padding=True, truncation=True)
</code></pre>
<p>and mapping it onto the dataset <code>data_encoded = data.map(tokenize, batched=True, batch_size=None)</code></p>
<p>Then, I convert the dataset to a torch format:</p>
<pre><code>data_encoded.set_format(&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;attention_mask&quot;, &quot;label&quot;])
</code></pre>
<p>I set up the trainer like so:</p>
<pre><code>from transformers import Trainer, TrainingArguments

batch_size = 16
logging_steps = len(data_encoded[&quot;train&quot;]) // batch_size
model_name = f&quot;{model_ckpt}-finetuned-sentiment&quot;
training_args = TrainingArguments(output_dir=model_name,
                                  num_train_epochs=2,
                                  learning_rate=2e-5,
                                  per_device_train_batch_size=batch_size,
                                  per_device_eval_batch_size=batch_size,
                                  weight_decay=0.01,
                                  evaluation_strategy=&quot;epoch&quot;,
                                  disable_tqdm=False,
                                  logging_steps=logging_steps,
                                  push_to_hub=False,
                                  log_level=&quot;error&quot;)

trainer = Trainer(model=model, args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=data_encoded[&quot;train&quot;],
                  eval_dataset=data_encoded[&quot;validation&quot;],
                  tokenizer=tokenizer)
trainer.train();
</code></pre>
<p>When I run this I get the following error:</p>
<pre><code>ValueError                                Traceback (most recent call last)

&lt;ipython-input-12-126d3830a6aa&gt; in &lt;cell line: 8&gt;()
      6                   eval_dataset=data_encoded[&quot;validation&quot;],
      7                   tokenizer=tokenizer)
----&gt; 8 trainer.train();

10 frames

/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in binary_cross_entropy_with_logits(input, target, weight, size_average, reduce, reduction, pos_weight)
   3222 
   3223     if not (target.size() == input.size()):
-&gt; 3224         raise ValueError(f&quot;Target size ({target.size()}) must be the same as input size ({input.size()})&quot;)
   3225 
   3226     return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)

ValueError: Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 3]))
</code></pre>
<p>I already looked at the compute metrics function, but removing that didn't change the outcome. I'm currently at a loss, and don't know how to troubleshoot this.</p>
","transformer-model"
"78572248","handle multiple date format in one output in datastage","2024-06-03 19:39:49","","0","32","<date><transformer-model><datastage>","<p>I have multiple date format on my source column (YYYY/DD/MM, YYYY/MM/DD,YYYY-DD-MM,YYYY-MM-DD...) and need to load it in one output column
the source column is varchar and the output is timestamp
how can i do this on a transformer</p>
","transformer-model"
"78568713","transformer model predicting the same token during infrence but performing well during training","2024-06-03 06:31:29","","0","43","<machine-learning><deep-learning><transformer-model><machine-translation>","<p>my transformer model is not working right.</p>
<p>Training loop :</p>
<pre class=""lang-py prettyprint-override""><code>for epoch in range(40):
 data_loader = tqdm(data_loader, desc=f&quot;Epoch {epoch + 1}/{20}&quot;, unit=&quot;batch&quot;)
for batch_idx, (en_batch, hi_batch) in enumerate(data_loader):
en_batch = en_batch.to('cuda').to(torch.long)
hi_batch = hi_batch.to('cuda').to(torch.long)
y_pred = model(en_batch, hi_batch)
loss = loss_fn(y_pred[:, 0:127, :].transpose(2,1), hi_batch[:, 1:128]).mean()
history.append(loss.item())
if batch_idx % 400 == 0:
clear_output(wait=False)
torch.save(history, 'history2.pth')
torch.save(losses, 'valLoss2.pth')
torch.save(model.state_dict(), 'model_weightsBPE2.pth')
model.eval()
val_loss = 0
with torch.no_grad():
for id , (en_batch, hi_batch) in enumerate(val_loader, 1):
en_batch, hi_batch = en_batch.to('cuda'), hi_batch.to('cuda')
y_pred = model(en_batch.to(torch.long), hi_batch[:, :-1].to(torch.long))
val_loss += loss_fn(y_pred[:, 0:127, :].transpose(2,1), hi_batch[:, 1:128].to(torch.long)).mean()
if id % 5 == 0:
break
val_loss /= 5
model.train()
print(f&quot;Validation Loss: {val_loss}&quot;)
losses.append(val_loss.item())
if batch_idx % 100 == 0:
print(&quot;-&quot;20, batch_idx, &quot;-&quot;, epoch, &quot;-&quot;, loss.item(), &quot;-&quot;20)
print(&quot;en : &quot;, id_to_token(en_batch, &quot;en&quot;))
print(&quot;hi : &quot;, id_to_token(hi_batch, &quot;hi&quot;))
print(&quot;out: &quot;, id_to_token_M(y_pred, &quot;hi&quot;))
optimizer.zero_grad()
loss.backward()
optimizer.step()
scheduler.step()

</code></pre>
<p>Training output :</p>
<pre><code>out: संघ के प्रदेशाध्यक्ष बृज मो गाुप्ता ने महारे्ल इेंदिर क के शबिदिन करे विद्रा।हन्थिया। कोप माया।

</code></pre>
<p>Inference loop :</p>
<pre class=""lang-py prettyprint-override""><code>def inference_loop( input_seq, max_output_length=128):
 input_seq = input_seq.unsqueeze(0) # Add batch dimension
current_token = torch.tensor([[1]], device=input_seq.device)
output_seq = []
for * in range(max*output_length):
predictions = model(input_seq, current_token)
next_token = predictions[:, -1, :].argmax(dim=-1)
current_token = torch.cat([current_token, next_token.unsqueeze(0)], dim=1)
if next_token.item() == 2:
break
output_seq.append(next_token.item())
return output_seq

</code></pre>
<p>Inference Output :</p>
<pre><code>' क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क क'


</code></pre>
<p>how can i fix this issue, both training and validation loss goes down but during inference it starts to predict the same token again and again.</p>
","transformer-model"
"78567507","DeepLearning - Positional embedding of transformer model","2024-06-02 19:31:35","","0","39","<pytorch><artificial-intelligence><transformer-model>","<p>I have a question when should I use positional embedding in Transformers. I want to make cross attention layer with m queries and n keys. More specifically, I will reduce the image to 512x7x7 using resnet and use 49 vectors as keys. In this case, where should I add positional embedding? Queries? Keys? Or both? Or neither?</p>
<p>I first tried learning both without positional embeddings, and later tried learning them by adding positional embeddings only to the key.Due to capacity issues, I ran about 20 epochs using 1000 image data, but no significant differences were found.</p>
","transformer-model"
"78559388","Warning: Gradients do not exist for variables","2024-05-31 09:53:15","78559389","0","64","<python><tensorflow><keras><transformer-model>","<p>I recently came across a warning in Tensorflow that caused some head-scratching and took a while to fix. Since I didn't find a solution online, I wanted to share.</p>
<p>I am building a transformer (encoder-decoder) architecture. But my training results are really bad. The transformer always gives the same answer no matter the input, although the training accuracy looks very good (above 0.95). On top of that, I get this warning:</p>
<p><code>WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0'] when minimizing the loss. If you're using 'model.compile()', did you forget to provide a 'loss' argument?</code></p>
<p>Both the encoder and decoder have</p>
<ul>
<li>a token embedding realized through a <code>keras.Embedding</code> layer</li>
<li>a positional embedding, realized through a
<code>keras_nlp.PositionEmbedding</code> layer.</li>
</ul>
<p>Here is the encoder code:</p>
<pre class=""lang-py prettyprint-override""><code>encoder_inputs = Input(shape=(encoder_inputs_size,), name=&quot;encoder_inputs&quot;)
token_embeddings = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)   (encoder_inputs)
position_embeddings = PositionEmbedding(sequence_length=encoder_inputs_size)(token_embeddings)
encoder_outputs = TransformerEncoder(intermediate_dim=intermediate_dim, num_heads=num_heads)(inputs=position_embeddings)
encoder = Model(encoder_inputs, encoder_outputs, name=&quot;encoder&quot;)
</code></pre>
<p>There is <code>keras_nlp.TokenAndPositionEmbedding</code> that combines two embeddings into a single layer and using it makes the problem disappear. But since I want to use other forms of embedding, like patch embedding for image processing, I can't use this combined layer.</p>
","transformer-model"
"78548637","Transformer from Scratch can not make inference","2024-05-29 09:39:34","","0","17","<deep-learning><transformer-model><machine-translation>","<p>I'm coding a Transformer base on this <a href=""https://github.com/hyunwoongko/transformer"" rel=""nofollow noreferrer"">Github link</a> but when the training process was already done, it can only infer the  characters and nothing else while testing.</p>
<p>I will describe what i had done so far here.<br />
I choose this <a href=""https://www.kaggle.com/datasets/mynames3m/train-vi-en"" rel=""nofollow noreferrer"">dataset</a> from Kaggle. It's a pair of file <code>train.en</code> contains english sentences and <code>train.vi</code> contains vietnamese sentences.<br />
Firstly, i go through 2 files to create 2 dictionaries <code>(index : word)</code> for 2 language.<br />
Then, i set MAX_LENGTH = 300 and change all sentences of both file into 300-dimension vectors.<br />
This is my code:</p>
<pre><code>SOS = 1
EOS = 0

class Language:
    def __init__(self, name):
        self.name = name
        self.word2idx = {}
        self.idx2word = {1: 'SOS', 0: 'EOS'}
        self.word2count = {}
        self.n_words = 2
    
    def addWord(self, word):
        word = word.replace('.', '')
        if word not in self.word2idx:
            self.idx2word[self.n_words] = word
            self.word2idx[word] = self.n_words
            self.n_words += 1
            self.word2count[word] = 1
        else:
            self.word2count[word] += 1
        return self.idx2word[self.n_words-1], self.word2idx[word], self.word2count[word]
            
    def addSentence(self, sentence):
        sentence = sentence.strip().replace('  ', ' ')
        return [self.addWord(word) for word in sentence.split()]
    
    def addManySentences(self, many_sentences):
        return [self.addSentence(sentence) for sentence in many_sentences]
    
    def sentence2vector(self, sentence, pretrain=None):
        sentence = sentence.replace('.', '')
        vector = []
        for word in sentence.split():
            if pretrain != None:
                if word in pretrain:
                    vector.append(pretrain.key_to_index[word])
                else:
                    vector.append(0)
            else:
                if word in self.word2idx:
                    vector.append(self.word2idx[word])
                else:
                    vector.append(0)
        vector.append(EOS)
        return vector
    
    def vector2sequence(self, vector, pretrain=None):
        words = []
        for idx in vector:
            if pretrain != None:
                words.append(pretrain.index_to_key[idx])
            else:
                words.append(self.idx2word[idx])
        words.append('EOS.')
        sequence = ' '.join(words).replace('SOS ', '').replace(' EOS', '')
        return sequence
    
    def manyVector2manySequece(self, manyVector, pretrain=None):
        return [self.vector2sequence(vector, pretrain) for vector in manyVector]

inputs = []
with open('/kaggle/input/train-vi-en/train.en', 'r') as f:
    lines = f.read().strip().split('\n')
    for line in lines:
        line = '\t'.join(line.split('\t')[:2])
        for p in string.punctuation:
            if p != &quot;'&quot; and p in line:
                line = line.replace(p, f'')
        inputs.append(line)
inputs = np.array(inputs)
        
targets = []
with open('/kaggle/input/train-vi-en/train.vi', 'r') as f:
    lines = f.read().strip().split('\n')
    for line in lines:
        line = '\t'.join(line.split('\t')[:2])
        for p in string.punctuation:
            if p != &quot;'&quot; and p in line:
                line = line.replace(p, f'')
        targets.append(line)
targets = np.array(targets)

eng = Language('english')
eng.addManySentences(inputs)

vie = Language('vietnamese')
vie.addManySentences(targets)

num_seq = len(inputs)

input_matrix = torch.zeros((num_seq, MAX_LENGTH), dtype=torch.int)
for idx in range(num_seq):
    vector = eng.sentence2vector(inputs[idx])
    end_pos = min(MAX_LENGTH, len(vector))
    input_matrix[idx, :end_pos] = torch.tensor(vector)[:end_pos]
    
target_matrix = torch.zeros((num_seq, MAX_LENGTH), dtype=torch.int)
for idx in range(num_seq):
    vector = vie.sentence2vector(targets[idx])
    end_pos = min(MAX_LENGTH, len(vector))
    target_matrix[idx, :end_pos] = torch.tensor(vector)[:end_pos]
</code></pre>
<p>Finally, i split it to trainset, testset and validset and use them to create torch DataLoader.</p>
<p>This is my Transformer code:</p>
<pre><code>def padding_mask(tensor, device='cpu'):
    # Padding mask để lọc ra mấy cái kiểu sos, eos mà ta thêm vào, vốn vô nghĩa
    return 1 - (tensor == 0).to(torch.int).to(device)

def lookahead_mask(tensor, device='cpu'):
    # Look-ahead mask để cho việc chỉ nhìn vào những từ ở đằng trước trong self-attention
    sq_len = tensor.shape[-1]
    return torch.ones(sq_len, sq_len).tril().to(device)

class Transformer(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, n_heads, ffw_hidden, n_encoders=1, n_decoders=1, dropout=0.5, device='cpu'):
        super().__init__()
        
        self.device = device
        self.output_size = output_size
        
        self.encoder = Encoder(input_size, hidden_size, n_heads, ffw_hidden, n_encoders, dropout, device=device)
        self.decoder = Decoder(hidden_size, output_size, n_heads, ffw_hidden, n_decoders, dropout, device=device)
        
        self.linear = nn.Linear(hidden_size, output_size)
        self.softmax = nn.LogSoftmax(dim=-1)
        
    def forward(self, input_tensor, target_tensor):
        encoder_mask = self.createEncoderMask(input_tensor)
        decoder_mask = self.createDecoderMask(target_tensor)
        
        encoder_output = self.encoder(input_tensor, encoder_mask)
        decoder_output = self.decoder(encoder_output, target_tensor, decoder_mask)
        out = self.softmax(self.linear(decoder_output))
        return out
    
    def createEncoderMask(self, encoder_input):
        return padding_mask(encoder_input, device=self.device).unsqueeze(1)
    
    def createDecoderMask(self, decoder_input):
        padding = padding_mask(decoder_input, device=self.device)
        lookahead = lookahead_mask(decoder_input, device=self.device)
        
        mask = torch.Tensor().to(self.device)
        for pad in padding:
            mask = torch.concat([mask, torch.min(pad, lookahead).unsqueeze(0)], dim=0)
        return mask
    
    def translate(self, x):
        x = x.squeeze().unsqueeze(0)
        target_tensor = torch.full(x.shape, EOS, device=self.device)
        return self.forward(x, target_tensor)

class Encoder(nn.Module):
    def __init__(self, input_size, embedding_dim, n_heads, ffw_hidden, n_encoders=1, dropout=0.1, device='cpu'):
        super().__init__()
        
        self.embedding = nn.Embedding(input_size, embedding_dim, device=device)
        self.PE = PositionalEncoding(embedding_dim, MAX_LENGTH, device=device)
        
        self.list_encoder_layers = nn.ModuleList([EncoderLayer(embedding_dim, n_heads, ffw_hidden, dropout=dropout) for _ in range(n_encoders)])
        
    def forward(self, input_tensor, mask=None):
        embedded = self.embedding(input_tensor)
        PE = self.PE(input_tensor)
        
        tensor = embedded + PE
        for layer in self.list_encoder_layers:
            tensor = layer(tensor, mask=mask)
        return tensor
    
class EncoderLayer(nn.Module):
    def __init__(self, input_size, n_heads, ffw_hidden, dropout=0.1):
        super().__init__()
        
        self.attention = MultiHeadAttention(input_size, n_heads)
        self.norm1 = Norm(input_size)
        self.dropout1 = nn.Dropout(dropout)
        
        self.ffw = FeedForward(input_size, ffw_hidden)
        self.norm2 = Norm(input_size)
        self.dropout2 = nn.Dropout(dropout)
        
    def forward(self, X, mask=None):
        _X = torch.clone(X)
        X = self.attention(X, X, X, mask=mask)
        X = self.norm1(self.dropout1(X + _X))
        
        _X = torch.clone(X)
        X = self.ffw(X)
        X = self.norm2(self.dropout2(X + _X))
        return X

class Decoder(nn.Module):
    def __init__(self, embedding_dim, output_size, n_heads, ffw_hidden, n_decoders=1, dropout=0.1, device='cpu'):
        super().__init__()
        
        self.embedding = nn.Embedding(output_size, embedding_dim, device=device)
        self.PE = PositionalEncoding(embedding_dim, MAX_LENGTH, device=device)
        
        self.list_decoder_layers = nn.ModuleList([DecoderLayer(embedding_dim, n_heads, ffw_hidden, dropout=dropout) for _ in range(n_decoders)])
        
    def forward(self, encoder_output, target_tensor, mask=None):
        embedded = self.embedding(target_tensor)
        PE = self.PE(target_tensor)
        
        X = embedded + PE
        for layer in self.list_decoder_layers:
            X = layer(encoder_output, X, mask=mask)
        return X
    
class DecoderLayer(nn.Module):
    def __init__(self, input_size, n_heads, ffw_hidden, dropout=0.1):
        super().__init__()
        
        self.attention1 = MultiHeadAttention(input_size, n_heads)
        self.norm1 = Norm(input_size)
        self.dropout1 = nn.Dropout(dropout)
        
        self.attention2 = MultiHeadAttention(input_size, n_heads)
        self.norm2 = Norm(input_size)
        self.dropout2 = nn.Dropout(dropout)
        
        self.ffw = FeedForward(input_size, ffw_hidden)
        self.norm3 = Norm(input_size)
        self.dropout3 = nn.Dropout(dropout)
        
    def forward(self, encoder_output, X, mask=None):
        _X = torch.clone(X)
        X = self.attention1(X, X, X, mask=mask)
        X = self.norm1(self.dropout1(X + _X))
        
        _X = torch.clone(X)
        X = self.attention2(X, encoder_output, encoder_output, mask=mask)
        X = self.norm2(self.dropout2(X + _X))
        
        _X = torch.clone(X)
        X = self.ffw(X)
        X = self.norm3(self.dropout3(X + _X))
        return X

class PositionalEncoding(nn.Module):
    def __init__(self, embedding_dim, max_length, device):
        super().__init__()
        
        self.embedding_dim = embedding_dim
        self.device = device
        
        embedding = torch.zeros(max_length, self.embedding_dim, device=self.device)
        
        pos = torch.arange(0, max_length, device=self.device).float().unsqueeze(dim=1)
        i = torch.arange(0, self.embedding_dim, device=self.device)
        
        embedding[:, 0::2] = torch.sin(pos / (10000 ** (i[0::2] / self.embedding_dim)))
        embedding[:, 1::2] = torch.cos(pos / (10000 ** ((i[1::2]-1) / self.embedding_dim)))
        
        self.embed = nn.Parameter(embedding)
        
    def forward(self, X):
        seq_length = X.shape[-1]
        
        return self.embed[:seq_length]

class MultiHeadAttention(nn.Module):
    def __init__(self, input_size, n_heads):
        super().__init__()
        
        self.n_heads = n_heads
        
        self.lv = nn.Linear(input_size, input_size)
        self.lk = nn.Linear(input_size, input_size)
        self.lq = nn.Linear(input_size, input_size)
        
        self.attention = ScaledDotProductAttention()
        
        self.l_concat = nn.Linear(input_size, input_size)
        
    def forward(self, q, k, v, mask=None):
        q, k, v = self.lq(q), self.lk(k), self.lv(v)
        q, k, v = self.split(q), self.split(k), self.split(v)
        out = self.attention(q, k, v, mask=mask)
        
        out = self.concat(out)
        out = self.l_concat(out)
        
        return out
    
    def split(self, tensor):
        batch_size, length, input_size = tensor.shape
        
        head_size = input_size // self.n_heads
        
        tensor = tensor.view(batch_size, length, self.n_heads, head_size).transpose(-2, -3)
        return tensor
    
    def concat(self, tensor):
        batch_size, n_heads, length, head_size = tensor.shape
        
        input_size = n_heads * head_size
        
        tensor = tensor.transpose(-2, -3).contiguous().view(batch_size, length, input_size)
        return tensor
    
class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super().__init__()
        
    def forward(self, q, k, v, mask=None):
        dk = k.shape[-1]
        k_t = k.transpose(-1, -2)
        
        matmulqk = q @ k_t
        scale = matmulqk / math.sqrt(dk)
        
        if mask is not None:
            for batch in range(scale.shape[0]):
                for head in range(scale.shape[1]):
                    scale[batch, head] = scale[batch, head].masked_fill(mask[batch] == 0, -1e9)
            
        softmax = nn.LogSoftmax(dim=-1)(scale)
        
        matmul = softmax @ v
        return matmul

class Norm(nn.Module):
    def __init__(self, input_size, eps=1e-7):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(input_size))
        self.beta = nn.Parameter(torch.zeros(input_size))
        self.eps = eps
        
    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        var = x.var(-1, unbiased=False, keepdim=True)
        
        out = (x - mean) / torch.sqrt(var + self.eps)
        out = self.gamma * out + self.beta
        return out

class FeedForward(nn.Module):
    def __init__(self, input_size, hidden_size, dropout=0.1):
        super().__init__()
        
        self.ln1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.ln2 = nn.Linear(hidden_size, input_size)
        
    def forward(self, X):
        X = self.ln1(X)
        X = self.relu(X)
        X = self.dropout(X)
        X = self.ln2(X)
        return X
</code></pre>
<p>This is my train function:</p>
<pre><code>def train(train_loader, model, n_epochs, criterion, learning_rate=0.001, print_every=100, device='cpu'):
    print_loss_total = 0
    
    model = model.to(device)

    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = criterion

    for epoch in range(1, n_epochs + 1):
        total_loss = 0
        for input_tensor, target_tensor in train_loader:
            input_tensor = input_tensor.to(device)
            target_tensor = target_tensor.to(device)

            optimizer.zero_grad()

            outputs = model(input_tensor, target_tensor)
            
            loss = criterion(
                outputs.view(-1, outputs.size(-1)),
                target_tensor.view(-1).long()
            )
            loss.backward()

            optimizer.step()
            
            del input_tensor
            del target_tensor
            torch.cuda.empty_cache()

            total_loss += loss.item()

        print_loss_total += total_loss / len(train_loader)

        if epoch % print_every == 0:
            print_loss_avg = print_loss_total / print_every
            print_loss_total = 0
            print(f'At {epoch}, loss is {print_loss_avg}')
</code></pre>
<p>And when i just use the trainset to test, an unexpexted error occur:</p>
<pre><code>words = []
for input_tensor, target_tensor in train_loader:
    input_tensor = input_tensor.to(device)[:1]
    target_tensor = target_tensor.to(device)[:1]
    
    print('Eng: ', eng.manyVector2manySequece(input_tensor.cpu().numpy()))
    print('Vie: ', vie.manyVector2manySequece(target_tensor.cpu().numpy()))
    
    en_mask = model.createEncoderMask(input_tensor)
    en_output = model.encoder(input_tensor, mask=en_mask)
    
    pred_tensor = torch.randint(1, output_size, (1, MAX_LENGTH), device=device)
    pred_tensor[0, 0] = SOS
    for i in range(2, MAX_LENGTH+1):
        de_input = pred_tensor[:, :i]
        en_out = en_output[:, :i]
        de_mask = model.createDecoderMask(de_input)
        de_output = model.decoder(en_out, de_input, de_mask)
        out = model.softmax(model.linear(de_output)).topk(1)[-1]
    print(out)
    break
</code></pre>
<p>The result of out is full of zeros.</p>
<p>I have tried to change the size of dataset but nothing change.<br />
I have use full code from the Github link which i'm base on but it requires torchtext==0.6 and Kaggle got error while i try to pip install it.<br />
I change the criterion at train, raise the batch_size, raise number of epochs, change the learning rate. That's all of solution a can figure but nothing can change the result that my Transformer can only predict a full-zero vector from train data.<br />
I haven't known where is wrong so far.<br />
Anyone help me!</p>
","transformer-model"
"78540831","Can I fine-tune a sentence transformer model using the transformer bib in python?","2024-05-27 19:38:56","","0","25","<python><artificial-intelligence><transformer-model><sentence-transformers><sts>","<p>I want to use a sentence transformer model for STS task, but the sentence transformer bib is poor, so I want to use transformer bib to track my tuning using callbacks, early stopping, etc.
I know I can use a sentence transformer with transformer bib creating a pooling function, but I don't know how to introduce that function into train function.</p>
<p>I am thinking of overwriting the train function to modify just the part I want, like before the evaluation to compute the sentence embeddings and them send to compute the loss, but I think there must be some better ideia.
Any suggestion are welcome.</p>
","transformer-model"
"78536518","ImportError: cannot import name 'UMAP' from 'umap' (unknown location)","2024-05-26 21:26:40","","0","81","<machine-learning><deep-learning><bert-language-model><transformer-model>","<p>I'm currently working on converting Word2Vec embedding to PubMedBERT embedding. I am working in Google Colab, and I'm running into the issue where: ImportError: cannot import name 'UMAP' from 'umap' (unknown location) keeps popping up even though I've uninstalled bertopic and reinstalled it.</p>
<pre><code>from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from umap import UMAP

docs = abstracts
sentence_model = SentenceTransformer(&quot;neuml/pubmedbert-base-embeddings&quot;)
embeddings = sentence_model.encode(docs, show_progress_bar = False)

topic_model = BERTopic()
topics, probs = topic_model.fit_transform(docs, embeddings = embeddings)

topic_model.visualize_documents(docs, embeddings = embeddings)
</code></pre>
<p>^is my code cell.</p>
<p>I've also uninstalled umap and reinstalled it. I read online that I need to restart the runtime, which I did, but it seems that I cannot get umap to get imported correctly. I've triied restarting the runtime, uninstalling and reinstalling bertopic, sentence_transformers, and umap. Are there any potential issues outside of this cell?</p>
","transformer-model"
"78525656","Loss function for training a transformer language model","2024-05-23 21:34:01","","0","57","<transformer-model>","<p>When training a language model, I would expect a maximum likelihood setting, i.e. searching for the model parameters that maximize the probability that the model generates the training text. Or equivalently, minimize <code>-ln</code> of that probability.</p>
<p>However the minimized loss function in <a href=""https://github.com/karpathy/nanoGPT/blob/master/train.py"" rel=""nofollow noreferrer"">Karpathy's Nano GPT</a> seems slightly different, <code>estimate_loss</code> line 216. He also talks about this loss in <a href=""https://www.youtube.com/watch?v=kCc8FmEb1nY"" rel=""nofollow noreferrer"">this video</a>, at minute 40.</p>
<p>If I understand correctly, his <code>estimate_loss</code> computes <code>-ln</code> of the probability to generate a substring, of same length as the context (i.e. the number of past words used to infer the next word), drawn at a random position in the training text. And <code>estimate_loss</code> does that multiple times and averages, so it is a Monte Carlo estimation of this global average:</p>
<pre><code>(1 / training text length) * sum_{k &lt; training text length} (-ln P (substring at k))
</code></pre>
<p>The constant denominator does not affect the minimization, so Karpathy's loss is equivalent to</p>
<pre><code>sum_{k &lt; training text length} (-ln P (substring at k))
</code></pre>
<p>Now the probability <code>P (substring at k)</code> is the product of the probabilities to generate each word in the substring, given the previous words in the substring, and the <code>ln</code> converts that into a sum, so we see that Karpathy's loss is actually a double sum. The number of terms in it is <code>training text length * context length</code>. This number is so high that Karpathy cannot fully compute this loss, which is probably why he just Monte Carlo estimates it in <code>estimate_loss</code>.</p>
<p>If we reorder the double sum, we do see the probability to generate the whole training text, as I was expecting: only take terms with full context. But in addition to those terms, the double sum also includes all subtrings with truncated contexts, of all sizes down to 1.</p>
<p>Is there a theoretical justification to use this alternative loss function, of which the vast majority of samples (truncated contexts) are not used during inference (generated texts always use full contexts, to have more information)?</p>
","transformer-model"
"78516979","How to Prevent NaN Values from Affecting Attention in a PyTorch TransformerEncoder","2024-05-22 10:45:46","","0","111","<python><pytorch><transformer-model><seq2seq>","<p>I'm working on a Transformer-based encoder model using PyTorch. I want the model to ignore certain values in the sequence that are marked as NaN and ensure these NaN values do not affect the rest of the sequence. I'm trying to implement a mask to achieve this, but the results are not as expected. Here is my test code:</p>
<pre><code>import torch as th
from torch import nn

B, S, C = 3, 5, 1
hidden_size = 3
num_heads = 1
num_encoder_layers = 1

# Layers
linear = nn.Linear(C, hidden_size)
encoder = nn.TransformerEncoder(
    nn.TransformerEncoderLayer(
        d_model=hidden_size,
        nhead=num_heads,
        batch_first=True,
        activation=nn.ReLU(),
    ),
    num_layers=num_encoder_layers,
    norm=nn.LayerNorm(hidden_size),
)

x = th.rand(B, S, C)
x[0, 2, :] = th.nan
x[2, 4, :] = th.nan

# Mask for lookahead
tri_mask = th.full((S, S), 1)
tri_mask = th.tril(tri_mask, diagonal=1) == 0

x = linear(x)
nan_mask = th.isnan(x).any(dim=-1)
valid_mask = ~nan_mask
attn_mask = valid_mask.unsqueeze(1) &amp; valid_mask.unsqueeze(2)
x_out = encoder(x, mask=attn_mask)

print(&quot;Output:\n&quot;, x_out)
print(&quot;Attention Mask:\n&quot;, attn_mask)
</code></pre>
<p>My goal is to ensure that the model ignores the NaN values in the sequence so that they do not affect the attention mechanism of the Transformer. However, the output is not as expected. Specifically, the values NaN in x[0, 2, :] and x[2, 4, :] seem to propagate through the model.</p>
<p>The current output of the code is:</p>
<pre><code>tensor([[[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]],

        [[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]],

        [[nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan],
         [nan, nan, nan]]], grad_fn=&lt;NativeLayerNormBackward0&gt;)
</code></pre>
<p>The attention mask being used is:</p>
<pre><code>tensor([[[ True,  True, False,  True,  True],
         [ True,  True, False,  True,  True],
         [False, False, False, False, False],
         [ True,  True, False,  True,  True],
         [ True,  True, False,  True,  True]],

        [[ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True]],

        [[ True,  True,  True,  True, False],
         [ True,  True,  True,  True, False],
         [ True,  True,  True,  True, False],
         [ True,  True,  True,  True, False],
         [False, False, False, False, False]]])
</code></pre>
<p>Once the problem is solved, I would like to combine that mask of valid records with the triangular mask with lookahead that is listed as tri_mask because I would like each record to only be able to see the one next to itself in the sequence and all the previous ones.</p>
<p>Is there any way to do this?</p>
","transformer-model"
"78501874","RuntimeError in a Simple QA model using Transformer","2024-05-19 07:28:22","","0","27","<python><machine-learning><deep-learning><transformer-model>","<p>I am new to transformer architecture. I am trying to build a model that will generate answers to questions. For simplicity, I am using three sample questions and answers in a pandas dataframe. After finishing the training, it shows an error. I have tried changing many parts of the code, but due to my limited knowledge, I have been stuck with this error for the last two days. The code and errors are provided below. Please help me solve the error.</p>
<p><strong>Code:</strong></p>
<pre><code>pip install -U torchtext==0.6
import torch
import torch.nn as nn
import torch.optim as optim
import torchtext
from torchtext.data import Field, BucketIterator, TabularDataset
import random
import numpy as np
import pandas as pd
data = {
    'Question': ['What is the capital of France?', 'Who wrote Hamlet?', 'What is the meaning of life?'],
    'Answer': ['Paris', 'William Shakespeare', '42']
}
df = pd.DataFrame(data)
df['Question'] = df['Question'].apply(lambda x: x.lower())
import spacy
nlp = spacy.load('en_core_web_sm')
question_field = Field(tokenize=lambda x: [tok.text for tok in nlp(x)], init_token='&lt;sos&gt;', eos_token='&lt;eos&gt;', lower=True, include_lengths=False, use_vocab=True)
answer_field = Field(tokenize=lambda x: [tok.text for tok in nlp(x)], init_token='&lt;sos&gt;', eos_token='&lt;eos&gt;', lower=True, include_lengths=False, use_vocab=True)
fields = [('Question', question_field), ('Answer', answer_field)]
examples = [torchtext.data.Example.fromlist([df['Question'][i], df['Answer'][i]], fields) for i in range(df.shape[0])]
question_field.build_vocab(df['Question'], min_freq=2)
answer_field.build_vocab(df['Answer'], min_freq=2)
dataset = torchtext.data.Dataset(examples, fields)
train_data, valid_data = dataset.split(split_ratio=0.8)
class TransformerModel(nn.Module):
    def __init__(self, input_dim, output_dim, n_layers, n_heads, hidden_dim, dropout):
        super().__init__()

        self.embedding = nn.Embedding(input_dim, hidden_dim)
        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(hidden_dim, n_heads, hidden_dim, dropout), n_layers)
        self.decoder = nn.TransformerDecoder(nn.TransformerDecoderLayer(hidden_dim, n_heads, hidden_dim, dropout), n_layers)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, trg):
        src = self.embedding(src)
        trg = self.embedding(trg)

        src = self.dropout(src)
        trg = self.dropout(trg)

        encoder_output = self.encoder(src)
        decoder_output = self.decoder(trg, encoder_output)

        output = self.fc_out(decoder_output)

        return output
input_dim = len(question_field.vocab)
output_dim = len(answer_field.vocab)
n_layers = 6
n_heads = 4
hidden_dim = 512  
dropout = 0.1
model = TransformerModel(input_dim, output_dim, n_layers, n_heads, hidden_dim, dropout)
optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
BATCH_SIZE = 64
train_iterator, valid_iterator = BucketIterator.splits(
    (train_data, valid_data),
    batch_size=BATCH_SIZE,
    sort_within_batch=True,
    sort_key=lambda x: len(x.Question),
    device=device  
)

def train(model, iterator, optimizer, criterion):
    model.train()
    epoch_loss = 0

    for batch in iterator:
        src = batch.Question
        trg = batch.Answer

        optimizer.zero_grad()

        output = model(src, trg)
        output_dim = output.shape[-1]

        output = output.view(-1, output_dim)

        trg = trg.transpose(0, 1)  
        trg = torch.tensor([answer_field.vocab.stoi[token.item()] for token in trg.reshape(-1)], dtype=torch.long, device=device)

        loss = criterion(output, trg)
        loss.backward()

        optimizer.step()

        epoch_loss += loss.item()

    return epoch_loss / len(iterator)

N_EPOCHS = 10
for epoch in range(N_EPOCHS):
    train_loss = train(model, train_iterator, optimizer, criterion)
    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}')

# Generate responses
def generate_response(model, question_field, answer_field, question):
    model.eval()

    tokenized_question = question_field.tokenize(question)
    tokenized_question = [question_field.init_token] + tokenized_question + [question_field.eos_token]
    numerical_question = [question_field.vocab.stoi[token] for token in tokenized_question]

    src_tensor = torch.LongTensor(numerical_question).unsqueeze(0).to(device)
    trg_tensor = torch.zeros(100).long().unsqueeze(0).to(device)

    with torch.no_grad():
        output = model(src_tensor, trg_tensor)

    output = output.squeeze(0)
    generated_answer = [answer_field.vocab.itos[idx] for idx in output.argmax(1).cpu().numpy()]
    generated_answer = generated_answer[1:]  # Remove &lt;sos&gt;

    if '&lt;eos&gt;' in generated_answer:
        generated_answer = generated_answer[:generated_answer.index('&lt;eos&gt;')]

    return ' '.join(generated_answer)

# Example usage
question = &quot;What is the capital of France?&quot;
response = generate_response(model, question_field, answer_field, question)
print(&quot;Response:&quot;, response) 
</code></pre>
<p><strong>Error:</strong></p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-129-bb346f3fdb29&gt; in &lt;cell line: 26&gt;()
     24 # Example usage
     25 question = &quot;What is the capital of France?&quot;
---&gt; 26 response = generate_response(model, question_field, answer_field, question)
     27 print(&quot;Response:&quot;, response)

14 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in multi_head_attention_forward(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)
   5380     q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
   5381     if static_k is None:
-&gt; 5382         k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
   5383     else:
   5384         # TODO finish disentangling control flow so we don't do in-projections when statics are passed

RuntimeError: shape '[1, 400, 128]' is invalid for input of size 4608
</code></pre>
","transformer-model"
"78494354","How to Quantize the ViT Model in timm to FP16 Precision","2024-05-17 08:17:30","","0","69","<pytorch><transformer-model><quantization>","<p>I am a hardware developer，and I want to map the ViT model of timm to some custom accelerators which only support FP16 precision. But I have learned that the model cannot be quantized to FP16 by <code>torch.quantization.quantize_static</code>(actually, I am not aware of the difference between <code>quantize_dynamic</code> and <code>quantize_static</code>, just someone told me to use quatize_static.)</p>
<p>I think there must be some ways to get it. Is there any tutorials?</p>
<p>By the way, is it neccesary to retrain the model if I find the way to use <code>quantize_static</code>? I not good at AI software.</p>
","transformer-model"
"78494220","Attention Tensor Shape meaning","2024-05-17 07:49:12","","1","37","<python><nlp><transformer-model><attention-model><self-attention>","<p>While I'm trying to extract Attention from a model, I see that there is a part where the attention changes its shape after <code>matmul()</code> with <code>v</code> (value).</p>
<p>The shape goes from:
attention_probs 2 shape: <code>torch.Size([1, 12, 464, 464])</code></p>
<p>To:
attention shape: <code>torch.Size([1, 464, 12, 64])</code></p>
<p>And then rearranged to: <code>torch.Size([464, 768])</code></p>
<p><strong>The most important question, is what part should I use to visualize the attention of each token to another ?</strong></p>
<p>If you can tell me why, I'd be happy. Especially, if I need to use the part before the <code>matmul()</code>, then I wonder why isn't <code>v</code> included in the attention.</p>
<pre><code>qkv = self.Wqkv(hidden_states)
        qkv = pad_input(qkv, indices, cu_seqlens.shape[0] - 1,
                        max_seqlen_in_batch)  # batch, max_seqlen_in_batch, thd
        qkv = rearrange(qkv,
                        'b s (t h d) -&gt; b s t h d',
                        t=3,
                        h=self.num_attention_heads)
        if self.p_dropout or flash_attn_qkvpacked_func is None:
            # if we have nonzero attention dropout (e.g. during fine-tuning) or no Triton, compute attention in PyTorch
            q = qkv[:, :, 0, :, :].permute(0, 2, 1, 3)  # b h s d
            k = qkv[:, :, 1, :, :].permute(0, 2, 3, 1)  # b h d s
            v = qkv[:, :, 2, :, :].permute(0, 2, 1, 3)  # b h s d
            attention_scores = torch.matmul(q, k) / math.sqrt(
                self.attention_head_size)
            attention_scores = attention_scores + bias
            attention_probs = nn.functional.softmax(attention_scores, dim=-1)
            attention_probs = self.dropout(attention_probs)

            # Before matmul(): Torch.Size([1, 12, 464, 464])
            print(f'BUSA: attention_probs 2 shape: {attention_probs.shape}')

            attention = torch.matmul(attention_probs, v).permute(0, 2, 1,
                                                                 3)  # b s h d
            # After matmul() torch.Size([1, 464, 12, 64])
            print(f'BUSA: attention shape: {attention.shape}')

        else:
            # Triton implementation only supports 0 attention dropout
            convert_dtype = qkv.dtype not in [torch.float16, torch.bfloat16]
            if convert_dtype:
                # Triton implementation only supports fp16 and bf16
            ...
            else:
                attention = flash_attn_qkvpacked_func(qkv, bias)
                print(f'BUSA Triton: attention 2 shape: {attention_probs.shape}')

        # attn_mask is 1 for attend and 0 for don't
        attention = unpad_input_only(attention, torch.squeeze(attn_mask) == 1)

        # Still the same: torch.Size([1, 12, 464, 464])
        print(f'BUSA unpadded final attention shape: {attention_probs.shape}')

        rearranged_attention = rearrange(attention, 'nnz h d -&gt; nnz (h d)')

        # torch.Size([464, 768]) which is [464,12,64]
        print(f'REARRANGED ATTENTION: {rearranged_attention.shape}')

        return rearrange(attention, 'nnz h d -&gt; nnz (h d)')
</code></pre>
","transformer-model"
"78493118","Is there any reversible implementation for LSTM or transformer models?","2024-05-17 01:47:00","","0","24","<machine-learning><lstm><transformer-model>","<p>LSTMs and transformers can be trained to injest a stream of data (e.g. typed text) and output a stream of probability scores, i.e. sequence-to-sequence.</p>
<p>This can be useful for the likes of stylistic scoring, hateful language detection, etc.</p>
<p>I'd picture a user interacting with the model through a web-based GUI.</p>
<p>If the backend model (for the sake of argument: LSTM) is used in order to dynamically display a score next to the user's block of text, then in most instances, typed characters can just be fed through the model one-by-one.</p>
<p>However, real-world writing tool users often create typos, and thereby hit the backspace a number of times before resuming writing.  This would break the flow of the seq2seq model.  Imaginably, the full block of text would need to be fed through the model again, to reach the model state that had been present at the earlier point, the point to which the user 'backspaced'.
Alternatively, for smaller models, the state of the model could be cached on each keypress, but I'd imagine this to be infeasible in most scenarios.</p>
<p>My question is: are there any seq2seq models out there, capable of 'rewinding' state?  I.e. rather than using input characters in order to determine the next output item, input characters would instead arrive in reverse, and the intended purpose would be to rewind the internal state of memory units?  This would be great, as backspace in the former example would not trigger the LSTM to pass the entire text from the very top.</p>
","transformer-model"
"78491317","Unexpected Attention dimension [nbr_layers, seq_length, hidden_layer_dim]","2024-05-16 16:25:07","","1","17","<pytorch><nlp><bert-language-model><transformer-model><attention-model>","<p>I'm working on extracting Attention from a modified Bert model that originally did NOT output any attention. When I extracted it by getting it from the BertEncoder (and through all the intermediate classes: ModelLayer, SelfUnpaddedAttention...), <strong>it seems to have  [nbr_layers, seq_length, hidden_layer_dim] as shape.</strong></p>
<p><strong>But if I understand well, I should somewhere have [seq_length, seq_length] matrix through which I can visualize the attention map.</strong></p>
<p>Here is the repository where I try to extract the attention : <a href=""https://huggingface.co/jaandoui/DNABERT2-AttentionExtracted/blob/main/bert_layers.py"" rel=""nofollow noreferrer"">HuggingFace Model</a>
And you can find the code where I extract the attention below.</p>
<p><strong>I'm not sure if I have done a mistake if how to extract the attention, or should I just change something to get that expected shape ?</strong></p>
<pre><code>class BertEncoder(nn.Module):
    &quot;&quot;&quot;A stack of BERT layers providing the backbone of Mosaic BERT.
    This module is modeled after the Hugging Face BERT's :class:`~transformers.model.bert.modeling_bert.BertEncoder`,
    but with substantial modifications to implement unpadding and ALiBi.
    Compared to the analogous Hugging Face BERT module, this module handles unpadding to reduce unnecessary computation
    at padded tokens, and pre-computes attention biases to implement ALiBi.
    &quot;&quot;&quot;

...
...
# OTHER CODE HERE (SEE SOURCE LINK)
...
...
        # PART WHERE I EXTRACT ATTENTION
        all_encoder_layers = []
        all_attention_weights = []  # List to store attention weights
    
        if subset_mask is None:
            for layer_module in self.layer:
                # Since we get now attention too, we need to unpack 2 elements instead of 1.
                hidden_states, attention_weights = layer_module(hidden_states,
                                                                cu_seqlens,
                                                                seqlen,
                                                                None,
                                                                indices,
                                                                attn_mask=attention_mask,
                                                                bias=alibi_attn_mask)
                
                all_attention_weights.append(attention_weights)  # Store attention weights
                if output_all_encoded_layers:
                    all_encoder_layers.append(hidden_states)
            # Pad inputs and mask. It will insert back zero-padded tokens.
            # Assume ntokens is total number of tokens (padded and non-padded)
            # and ntokens_unpad is total number of non-padded tokens.
            # Then padding performs the following de-compression:
            #     hidden_states[ntokens_unpad,hidden] -&gt; hidden_states[ntokens,hidden]
            hidden_states = pad_input(hidden_states, indices, batch, seqlen)
        else:
            for i in range(len(self.layer) - 1):
                layer_module = self.layer[i]
                # Since we get now attention too, we need to unpack 2 elements instead of 1.
                hidden_states, attention_weights = layer_module(hidden_states,
                                                                cu_seqlens,
                                                                seqlen,
                                                                None,
                                                                indices,
                                                                attn_mask=attention_mask,
                                                                bias=alibi_attn_mask)
                all_attention_weights.append(attention_weights)  # Store attention weights
                if output_all_encoded_layers:
                    all_encoder_layers.append(hidden_states)
            subset_idx = torch.nonzero(subset_mask[attention_mask_bool],
                                       as_tuple=False).flatten()
            # Since we get now attention too, we need to unpack 2 elements instead of 1.
            hidden_states, attention_weights = self.layer[-1](hidden_states,
                                                              cu_seqlens,
                                                              seqlen,
                                                              subset_idx=subset_idx,
                                                              indices=indices,
                                                              attn_mask=attention_mask,
                                                              bias=alibi_attn_mask)
            all_attention_weights.append(attention_weights)  # appending the attention of different layers together.
        if not output_all_encoded_layers:
            all_encoder_layers.append(hidden_states)

        # Since we now return both, we need to handle them wherever BertEncoder forward is called.
        return all_encoder_layers, all_attention_weights  # Return both hidden states and attention weights
        # return all_encoder_layers  # original return.
</code></pre>
","transformer-model"
"78490608","How can i make a transformer output a translation relative to a specific context","2024-05-16 14:25:29","","1","26","<machine-learning><nlp><transformer-model><machine-translation>","<p>I am working a machine-translation-like project, where i have a transformer with the encoder-decoder structure, which is supposed to generate SQL queries from natural language commands, example:</p>
<p><strong>Input:</strong> Count the number of aircrafts produced by the company XYZ</p>
<p><strong>Output:</strong> SELECT COUNT(*) FROM aircrafts WHERE manufacturer='XYZ';</p>
<p>Now i have kind of achieved my goal and my model can generate decent looking queries in most of cases, but there's still one problem to solve, which is that the queries it could generate are all SYNTACTICALLY correct but it doesn't nail the attributes/tables names, for example:</p>
<p><strong>Input:</strong> Count the number of aircrafts produced by the company XYZ</p>
<p><strong>Output:</strong> SELECT COUNT(*) FROM aircrafts WHERE produced='XYZ';</p>
<p>or:</p>
<p><strong>Input:</strong> Show all the employees older than 40</p>
<p><strong>Output:</strong> SELECT * FROM employee WHERE country &gt; 40;</p>
<p>So like i said, syntactically-speaking the query has no errors but it sure cannot be executed correctly on the database itself.</p>
<p>My model is trained on a json dataset that contains a list of samples who follows the following form:</p>
<pre><code>[
  [
    &quot;Count the number of aircraft produced by company XYZ&quot;,
    &quot;SELECT COUNT(*) FROM aircraft WHERE manufacturer = 'XYZ';&quot;
  ],
  [
    &quot;How many marine species are found in the Atlantic Ocean?&quot;,
    &quot;SELECT COUNT(*) FROM marine_species WHERE location = 'Atlantic Ocean';&quot;
  ]
]
</code></pre>
<p>Although i found some datasets that contains the schema of the database as a set of CREATE queries like this one:</p>
<pre><code>{&quot;instruction&quot;: &quot;CREATE TABLE table_72445 (
    \&quot;County\&quot; text,
    \&quot;Population\&quot; real,
    \&quot;Per capita income\&quot; text,
    \&quot;Median household income\&quot; text,
    \&quot;Median family income\&quot; text
)
-- Name the median family income for riverside&quot;,
&quot;output&quot;: &quot;SELECT \&quot;Median family income\&quot; FROM table_72445 WHERE \&quot;County\&quot; = 'Riverside'&quot;}
</code></pre>
<p>So is there anyway i can leverage this kind of dataset so i can give my transformer the database context it should work on, or is there any other way to achieve this task goal?</p>
<p>NOTE: i cannot make the previous corpus as a whole my input, because it will result in a huge performance-overhead. Imagine defining the very SAME set that contains tens of tables every single time that i want to execute, so this is no choice to take.</p>
","transformer-model"
"78487800","How to use <unk> default token in input sentence in Transformer (OpenNMT) model","2024-05-16 05:53:57","","0","16","<nlp><translate><transformer-model><opennmt>","<p>I'm using the OpenNMT library to develop a translation model.
I want to display certain tokens in their original language without translating them, or translate certain tokens to certain tokens only.
If you have any good ideas, please recommend them.</p>
<p>I've been trying to use custom tokens or  tokens to prevent them from being translated during translation, but I'm having trouble with this.</p>
<p>And modifying certain tokens before the attention operation has a negative impact on the attention operation, and I'm trying to figure out how to fix this.</p>
<br/>
What i want
<p>ex) I like apples for breakfast. -&gt; 나는 아침에 먹는 apples를 좋아합니다.</p>
<p>ex) I like <strong>unk</strong> for breackfast. -&gt; 나는 아침에 먹는 <strong>unk</strong>를 좋아합니다.</p>
<p>or</p>
<p>ex) I like <strong>apples</strong> for breakfast. -&gt; 나는 아침에 먹는 <strong>사과</strong>를 좋아합니다.</p>
<p>I want find that position of &quot;apples&quot;  = position of &quot;사과&quot;</p>
","transformer-model"
"78473578","Explainable ViT for the attention maps and gradient","2024-05-13 16:19:40","","0","80","<python><pytorch><transformer-model>","<p>I am using the code found in this <a href=""https://jacobgil.github.io/deeplearning/vision-transformer-explainability"" rel=""nofollow noreferrer"">tutorial</a> and the <a href=""https://github.com/jacobgil/vit-explain"" rel=""nofollow noreferrer"">GitHub</a> to visualize the inner mechanisms of the <code>ViT</code> model. z</p>
<pre><code>model = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True)
</code></pre>
<p>This tutorial make use of the <code>deit_tiny_patch16_224</code> and performs two different explainable methods (to extract attention maps) namely <code>rollout</code> and <code>grad_rollout</code>. In both methods a <code>forward and backward hook</code> is introduced in the <code>attn_drop</code> module (that is part of each block or encoder layer of the <code>ViT</code>) and then when the model is applied to an input image the attention maps for this layer and the gradient is returned . The input image size is <code>224x224x3</code> that is transformed to <code>14x14 = 196</code> patches.</p>
<p>When the model with the hook is applied to the input image then it returns a list of attention maps (each one has size of <code>torch.Size([1, 3, 197, 197])</code>) and a list of gradients (each one has size of <code>torch.Size([1, 3, 197, 197])</code>).</p>
<p>I am trying to do the same thing but for another model and more specifically for the <code>ViT_B_16_Weights</code> model:</p>
<pre><code>retrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, &quot;DEFAULT&quot; means best available
pretrained_vit = torchvision.models.vit_b_16(weights=retrained_vit_weights).to(device)
</code></pre>
<p>Note that both models have been developed using different implementations and the <code>embedding_dim</code> is different. It is <code>64</code> for the former and <code>768</code> for the latter. Moreover, the naming of the layers is different. However, should not be a problem and the code of the tutorial should work smoothly for this model as well. I am trying to use the <code>ViT_B_16_Weights</code>. In this case, I am trying to apply the hook in the <code>dropout layer</code> that is right after the <code>self_attention</code> layer.</p>
<p>However, the problem now is that the attention maps that I am receiving and the gradient does not have a proper size and they are as follows: <code>torch.Size([197, 1, 768])</code> for the gradient and <code>torch.Size([1, 197, 768])</code> for the attention maps. That does not make sense though to correspond to the code in the tutorial and the hoop is attached in the only layer that seems to be relevant. All the other options do not seem to work.</p>
<p>Here is the list with all my options for the <code>deit_tiny_patch16_224</code> model:</p>
<pre><code>blocks.11.norm1
blocks.11.attn
blocks.11.attn.qkv
blocks.11.attn.q_norm
blocks.11.attn.k_norm
blocks.11.attn.attn_drop # the hook is attached here
blocks.11.attn.proj
blocks.11.attn.proj_drop
blocks.11.ls1
blocks.11.drop_path1
blocks.11.mlp
blocks.11.mlp.fc2
blocks.11.drop_path2
</code></pre>
<p>while for the latter <code>ViT_B_16_Weights</code> model is</p>
<pre><code>encoder.layers.encoder_layer_10.ln_1
encoder.layers.encoder_layer_10.self_attention
Registering hook for  encoder.layers.encoder_layer_10.self_attention
encoder.layers.encoder_layer_10.self_attention.out_proj
Registering hook for  encoder.layers.encoder_layer_10.self_attention.out_proj
encoder.layers.encoder_layer_10.dropout # the hook is attached here
encoder.layers.encoder_layer_10.ln_2
encoder.layers.encoder_layer_10.mlp
encoder.layers.encoder_layer_10.mlp.0
encoder.layers.encoder_layer_10.mlp.1
encoder.layers.encoder_layer_10.mlp.2
encoder.layers.encoder_layer_10.mlp.3
encoder.layers.encoder_layer_10.mlp.4
</code></pre>
<p>What can it be wrong here? I was expecting to have a square tensor as in the case of the first model, is it something special with the implementation?</p>
","transformer-model"
"78466469","Is Normalized Simple Index a Viable Alternative for Positional Encoding in Constant Length Inputs?","2024-05-12 01:10:51","","0","37","<transformer-model><encoder>","<p>I have a question about using positional encoding in transformer models, particularly regarding the suitability of a normalized simple index in scenarios where the input length is consistently constant.</p>
<p>Typically, positional encoding in transformers involves sinusoidal functions, but this approach doesn't feel intuitive to me. Given that my task always deals with inputs of the same size to the transformer encoder, I'm considering the potential benefits of adopting a normalized simple index instead. This alternative seems simpler and could potentially streamline computations, given the fixed size of the input.</p>
<p>I'm curious about several aspects:</p>
<ol>
<li><p>Simplicity and Efficiency: Could the straightforward nature of a normalized simple index lead to more efficient computations and easier implementation without sacrificing performance?</p>
</li>
<li><p>Consistency in Encoding: With constant input length, each position would consistently correspond to the same normalized value. Might this consistent mapping enhance the model’s ability to learn position-dependent features effectively?</p>
</li>
<li><p>Reduced Overfitting: Could the non-cyclical nature of a normalized index help in reducing the model's tendency to overfit, especially compared to sinusoidal methods?</p>
</li>
<li><p>Direct Proportionality: Since the normalized index scales directly with position, could this provide a more linear and proportional representation of positional information which is beneficial for my specific task?</p>
</li>
</ol>
<p>I would appreciate any insights or experiences regarding the effectiveness of using a normalized simple index for positional encoding in such settings. Thank you!</p>
<p><a href=""https://i.sstatic.net/fJfZD56t.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","transformer-model"
"78464268","Failing to export the Swin Transformer model into onnx file","2024-05-11 10:44:26","","0","64","<python><pytorch><runtime-error><transformer-model><onnx>","<p>I want to export the onnx format of Swin Transformer model in python. However, in the export process, I got the following error:</p>
<pre><code>`Traceback (most recent call last):
  File &quot;/content/Rethinking_of_PAR/export_onnx.py&quot;, line 163, in &lt;module&gt;
    main(cfg, args)
  File &quot;/content/Rethinking_of_PAR/export_onnx.py&quot;, line 105, in main
    torch.onnx.export(model, x, &quot;swin_b.onnx&quot;, opset_version=12)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py&quot;, line 516, in export
    _export(
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py&quot;, line 1613, in _export
    graph, params_dict, torch_out = _model_to_graph(
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py&quot;, line 1139, in _model_to_graph
    graph = _optimize_graph(
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py&quot;, line 677, in _optimize_graph
    graph = _C._jit_pass_onnx(graph, operator_export_type)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py&quot;, line 1957, in _run_symbolic_function
    return symbolic_fn(graph_context, *inputs, **attrs)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py&quot;, line 7153, in onnx_placeholder
    return torch._C._jit_onnx_convert_pattern_from_subblock(block, node, env)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py&quot;, line 1957, in _run_symbolic_function
    return symbolic_fn(graph_context, *inputs, **attrs)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset11.py&quot;, line 311, in index_put
    values = symbolic_helper._reshape_helper(g, values, values_shape)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_helper.py&quot;, line 1418, in _reshape_helper
    return g.op(&quot;Reshape&quot;, input, shape)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py&quot;, line 87, in op
    return _add_op(self, opname, *raw_args, outputs=outputs, **kwargs)
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py&quot;, line 246, in _add_op
    node = _create_node(
  File &quot;/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/jit_utils.py&quot;, line 307, in _create_node
    _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)
RuntimeError: minus_one_pos != -1 INTERNAL ASSERT FAILED at &quot;../torch/csrc/jit/passes/onnx/shape_type_inference.cpp&quot;:534, please report a bug to PyTorch. There are no examples for shape_has_zero = true &amp;&amp; minus_one_pos == -1.`
</code></pre>
<p>Has anyone encountered such a problem?</p>
<p>The following line shows the export process of Swin Transformer model:</p>
<pre><code>torch.onnx.export(model, x, &quot;swin_b.onnx&quot;, opset_version=12)
</code></pre>
","transformer-model"
"78463110","How to inference transformer Tflite model Tensorflow?","2024-05-11 01:34:36","","0","89","<python-3.x><tensorflow><raspberry-pi><transformer-model><tflite>","<p>I want to deploy this model to Raspberry Pi later, but I don't know how to inference the translation using Tflite model. This model is based on this <a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">tensorflow tutorial</a>. Here the <a href=""https://drive.google.com/drive/folders/1yIJ8u9jk5AeI-C2hjLBe5M3AeUTZTgLN?usp=drive_link"" rel=""nofollow noreferrer"">dataset and tokenizer</a> that I used. Here the model <a href=""https://colab.research.google.com/drive/1awPy2x8DcA3yX0gWONo_dnXP2bInQOSa?usp=sharing"" rel=""nofollow noreferrer"">notebook</a>. Please guide me how I can inference this Tflite model? Is the inference process same as normal tf model?</p>
<pre><code>converter = tf.lite.TFLiteConverter.from_saved_model(&quot;translator_id-en&quot;)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS,  # enable TensorFlow ops.
]
# converter._experimental_lower_tensor_list_ops = False
converter.allow_custom_ops = True
tflite_model = converter.convert()
with open(&quot;lite.tflite&quot;, &quot;wb&quot;) as f:
    f.write(tflite_model)
</code></pre>
","transformer-model"
"78451494","ValueError: Exception encountered when calling PositionalEmbedding.call()","2024-05-08 23:00:38","","0","161","<python><types><embedding><transformer-model>","<p>I am running Neural machine translation with a Transformer and Keras. <a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<p>pt is an tokenized vectors of size (64, 79). For the following class, PositionalEmbedding.call() throws an error.</p>
<pre><code>class PositionalEmbedding(tf.keras.layers.Layer):
  def __init__(self, vocab_size, d_model):
    super().__init__()
    self.d_model = d_model
    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) 
    self.pos_encoding = positional_encoding(length=2048, depth=d_model)

  def compute_mask(self, *args, **kwargs):
    return self.embedding.compute_mask(*args, **kwargs)

  def call(self, x):
    length = tf.shape(x)[1]
    x = self.embedding(x)
    # This factor sets the relative scale of the embedding and positonal_encoding.
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x = x + self.pos_encoding[tf.newaxis, :length, :]
    return x
</code></pre>
<p>This call is throwing an error.</p>
<pre><code>    embed_pt = PositionalEmbedding(vocab_size=tokenizers.pt.get_vocab_size(), d_model=512)
    pt_emb = embed_pt(pt)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-167-44987cede320&gt; in &lt;cell line: 4&gt;()
      2 embed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)
      3 
----&gt; 4 pt_emb = embed_pt(pt)
      5 en_emb = embed_en(en)

1 frames
&lt;ipython-input-166-e9ab4e283481&gt; in call(self, x)
     11   def call(self, x):
     12     length = tf.shape(x)[1]
---&gt; 13     x = self.embedding(x)
     14     # This factor sets the relative scale of the embedding and positonal_encoding.
     15     x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))

ValueError: Exception encountered when calling PositionalEmbedding.call().

Invalid dtype: &lt;property object at 0x7a6808eb53f0&gt;

Arguments received by PositionalEmbedding.call():
  • x=tf.Tensor(shape=(64, 70), dtype=int64)
</code></pre>
<p>I tried to change the datatype to int32 and it didn't work.</p>
","transformer-model"
"78447813","Why does performance differ due to differences in model architecture?","2024-05-08 10:28:45","","0","8","<pytorch><transformer-model><multimodal>","<h1>Case 1</h1>
<pre><code>import resnet

class ViT(nn.Module):
   def __init__(self, ...):
      self.backbone = resnet()
      ...
   def forward(self, x):
      x = self.backbone(x)
      x = ...
      return x
   ...
</code></pre>
<h1>Case 2</h1>
<pre><code>clas Multi_model(nn.Module):
   def __init__(self, ...):
      self.ViT = ViT()
      self.backbone = resnet()

   def forward(self, x):
      x = self.resnet(x)
      x = self.ViT(x)
      return x
</code></pre>
<p>Why do these two performance differences occur? In Case 1, we declare a CNN model inside the ViT class and add it to the Forward function. In Case 2, we declare ViT and CNN models separately inside a class called Multi_model and add them to Forward individually. My experimental results show that Case 1 has a 1-MAE performance of 0.91, while Case 2 has a 1-MAE performance of over 0.95. What's the difference, and is Case 2 incorrect?</p>
<p>Why do these two performance differences occur? In Case 1, we declare a CNN model inside the ViT class and add it to the Forward function. In Case 2, we declare ViT and CNN models separately inside a class called Multi_model and add them to Forward individually. My experimental results show that Case 1 has a 1-MAE performance of 0.91, while Case 2 has a 1-MAE performance of over 0.95. What's the difference, and is Case 2 incorrect?</p>
","transformer-model"
"78437711","Transfer learning from a english-to.spanish model to a english-to-equation model in pytorch","2024-05-06 15:41:26","","0","18","<python><pytorch><transformer-model><transfer-learning>","<p>im training a model lets say from english to equation model (cannot say the real translation model but is similar) for example: &quot;A plus B hould be 0&quot;, the output of the model should be: A+B=0. Im using transfer learning from a MarianDB which code was created with chat-gpt3, but i do not know why the output is not near to be the expected, this is my training code:</p>
<pre><code>import pandas as pd
import torch
from transformers import MarianTokenizer, MarianMTModel, MarianConfig
from torch.utils.data import DataLoader, Dataset
from transformers import AdamW, get_linear_schedule_with_warmup, AutoTokenizer
from tqdm import tqdm

# Define a custom dataset class for your translation data
class CustomTranslationDataset(Dataset):
    def __init__(self, source_texts, target_texts, tokenizer, max_length):
        self.source_texts = source_texts
        self.target_texts = target_texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.source_texts)

    def __getitem__(self, idx):
        source_text = self.source_texts[idx]
        target_text = self.target_texts[idx]

        encoding = self.tokenizer(
            source_text,
            target_text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding=&quot;max_length&quot;,
            return_tensors=&quot;pt&quot;,
            truncation=True,
        )

        return {
            &quot;input_ids&quot;: encoding[&quot;input_ids&quot;].squeeze(),
            &quot;attention_mask&quot;: encoding[&quot;attention_mask&quot;].squeeze(),
            &quot;labels&quot;: encoding[&quot;input_ids&quot;].squeeze(),
        }
# Define the paths to your CSV files
train_csv_path = 'train_dataset.csv'  # Replace with the path to your training data CSV
validation_csv_path = 'val_dataset.csv'  # Replace with the path to your validation data CSV

# Define the column names containing the source and target texts
source_column_name = 'english_statement'  # Replace with the name of the source text column
target_column_name = 'eq_representation'  # Replace with the name of the target text column

# Initialize the MarianMT model and tokenizer for transfer learning
pretrained_model_name = &quot;Helsinki-NLP/opus-mt-en-es&quot;  # Replace with a pre-trained model for a related language pair
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)
config = MarianConfig.from_pretrained(pretrained_model_name)
model = MarianMTModel.from_pretrained(pretrained_model_name, config=config)

# Prepare your training dataset and dataloader
train_data = pd.read_csv(train_csv_path)
train_source_texts = train_data[source_column_name].tolist()
train_target_texts = train_data[target_column_name].tolist()
max_seq_length = 128  # Maximum sequence length for training data

train_dataset = CustomTranslationDataset(train_source_texts, train_target_texts, tokenizer, max_seq_length)
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Initialize the optimizer and learning rate scheduler
num_epochs = 500  # Adjust as needed
optimizer = AdamW(model.parameters(), lr=2e-5)
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(train_dataloader) * num_epochs)

# Training loop
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0

    for batch in tqdm(train_dataloader, desc=f&quot;Epoch {epoch + 1}/{num_epochs}&quot;, leave=False):
        input_ids = batch[&quot;input_ids&quot;].to(device)
        attention_mask = batch[&quot;attention_mask&quot;].to(device)
        labels = batch[&quot;labels&quot;].to(device)

        optimizer.zero_grad()

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
        )

        loss = outputs.loss
        loss.backward()
        optimizer.step()
        scheduler.step()

        total_loss += loss.item()

    average_loss = total_loss / len(train_dataloader)
    print(f&quot;Epoch {epoch + 1}/{num_epochs} - Average Loss: {average_loss:.4f}&quot;)

# Save the fine-tuned model
model.save_pretrained(&quot;fine_tuned_marianmt_model&quot;)

# Save the tokenizer
tokenizer.save_pretrained(&quot;fine_tuned_marianmt_model/tokenizer&quot;)
</code></pre>
<p>And this is my evaluation code:</p>
<pre><code>from transformers import MarianTokenizer, MarianMTModel, AutoTokenizer
# Load the saved tokenizer using AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;fine_tuned_marianmt_model/tokenizer&quot;)

# Initialize the MarianMT model for inference
model_name = &quot;fine_tuned_marianmt_model&quot;  # Replace with the path to your fine-tuned model
model = MarianMTModel.from_pretrained(model_name)
# Load the saved tokenizer

# Define the source text to be translated
source_text = &quot;when signal1 rise, signal2 should fell&quot;

# Tokenize the source text
inputs = tokenizer(source_text, return_tensors=&quot;pt&quot;)
decoded_tokens = tokenizer.decode(inputs[&quot;input_ids&quot;][0], skip_special_tokens=True)

# Split the decoded tokens into individual tokens
tokens = decoded_tokens.split()

# Print out the individual tokens
print(&quot;Individual Tokens:&quot;)
for token in tokens:
    print(token)
# Translate the source text to the target language
translated_ids = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)
translated_text = tokenizer.decode(translated_ids[0], skip_special_tokens=True)

# Print the translated text
print(&quot;Translated Text:&quot;, translated_text)
</code></pre>
<p>the output im getting from the evaluation code looks like this:</p>
<pre><code>Individual Tokens:
▁when▁signal1▁rise,▁signal2▁should▁fell
Translated Text: cuando▁signal1▁rise,▁signal2▁should▁fell
</code></pre>
<p>(i know the example given is not related to equations but it is related to the real application, the expecteds output should contain symbols like an equation)</p>
<p>im not sure why is the output kind of being translated to spanish, it looks that the transfer learning is not being applied to my training data, i have no idea what canbe the cause of this and neither chat-gpt has an idea.</p>
<p>My training data consist on 20 examples which i augmented by using synonyms and changing the way the sentence is written and modifying the value of numbers on each example. So at the end i have more than 10K training data.</p>
<p>i tried using different models like BERT transformer but results were not good, the translated text consisted on random symbols unrelated to my training data.</p>
<p>The loss seems to be near 0 during training.
<a href=""https://i.sstatic.net/0bZQejoC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0bZQejoC.png"" alt=""loss during training"" /></a></p>
","transformer-model"
"78425865","What are the inputs of the first decoder in the transformer architecture","2024-05-03 16:24:53","78431653","0","228","<transformer-model><encoder><decoder>","<p>In the transformer architecture from the original paper, I referred many texts but I couldn't solve this insight.</p>
<p>Lets start with input sentence, &quot;The cat jumped.&quot;</p>
<p>My understanding is, each word is parallely processed by each encoder. That is, taking the word cat for example, its embedding is produced, then its positionally encoded to produce another vector. Then, based on the attention computations, a final vector is produced. This is then passed to the feedforward part of the encoder. This process is done in parallel for the all other words of the input sentence. This is repeated until the 6th encoder. Hence, at the last encoder, we have three outputs for each of the words of the sentence. If this is the case, what does the first decoder receive as input? All the three outputs one after the other, in parallel, concatenated or merged? I don't think concatenation is used since the decoder has fixed input size.</p>
","transformer-model"
"78417714","ValidationError: 1 validation error for RetrievalQA retriever","2024-05-02 08:05:03","","2","198","<python><langchain><large-language-model><transformer-model><ctransformers>","<p>I am using pinecone vector database for storing the veactors of the text chunks, then I am imlementing similarity search to get similar document after that I am loading my llm model and using the retrieveralQA chain I am trying to use &quot;from langchain.chains import RetrievalQA &quot; for my chatbot but I am always getting a validation error in RetrieveralQA.from_chain_type.</p>
<pre><code>        #Initializing the Pinecone
    
        from langchain_pinecone import PineconeVectorStore
        from pinecone import Pinecone
        from langchain.chains import RetrievalQA 
    
        pc = Pinecone(api_key=&quot;1e094edb-8730-46a7-8178-615a08ca303b&quot;)
        index = pc.Index(&quot;medical-chatbot&quot;)
    
        # index_name=&quot;medical-chatbot&quot;
    
        #Creating Embeddings for Each of The Text Chunks &amp; storing
        docsearch=PineconeVectorStore.from_texts([t.page_content for t in text_chunks], embeddings, index_name='medical-chatbot')
    
        #If we already have an index we can load it like this
        index_name=&quot;medical-chatbot&quot;
        docsearch=PineconeVectorStore.from_existing_index(index_name, embeddings)
    
        query = &quot;What are Salivary Gland Disease&quot;
    
        docs=docsearch.similarity_search(query, k=3)
    
        print(&quot;Result&quot;, docs)
    
        prompt_template=&quot;&quot;&quot;
        Use the following pieces of information to answer the user's question.
        If you don't know the answer, just say that you don't know, don't try to make up an answer.
    
        Context: {context}
        Question: {question}
    
        Only return the helpful answer below and nothing else.
        Helpful answer:
        &quot;&quot;&quot;
    
        PROMPT=PromptTemplate(template=prompt_template, input_variables=[&quot;context&quot;, &quot;question&quot;])
        chain_type_kwargs={&quot;prompt&quot;: PROMPT}
    
        llm=CTransformers(model=&quot;../model/llama-2-7b-chat.ggmlv3.q4_0.bin&quot;,
                      model_type=&quot;llama&quot;,
                      config={'max_new_tokens':512,
                              'temperature':0.8})
    
    
        # retriever = docsearch.as_retriever()
    
    
        qa=RetrievalQA.from_chain_type(
            llm=llm, 
        chain_type=&quot;stuff&quot;, 
        retriever=docsearch.as_retriever(search_kwargs={'k': 2}),
        return_source_documents=True, 
            chain_type_kwargs=chain_type_kwargs)
</code></pre>
<p>this is the error Message</p>
<pre><code>    ValidationError Traceback (most recent call last)
    Cell In[49], line 5
    1 from langchain.chains import RetrievalQA
    2 # retriever = docsearch.as_retriever()
    ----&gt; 5 qa=RetrievalQA.from_chain_type(
    6 llm=llm,
    7 chain_type=&quot;stuff&quot;,
    8 retriever=docsearch.as_retriever(search_kwargs={'k': 2}),
    9 return_source_documents=True,
    10 chain_type_kwargs=chain_type_kwargs)
    
    File c:\End-to-end-Medical-Chatbot-using-Llama2-main\myenv\lib\site-packages\langchain\chains\retrieval_qa\base.py:95, in BaseRetrievalQA.from_chain_type(cls, llm, chain_type, chain_type_kwargs, **kwargs)
    91 _chain_type_kwargs = chain_type_kwargs or {}
    92 combine_documents_chain = load_qa_chain(
    93 llm, chain_type=chain_type, **_chain_type_kwargs
    94 )
    ---&gt; 95 return cls(combine_documents_chain=combine_documents_chain, **kwargs)
    
    File c:\End-to-end-Medical-Chatbot-using-Llama2-main\myenv\lib\site-packages\langchain\load\serializable.py:74, in Serializable.init(self, **kwargs)
    73 def init(self, **kwargs: Any) -&gt; None:
    ---&gt; 74 super().init(**kwargs)
    75 self._lc_kwargs = kwargs
    
    File c:\End-to-end-Medical-Chatbot-using-Llama2-main\myenv\lib\site-packages\pydantic\main.py:341, in pydantic.main.BaseModel.init()
    
    ValidationError: 1 validation error for RetrievalQA
    retriever
    Can't instantiate abstract class BaseRetriever with abstract methods _aget_relevant_documents, _get_relevant_documents (type=type_error)
</code></pre>
","transformer-model"
"78415968","Can't understand how text generator works when passing a shifted vector to the model during training","2024-05-01 21:01:52","","0","29","<python><tokenize><transformer-model><gpt-2>","<p>I'm really struggling understanding how a text generator can output the next word (token) given a text sequence. Since I'm trying to understand how transformers in such cases work, I'd be very happy, if you can help me to understand the &quot;logic&quot; behind it.</p>
<p>All the code posted here is taken from the jupyter notebook which comes with the book: <a href=""https://github.com/bpbpublications/Building-Transformer-Models-with-PyTorch-2.0"" rel=""nofollow noreferrer"">Building Transformer Models with PyTorch 2.0</a>. Going to their code in their repository <a href=""https://github.com/bpbpublications/Building-Transformer-Models-with-PyTorch-2.0/blob/main/chapter3_TransformerInPytorch/Implementing_Transformer_with_PyTorch.ipynb"" rel=""nofollow noreferrer"">here</a>, there is a model declaration:</p>
<pre><code>vocab_size = tokenizer.vocab_size
embedding_dim = 512
nhead = 8
num_layers = 6
num_classes = 2

# Create the model
model = TextClassifier(vocab_size, embedding_dim, nhead, num_layers,  num_classes).to(device)
criterion = nn.BCELoss().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
</code></pre>
<p>and the main loop definition:</p>
<pre><code>num_epochs = 1
for epoch in range(num_epochs):
    i=0
    for batch_data, batch_attention_mask, batch_labels in train_dataloader:
      
        optimizer.zero_grad()

        # Convert attention_mask to boolean tensor
        batch_attention_mask = (batch_attention_mask==0).to(device)

        outputs = model(batch_data.to(device), key_padding_mask=batch_attention_mask)
        loss = criterion(outputs, batch_labels.to(device))
        if i%100==0:
          print (&quot;epoch &quot;, epoch, &quot;batch &quot;, i, &quot;loss &quot;, loss)

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()
        i=i+1

    print(f&quot;Epoch: {epoch + 1}, Loss: {loss.item()}&quot;)
</code></pre>
<p>My problem is to understand how can the <em>input_ids</em>, the <em>attention_mask</em> and the <em>labels</em> interact with themselves for training the model.
Let's take a single batch of data from the <em>train_dataloader</em> (I took 1 batch to simplify as much as I could):</p>
<pre><code>input_ids = [  101, 10047,  2125,  1012,  2070,  2028,  6045,  2232,  2275,  2017, 2006, 102, 0, 0, 0, 0, 0, 0,     0,     0,  0,     0,     0,     0,     0,     0,     0,     0,     0,     0,    0,     0]
attention_mask = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
label = [10047,  2125,  1012,  2070,  2028,  6045,  2232,  2275,  2017,  2006,  102,     0,     0,     0,     0,     0,     0,     0,     0,     0,   0,     0,     0,     0,     0,     0,     0,     0,     0,     0,   0,     0]
</code></pre>
<p>The <code>input_ids</code> is the original sentence, which has been tokenized and padded to reach a predefined length: Ok.
The <em>attention_mask</em> is the masking passed to the model during training. Now the first question here:</p>
<ul>
<li>Why has the masking vector all 1s where a token is different then 0s? What is the purpose of this form of masking? I would expect masking the last word in the sentence (we want to train a text generator), so the last word should be masked. Or am I wrong here?
Then, we have the <em>label</em>. So the second question here:</li>
<li>The label vector is basically the same input_ids vector but shifted to the left. So let's take an output model which predicts the token after the token 6045 (index 6). In the original sentence (input_ids) the &quot;right&quot; token is 2232 (index 7). But in the label vector the token at index 7 is 2275. So there should be a mismatch wenn calculating the loss. And this mismatch has its origin in the shifting of the label vector and not for other reasons.
So it should converge at all.</li>
</ul>
<p>It is clear, that I'm overlooking something very fundamental and important. I would thank you 10000, if you can point me in the right direction. Tkx</p>
","transformer-model"
"78414511","How to retrieve query/key/values/output parameters of BERT pretrained?","2024-05-01 15:15:45","","0","46","<python><tensorflow><bert-language-model><transformer-model><self-attention>","<pre><code>model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)
layer = 0
attention_block = model.encoder.layer[layer].attention.self
</code></pre>
<p>I want to retrieve the parameters (queries, keys, values, attention_output) of <code>attention_block</code>, for each head separately.</p>
<p>For head 0 for example, I have tried</p>
<pre><code>queries = []
for name, mod in model.named_modules(): 
   if name==f'encoder.layer.{layer}.attention.self.query':
       queries.append(next(mod.parameters()))
h = 0  # head number
dim_per_head = 64
Q_0 = queries[0][h * dim_per_head : (h + 1) * dim_per_head, :]  # query matrix of head 0
</code></pre>
<p>and same for keys, values and output weights, but I am not sure this is the right way of dividing the tensor <code>queries[0]</code> in slices. Does anyone have more information about parameter implementation in bert_pretrained?</p>
<p>Thank you!</p>
","transformer-model"
"78407508","Issue with Deserializing a Custom Transformer Model in TensorFlow","2024-04-30 09:18:13","","0","255","<python><tensorflow><keras><deserialization><transformer-model>","<p>I'm encountering an issue when trying to deserialize a custom Transformer model in TensorFlow.And therefore I cannot save and load my model like this : <code> model_directory =&quot;saved_model.keras&quot; transformer_model.save(model_directory) load_transformer_model = load_model(&quot;saved_model.keras&quot;, custom_objects=custom_objects)</code>
The error message I'm receiving is:</p>
<pre><code>TypeError: &lt;class '__main__.Transformer'&gt; could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.

config={'module': None, 'class_name': 'Transformer', 'config': {'trainable': True, 'dtype': 'float32'}, 'registered_name': 'Custom&gt;Transformer', 'build_config': {'input_shape': [1, 10]}}.

Exception encountered: Unable to revive model from config. When overriding the `get_config()` method, make sure that the returned config contains all items used as arguments in the constructor to &lt;class '__main__.Transformer'&gt;, which is the default behavior. You can override this default behavior by defining a `from_config(cls, config)` class method to specify how to create an instance of Transformer from its config.

Received config={'trainable': True, 'dtype': 'float32'}

Error encountered during deserialization: Transformer.__init__() got an unexpected keyword argument 'trainable'

</code></pre>
<p>Here's a code snippet of my Transformer Class (I have the following classes: Transformer, Encoder(tf.keras.layers.Layer), Decoder(tf.keras.layers.Layer), EncoderLayer(tf.keras.layers.Layer), DecoderLayer(tf.keras.layers.Layer)):</p>
<pre><code>class Transformer(tf.keras.Model):
    &quot;&quot;&quot;
    The Transformer model architecture, consisting of an Encoder and Decoder.
    &quot;&quot;&quot;

    def __init__(
        self,
        num_layers,
        d_model,
        num_heads,
        d_feedforward,
        input_vocab_size,
        target_vocab_size,
        max_num_positions_in_pe_encoder,
        max_num_positions_in_pe_decoder,
        dropout_rate=0.1
        #**kwargs,
     ):
        &quot;&quot;&quot;
        Parameters:
            num_layers (int): Number of layers in both Encoder and Decoder.
            d_model (int): Dimension of the model.
            num_heads (int): Number of attention heads.
            d_feedforward (int): Dimension of the feed forward network.
            input_vocab_size (int): Size of the input vocabulary.
            target_vocab_size (int): Size of the target vocabulary.
            max_num_positions_in_pe_encoder (int): The maximum positions for input.
            max_num_positions_in_pe_decoder (int): The maximum positions for
                target.
            dropout_rate (float): Dropout dropout_rate.
        &quot;&quot;&quot;
        super(Transformer, self).__init__()
        self.encoder = Encoder(
            num_layers=num_layers,
            d_model=d_model,
            num_heads=num_heads,
            d_feedforward=d_feedforward,
            input_vocab_size=input_vocab_size,
            maximum_positions_in_pe=max_num_positions_in_pe_encoder,
            dropout_rate=dropout_rate,
        )
        self.decoder = Decoder(
            num_layers=num_layers,
            d_model=d_model,
            num_heads=num_heads,
            d_feedforward=d_feedforward,
            target_vocab_size=target_vocab_size,
            maximum_positions_in_pe=max_num_positions_in_pe_decoder,
            dropout_rate=dropout_rate,
        )
        self.final_layer = Dense(target_vocab_size)
</code></pre>
<p>Configuration:</p>
<pre><code>TensorFlow version: 2.16.1
Python version: 3.10.12
</code></pre>
<p>What I've Tried:</p>
<pre><code>I've tried to implement/override a get_config(), from_config(), build_from_config() method for my Transformer class, then for each of my classes. but I get this kind of mistakes
I've attempted to use the custom_objects when loading.
</code></pre>
<p>Here's an example of one the get_config() method I tried to implement for my Transformer class:</p>
<pre><code># def get_config(self):
    #     config = {
    #         'num_layers': self.num_layers,
    #         'd_model': self.d_model,
    #         'num_heads': self.num_heads,
    #         'd_feedforward': self.d_feedforward,
    #         'input_vocab_size': self.input_vocab_size,
    #         'target_vocab_size': self.target_vocab_size,
    #         'max_num_positions_in_pe_encoder': self.max_num_positions_in_pe_encoder,
    #         'max_num_positions_in_pe_decoder': self.max_num_positions_in_pe_decoder,
    #         'dropout_rate': self.dropout_rate,
    #     }
    #     return config
</code></pre>
<p>But I still get this</p>
<pre><code>KeyError: 'num_layers'
</code></pre>
<p>Expected Outcome:
I would like to be able to save and load my model properly model without encountering any errors. Any guidance or suggestions on how to properly handle the save and load/deserialization process would be greatly appreciated.</p>
","transformer-model"
"78406828","how to interpret that for each batch the same class is predicted","2024-04-30 07:12:44","","0","24","<python><machine-learning><deep-learning><neural-network><transformer-model>","<p>I am trying to implement an action recognition model, using a Transformer. Basically, it is based on a spatial transformer  which takes joints as input and temporal one which takes the output of the first model (cls token). For training each batch is almost predicted the same, for example:</p>
<pre><code>predictions:  tensor([30,  2, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30])
predictions:  tensor([58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58])
predictions:  tensor([36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36])
predictions:  tensor([53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53])
predictions:  tensor([58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58])
predictions:  tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30])
predictions:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
predictions:  tensor([41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41])
predictions:  tensor([20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20])
predictions:  tensor([17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17])
predictions:  tensor([34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34])
predictions:  tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13])
predictions:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])
predictions:  tensor([43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43])
predictions:  tensor([24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24])
predictions:  tensor([18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18])
predictions:  tensor([35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35])
predictions:  tensor([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15])
predictions:  tensor([45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45])
predictions:  tensor([38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38])
predictions:  tensor([46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46])
predictions:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11])
predictions:  tensor([27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27])
predictions:  tensor([41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41])
predictions:  tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])
predictions:  tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50])
</code></pre>
<p>Is it a comman result espacially for first epochs or something is going wrong (which i think it is the case). I share also the main class</p>
<pre><code>class ActRecogTransformer(nn.Module):
    def __init__(self, device='cpu', mocap_frames=100, num_joints=50, in_chans=3, spatial_embed=64, sdepth=4, tdepth=4,
                 num_heads=8, mlp_ratio=2., qkv_bias=True, qk_scale=None, op_type='cls', embed_type='conv',
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.2, norm_layer=None, num_classes=60):
        super().__init__()

        #Spatial patch and pos embeddings
        if embed_type=='lin':
            self.Spatial_patch_to_embedding = nn.Linear(in_chans, spatial_embed)#Linear patch embedding
        else:
            self.Spatial_patch_to_embedding = nn.Conv1d(in_chans, spatial_embed, 1, 1)#Conv patch embedding


        self.Spatial_pos_embed = nn.Parameter(torch.zeros(1, num_joints+1, spatial_embed))
        self.spat_token = nn.Parameter(torch.zeros(1,1,spatial_embed))
        self.proj_up_clstoken = nn.Linear(mocap_frames*spatial_embed, num_joints*spatial_embed)
        self.sdepth = sdepth
        self.num_joints = num_joints

        #Temporal embedding
        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
        temp_embed = spatial_embed * (num_joints)
        temp_frames = mocap_frames
        acc_embed = temp_embed
        self.op_type = op_type
        self.embed_type = embed_type
        self.Temporal_pos_embed = nn.Parameter(torch.zeros(1, temp_frames+1, temp_embed)) #additional pos embedding zero for class token
        self.temp_frames = mocap_frames
        self.joint_coords = in_chans



        sdpr = [x.item() for x in torch.linspace(0, drop_path_rate, sdepth)]  #Stochastic depth decay rule
        tdpr = [x.item() for x in torch.linspace(0, drop_path_rate, tdepth)]  #Stochastic depth decay rule
      

        self.Spatial_blocks = nn.ModuleList([
            Block(
                dim=spatial_embed, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=sdpr[i], norm_layer=norm_layer)
            for i in range(sdepth)])

        self.Temporal_blocks = nn.ModuleList([
            Block(
                dim=temp_embed, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=tdpr[i], norm_layer=norm_layer)
            for i in range(tdepth)])



        self.Spatial_norm = norm_layer(spatial_embed)
        self.Temporal_norm = norm_layer(temp_embed)

        self.pos_drop = nn.Dropout(p=drop_rate)
                #Classification head
        self.class_head = nn.Sequential(
            nn.LayerNorm(temp_embed),
            nn.Linear(temp_embed, num_classes)
        )



    def forward(self, x, dims = 3):
        #x = self.pes(x)

        b, f, p, c = x.shape  # b is batch size, f is number of frames, p is number of joints, c is in_chan 3


        x = rearrange(x, 'b f p c  -&gt; (b f) p c', )  # 4, 100, 50, 3 =&gt; 400,50,3
        if self.embed_type == 'conv':
            x = rearrange(x, '(b f) p c  -&gt; (b f) c p',b=b ) # b x 3 x Fa  - Conv k liye channels first,, 400,3,50
            x = self.Spatial_patch_to_embedding(x) # B x c x p -&gt;  B x Se x p
            x = rearrange(x, '(b f) Se p  -&gt; (b f) p Se', b=b)
        else:
            x = self.Spatial_patch_to_embedding(x) # B x p x c -&gt;  B x p x Se

        # (1,1,spatial_embed which is 32) for spat_token size and the results is (400,1,32)
        class_token=torch.tile(self.spat_token,(b*f,1,1)) # (B,1,1)
        x = torch.cat((x,class_token),dim=1) # b x (p+1) x Se
        x += self.Spatial_pos_embed
        #x = self.pes(x)
        x = self.pos_drop(x)


        # Spatial transformer
        #x = self.spatial_encoder(x)
        for blk in self.Spatial_blocks:
            x = blk(x)
        x = self.Spatial_norm(x)

        Se = x.shape[-1]
        #cls_token = x[:,0]
        cls_token = x[:,-1,:]

        cls_token = torch.reshape(cls_token, (b,f*Se)) #4, 100*32
        x = x[:,:p,:]
        x = rearrange(x, '(b f) p Se-&gt; b f (p Se)', f=f)


        # Temporal transformer
        b,f,St = x.shape
     #   x = self.pet(x)


        b  = x.shape[0]
        temp_cls_token = self.proj_up_clstoken(cls_token) #in: B x mocap_frames * Se -&gt; op: B x num_joints*Se
        temp_cls_token = torch.unsqueeze(temp_cls_token,dim=1) #op: B x 1 x num_joints*Se

        x = torch.cat((x,temp_cls_token), dim=1) #B x mocap_frames +1 x temp_embed | temp_embed = num_joints*Se

        x += self.Temporal_pos_embed
       # x = self.pet(x)
        x = self.pos_drop(x)
       # x = self.temporal_encoder(x)
      #  print(x.shape)
        for blk in self.Temporal_blocks:
            x = blk(x)

        x = self.Temporal_norm(x)

        ###Extract Class token head from the output
        if self.op_type=='cls':
            #cls_token = x[:,0]
            cls_token = x[:,-1,:]
            cls_token = cls_token.view(b, -1) # (Batch_size, temp_embed)
            x = self.class_head(cls_token)


        else:
            x = x[:,:f,:]
            x = rearrange(x, 'b f St -&gt; b St f')
            x = F.avg_pool1d(x,x.shape[-1],stride=x.shape[-1]) #b x St x 1
            x = torch.reshape(x, (b,St))
        # Classification head...

        return (x)


</code></pre>
<p>For training:</p>
<pre><code>    for epoch in range(num_epochs):
        correct=0
        total=0
        running_loss = 0.0
        epoch_loss = 0.0
        L = []
#        for i in range(steps):
        for i, data_ in enumerate(train_loader, 0):
            # Get the input sequences and their corresponding targets
            inputs, targets = data_
            #x, y = X_train[i*batch_size:(i*batch_size)+batch_size], y_train[i*batch_size:(i*batch_size)+batch_size]
            #inputs, targets = torch.from_numpy(inputs).to(device), torch.from_numpy(targets).to(device)
            inputs, targets = (inputs).to(device),(targets).to(device)
            optimizer.zero_grad()
            predictions = model(inputs.float())
            targets = targets.long()
            print(&quot;predictions: &quot;,torch.max(predictions, 1) )

            loss = criterion(predictions, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            epoch_loss += loss.item()
            #print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss ))
            running_loss = 0.0
            y_pred_softmax = torch.log_softmax(predictions.data, dim = 1)
            _, predicted = torch.max(y_pred_softmax, 1)
            total += targets.size(0)
            correct += (predicted ==targets).sum().item()
            # print(&quot;predicted&quot;,torch.max(y_pred_softmax, 1))
            # print(&quot;targets&quot;, targets.long())
        accuracy = 100 * correct / total
        epoch_loss = epoch_loss / steps
        print(&quot;Training Accuracy = {} : Training Loss {}&quot;.format(accuracy,epoch_loss))
</code></pre>
<p>I tried also to use TransformerEncoder instead of Block clas, but i am still getting the same. Is it a common behavior, for maybe the first 3, 4 epochs, still getting very bad accuracy no more than 2%.</p>
<p>Thanks</p>
","transformer-model"
"78406062","Transformer-based Yorùbá to English language processor : requirements issues","2024-04-30 03:08:13","","0","16","<nlp><artificial-intelligence><translation><transformer-model><non-english>","<p>Following the successful implementation of the first transduction model that leveraged attention mechanism, I want to implement similar model applying it to a Nigerian language e.g. Yorùbá. However, I do not have a corpus that can help actualize this experiment. Is there a way of getting it considering the idea in Vaswani et al (2017) in their article 'Attention Is All You Need'? It is expected that the model achieves high Bilingual Evaluation Understudy metric. Help is needed here regarding the corpus as mentioned.</p>
<p>#Yorùbá-to-English translation task</p>
<p>Attempts had been made to browse the internet for it but all to no avail. Equally, I tried to check the WMT 2014 site, all to  no avail.</p>
","transformer-model"
"78399335","fix error "" With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.""","2024-04-28 19:04:20","","0","78","<python><nlp><runtime-error><bert-language-model><transformer-model>","<p>I wrote a program in python and I want to implement NLP by BERT algorithm.I have a dataset and the below code but when I ran the program at colab I encounter below error</p>
<pre><code>    import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
from sklearn.utils import shuffle
from bs4 import BeautifulSoup
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt
import hazm
from hazm import Lemmatizer
from cleantext import clean
import nltk
import plotly.graph_objects as go
from tqdm.notebook import tqdm
import os
import re
import json
import copy
import collections

from transformers import BertConfig, BertTokenizer
from transformers import TFBertModel, TFBertForSequenceClassification
from transformers import glue_convert_examples_to_features

import tensorflow as tf
MAX_LEN = 128
TRAIN_BATCH_SIZE = 16
VALID_BATCH_SIZE = 16
TEST_BATCH_SIZE = 16

EPOCHS = 3
EEVERY_EPOCH = 1000
LEARNING_RATE = 2e-5
CLIP = 0.0

MODEL_NAME_OR_PATH = 'HooshvareLab/bert-fa-base-uncased'
OUTPUT_PATH = '/content/bert-fa-base-uncased-sentiment-taaghceh/pytorch_model.bin'

os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
from google.colab import files
uploaded = files.upload()
data = pd.read_csv('Education.csv',header=None)
data.drop(0,inplace=True,axis=1)
data
data[1].replace('ترس','ترس ',inplace=True)
data[1].unique()
with open('stopwords.txt') as stopwords_file:
   stopwords = stopwords_file.readlines()
nltk_stopwords = [str(line).replace('\n', '') for line in stopwords]
data['comment_len_by_words'] = data[1].apply(lambda t: len(hazm.word_tokenize(t)))
data
min_max_len = data[&quot;comment_len_by_words&quot;].min(), data[&quot;comment_len_by_words&quot;].max()
print(f'Min: {min_max_len[0]} \tMax: {min_max_len[1]}')
def data_gl_than(data, less_than=100.0, greater_than=0.0, col='comment_len_by_words'):
    data_length = data[col].values

    data_glt = sum([1 for length in data_length if greater_than &lt; length &lt;= less_than])

    data_glt_rate = (data_glt / len(data_length)) * 100

    print(f'Texts with word length of greater than {greater_than} and less than {less_than} includes {data_glt_rate:.2f}% of the whole!')
data_gl_than(data, 256, 3)

# remove comments with the length of fewer than three words
minlim, maxlim = 3, 256

data['comment_len_by_words'] = data['comment_len_by_words'].apply(lambda len_t: len_t if minlim &lt; len_t &lt;= maxlim else None)
data = data.dropna(subset=['comment_len_by_words'])
data = data.reset_index(drop=True)

def cleanhtml(raw_html):
    cleanr = re.compile('&lt;.*?&gt;')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext

def cleaning(text):
    text = text.strip()
    
    # regular cleaning
    text = clean(text,fix_unicode=True,to_ascii=False,lower=True,no_line_breaks=True,no_urls=True,no_emails=True,no_phone_numbers=True,
        no_numbers=False,no_digits=False,no_currency_symbols=True,no_punct=False,replace_with_url=&quot;&quot;,
        replace_with_email=&quot;&quot;,
        replace_with_phone_number=&quot;&quot;,
        replace_with_number=&quot;&quot;,
        replace_with_digit=&quot;0&quot;,
        replace_with_currency_symbol=&quot;&quot;,
    )

    # cleaning htmls
    text = cleanhtml(text)
    
    # normalizing
    normalizer = hazm.Normalizer()
    text = normalizer.normalize(text)
    # removing wierd patterns
    wierd_pattern = re.compile(&quot;[&quot;
        u&quot;\U0001F600-\U0001F64F&quot;  # emoticons
        u&quot;\U0001F300-\U0001F5FF&quot;  # symbols &amp; pictographs
        u&quot;\U0001F680-\U0001F6FF&quot;  # transport &amp; map symbols
        u&quot;\U0001F1E0-\U0001F1FF&quot;  # flags (iOS)
        u&quot;\U00002702-\U000027B0&quot;
        u&quot;\U000024C2-\U0001F251&quot;
        u&quot;\U0001f926-\U0001f937&quot;
        u'\U00010000-\U0010ffff'
        u&quot;\u200d&quot;
        u&quot;\u2640-\u2642&quot;
        u&quot;\u2600-\u2B55&quot;
        u&quot;\u23cf&quot;
        u&quot;\u23e9&quot;
        u&quot;\u231a&quot;
        u&quot;\u3030&quot;
        u&quot;\ufe0f&quot;
        u&quot;\u2069&quot;
        u&quot;\u2066&quot;
        # u&quot;\u200c&quot;
        u&quot;\u2068&quot;
        u&quot;\u2067&quot;
        &quot;]+&quot;, flags=re.UNICODE)
    
    text = wierd_pattern.sub(r'', text)
    
    # removing extra spaces, hashtags
    text = re.sub(&quot;#&quot;, &quot;&quot;, text)
    text = re.sub(&quot;\s+&quot;, &quot; &quot;, text)
    
    return text
    data
data['cleaned_comment'] = data[1].apply(cleaning)

# # calculate the length of comments based on their words
data['cleaned_comment_len_by_words'] = data['cleaned_comment'].apply(lambda t: len(hazm.word_tokenize(t)))

# # # remove comments with the length of fewer than three words
data['cleaned_comment_len_by_words'] = data['cleaned_comment_len_by_words'].apply(lambda len_t: len_t if minlim &lt; len_t &lt;= maxlim else len_t)
data = data.dropna(subset=['cleaned_comment_len_by_words'])
data = data.reset_index(drop=True)
data.head()
data['tweet_without_stopwords'] = data['cleaned_comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))
data = data[[1,'cleaned_comment']]
data.columns = ['label','comment']
data.head()

fig = go.Figure()

groupby_label = data.groupby('label')['label'].count()

fig.add_trace(go.Bar(
    x=list(sorted(groupby_label.index)),
    y=groupby_label.tolist(),
    text=groupby_label.tolist(),
    textposition='auto'
))

fig.update_layout(
    title_text='Distribution of label within comments [DATA]',
    xaxis_title_text='Label',
    yaxis_title_text='Frequency',
    bargap=0.2,
    bargroupgap=0.2)

fig.show()


labels = list(sorted(data['label'].unique()))
label2id = {label: i for i, label in enumerate(labels)}
id2label = {v: k for k, v in label2id.items()}
print(f'label2id: {label2id}')
print(f'id2label: {id2label}')

data['label_id'] = data['label'].apply(lambda t: labels.index(t))
train, test = train_test_split(data, test_size=0.1, random_state=1, stratify=data['label'])
train, valid = train_test_split(train, test_size=0.1, random_state=1, stratify=train['label'])

train = train.reset_index(drop=True)
valid = valid.reset_index(drop=True)
test = test.reset_index(drop=True)

x_train, y_train = train['comment'].values.tolist(), train['label_id'].values.tolist()
x_valid, y_valid = valid['comment'].values.tolist(), valid['label_id'].values.tolist()
x_test, y_test = test['comment'].values.tolist(), test['label_id'].values.tolist()

print(train.shape)
print(valid.shape)
print(test.shape)

tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_OR_PATH)
config = BertConfig.from_pretrained(
    MODEL_NAME_OR_PATH, **{
        'label2id': label2id,
        'id2label': id2label,
    })
print(config.to_json_string())
data
idx = np.random.randint(0, len(train))
sample_comment = train.iloc[idx]['comment']
sample_label = train.iloc[idx]['label']

print(f'Sample: \n{sample_comment}\n{sample_label}')

tokens = tokenizer.tokenize(sample_comment)
token_ids = tokenizer.convert_tokens_to_ids(tokens)

print(f'  Comment: {sample_comment}')
print(f'Token IDs: {token_ids}')
encoding = tokenizer.encode_plus(
    sample_comment,
    max_length=32,
    truncation=True,
    add_special_tokens=True, # Add '[CLS]' and '[SEP]'
    return_token_type_ids=True,
    return_attention_mask=True,
    padding='max_length',
    return_tensors='pt',  # Return tensors
)

print(f'Keys: {encoding.keys()}\n')
for k in encoding.keys():
    print(f'{k}:\n{encoding[k]}')

class InputExample:
    &quot;&quot;&quot; A single example for simple sequence classification. &quot;&quot;&quot;

    def __init__(self, guid, text_a, text_b=None, label=None):
        &quot;&quot;&quot; Constructs a InputExample. &quot;&quot;&quot;
        self.guid = guid
        self.text_a = text_a
        self.text_b = text_b
        self.label = label


def make_examples(tokenizer, x, y=None, maxlen=128, output_mode=&quot;classification&quot;, is_tf_dataset=True):
    examples = []
    y = y if isinstance(y, list) or isinstance(y, np.ndarray) else [None] * len(x)

    for i, (_x, _y) in tqdm(enumerate(zip(x, y)), position=0, total=len(x)):
        guid = &quot;%s&quot; % i
        label = int(_y)
        
        if isinstance(_x, str):
            text_a = _x
            text_b = None
        else:
            assert len(_x) == 2
            text_a = _x[0]
            text_b = _x[1]
        
        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))
    
    features = glue_convert_examples_to_features(
        examples, 
        tokenizer, 
        maxlen, 
        output_mode=output_mode, 
        label_list=list(np.unique(y)))

    all_input_ids = []
    all_attention_masks = []
    all_token_type_ids = []
    all_labels = []

    for f in tqdm(features, position=0, total=len(examples)):
        if is_tf_dataset:
            all_input_ids.append(tf.constant(f.input_ids))
            all_attention_masks.append(tf.constant(f.attention_mask))
            all_token_type_ids.append(tf.constant(f.token_type_ids))
            all_labels.append(tf.constant(f.label))
        else:
            all_input_ids.append(f.input_ids)
            all_attention_masks.append(f.attention_mask)
            all_token_type_ids.append(f.token_type_ids)
            all_labels.append(f.label)

    if is_tf_dataset:
        dataset = tf.data.Dataset.from_tensor_slices(({
            'input_ids': all_input_ids,
            'attention_mask': all_attention_masks,
            'token_type_ids': all_token_type_ids
        }, all_labels))

        return dataset, features
    
    xdata = [np.array(all_input_ids), np.array(all_attention_masks), np.array(all_token_type_ids)]
    ydata = all_labels

    return [xdata, ydata], features
    train_dataset_base, train_examples = make_examples(tokenizer, x_train, y_train, maxlen=128)
valid_dataset_base, valid_examples = make_examples(tokenizer, x_valid, y_valid, maxlen=128)

test_dataset_base, test_examples = make_examples(tokenizer, x_test, y_test, maxlen=128)
[xtest, ytest], test_examples = make_examples(tokenizer, x_test, y_test, maxlen=128, is_tf_dataset=False)
for value in train_dataset_base.take(1):
    print(f'     input_ids: {value[0][&quot;input_ids&quot;]}')
    print(f'        target: {value[1]}')
    def get_training_dataset(dataset, batch_size):
       dataset = dataset.repeat()
       dataset = dataset.shuffle(2048)
       dataset = dataset.batch(batch_size)

    return dataset

def get_validation_dataset(dataset, batch_size):
    dataset = dataset.batch(batch_size)

    return dataset

train_dataset = get_training_dataset(train_dataset_base, TRAIN_BATCH_SIZE)
valid_dataset = get_training_dataset(valid_dataset_base, VALID_BATCH_SIZE)

train_steps = len(train_examples) // TRAIN_BATCH_SIZE
valid_steps = len(valid_examples) // VALID_BATCH_SIZE

train_steps, valid_steps

def build_model(model_name, config, learning_rate=3e-5):
    model = TFBertForSequenceClassification.from_pretrained(model_name, config=config)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

    return model
    r = model.fit(
    train_dataset,
    validation_data=valid_dataset,
    steps_per_epoch=train_steps,
    validation_steps=valid_steps,
    epochs=EPOCHS,
    verbose=1)

final_accuracy = r.history['val_accuracy']
print('FINAL ACCURACY MEAN: ', np.mean(final_accuracy))
model.save_pretrained(os.path.dirname(OUTPUT_PATH))
ev = model.evaluate(test_dataset_base.batch(TEST_BATCH_SIZE))
print()
print(f'Evaluation: {ev}')
print()

predictions = model.predict(xtest)
ypred = predictions[0].argmax(axis=-1).tolist()

print()
print(classification_report(ytest, ypred, target_names=labels))
print()

print(f'F1: {f1_score(ytest, ypred, average=&quot;weighted&quot;)}')
</code></pre>
<blockquote>
<pre><code>ValueError                                Traceback (most recent call last)

&lt;ipython-input-13-1b0a029a88ab&gt; in &lt;cell line: 175&gt;()
    173 
    174 data['label_id'] = data['label'].apply(lambda t: labels.index(t))
--&gt; 175 train, test = train_test_split(data, test_size=0.1, random_state=1, stratify=data['label'])
    176 train, valid = train_test_split(train, test_size=0.1, random_state=1, stratify=train['label'])
    177 

1 frames

/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py
</code></pre>
<p>in _validate_shuffle_split(n_samples, test_size, train_size,
default_test_size)
2234
2235     if n_train == 0:
-&gt; 2236         raise ValueError(
2237             &quot;With n_samples={}, test_size={} and train_size={}, the &quot;
2238             &quot;resulting train set will be empty. Adjust any of the &quot;</p>
<pre><code>ValueError: With n_samples=0, test_size=0.1 and train_size=None, the resulting train set will be empty. Adjust any of the
</code></pre>
<p>aforementioned parameters.</p>
</blockquote>
<p>please help me to solve a error</p>
<p>dataset:</p>
<blockquote>
<p>614484565059596288,&quot;Dorian Gray with Rainbow Scarf&quot;,happy</p>
</blockquote>
","transformer-model"
"78394695","Understanding Feature Capture in Vision Transformers compared to CNN-based Networks like ResNets","2024-04-27 10:39:10","","0","34","<transformer-model><vision-transformer>","<p>I'm delving into the realm of computer vision and I'm particularly interested in understanding how Vision Transformers (ViTs) create features compared to Convolutional Neural Networks (CNNs), such as ResNet architectures.</p>
<p>I've been reading about ViTs and how they operate on sequences of image patches, using self-attention mechanisms to capture global dependencies. However, I'm still unclear about how do ViTs capture low-level and high-level features in images? How is this different from the feature extraction process in CNNs like ResNets?</p>
<p>I'd appreciate any insights, references to relevant papers or articles, or personal experiences that shed light on these comparisons. Understanding these nuances will greatly aid my decision-making process when choosing a model architecture for my computer vision projects.</p>
<p>There are so many resources online that it is hard to discriminate, aside from the original paper which I already studied, so a nudge in the right direction would be appreciated.</p>
","transformer-model"
"78376537","Positional encoding for VIsion transformer","2024-04-24 07:07:20","78426918","0","128","<pytorch><transformer-model><vision-transformer>","<p>why the positional encoding is (1,patch,emb) size, it should be (batch_size,patch,emb) in general
even in the pytorch github code <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"" rel=""nofollow noreferrer"">https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py</a>  they are defining<br />
self.pos_embedding = nn.Parameter(torch.empty(<em>1</em>, seq_length, hidden_dim).normal_(std=0.02))  # from BERT</p>
<p>can anyone help me, what should I use as pos_encoding in my code</p>
<p>self.pos_embedding = nn.Parameter(torch.empty(<em>batch_size</em>, seq_length, hidden_dim).normal_(std=0.02))</p>
<p>is it correct?</p>
","transformer-model"
"78371741","No Attention returned even when output_attentions= True","2024-04-23 10:48:50","78492414","0","88","<nlp><huggingface-transformers><bert-language-model><transformer-model><attention-model>","<p>I'm using a pretrained model based BERT (github link:<a href=""https://github.com/MAGICS-LAB/DNABERT_2"" rel=""nofollow noreferrer"">DNABERT-2</a>)</p>
<p>It uses AutoModelForSequenceClassification and mosaicml/mosaic-bert-base.</p>
<p>I'm having the problem that I cannot extract the attention. I have read many posts which show ways of dealing with that by activating output_attentions=True in the model, but none of the posts solved the problem.
<code>output</code> is of length 2 and each element is of shape: <code>torch.Size([1, 7, 768])</code> and
<code>torch.Size([1, 768])</code>. When trying to get <code>output.attentions</code> I get <code>None</code>.</p>
<p>I'm not sure where to search and what a solution would be.</p>
<p>I'm providing my whole code:</p>
<p>Defining model, trainer, data, tokenizer:</p>
<pre><code>from copy import deepcopy

from sklearn.metrics import precision_recall_fscore_support import wandb from transformers import TrainerCallback
# END NEW import os import csv import json import logging from dataclasses import dataclass, field from typing import Optional, Dict, Sequence, Tuple, List

import torch import transformers import sklearn import numpy as np from torch.utils.data import Dataset

@dataclass class ModelArguments:
    model_name_or_path: Optional[str] = field(default=&quot;facebook/opt-125m&quot;)
    use_lora: bool = field(default=False, metadata={&quot;help&quot;: &quot;whether to use LoRA&quot;})
    lora_r: int = field(default=8, metadata={&quot;help&quot;: &quot;hidden dimension for LoRA&quot;})
    lora_alpha: int = field(default=32, metadata={&quot;help&quot;: &quot;alpha for LoRA&quot;})
    lora_dropout: float = field(default=0.05, metadata={&quot;help&quot;: &quot;dropout rate for LoRA&quot;})
    lora_target_modules: str = field(default=&quot;query,value&quot;, metadata={&quot;help&quot;: &quot;where to perform LoRA&quot;})


@dataclass class DataArguments:
    data_path: str = field(default=None, metadata={&quot;help&quot;: &quot;Path to the training data.&quot;})
    kmer: int = field(default=-1, metadata={&quot;help&quot;: &quot;k-mer for input sequence. -1 means not using k-mer.&quot;})


@dataclass class TrainingArguments(transformers.TrainingArguments):
    cache_dir: Optional[str] = field(default=None)
    run_name: str = field(default=&quot;run&quot;)
    optim: str = field(default=&quot;adamw_torch&quot;)
    model_max_length: int = field(default=512, metadata={&quot;help&quot;: &quot;Maximum sequence length.&quot;})
    gradient_accumulation_steps: int = field(default=1)
    per_device_train_batch_size: int = field(default=1)
    per_device_eval_batch_size: int = field(default=1)
    num_train_epochs: int = field(default=1)
    logging_steps: int = field(default=100)
    save_steps: int = field(default=100)
    fp16: bool = field(default=False)
    # START NEW
    # eval_steps: int = field(default=100)
    eval_steps: int = field(default=0.1)
    # END NEW
    evaluation_strategy: str = field(default=&quot;steps&quot;)
    warmup_steps: int = field(default=50)
    weight_decay: float = field(default=0.01)
    learning_rate: float = field(default=1e-4)
    save_total_limit: int = field(default=3)
    load_best_model_at_end: bool = field(default=True)
    output_dir: str = field(default=&quot;output&quot;)
    find_unused_parameters: bool = field(default=False)
    checkpointing: bool = field(default=False)
    dataloader_pin_memory: bool = field(default=False)
    eval_and_save_results: bool = field(default=True)
    save_model: bool = field(default=False)
    seed: int = field(default=42)


def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    &quot;&quot;&quot;Collects the state dict and dump to disk.&quot;&quot;&quot;
    state_dict = trainer.model.state_dict()
    if trainer.args.should_save:
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        del state_dict
        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa


&quot;&quot;&quot; Get the reversed complement of the original DNA sequence. &quot;&quot;&quot;


def get_alter_of_dna_sequence(sequence: str):
    MAP = {&quot;A&quot;: &quot;T&quot;, &quot;T&quot;: &quot;A&quot;, &quot;C&quot;: &quot;G&quot;, &quot;G&quot;: &quot;C&quot;}
    # return &quot;&quot;.join([MAP[c] for c in reversed(sequence)])
    return &quot;&quot;.join([MAP[c] for c in sequence])


&quot;&quot;&quot; Transform a dna sequence to k-mer string &quot;&quot;&quot;


def generate_kmer_str(sequence: str, k: int) -&gt; str:
    &quot;&quot;&quot;Generate k-mer string from DNA sequence.&quot;&quot;&quot;
    return &quot; &quot;.join([sequence[i:i + k] for i in range(len(sequence) - k + 1)])


&quot;&quot;&quot; Load or generate k-mer string for each DNA sequence. The generated k-mer string will be saved  to the same directory as the original data with the same name but with a suffix of &quot;_{k}mer&quot;. &quot;&quot;&quot;


def load_or_generate_kmer(data_path: str, texts: List[str], k: int) -&gt; List[str]:
    &quot;&quot;&quot;Load or generate k-mer string for each DNA sequence.&quot;&quot;&quot;
    kmer_path = data_path.tokenizerreplace(&quot;.csv&quot;, f&quot;_{k}mer.json&quot;)
    if os.path.exists(kmer_path):
        logging.warning(f&quot;Loading k-mer from {kmer_path}...&quot;)
        with open(kmer_path, &quot;r&quot;) as f:
            kmer = json.load(f)
    else:
        logging.warning(f&quot;Generating k-mer...&quot;)
        kmer = [generate_kmer_str(text, k) for text in texts]
        with open(kmer_path, &quot;w&quot;) as f:
            logging.warning(f&quot;Saving k-mer to {kmer_path}...&quot;)
            json.dump(kmer, f)

    return kmer


class SupervisedDataset(Dataset):
    &quot;&quot;&quot;Dataset for supervised fine-tuning.&quot;&quot;&quot;

    def __init__(self,
                 data_path: str,
                 tokenizer: transformers.PreTrainedTokenizer,
                 kmer: int = -1):

        super(SupervisedDataset, self).__init__()

        # load data from the disk
        with open(data_path, &quot;r&quot;) as f:
            data = list(csv.reader(f))[1:]
        if len(data[0]) == 2:
            # data is in the format of [text, label]
            logging.warning(&quot;Perform single sequence classification...&quot;)
            texts = [d[0] for d in data]
            labels = [int(d[1]) for d in data]
        # All genes sequences are concat: we don't work with the sequence-pair,
        # But we are tricking the model to think it is single sequence.
        elif len(data[0]) == 3:
            # data is in the format of [text1, text2, label]
            logging.warning(&quot;Perform sequence-pair classification...&quot;)
            texts = [[d[0], d[1]] for d in data]
            labels = [int(d[2]) for d in data]
        else:
            raise ValueError(&quot;Data format not supported.&quot;)

        if kmer != -1:
            # only write file on the first process
            if torch.distributed.get_rank() not in [0, -1]:
                torch.distributed.barrier()

            logging.warning(f&quot;Using {kmer}-mer as input...&quot;)
            texts = load_or_generate_kmer(data_path, texts, kmer)

            if torch.distributed.get_rank() == 0:
                torch.distributed.barrier()

        output = tokenizer(
            texts,
            return_tensors=&quot;pt&quot;,
            padding=&quot;longest&quot;,
            max_length=tokenizer.model_max_length,
            truncation=True,
        )

        self.input_ids = output[&quot;input_ids&quot;]
        # CHANGE
        self.input_ids[0][self.input_ids[0] == 0] = 2
        # Change to which tokens we want to attend and to which we don't
        self.attention_mask = output[&quot;attention_mask&quot;]
        self.labels = labels
        self.num_labels = len(set(labels))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, i) -&gt; Dict[str, torch.Tensor]:
        return dict(input_ids=self.input_ids[i], labels=self.labels[i])


@dataclass class DataCollatorForSupervisedDataset(object):
    &quot;&quot;&quot;Collate examples for supervised fine-tuning.&quot;&quot;&quot;

    tokenizer: transformers.PreTrainedTokenizer

    def __call__(self, instances: Sequence[Dict]) -&gt; Dict[str, torch.Tensor]:
        input_ids, labels = tuple([instance[key] for instance in instances] for key in (&quot;input_ids&quot;, &quot;labels&quot;))
        input_ids = torch.nn.utils.rnn.pad_sequence(
            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.Tensor(labels).long()
        return dict(
            input_ids=input_ids,
            labels=labels,
            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),
        )


&quot;&quot;&quot; Manually calculate the accuracy, f1, matthews_correlation, precision, recall with sklearn. &quot;&quot;&quot;

def calculate_metric_with_sklearn(logits: np.ndarray, labels: np.ndarray):
    if logits.ndim == 3:
        # Reshape logits to 2D if needed
        logits = logits.reshape(-1, logits.shape[-1])
    predictions = np.argmax(logits, axis=-1)
    valid_mask = labels != -100  # Exclude padding tokens (assuming -100 is the padding token ID)
    valid_predictions = predictions[valid_mask]
    valid_labels = labels[valid_mask]
    return {
        # START NEW
        &quot;sum prediction&quot;: f'{sum(valid_predictions)}/{len(valid_predictions)}',
        # END NEW
        &quot;accuracy&quot;: sklearn.metrics.accuracy_score(valid_labels, valid_predictions),
        &quot;f1&quot;: sklearn.metrics.f1_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
        &quot;matthews_correlation&quot;: sklearn.metrics.matthews_corrcoef(
            valid_labels, valid_predictions
        ),
        &quot;precision&quot;: sklearn.metrics.precision_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
        &quot;recall&quot;: sklearn.metrics.recall_score(
            valid_labels, valid_predictions, average=&quot;macro&quot;, zero_division=0
        ),
    }

&quot;&quot;&quot; Compute metrics used for huggingface trainer. &quot;&quot;&quot; def compute_metrics(eval_pred):
    logits, labels = eval_pred
    if isinstance(logits, tuple):  # Unpack logits if it's a tuple
        logits = logits[0]
    return calculate_metric_with_sklearn(logits, labels)


class CustomTrainer(transformers.Trainer):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.epoch_predictions = []
        self.epoch_labels = []
        self.epoch_loss = []

    def compute_loss(self, model, inputs, return_outputs=False):
        &quot;&quot;&quot;
        MAX: Subclassed to compute training accuracy.

        How the loss is computed by Trainer. By default, all models return the loss in
        the first element.

        Subclass and override for custom behavior.
        &quot;&quot;&quot;
        if self.label_smoother is not None and &quot;labels&quot; in inputs:
            labels = inputs.pop(&quot;labels&quot;)
        else:
            labels = None
        outputs = model(**inputs, output_attentions=True)
        # TEST
        try:
            print(f&quot;Attention: {outputs.attentions}&quot;)
        except Exception:
            print(&quot;No Attention returned&quot;)

        if &quot;labels&quot; in inputs:
            preds = outputs.logits.detach()

            # Log accuracy
            acc = (
                (preds.argmax(axis=1) == inputs[&quot;labels&quot;])
                .type(torch.float)
                .mean()
                .item()
            )
            # Uncomment it if you want to plot the batch accuracy
            # wandb.log({&quot;batch_accuracy&quot;: acc})  # Log accuracy

            # Store predictions and labels for epoch-level metrics
            self.epoch_predictions.append(preds.cpu().numpy())
            self.epoch_labels.append(inputs[&quot;labels&quot;].cpu().numpy())

        # Save past state if it exists
        if self.args.past_index &gt;= 0:
            self._past = outputs[self.args.past_index]

        if labels is not None:
            loss = self.label_smoother(outputs, labels)
        else:
            loss = outputs[&quot;loss&quot;] if isinstance(outputs, dict) else outputs[0]
            # Uncomment it if you want to plot the batch loss
            # wandb.log({&quot;batch_loss&quot;: loss})
            self.epoch_loss.append(loss.item())  # Store loss for epoch-level metrics

        return (loss, outputs) if return_outputs else loss

# Define a custom callback to calculate metrics at the end of each epoch class CustomCallback(TrainerCallback):

    def __init__(self, trainer) -&gt; None:
        super().__init__()
        self._trainer = trainer

    def on_epoch_end(self, args, state, control, **kwargs):
        # Aggregate predictions and labels for the entire epoch
        epoch_predictions = np.concatenate(self._trainer.epoch_predictions)
        epoch_labels = np.concatenate(self._trainer.epoch_labels)

        # Compute accuracy
        accuracy = np.mean(epoch_predictions.argmax(axis=1) == epoch_labels)

        # Compute mean loss
        mean_loss = np.mean(self._trainer.epoch_loss)

        # Compute precision, recall, and F1-score
        precision, recall, f1, _ = precision_recall_fscore_support(
            epoch_labels, epoch_predictions.argmax(axis=1), average=&quot;weighted&quot;
        )

        # Log epoch-level metrics
        wandb.log({&quot;epoch_accuracy&quot;: accuracy, &quot;epoch_loss&quot;: mean_loss})
        wandb.log({&quot;precision&quot;: precision, &quot;recall&quot;: recall, &quot;f1&quot;: f1})

        # Clear stored predictions, labels, and loss for the next epoch
        self._trainer.epoch_predictions = []
        self._trainer.epoch_labels = []
        self._trainer.epoch_loss = []
        return None

        # TODO: use this function to gather the prediction and labels and get the metrics
#%%
</code></pre>
<p>Instantiating and training:</p>
<pre><code>from transformer_model import SupervisedDataset, DataCollatorForSupervisedDataset, ModelArguments, \
    TrainingArguments, DataArguments, safe_save_model_for_hf_trainer, CustomTrainer, CustomCallback, \
    compute_metrics


from copy import deepcopy
from transformers import TrainerCallback
# END NEW
import os
import json
import torch
import transformers

from peft import (
    LoraConfig,
    get_peft_model,
    get_peft_model_state_dict,
)

import wandb
run = wandb.init()
assert run is wandb.run

def train(device):
    parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))
    model_args, data_args, training_args = parser.parse_args_into_dataclasses()

    # load tokenizer
    tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        model_max_length=training_args.model_max_length,
        padding_side=&quot;right&quot;,
        use_fast=True,
        trust_remote_code=True,
    )

    if &quot;InstaDeepAI&quot; in model_args.model_name_or_path:
        tokenizer.eos_token = tokenizer.pad_token

    # define datasets and data collator
    train_dataset = SupervisedDataset(tokenizer=tokenizer,
                                      data_path=os.path.join(data_args.data_path, &quot;train.csv&quot;),
                                      kmer=data_args.kmer)
    val_dataset = SupervisedDataset(tokenizer=tokenizer,
                                    data_path=os.path.join(data_args.data_path, &quot;dev.csv&quot;),
                                    kmer=data_args.kmer)
    test_dataset = SupervisedDataset(tokenizer=tokenizer,
                                     data_path=os.path.join(data_args.data_path, &quot;test.csv&quot;),
                                     kmer=data_args.kmer)
    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)

    # load model
    model = transformers.AutoModelForSequenceClassification.from_pretrained(
        model_args.model_name_or_path,
        cache_dir=training_args.cache_dir,
        num_labels=train_dataset.num_labels,
        trust_remote_code=True,
        output_attentions = True
    ).to(device)

    # configure LoRA
    if model_args.use_lora:
        lora_config = LoraConfig(
            r=model_args.lora_r,
            lora_alpha=model_args.lora_alpha,
            target_modules=list(model_args.lora_target_modules.split(&quot;,&quot;)),
            lora_dropout=model_args.lora_dropout,
            bias=&quot;none&quot;,
            task_type=&quot;SEQ_CLS&quot;,
            inference_mode=False,
        )
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()

    trainer = CustomTrainer(model=model,
                            tokenizer=tokenizer,
                            args=training_args,
                            compute_metrics=compute_metrics,
                            train_dataset=train_dataset,
                            eval_dataset=val_dataset,
                            data_collator=data_collator
                            )

    trainer.add_callback(CustomCallback(trainer))
    trainer.train()

    # train_result = trainer.train()
    # loss = train_result[&quot;loss&quot;]
    # print(f&quot;loss issss: {loss}&quot;)
    # print(f&quot;Train reusults: {train_result}&quot;) # NEW: result: only returns metrics at the end of training

    if training_args.save_model:
        trainer.save_state()
        safe_save_model_for_hf_trainer(trainer=trainer, output_dir=training_args.output_dir)

    # get the evaluation results from trainer
    if training_args.eval_and_save_results:
        results_path = os.path.join(training_args.output_dir, &quot;results&quot;, training_args.run_name)
        results = trainer.evaluate(eval_dataset=test_dataset)
        os.makedirs(results_path, exist_ok=True)
        with open(os.path.join(results_path, &quot;eval_results.json&quot;), &quot;w&quot;) as f:
            json.dump(results, f)


if __name__ == &quot;__main__&quot;:
    # Define device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print('Using device:', device)
    # Call the train function with the device
    train(device)
</code></pre>
<p>After training, I try to run it on an example:</p>
<pre><code>model_path = './finetune/output/dnabert2'
tokenizer = AutoTokenizer.from_pretrained(model_path)
# Load the model with output_attention=True
model = AutoModel.from_pretrained(model_path, trust_remote_code=True, output_attentions=True)
model_input = tokenizer(&quot;ACTGACGGGTAGTGACTG&quot;, return_tensors=&quot;pt&quot;)

with torch.inference_mode():
  output = model(**model_input, output_attentions=True)
</code></pre>
<p>My code might have some tests and prints. Let me know if anything is missing. Thank you very much for the help.</p>
","transformer-model"
"78357420","Image Captioning Transforner","2024-04-20 07:51:54","","0","30","<image><transformer-model><caption>","<p>I get the following error message when calling the training.TypeError: PhiForCausalLM.forward() got an unexpected keyword argument 'encoder_hidden_states' I am using following code;</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/phi-2&quot;, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
AutoModelForCausalLM.from_pretrained(&quot;microsoft/phi-2&quot;, trust_remote_code=True)

feature_extractor = ViTFeatureExtractor.from_pretrained(&quot;hustvl/yolos-small&quot;, cache_dir=cache_dir)

model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(&quot;hustvl/yolos-small&quot;,
    decoder_pretrained_model_name_or_path=&quot;microsoft/phi-2&quot;,   # Phi2 
    tie_encoder_decoder=True,  # Tie encoder and decoder weights for joint training
    cache_dir=/   cache_dir&quot;+&quot;models&quot;) 
</code></pre>
<p>I tried to fix the problem with model.config.encoder_hidden_layers= False, but it didn't work.</p>
","transformer-model"
"78348481","Using transformer package in PySpark gives module not found error","2024-04-18 14:59:48","","0","49","<pyspark><huggingface-transformers><transformer-model><huggingface><johnsnowlabs-spark-nlp>","<p>I am using a pretrained huggingface model inside a SparkNLP pipeline:</p>
<pre><code>class T5_EP(Wikify):
    &quot;&quot;&quot;This pipeline generates elevator pitch for a given paper abstract using T5 model&quot;&quot;&quot;
    def __init__(self, feature_col, index_col, result_col):
        super().__init__(feature_col, index_col, result_col)
        self.model_name = const.DEFAULT_T5_EP_MODEL
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
        
        self.task = &quot;summarize:&quot;
        self.ep = &quot;elevator pitch&quot;
        self.feature_col = feature_col
        self.index_col = index_col
        self.result_col = result_col
        self.max_output = const.ELEVATOR_PITCH_MAX_OUTPUT


    def gen_elevator_pitch(self, spark_sess, data):
        &quot;&quot;&quot;Function to generate an elevator pitch for a given paper abstract&quot;&quot;&quot;
        def summarizer(text):
            inputs = self.tokenizer.encode(&quot;summarize: &quot; + text, return_tensors=&quot;pt&quot;, truncation=True)
            outputs = self.model.generate(inputs, max_length=const.ELEVATOR_PITCH_MAX_OUTPUT, num_beams=4, early_stopping=True)
            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        summarize_udf = udf(summarizer, StringType())
        result = data.withColumn(self.result_col, summarize_udf(data[self.feature_col]))

        result = self.post_process(result)
        results_df.show(truncate=False)
        return result
    
    def post_process(self, result):
        # Implement any post-processing steps if needed
        return result
</code></pre>
<p>This whole code is inside the file x.py; however, when I run the pipeline, it keeps throwing &quot;no module named x&quot; error. I suspect its a problem of PySpark trying to serialize transformer functions, is there anyway to solve it?</p>
<p>My attempt is based on: <a href=""https://towardsdatascience.com/large-models-meet-big-data-spark-and-llms-in-harmony-5e2976b69b62"" rel=""nofollow noreferrer"">https://towardsdatascience.com/large-models-meet-big-data-spark-and-llms-in-harmony-5e2976b69b62</a></p>
","transformer-model"
"78346343","Embeddings for Vision Transformers","2024-04-18 09:30:09","","0","36","<embedding><transformer-model><vision-transformer>","<p>In Natural Language, we know that the embeddings are trained vector representations of words. How does this translate to the embeddings of image patches in vision transformers? How are the embeddings curated?</p>
<p>Also, in language the positional embeddings are computed with the following formula:<a href=""https://i.sstatic.net/914KV.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>Are the positional embeddings a learnable entity in case of vision? How's this better than the fixed embeddings in case of the natural language?</p>
<p>I tried reading the ViT paper but it wasn't really clear on these topics</p>
","transformer-model"
"78345158","How do I complete the preprocessing of NGSIM dataset?","2024-04-18 06:03:28","","0","51","<python><excel><prediction><transformer-model>","<p>In the proposed work, it is required to preprocess the NGSIM dataset for vehicle trajectory prediction. Currently, I have visualized the dataset using
'''
import pandas as pd
data = read_csv'URL'
data
'''
Equally, I have been able to extract the data for US-101 through filtering in excel.</p>
<p>Question:
How do I:</p>
<ol>
<li><p>Filter Vehicles on Lanes 1-5:</p>
<ul>
<li>Consider only the vehicles present on lanes 1 to 5.</li>
</ul>
</li>
<li><p>Identify Target Vehicles on Lanes 2-4:</p>
<ul>
<li>Find the target vehicles that are present on lanes 2 to 4.</li>
</ul>
</li>
<li><p>Extract Trajectory Data:</p>
<ul>
<li>Extract the trajectory data along with the frame number, vehicle ID, lane number, time, and position for the target vehicles.</li>
</ul>
</li>
<li><p>Frame-by-Frame Analysis for Each Target Vehicle:</p>
<ul>
<li>For each target vehicle, process the data frame by frame.</li>
</ul>
<p>4.1 Identify Vehicles in Each Frame:
- For each frame, identify the target vehicle and the surrounding vehicle IDs.</p>
<p>4.2 Determine Positions:
- Find the positions of the identified vehicles.</p>
</li>
<li><p>Save Variables:</p>
<ul>
<li>Save all variables for the target vehicle.</li>
</ul>
</li>
<li><p>Repeat for Next Target Vehicle:</p>
<ul>
<li>Repeat steps 3 to 5 for the next target vehicle.</li>
</ul>
</li>
</ol>
<p>problem based on the observed historical trajectory. The dataset to be used for evaluating different vehicle trajectory prediction models is that of &quot;the Next Generation Simulation (NGSIM)&quot; data, collected by the U.S. Department of Transportation. This dataset consists of 11.8 million items of data with 25 attributes (columns), such as the  vehicle’s coordinates and velocity. The sampling frequency of data is 10 Hz.</p>
<p>However, I had gotten the dataset (NGSIM) and prepared the mathematical model as that in this link <a href=""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832594"" rel=""nofollow noreferrer"">https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9832594</a></p>
<p>Since am using Python, Pytorch, and Anaconda IDE, what libraries do I need t  launch first?</p>
","transformer-model"
"78330930","Disabling fusing attention in ViT models in PyTorch","2024-04-15 20:40:16","","2","285","<python><pytorch><transformer-model>","<p>I am inspecting different vision transformer models on <code>PyTorch</code> and I am trying to understand their differences. My models can be seen in the following code:</p>
<pre><code>retrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, &quot;DEFAULT&quot; means best available
pretrained_vit_1 = torchvision.models.vit_b_16(weights=retrained_vit_weights).to(device)

pretrained_vit_2 = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True).to(device)

for block in pretrained_vit_2.blocks:
        block.attn.fused_attn = False
</code></pre>
<p>This code loads two different <code>ViT</code> models and disables the <code>fused_attn</code> variable for the second model. The models are a bit different, so I am trying to understand what exactly this code (<code>block.attn.fused_attn = False</code>) does and how can I apply it (or if its necessary to) in the first model.</p>
<p><strong>Edit</strong>: The idea is that I would like to run the model in an image and return the attention maps and the gradient up to a specific layer (<code>dropout</code> after the self-attention). In the same way as it is explained <a href=""https://github.com/jacobgil/vit-explain/blob/main/vit_explain.py"" rel=""nofollow noreferrer"">here</a>. Initially, I tried to use the <code>facebookresearch</code> <code>deit_tiny_patch16_224</code> model, and then I am commenting out the following code lines:</p>
<pre><code>for block in pretrained_vit_2.blocks:
        block.attn.fused_attn = False
</code></pre>
<p>When I commend out this code it results in empty list of attention maps (for each of the <code>ViT</code> layer) and gradient (the code uses a hook for <code>attn_drop</code> module). I am experiencing the same behavior for the <code>ViT_B_16_Weights</code> for which I am not sure how to disable the <code>fused_attn</code> and by default the gradient for the <code>dropout </code> layer returns an empty list (while the attention maps are not empty in this case).</p>
<p>Any idea why this holds? Why when I comment this parameter the result is empty gradient/maps?</p>
<p>Secondly, how can I do the same for the <code>ViT_B_16_Weights</code> (that is loaded from torchvision)?</p>
<p>In the first model indeed there its composed of blocks that contains variable <code>attn</code> and the <code>boolean</code> variable <code>fused_attn</code>.</p>
<p>The second model composed of stacked Encoder layers <code>pretrained_vit.encoder.layers</code> each of them contain a Boolean <code>add_zero_attention</code> and I could not find nowhere <code>fused_attn</code> or something similar. It seems that in the second model by default it uses this attribute: <code>scaled_dot_product_attention</code></p>
<p><strong>Edit2:</strong></p>
<p>Digging a bit into the implementation for attention in both models, for the case of the <code>ViT_B_16_Weights</code> I have noticed that there is the following comment in the <code>self.attention</code> implementation:</p>
<pre><code>``nn.MultiHeadAttention`` will use the optimized implementations of
``scaled_dot_product_attention()`` when possible.
</code></pre>
<p>It feels that by default this model sets this variable to <code>True</code>. However, not sure how to disable and replace that in the code for the second model. Is there a way to disable <code>scaled_dot_product_attention</code> in the same way as the other model?</p>
<p><strong>Edit 3:</strong>: After inspecting more about the <code>ViT_B_16_Weights</code> I could find in the <code>vision_transformer.py</code> a call to <code>activation.py</code> that in turns call the <code>functional.py</code>, there is some code with an if-else that relates to <code>scaled_dot_product_attention</code>. However, I am not sure how to activate if else statement for this model in the same way with the <code>deit_tiny_patch16_224</code>. Not even sure why the different calculations leads to different gradient/attention. Also, what it means to deactivate this variable when pretraining is set to <code>false</code>? Isn't that a bit weird?</p>
","transformer-model"
"78323826","ValueError: Expected input batch_size (8) to match target batch_size (2048)","2024-04-14 12:42:51","","0","34","<nlp><huggingface-transformers><transformer-model><huggingface>","<p>I am new to this field.
I have been stuck on this for quite a while now, and everything I tried has not helped me.</p>
<p>I want to finetune a model with data I have for machine translation. I am using the torch and transformers libraries.
Here is my code:</p>
<pre><code>tokenizer = MT5Tokenizer.from_pretrained(&quot;google/mt5-small&quot;)
model = MT5ForSequenceClassification.from_pretrained(&quot;google/mt5-small&quot;)
training_args = TrainingArguments(
    output_dir=&quot;./data&quot;,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir=&quot;./logs&quot;,
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=evalu_ds,
    tokenizer=tokenizer,
)
</code></pre>
<p><code>train_dataset</code> and <code>eval_dataset</code> are of type CustomDataset class I have defined that inherits from <code>torch.utils.data.Dataset</code>. They return:</p>
<pre><code>return {
    &quot;input_ids&quot;: input_ids_lang1,
    &quot;attention_mask&quot;: attention_mask_lang1,
    &quot;labels&quot;: input_ids_lang2,
    #&quot;attention_mask_lang2&quot;: attention_mask_lang2.float()
}
</code></pre>
<p><code>tokenizer</code> is <code>MT5Tokenizer.from_pretrained(&quot;google/mt5-small&quot;)</code></p>
<p>I get this error:</p>
<pre><code>  File &quot;C:\Users\X\AppData\Local\Programs\Python\Python39\lib\site-packages\torch\nn\functional.py&quot;, line 3060, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
ValueError: Expected input batch_size (8) to match target batch_size (2048).
</code></pre>
<p>printing out the shapes of <code>input</code> and <code>target</code> Tensors in the functional.py I get</p>
<p>input:  torch.Size([8, 256])</p>
<p>target: torch.Size([2048])</p>
<p>Just as it is mentioned <a href=""https://discuss.huggingface.co/t/reshaping-logits-when-using-trainer/18214"" rel=""nofollow noreferrer"">here</a> the target size seems to be 8 * 256 = 2048. But trying the CustomTrainer class did not help either.</p>
<p>I think the main problem I have is understanding what that means.</p>
<p>I tried many different variants, different models.
I expect to not get this error</p>
","transformer-model"
"78321212","Transformer Model Not Training Correctly, Likely Tensor Size Mismatch","2024-04-13 16:09:50","","0","62","<pytorch><classification><transformer-model><large-language-model><multiclass-classification>","<p>I am very new to Transformers and LLMs, and I am trying to re-purpose a model from this tutorial (<a href=""https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#1b3f"" rel=""nofollow noreferrer"">https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#1b3f</a>) for my dataset.</p>
<p>My data consists of a sequence of words and a single label (language id) attached to it. I want to train a bag-of-words style model so that the Transformer learns which language the words belong to, regardless of what order they are in.</p>
<p>My code looks like this:</p>
<h1>Data Preprocessing</h1>
<pre><code>import pandas as pd
import numpy as np
import time
import logging

logging.basicConfig(
     filename='logfile.log',
     level=logging.INFO, 
     format= '[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s',
     datefmt='%H:%M:%S'
 )

console = logging.StreamHandler()
console.setLevel(logging.DEBUG)
# set a format which is simpler for console use
formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s')
console.setFormatter(formatter)
# add the handler to the root logger
logging.getLogger('').addHandler(console)

logger = logging.getLogger(__name__)

lang_data = pd.read_csv('lang_data.csv')

labels = lang_data['langId']
sequences = lang_data['sequences']

word_to_token = {}
token_id = 1
total_X_tokenized = []

for i in sequences:
   for word in i:
       # Tokenize each word in a sequence
       X_tokenized = []
    
       if word not in word_to_token:
           word_to_token[word] = token_id
           token_id += 1
       X_tokenized.append(word_to_token[word])
    total_X_tokenized.append(X_tokenized)

max_sequence_length = 50  # Define maximum sequence length

for i in range(len(total_X_tokenized)):
    total_X_tokenized[i] = total_X_tokenized[i] + [0] * (max_sequence_length - len(total_X_tokenized[i]))

X_tensor = torch.tensor(total_X_tokenized, dtype=torch.long)
y_tensor = torch.tensor(labels, dtype=torch.long)

</code></pre>
<h1>Transformer Functions</h1>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class Norm(nn.Module):
    def __init__(self, d_model, eps = 1e-6):
        super().__init__()
    
        self.size = d_model
        
        # create two learnable parameters to calibrate normalisation
        self.alpha = nn.Parameter(torch.ones(self.size))
        self.bias = nn.Parameter(torch.zeros(self.size))
        
        self.eps = eps
    
    def forward(self, x):
        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \
        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias
        return norm
    
def attention(q, k, v, d_k, mask=None, dropout=None):
    
    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)
    
    if mask is not None:
        mask = mask.unsqueeze(1)
        scores = scores.masked_fill(mask == 0, -1e9)
    
    scores = F.softmax(scores, dim=-1)
    
    if dropout is not None:
        scores = dropout(scores)
        
    output = torch.matmul(scores, v)
    return output


class MultiHeadAttention(nn.Module):
    def __init__(self, heads, d_model, dropout = 0.1):
        super().__init__()
        
        self.d_model = d_model
        self.d_k = d_model // heads
        self.h = heads
        
        self.q_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.out = nn.Linear(d_model, d_model)
    
    def forward(self, q, k, v, mask=None):
        
        bs = q.size(0)
        
        # perform linear operation and split into N heads
        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)
        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)
        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)
        
        # transpose to get dimensions bs * N * sl * d_model
        k = k.transpose(1,2)
        q = q.transpose(1,2)
        v = v.transpose(1,2)
        

        # calculate attention using function we will define next
        scores = attention(q, k, v, self.d_k, mask, self.dropout)
        # concatenate heads and put through final linear layer
        concat = scores.transpose(1,2).contiguous()\
        .view(bs, -1, self.d_model)
        output = self.out(concat)
    
        return output

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff=2048, dropout = 0.1):
        super().__init__() 
    
        # We set d_ff as a default to 2048
        self.linear_1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)
    
    def forward(self, x):
        x = self.dropout(F.relu(self.linear_1(x)))
        x = self.linear_2(x)
        return x

class EncoderLayer(nn.Module):
    def __init__(self, d_model, heads, dropout=0.1):
        super().__init__()
        self.norm_1 = Norm(d_model)
        self.norm_2 = Norm(d_model)
        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)
        self.ff = FeedForward(d_model, dropout=dropout)
        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)
        
    def forward(self, x, mask):
        x2 = self.norm_1(x)
        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))
        x2 = self.norm_2(x)
        x = x + self.dropout_2(self.ff(x2))
        return x
    

class DecoderLayer(nn.Module):
    def __init__(self, d_model, heads, dropout=0.1):
        super().__init__()
        self.norm_1 = Norm(d_model)
        self.norm_2 = Norm(d_model)
        self.norm_3 = Norm(d_model)
        
        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)
        self.dropout_3 = nn.Dropout(dropout)
        
        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)
        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)
        self.ff = FeedForward(d_model, dropout=dropout)

    def forward(self, x, e_outputs, src_mask, trg_mask):
        x2 = self.norm_1(x)
        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))
        x2 = self.norm_2(x)
        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, \
        src_mask))
        x2 = self.norm_3(x)
        x = x + self.dropout_3(self.ff(x2))
        return x
    
import copy
class Embedder(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
    def forward(self, x):
        return self.embed(x)
    
def get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])

class Encoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads):
        super().__init__()
        self.N = N
        self.embed = Embedder(vocab_size, d_model)
        #self.pe = PositionalEncoder(d_model)
        self.layers = get_clones(EncoderLayer(d_model, heads), N)
        self.norm = Norm(d_model)
    def forward(self, src, mask):
        x = self.embed(src)
        #x = self.pe(x)
        for i in range(self.N):
            x = self.layers[i](x, mask)
        return self.norm(x)
    
class Decoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads):
        super().__init__()
        self.N = N
        self.embed = Embedder(vocab_size, d_model)
        #self.pe = PositionalEncoder(d_model)
        self.layers = get_clones(DecoderLayer(d_model, heads), N)
        self.norm = Norm(d_model)
    def forward(self, trg, e_outputs, src_mask, trg_mask):
        x = self.embed(trg)
       #x = self.pe(x)
        for i in range(self.N):
            x = self.layers[i](x, e_outputs, src_mask, trg_mask)
        return self.norm(x)
    
class Transformer(nn.Module):
    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):
        super().__init__()
        self.encoder = Encoder(src_vocab, d_model, N, heads)
        self.decoder = Decoder(trg_vocab, d_model, N, heads)
        self.out = nn.Linear(d_model, trg_vocab)
    def forward(self, src, trg, src_mask, trg_mask):
        e_outputs = self.encoder(src, src_mask)
        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)
        output = self.out(d_output)
        return output
</code></pre>
<p>Everything in the preprocessing step and the Transformer setup runs correctly and as intended. The issue arises when I try to train it.</p>
<h1>Training Loop</h1>
<pre><code>from torch.utils.data import DataLoader, TensorDataset, random_split
batch_size = 20
dataset = TensorDataset(X_tensor, y_tensor)

#EVERYTHING LOOKS FINE UP TO HERE


train_ratio = 0.8
val_ratio = 0.1
test_ratio = 0.1

# Calculate lengths of each subset
train_size = int(train_ratio * len(dataset))
val_size = int(val_ratio * len(dataset))
test_size = len(dataset) - train_size - val_size

# Split dataset into train, validation, and test sets
train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# Create DataLoader for each subset
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

src_vocab_size = len(word_to_token)+1  # Size of the source vocabulary
trg_vocab_size = len(np.unique(labels))+1  # Number of classes 
d_model = 128  # Embedding dimension
N = 4  # Number of encoder/decoder layers
heads = 8  # Number of attention heads

model = Transformer(src_vocab_size, trg_vocab_size, d_model, N, heads).to('cuda:0')
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)
num_epochs = 10000
num_classes = 360
val_losses = []
model.train()
for epoch in range(num_epochs):
    running_loss = 0.0
    batch = 0
    for inputs, labels in train_loader:
        optimizer.zero_grad()  # Zero the gradients
        inputs = inputs.to('cuda:0')
        labels = labels.to('cuda:0')
        #print(inputs)
        #print(labels.shape)

        #print(labels)
        
        outputs = model(inputs, labels, None #src_mask, None #trg_mask)

        labels = F.one_hot(labels, num_classes)
       
        loss = criterion(outputs, labels)
        
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item() * inputs.size(0)
        #print(&quot;Completed for batch: &quot; + str(batch))
        batch = batch + 1
    
    epoch_loss = running_loss / len(train_dataset)
    print(f&quot;Epoch {epoch + 1}, Loss: {epoch_loss:.4f}&quot;)
    logger.info(f&quot;Epoch {epoch + 1}, Loss: {epoch_loss:.4f}&quot;)

    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    with torch.no_grad():
        for val_inputs, val_labels in val_loader:
            val_inputs = val_inputs.to('cuda:0')
            val_labels = val_labels.to('cuda:0')
            val_outputs = model(val_inputs, val_labels, None, None)

            #print(val_labels)

            val_labels = F.one_hot(val_labels, num_classes)
            val_loss += criterion(val_outputs, val_labels).item()
            _, predicted = torch.max(val_outputs, 1)
            print(&quot;Max: &quot; + str(torch.max(predicted)))
            total += val_labels.size(0)
            correct += (predicted == val_labels).sum().item()
            print(&quot;Correct:&quot; + str(correct))

    val_loss /= len(val_loader)
    val_accuracy = 100 * correct / total
    val_losses.append(val_loss)
    print(val_accuracy)

    if val_loss == min(val_losses):
        torch.save(model.state_dict(), 'directory/test_transformer_'+str(epoch+1)+'.pth')
        print(f'New Model Checkpoint Saved. Validation Loss is: {val_loss:.4f}')
        logger.info(f'New Model Checkpoint Saved. Validation Loss is: {val_loss:.4f}')
</code></pre>
<p>The Training and Validation losses never improve, which is likely due to the model predicting on only 20 classes as opposed to the entire vocab size, which is 360. I know something (or multiple things) is off with my training loop, as the output shape is not what I expect (Tensor.shape([20, 20, 360]), and the validation loop only predicts from 0 to 19. I'm guessing it's somehow training on the batch data instead of the vocab.</p>
<p>As you can probably tell with the format of this training loop, my background is mostly in CNNs and other neural nets, not LLMs, so I would also appreciate any input you have on how the format of the training loop differs between the two.</p>
<p>Thank you for all of your help!</p>
<p>I've tried to look into Transformer documentation and the shapes/values of the inputs, outputs, and labels tensors. The inputs and labels tensors look correct (inputs contains a tokenized sequence whereas labels consists of an integer value corresponding to a language id). The outputs tensor does contain values that look to be correct, but the losses never improve, so something must be off there.</p>
","transformer-model"
"78318458","Question about Xavier Initialization for a Transformer model","2024-04-12 19:49:18","","0","59","<python><artificial-intelligence><transformer-model>","<p>I am coding since I was little but I started python recently and I am struggling to program a simple Transformer model that predicts the next word of a sequence. I am experimenting with max input sequence length set to 6 and token embedding size 512. When the user writes a new word, the program adds it to the vocabulary list and creates a randomly initialized token embedding. I am trying to do this with Xavier initialization but I don't understand what ninputs and noutputs are in the Xavier formula x=sqrt(2/ninputs+noutputs) where x is the uniform distribution for picking a random number. Would the ninputs and noutputs be the max sequence length(6) or the embedding size(512)? Sorry for my English.</p>
<p>I would appreciate it if someone could explain the 2 types of Xavier initialization (uniform and normal) and those 2 variables.</p>
","transformer-model"
"78317127","How to plot an attention map for Vision Transformer model","2024-04-12 14:59:45","","1","370","<python><tensorflow><keras><transformer-model><attention-model>","<p>I am implementing a Vision Transformer model as part of a school project and I am required to plot an attention map to compare the differences between a CNN model and ViT model, but I am not sure how to go about doing it.</p>
<p>For reference, I have been referring to this notebook for the code, except that I used google/vit-base-patch16-224-in21k for the ViT model
<a href=""https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-08-03-tensorflow-i-know-flowers-deit/2023-08-03/#deit-model"" rel=""nofollow noreferrer"">https://mpolinowski.github.io/docs/IoT-and-Machine-Learning/ML/2023-08-03-tensorflow-i-know-flowers-deit/2023-08-03/#deit-model</a></p>
<p>This is the output from vit_model.summary():</p>
<pre><code>Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 224, 224, 3)]     0         
                                                                 
 sequential (Sequential)     (None, 3, 224, 224)       0         
                                                                 
 vit (TFViTMainLayer)        TFBaseModelOutputWithPo   29686272  
                             oling(last_hidden_state             
                             =(None, 197, 768),                  
                              pooler_output=(None, 7             
                             68),                                
                              hidden_states=None, at             
                             tentions=None)                      
                                                                 
 tf.__operators__.getitem (  (None, 768)               0         
 SlicingOpLambda)                                                
                                                                 
 dense (Dense)               (None, 2)                 1538      
                                                                 
=================================================================
Total params: 29687810 (113.25 MB)
Trainable params: 29687810 (113.25 MB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre>
<p>These are the configurations for the model:</p>
<pre><code>ViTConfig {
  &quot;_name_or_path&quot;: &quot;google/vit-base-patch16-224-in21k&quot;,
  &quot;attention_probs_dropout_prob&quot;: 0.0,
  &quot;encoder_stride&quot;: 16,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.0,
  &quot;hidden_size&quot;: 768,
  &quot;image_size&quot;: 224,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;model_type&quot;: &quot;vit&quot;,
  &quot;num_attention_heads&quot;: 8,
  &quot;num_channels&quot;: 3,
  &quot;num_hidden_layers&quot;: 4,
  &quot;patch_size&quot;: 16,
  &quot;qkv_bias&quot;: true,
  &quot;transformers_version&quot;: &quot;4.38.2&quot;
}
</code></pre>
<p>I tried to extract the activation and output layer from the original model like so, but I am unsure of how to reshape the NumPy arrays in order to get the weights to match a 224x224 image:</p>
<pre><code>activation_layer = vit_model.get_layer(&quot;vit&quot;)
new_model = Model(inputs = vit_model.input, outputs = activation_layer.output)
final_dense = vit_model.get_layer('dense')
W = final_dense.get_weights()[0]
</code></pre>
<p>But this does not seem to give me what I need.</p>
","transformer-model"
"78315964","Loading pre-trained weights properly in Pytorch","2024-04-12 11:27:08","78317551","1","203","<python><pytorch><transformer-model><transfer-learning>","<p>I would like to perform transfer learning by loading a pretrained vision transformer model, modify its last layer and training it with my own data.</p>
<p>Hence, I am loading my dataset perform the typical transformation similar to the ImageNet, then, load the model, disable the grad from all its layer remove the last layer and add a trainable one using the number of classes of my dataset. My code could look like as follows:</p>
<pre><code>#retrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, &quot;DEFAULT&quot; means best available
#pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)
pretrained_vit = torch.hub.load('facebookresearch/deit:main', 'deit_tiny_patch16_224', pretrained=True).to(device)

for parameter in pretrained_vit.parameters():
    parameter.requires_grad = False

pretrained_vit.heads = nn.Linear(in_features=192, out_features=len(class_names)).to(device)
optimizer(torch.optim.Adam(params=pretrained_vit.parameters(), ... )
loss_fn = torch.nn.CrossEntropyLoss()

esults = engine.train(model=pretrained_vit, ..., ... )
</code></pre>
<p>When I am using <code>torchvision.models.ViT_B_16_Weights.DEFAULT</code> then the code works smoothly and I can run my code without any problem. However, when I am using instead the <code>deit_tiny_patch16_224</code> and I set the <code>requires_grade = False</code>  then I got the following error:</p>
<pre><code>Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
</code></pre>
<p>When the varialble is set to True, the code works smoothly but ofc the training is really bad since I had a very small amount of pictures. How, can I set properly the <code>deit_tiny_patch16_224</code> parametes to <code>parameter.requires_grad = False</code>?</p>
<p>Is there an issue with the way I am loading the pre-trained weights?</p>
","transformer-model"
"78311534","Batch and Epoch training metrics for transformers Trainer","2024-04-11 15:38:25","","1","690","<huggingface-transformers><metrics><text-classification><transformer-model><huggingface-trainer>","<p>There are several ways to <strong>get metrics</strong> for <code>transformers.Trainer</code> but only for the evaluation and not <strong>for the training</strong>. I read and found answers scattered in different posts such as <a href=""https://discuss.huggingface.co/t/metrics-for-training-set-in-trainer/2461/9"" rel=""nofollow noreferrer"">this post</a>.</p>
<p>But nowhere did I find how to get metrics <strong>for an epoch</strong> (loss, accuracy, recall, precision, f1)**</p>
<p>I provide the solution in the first answer.</p>
<p>Let me know if there are some ways to optimize or errors in the code, I'll be happy to hear them.</p>
","transformer-model"
"78304710","Can I appoint a Masked Language Model's outputs' range?","2024-04-10 13:11:53","","0","43","<nlp><huggingface-transformers><transformer-model><mlmodel>","<p>When different kinds of models are trained with masked language modeling, the input embeddings at masked positions are replaced with a MASK token. I'm wondering if I could appoint the range of a MASK token? For example:</p>
<pre><code>1) &quot;What a [MASK] weather!&quot; 
2) &quot;What a [MASK] person he is!&quot;
3) &quot;How can you do such a [MASK] thing!&quot; 

...
</code></pre>
<p>Instead of let a pretrained model using its ability to find a suitable word in its whole vocab, I want the pretrained model to pick a word which is from a specific token set, e.g {&quot;good&quot;,&quot;great&quot;,&quot;stupid&quot;,&quot;bad&quot;}, to replace the MASK token. In another words, when facing all different kinds of input, I wish the model could replace the MASK token using the word from the specific token set. Could anyone give me some hints to do this? Thanks!</p>
","transformer-model"
"78304515","for transformers import AutoImageProcessor, AutoModel: ValueError: source code string cannot contain null bytes for transformers import","2024-04-10 12:43:30","","0","48","<import><null><valueerror><transformer-model>","<p>When trying to use the AutoImageProcessor and the AutoModel from the transformers library as in:</p>
<pre><code>from transformers import AutoImageProcessor, AutoModel
from PIL import Image
import faiss
import numpy as np
import os

#load the model and processor
device = torch.device('cuda' if torch.cuda.is_available() else &quot;cpu&quot;)
processor = AutoImageProcessor.from_pretrained('facebook/dinov2-small')
model = AutoModel.from_pretrained('facebook/dinov2-small').to(device)

#Populate the images variable with all the images in the dataset folder
images = []
for root, dirs, files in os.walk('./dataset'):
    for file in files:
        if file.endswith('jpg'):
            images.append(root  + '/'+ file)
</code></pre>
<p>I get the error: ValueError: source code string cannot contain null bytes. Where is the problem? Thank you in advance</p>
","transformer-model"
"78282168","tensorflow transformer example","2024-04-05 20:16:12","","0","48","<tensorflow><transformer-model>","<p>I am trying to run TensorFlow transformer example for english-Portuguese neural machine translation <a href=""https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb#scrollTo=LTEVgBxklzdq"" rel=""nofollow noreferrer"">transformer.ipynb</a> in google colab but the the notebook is not working It has an error, anyone tried to run this notebook and figured out what is the problem?</p>
","transformer-model"
"78274940","MultiHeadAttention With 2 Attention Axes And An Attention Mask - How to apply mask","2024-04-04 15:34:07","","0","22","<python><tensorflow><deep-learning><transformer-model><text-generation>","<p>I have a small GPT model for text generation. I modified it to work on vectors rather than softmax outputs. So it basically generates a vector for the predicted next token. Normally, output shape of the model would be <strong>(None, seq_len, vocab_size)</strong>. Now, it's <strong>(None, seq_len, vector_size)</strong>. There's no problem with that. I can run the model. What I'd like to add is another dimension to the vectors. I want for each token to be represented by two vectors. So the output shape will be <strong>(None, seq_len, num_vectors, vector_size)</strong>. And I want to atten over last to dimensions <strong>(num_vectors, vector_size)</strong>. I can run the model with this structure. However, I am having problems with how to apply attention mask to such an architecture. There arises some problems with shapes within Transformer model. Can you help me build an maskfunction?</p>
<p>Transformer model:</p>
<pre><code>def causal_attention_mask(batch_size, n_dest, n_src, num_vectors, dtype):
    &quot;&quot;&quot;
    Mask the upper half of the dot product matrix in self attention.
    This prevents flow of information from future tokens to current token.
    1's in the lower triangle, counting from the lower right corner.
    &quot;&quot;&quot;
    i = tf.range(n_dest)[:, None]
    j = tf.range(n_src)
    m = i &gt;= j - n_src + n_dest
    mask = tf.cast(m, dtype)
    tf.print(mask)
    mask = tf.reshape(mask, [1, n_dest, n_src])
    mult = tf.concat(
        [tf.expand_dims(batch_size, -1), tf.convert_to_tensor([1, 1])], 0
    )
    return tf.tile(mask, mult)

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):
        super().__init__(**kwargs)
        self.att = layers.MultiHeadAttention(num_heads, embed_dim, attention_axes=[1, 2])
        self.ffn = keras.Sequential(
            [
                layers.Dense(ff_dim, activation=&quot;relu&quot;),
                layers.Dense(embed_dim),
            ]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size = input_shape[0]
        seq_len = input_shape[2]
        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, 2, &quot;bool&quot;)
        attention_output = self.att(inputs, inputs)
        attention_output = self.dropout1(attention_output)
        out1 = self.layernorm1(inputs + attention_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output)
        return self.layernorm2(out1 + ffn_output)

    def get_weights(self):
        return [self.att.get_weights(), self.ffn.layers[0].get_weights(), self.ffn.layers[1].get_weights()]

    def set_weights(self, weights):
        self.att.set_weights(weights[0])
        self.ffn.layers[0].set_weights(weights[1])
        self.ffn.layers[1].set_weights(weights[2])
</code></pre>
<p>Main model:</p>
<pre><code>def create_model():
    inputs = layers.Input(shape=(maxlen,), dtype=&quot;float32&quot;)
   
    node_embedding_layer = NodeEmbedding(embed_dim, final_embeddings, name='node_embedding_layer')
    x2 = node_embedding_layer(inputs)
    # x2 = tf.transpose(x2, perm=[0, 2, 1, 3])  # (batch_size, vector_count, seq_len, vector_dim)
    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim, name='node_transformer_layer')
    x2 = transformer_block(x2)

    # Calculate final output node embeddings
    outputs_2 = layers.Dense(node_embedding_dim / 2, name='graph_embedding_output')(x2)
    
    # Concatenate outputs of both branches
    # outputs = layers.Average()([outputs_1, outputs_2])

    outputs = outputs_2

    # Softmax layer
    # softmax_output = layers.Softmax()(outputs)
    
    model = tf.keras.Model(inputs=inputs, outputs=[outputs])
    # loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
    model.compile(
        optimizer=keras.optimizers.Adam(0.001),
        loss=[euclidean_distance_loss],
        metrics=['mse']
    )  # No loss and optimization based on word embeddings from transformer block
    return model
</code></pre>
","transformer-model"
"78272480","Attention Model Positional Encoding Dimension Problem","2024-04-04 08:00:43","","0","16","<deep-learning><encoding><neural-network><transformer-model><attention-model>","<pre><code>import torch
import torch.nn as nn
import numpy as np

# Positional Encoding for Transformer
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=1000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

# Model definition using Transformer
class AttentionModel(nn.Module):
    def __init__(self, input_dim=1, d_model=64, output_dim = 3, nhead=4, num_layers=2, dropout=0.2, n_fc_layers = 1,
                fc_hidden_units = 10):

        super(AttentionModel, self).__init__()
        layers = []
        self.encoder = nn.Linear(input_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)
        for i in range(n_fc_layers):
            if i == 0:
                layers += [nn.Linear(d_model, fc_hidden_units)]
            else:
                layers += [nn.Linear(fc_hidden_units, fc_hidden_units)]
        self.network = nn.Sequential(*layers)
        self.decoder = nn.Linear(fc_hidden_units, output_dim)

    def forward(self, x):
        print(x.shape)
        x = self.encoder(x)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        x = self.network(x)
        x = self.decoder(x[:, -1, :])
        return x
</code></pre>
<p><strong>Output:
2020-04-24 03:58:46,007 [Trial 0] Failed with parameters: {'batch_size': 40, 'fc_layers': 3, 'hidden_units': 240, 'num_layers': 2,'head_num':4}...
RuntimeError: The size of tensor a (230) must match the size of tensor b (21) at non-singleton dimension 1</strong></p>
<p>My code was running fine in the previous trial and it suddenly got into this error and I cant fix it.
Anyone knows why?</p>
","transformer-model"
"78253997","vision transformers: RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x1000 and 768x32)","2024-04-01 06:33:32","78254690","-2","74","<python><machine-learning><deep-learning><pytorch><transformer-model>","<p>I am trying to do Regression on the vision transformers model and I cannot replace the last layer of classification with the regression layer</p>
<pre><code>class RegressionViT(nn.Module):
    def __init__(self, in_features=224 * 224 * 3, num_classes=1, pretrained=True):
        super(RegressionViT, self).__init__()
        self.vit_b_16 = vit_b_16(pretrained=pretrained)
        # Accessing the actual output feature size from vit_b_16
        self.regressor = nn.Linear(self.vit_b_16.heads[0].in_features, num_classes * batch_size)

    def forward(self, x):
        x = self.vit_b_16(x)
        x = self.regressor(x)
        return x


# Model
model = RegressionViT(num_classes=1)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model.to(device)

criterion = nn.MSELoss()  # Use appropriate loss function for regression
optimizer = optim.Adam(model.parameters(), lr=0.0001)

</code></pre>
<p>I get this error when I try to initialize and run the model</p>
<pre><code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x1000 and 768x32)
</code></pre>
<p>The problem is that there is a mismatch between the regression layer and the <strong>vit_b_16</strong> model layer, what would be the correct way to solve this issue</p>
","transformer-model"
"78245568","Understanding batching in pytorch models","2024-03-29 19:24:58","78245883","0","42","<python><machine-learning><deep-learning><pytorch><transformer-model>","<p>I have following model which forms one of the step in my overall model pipeline:</p>
<pre><code>import torch
import torch.nn as nn

class NPB(nn.Module):
    def __init__(self, d, nhead, num_layers, dropout=0.1):
        super(NPB, self).__init__()
            
        self.te = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=d, nhead=nhead, dropout=dropout, batch_first=True),
            num_layers=num_layers,
        ) 

        self.t_emb = nn.Parameter(torch.randn(1, d))
        
        self.L = nn.Parameter(torch.randn(1, d)) 

        self.td = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model=d, nhead=nhead, dropout=dropout, batch_first=True),
            num_layers=num_layers,
        ) 

        self.ffn = nn.Linear(d, 6)
    
    def forward(self, t_v, t_i):
        print(&quot;--------------- t_v, t_i -----------------&quot;)
        print('t_v: ', tuple(t_v.shape))
        print('t_i: ', tuple(t_i.shape))

        print(&quot;--------------- t_v + t_i + t_emb -----------------&quot;)
        _x = t_v + t_i + self.t_emb
        print(tuple(_x.shape))

        print(&quot;--------------- te ---------------&quot;)
        _x = self.te(_x)
        print(tuple(_x.shape))
        
        print(&quot;--------------- td ---------------&quot;)
        _x = self.td(self.L, _x)
        print(tuple(_x.shape))

        print(&quot;--------------- ffn ---------------&quot;)
        _x = self.ffn(_x)
        print(tuple(_x.shape))

        return _x
</code></pre>
<p>Here <code>t_v</code> and <code>t_i</code> are inputs from earlier encoder blocks. I pass them as shape of <code>(4,256)</code>, where <code>256</code> is number of features and <code>4</code> is batch size. <code>t_emb</code> is temporal embedding.  <code>L</code> represents learned matrix representing the embedding of the query. I tested this module block with following code:</p>
<pre><code>t_v = torch.randn((4,256))
t_i = torch.randn((4,256))
npb = NPB(d=256, nhead=8, num_layers=2)
npb(t_v, t_i)
</code></pre>
<p>It outputted:</p>
<pre><code>=============== NPB ===============
--------------- t_v, t_i -----------------
t_v:  (4, 256)
t_i:  (4, 256)
--------------- t_v + t_i + t_emb -----------------
(4, 256)
--------------- te ---------------
(4, 256)
--------------- td ---------------
(1, 256)
--------------- ffn ---------------
(1, 6)
</code></pre>
<p>I was expecting the output should be of shape <code>(4,6)</code>, 6 values for each sample in the batch of size <code>6</code>. But the output was of size <code>(1,6)</code>. After a lot of tweaking, I tried changing <code>t_emb</code> and <code>L</code> shape from <code>(1,d)</code> to <code>(4,d)</code>, since I did not wanted all sampled to share these variables (through broadcasting:</p>
<pre><code>self.t_emb = nn.Parameter(torch.randn(4, d)) # [n, d] = [4, 256]     
self.L = nn.Parameter(torch.randn(4, d)) 
</code></pre>
<p>This gives desired output of shape (4,6:</p>
<pre><code>--------------- t_v, t_i -----------------
t_v:  (4, 256)
t_i:  (4, 256)
--------------- t_v + t_i + t_emb -----------------
(4, 256)
--------------- te ---------------
(4, 256)
--------------- td ---------------
(4, 256)
--------------- ffn ---------------
(4, 6)
</code></pre>
<p>I have following doubts:</p>
<p><strong>Q1.</strong> Exactly why changing <code>L</code> and <code>t_emb</code> shape from <code>(1,d)</code> to <code>(4,d)</code> worked? Why it did not work with <code>(1,d)</code> through broadcasting?<br />
<strong>Q2.</strong> Am I doing batching right way or the output is artificially correct while under the hood its doing something different than what I am expecting (predicting 6 values for each sample in the batch of size 4)?</p>
","transformer-model"
"78243585","Using an upstream-downstream ML model, with the upstream being Wav2Vec 2.0 transformer and the downstream CNN. The model's accuracy is plateaued, why?","2024-03-29 11:48:46","","1","26","<machine-learning><transformer-model><pre-trained-model>","<p>Trying to use wav2vec 2.0 in conjuction with CNN for speech emotion recognition. Four classes have been defined. All the audios has been preprocessed and adequately truncated/padded and resampled according to the need of the wav2vec 2.0 model. This is how the model has been defined:</p>
<pre><code>class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.softmax(x)
        return x

tokenizer = Wav2Vec2Tokenizer.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)
model = Wav2Vec2Model.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)

# Modify the last layer to match the number of output classes
label_mapping = {'OAF_Fear': 0, 'OAF_angry': 1, 'OAF_happy': 2, 'OAF_neutral': 3}
num_classes = len(label_mapping)

model.lm_head = nn.Linear(in_features=model.config.hidden_size, out_features=num_classes, bias=True)

for param in model.parameters():
    param.requires_grad = False
for param in model.lm_head.parameters():
    param.requires_grad = True

input_size = 768  # Size of features extracted from pre-trained model
hidden_size = 256
output_size = num_classes  # Number of emotion classes
learning_rate = 0.001
num_epochs = 50
batch_size = 32
root_dir = &quot;/content/drive/MyDrive/BTP_hanan_dataset/Dataset/TESS&quot;

class FullModel(nn.Module):
    def __init__(self, wav2vec_model, simple_nn_model):
        super(FullModel, self).__init__()
        self.wav2vec_model = wav2vec_model
        self.simple_nn_model = simple_nn_model

    def forward(self, x):
        # Get hidden states from pre-trained model
        hidden_states = self.wav2vec_model(x)[0]
        
        # Aggregate hidden states (e.g., by averaging or max-pooling)
        aggregated_hidden_state = torch.mean(hidden_states, dim=1)  # Example: averaging
        
        # Pass through simple neural network
        output = self.simple_nn_model(aggregated_hidden_state)
        
        return output
simple_nn = SimpleNN(input_size, hidden_size, output_size)

for param in simple_nn.parameters():
    param.requires_grad = True

# Combine pre-trained model and simple neural network into a single model
full_model = FullModel(model, simple_nn)

# Loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(full_model.parameters(), lr=learning_rate)

</code></pre>
<p>The last layer of the pretrained model has been trained, whose parameters are passed on to the simple CNN. The model is stagnant at 35 percent accuracy.</p>
<p>Tried it on two different datasets, yet, nothing is improving. Early stopping is triggered after 7 - 10 epochs, with patience = 5. What am I doing wrong?</p>
","transformer-model"
"78233208","How to obtain latent vectors from fine-tuned model with transformers","2024-03-27 16:29:33","","0","30","<huggingface-transformers><transformer-model><fine-tuning>","<p>I’m working on a deep learning project for music recommendation. The idea is either build a CNN or load a pretrained model and train/fine-tune with GTZAN dataset in order to learn how to identify music patterns from songs raw mp3. Once the model is succesfully trained/tuned, I would try to obtain latent vectors from last layers for each songs, and then compute euclidean/cosine distance to compute similarity for recommendation.</p>
<p>So far, I’ve fine-tuned ntu-spml/distilhubert model using transformers library and finally obtained 80% accuracy. However, I don’t know how to extract latent vectors from fine-tuned model. If anyone would be about to help me I will provide code cells.</p>
","transformer-model"
"78220853","Improving Train Punctuality Prediction Using a Transformer Model: Model Setup and Performance Issues","2024-03-25 17:41:18","","0","18","<machine-learning><deep-learning><pytorch><transformer-model>","<p>I am working on a project to predict the punctuality of trains, measured in minutes, at each station during various trips. I initially employed a Deep Feedforward Neural Network (DFFNN) for this task but wanted to explore the capabilities of a Transformer model to potentially improve predictions. I prepared my dataset into sequences, accounting for the varying number of stations per trip by implementing padding. Given the sequential nature of my data, I assumed that an encoder-only Transformer would suffice for this task.</p>
<p>During the model training process, I've not observed significant improvement, even after numerous epochs, which leads me to question whether my Transformer model setup is optimal for this particular problem. I've also integrated Optuna for hyperparameter tuning to aid in optimizing the model's performance.</p>
<pre class=""lang-py prettyprint-override""><code>grouped_data = df.groupby([&lt;features to identify trips&gt;])
grouped_data.ngroups

sequences = []
targets = []
masks = []

for trip, group in grouped_data:
    sequence = group[
        [
            &lt;features&gt;
        ]
    ]

    sequences.append(sequence.values)
    targets.append(group[&lt;target&gt;].values)

    # Create a mask for this sequence (1 for data, 0 for padding)
    mask = np.ones(len(sequence), dtype=np.float32)
    masks.append(mask)

max_seq_length = max(len(sequence) for sequence in sequences)
padding_value = 0  # Assume 0 is used for padding
padded_sequences = []
padded_targets = []
padded_masks = []  # List for padded masks

for sequence, target, mask in zip(sequences, targets, masks):
    padding_length = max_seq_length - len(sequence)

    # Pad sequence and target as before
    sequence_padding = np.full((padding_length, sequence.shape[1]), padding_value)
    target_padding = np.full(padding_length, padding_value)
    padded_sequence = np.concatenate((sequence, sequence_padding), axis=0)
    padded_target = np.concatenate((target, target_padding), axis=0)

    # Pad mask
    mask_padding = np.zeros(padding_length, dtype=np.float32)  # Padding for mask is 0
    padded_mask = np.concatenate((mask, mask_padding), axis=0)

    padded_sequences.append(padded_sequence)
    padded_targets.append(padded_target)
    padded_masks.append(padded_mask)  # Add the padded mask to the list

padded_sequences = np.array(padded_sequences)
padded_targets = np.array(padded_targets)
padded_masks = np.array(padded_masks)  # Convert padded masks list to a numpy array

(
    train_sequences,
    test_sequences,
    train_masks,
    test_masks,
    train_targets,
    test_targets,
) = train_test_split(
    padded_sequences, padded_masks, padded_targets, test_size=0.2, random_state=42
)

train_sequence_tensor = torch.tensor(train_sequences, dtype=torch.float32)
train_mask_tensor = torch.tensor(train_masks, dtype=torch.bool)
train_target_tensor = torch.tensor(train_targets, dtype=torch.float32)

test_sequence_tensor = torch.tensor(test_sequences, dtype=torch.float32)
test_mask_tensor = torch.tensor(test_masks, dtype=torch.bool)
test_target_tensor = torch.tensor(test_targets, dtype=torch.float32)

train_dataset = TensorDataset(
    train_sequence_tensor, train_mask_tensor, train_target_tensor
)
test_dataset = TensorDataset(test_sequence_tensor, test_mask_tensor, test_target_tensor)

batch_size = 32
train_dataloader = DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True
)
test_dataloader = DataLoader(
    test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True
)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
        )
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer(&quot;pe&quot;, pe)

    def forward(self, x):
        x = x + self.pe[: x.size(0), :]
        return x


class TransformerModel(nn.Module):
    def __init__(
        self,
        input_dim,
        model_dim,
        num_heads,
        num_layers,
        dropout_rate=0.1,
        max_len=5000,
    ):
        super(TransformerModel, self).__init__()
        self.model_dim = model_dim
        self.feature_embedding = nn.Linear(input_dim, model_dim)
        self.positional_encoding = PositionalEncoding(model_dim, max_len)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=model_dim, nhead=num_heads, dropout=dropout_rate, batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer, num_layers=num_layers
        )
        self.output_layer = nn.Linear(model_dim, 1)

    def forward(self, src, src_mask=None):
        src = self.feature_embedding(src)
        src = self.positional_encoding(src)
        output = self.transformer_encoder(src, src_key_padding_mask=src_mask)
        output = self.output_layer(output)
        output = output.squeeze(-1)
        return output


def objective(trial):
    start_time = time.time()
    max_duration = 500  # seconds

    lr = trial.suggest_float(&quot;lr&quot;, 1e-5, 1e-1, log=True)
    model_dim = trial.suggest_categorical(&quot;model_dim&quot;, [128, 256, 512])
    num_heads = trial.suggest_categorical(&quot;num_heads&quot;, [4, 8, 16])
    num_layers = trial.suggest_int(&quot;num_layers&quot;, 1, 4)
    dropout_rate = trial.suggest_float(&quot;dropout_rate&quot;, 0.1, 0.5)
    weight_decay = trial.suggest_float(&quot;weight_decay&quot;, 1e-5, 1e-1, log=True)

    input_dim = len(padded_sequences[0][0])  # Number of features in the input
    model = TransformerModel(
        input_dim, model_dim, num_heads, num_layers, dropout_rate
    ).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

    num_epochs = 10
    first_epoch_loss = None
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        for sequences, masks, targets in train_dataloader:
            sequences = sequences.to(device)
            masks = masks.to(device)
            targets = targets.to(device)

            optimizer.zero_grad()
            outputs = model(
                sequences, ~masks
            )  
            loss = criterion(outputs[masks], targets[masks])  # Apply masks
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        if epoch == 0:
            first_epoch_loss = train_loss / len(train_dataloader)

        scheduler.step()

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for sequences, masks, targets in test_dataloader:
                sequences = sequences.to(device)
                masks = masks.to(device)
                targets = targets.to(device)

                outputs = model(
                    sequences, ~masks
                )  # Inverting masks for src_key_padding_mask
                loss = criterion(outputs[masks], targets[masks])  # Apply masks
                val_loss += loss.item()

        val_loss /= len(test_dataloader)
        trial.report(val_loss, epoch)

        val_rmse = torch.sqrt(torch.tensor(val_loss))
        trial.set_user_attr(&quot;val_rmse&quot;, val_rmse.item())

        if trial.should_prune():
            raise optuna.TrialPruned()

        elapsed_time = time.time() - start_time
        if elapsed_time &gt; max_duration:
            print(f&quot;Pruning trial due to timeout: elapsed time {elapsed_time}s&quot;)
            raise optuna.TrialPruned()

        print(f&quot;Epoch: {epoch + 1}, Loss: {val_loss}&quot;)

    print(
        f&quot;Trial {trial.number}: First Epoch Loss = {first_epoch_loss}, Final Epoch Loss = {val_loss}&quot;
    )

    return val_loss


study = optuna.create_study(direction=&quot;minimize&quot;, pruner=optuna.pruners.MedianPruner())
study.optimize(objective, n_trials=20)
</code></pre>
<p>Here's a brief overview of my approach:</p>
<ul>
<li><p>Data Preparation: I grouped my dataset by trips and created sequences for each trip, along with corresponding targets and masks to handle variable sequence lengths through padding.</p>
</li>
<li><p>Model Setup: I implemented an encoder-only Transformer model, considering the sequential nature of the data. The model includes a feature embedding layer, positional encoding, and an encoder with specified layers and heads.</p>
</li>
<li><p>Training and Tuning: Using PyTorch, I prepared my data for training and employed Optuna for hyperparameter tuning, expecting that it would help find an optimal configuration for my model.
Despite these efforts, the model's performance hasn't improved as expected during training. I anticipated seeing a noticeable reduction in loss over epochs, reflecting the model's ability to better predict train punctuality. However, this hasn't been the case, and the performance seems stagnant over a large number of epochs.</p>
</li>
</ul>
<p>Could there be an issue with how I've set up my Transformer model for this specific problem, or might there be other aspects of my approach that are hindering the model's learning? I'm particularly concerned about the encoder-only design, data padding handling, or perhaps the way I've implemented the positional encoding and feature embedding in the context of predicting time-based outcomes. Any insights or suggestions to improve the model's performance would be greatly appreciated.</p>
","transformer-model"
"78219076","How to remove layers in Huggingface's transformers GPT2 pre-trained models?","2024-03-25 12:28:15","78219403","0","179","<python><machine-learning><deep-learning><nlp><transformer-model>","<p>My code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import GPT2Config, GPT2Model
from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(&quot;openai-community/gpt2&quot;)
print(decoder)
</code></pre>
<p>Here is the output of the console, listing the model architecture:</p>
<pre><code>GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
</code></pre>
<p>I want to remove the first layer:</p>
<pre><code>(wte): Embedding(50257, 768)
</code></pre>
<p>I've tried the following way:</p>
<pre class=""lang-py prettyprint-override""><code>def deleteEncodingLayers(model, num_layers_to_keep):  # must pass in the full bert model
    oldModuleList = model.bert.encoder.layer
    newModuleList = nn.ModuleList()

    # Now iterate over all layers, only keepign only the relevant layers.
    for i in range(0, len(num_layers_to_keep)):
        newModuleList.append(oldModuleList[i])

    # create a copy of the model, modify it with the new list, and return
    copyOfModel = copy.deepcopy(model)
    copyOfModel.bert.encoder.layer = newModuleList

    return copyOfModel
</code></pre>
<p>But it didn't work. Who knows how to fix it?</p>
","transformer-model"
"78210297","How to convert pretrained hugging face model to .pt and run it fully locally?","2024-03-23 09:18:49","78256222","-1","409","<machine-learning><pytorch><huggingface-transformers><transformer-model>","<p>I'm attempting to convert this <a href=""https://huggingface.co/UrukHan/wav2vec2-russian"" rel=""nofollow noreferrer"">model</a> in .pt format. It's working fine for me so i dont want to fine-tune it. How can i export it to .pt and run interface?</p>
<p>I tried using this to convert to .pt:</p>
<pre><code>from transformers import AutoConfig, AutoProcessor, AutoModelForCTC, AutoTokenizer, Wav2Vec2Processor
import librosa
import torch



# Define the model name
model_name = &quot;UrukHan/wav2vec2-russian&quot;

# Load the model and tokenizer
config = AutoConfig.from_pretrained(model_name)
model = AutoModelForCTC.from_pretrained(model_name, config=config)
processor = Wav2Vec2Processor.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Save the model as a .pt file
torch.save(model.state_dict(), &quot;model.pt&quot;)

# Save the tokenizer as well if needed
tokenizer.save_pretrained(&quot;model-tokenizer&quot;)
</code></pre>
<p>but unfortunately its not running the interface :</p>
<pre><code>model = AutoModelForCTC.from_pretrained(&quot;model.pt&quot;)
processor = AutoProcessor.from_pretrained(&quot;model.pt&quot;)


# Perform inference with the model
FILE = 'here is wav.wav'
audio, _ = librosa.load(FILE, sr = 16000)
audio = list(audio)
def map_to_result(batch):
  with torch.no_grad():
    input_values = torch.tensor(batch, device=&quot;cpu&quot;).unsqueeze(0) #, device=&quot;cuda&quot;
    logits = model(input_values).logits
  pred_ids = torch.argmax(logits, dim=-1)
  batch = processor.batch_decode(pred_ids)[0]
  return batch
map_to_result(audio)
print(map_to_result(audio))


model.eval()
</code></pre>
<p>And encountered an error:
`model.pt is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'</p>
<p>`</p>
","transformer-model"
"78208687","Inference question through LoRA in Whisper model","2024-03-22 20:25:33","78716708","0","155","<transformer-model><openai-whisper>","<p>I trained Whisper model through LoRA.<br />
But there's a problem.</p>
<p>The original model directory I trained has a capacity of 2.7G.
However, the size of the model directory learned through LoRA is 57M.</p>
<p>From this, I found that only additional weighting information was save to the LoRA directory.<br />
(That is, the original weighting information is not included.)</p>
<p>Therefore, this is a question.<br />
How can I combine the existing Whisper model with a model trained with LoRA for Inference?</p>
<p>Below, I'm attaching my code for your convenience.</p>
<pre><code>import numpy as np
import torch
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline
from peft import PeftModel, PeftConfig

class whisper:
    # model_str
    # 1. large - &quot;openai/whisper-large-v3&quot;
    # 2. medium - &quot;openai/whisper-medium&quot;
    # 3. small - &quot;openai/whisper-small&quot;
    def __init__(self, baseModelPath):
        device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;
        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        
        
        model = AutoModelForSpeechSeq2Seq.from_pretrained(baseModelPath, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True)
        model.to(device)

        processor = AutoProcessor.from_pretrained(baseModelPath)

        self.pipe = pipeline(
        &quot;automatic-speech-recognition&quot;,
        model=model,
        tokenizer=processor.tokenizer,
        feature_extractor=processor.feature_extractor,
        max_new_tokens=128,
        chunk_length_s=30,
        batch_size=16,
        return_timestamps=True,
        torch_dtype=torch_dtype,
        device=device,
        )

        
    # korean
    def getText(self, audioPath, language='&lt;|ko|&gt;'):
        sentence = self.pipe(audioPath, generate_kwargs={&quot;task&quot;:&quot;transcribe&quot;, &quot;language&quot;:language})
        return sentence['text']

</code></pre>
<p>There is some information related to <a href=""https://huggingface.co/docs/diffusers/en/tutorials/using_peft_for_inference"" rel=""nofollow noreferrer"">that link</a>, but I don't know if it can be applied to the Whisper model.</p>
<p>Below are the files in the original directory.</p>
<pre><code>-rw-r--r--  1 root root   34K Mar  4 17:49 added_tokens.json
-rw-r--r--  1 root root  1.4K Mar  5 09:48 config.json
-rw-r--r--  1 root root  3.0K Mar  5 09:48 generation_config.json
-rw-r--r--  1 root root  483K Mar  4 17:49 merges.txt
-rw-r--r--  1 root root  923M Mar  5 09:48 model.safetensors
-rw-r--r--  1 root root   52K Mar  4 17:49 normalizer.json
-rw-r--r--  1 root root  1.8G Mar  5 09:49 optimizer.pt
-rw-r--r--  1 root root   339 Mar  5 09:48 preprocessor_config.json
-rw-r--r--  1 root root   14K Mar  5 09:49 rng_state.pth
drwxr-xr-x  4 root root  4.0K Mar  4 17:49 runs
-rw-r--r--  1 root root  1.1K Mar  5 09:49 scheduler.pt
-rw-r--r--  1 root root  2.2K Mar  4 17:49 special_tokens_map.json
-rw-r--r--  1 root root  277K Mar  4 17:49 tokenizer_config.json
-rw-r--r--  1 root root   60K Mar  5 09:49 trainer_state.json
-rw-r--r--  1 root root  4.9K Mar  5 09:48 training_args.bin
-rw-r--r--  1 root root 1013K Mar  4 17:49 vocab.json
</code></pre>
<p>Below are the files in the LoRA-processed directory for the original files.</p>
<pre><code>drwxr-xr-x  3 root root  4.0K Mar 21 06:50 .
drwxr-xr-x 11 root root  4.0K Mar 21 13:16 ..
-rw-r--r--  1 root root  5.0K Mar 21 06:13 README.md
-rw-r--r--  1 root root   789 Mar 21 06:13 adapter_config.json
drwxr-xr-x  2 root root  4.0K Mar 21 06:13 adapter_model
-rw-r--r--  1 root root   14M Mar 21 06:13 adapter_model.safetensors
-rw-r--r--  1 root root   34K Mar 20 12:55 added_tokens.json
-rw-r--r--  1 root root  483K Mar 20 12:55 merges.txt
-rw-r--r--  1 root root   52K Mar 20 12:55 normalizer.json
-rw-r--r--  1 root root   28M Mar 21 06:13 optimizer.pt
-rw-r--r--  1 root root   339 Mar 21 06:13 preprocessor_config.json
-rw-r--r--  1 root root   14K Mar 21 06:13 rng_state.pth
-rw-r--r--  1 root root  1.1K Mar 21 06:13 scheduler.pt
-rw-r--r--  1 root root  2.2K Mar 20 12:55 special_tokens_map.json
-rw-r--r--  1 root root  277K Mar 20 12:55 tokenizer_config.json
-rw-r--r--  1 root root   31K Mar 21 06:13 trainer_state.json
-rw-r--r--  1 root root  4.9K Mar 21 06:13 training_args.bin
-rw-r--r--  1 root root 1013K Mar 20 12:55 vocab.json
</code></pre>
","transformer-model"
"78207480","is there any way to use RL for decoder only models","2024-03-22 16:05:50","","1","30","<nlp><transform><huggingface-transformers><reinforcement-learning><transformer-model>","<p>Could you please assist me in finding resources related to training models like BERT using RLHF? Additionally, I'm curious about the scarcity of research on applying reinforcement learning to decoder-only models. Are there any specific challenges or issues associated with this area that limit the research? Any guidance or insights would be greatly appreciated. Thank you!</p>
<p>while exploring trl library i found this issue (<a href=""https://github.com/huggingface/trl/issues/747"" rel=""nofollow noreferrer"">https://github.com/huggingface/trl/issues/747</a>)
so have brainstorming a lot about how to define the trajectories for this</p>
","transformer-model"
"78201090","What's the exact input size in MultiHead-Attention of BERT?","2024-03-21 15:25:57","","0","23","<bert-language-model><transformer-model><attention-model><multihead-attention>","<p>I just recently learned <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">BERT</a>.</p>
<p>Some tutorials show that after embedding a sentence, a matrix <strong>X of [seq_len, 768]</strong> will be formed, and <strong>X will be sent to MultiHead_Attention</strong>, that is, multiple Self-Attentions.</p>
<p>But in <a href=""https://github.com/NVIDIA/FasterTransformer"" rel=""nofollow noreferrer"">fasterTransformer</a>, why is the input [seq_len, head_num, size_per_head]? <strong>It seems that it divides the matrix X equally according to the number of heads</strong> and sends it to each head, instead of the complete matrix X.</p>
<p><strong>So what is the real input?</strong></p>
","transformer-model"
"78175886","Transformers // Predicting next transaction based on sequence of previous transactions // Sequence2One task","2024-03-17 15:44:18","","0","32","<nlp><time-series><transformer-model><chatgpt-api><seq2seq>","<p>We are solving the following task.
Our company has sequence of events like</p>
<p><strong>DATA:</strong>
1000$ / Oranges / 11.00 am</p>
<p>500$ / Car wash / 03.00 pm</p>
<p>15$ / Flowers / 09.00 pm</p>
<p><strong>TASK:</strong>
The task is - To predict next transaction based on previous sequence of transactions</p>
<p><strong>MY IDEA:</strong>
I think generative models with similar to GPT architecture can perform well in this task. I want the model to consider (N) transactions given as an input as prompt and train model to output 3 categories separately (sum / category / time).</p>
<p>I was looking for code or approaches to solve similar tasks on the internet, but found nothing?</p>
<p><strong>QUESTION:</strong></p>
<ol>
<li>Can anyone share of github code to solve a task like this?</li>
<li>Give suggestions on the approach and architecture?</li>
</ol>
<p>Thx a lot :)</p>
<p>I think generative models with similar to GPT architecture can perform well in this task. I want the model to consider (N) transactions given as an input as prompt and train model to output 3 categories separately (sum / category / time).</p>
","transformer-model"
"78171902","I was using colab: I want to run a .py file having argparse function to train a model","2024-03-16 13:16:01","","0","191","<python><kernel><google-colaboratory><transformer-model><pre-trained-model>","<pre><code>if __name__ == '__main__':
    torch.cuda.empty_cache()
    
    &quot;&quot;&quot;Parameters&quot;&quot;&quot;
    parser  = argparse.ArgumentParser(description = &quot;Emotion Classifier&quot; )
    parser.add_argument( &quot;--batch&quot;, type=int, help = &quot;batch_size&quot;, default = 1)
    
    parser.add_argument( &quot;--epoch&quot;, type=int, help = 'training epohcs', default = 10) # 12 for iemocap
    parser.add_argument( &quot;--norm&quot;, type=int, help = &quot;max_grad_norm&quot;, default = 10)
    parser.add_argument( &quot;--lr&quot;, type=float, help = &quot;learning rate&quot;, default = 1e-6) # 1e-5
    parser.add_argument( &quot;--sample&quot;, type=float, help = &quot;sampling trainign dataset&quot;, default = 1.0) # 

    parser.add_argument( &quot;--dataset&quot;, help = 'MELD or EMORY or iemocap or dailydialog', default = 'MELD')
    
    parser.add_argument( &quot;--pretrained&quot;, help = 'roberta-large or bert-large-uncased or gpt2 or gpt2-large or gpt2-medium', default = 'roberta-large')    
    parser.add_argument( &quot;--initial&quot;, help = 'pretrained or scratch', default = 'pretrained')
    parser.add_argument('-dya', '--dyadic', action='store_true', help='dyadic conversation')
    parser.add_argument('-fr', '--freeze', action='store_true', help='freezing PM')
    parser.add_argument( &quot;--cls&quot;, help = 'emotion or sentiment', default = 'emotion')
        
    args = parser.parse_args()
    
    logger = logging.getLogger(__name__)
    streamHandler = logging.StreamHandler()
    
    # Add the line below to fix the issue.
    sys.exit(main())
</code></pre>
<p>here's the error colab is showing</p>
<pre><code>usage: colab_kernel_launcher.py [-h] [--batch BATCH] [--epoch EPOCH] [--norm NORM] [--lr LR]
                                [--sample SAMPLE] [--dataset DATASET] [--pretrained PRETRAINED]
                                [--initial INITIAL] [-dya] [-fr] [--cls CLS]
colab_kernel_launcher.py: error: unrecognized arguments: /root/.local/share/jupyter/runtime/kernel-1fba85a1-85fd-4471-a790-9c103c9dcac9.json
An exception has occurred, use %tb to see the full traceback.

SystemExit: 2
/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.
  warn(&quot;To exit: use 'exit', 'quit', or Ctrl-D.&quot;, stacklevel=1)
</code></pre>
<p>I have a <code>train.py</code> file, that I want to run on my <code>colab</code>.</p>
","transformer-model"
"78171416","Feeding a Transformer with a matrix","2024-03-16 10:32:02","","0","21","<deep-learning><matrix-multiplication><transformer-model>","<p>I have a folder that has grayscale images of 32 x 32 pixel of handwritten characters. I divide each image into segments of size 8x8 pixels. Then each segment is then divided into sub-segments of size 2x2 pixels. I keep the original image location for each sub-segment. Normally I will result in a matrix of 16 rows and 16 columns, each element of which is a sub-segment of 2 x 2 pixels.
This is why I created a Transformer which accepts a 16 x 16 matrix as input.
Every time I start training this model, an error occurs: &quot;RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x1024 and 2x256)&quot;.
Help me to solve this issue please.</p>
","transformer-model"
"78167886","nn.TransformerDecoder output the same result from the second frames","2024-03-15 15:06:00","","0","22","<python><pytorch><transformer-model><encoder-decoder>","<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
from smplx import SMPL
from einops import rearrange
from models.loss import Loss
from transformers import CLIPProcessor, CLIPModel
from utils.utils import get_keypoints
from models.module import MusicEncoderLayer, MotionDecoderLayer
import math

class GPT(nn.Module):
    def __init__(self, p=2, \
                    input_size=438, embed_size=512, num_layers=6, heads=8, forward_expansion=4, dropout=0.1, output_size=75):
        super(GPT, self).__init__()

        max_len, max_per = 450, 6
        self.motion_pos_emb_t = nn.Parameter(torch.zeros(max_len, embed_size))
        # self.motion_pos_emb_p = nn.Parameter(torch.zeros(max_per, embed_size))
        # self.music_pose_emb_t = nn.Parameter(torch.zeros(max_len, embed_size))
        self.music_emb = nn.Linear(input_size, embed_size)
        self.motion_emb = nn.Linear(output_size, embed_size)
        self.text_encoder = TextEncoder()
        self.music_encoder = MusicEncoder(embed_size, num_layers, heads, forward_expansion, dropout)
        self.motion_decoder = MotionDecoder(embed_size, num_layers, heads, forward_expansion, dropout, output_size)

        # self.mask = generate_square_subsequent_mask(max_len, 'cuda')
        # self.mask = self.mask.masked_fill(self.mask==0, float('-inf')).masked_fill(self.mask==1, float(0.0))

        self.loss = nn.MSELoss()
        # self.loss = Loss()

    def forward(self, text, music, motion):

        
        motion_src, motion_trg = motion[:, :, :-1, :], motion[:, :, 1:, :]
        b, p, t, _ = motion_src.shape

        text_encode = self.text_encoder(text)
        music_encode = self.music_encoder(self.music_emb(music[:, :-1, :]))\
                            .reshape(b, 1, t, -1).repeat(1, p, 1, 1).reshape(b*p, t, -1)

        mask = torch.nn.Transformer().generate_square_subsequent_mask(t).transpose(0, 1).cuda()
        motion_emb = self.motion_emb(motion_src) + self.motion_pos_emb_t[:t, :].reshape(1, 1, t, -1).repeat(b, p, 1, 1)
        motion_pred = self.motion_decoder(motion_emb, music_encode, mask=mask).reshape(b, p, t, -1)

        loss = self.loss(motion_pred, motion_trg)

        return motion_pred, loss


    def inference(self, text, music, motion):
        self.eval()
        with torch.no_grad():
            music, motion = music[:, :-1, :], motion[:, :, :-1, :]
            b, p, t, c = motion.shape

            music_encode = self.music_encoder(self.music_emb(music))\
                                .reshape(b, 1, t, -1).repeat(1, p, 1, 1).reshape(b*p, t, -1)

            preds = torch.zeros(b, p, t, c).cuda()
            preds[:, :, 0, :] = motion[:, :, 0, :]
            mask = torch.nn.Transformer().generate_square_subsequent_mask(t).transpose(0, 1).cuda()
            for i in range(1, t):
                motion_emb = self.motion_emb(preds) + self.motion_pos_emb_t[:t, :].reshape(1, 1, t, -1).repeat(b, p, 1, 1)
                current_pred = self.motion_decoder(motion_emb, music_encode, mask=mask).reshape(b, p, t, -1)
                preds[:, :, i, :] += current_pred[:, :, i-1, :]

            motion_pred = preds.reshape(b, p, t, -1)
            print(motion_pred[0, 0, :10, :6])
            import sys
            sys.exit()
            pred_keypoints = get_keypoints(motion_pred)
    
        return {'keypoints': pred_keypoints, 'smpl': motion_pred}
    


class MusicEncoder(nn.Module):
    def __init__(self, embed_size, num_layers, heads, forward_expansion, dropout):
        super(MusicEncoder, self).__init__()
        self.layers = nn.ModuleList(
            [nn.TransformerEncoderLayer(d_model=embed_size, nhead=heads, dim_feedforward=embed_size*forward_expansion, \
                dropout=dropout, batch_first=True) for _ in range(num_layers)]
        )

    def forward(self, x):
        b, t, _ = x.shape
        out = x
        for layer in self.layers:
            out = layer(out)
        return out


class MotionDecoder(nn.Module):
    def __init__(self, embed_size, num_layers, heads, forward_expansion, dropout, output_size):
        super(MotionDecoder, self).__init__()
        self.num_layers = num_layers
        self.fc_out = nn.Linear(embed_size, output_size)
        self.layers = nn.ModuleList(
            [nn.TransformerDecoderLayer(d_model=embed_size, nhead=heads, dim_feedforward=embed_size*forward_expansion, \
                dropout=dropout, batch_first=True) for _ in range(num_layers)]
        )
        
    def forward(self, motion_src, music_text_encode, mask=None):
        b, p, t, _ = motion_src.shape
        out = motion_src.reshape(b*p, t, -1)
        for layer in self.layers:
            out = layer(out, music_text_encode, tgt_mask=mask)
        return self.fc_out(out)


class TextEncoder(nn.Module):
    def __init__(self):
        super(TextEncoder, self).__init__()
        self.text_clip = CLIPModel.from_pretrained(&quot;./Pretrained/CLIP/Model&quot;)
        self.text_processor = CLIPProcessor.from_pretrained(&quot;./Pretrained/CLIP/Processor&quot;)
        

    def forward(self, texts):
        texts_process = self.text_processor(text=texts, return_tensors=&quot;pt&quot;, padding=True, truncation=True)
        text_process = {name: tensor.to(self.text_clip.device) for name, tensor in texts_process.items()}
        text_output = self.text_clip.get_text_features(**text_process)

        return text_output

</code></pre>
<p><a href=""https://i.sstatic.net/wIXUO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wIXUO.png"" alt=""enter image description here"" /></a></p>
<p>I tend to finish a Music2Dance Task, Dance is the SMPL-Data, Music is a 439-dimension feature, and I have aligned their FPS.
The training loss is decrease, but the inferecne result is absolutely wrong, the frames after the second's is the same.
above is the error log and my code, please help me to find out the mistakes!</p>
<p>Thanks!</p>
","transformer-model"
"78164651","Using the ENCODE function","2024-03-15 04:17:25","","0","11","<deep-learning><transformer-model><huggingface>","<p>when running this code:
data = next(iter(train_dataset))</p>
<p>print(&quot;Example data from the dataset: \n&quot;, data)</p>
<hr />
<p>{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'id': '5733be284776f41900661182', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}</p>
<hr />
<p>and  when running this code:
train_ds=  train_dataset.map(encode)</p>
<p>valid_ds=  valid_dataset.map(encode)</p>
<p>ex = next(iter(train_ds))</p>
<p>print(&quot;Example data from the mapped dataset: \n&quot;, ex)</p>
<hr />
<p>{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']}, 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'decoder_attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'id': '5733be284776f41900661182', 'input_ids': [1525, 834, 526, 10, 304, 4068, 410, 8, 16823, 3790, 3, 18280, 2385, 16, 507, 3449, 16, 301, 1211, 1395, 1410, 58, 2625, 10, 30797, 120, 6, 8, 496, 65, 3, 9, 6502, 1848, 5, 71, 2916, 8, 5140, 5450, 31, 7, 2045, 22161, 19, 3, 9, 7069, 12647, 13, 8, 16823, 3790, 5, 3, 29167, 16, 851, 13, 8, 5140, 5450, 11, 5008, 34, 6, 19, 3, 9, 8658, 12647, 13, 2144, 28, 6026, 3, 76, 24266, 28, 8, 9503, 96, 553, 15, 7980, 1980, 1212, 13285, 1496, 1280, 3021, 12, 8, 5140, 5450, 19, 8, 23711, 2617, 13, 8, 3, 24756, 6219, 5, 3, 29167, 1187, 8, 20605, 2617, 19, 8, 8554, 17, 235, 6, 3, 9, 17535, 286, 13, 7029, 11, 9619, 5, 94, 19, 3, 9, 16455, 13, 8, 3, 3844, 17, 235, 44, 301, 1211, 1395, 6, 1410, 213, 8, 16823, 3790, 3, 28285, 26, 120, 4283, 12, 2788, 8942, 9, 26, 1954, 264, 8371, 8283, 16, 507, 3449, 5, 486, 8, 414, 13, 8, 711, 1262, 41, 232, 16, 3, 9, 1223, 689, 24, 1979, 7, 190, 220, 12647, 7, 11, 8, 2540, 10576, 15, 201, 19, 3, 9, 650, 6, 941, 3372, 12647, 13, 3790, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2788, 8942, 9, 26, 1954, 264, 8371, 8283, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'title': 'University_of_Notre_Dame'}</p>
<hr />
<p>The question is attention_mask': [1, 1, 1, 1, 1, .... Where does it come from and what does it mean and 'decoder_attention_mask': 'input_ids': Where do labels come from and why do they appear What does it mean?</p>
","transformer-model"
"78139547","Transformer for time series data","2024-03-11 09:23:29","","0","90","<deep-learning><time-series><transformer-model>","<p>For deep learning models such as cnn and lstm, in order to use them for time series data, we need to apply a rolling/sliding window to divide the dataset into segments and then feed it into the model, my question is: does this also apply to transofmers? or can I go straight without a sliding window?</p>
<p>I want to know, whether the transformer requries a sliding window of the dataset or not ?</p>
","transformer-model"
"78137217","How to enable Temporal Fusion Transformer (TFT) to return full output sequence?","2024-03-10 19:27:19","","0","63","<python-3.x><time-series><transformer-model><pytorch-lightning><pytorch-forecasting>","<p>I'm currently utilizing the Temporal Fusion Transformer (TFT) to predict nursing workload for each patient over the next 3 shifts in a hospital, using past workload data and additional features. My dataset comprises 30,000 patient time series of varying lengths.</p>
<p>The model should predict the workload for the subsequent shifts regardless of the patient's length of stay in the hospital. Thus, it should be able to predict each timestep within a patient's time series based on the corresponding past true values.</p>
<p>My goal is to evaluate how well the model performs in relation to the number of available past values. For this, the TFT needs to return the full output sequence, similar to the return_sequences=True parameter in LSTMs.
Could someone provide insights on how to enable this functionality in the TFT? Thank you very much!</p>
<p>I've already tried to divide each patient time series into &quot;sub-timeseries&quot; with increasing number of timesteps and then predicting the last three shifts. However, this results in a dataset that is far too large.</p>
","transformer-model"
"78129126","TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings)","2024-03-08 16:48:31","78135776","0","1038","<tensorflow><deep-learning><nlp><bert-language-model><transformer-model>","<p>My model was wholly workable two weeks back, but now it's showing the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-23-a3e5a45f06c9&gt; in &lt;cell line: 14&gt;()
     12 
     13 # Encode input using BERT model
---&gt; 14 bert_output = bert_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
     15 
     16 # Get pooled output and pass through dropout layer

8 frames
/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---&gt; 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

TypeError: Exception encountered when calling layer 'embeddings' (type TFBertEmbeddings).

Could not build a TypeSpec for name: &quot;tf.debugging.assert_less/assert_less/Assert/Assert&quot;
op: &quot;Assert&quot;
input: &quot;tf.debugging.assert_less/assert_less/All&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_0&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_1&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_2&quot;
input: &quot;Placeholder&quot;
input: &quot;tf.debugging.assert_less/assert_less/Assert/Assert/data_4&quot;
input: &quot;tf.debugging.assert_less/assert_less/y&quot;
attr {
  key: &quot;T&quot;
  value {
    list {
      type: DT_STRING
      type: DT_STRING
      type: DT_STRING
      type: DT_INT32
      type: DT_STRING
      type: DT_INT32
    }
  }
}
attr {
  key: &quot;summarize&quot;
  value {
    i: 3
  }
}
 of unsupported type &lt;class 'tensorflow.python.framework.ops.Operation'&gt;.

Call arguments received by layer 'embeddings' (type TFBertEmbeddings):
  • input_ids=&lt;KerasTensor: shape=(None, 50) dtype=int32 (created by layer 'input_ids')&gt;
  • position_ids=None
  • token_type_ids=&lt;KerasTensor: shape=(None, 50) dtype=int32 (created by layer 'token_type_ids')&gt;
  • inputs_embeds=None
  • past_key_values_length=0
  • training=False
</code></pre>
<p>I think this error occurs when the input layers are sent to the corresponding BERT layer. If I use old versions of TensorFlow instead of 2.15.0, the error is resolved. However, with those old versions, I did not get GPU and faced a Graph Execution Error.</p>
<p>Can anyone help me with this</p>
","transformer-model"
"78106785","When profiling using torch.autograd.profiler, the run time changes every time I added profiler.record_function to different plac","2024-03-05 09:56:15","","1","390","<pytorch><profiler><transformer-model>","<p>I'm trying to use torch.autograd.profiler to profile the run time of different steps in a multi head attention block. I added profiler.record_function to different places. But the run time changes every time I added record_function.</p>
<p>For example, I added one &quot;with profiler.record_function(&quot;SOFTMAX PASS&quot;):&quot; to the softax step, and I run the profiling and printed the results using:</p>
<pre><code>with profiler.profile(use_cuda=True, record_shapes=True, profile_memory=True) as prof:
    #with record_function(&quot;model_inference&quot;):
        a=multihead_attention(queries, keys, values)

print(prof.key_averages().table(sort_by=&quot;self_cpu_time_total&quot;, row_limit=10))
</code></pre>
<p>This profiler prints both CPU and CUDA run time. However, only the step with record_function will take CUDA run time, all other steps exists in CPU doesn't run on CUDA. Because I got the results that SOFTMAX PASS takes 100% of the CUDA run time and it takes 295us. Other events like cudaDeviceSynchronize takes 0us in CUDA</p>
<p>After I added &quot;with profiler.record_function(&quot;QK MATMUL PASS&quot;):&quot; to one matrix multiplication step, the results changed that: QK MATMUL PASS takes 79.2% of total CUDA time (811us), SOFTMAX PASS takes 20.80% (213us).</p>
<p>So I would like to know why the cuda time profile changes with different record function added.
Does it only shows cuda run time for steps with record_function added?
The most important question is: What should I do if I want to get all the steps' run time on CUDA? Is there any example or tutorial on it? I tried with the Pytorch document examples but it's not very detailed.
Also, what does this events mean? Where can I find documents for it</p>
<p><img src=""https://i.sstatic.net/RVRKf.png"" alt=""enter image description here"" /></p>
<p>My program is in colab here:
<a href=""https://colab.research.google.com/drive/1FGQ_tHNDxEDAGiaYilGIj5HPlEMiT5pp#scrollTo=AIHpP5g1SXSS"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1FGQ_tHNDxEDAGiaYilGIj5HPlEMiT5pp#scrollTo=AIHpP5g1SXSS</a></p>
","transformer-model"
"78103791","Streamlit with Pyinstaller issue","2024-03-04 20:02:15","","0","55","<pyinstaller><executable><streamlit><transformer-model><large-language-model>","<p>my streamlit app uses transformer and also i have no streamli and transformer installed in my global pip. I have my development using virtual env and when i am running pyinstaller created .exe file from my virtual environment, it is working fine, but when i am double clicking or opening the .exe file using powershell or cmd, it is not working.</p>
<p>Clicking .exe from cmd is opening and closing black and white window, i used streamlit and transformers in hidden import, still not working.</p>
","transformer-model"
"78102814","Custom Transformer Decoder Layer Implementation EECS598","2024-03-04 16:39:54","","0","71","<deep-learning><pytorch><transformer-model><encoder-decoder>","<p>I am working on the assignments for EECS598(I am not a student) - Deep Learning for Computer Vision taught by Justin Johnson, and my implementation does not seem to be working for the Decoder Layer implementation of Assignment 5, which can be found <a href=""https://web.eecs.umich.edu/%7Ejustincj/teaching/eecs498/WI2022/assignment5.html"" rel=""nofollow noreferrer"">here</a>. I have looked at other sources, and I am fairly sure that my implementation is right, so I am getting convinced everyday that the answer provided in the notebook is wrong.</p>
<p>My implementation of <code>DecoderBlock</code> currently is as follows, but there are other layers in this implementation that were implemented by me, and I might have something wrong there. All the other layers match with the expected answers in the notebook, so I highly doubt that.</p>
<p>Can someone please take a look at the notebook and let me know if they are able to get the expected answer on the notebook? Thank you!</p>
<pre><code>class DecoderBlock(nn.Module):
    def __init__(
        self, num_heads: int, emb_dim: int, feedforward_dim: int, dropout: float
    ):
        super().__init__()
        if emb_dim % num_heads != 0:
            raise ValueError(
                f&quot;&quot;&quot;The value emb_dim = {emb_dim} is not divisible
                             by num_heads = {num_heads}. Please select an
                             appropriate value.&quot;&quot;&quot;
            )

        &quot;&quot;&quot;
        The function implements the DecoderBlock for the Transformer model. In the 
        class we learned about encoder only model that can be used for tasks like 
        sequence classification but for more complicated tasks like sequence to 
        sequence we need a decoder network that can transformt the output of the 
        encoder to a target sequence. This kind of architecture is important in 
        tasks like language translation where we have a sequence as input and a 
        sequence as output. 
        
        As shown in the Figure 1 of the paper attention is all you need
        https://arxiv.org/pdf/1706.03762.pdf, the encoder consists of 5 components:   
        
        1. Masked MultiHead Attention
        2. MultiHead Attention
        3. FeedForward layer
        4. Residual connections after MultiHead Attention and feedforward layer
        5. LayerNorm        
        
        The Masked MultiHead Attention takes the target, masks it as per the 
        function get_subsequent_mask and then gives the output as per the MultiHead  
        Attention layer. Further, another Multihead Attention block here takes the  
        encoder output and the output from Masked Multihead Attention layer giving  
        the output that helps the model create interaction between input and 
        targets. As this block helps in interation of the input and target, it  
        is also sometimes called the cross attention.

        The architecture is as follows:
        
        inp - masked_multi_head_attention - out1 - layer_norm(inp + out1) - \
        dropout - (out2 and enc_out) -  multi_head_attention - out3 - \
        layer_norm(out3 + out2) - dropout - out4 - feed_forward - out5 - \
        layer_norm(out5 + out4) - dropout - out
        
        Here, out1, out2, out3, out4, out5 are the corresponding outputs for the 
        layers, enc_out is the encoder output and we add these outputs to their  
        respective inputs for implementing residual connections.
        
        args:
            num_heads: int value representing number of heads

            emb_dim: int value representing embedding dimension

            feedforward_dim: int representing hidden layers in the feed forward 
                model

            dropout: float representing the dropout value
        &quot;&quot;&quot;
        self.attention_self = None
        self.attention_cross = None
        self.feed_forward = None
        self.norm1 = None
        self.norm2 = None
        self.norm3 = None
        self.dropout = None
        self.feed_forward = None
        ##########################################################################
        # TODO: Initialize the following layers:                                 #
        # 1. Two MultiheadAttention layers with num_heads number of heads, emb_dim
        #     as the embedding dimension. As done in Encoder, you should be able to
        #     figure out the output dimension of both the MultiHeadAttention.    #
        # 2. One FeedForward block that takes in emb_dim as input dimension and  #
        #   feedforward_dim as hidden layers                                     #
        # 3. LayerNormalization layers after each of the block                   #
        # 4. Dropout after each of the block                                     #
        ##########################################################################

        # Replace &quot;pass&quot; statement with your code
        self.attention_self = MultiHeadAttention(num_heads=num_heads, dim_in=emb_dim, dim_out=emb_dim//num_heads)
        self.attention_cross = MultiHeadAttention(num_heads=num_heads, dim_in=emb_dim, dim_out=emb_dim//num_heads)
        self.layer_norm1 = LayerNormalization(emb_dim=emb_dim)
        self.layer_norm2 = LayerNormalization(emb_dim=emb_dim)
        self.layer_norm3 = LayerNormalization(emb_dim=emb_dim)
        self.feed_forward = FeedForwardBlock(inp_dim=emb_dim, hidden_dim_feedforward=feedforward_dim)
        self.dropout = nn.Dropout(p=dropout)
        ##########################################################################
        #               END OF YOUR CODE                                         #
        ##########################################################################

    def forward(
        self, dec_inp: Tensor, enc_inp: Tensor, mask: Tensor = None
    ) -&gt; Tensor:

        &quot;&quot;&quot;
        args:
            dec_inp: a Tensor of shape (N, K, M)
            enc_inp: a Tensor of shape (N, K, M)
            mask: a Tensor of shape (N, K, K)

        This function will handle the forward pass of the Decoder block. It takes
        in input as enc_inp which is the encoder output and a tensor dec_inp which
        is the target sequence shifted by one in case of training and an initial
        token &quot;BOS&quot; during inference
        &quot;&quot;&quot;
        y = None
        ##########################################################################
        # TODO: Using the layers initialized in the init function, implement the #
        # forward pass of the decoder block. Pass the dec_inp to the             #
        # self.attention_self layer. This layer is responsible for the self      #
        # interation of the decoder input. You should follow the Figure 1 in     #
        # Attention is All you need paper to implenment the rest of the forward  #
        # pass. Don't forget to apply the residual connections for different layers.
        ##########################################################################
        # Replace &quot;pass&quot; statement with your code
        out = self.attention_self(dec_inp, dec_inp, dec_inp, mask)
        out = self.dropout(self.layer_norm1(dec_inp + out))
        out2 = self.attention_cross(out, enc_inp, enc_inp)
        out = self.dropout(self.layer_norm2(out2 + out))
        out2 = self.feed_forward(out)
        y = self.dropout(self.layer_norm3(out + out2))
        ##########################################################################
        #               END OF YOUR CODE                                         #
        ##########################################################################
        return y
</code></pre>
","transformer-model"
"78098469","Python setup.py error: 'install_requires' must be a string or list of strings","2024-03-04 00:50:06","","0","169","<python><google-cloud-platform><deep-learning><pip><transformer-model>","<p>I am trying to build the Transformer Grammars code here: <a href=""https://github.com/google-deepmind/transformer_grammars/tree/main"" rel=""nofollow noreferrer"">https://github.com/google-deepmind/transformer_grammars/tree/main</a></p>
<p>I am running a Google Cloud Deep Learning VM, N1 instance, TensorFlow 2.10 (CUDA 11.3, Python 3.7).</p>
<p>Here is the contents of the <code>install.sh</code> file that I am trying to run:</p>
<pre><code>set verbose
set -o errexit

rm -rf .dependencies
mkdir .dependencies
cd .dependencies
git clone -b 20220623.1 https://github.com/abseil/abseil-cpp.git
git clone -b 3.4.0 https://gitlab.com/libeigen/eigen.git
git clone -b v2.10.2 https://github.com/pybind/pybind11.git

sudo apt-get install cmake build-essential pkg-config libgoogle-perftools-dev
git clone -b v0.1.97 https://github.com/google/sentencepiece.git
cd sentencepiece
mkdir build
cd build
make -j
sudo make install
sudo ldconfig -v

cd ../../..
pip install --require-hashes -r requirements.txt
pip install -e . --no-deps --no-index

rm -rf .dependencies
</code></pre>
<p>The error text reads:</p>
<pre><code>error in transformer_grammars setup command: 'install_requires' must be a string or list of strings containing valid project/version requirement specifiers; Invalid requirement, parse error at &quot;'\\'&quot;
</code></pre>
<p>In the <code>setup()</code> command in setup.py, the code pertaining to <code>install_requires</code> reads</p>
<pre><code>install_requires=dependencies
</code></pre>
<p>and <code>dependencies</code> is gotten as</p>
<pre><code>with open(&quot;requirements.txt&quot;, &quot;r&quot;) as f:
     dependencies = list(map(lambda x: x.strip(), f.readlines()))
</code></pre>
<p>Notably, this is at the bottom of <code>requirements.txt</code>:</p>
<pre><code># WARNING: The following packages were not pinned, but pip requires them to be
# pinned when the requirements file includes hashes. Consider using the --allow-unsafe flag.
# setuptools
</code></pre>
<p>I don't really know what this means but assumed it might be relevant because other posts I've seen that have a similar error to this involve setuptools.</p>
<p>Things I have tried to no avail:</p>
<ul>
<li>Upgrading pip</li>
<li>Upgrading setuptools</li>
<li>Reverting to an older version of setuptools (39.1.0)</li>
<li>Contacting relevant people at DeepMind</li>
</ul>
<p>Any help is greatly appreciated.</p>
","transformer-model"
"78092370","Understanding state_dict() in nn.Transformer of PyTorch","2024-03-02 10:58:39","","0","173","<pytorch><transformer-model>","<p>I am trying to interpret the transformer model. This is the structure of my model:</p>
<pre><code>self.src_mask = None
self.pos_encoder = PositionalEncoding(feature_size)
self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=nhead, dropout=dropout)
self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)   
self.decoder = nn.Linear(feature_size, output_size)
</code></pre>
<p>This is an encoder-only transformer model, with a linear decoder layer (please correct me if I'm wrong). I am trying to find the attention passed to the decoder from the encoder.</p>
<p>I tried using state_dict() to find the attention. These are the keys for the last encoder layer:</p>
<pre><code>- transformer_encoder.layers.2.self_attn.in_proj_weight
- transformer_encoder.layers.2.self_attn.in_proj_bias
- transformer_encoder.layers.2.self_attn.out_proj.weight
- transformer_encoder.layers.2.self_attn.out_proj.bias
- transformer_encoder.layers.2.linear1.weight
- transformer_encoder.layers.2.linear1.bias
- transformer_encoder.layers.2.linear2.weight
- transformer_encoder.layers.2.linear2.bias
- transformer_encoder.layers.2.norm1.weight
- transformer_encoder.layers.2.norm1.bias
- transformer_encoder.layers.2.norm2.weight
- transformer_encoder.layers.2.norm2.bias
</code></pre>
<p>I am wondering which of them is the attention passed to the decoder? As I understand, the attention should be the same length as the input. I tried looking for the documentation but couldn't find anything useful.</p>
<p>Thanks for the help!</p>
","transformer-model"
"78081304","Issue with Padding Mask in PyTorch Transformer Encoder","2024-02-29 12:08:16","78104180","2","736","<python><pytorch><transformer-model>","<p>I'm encountering an issue with the padding mask in PyTorch's Transformer Encoder. I'm trying to ensure that the values in the padded sequences do not affect the output of the model. However, even after setting the padded values to zeros in the input sequence, I'm still observing differences in the output.</p>
<p>Here's a simplified version of my code:</p>
<pre><code>import torch as th
from torch import nn

# Data
batch_size = 2
seq_len = 5
input_size = 16
src = th.randn(batch_size, seq_len, input_size)

# Set some values to a high value
src[0, 2, :] = 1000.0
src[1, 4, :] = 1000.0

# Generate a padding mask
padding_mask = th.zeros(batch_size, seq_len, dtype=th.bool)
padding_mask[0, 2] = 1
padding_mask[1, 4] = 1

# Pass the data through the encoder of the model
encoder = nn.TransformerEncoder(
    nn.TransformerEncoderLayer(
        d_model=input_size,
        nhead=1,
        batch_first=True,
    ),
    num_layers=1,
    norm=None,
)
out1000 = encoder(src, src_key_padding_mask=padding_mask)

# Modify the input data so that the masked vector does not affect
src[0, 2, :] = 0.0
src[1, 4, :] = 0.0

# Pass the modified data through the model
out0 = encoder(src, src_key_padding_mask=padding_mask)

# Check if the results are the same
assert th.allclose(
    out1000[padding_mask == 0],
    out0[padding_mask == 0],
    atol=1e-5,
)
</code></pre>
<p>Despite setting the padded values to zeros in the input sequence, I'm still observing differences in the output of the Transformer Encoder. Could someone please help me understand why this might be happening? How can I ensure that the values in the padded sequences do not affect the output of the model?</p>
","transformer-model"
"78075053","Issue with 'ValueError' when computing metrics in NER using transformers library (Tuple is empty)","2024-02-28 14:04:59","","1","32","<nlp><bert-language-model><named-entity-recognition><transformer-model>","<p>Description:
I am encountering issues while trying to compute metrics for Named Entity Recognition (NER) using the Hugging Face transformers library. The specific errors are 'ValueError' and I've been struggling to resolve them for quite some time.</p>
<p>Code:
I have a function compute_metrics that takes an EvalPrediction object as input. The function aims to process the predicted logits and true labels, but I'm facing issues with the structure of the EvalPrediction object.</p>
<p>Here is the relevant part of the code:</p>
<pre><code>from transformers.trainer_utils import EvalPrediction


def compute_metrics(eval_preds):
    &quot;&quot;&quot;
    Compute evaluation metrics for Named Entity Recognition (NER) tasks.

    Parameters:
    eval_preds (EvalPrediction): An object containing the predicted logits and the true labels.

    Returns:
    A dictionary containing precision, recall, F1 score, and accuracy.
    &quot;&quot;&quot;
    if not isinstance(eval_preds, EvalPrediction):
        raise ValueError(&quot;Invalid eval_preds structure. Expected an EvalPrediction object.&quot;)

    predictions = eval_preds.predictions

    if isinstance(predictions, tuple):
        if len(predictions) &gt; 0:
            pred_logits = predictions[0]
        else:
            raise ValueError(&quot;Tuple predictions is empty.&quot;)
    else:
        pred_logits = predictions

    # Ensure pred_logits has at least two dimensions
    if len(pred_logits.shape) == 1:
        pred_logits = np.expand_dims(pred_logits, axis=0)

    # Rest of your code...
    # Get predicted labels by argmax along the token dimension
    pred_labels = np.argmax(pred_logits, axis=2)
    # ... (rest of your code)
      # Filter out padding tokens where label is -100
    predictions = [
        [label_list[pred] for (pred, label) in zip(pred_label, true_label) if label != -100]
        for pred_label, true_label in zip(pred_labels, labels)
    ]

    # Filter out padding tokens in true labels
    true_labels = [
        [label_list[label] for label in true_label if label != -100]
        for true_label in labels
    ]

    # Compute metrics
    results = metric.compute(predictions=predictions, references=true_labels)

    return {
        &quot;precision&quot;: results[&quot;overall_precision&quot;],
        &quot;recall&quot;: results[&quot;overall_recall&quot;],
        &quot;f1&quot;: results[&quot;overall_f1&quot;],
        &quot;accuracy&quot;: results[&quot;overall_accuracy&quot;],
    }

trainer = Trainer(
    model,
    args,
   train_dataset=tokenized_datasets[&quot;train&quot;],
   eval_dataset=tokenized_datasets[&quot;validation&quot;],
   data_collator=data_collator,
   tokenizer=tokenizer,
   compute_metrics=compute_metrics
)

trainer.train()

</code></pre>
<p>Error Messages:
The errors I'm encountering are as follows:</p>
<pre><code>
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-103-3435b262f1ae&gt; in &lt;cell line: 1&gt;()
----&gt; 1 trainer.train()

5 frames
&lt;ipython-input-101-8c3cb1696dcb&gt; in compute_metrics(eval_preds)
     21             pred_logits = predictions[0]
     22         else:
---&gt; 23             raise ValueError(&quot;Tuple predictions is empty.&quot;)
     24     else:
     25         pred_logits = predictions

ValueError: Tuple predictions is empty.

</code></pre>
<p>Objective:
I'm seeking assistance to understand and resolve these errors. I suspect the issue might be related to the structure of the EvalPrediction object and how I'm accessing the predicted logits.</p>
<p>Additional Information:</p>
<p>I'm using the Hugging Face transformers library for NER.
I have verified that the input data and labels are correctly formatted.
My full code is <a href=""https://colab.research.google.com/drive/1yUGdIiPuB0-JnojCxbLnrRPAsv4HjZ2O?usp=sharing"" rel=""nofollow noreferrer"">here</a></p>
","transformer-model"
"78069814","Unusual behaviour with PyTorch transformer decoder layer","2024-02-27 18:32:06","","0","84","<python><deep-learning><pytorch><nlp><transformer-model>","<p>I was turning the decoder model code with pytorch transformer decoder layer and I am getting different loss even though I tried to match the implementation and the tokens are getting repetitive when using the pytorch transformer decoder layer rather the from scratch</p>
<p>The code below the the correct implementation of the decoder</p>
<pre><code>import torch
import torch.nn as nn
from torch.nn import functional as F

# hyperparameters
batch_size = 16 # how many independent sequences will we process in parallel?
block_size = 32 # what is the maximum context length for predictions?
max_iters = 5000
eval_interval = 100
learning_rate = 1e-3
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200
n_embd = 64
n_head = 4
n_layer = 4
dropout = 0.0
# ------------

torch.manual_seed(1337)

# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# here are all the unique characters that occur in this text
chars = sorted(list(set(text)))
vocab_size = len(chars)
# create a mapping from characters to integers
stoi = { ch:i for i,ch in enumerate(chars) }
itos = { i:ch for i,ch in enumerate(chars) }
encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers
decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string

# Train and test splits
data = torch.tensor(encode(text), dtype=torch.long)
n = int(0.9*len(data)) # first 90% will be train, rest val
train_data = data[:n]
val_data = data[n:]

# data loading
def get_batch(split):
    # generate a small batch of data of inputs x and targets y
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

u/torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

class Head(nn.Module):
    &quot;&quot;&quot; one head of self-attention &quot;&quot;&quot;

    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(n_embd, head_size, bias=False)
        self.query = nn.Linear(n_embd, head_size, bias=False)
        self.value = nn.Linear(n_embd, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        B,T,C = x.shape
        k = self.key(x)   # (B,T,C)
        q = self.query(x) # (B,T,C)
        # compute attention scores (&quot;affinities&quot;)
        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)
        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)
        wei = F.softmax(wei, dim=-1) # (B, T, T)
        wei = self.dropout(wei)
        # perform the weighted aggregation of the values
        v = self.value(x) # (B,T,C)
        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)
        return out

class MultiHeadAttention(nn.Module):
    &quot;&quot;&quot; multiple heads of self-attention in parallel &quot;&quot;&quot;

    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.proj = nn.Linear(n_embd, n_embd)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.proj(out))
        return out

class FeedFoward(nn.Module):
    &quot;&quot;&quot; a simple linear layer followed by a non-linearity &quot;&quot;&quot;

    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),
            nn.ReLU(),
            nn.Linear(4 * n_embd, n_embd),
            nn.Dropout(dropout),
        )

    def forward(self, x):
        return self.net(x)

class Block(nn.Module):
    &quot;&quot;&quot; Transformer block: communication followed by computation &quot;&quot;&quot;

    def __init__(self, n_embd, n_head):
        # n_embd: embedding dimension, n_head: the number of heads we'd like
        super().__init__()
        head_size = n_embd // n_head
        self.sa = MultiHeadAttention(n_head, head_size)
        self.ffwd = FeedFoward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)

    def forward(self, x):
        x = x + self.sa(self.ln1(x))
        x = x + self.ffwd(self.ln2(x))
        return x

# super simple bigram model
class TransformerDecoder(nn.Module):

    def __init__(self):
        super().__init__()
        # each token directly reads off the logits for the next token from a lookup table
        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)
        self.position_embedding_table = nn.Embedding(block_size, n_embd)
        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])
        self.ln_f = nn.LayerNorm(n_embd) # final layer norm
        self.lm_head = nn.Linear(n_embd, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape

        # idx and targets are both (B,T) tensor of integers
        tok_emb = self.token_embedding_table(idx) # (B,T,C)
        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)
        x = tok_emb + pos_emb # (B,T,C)
        x = self.blocks(x) # (B,T,C)
        x = self.ln_f(x) # (B,T,C)
        logits = self.lm_head(x) # (B,T,vocab_size)

        if targets is None:
            loss = None
        else:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            loss = F.cross_entropy(logits, targets)

        return logits, loss

    def generate(self, idx, max_new_tokens):
        # idx is (B, T) array of indices in the current context
        for _ in range(max_new_tokens):
            # crop idx to the last block_size tokens
            idx_cond = idx[:, -block_size:]
            # get the predictions
            logits, loss = self(idx_cond)
            # focus only on the last time step
            logits = logits[:, -1, :] # becomes (B, C)
            # apply softmax to get probabilities
            probs = F.softmax(logits, dim=-1) # (B, C)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)
            # append sampled index to the running sequence
            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)
        return idx

model = TransformerDecoder()
m = model.to(device)
# print the number of parameters in the model
print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')

# create a PyTorch optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

for iter in range(max_iters):

    # every once in a while evaluate the loss on train and val sets
    if iter % eval_interval == 0 or iter == max_iters - 1:
        losses = estimate_loss()
        print(f&quot;step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}&quot;)

    # sample a batch of data
    xb, yb = get_batch('train')

    # evaluate the loss
    logits, loss = model(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

# generate from the model
context = torch.zeros((1, 1), dtype=torch.long, device=device)
print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))
</code></pre>
<p>0.203279 M parameters
step 0: train loss 3.0684, val loss 3.0634
step 100: train loss 0.9685, val loss 0.9290
step 200: train loss 0.6753, val loss 0.6564
step 300: train loss 0.5599, val loss 0.5882
step 400: train loss 0.3871, val loss 0.5186
step 500: train loss 0.2547, val loss 0.3679
step 600: train loss 0.2038, val loss 0.2841
step 700: train loss 0.1839, val loss 0.2364
step 800: train loss 0.1704, val loss 0.2210
step 900: train loss 0.1627, val loss 0.2135
step 1000: train loss 0.1568, val loss 0.1984</p>
<p>This is the implementation with TransformerDecoder Layer with pytorch and I have problem with this implementation</p>
<pre><code>import torch
import torch.nn as nn
from torch.nn import functional as F

# hyperparameters
batch_size = 16  # how many independent sequences will we process in parallel?
block_size = 32  # what is the maximum context length for predictions?
max_iters = 1000
eval_interval = 100
learning_rate = 1e-3
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200
n_embd = 64
n_head = 4
n_layer = 4
dropout = 0.0
# ------------

torch.manual_seed(1337)

# Assuming input.txt is already downloaded and available
with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()

# here are all the unique characters that occur in this text
chars = sorted(list(set(text)))
vocab_size = len(chars)
# create a mapping from characters to integers
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}
encode = lambda s: [stoi[c] for c in s]  # encoder: take a string, output a list of integers
decode = lambda l: ''.join([itos[i] for i in l])  # decoder: take a list of integers, output a string

# Train and test splits
data = torch.tensor(encode(text), dtype=torch.long)
n = int(0.9 * len(data))  # first 90% will be train, rest val
train_data = data[:n]
val_data = data[n:]

# data loading
def get_batch(split):
    # generate a small batch of data of inputs x and targets y
    data = train_data if split == 'train' else val_data
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i + block_size] for i in ix])
    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])
    x, y = x.to(device), y.to(device)
    return x, y

u/torch.no_grad()
def estimate_loss():
    out = {}
    model.eval()
    for split in ['train', 'val']:
        losses = torch.zeros(eval_iters)
        for k in range(eval_iters):
            X, Y = get_batch(split)
            logits, loss = model(X, Y)
            losses[k] = loss.item()
        out[split] = losses.mean()
    model.train()
    return out

class TransformerDecoderModel(nn.Module):
    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout=0.1):
        super().__init__()
        self.vocab_size = vocab_size
        self.n_embd = n_embd
        self.n_head = n_head
        self.n_layer = n_layer
        self.block_size = block_size

        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        self.position_embedding = nn.Embedding(block_size, n_embd)

        decoder_layer = nn.TransformerDecoderLayer(d_model=n_embd, nhead=n_head, dropout=dropout)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layer)

        self.ln_f = nn.LayerNorm(n_embd)  # Final LayerNorm
        self.lm_head = nn.Linear(n_embd, vocab_size, bias=False)

    def generate_square_subsequent_mask(self, sz):
        mask = torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)
        return mask

    def forward(self, idx, targets=None):
        B, T = idx.shape
        device = idx.device

        # Create mask with shape [block_size, block_size]
        mask = self.generate_square_subsequent_mask(T).to(device)

        tok_emb = self.token_embedding(idx)  # [B, T, C]
        pos_emb = self.position_embedding(torch.arange(T, dtype=torch.long, device=device))  # [T, C]
        pos_emb = pos_emb.unsqueeze(0).repeat(B, 1, 1)  # Expand to match batch size: [B, T, C]
        x = tok_emb + pos_emb

        # Transformer requires input shape as [T, B, C]
        x = x.permute(1, 0, 2)  # [T, B, C]
        x = self.transformer_decoder(x, x, tgt_mask=mask)

        # Revert shape to [B, T, C] for the linear layer
        x = x.permute(1, 0, 2)

        x = self.ln_f(x)
        logits = self.lm_head(x)

        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, self.vocab_size), targets.view(-1))
            return logits, loss
        else:
            return logits, None

    def generate(self, idx, max_new_tokens):
        idx = idx.to(device)
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -self.block_size:]
            logits = self.forward(idx_cond)[0]
            probs = F.softmax(logits[:, -1, :], dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat([idx, idx_next], dim=1)
        return idx


model = TransformerDecoderModel(vocab_size=vocab_size, n_embd=n_embd, n_head=n_head, n_layer=n_layer, block_size=block_size, dropout=dropout)

m = model.to(device)
# print the number of parameters in the model
print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')

# create a PyTorch optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

for iter in range(max_iters):

    # every once in a while evaluate the loss on train and val sets
    if iter % eval_interval == 0 or iter == max_iters - 1:
        losses = estimate_loss()
        print(f&quot;step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}&quot;)

    # sample a batch of data
    xb, yb = get_batch('train')

    # evaluate the loss
    logits, loss = model(xb, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()

# generate from the model
context = torch.zeros((1, 1), dtype=torch.long, device=device)
print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))
</code></pre>
<p>step 0: train loss 3.1871, val loss 3.2023
step 100: train loss 0.9042, val loss 0.8612
step 200: train loss 0.1210, val loss 0.1222
step 300: train loss 0.0269, val loss 0.0262
step 400: train loss 0.0178, val loss 0.0231
step 500: train loss 0.0153, val loss 0.0221
step 600: train loss 0.0072, val loss 0.0173
step 700: train loss 0.0110, val loss 0.0102
step 800: train loss 0.0027, val loss 0.0041
step 900: train loss 0.0019, val loss 0.0015
step 999: train loss 0.0006, val loss 0.0006</p>
<p>even though I implemented one thing form scratch and the other with transformer decoder layers both loss is different and I see PyTorch transformer decoder implementation wrong as its generating same token repeatedly why?</p>
","transformer-model"
"78066932","Issue with AttributeError in PyTorch TransformerEncoderLayer","2024-02-27 10:47:55","","0","296","<python><pytorch><transformer-model>","<p>Description:</p>
<p>Hello everyone,</p>
<p>I am encountering an issue while working with PyTorch's TransformerEncoderLayer, and I'm seeking some assistance from the community to resolve it.</p>
<p>Problem:
I am attempting to use PyTorch's TransformerEncoderLayer in my code, specifically in the context of a Transformer-based model. However, when I try to run the code, I encounter the following AttributeError:</p>
<pre><code>Traceback (most recent call last):
  File &quot;E:/A_projects/Python/expTest/train.py&quot;, line 208, in &lt;module&gt;
    train_eval(epoch)
  File &quot;E:/A_projects/Python/expTest/train.py&quot;, line 127, in train_eval
    rppg = model(video)
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\module.py&quot;, line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;E:\A_projects\Python\expTest\models\Sformer.py&quot;, line 130, in forward
    features = self.temporal_encoder(x)
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\module.py&quot;, line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;E:\A_projects\Python\expTest\models\Sformer.py&quot;, line 46, in forward
    token = self.transformer_enc(x)
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\module.py&quot;, line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\transformer.py&quot;, line 388, in forward
    output = mod(output, src_mask=mask, is_causal=is_causal,
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\module.py&quot;, line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\transformer.py&quot;, line 707, in forward
    if self.norm_first:
  File &quot;D:\CondaVirtualEnvironment\envs\cuda118py38\lib\site-packages\torch\nn\modules\module.py&quot;, line 1695, in __getattr__
    raise AttributeError(f&quot;'{type(self).__name__}' object has no attribute '{name}'&quot;)
AttributeError: 'TransformerEncoderLayer' object has no attribute 'norm_first'

Process finished with exit code 1

</code></pre>
<p>Context:
The error occurs when I call the forward() method of my model, which includes a TransformerEncoderLayer. It seems to be related to an attribute called norm_first, which is not recognized by the TransformerEncoderLayer class.</p>
<p>Request for Help:
I have reviewed the PyTorch documentation and other resources, but I'm unable to identify the cause of this error. I'm reaching out to the community for assistance in understanding why this error is occurring and how I can resolve it.</p>
<p>Additional Information:</p>
<p>I am using PyTorch version 2.1.0.
The code snippet where the error occurs is related to the application of the TransformerEncoderLayer within my model's forward pass.
I have also checked for any potential issues with my installation or environment, but everything seems to be in order.
Any insights or suggestions on how to troubleshoot and fix this issue would be greatly appreciated.</p>
<p>Thank you in advance for your help!</p>
<p>Best regards!
<a href=""https://i.sstatic.net/ajyw3.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ajyw3.jpg"" alt=""Related codes"" /></a></p>
<p>I have tried many methods but none of them have solved the problem. I beg the experts to help me clarify.</p>
<hr />
<p>I have organized the code, and below is a simplified portion of it. I have annotated the part that is causing the error.</p>
<p>This is the code for the training section:</p>
<pre><code>
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

'''
Due to space constraints, only the relevant key code for the training part is retained here.
'''

ac_train = AC_RPPG(video_length=video_length, image_size=img_size, source_file=ac_train_source_file,
                       base_dir=ac_base_dir, target_fps=target_fps, bvp_length=bvp_length)

train_dataset = MergedSet([ac_train])

# dataloader
train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=0)

model = RPPGSFormer(num_frames=video_length * target_fps, signal_length=bvp_length)

model = model.to(device)

for epoch in range(epochs):
    train_eval(epoch)


def train_eval(epoch):
    global eval_times, best_model, min_eval_loss
    epoch_loss = 0.
    start_time = time.time()
    for batch_id, (video, bvp, hr, fps, bvp_sampling_rate) in enumerate(train_loader):
        # to device
        video = video.float().to(device)
        bvp = bvp.float().to(device)

        # predict
        # The code encountered an error at this point in execution,
        # and the error message describes the specific issue mentioned in this problem.
        rppg = model(video)
</code></pre>
<p>This is the structural code related to the model:</p>
<pre><code>class RPPGSFormer(nn.Module):
    def __init__(self, num_frames=96, signal_length=96, pretrained=True, **kwargs):
        super().__init__()
        self.embed_dim = 192
        self.signal_length = signal_length
        self.vit = VisionTransformer(patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4,
                                     qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
        if pretrained:
            path = r'E:\A_projects\Python\expTest\saved_models\deit_tiny_patch16_224-a1311bcf.pth'
            checkpoint = torch.load(path)
            self.vit.load_state_dict(checkpoint[&quot;model&quot;])
        self.vit.head = nn.Identity()
        self.temporal_encoder = TemporalEncoder(clip_length=num_frames, embed_dim=self.embed_dim, n_layers=6)

        # Omit part of the code...

    def forward(self, x):
        '''
            x.shape == [B, C, T, H, W]
        '''
        B, C, T, H, W = x.shape
        x = x.permute(0, 2, 1, 3, 4)
        x = x.reshape(B * T, C, H, W)  # [B*T, 3, H, W]
        x = self.vit(x)  # [B*T, 192]

        features = self.temporal_encoder(x)
        rppg = self.head(features)
        return rppg

def trunc_normal_(x, mean=0., std=1.): 
    return x.normal_().fmod_(2).mul_(std).add_(mean)

class TemporalEncoder(nn.Module):
    def __init__(self, clip_length=None, embed_dim=2048, n_layers=6):
        super(TemporalEncoder, self).__init__()
        self.clip_length = clip_length
        enc_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8)

        # The error can be traced back to this point.
        self.transformer_enc = nn.TransformerEncoder(enc_layer, num_layers=n_layers, norm=nn.LayerNorm(embed_dim))
        # ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑
        
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, clip_length + 1, embed_dim))
        with torch.no_grad():
            trunc_normal_(self.pos_embed, std=.02)
            trunc_normal_(self.cls_token, std=.02)
        self.apply(self._init_weights)

    def forward(self, x):
        batch_size = x.shape[0] // self.clip_length
        x = x.reshape((batch_size, self.clip_length, -1))
        cls_tokens = self.cls_token.expand(batch_size, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)  # [B, T+1, D]
        x = x + self.pos_embed
        x.transpose_(1, 0)

        # The error can be traced back to this point.
        token = self.transformer_enc(x)
        return token[0]
</code></pre>
","transformer-model"
"78062739","HuggingFace Transformers Error When Saving Model: TypeError: Object of type method is not JSON serializable","2024-02-26 17:14:47","","0","323","<pytorch><huggingface-transformers><transformer-model>","<p>I get the following error when trying to save a BERT-based model (astroBERT; also observed with sciBERT):</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[15], line 1
----&gt; 1 trainer.train()
      2 trainer.save_model(f&quot;astrobert-output/ft-{model_name}-{run_name}-final&quot;)

File ~/miniconda3/envs/paper-class/lib/python3.11/site-packages/transformers/trainer.py:1624, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1622         hf_hub_utils.enable_progress_bars()
   1623 else:
-&gt; 1624     return inner_training_loop(
   1625         args=args,
   1626         resume_from_checkpoint=resume_from_checkpoint,
   1627         trial=trial,
   1628         ignore_keys_for_eval=ignore_keys_for_eval,
   1629     )

File ~/miniconda3/envs/paper-class/lib/python3.11/site-packages/transformers/trainer.py:2029, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   2026     self.state.epoch = epoch + (step + 1 + steps_skipped) / steps_in_epoch
   2027     self.control = self.callback_handler.on_step_end(args, self.state, self.control)
-&gt; 2029     self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
   2030 else:
   2031     self.control = self.callback_handler.on_substep_end(args, self.state, self.control)

File ~/miniconda3/envs/paper-class/lib/python3.11/site-packages/transformers/trainer.py:2423, in Trainer._maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
   2420         self.lr_scheduler.step(metrics[metric_to_check])
   2422 if self.control.should_save:
-&gt; 2423     self._save_checkpoint(model, trial, metrics=metrics)
   2424     self.control = self.callback_handler.on_save(self.args, self.state, self.control)

File ~/miniconda3/envs/paper-class/lib/python3.11/site-packages/transformers/trainer.py:2499, in Trainer._save_checkpoint(self, model, trial, metrics)
   2497 else:
   2498     staging_output_dir = os.path.join(run_dir, f&quot;tmp-{checkpoint_folder}&quot;)
-&gt; 2499 self.save_model(staging_output_dir, _internal_call=True)
   2501 if not self.args.save_only_model:
   2502     # Save optimizer and scheduler
   2503     self._save_optimizer_and_scheduler(staging_output_dir)

File ~/miniconda3/envs/paper-class/lib/python3.11/site-packages/transformers/trainer.py:3016, in Trainer.save_model(self, output_dir, _internal_call)
   3013         self.model_wrapped.save_checkpoint(output_dir)
   3015 elif self.args.should_save:
-&gt; 3016     self._save(output_dir)
   3018 # Push to the Hub when `save_model` is called by the user.
   3019 if self.args.push_to_hub and not _internal_call:

File ~/miniconda3/envs/paper-class/lib/python3.11/site-packages/transformers/trainer.py:3094, in Trainer._save(self, output_dir, state_dict)
   3089     self.model.save_pretrained(
   3090         output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors
   3091     )
   3093 if self.tokenizer is not None:
-&gt; 3094     self.tokenizer.save_pretrained(output_dir)
   3096 # Good practice: save your training arguments together with the trained model
   3097 torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))

File ~/miniconda3/envs/paper-class/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2464, in PreTrainedTokenizerBase.save_pretrained(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)
   2462     print(&quot; &quot;)
   2463 with open(tokenizer_config_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:
-&gt; 2464     out_str = json.dumps(tokenizer_config, indent=2, sort_keys=True, ensure_ascii=False) + &quot;\n&quot;
   2465     f.write(out_str)
   2466 logger.info(f&quot;tokenizer config file saved in {tokenizer_config_file}&quot;)

File ~/miniconda3/envs/paper-class/lib/python3.11/json/__init__.py:238, in dumps(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)
    232 if cls is None:
    233     cls = JSONEncoder
    234 return cls(
    235     skipkeys=skipkeys, ensure_ascii=ensure_ascii,
    236     check_circular=check_circular, allow_nan=allow_nan, indent=indent,
    237     separators=separators, default=default, sort_keys=sort_keys,
--&gt; 238     **kw).encode(obj)

File ~/miniconda3/envs/paper-class/lib/python3.11/json/encoder.py:202, in JSONEncoder.encode(self, o)
    200 chunks = self.iterencode(o, _one_shot=True)
    201 if not isinstance(chunks, (list, tuple)):
--&gt; 202     chunks = list(chunks)
    203 return ''.join(chunks)

File ~/miniconda3/envs/paper-class/lib/python3.11/json/encoder.py:432, in _make_iterencode.&lt;locals&gt;._iterencode(o, _current_indent_level)
    430     yield from _iterencode_list(o, _current_indent_level)
    431 elif isinstance(o, dict):
--&gt; 432     yield from _iterencode_dict(o, _current_indent_level)
    433 else:
    434     if markers is not None:

File ~/miniconda3/envs/paper-class/lib/python3.11/json/encoder.py:406, in _make_iterencode.&lt;locals&gt;._iterencode_dict(dct, _current_indent_level)
    404         else:
    405             chunks = _iterencode(value, _current_indent_level)
--&gt; 406         yield from chunks
    407 if newline_indent is not None:
    408     _current_indent_level -= 1

File ~/miniconda3/envs/paper-class/lib/python3.11/json/encoder.py:439, in _make_iterencode.&lt;locals&gt;._iterencode(o, _current_indent_level)
    437         raise ValueError(&quot;Circular reference detected&quot;)
    438     markers[markerid] = o
--&gt; 439 o = _default(o)
    440 yield from _iterencode(o, _current_indent_level)
    441 if markers is not None:

File ~/miniconda3/envs/paper-class/lib/python3.11/json/encoder.py:180, in JSONEncoder.default(self, o)
    161 def default(self, o):
    162     &quot;&quot;&quot;Implement this method in a subclass such that it returns
    163     a serializable object for ``o``, or calls the base implementation
    164     (to raise a ``TypeError``).
   (...)
    178 
    179     &quot;&quot;&quot;
--&gt; 180     raise TypeError(f'Object of type {o.__class__.__name__} '
    181                     f'is not JSON serializable')

TypeError: Object of type method is not JSON serializable
</code></pre>
<p>Here is an MRE excluding data (the problem persists across different datasets):</p>
<pre><code>model_checkpoint = &quot;adsabs/astroBERT&quot;
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_special_tokens=True, do_lower_case=False, use_fast=False)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, problem_type=&quot;multi_label_classification&quot;, num_labels=num_labels, id2label=id2label, label2id=label2id)

trainer = Trainer(
    model=model,
    train_dataset=encoded_dataset[&quot;train&quot;],
    eval_dataset=encoded_dataset[&quot;valid&quot;],
    tokenizer=tokenizer)

trainer.train()
trainer.save_model(f&quot;model&quot;)
</code></pre>
<p>The error occurs when the model attempts to save, either through trainer.train() or trainer.save_model(). I expected the model to save without error from either attempt. I have dug through the transformers source code, namely tokenization_utils_base.py and found that the 'add_special_tokens' attribute of the 'tokenizer_config' object is of type &lt;class 'method'&gt;. I don't know why this is the case because above, when I define the tokenizer, I set 'add_special_tokens' to be simply True (same problem when I set this to be False as well). When I print add_special_tokens within tokenization_utils_base.py, I see this:</p>
<pre><code>&lt;bound method SpecialTokensMixin.add_special_tokens of BertTokenizer(name_or_path='adsabs/astroBERT', vocab_size=30000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={
    16338: AddedToken(&quot;[CLS]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    16339: AddedToken(&quot;[MASK]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    16340: AddedToken(&quot;[PAD]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    16341: AddedToken(&quot;[SEP]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
    16342: AddedToken(&quot;[UNK]&quot;, rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}&gt;
</code></pre>
<p>Again, not sure where this is coming from. I'm using version 4.38.1 of transformers. What is the cause of this error? Could there be a bug in the transformers package?</p>
","transformer-model"
"78061572","How is it possible to use a pre-trained ViT backbone of a masked autoencoder in downstream tasks?","2024-02-26 14:13:43","","0","351","<transformer-model><vision-transformer>","<p>When pre-training a ViT backbone through a masked autoencoder architecture, the input patches are randomly masked and the unsmasked patches are fed to the encoder layers of the ViT, as shown in the KERAS tutorial on (<a href=""https://keras.io/examples/vision/masked_image_modeling/"" rel=""nofollow noreferrer"">masked image modeling</a>.</p>
<p>If I understood correctly, the approach is to pre-train the ViT backbone in the masked image modeling paradigm and then use the pre-trained ViT (which includes the transformer encoder blocks) for a downstream model, which may serve a specific downstream task, like image classification for example, by using the CLS-token aproach or some sort of pooling operation. The question that bothers me is now:</p>
<p>When adopting the pre-trained layers for the downstream task one is usually not masking anything, thus the input shape for the encoder has to change. Since the encoders are transformer encoder blocks, utilizing self-attention (or mulit-head attention, actually), where wheight matrices for query, key and value are initialized and optimized during pre-training, how is it possible to use this layer when the input is now the whole image, or respectively the whole sequence of embedded patches?</p>
<p>For example (for simplicity, lets not consider batch_size here) we have an image, which gets split into let's say 100 (16x16) patches, then one obtains an input sequence of shape [100, embedding_dimension]. Then, say, 80% of the patches are getting masked, so we have masked patches with shape [80, embedding_dimension] and unmasked patches of shape [20, embedding_dimension]. We feed the unmasked patches to the encoder layers performing multi-head-attention, and train the network as proposed in the keras tutorial (or the original paper, as you like).
When switching to the downstream task lets consider again images of the same size, so we have again input to the model of shape [100, embedding_dimension]. But we're not masking anymore, so the encoder receives the whole sequence of 100 patches.</p>
<p>How does that fit the pre-trained weight matrices? What am I getting wrong here? Thanks for your help in advance!</p>
<p>I tried implemeningt this in tensorflow and was able to pre-train the masked autoencoder with a masking rate of .8, then extracting the encoder layers of the pre-trained model and plug it into a downstream model, which gets the whole sequence as input. No errors were thrown ...</p>
","transformer-model"
"78059837","unable to implement tgt_mask and tgt_key_padding mask properly in transformer decoder model","2024-02-26 09:28:41","","1","132","<python><pytorch><nlp><transformer-model><attention-model>","<p>I have implemented a transformer decoder for next token prediction. I am passing the tgt_mask and tgt_key_padding mask to not attend the future tokens and to ignore the padding. but I am constantly receiving the error</p>
<pre><code>Training Epoch 1/1:   0%|                                                                                       | 0/563 [00:00&lt;?, ?it/s]

tgt_emb torch.Size([16, 875, 256])
tgt_mask torch.Size([875, 875])
padding_mask torch.Size([16, 875])

/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Training Epoch 1/1:   0%|                                                                                       | 0/563 [00:01&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;/scratch/harsha.vasamsetti/decoder_aug_made/main.py&quot;, line 146, in &lt;module&gt;
    output = model(input_batch)
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/scratch/harsha.vasamsetti/decoder_aug_made/transformer.py&quot;, line 47, in forward
    output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=padding_mask)
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/transformer.py&quot;, line 460, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/transformer.py&quot;, line 846, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/transformer.py&quot;, line 855, in _sa_block
    x = self.self_attn(x, x, x,
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/modules/activation.py&quot;, line 1241, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File &quot;/home2/harsha.vasamsetti/miniconda3/envs/slices/lib/python3.9/site-packages/torch/nn/functional.py&quot;, line 5318, in multi_head_attention_forward
    raise RuntimeError(f&quot;The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.&quot;)
RuntimeError: The shape of the 2D attn_mask is torch.Size([875, 875]), but should be (16, 16).
</code></pre>
<p>I read the PyTorch documentation and understood the shape of <code>tgt_mask</code> should be <code>(T,T)</code> which is the lenght if the sequence and <code>tgt_key_padding mask</code> should be <code>(batch_size,T)</code> as you can see the shapes in the error I am exactly passing the same shape but I am receiving the error I dont know were I am doing wrong.</p>
<pre class=""lang-py prettyprint-override""><code>import torch.nn as nn
import torch
import math


# Define the Transformer model class
class TransformerModel(nn.Module):
    def __init__(self, vocab_size,pad_idx, n_embd, n_head, n_layers, max_length, dropout=0.1):
        super().__init__()
        self.pad_idx = pad_idx  # Add this line

        self.embed = nn.Embedding(vocab_size, n_embd)
        decoder_layer = nn.TransformerDecoderLayer(d_model=n_embd, nhead=n_head, dropout=dropout)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)
        self.pos_encoder = PositionalEncoding(n_embd, dropout, max_length)
        self.n_embd = n_embd
        self.generator = nn.Linear(n_embd, vocab_size)

    def generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(self, tgt):
            tgt_emb = self.pos_encoder(self.embed(tgt) * math.sqrt(self.n_embd))  # (batch_size, seq_len, emb_dim)
            print(&quot;tgt_emb&quot;, tgt_emb.shape)
            tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)  # Adjusted to tgt.size(1) for seq_len
            print(&quot;tgt_mask&quot;, tgt_mask.shape)

            # Create padding mask based on EOS token used for padding
            if self.pad_idx is not None:
                padding_mask = (tgt == self.pad_idx)  # (batch_size, seq_len)
                print(&quot;padding_mask&quot;, padding_mask.shape)
            else:
                padding_mask = None

            memory = torch.zeros_like(tgt_emb)  # Simplified memory initialization
            output = self.transformer_decoder(tgt_emb, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=padding_mask)
            return self.generator(output.transpose(0, 1))  # Adjust generator input if necessary

# Positional encoding class adds information about the order of tokens
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)
</code></pre>
","transformer-model"
"78059186","How to calculate the Mult-Adds of transformer as mentioned in Lite Transformer","2024-02-26 07:25:51","","0","14","<time-complexity><transformer-model>","<p>How to calculate the Mult-Adds of transformer as mentioned in Lite Transformer so that For the attention layer, the Mult-Adds would be O(4N d2 + N 2d); for FFN, the Mult-Adds is O(2 × 4N d2).<a href=""https://i.sstatic.net/DEgvg.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.sstatic.net/wxxQc.jpg"" rel=""nofollow noreferrer"">enter image description here</a></p>
","transformer-model"
"78028249","How to replace the softmax function by an approximate softmax function inside a transformer application like NMT","2024-02-20 14:44:28","","0","27","<transformer-model><softmax>","<p>To calculate the BLEU score of both accurate and approximate softmax and compare.</p>
","transformer-model"
"77987097","Error Message like RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation","2024-02-13 10:12:30","","0","54","<python-3.x><nlp><huggingface-transformers><transformer-model><accelerate>","<p>I'm trying to run the transformer model, which is listed on huggingface. My code is identical to link below <a href=""https://medium.com/@pazuzzu/in-depth-llm-fine-tuning-guide-efficiently-fine-tune-and-use-zephyr-7b-beta-assistant-using-lora-e23d8151e067"" rel=""nofollow noreferrer"">https://medium.com/@pazuzzu/in-depth-llm-fine-tuning-guide-efficiently-fine-tune-and-use-zephyr-7b-beta-assistant-using-lora-e23d8151e067</a></p>
<p>Whenever I tried to run above code in my local gpu server, Error message such as</p>
<pre><code>Traceback (most recent call last):
  File &quot;Main.py&quot;, line 163, in &lt;module&gt;
    trainer.train()
  File &quot;/home/jhkcool97/.local/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1561, in train
    return inner_training_loop(
  File &quot;/home/jhkcool97/.local/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 1895, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File &quot;/home/jhkcool97/.local/lib/python3.8/site-packages/transformers/trainer.py&quot;, line 2830, in training_step
    self.accelerator.backward(loss)
  File &quot;/home/jhkcool97/.local/lib/python3.8/site-packages/accelerate/accelerator.py&quot;, line 1964, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File &quot;/home/jhkcool97/.local/lib/python3.8/site-packages/torch/_tensor.py&quot;, line 522, in backward
    torch.autograd.backward(
  File &quot;/home/jhkcool97/.local/lib/python3.8/site-packages/torch/autograd/__init__.py&quot;, line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File &quot;/home/jhkcool97/.local/lib/python3.8/site-packages/torch/autograd/function.py&quot;, line 289, in apply
    return user_fn(self, *args)
  File &quot;/home/jhkcool97/.local/lib/python3.8/site-packages/torch/utils/checkpoint.py&quot;, line 275, in backward
    tensors = ctx.saved_tensors
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [6, 1, 201, 201]] is at version 16; expected version 14 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!
</code></pre>
","transformer-model"
"77975331","repetitive generation of tokens in conditioning transformer","2024-02-11 01:47:25","","0","212","<pytorch><nlp><transformer-model>","<p>I implemented a transformer decoder and initially trained it on text sequences, achieving satisfactory performance. Subsequently, I aimed to condition the model on floating-point values. To accomplish this, I converted the floating-point values into embeddings using linear layers, which were then passed to the decoder as memory inputs. However, upon training with this setup, I observed an unusually low loss from the outset, leading me to suspect overfitting.</p>
<p>Despite my concerns, when I sampled from the model by providing a floating-point value and a start token to the decoder, the output sequences were unexpectedly repetitive, consisting of merely two alternating letters. This outcome suggested a problem beyond simple overfitting, as overfitting would typically result in the model reproducing identical sequences rather than generating repetitive patterns.</p>
<p>To diagnose the issue, I sampled outputs every 100 batches during training, and these sequences appeared to be normal, indicating that the training process itself may not be fundamentally flawed.</p>
<p>Given these observations, what do you think could be the problem with my approach?</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        
        self.dropout = nn.Dropout(p=dropout)
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)

class ConditionalTransformerModel(nn.Module):
    def __init__(self, vocab_size, n_embd, n_head, n_layers, max_length, property_dim, property_hidden_dim, dropout=0.1):
        super().__init__()

        self.embed = nn.Embedding(vocab_size, n_embd)
        self.n_embd = n_embd
        self.property_processor = nn.Sequential(
            nn.Linear(property_dim, property_hidden_dim),
            nn.ReLU(),
            nn.Linear(property_hidden_dim, n_embd)
        )

        decoder_layer = nn.TransformerDecoderLayer(d_model=n_embd, nhead=n_head, dropout=dropout)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)
        self.pos_encoder = PositionalEncoding(n_embd, dropout, max_length)
        self.generator = nn.Linear(n_embd, vocab_size)
        self.max_length = max_length

    def generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(self, src, tgt, property):

        batch_size, seq_length = src.size(0), src.size(1)
        src = self.embed(src) * math.sqrt(self.n_embd)
        tgt = self.embed(tgt) * math.sqrt(self.n_embd)
        src = self.pos_encoder(src)
        tgt = self.pos_encoder(tgt)
        property_encoded = self.property_processor(property)
        property_encoded = property_encoded.unsqueeze(1).repeat(1, seq_length, 1)
        memory = property_encoded.transpose(0, 1)

        tgt_mask = self.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)

        output = self.transformer_decoder(tgt.transpose(0, 1), memory, tgt_mask=tgt_mask)
        output = output.transpose(0, 1)
        return self.generator(output)


# Define your model
model = ConditionalTransformerModel(vocab_size=vocab.vocab_size, n_embd=64, n_head=4, n_layers=4, max_length=512, property_dim=1, property_hidden_dim=128).to('cuda')

# Training loop
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

def sample_predictions(output, vocab, k=5):
    &quot;&quot;&quot;Sample some predictions from the model output&quot;&quot;&quot;
    # Get the most probable next tokens (top-k)
    _, topk_indices = torch.topk(output, k, dim=-1)
    sampled_indices = topk_indices[:, :, 0]  # For simplicity, use the top-1 prediction
    sampled_tokens = [vocab.decode(indices.tolist()) for indices in sampled_indices]
    return sampled_tokens

for epoch in range(10):  # Let's still assume 10 epochs for simplicity
    model.train()  # Set the model to training mode
    total_loss = 0  # To accumulate loss over the epoch
    
    for i, (src_batch, tgt_batch, properties_batch) in enumerate(data_loader):
        src_batch, tgt_batch, properties_batch = src_batch.to('cuda'), tgt_batch.to('cuda'), properties_batch.to('cuda')
        
        optimizer.zero_grad()  # Zero the gradients
        
        # Adjust the model's forward pass call to match the updated signature.
        # Now src is used as input to the encoder and tgt is used as input to the decoder
        # Note: You need to ensure that your model's forward method accepts these arguments correctly
        output = model(src_batch, tgt_batch, properties_batch)  # src_batch is now explicitly separated from tgt_batch
        
        # Calculate loss. Note that we need to adjust the targets to match the output dimensions.
        # Since output is likely of shape [batch_size, seq_len, vocab_size] and tgt_batch is [batch_size, seq_len],
        # we transpose output to [batch_size, vocab_size, seq_len] before passing it along with tgt_batch to the criterion.
        loss = criterion(output.transpose(1, 2), tgt_batch)  # Adjust loss calculation to use tgt_batch
        
        loss.backward()  # Backpropagate the loss
        optimizer.step()  # Update model parameters
        total_loss += loss.item()  # Accumulate the loss
    
        if i % 100 == 0:  # Sample predictions every 100 batches
            sampled_tokens = sample_predictions(output, vocab)
            print(f&quot;Batch {i}: Sample Predictions: {sampled_tokens[:5]}&quot;)  # Print sample predictions from the first 5 sequences

    # Print average loss for the epoch
    print(f&quot;Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}&quot;)
</code></pre>
","transformer-model"
"77965274","Checkpoints implementation in huggingface","2024-02-08 23:16:08","","0","29","<python><pytorch><huggingface-transformers><transformer-model><large-language-model>","<p>I have a dataset that is 22 million rows that I would like to grab the embeddings for from a huggingface transformer model. I want to embed the input_ids I got from the tokenization process. The code below takes 5 hours to embed 20% of the dataset and then it crashes.
I'm looking for:</p>
<ol>
<li>How to implement checkpoints with this code so that I don't have to be worried if it crashes</li>
<li>How to make it run faster (I am using 2 NVIDIAA100_SXM4_80GB GPUs). I've already tried increasing the batch size-- whenever I increase the batch size above 2000, it says the GPU has run out of memory.</li>
</ol>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(&quot;InstaDeepAI/nucleotide-transformer-500m-human-ref&quot;, num_labels=2)
model = torch.nn.DataParallel(model)
model = model.to(device)

ds1 = Dataset.from_file('data_train') # already tokenized

def embed_function(examples):
    inputs = torch.tensor(examples['input_ids'])  # Convert to tensor
    inputs = inputs.to(device)

    with torch.no_grad():
        outputs = model(input_ids=inputs, output_hidden_states=True)
    
    # Step 3: Extract the embeddings
    hidden_states = outputs.hidden_states  # List of hidden states from all layers
    embeddings = hidden_states[-1]  # Assuming you want embeddings from the last layer

    return {'embeddings': embeddings}

embedding1 = ds1.map(
    embed_function,
    batched=True, batch_size=2000)

embedding1
embedding1.save_to_disk(&quot;embeddings/train&quot;, num_shards=1)
</code></pre>
","transformer-model"
"77965060","Key matrix redundant in Transformer language models?","2024-02-08 22:14:21","78476753","0","61","<nlp><transformer-model>","<p>Simple implementations of Transformer language models such as <a href=""https://www.youtube.com/watch?v=kCc8FmEb1nY"" rel=""nofollow noreferrer"">this one</a> define 3 matrices K,Q,V to compute keys, queries and values. However matrices K and Q are never used separately: all Transformer computations form their product <code>Q^t K</code>. So I wonder why not learn this product matrix directly instead of splitting it into 2 matrices K and Q.</p>
<p>Part of the answer may come from the size of K and Q, which is <code>d -&gt; n</code>, where d is the dimension of the token embeddings and n is the dimension of keys and queries. The size of <code>Q^t K</code> is <code>d -&gt; d</code>. So learning K and Q separately means optimizing <code>2*n*d</code> parameters, whereas learning the product <code>Q^t K</code> is <code>d*d</code> parameters. The only useful splitting I see is when <code>n &lt;= d/2</code>, because that's less parameters to optimize. But at the limit case <code>n = d/2</code>, the rank of the product matrix <code>Q^t K</code> is <code>d/2</code>, which is very degenerate. With the same number of parameters <code>d^2</code>, we could learn an unconstrained square matrix. That might learn more flexible and subtle patterns in the training data.</p>
<p>In the <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention is all you need</a> paper, base model page 9, we see d = 512 and n = 64, so the product matrix <code>Q^t K</code> does have degenerate rank. Is reducing the number of parameters the true and unique intent here? Is there a theoretical justification that these degenerate ranks help natural language processing?</p>
","transformer-model"
"77954591","Nan output after masked TransforrmerDecoder","2024-02-07 12:05:59","","0","173","<pytorch><nlp><nan><transformer-model><attention-model>","<p>What is wrong with my mask, or why it doesn't work?
I try to do predict mask token based only on the first token and masked token.
For this i created special create_casual_mask method that creates this mask for multihead attention. When i run it, nan tensor is returned. Pad mask and mask of masked tokens are not intersected, i full my attention mask as it is described in torch documentations for torch.nn.TransformerDecoder. Also output of attention shouldn't be empty because there are some False values in attention mask.
So why it doesn't work?</p>
<pre><code>import torch
from torch import nn

torch.manual_seed(0)


class LookOnFirstDecoder(nn.Module):
    def __init__(self, depth, d_model, nhead, d_ff,
                 dropout, activation,
                sent_length, n_tokens, pad_idx
    ):
        super().__init__()
        &quot;&quot;&quot;
        :param sent_length: max length of sentence
        :param n_tokens: number of tokens to use including mask and padding tokens
        :param pad_idx: index of padding to don't compute the gradient
        &quot;&quot;&quot;
        self.d_model = d_model
        self.nhead = nhead
        self.n_tokens = n_tokens
        self.emb = nn.Embedding(
            num_embeddings=n_tokens,
            embedding_dim=d_model,
            padding_idx=pad_idx
        )

        self.pos_embed = nn.Parameter(
            torch.zeros(1, sent_length, d_model),
            requires_grad=True
        )
        torch.nn.init.normal_(self.pos_embed, std=.02)

        self.transformer = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(
                d_model=d_model,
                nhead=nhead,
                dim_feedforward=d_ff,
                dropout=dropout,
                activation=activation,
                batch_first=True,
                norm_first=True,
            ),
            num_layers=depth,
        )

        self.fin_lin = nn.Linear(d_model, n_tokens)

    def create_causal_mask(self, mask):
        &quot;&quot;&quot;
            The purpose is to create mask that allows all not first tokens
        look only on the first token and itself
        :param mask: (B, L)
        :return: (B * nhead, L)
        &quot;&quot;&quot;

        mask[:, 0] = True  # to depend on first token
        b, l = mask.shape
        batch_causal_mask = ~torch.tril(mask.unsqueeze(-1) * mask.unsqueeze(-2))  # (B, L, L)
        # batch_causal_mask = torch.tril(torch.ones((b, l, l))).to(&quot;cuda&quot;) == 0

        # batch_causal_mask = torch.where(batch_causal_mask, 0, float('-inf'))
        print(f&quot;Batch causal mask: \n{batch_causal_mask}&quot;)

        causal_mask = (
            batch_causal_mask.
            unsqueeze(1).  # (B, 1, L, L)
            expand(b, self.nhead, l, l).  # (B, nhead, L, L)
            reshape(b * self.nhead, l, l)  # (B * nhead, L, L)
        )

        return causal_mask

    def forward(self, tgt, memory, is_masked_mask, is_pad_mask):
        &quot;&quot;&quot;
        :param tgt: (B, L)
        :param memory: (B, L1, D)
        :param is_masked_mask: (B, L) - True - mask token, False - not
        :param is_pad_mask: (B, L), True - pad token, False - not
        :return: tensor of shape (B, n_tokens)
        &quot;&quot;&quot;
        b, l = tgt.shape
        tgt_tokens = self.emb(tgt) + self.pos_embed[:, :l].expand(b, l, self.d_model)

        tgt_tokens = self.transformer(
            tgt_tokens,
            memory,
            tgt_mask=self.create_causal_mask(is_masked_mask.clone()),
            tgt_is_causal=True,
            tgt_key_padding_mask=is_pad_mask
        )  # (B, L, D)

        fin_tokens = self.fin_lin(tgt_tokens[is_masked_mask])
        return fin_tokens


# my vocabulary
n_tokens = 10  # pad_idx - 9, mask_idx - 8
pad_idx = n_tokens - 1
mask_idx = n_tokens - 2

d_model = 4
nhead = 2
b, l = 3, 8

model = LookOnFirstDecoder(
    depth=2,
    d_model=4,
    nhead=2,
    d_ff=8,
    dropout=0.1,
    activation=&quot;gelu&quot;,
    sent_length=l,
    n_tokens=n_tokens,
    pad_idx=pad_idx
)

memory = torch.randn(b, l, d_model)

# so i create some random tokens, without padding and mask
in_tokens = torch.randint(0, mask_idx - 1, (b, l))

# mask and paddings add manually
in_tokens[0, 6:] = pad_idx
in_tokens[0, 5] = mask_idx

in_tokens[1, 7:] = pad_idx
in_tokens[1, 4] = mask_idx

in_tokens[2, 5:] = pad_idx
in_tokens[2, 0] = mask_idx

is_masked_mask = in_tokens == mask_idx
is_pad_mask = in_tokens == pad_idx

pred = model(in_tokens, memory, is_masked_mask, in_tokens == pad_idx)

print(f&quot;In tokens: \n{in_tokens}&quot;)
print(f&quot;Pad mask: \n{is_pad_mask}&quot;)
print(f&quot;Masked mask: \n{is_masked_mask}&quot;)
print(f&quot;Pred: \n{pred}&quot;)

</code></pre>
<p>these is my requirements.txt</p>
<pre><code>torch == 2.1.1
torchvision == 0.16.1
xformers
albumentations==1.3.1

numpy == 1.26.2
scipy == 1.11.4
scikit-learn == 1.3.2
pandas == 2.1.4
matplotlib == 3.8.2
seaborn == 0.13.0
</code></pre>
<p>That is my result after execution:</p>
<pre><code>Batch causal mask: 
tensor([[[False,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True,  True, False,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True]],

        [[False,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [False,  True,  True,  True, False,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True]],

        [[False,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True],
         [ True,  True,  True,  True,  True,  True,  True,  True]]])
In tokens: 
tensor([[2, 1, 4, 0, 3, 8, 9, 9],
        [6, 4, 0, 6, 8, 0, 5, 9],
        [8, 2, 5, 2, 6, 9, 9, 9]])
Pad mask: 
tensor([[False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False,  True,  True,  True]])
Masked mask: 
tensor([[False, False, False, False, False,  True, False, False],
        [False, False, False, False,  True, False, False, False],
        [ True, False, False, False, False, False, False, False]])
Pred: 
tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],
        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],
       grad_fn=&lt;AddmmBackward0&gt;)
</code></pre>
","transformer-model"
"77952028","How to add encoder/decoder layers to PyTorch transformers after inference?","2024-02-07 03:17:33","","0","182","<machine-learning><pytorch><overriding><transformer-model>","<p>I have a PyTorch encode-decoder transformer: I want to save the model and then load in the weights, except in an upscaled model. So, for example, I want to train a model on 3 encoder layers and 3 decoder layers, save the parameters. Then, I want to load in all the parameters, but add another random encoder and decoder layer at the end. Then, I want to freeze all the parameters during inference except for the new layers. These are essentially adapter layers, but I want to do them as blocks instead.</p>
<p>Class code:</p>
<pre><code>class Seq2SeqTransformer(nn.Module):

  def __init__(self,
              num_encoder_layers: int,
              num_decoder_layers: int,
              emb_size: int,
              nhead: int,
              src_vocab_size: int,
              tgt_vocab_size: int,
              dim_feedforward: int = 512,
              dropout: float = 0.1):
    super(Seq2SeqTransformer, self).__init__()
    self.transformer = Transformer(d_model=emb_size,
                                    nhead=nhead,
                                    num_encoder_layers=num_encoder_layers,
                                    num_decoder_layers=num_decoder_layers,
                                    dim_feedforward=dim_feedforward,
                                    dropout=dropout)
    self.generator = nn.Linear(emb_size, tgt_vocab_size)
    self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
    self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
    self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)

  def forward(self,
              src: Tensor,
              trg: Tensor,
              src_mask: Tensor,
              tgt_mask: Tensor,
              src_padding_mask: Tensor,
              tgt_padding_mask: Tensor,
              memory_key_padding_mask: Tensor):
      src_emb = self.positional_encoding(self.src_tok_emb(src))
      tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
      outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,
                              src_padding_mask, tgt_padding_mask, memory_key_padding_mask)
      return self.generator(outs)

  def encode(self, src: Tensor, src_mask: Tensor):
      return self.transformer.encoder(self.positional_encoding(
                          self.src_tok_emb(src)), src_mask)

  def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):
      return self.transformer.decoder(self.positional_encoding(
                        self.tgt_tok_emb(tgt)), memory,
                        tgt_mask)

</code></pre>
","transformer-model"
"77947698","Problem during Custom Sentence Translations with Seq2Seq Transformer Model (English to Spanish)","2024-02-06 12:30:26","","0","47","<python><pytorch><neural-network><transformer-model><machine-translation>","<p>I am trying to create a translator from English to Spanish based on the Transformer architecture and code mostly taken from <a href=""https://pytorch.org/tutorials/beginner/translation_transformer.html"" rel=""nofollow noreferrer"">the Pytorch Docs</a>. For training I used a <a href=""https://stackoverflow.com"">Kaggle Dataset</a>.</p>
<p>I trained the neural network until it was doing well. In the training with input and target it is doing well.
Now I want to translate custom sentences (I do not have a translation/target for them).
With my implementation that should do this job, I only get SOS tokens returned.</p>
<p>Here again the architecture of the Transformer Model:</p>
<pre><code>class Seq2SeqTransformer(nn.Module):
  &quot;&quot;&quot;
  Basic Transformer for Neural Machine Translation tasks.
  &quot;&quot;&quot;
  def __init__(self,
    num_encoder_layers: int,
    num_decoder_layers: int,
    emb_size: int,
    nhead: int,
    src_vocab_size: int,
    tgt_vocab_size: int,
    dim_feedforward: int = 512,
    dropout: float = 0.1
  ):
    super(Seq2SeqTransformer, self).__init__()
    self.transformer = nn.Transformer(
      d_model=emb_size,
      nhead=nhead,
      num_encoder_layers=num_encoder_layers,
      num_decoder_layers=num_decoder_layers,
      dim_feedforward=dim_feedforward,
      dropout=dropout)

    self.generator = nn.Linear(emb_size, tgt_vocab_size)
    self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)
    self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)
    self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)

  def forward(self,
    src: torch.Tensor,
    trg: torch.Tensor,
    src_mask: torch.Tensor,
    tgt_mask: torch.Tensor,
    src_padding_mask: torch.Tensor,
    tgt_padding_mask: torch.Tensor,
    memory_key_padding_mask: torch.Tensor
  ):
    src_emb = self.positional_encoding(self.src_tok_emb(src))
    tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))
    outs = self.transformer(
      src_emb,
      tgt_emb,
      src_mask,
      tgt_mask,
      None,
      src_padding_mask,
      tgt_padding_mask,
      memory_key_padding_mask)
    return self.generator(outs)

  def encode(self, src: torch.Tensor, src_mask: torch.Tensor):
    return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)

  def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor):
    return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)
</code></pre>
<p>Now (after training) I have a well trained model that seems to work well with the <code>model.forward(src, target, ...)</code> function. I implemented a function based on <code>model.forward()</code> to generate some translations for random sentences taken from a <code>torch.utils.data.DataLoader</code>. I used it to get progress insights while training and evaluating my model (<code>get_sentence_variants()</code> is for decoding sentences in different human readable formats and <code>get_bleu_score()</code> is implemented to calculate the BLEU score):</p>
<pre><code>def get_transformer_translation(loader: torch.utils.data.DataLoader):
  model.eval()

  data_iterator = iter(loader)
  input_batch, target_batch = next(data_iterator)
  input_sentence, target_sentence = input_batch[:, 0], target_batch[:, 0]
  # input shape: (83) = (eng_seq_len), target shape: (80) = (spa_seq_len)

  eng_decoded_words, eng_filtered_words, eng_decoded_sentence = get_sentence_variants(input_sentence, eng_itos)
  spa_decoded_words, spa_filtered_words, spa_decoded_sentence = get_sentence_variants(target_sentence, spa_itos)

  input_sentence = input_sentence.unsqueeze(1)
  target_sentence = target_sentence.unsqueeze(1)
  # input shape: (83, 1) = (seq_len_input, batch_size), target shape: (80, 1) = (seq_len_target, batch_size)
 
  outputs = model.forward(
    src=input_sentence,
    trg=target_sentence,
    src_mask=None,
    tgt_mask=None,
    src_padding_mask=None,
    tgt_padding_mask=None,
    memory_key_padding_mask=None
  )
  # outputs: (80, 1, 16557) = (target_seq_len, batch_size, vocab_size_target)

  result_indices = torch.argmax(outputs, dim=-1)
  # result_indices (80, 1) = (seq_len_target, batch_size)

  model_predicted_words, model_filtered_words, model_decoded_sentence = get_sentence_variants(result_indices, spa_itos)

  bleu_score = get_bleu_score(model_predicted_words, spa_decoded_words)

  model.train()
  return eng_decoded_words, eng_filtered_words, eng_decoded_sentence, spa_decoded_words, spa_filtered_words, spa_decoded_sentence, model_predicted_words, model_filtered_words, model_decoded_sentence, bleu_score
</code></pre>
<p>For example during training, I used it to generate the outputs in the following manner (shows that model is trained properly):</p>
<pre><code>Eng: if i find your passport , i ' ll call you .
Spa: si encuentro tu pasaporte , te llamaré .
Pre: si encuentro tu pasaporte , te llamaré .
Loss: 0.010215843096375465, Bleu: 100.0
</code></pre>
<p>Now I want to translate my own sentences. I am struggling to implement a function that does this job for me. I can imagine that I use wrong masks during the training (I am no neural net expert yet). However, here is the code:</p>
<pre><code>def get_translation(model, src, src_mask, max_len, start_symbol):
    src = src.to(device)
    src_mask = src_mask.to(device)

    memory = model.encode(src, src_mask)
    # memory: (input_seq_len, 1, src_emb_dim)
    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)
    for i in range(max_len-1):
        memory = memory.to(device)
        tgt_mask = (generate_square_subsequent_mask(ys.size(0))).to(device)
        # tgt_mask: (num_predicted_tokens, num_predicted_tokens)

        out = model.decode(ys, memory, tgt_mask)
        out = out.transpose(0, 1)
        # out after transform: (1, tgt_emb_dim)

        prob = model.generator(out[:, -1])
        # prob: (batch_size, tgt_vocab_size)
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.item()

        ys = torch.cat([ys,
                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)
        # ys: (num_predicted_tokens, 1)
        #if next_word == EOS_IDX:
        #    break
    return ys
</code></pre>
<p>...and here the code where I call the function above:</p>
<pre><code>user_input = &quot;i love you.&quot;

padded_user_input = pad_punctuation(user_input.lower())
tokenized_user_input = padded_user_input.split() + [&quot;&lt;EOS&gt;&quot;]
padded_token_sequence = np.zeros(eng_padded_seq_len, dtype=int)
token_sequence = [word_to_number(word, eng_stoi) for word in tokenized_user_input]
padded_token_sequence[:len(token_sequence)] = token_sequence
padded_token_sequence = torch.tensor(padded_token_sequence, dtype=int).to(device)
input_sentence = padded_token_sequence.unsqueeze(1)
# input_sentence: (seq_len_input, 1), needs batch size = 1

src_mask = generate_square_subsequent_mask(eng_padded_seq_len)#(torch.zeros(eng_padded_seq_len, eng_padded_seq_len)).type(torch.bool)
print(src_mask)

tgt_tokens = get_translation(model=model, src=input_sentence, src_mask=src_mask, max_len=spa_padded_seq_len, start_symbol=spa_stoi[&quot;&lt;SOS&gt;&quot;])
print(tgt_tokens)
</code></pre>
<p><code>generate_square_subsequent_mask()</code> creates masks in the following format:</p>
<pre><code>tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],
        [0., 0., -inf,  ..., -inf, -inf, -inf],
        [0., 0., 0.,  ..., -inf, -inf, -inf],
        ...,
        [0., 0., 0.,  ..., 0., -inf, -inf],
        [0., 0., 0.,  ..., 0., 0., -inf],
        [0., 0., 0.,  ..., 0., 0., 0.]])
</code></pre>
<p><code>tgt_tokens</code> always has shape (tgt_seq_len, 1). All values in it are equal to <code>start_symbol</code>, the last param in <code>get_translation</code>, so currently I only receive back SOS tokens.</p>
<p>Where is the mistake and how can I fix it?</p>
","transformer-model"
"77947679","Model's predictions always 0","2024-02-06 12:26:30","77948485","0","159","<tensorflow><machine-learning><keras><deep-learning><transformer-model>","<p>I have a training set of shape (1280, 100, 20, 4096) which I feed to a transformer-based model for binary classification (labels are either 0 or 1). This results in an big amount of features that I'm struggling to handle (I've tried to feed it to the model in batches, but I'm not sure about the best approach. Right now I just reduced it to (450, 100, 20, 4096), but any suggestion is appreciated), but my problem at the moment is that no matter how many epochs I train my model on, the accuracy will always be of 67,5% (which is the percentage of 0-labeled features in the test set), precision and recall on the test set will always be 0%. I've tried to normalize my data before feeding it to the model:</p>
<pre><code>    scaler = StandardScaler()
    train_data = scaler.fit_transform(train_data.reshape(-1, train_data.shape[-1])).reshape(train_data.shape)
    test_data = scaler.transform(test_data.reshape(-1, test_data.shape[-1])).reshape(test_data.shape)
</code></pre>
<p>but this didn't result in any improvement. The model I'm using is based on an encoder-only transformer:</p>
<pre><code>Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 20, 4096)]   0         
_________________________________________________________________
frame_position_embedding (Po (None, 100, 20, 4096)     8192000   
_________________________________________________________________
transformer_layer (Encoder)  (None, 100, 20, 4096)     134299652 
_________________________________________________________________
global_max_pooling (GlobalMa (None, 4096)              0         
_________________________________________________________________
dropout (Dropout)            (None, 4096)              0         
_________________________________________________________________
output (Dense)               (None, 1)                 4097      
=================================================================
Total params: 142,495,749
Trainable params: 142,495,749
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p>During training, I can see that loss, accuracy, precision and recall reach decent levels, but when I evaluate the model on the test set all these values are as I previously described:</p>
<pre><code>Epoch 100/100
29/29 [==============================] - 90s 3s/step - loss: 0.0839 - accuracy: 0.9610 - recall: 0.9316 - precision: 0.9589
2024-02-06 12:38:38.815759: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 9175040000 exceeds 10% of free system memory.
9/9 [==============================] - 21s 2s/step - loss: 9.4117 - accuracy: 0.6750 - recall: 0.0000e+00 - precision: 0.0000e+00
Test accuracy: 67.5%
Test recall: 0.0%
Test precision: 0.0%
</code></pre>
<p>The model optimizer is adam, the loss is binary crossentropy. Activation is sigmoid.
I'm struggling in finding an adeguate tuning for the model, and even to understand its current behavior. In addition, it's not clear to me if feeding the sets in batches both to the scaler and the fit functions would change the actual training of the model.</p>
","transformer-model"
"77930495","Trying to run falcon-40b model locally. need help the model is not giving any output and is showing exit code 1 on VS Code","2024-02-03 00:38:50","","0","76","<python><machine-learning><pytorch><transformer-model><large-language-model>","<p>I have Nvidia rtx 3060 and decided to give it a try for my own project. So i have downloaded all the model files into a folder ./Model, parallel to app.py. I just wanted to test the llm before i proceed further. But when i try to execute the code it doesnt produce any output and also when i hover over the executed command on VScode it says <code>Command Executed now and failed (Exit Code 1)</code>.</p>
<p>Here is the testing code:</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Set the path to your model directory
model_directory = &quot;./model&quot;

# Initialize the tokenizer and model from the specified directory
tokenizer = AutoTokenizer.from_pretrained(model_directory)
model = AutoModelForCausalLM.from_pretrained(model_directory)

# Ensure the model is using the GPU
model = model.to(&quot;cuda&quot;)

# Define the prompt
prompt = &quot;&quot;&quot;
&lt;human&gt;: explain llms like i am five
&lt;assistant&gt;:
&quot;&quot;&quot;

# Configuration for the generation
generation_config = {
    &quot;max_length&quot;: 200,  # Adjust the maximum length of the generated tokens
    &quot;temperature&quot;: 0.7,  # Temperature controls the randomness
    &quot;top_p&quot;: 0.7,        # top_p controls the nucleus sampling
    &quot;num_return_sequences&quot;: 1,  # Number of sequences to generate
    &quot;pad_token_id&quot;: tokenizer.eos_token_id,  # Padding token
    &quot;eos_token_id&quot;: tokenizer.eos_token_id,  # End of sequence token
}

# Encode the prompt
encoding = tokenizer(prompt, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)

# Generate a response using the encoded prompt and generation configuration
with torch.no_grad():  # Disables gradient calculation to save memory and speeds up computation
    outputs = model.generate(
        input_ids=encoding[&quot;input_ids&quot;], 
        attention_mask=encoding[&quot;attention_mask&quot;], 
        **generation_config
    )

# Decode the generated tokens to text
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Print the generated text
print(generated_text)
</code></pre>
<p>Here are the VScode screen shots:
<a href=""https://i.sstatic.net/qxX0A.png"" rel=""nofollow noreferrer"">VScode showing Exit Code</a></p>
<p>What am I doing wrong and how to resolve this?</p>
<p>I tried to execute the above given code. I should have got a text response but i got nothing.</p>
","transformer-model"
"77923293","Add tensorflow embedding layer to dense layer","2024-02-01 20:50:21","","0","49","<python><tensorflow><embedding><transformer-model>","<p>I'm building a tensorflow model that's a bit of a light transformer.</p>
<p>The input data is of shape (None, 49, 16). Sort of like a language model where every sentence in the data has 49 words, and each word is represented by a vector of size 16.</p>
<p>I will pass each of those 49 vectors of size 16 through a dense layer that has 32 neurons.  The output of this is (None, 49, 32).</p>
<p>To each of these 49 vectors I want to add a vector of size 32, so that I have 49 vectors of size 32, where each entry of these needs to be learned. This is 49*32 = 1568 numbers that need to be learned.</p>
<p>I can build such a model with</p>
<pre><code>inp = tf.keras.layers.Input(shape=(49,16))
inp2 = tf.keras.layers.Input(shape=(49))
mid = tf.keras.layers.Dense(32)(inp)
emb = tf.keras.layers.Embedding(input_dim=49, output_dim=32)(inp2)
out = mid+emb
mod = tf.keras.models.Model([inp,inp2],out)
</code></pre>
<p>But the inp2 layer must ALWAYS take the same input - [0,1,2,..,48] - for this to work.</p>
<p>Is there a way to simplify this so I don't need 2 separate input layers?</p>
<p>I thought I could do:</p>
<pre><code>inp = tf.keras.layers.Input(shape=(49,16))
mid = tf.keras.layers.Dense(32)(inp)
pos = tf.range(start=0, limit=49, delta=1)
emb = tf.keras.layers.Embedding(input_dim=49, output_dim=32)(pos)
out = mid+emb
mod = tf.keras.models.Model(inp,out)
</code></pre>
<p>But when I do this, the 1568 numbers in the embedding are initialized at random and then never learned, which is not what I want.</p>
","transformer-model"
"77921072","Visual Transformers predict() returning same prediction output for all values in test inputs","2024-02-01 14:41:19","","0","44","<python><tensorflow><deep-learning><neural-network><transformer-model>","<p>Hello So Recently I was used some implementation of visual transformers and tried to implement it. on validation and testing MAE = 27. and when i decided to make model predict on same datasets it gave me obscure numbers. can anyone help me to solve it?</p>
<pre class=""lang-py prettyprint-override""><code>class VisionTransformer(tf.keras.Model):
    def __init__(
        self,
        image_size,
        patch_size,
        num_layers,
        num_classes,
        d_model,
        num_heads,
        mlp_dim,
        channels=3,
        dropout=0.1,
    ):
        super(VisionTransformer, self).__init__()
        num_patches = (image_size // patch_size) ** 2
        self.patch_dim = channels * patch_size ** 2

        self.patch_size = patch_size
        self.d_model = d_model
        self.num_layers = num_layers

        self.rescale = Rescaling(1./255)
        self.pos_emb = self.add_weight(
            &quot;pos_emb&quot;, shape=(1, num_patches + 1, d_model)
        )
        self.class_emb = self.add_weight(&quot;class_emb&quot;, shape=(1, 1, d_model))
        self.patch_proj = Dense(d_model)
        self.enc_layers = [
            TransformerBlock(d_model, num_heads, mlp_dim, dropout)
            for _ in range(num_layers)
        ]
        self.mlp_head = tf.keras.Sequential(
            [
                Dense(mlp_dim, activation=tfa.activations.gelu),
                Dropout(dropout),
                Dense(num_classes),
            ]
        )

    def extract_patches(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding=&quot;VALID&quot;,
        )
        patches = tf.reshape(patches, [batch_size, -1, self.patch_dim])
        return patches

    def call(self, x, training):
        batch_size = tf.shape(x)[0]
        x = self.rescale(x)
        patches = self.extract_patches(x)
        x = self.patch_proj(patches)

        class_emb = tf.broadcast_to(
            self.class_emb, [batch_size, 1, self.d_model]
        )
        x = tf.concat([class_emb, x], axis=1)
        x = x + self.pos_emb

        for layer in self.enc_layers:
            x = layer(x, training)

        # First (class token) is used for classification
        x = self.mlp_head(x[:, 0])
        return x
</code></pre>
<pre class=""lang-py prettyprint-override""><code>model.predict(datasetTrain)
</code></pre>
<p>And it gave me same values for each year (i think mae would be 900 at min) what just happened??</p>
","transformer-model"
"77907423","why Librosa Mel Spectrograms shape is like 4 dimensional every time like (Nsamples, Nframes, N_mel, N_t_bins)? is that number impact on spectro shape?","2024-01-30 15:16:07","","0","11","<transformer-model><librosa><time-frequency>","<p>I am doing classification on project type of audio processing. The original sampling rate is about 32000 Hz I use normalization and resembling and new sampling rate is 16000 Hz and make uniform size chunks 4 sec, so each have 4*16000 = 64000 sampling points.
Frame size is 512 and hop is 128 (4 time overlap) and I got total 500 frames form each sample. Shape of audio data is like (#samples, #frames, #samples_in_frame) = (x, 500, 512).
Using the Librosa Library I extract Mels Spectrograms where n_mel = 100, now the shape is like  (#samples, #frames, #n_mel, #t_bins) = (x, 500, 128, 5)</p>
<p>Here ismy code &quot;Scaled_Mel = scale_minmax(Mel_reshaped) # Iterate over each Mel spectrogram and save it as an image
for i, file_name in enumerate(sliced_sample_names4):
mel_spectrogram = Scaled_Mel[i, :, :]  # Extract the scaled Mel spectrogram
spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)</p>
<pre><code># Transpose the spectrogram to match the expected shape
spectrogram = np.transpose(spectrogram)  # Transpose the spectrogram

# Create a new figure and axis with the desired dimensions
plt.figure(figsize=(5, 5))  # Set the figure size to 500x500 pixels

# off the axis
plt.axis('off')

# Display the spectrogram as an image
plt.imshow(spectrogram, aspect='auto', cmap='viridis')

# Construct the filename using the current file name and index
filename = f&quot;spectrogram_{file_name}_{i+1}.png&quot;
filepath = os.path.join(msp_path, filename)

# Check if the file already exists
if os.path.exists(filepath):
    print(f&quot;Image {filename} already exists in the folder path. Skipping...&quot;)
    continue

# Save the plot as an image
plt.savefig(filepath, bbox_inches='tight', pad_inches=0, dpi=500)

# Close the plot to free memory
plt.close()

print(f&quot;Spectrogram {i+1} saved as {filepath}&quot;)&quot;
</code></pre>
<p>Question 1: wht t_bins = 5 is it becaue of (frame_size/hop_size + 1)?
Question 2: I make calculation like if i reshape it, it would be like
Mel_reshaped = Mel.reshape(Mel.shape[0], Mel.shape[1], -1) = (x, 500, 100*5) = (x, 500, 500) is that strategy make sence?
Question 3: I want to use vision transformer model and all my calculation are for make the shape  like 500 by 500 but when i save the spectrograms they give me dimentions like (1937,1935). I know dpi impact alot but if I select dpi to short its disturm the resoulation.</p>
","transformer-model"
"77896470","Why do we need two normalization instances","2024-01-28 21:17:34","","0","55","<python><keras><transformer-model>","<pre><code>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

class TransformerEncoder(layers.Layer):
   # This is a custom layer that inherits from keras.layers.Layer, used to implement the encoder part of a Transformer model. This class uses the multi-head attention mechanism.

   def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):
       super().__init__(**kwargs)
       self.embed_dim = embed_dim  # The dimension of the input embeddings.
       self.dense_dim = dense_dim  # The dimension of the dense layer in the feedforward network.
       self.num_heads = num_heads  # The number of heads in the multi-head attention mechanism.
       self.attention = layers.MultiHeadAttention(
           num_heads=num_heads, key_dim=embed_dim)  # A MultiHeadAttention instance for calculating attention weights.
       self.dense_proj = keras.Sequential(
           [layers.Dense(dense_dim, activation=&quot;relu&quot;),  # The output dimension is dense_dim, and the input dimension is automatically matched.
            layers.Dense(embed_dim),]
       )  # A Sequential model containing two Dense layers for processing attention outputs.
       self.layernorm_1 = layers.LayerNormalization()  # A LayerNormalization layer for normalizing the input data.
       self.layernorm_2 = layers.LayerNormalization()  # Another LayerNormalization layer for normalizing the output after the feedforward network.

   def call(self, inputs, mask=None):
       # This method accepts two parameters: inputs (input data) and mask (an optional mask). The mask is usually used to indicate which positions in the sequence should be ignored, such as when dealing with padded sequences.

       if mask is not None:
           mask = mask[:, tf.newaxis, :]  # Expand the shape of the mask tensor from (batch_size, sequence_length) to (batch_size, 1, sequence_length).

       attention_output = self.attention(inputs, inputs, attention_mask=mask)  # Calculate attention weights using the MultiHeadAttention layer.

       proj_input = self.layernorm_1(inputs + attention_output)  # Normalize the sum of inputs and attention_output using the LayerNormalization layer.

       proj_output = self.dense_proj(proj_input)  # Process the normalized input through the feedforward network.

       return self.layernorm_2(proj_input + proj_output)  # Normalize the sum of proj_input and proj_output using another LayerNormalization layer.

   def get_config(self):
       # This method returns a dictionary containing the configuration of the layer, including the layer's type, parameters, weights, etc. When you use model.save() to save the model, Keras uses this configuration information to reconstruct the model's structure.

       config = super().get_config()  # First, call the parent class's get_config method to get a dictionary containing the parent class's configuration.

       config.update({
           &quot;embed_dim&quot;: self.embed_dim,
           &quot;num_heads&quot;: self.num_heads,
           &quot;dense_dim&quot;: self.dense_dim,
       })
       return config  # Then, update the dictionary with the configuration information specific to this layer (embed_dim, num_heads, and dense_dim). This ensures that the custom layer's specific configuration is saved when saving the model.

</code></pre>
<p>The above is from Deep Learning with Python Second Edition by FrancoisChollet
Code snippet about implementing Transformer encoders as Layer subclasses;
My problem is this: I don't understand the need to instantiate two normalization layers specifically (see figure 2 for an indication); They're clearly the same
(<a href=""https://i.sstatic.net/DjObD.png"" rel=""nofollow noreferrer"">Figure 2</a>).</p>
<p>i didn't try anything because i just wanna figure out why the code author do this.</p>
","transformer-model"
"77881638","Logit implementation in Text Generation model","2024-01-25 16:48:22","","0","54","<pytorch><transformer-model>","<p>I am trying to get the next tokens predicted from a language model, using PyTorch. What would be the best way to compute all the generated tokens and their associated probabilities. I was thinking to take the logit and then use a Softmax operation to obtain the probabilities?</p>
<p>Tried to implement via a <code>nn.embedding</code> table, but struggling with the rest of the implementation?</p>
","transformer-model"
"77864704","Annotated Transformer - Why x + DropOut(Sublayer(LayerNorm(x)))?","2024-01-23 07:49:43","77870197","2","485","<python><pytorch><transformer-model><encoder>","<p>Please clarify if the <a href=""https://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks"" rel=""nofollow noreferrer"">Annotated Transformer</a> Encoder LayerNorm implementation is correct.</p>
<p><a href=""https://i.sstatic.net/8rLZkm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8rLZkm.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Transformer paper</a> says the output of the sub layer is <code>LayerNorm(x + Dropout(SubLayer(x)))</code>.</p>
<p><a href=""https://i.sstatic.net/bl9MS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bl9MS.png"" alt=""enter image description here"" /></a></p>
<p><code>LayerNorm</code> should be applied <strong>after</strong> the <code>DropOut(SubLayer(x))</code> as per the paper:</p>
<p><a href=""https://i.sstatic.net/HrYPa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HrYPa.png"" alt=""enter image description here"" /></a></p>
<p>However, the <a href=""https://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks"" rel=""nofollow noreferrer"">Annotated Transformer</a> implementation does <code>x + DropOut(SubLayer(LayerNorm(x)))</code> where <code>LayerNorm</code> is applied <strong>before</strong> <code>Sublayer</code>, which is the other way around.</p>
<pre><code>class SublayerConnection(nn.Module):
    &quot;&quot;&quot;
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    &quot;&quot;&quot;

    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        &quot;Apply residual connection to any sublayer with the same size.&quot;
        return x + self.dropout(sublayer(self.norm(x)))   # &lt;--- LayerNorm before SubLayer
</code></pre>
","transformer-model"
"77851835","Masking in pytorch TransformerEncoder","2024-01-20 16:38:51","","0","88","<pytorch><masking><transformer-model>","<p>I am wondering how you can make TransformerEncoder predict mask values in the training session. Currenlty what I am doing is that the parts I am &quot;masking&quot; in the input vector is to change their value with 0.0 and then pass the new vector into the model. But I think this is not working properly and that the model doesn't recognize that this 0.0 is in fact a mask value to predict but rather that this is just the input vector value.</p>
<p>I tried just changing the value to 0.0</p>
","transformer-model"
"77824012","Pytorch LayerNorm’s mean and std div are not fixed while inferencing","2024-01-16 07:15:28","78059111","0","425","<deep-learning><pytorch><normalization><transformer-model><inference>","<p>I’m working on recreating the input after torch.LayerNorm. As far as I know, the mean and standard deviation for LayerNorm are fixed during the inference phase. Therefore, I thought I could extract these factors and recreate the original input from the LayerNorm output.</p>
<p>I have successfully extracted the weight and bias, which are not necessarily identical to the mean and standard deviation because LayerNorm has its own weight and bias parameters. My weight and bias parameters are fused from various factors, but they successfully recreate the original input from the LayerNorm output.</p>
<p>However, when I applied these extracted weight and bias parameters to another input tensor and expected LayerNorm to work in the same way as with the previous input, I obtained a completely different output. I assumed that LayerNorm calculated new mean and standard deviation values for the second input, causing the difference. But I’m puzzled as to why LayerNorm computed the mean and standard deviation for the second input; they should have remained fixed during inference.
below is my code</p>
<pre><code>layer = layer().eval()
with torch.inference_mode():
    out = layer(input_data)

w = torch.zeros(len(out[0, :, 0]))
b = torch.zeros(len(out[0, :, 0]))

for i in range(len(out[0, :, 0])):
    w[i] = (input_data[0, i, 0] - input_data[0, i, 10]) / (out[0, i, 0] - out[0, i, 10])
    b[i] = (input_data[0, i, 0] * out[0, i, 10] - input_data[0, i, 10] * out[0, i, 0]) / (out[0, i, 10] - out[0, i, 0])

for i1 in range(len(input_remade[0, :, 0])):
    input_remade[0, i1, :] = out[0, i1, :] * w[i1] + b[i1]
print(torch.sum(input_remade - input_data))


input_data2 = torch.randn(1, 577, 768)
input_remade2 = torch.randn(1, 577, 768)
with torch.inference_mode():
    out2 = layer(input_data2)

for i1 in range(len(input_remade2[0, :, 0])):
    input_remade2[0, i1, :] = out2[0, i1, :] * w[i1] + b[i1]
print(torch.sum(input_remade2 - input_data2))

w1 = torch.zeros(len(out2[0, :, 0]))
b1 = torch.zeros(len(out2[0, :, 0]))

for i in range(len(out2[0, :, 0])):
    w1[i] = (input_data2[0, i, 0] - input_data2[0, i, 10]) / (out2[0, i, 0] - out2[0, i, 10])
    b1[i] = (input_data2[0, i, 0] * out2[0, i, 10] - input_data2[0, i, 10] * out2[0, i, 0]) / (out2[0, i, 10] - out2[0, i, 0])

for i1 in range(len(input_remade2[0, :, 0])):
    input_remade2[0, i1, :] = out2[0, i1, :] * w1[i1] + b1[i1]
print(torch.sum(input_remade2 - input_data2))
</code></pre>
<pre><code>tensor(-0.0061)
tensor(1280.9966)
tensor(0.0014)
</code></pre>
<p>Or is there Any way to extracte fixed mean and standard deviation from LayerNorm layer?</p>
","transformer-model"
"77823366","BERT fine tuned transformer for chat bot not meeting expected performance","2024-01-16 03:35:46","","0","238","<pytorch><chatbot><bert-language-model><transformer-model><seq2seq>","<h2>Question:</h2>
<p>I've been working on a project using a transformer with a pre-trained BERT encoder for a Seq2Seq task. The goal is to create a chatbot that has access to my computer, allowing me to control my PC with my voice. However, I've noticed that the language model's performance is not meeting my expected levels and is constantly either overfitting or just getting bad results overall. I'm seeking guidance on potential issues or new approaches to this project. One issue is that when I have queries where segments of the input should be used in the output(seen in the example below), the model either copies what it saw last in the dataset or produces something random that it saw in the dataset.<br />
e.g.<br />
Query: Add bread to my shopping list<br />
Response: Alright /uShoppingList'water'<br />
Query: Lower the volume by sixteen<br />
Response: Okay /uVolume'decrease four'</p>
<p>I'm worried that if I use data augmentations to grow the size of the dataset, it will ignore the other training samples like turning lights off or getting the temperature. Instead, I have tried randomly replacing shopping items(for example) with another item from a list of replacements each time getitem in the dataloader is called. With this approach, the size of the dataset stays the same but I don't know if the model will get a representation of how to do these tasks. Thus, it still fails to learn to take what was in the input and use it in the output. It instead uses random things as seen in the previous example.</p>
<h2>Details:</h2>
<p>GitHub repository: <a href=""https://github.com/NateTheGreat7117/Jarvis/tree/main"" rel=""nofollow noreferrer"">https://github.com/NateTheGreat7117/Jarvis/tree/main</a></p>
<ol>
<li>I made my own custom dataset where controls over my computer are embedded into the outputs of the chatbot's response. The dataset can be found on the GitHub repository in a file called conversation.txt. The query-response pairs are split between separate conversations in case I want to have the transformer look at previous queries when generating the response like a normal conversation. The other files in the repository are just replacements used for data augmentations.</li>
</ol>
<p>Here is an example:<br />
User: Lower the volume by sixteen<br />
Chatbot: Okay /uVolume'decrease sixteen'</p>
<ol start=""2"">
<li>I'm using a Seq2Seq transformer architecture created in PyTorch with attention layers, residual connections, and positional embedding. The encoder is a pre-trained BERT variant from HuggingFace that is being fine-tuned separately(with a different optimizer at a lower learning rate) from the decoder.</li>
</ol>
<p>Model: The model can be found in the /NeuralNetworks/Jarvis(bert encoder + Seq2Seq).ipynb file in my GitHub repository.</p>
<pre><code>def positional_encoding(length, depth):
    depth = depth/2

    positions = torch.unsqueeze(torch.arange(length), 1)
    depths = torch.unsqueeze(torch.arange(depth), 0)/depth

    angle_rates = 1 / (10000**depths)         # (1, depth)
    angle_rads = positions * angle_rates      # (pos, depth)

    pos_encoding = torch.cat(
      [torch.sin(angle_rads), torch.cos(angle_rads)],
      axis=-1) 

    return pos_encoding.to(device, dtype=torch.float32)


class PositionalEmbedding(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)
        # The positional encoding is used to introduce sequence to a sentence by causing words near 
        # eachother to have similar vectors
        self.pos_encoding = positional_encoding(length=2048, depth=d_model)

    def compute_mask(self, *args, **kwargs):
        return self.embedding.compute_mask(*args, **kwargs)

    def forward(self, x):
        length = np.shape(x)[1]
        x = self.embedding(x)
        # This factor sets the relative scale of the embedding and positonal_encoding.
        x *= math.sqrt(torch.tensor(self.d_model).type(torch.float32))
        x = x + torch.unsqueeze(self.pos_encoding, 0)[:, :length]
        return x

# Attention
class BaseAttention(nn.Module):
    def __init__(self, d_model, **kwargs):
        super().__init__()
        self.num_heads = kwargs.get('num_heads')
        self.mha = nn.MultiheadAttention(**kwargs)
        self.layernorm = nn.LayerNorm(d_model)
class CrossAttention(BaseAttention):
    def forward(self, x, context):
        x_ = x.permute(1, 0, 2)
        context_ = context.permute(1, 0, 2)
        attn_output, attn_scores = self.mha(
            query=x_,
            key=context_,
            value=context_,
            need_weights=True)
        attn_output = attn_output.permute(1, 0, 2)
        attn_scores = attn_scores.permute(1, 0, 2)

        # Cache the attention scores for plotting later.
        self.last_attn_scores = attn_scores

        x =x + attn_output
        x = self.layernorm(x)

        return x


class CausalSelfAttention(BaseAttention):
    def forward(self, x):
        x_ = x.permute(1, 0, 2)
        attention_mask = nn.Transformer.generate_square_subsequent_mask(x_.shape[0]).to(device)
        attention_mask = attention_mask.expand(x_.shape[1]*self.num_heads, -1, -1).to(device)
        
        attn_output = self.mha(
            query=x_,
            value=x_,
            key=x_,
            attn_mask=attention_mask,
            is_causal=True)[0]
        attn_output = attn_output.permute(1, 0, 2)
        x = x + attn_output
        x = self.layernorm(x)
        return x
    
sample_csa = CausalSelfAttention(d_model=256, embed_dim=128, 
                                 num_heads=2, kdim=256)

# Encoder
class FeedForward(nn.Module):
    def __init__(self, d_model, dff, dropout_rate=0.1):
        super().__init__()
        self.seq = nn.Sequential(
            nn.Linear(d_model, dff),
            nn.ReLU(),
            nn.Linear(dff, d_model),
            nn.Dropout(dropout_rate)
        ).to(device)
        self.layer_norm = nn.LayerNorm(d_model).to(device)
        
    def forward(self, x):
        x = x + self.seq(x)
        x = self.layer_norm(x)
        return x
    

class Encoder(nn.Module):
    def __init__(self, *, emb_size, d_model, dff,
                   dropout_rate=0.1):
        super(Encoder, self).__init__()

        self.bert_encoder = bert_encoder
        
        self.ffn = FeedForward(emb_size, dff)
        self.linear = nn.Linear(emb_size, d_model)

    def forward(self, x):
        input_tensor, input_type, input_mask = x
        x = self.bert_encoder(input_tensor, input_type, input_mask).last_hidden_state
        x = self.ffn(x)
        x = self.linear(x)
        return x

#Decoder
class DecoderLayer(nn.Module):
    def __init__(self,
                   *,
                   d_model,
                   num_heads,
                   dff,
                   dropout_rate=0.1):
        super(DecoderLayer, self).__init__()

        self.causal_self_attention = CausalSelfAttention(
            d_model=d_model,
            embed_dim=d_model,
            num_heads=num_heads,
            kdim=d_model,
            dropout=dropout_rate).to(device)
        
        self.cross_attention = CrossAttention(
            d_model=d_model,
            embed_dim=d_model,
            num_heads=num_heads,
            kdim=d_model,
            dropout=dropout_rate).to(device)

        self.ffn = FeedForward(d_model, dff)

    def forward(self, x, context):
        x = self.causal_self_attention(x=x)
        x = self.cross_attention(x=x, context=context)

        # Cache the last attention scores for plotting later
        self.last_attn_scores = self.cross_attention.last_attn_scores

        x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.
        return x

class Decoder(nn.Module):
    def __init__(self, *, emb_size, num_layers, d_model, num_heads, dff, vocab_size,
                   dropout_rate=0.1):
        super(Decoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.linear = nn.Linear(emb_size, d_model)
        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,
                                                 d_model=d_model).to(&quot;cuda&quot;)
        self.dropout = nn.Dropout(dropout_rate)
        self.dec_layers = [
            DecoderLayer(d_model=d_model, num_heads=num_heads,
                         dff=dff, dropout_rate=dropout_rate)
            for _ in range(num_layers)]
        self.dec_layers = nn.ModuleList(self.dec_layers)

        self.last_attn_scores = None
        
        self.final_layer = nn.Linear(d_model, vocab_size)

    def forward(self, x, context):
        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)

        x = self.dropout(x)

        for i in range(self.num_layers):
            x  = self.dec_layers[i](x, context)

        self.last_attn_scores = self.dec_layers[-1].last_attn_scores
        logits = self.final_layer(x)
        
        return logits

</code></pre>
<p>Performance Metrics: Most of the changes I make to the transformer only lead to it overfitting. The best metrics I am getting have a loss of 0.2 with PyTorch's NLLLoss, and a validation loss of around 0.55. With these metrics, I can consistently get good responses to simple queries such as asking for the time or weather. However, when I ask for anything related to changing the volume of my computer, adding items to my shopping list, or entering a Wikipedia query into a separate neural network, the output has the right format but uses random volumes/shopping items/Wikipedia queries(just some examples, there are other circumstances like these).</p>
<p>Questions:</p>
<p>What could be potential reasons for the suboptimal performance?<br />
Are there specific considerations or modifications I should make when using a pre-trained BERT encoder in a transformer for Seq2Seq tasks?<br />
How can I diagnose and address performance issues in transformer models?<br />
What other options do I have to make this work?</p>
","transformer-model"
"77805776","How to calculate word and sentence embedding using Roberta?","2024-01-12 10:05:01","77812267","1","1294","<python><machine-learning><nlp><huggingface-transformers><transformer-model>","<p>I'm trying to calculate word and sentence embeddings using Roberta, for word embeddings, I extract the last hidden state <code>outputs[0]</code> from the <code>RobertaModel</code> class, but I'm not sure if this is the correct way to calculate.</p>
<p>As for sentence embeddings, I don't know how to calculate them, this is the code I have tried:</p>
<pre><code>from transformers import RobertaModel, RobertaTokenizer
import torch

model = RobertaModel.from_pretrained('roberta-base')
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
captions = [&quot;example caption&quot;, &quot;lorem ipsum&quot;, &quot;this bird is yellow has red wings&quot;, &quot;hi&quot;, &quot;example&quot;]

encoded_captions = [tokenizer.encode(caption) for caption in captions]

# Pad sequences to the same length with 0s
max_len = max(len(seq) for seq in encoded_captions)
padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions]

# Convert to a PyTorch tensor with batch size 5
input_ids = torch.tensor(padded_captions)

outputs = model(input_ids)
word_embedding = outputs[0].contiguous()
sentence_embedding = ?????
</code></pre>
<p>How to calculate word and sentence embeddings using Roberta?</p>
","transformer-model"
"77803608","Splitting Pytorch transformer into encoder & decoder","2024-01-11 23:39:56","","0","149","<pytorch><nlp><transformer-model><decoder>","<p>[repost from pytorch forums because no one responded.]</p>
<p>Hello, I’m messing around with transformers right now, and I’m trying to modify the encoded representation with a modified LSTM (the goal is to continue text in a specific style). I’ve found an example on how to use <code>T.nn.TransformerEncoder</code>, but no examples on how to properly use <code>T.nn.TransformerDecoder</code>. How am I supposed to use it? I’ve read about how decoders work in general, but I can’t find anything about the specific pytorch implementation. How should I use it for training vs inference? do I manually have to put the output of the transformer into the tgt during inference, or is that done automatically? What does <code>tgt_is_causal</code> do?</p>
<p>I’ve included a snippet of my code if that’s useful at all.</p>
<pre><code>def forward(self, x, mhx, tgt= None):
    
    
    embedded_seq = self.embedding(x) * math.sqrt(self.emb_dim)
    embedded_seq = self.pos_encoder(embedded_seq)
    if src_mask is None:
        &quot;&quot;&quot;Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').
        Unmasked positions are filled with float(0.0).
        &quot;&quot;&quot;
        src_mask = nn.Transformer.generate_square_subsequent_mask(len(embedded_seq)).to(device)
    encoded_seq = self.transformer_encoder(embedded_seq, src_mask)
    
    
    
    # Take the encoded sequence, repeat once over the time axis, and stick that into the DNC. (to give the DNC time to analyze &amp; plan)
    processed_seq, (chx, mhx, rv) = self.vector_machine(T.cat( (encoded_seq, encoded_seq),1), (None, mhx, None), reset_experience=True, pass_through_memory=True)

    #split the processed sequence into two parts, taking the second half and adding it to the encoded sequence as a skip layer.
    processed_seq = T.chunk(processed_seq,2,dim=1)[1] + encoded_seq
    
    #TODO: put decoder here. 
    
    return decoded_seq, (chx, mhx, rv)
</code></pre>
<p>I pretty much copied the first half of this code from the pytorch transformer encoder example, but I can't find a good example for the encoder &amp; decoder I can look at. All I can find are decoder-only models that don't fit what I'm trying to do. <strong>Can someone please give me one example, or at least an explanation of how I should go about this?</strong></p>
<p>I've tried looking at the pytorch documentation for the function, but it doesn't show anything about how to use the decoder. I've tried looking for examples on github, stack overflow, and the pytorch forums, but none of them actually fit with what I'm trying to do.</p>
","transformer-model"
"77800573","How to out put attentions in Transformers BART model","2024-01-11 13:53:21","","0","36","<python><transformer-model><self-attention><bart>","<p>New to Transformers. When testing <code>BART</code>, I try to output attentions, so set the parameter <code>Output_attentions=True</code>, but results is missing the attention tensor.</p>
<p>Code:</p>
<pre><code>from transformers import AutoTokenizer, BartForConditionalGeneration

model = BartForConditionalGeneration.from_pretrained(&quot;facebook/bart-large-cnn&quot;,output_attentions=True)
tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/bart-large-cnn&quot;)

ARTICLE_TO_SUMMARIZE = (
    &quot;PG&amp;E stated it scheduled the blackouts in response to forecasts for high winds &quot;
    &quot;amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were &quot;
    &quot;scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.&quot;
)
inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=&quot;pt&quot;)

# Generate Summary
summary_ids = model.generate(inputs[&quot;input_ids&quot;], num_beams=2, min_length=0, max_length=20)
tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
</code></pre>
<p>Output:</p>
<pre><code>summary_ids:
tensor([[   2,    0, 8332,  947,  717, 1768,    5,  909, 4518,   11, 1263,    7,
         5876,   13,  239, 2372, 2876, 3841, 1274,    2]])
</code></pre>
<p>Any help is appreciated.</p>
","transformer-model"
"77783861","how can I fine tune Deplot model(VQA) + LLM model?","2024-01-09 00:53:58","","0","181","<transformer-model><large-language-model>","<p>I was trying to fine tune DePlot model from huggingface(<a href=""https://huggingface.co/google/deplot"" rel=""nofollow noreferrer"">https://huggingface.co/google/deplot</a>). It was able to load model and test with chart image to convert into table.
here is the result.
<a href=""https://i.sstatic.net/GG9tc.jpg"" rel=""nofollow noreferrer"">Deplot test image and result</a></p>
<p>Question is how can i use the result of decoded table to LLM model. if possible i wanna use with T5model for text summation and question answering.</p>
<p>Anyone can help me how to apply LLM to the deplot model?</p>
<p>I've tried to implement image captioning based on huggingface(<a href=""https://huggingface.co/docs/transformers/main/tasks/image_captioning"" rel=""nofollow noreferrer"">image captioning demo</a>) but i got error and would u help me to resolve this issue?</p>
<p>here is the code</p>
<pre class=""lang-py prettyprint-override""><code># Load Library
from torch.utils.data import Dataset, DataLoader
import torch
from tqdm import tqdm, trange
from datasets import load_dataset
from transformers import AutoProcessor, AutoModelForSeq2SeqLM
from datasets import concatenate_datasets, load_dataset

# Customized Dataset
dataset = load_dataset(&quot;/content/drive/MyDrive/sample_dataset&quot;, split=&quot;train[:90%]&quot;)
dataset

&gt;Results:
Dataset({
    features: ['image', 'metadata'],
    num_rows: 54
})

# features 
dataset.features

&gt;{'image': [{'id': Value(dtype='int64', id=None),
   'filename': Value(dtype='string', id=None),
   'width': Value(dtype='int64', id=None),
   'height': Value(dtype='int64', id=None)}],
 'metadata': {'image_id': Value(dtype='int64', id=None),
  'data_category': Value(dtype='string', id=None),
  'chart_source': Value(dtype='string', id=None),
  'chart_color': Value(dtype='string', id=None),
  'chart_multi': Value(dtype='string', id=None),
  'chart_year': Value(dtype='string', id=None),
  'chart_main': Value(dtype='string', id=None),
  'chart_sub': Value(dtype='string', id=None),
  'chart_text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}}


# Checkpoint, processor, model
checkpoint = &quot;google/deplot&quot;
processor = AutoProcessor.from_pretrained(checkpoint)
model = Pix2StructForConditionalGeneration.from_pretrained(checkpoint)

def transforms(example_batch):
    images = [x for x in example_batch[&quot;image&quot;]]
    captions = [x for x in example_batch[&quot;text&quot;]]
    inputs = processor(images=images,
                       text=captions,
                       padding=True,
                       max_length=512,
                       truncation=True )
    inputs.update({&quot;labels&quot;: inputs[&quot;input_ids&quot;]})
    return inputs



train_ds.set_transform(transforms)
test_ds.set_transform(transforms)

# evaluate function
from evaluate import load
import torch

wer = load(&quot;wer&quot;)


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predicted = logits.argmax(-1)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=True)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=True)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    return {&quot;wer_score&quot;: wer_score}

# Training args
from transformers import TrainingArguments, Trainer

model_name = checkpoint.split(&quot;/&quot;)[1]

training_args = TrainingArguments(
    output_dir=f&quot;{model_name}-ko-deplot&quot;,
    learning_rate=5e-5,
    num_train_epochs=10,
    fp16=True,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    gradient_accumulation_steps=2,
    save_total_limit=3,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=50,
    save_strategy=&quot;steps&quot;,
    save_steps=50,
    logging_steps=50,
    remove_unused_columns=False,
    push_to_hub=True,
    label_names=[&quot;labels&quot;],
    load_best_model_at_end=True,
)

# set trainer 
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)
trainer.train()

&gt; KeyError                                  Traceback (most recent call last)
&lt;ipython-input-58-3435b262f1ae&gt; in &lt;cell line: 1&gt;()
----&gt; 1 trainer.train()

12 frames
&lt;ipython-input-57-fda57d6f6ce9&gt; in transforms(example_batch)
      1 def transforms(example_batch):
      2     images = [x for x in example_batch[&quot;image&quot;]]
----&gt; 3     captions = [x for x in example_batch[&quot;text&quot;]]
      4     inputs = processor(images=images, 
      5                        text=captions,

KeyError: 'text'

</code></pre>
","transformer-model"
"77782158","How to modify this Pytorch model for faster inference? (profiling included)","2024-01-08 17:15:16","","0","107","<pytorch><neural-network><profiling><transformer-model>","<p>I would like to make the following transformer model a lot faster for inference on cpu, as right now takes 4.5 sec to execute (I used <code>summary(self.model, input_size=torch.Size([1, 256, 192])</code> from torchinfo)
<a href=""https://github.com/sony/hFT-Transformer/blob/master/model/model_spec2midi.py"" rel=""nofollow noreferrer"">model code</a></p>
<pre><code>=========================================================================================================
Layer (type:depth-idx)                                  Output Shape              Param #
=========================================================================================================
Model_SPEC2MIDI                                         [1, 128, 88]              --
├─Encoder_SPEC2MIDI: 1-1                                [1, 128, 256, 256]        --
│    └─Conv2d: 2-1                                      [128, 4, 256, 61]         24
│    └─Linear: 2-2                                      [128, 256, 256]           62,720
│    └─Embedding: 2-3                                   [128, 256, 256]           65,536
│    └─Dropout: 2-4                                     [128, 256, 256]           --
│    └─ModuleList: 2-5                                  --                        --
│    │    └─EncoderLayer: 3-1                           [128, 256, 256]           526,592
│    │    └─EncoderLayer: 3-2                           [128, 256, 256]           526,592
│    │    └─EncoderLayer: 3-3                           [128, 256, 256]           526,592
├─Decoder_SPEC2MIDI: 1-2                                [1, 128, 88]              --
│    └─Embedding: 2-6                                   [128, 88, 256]            22,528
│    └─DecoderLayer_Zero: 2-7                           [128, 88, 256]            --
│    │    └─MultiHeadAttentionLayer: 3-4                [128, 88, 256]            263,168
│    │    └─Dropout: 3-5                                [128, 88, 256]            --
│    │    └─LayerNorm: 3-6                              [128, 88, 256]            512
│    │    └─PositionwiseFeedforwardLayer: 3-7           [128, 88, 256]            262,912
│    │    └─Dropout: 3-8                                [128, 88, 256]            --
│    │    └─LayerNorm: 3-9                              [128, 88, 256]            (recursive)
│    └─ModuleList: 2-8                                  --                        --
│    │    └─DecoderLayer: 3-10                          [128, 88, 256]            789,760
│    │    └─DecoderLayer: 3-11                          [128, 88, 256]            789,760
│    └─Linear: 2-9                                      [128, 88, 1]              257
│    └─Sigmoid: 2-10                                    [1, 128, 88]              --
│    └─Linear: 2-11                                     [128, 88, 1]              257
│    └─Sigmoid: 2-12                                    [1, 128, 88]              --
│    └─Linear: 2-13                                     [128, 88, 1]              257
│    └─Sigmoid: 2-14                                    [1, 128, 88]              --
│    └─Linear: 2-15                                     [128, 88, 128]            32,896
│    └─Embedding: 2-16                                  [88, 128, 256]            32,768
│    └─Dropout: 2-17                                    [88, 128, 256]            --
│    └─ModuleList: 2-18                                 --                        --
│    │    └─EncoderLayer: 3-12                          [88, 128, 256]            526,592
│    │    └─EncoderLayer: 3-13                          [88, 128, 256]            526,592
│    │    └─EncoderLayer: 3-14                          [88, 128, 256]            526,592
│    └─Linear: 2-19                                     [88, 128, 1]              257
│    └─Sigmoid: 2-20                                    [1, 128, 88]              --
│    └─Linear: 2-21                                     [88, 128, 1]              257
│    └─Sigmoid: 2-22                                    [1, 128, 88]              --
│    └─Linear: 2-23                                     [88, 128, 1]              257
│    └─Sigmoid: 2-24                                    [1, 128, 88]              --
│    └─Linear: 2-25                                     [88, 128, 128]            32,896
=========================================================================================================
Total params: 5,516,574
Trainable params: 5,516,574
Non-trainable params: 0
Total mult-adds (M): 688.90
=========================================================================================================
Input size (MB): 0.20
Forward/backward pass size (MB): 3820.50
Params size (MB): 22.07
Estimated Total Size (MB): 3842.77
=========================================================================================================

</code></pre>
<p>Profiling through <code>torch.profiler</code> returns</p>
<pre><code>----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  
----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
             model_inference         9.38%     430.768ms       100.00%        4.591s        4.591s             1  
                aten::matmul         1.23%      56.495ms        72.63%        3.335s      35.856ms            93  
                aten::linear         0.03%       1.353ms        55.15%        2.532s      35.667ms            71  
                    aten::mm        53.39%        2.451s        53.39%        2.451s      34.527ms            71  
                   aten::bmm        14.66%     673.043ms        14.66%     673.265ms      30.603ms            22  
               aten::softmax         0.00%     144.000us         4.67%     214.293ms      19.481ms            11  
              aten::_softmax         4.66%     214.149ms         4.66%     214.149ms      19.468ms            11  
                 aten::clone         0.02%     866.000us         4.57%     209.970ms       4.117ms            51  
                 aten::copy_         4.54%     208.307ms         4.54%     208.307ms       3.858ms            54  
             aten::clamp_min         1.69%      77.715ms         3.38%     155.278ms       8.627ms            18  
----------------------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 4.591s
</code></pre>
<p>While I can see the bottleneck from the profiler, I am not sure if I could make the model a lot faster than what it is now.
Any ideas?</p>
","transformer-model"
"77762264","TF Transformer model never overfits and just plateaus: Interpretation of this training curve and suggestions for improvement","2024-01-05 02:47:25","77772939","0","357","<python><machine-learning><keras><deep-learning><transformer-model>","<p>This training curve is for a Transformer model that processes 2D (excluding batch) sequential signal and uses Adam optimizer, 32 batch size and for the learning rate: a custom LR Scheduler that replicates the warmup scheduler that is used at 'Attention is All You Need' paper. Training curve as below plateaus with eventual Training loss slightly lower than Validation loss, but training loss never starts back to climb, which I interpreted as the model never starts overfitting and just stops re-adjusting weights after around epoch 90.</p>
<p>Better interpretation and solutions to improve this model?</p>
<p><a href=""https://i.sstatic.net/Fi5OY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Fi5OY.png"" alt=""enter image description here"" /></a></p>
<p>Below is my brief reproducible code:</p>
<pre><code>x_train = np.random.normal(size=(32, 512, 512))
batch_size = 32
H, W = x_train.shape
rows, cols = np.indices((H, W), sparse=True)
padding_mask_init = np.zeros((H, W, W), dtype=np.bool_)
padding_mask_init[rows, 1:, cols] = 1
padding_mask = padding_mask_init[:batch_size]
embed_dim = 512
dense_dim = 2048
num_heads = 2
shape = (batch_size, embed_dim, 512) #(32, 512, 512)
decoder_inputs = layers.Input(batch_input_shape=shape, dtype=tensorflow.float16)
mha_1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
mha_2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
layernorm_1 = layers.LayerNormalization()

Z = decoder_inputs
Z = mha_1(query=Z, value=Z, key=Z, use_causal_mask=True, attention_mask=padding_mask)
Z = layernorm_1(Z + decoder_inputs)
Z = mha_2(query=Z, value=decoder_inputs, key=decoder_inputs, attention_mask=padding_mask)
outputs = layers.TimeDistributed(keras.layers.Dense(embed_dim, activation=&quot;softmax&quot;))(Z)

model = keras.Model(decoder_inputs, outputs)
model.compile(loss=&quot;mean_squared_error&quot;, optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule(embed_dim, 3000),beta_1=0.9,beta_2=0.98,epsilon=1.0e-9), metrics=[&quot;accuracy&quot;])

history = model.fit(dataset, epochs=200, validation_data=val_dataset)
</code></pre>
","transformer-model"
"77754035","Is the mask in attention doing the same job as positional encoding?","2024-01-03 18:47:01","","1","979","<nlp><transformer-model><gpt-2>","<p>I am new to transformer decoder and confused about the mask in attention. It seems to mask all the words before a particular word. If that is what it does, then is it making the network position-aware, so that positional encoding is no more needed?</p>
<p>Because let's assume no positional encoder. Consider inputs &quot;I am good&quot; and &quot;am I good&quot;. Let's say after the processing of the first decoder, the &quot;I&quot; will become the vector x, &quot;am&quot; will become y, &quot;good&quot; will become z. The z of these two input sequences will be exactly the same. But, x and y will be completely different because of the mask in decoder. Then when x, y, z as the inputs of the second decoder, the output of z will be different between these two sequences because x and y are different. So the whole network is actually position-aware.</p>
<p>Am I missing something?</p>
<p>I tried read papers but haven't figure it out, thanks for help.</p>
","transformer-model"
"77749499","how to get the input of the deep learning model","2024-01-03 02:54:46","","0","25","<deep-learning><transformer-model>","<p>I trained a transformer encoder model with 10 dimensional inputs and 300 dimensional outputs. How can I backtrack to get this 10 dimensional input now that I have a new output? My current idea is to use mcmc but it's too slow, does anyone have a more efficient method?</p>
","transformer-model"
"77748737","How to calculate word and sentence embedding using GPT-2?","2024-01-02 21:55:52","77751619","0","946","<python><machine-learning><nlp><huggingface-transformers><transformer-model>","<p>I'm working on a program that calculates word and sentence embeddings using GPT-2, specifically the <code>GPT2Model</code> class. For word embedding, I extract the last hidden state <code>outputs[0]</code> after forwarding the <code>input_ids</code>, that has a shape of <code>batch size x seq len</code>, to the <code>GPT2Model</code> class. As for sentence embedding, I extract the hidden state of the word at the end of sequence. This is the code I have tried:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')
captions = [&quot;example caption&quot;, &quot;example bird&quot;, &quot;the bird is yellow has red wings&quot;, &quot;hi&quot;, &quot;very good&quot;]

encoded_captions = [tokenizer.encode(caption) for caption in captions]

# Pad sequences to the same length with 0s
max_len = max(len(seq) for seq in encoded_captions)
padded_captions = [seq + [0] * (max_len - len(seq)) for seq in encoded_captions]

# Convert to a PyTorch tensor with batch size 5
input_ids = torch.tensor(padded_captions)

outputs = model(input_ids)
word_embedding = outputs[0].contiguous()
sentence_embedding = word_embedding[ :, -1, : ].contiguous()

</code></pre>
<p>I'm not sure if my calculation for word and sentence embedding are correct, can anyone help me confirm this?</p>
","transformer-model"
"77746809","Use nn.transformerEncoder for context-free grammar parsing (sequence classification)","2024-01-02 14:51:28","","0","43","<python><deep-learning><pytorch><nlp><transformer-model>","<p>I want to use a transformer to do context-free grammar parsing (to classify whether a sequence is in the grammar or not). The input are sequences like &quot;abbaba&quot;, the output is 0 or 1. Here's my transformer architecture:</p>
<pre><code>class PositionalEncoding(nn.Module):

    def __init__(self, d_model, max_len=200, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float()
            * (-math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer(&quot;pe&quot;, pe)

    def forward(self, x):
        # print(self.pe.shape, x.shape)
        x = x + self.pe[:, : x.size(1), :]
        return self.dropout(x)


class TransformerClassifier(nn.Module):
    def __init__(self, input_size, d_model, num_classes, num_layers, nhead, dim_feedforward, dropout):
        super(TransformerClassifier, self).__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(input_size, d_model)

        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)

        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)

        self.pos_encoder = PositionalEncoding(d_model=d_model, dropout=dropout)
        
        self.linear = nn.Linear(d_model, num_classes)

    def forward(self, x):
        x = self.embedding(x) * math.sqrt(self.d_model)
        x = self.pos_encoder(x)
        x = self.transformer_encoder(x)
        x = x.mean(dim=1)  # Aggregate across the sequence dimension
        x = self.linear(x)
        return x
</code></pre>
<p>I set the hyperparameters to:</p>
<pre><code>input_size = 27 # 26 letters + &lt;pad&gt;
d_model = 50
dim_feedforward = 50
nhead = 5
num_classes = 2  # Binary classification (0 or 1)
num_layers = 4
dropout = 0.1
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code></pre>
<p>The lr was set to 1e-4, batch size was 20.
The training loop:</p>
<pre><code>epochs = 100
epoch_losses = []
for epoch in range(epochs):
    batch_loss = []
    for inputs, targets in tqdm(train_dataloader):
        inputs = inputs.to(device)
        targets = targets.to(device)
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        batch_loss.append(loss.item())

    epoch_loss = sum(batch_loss) / len(batch_loss)
    epoch_losses.append(sum(batch_loss) / len(batch_loss))
    print(f&quot;Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss:.4f}&quot;)
</code></pre>
<p>However the result seems to be abnormal.
The training curve seems to be a bit noisy:
<a href=""https://i.sstatic.net/ABcNy.png"" rel=""nofollow noreferrer""></a>
Also, even if I use a very large transformer (large values for d_model, nhead, num_layer, etc), the accuracy on the training set is about 0.7-0.8 and don't go higher. Also, sometimes the accuracy on the test set was even higher than the training set. Additionally, I want to test how the transformer perform on OOD data (I set OOD here to be longer sequences, and only train the model with shorter sequences), I found that the accuracy on OOD test set was sometimes higher than ID (in distribution) test set, and sometimes even higher than training set.
<a href=""https://i.sstatic.net/AKEyA.png"" rel=""nofollow noreferrer""></a>
I was very confused and was wondering what's wrong.</p>
<p>I want to know if there's anything wrong with my codes or are there any other mistakes.</p>
","transformer-model"
"77732565","Understanding the Classification of Sinusoidal Model as Absolute Positional Encoding in Transformer Architecture","2023-12-29 14:46:22","","1","93","<transformer-model><self-attention>","<p>I am currently reading in depth about positional encodings, and as we know there are two types of positional encodings: Absolute and relative.</p>
<h4>My question:</h4>
<p>Why in some literature is the sinusoidal model classified as absolute positional encoding, given that in Vaswani's original paper it was said that it captures relative relationships between words, and this has been proven <a href=""https://blog.timodenk.com/linear-relationships-in-the-transformers-positional-encoding/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>However, while I was reading a <a href=""https://direct.mit.edu/coli/article/48/3/733/111478/Position-Information-in-Transformers-An-Overview"" rel=""nofollow noreferrer"">research</a>, it was mentioned that projections that occur in the attention layer destroy this:</p>
<blockquote>
<p>Indeed, sinusoidal position embeddings exhibit useful properties in theory. Yan et al. (2019) investigate the dot product of sinusoidal position embeddings and prove important properties:
(1) The dot product of two sinusoidal position embeddings depends only on their relative distance. That is, <img src=""https://latex.codecogs.com/svg.image?&amp;space;P_t%5ET.P_%7Bt+r%7D"" alt="" P_t^T.P_{t+r}"" /> is independent of <img src=""https://latex.codecogs.com/svg.image?t"" alt=""t"" />.
(2) <img src=""https://latex.codecogs.com/svg.image?&amp;space;P_t%5ET.P_%7Bt+r%7D=P_t%5ET.P_%7Bt-r%7D"" alt=""P_t^T.P_{t+r}=P_t^T.P_{t-r}"" />, which means that sinusoidal position embeddings are unaware of direction. However, in practice the sinusoidal embeddings are projected with two different projection matrices, which destroys these properties.</p>
</blockquote>
<p>Is this the reason?</p>
","transformer-model"
"77724631","Advice for using multilingual model to improve translation","2023-12-28 01:15:53","","0","38","<nlp><artificial-intelligence><translation><multilingual><transformer-model>","<p>I am working on a project that tries to improve low-resource language translation. Here is an example:</p>
<p>Suppose I am trying to translate German to English. To make it realistic, the data is limited to 20,000 german sentences and english translations.</p>
<p>I want to train a sequence to sequence model for translation. So, would training a multilingual many-to-one model on languages related to German, like Danish, Dutch, Afrikaans, etc... improve the quality of my English to German translation.</p>
<p>If anyone has any ideas or recommendations, please let me know. Thank you</p>
","transformer-model"
"77718720","Changing the Attention Layer of a Transformer","2023-12-26 18:53:15","","-2","764","<python><pytorch><transformer-model><attention-model>","<p>I would like to test my own formulation of the attention mechanism for a transformer. To that end I would like to find an existing pre trained transformer that is easy to read through and that uses not too large of a dataset. I just want to take that model and replace the code of the attention mechanism with my own.</p>
<p>I have already tried using <a href=""https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;opi=89978449&amp;url=https://github.com/hkproj/pytorch-transformer&amp;ved=2ahUKEwiKjpDF4q2DAxX-WUEAHXDMA8sQFnoECBIQAQ&amp;usg=AOvVaw2HTqVDl_mTK23mqiuQ7_wH"" rel=""nofollow noreferrer"">https://www.google.com/url?sa=t&amp;source=web&amp;rct=j&amp;opi=89978449&amp;url=https://github.com/hkproj/pytorch-transformer&amp;ved=2ahUKEwiKjpDF4q2DAxX-WUEAHXDMA8sQFnoECBIQAQ&amp;usg=AOvVaw2HTqVDl_mTK23mqiuQ7_wH</a> but for some reason I get an error after the first epoch with my code. The epoch takes about 30 mins so I'm struggling to debug to see where I've gone wrong.</p>
<p>I've also tried looking at the hugging face github repository but there are so many options and I'm lost there. I'm very new to deep learning and this is my first time trying to code a model.</p>
<p>Thanks</p>
","transformer-model"
"77714877","Model Accuracy Using Transformer","2023-12-25 20:14:02","","0","41","<sentiment-analysis><bert-language-model><floating-accuracy><transformer-model><imdb>","<p>I'm trying to increase the Accuracy of my Model. The model aims to leverage BERT's contextual understanding of language to perform binary classification on IMDb movie reviews. By fine-tuning specific layers of the pre-trained BERT model and integrating it into a neural network, the goal is to achieve high accuracy in classifying sentiment polarity (positive/negative) of movie reviews from the IMDb dataset.</p>
<p>Below is the code:</p>
<pre><code># Load the dataset but only keep the top n words, zero the rest
top_words = 5000
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)

# Truncate and pad input sequences
max_review_length = 500
X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)
X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)


# Convert integer sequences back to text
X_train_texts = [' '.join(map(str, x)) for x in X_train]
X_test_texts = [' '.join(map(str, x)) for x in X_test]

# Tokenize the input sequences
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Truncate sequences to fit within BERT's maximum sequence length
max_sequence_length = 512
X_train = [tokenizer.encode(text, add_special_tokens=True, max_length=max_sequence_length, truncation=True) for text in X_train_texts]
X_test = [tokenizer.encode(text, add_special_tokens=True, max_length=max_sequence_length, truncation=True) for text in X_test_texts]

# Pad the tokenized sequences
X_train = sequence.pad_sequences(X_train, maxlen=max_sequence_length, padding='post', truncating='post')
X_test = sequence.pad_sequences(X_test, maxlen=max_sequence_length, padding='post', truncating='post')

# Load pre-trained BERT model
bert_model = TFBertModel.from_pretrained('bert-base-uncased')

# Define input layer
input_layer = Input(shape=(max_sequence_length,), dtype='int32')

# BERT layer
bert_output = bert_model(input_layer)[0]

# Flatten layer
flatten_layer = Flatten()(bert_output)

# Output layer
output_layer = Dense(1, activation='sigmoid')(flatten_layer)

# Create model
model = Model(inputs=input_layer, outputs=output_layer)

# Make BERT layers trainable
for layer in bert_model.layers:
    if layer.name.startswith('pooler'):
        layer.trainable = True
    else:
        layer.trainable = False

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())


# Train the model
model.fit(X_train, y_train, epochs=3, batch_size=64)

# Evaluate the model
scores = model.evaluate(X_test, y_test, verbose=32)
print(&quot;BERT-based Model Accuracy: %.2f%%&quot; % (scores[1] * 100))
</code></pre>
<p>Using the above code. I am getting this accuracy with <strong>3 Epochs</strong></p>
<p><strong><a href=""https://i.sstatic.net/wt4VU.png"" rel=""nofollow noreferrer"">Accuracy Screenshot with 3 Epochs</a></strong></p>
<p>When I tried using <strong>10 Epochs</strong>, I didn't see any great difference and it took almost 6 to 8 hours on <strong>Kaggle Notebook</strong> using <strong>GPU T4x2</strong>. It did stop due to inactivity insufficient memory of might be due to inactivity. I'm attaching the image as well so you guys can get an idea.</p>
<p><strong><a href=""https://i.sstatic.net/CU3qU.png"" rel=""nofollow noreferrer"">Screenshot failed attempt using 10 Epochs</a>.</strong></p>
<p>Kindly, share some suggestions and solution to improve the accuracy of this model.</p>
","transformer-model"
"77711856","How to make Data Loader for ""Multi-Head"" Regression which can be used with HuggingFace Transformers & Trainer?","2023-12-24 20:49:28","","3","200","<pytorch><nlp><regression><huggingface-transformers><transformer-model>","<p>I am working on a Multi head regression problem where for each text I want to predict 5 scores. You can do this by setting <code>problem_type = 'regression'</code> <a href=""https://github.com/huggingface/transformers/blob/7ae6f070044b0171a71f3269613bf02fd9fca6f2/src/transformers/models/deberta_v2/modeling_deberta_v2.py#L1327C1-L1333C1"" rel=""nofollow noreferrer"">as given in transformers code</a></p>
<p>Issue is that when I run my model with <code>Trainer</code>, it gives an error like:</p>
<h1>Error:</h1>
<pre><code>raise ValueError(
ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`labels` in this case) have excessive nesting (inputs type `list` where type `int` is expected).
</code></pre>
<p>It worked with <code>num_classes = 1</code> but when I do it with 5, it throws this error. Below are the minimal code for my model, data.</p>
<h1>Model</h1>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;, 
                                                           num_labels=5, 
                                                          problem_type = &quot;regression&quot;)
</code></pre>
<h1>Custom DataLoader:</h1>
<pre><code>class MultiRegressionDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels):
        self.labels = labels
        self.texts = texts
        

    def __getitem__(self, idx, sanity_check = False):
        output = tokenizer(self.texts[idx], truncation=True,
                              padding=&quot;max_length&quot;,
                              max_length = 128) # This returns a dict

        output['labels'] = torch.tensor(self.labels[idx])
        
        return output

data = MultiRegressionDataset([&quot;text1&quot;, &quot;text2&quot;], [[1,2,3,4,5], [5,4,3,2,1]])

data.__getitem__(0) # Gives a value
</code></pre>
<p>Tried doing it with</p>
<ol>
<li><code>output['labels'] = torch.tensor(self.labels[idx]).unsqueeze(-1)</code></li>
<li>Combination of  <code>return_tensors = &quot;pt&quot;</code> with the above</li>
</ol>
<p>Nothing worked. What am I doing wrong here?</p>
","transformer-model"
"77710324","should I use mask in pytorch encoder only model?","2023-12-24 10:36:56","","0","88","<nlp><transformer-model><language-translation>","<p>I want to use transformer to build one translation model, and I have achieve one encoder-decoder model by pytorch.</p>
<p>but now I wounder could I use encoder only to achieve this, and I have write some code</p>
<pre class=""lang-py prettyprint-override""><code>class Transformer(nn.Module):
    def __init__(self):
        super(Transformer, self).__init__()
        self.device = device
        self.src_emb = nn.Embedding(src_vocab_size, d_model)
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=d_ff, batch_first=True),
            num_layers=n_layers,
        )
        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=True)

    def forward(self, src):
        src_padding_mask = (src == 0)
        # src_mask = nn.Transformer.generate_square_subsequent_mask(src.size(1)).bool().to(self.device)
        memory = self.src_emb(src)
        # memory = self.transformer_encoder(memory, src_key_padding_mask=src_padding_mask, mask=src_mask)
        memory = self.transformer_encoder(memory, src_key_padding_mask=src_padding_mask)
        output = self.projection(memory)
        return output.view(-1, output.shape[-1])
</code></pre>
<p>but i don't know if I should use src_mask or not,</p>
<p>by reading other author answer, I know that we can use the nn.encoder with mask to achieve gpt like model.</p>
<p>so i think the nn.encoder only with mask may be correct, but i don't know without that mask this code is right or not.</p>
<p>that's all, thanks.</p>
<p>could u tell me something about this?</p>
","transformer-model"
"77697872","Question answering model of hugging face transformers shows errors in live server","2023-12-21 12:10:23","","0","76","<transformer-model><nlp-question-answering>","<p>I am using deepset/roberta-base-squad2 model of hugging face transformer for question answering in my chatbot in python, django framework, this model is working fine in local site but not working in live server</p>
<p>Below is my code</p>
<pre class=""lang-py prettyprint-override""><code>def Chatbot(request):
    if request.method == 'POST':
        question = request.POST['message']
        print(question)
        ans = ''
        data1 = Chat.objects.filter(message__icontains=question)
        for i in data1:
            print(i.response)
            ans += i.response
        print(ans)
        if ans:
            return JsonResponse({'response':ans})
        else:
            starting_time = time.time()
            print(starting_time)
            data = Chat.objects.get(id=1)
            passage = data.response 
            answerer = Extract_Answer()
            answer = answerer.Answer(question, passage)
            
            total_time = time.time() - starting_time
            print(total_time)
            if answer:
                print(&quot;Answer:&quot;, answer)
                return JsonResponse({'response':answer})
            else:
                return JsonResponse({'response':'Answer Not Found'})
    return render(request, 'chatbot.html')

class Extract_Answer():
    def __init__(self):
        self.model_name = &quot;deepset/roberta-base-squad2&quot;
        self.qa_pipeline = pipeline(&quot;question-answering&quot;, model=self.model_name, tokenizer=self.model_name)
    
    def Answer(self, question, passage):
    
        QA_input = {
            'question':question,
            'context':passage
        }
        res = self.qa_pipeline(QA_input)
        print(res)
        print(res['answer'])
        return res['answer'] 
</code></pre>
<p>Then this error I'm getting in live server</p>
<pre class=""lang-py prettyprint-override""><code>  Truncated or oversized response headers received from daemon process 'djangoproject': /var/www/html/humari/djangoproject/wsgi.py, referer: https://chat.humaricoding.com/
</code></pre>
","transformer-model"
"77688790","Image transformer model for image inpainting not converging on FashionMNIST","2023-12-20 01:25:40","","0","45","<python><deep-learning><computer-vision><transformer-model><vision-transformer>","<p>I am trying to do an image inpainting task using a transformer model. I randomly mask out some pixels in an image (setting the corresponding positions to 0), then divide it into patches like ViT does. I embed each patch (using convolution or FC) to get a 1D sequence and feed that into the decoder part of the transformer (depth = 6) to generate the original image size. I use MSE loss between the masked pixels and generated pixels.</p>
<p>However, my model is currently not converging. I have tried varying learning rate, batch size, and patch size with no success. The output is very noisy/artifacted. To avoid slow training with large images, I am using the 28x28 images from FashionMNIST (60k images). I thought this dataset could help verify if my model setup makes sense before moving to larger images.</p>
<p>Currently unclear why it's not converging. I discussed with a friend and he said it's because 28x28 is too small and some pixels are masked, so the model cannot learn. But I feel that a simpler distribution from smaller images should be easier to learn, and my mask ratio is not very high compared to other CV papers.</p>
<p>My questions:</p>
<p>Does image size really affect convergence that much in this case?
Apart from image size, what other potential model configuration issues could lead to lack of convergence?
Any other suggestions for my task?</p>
","transformer-model"
"77683979","Why does t5 tokenizer may sometimes output the ▁ as a separate token from the following word?","2023-12-19 09:02:09","","0","76","<nlp><huggingface-transformers><transformer-model><large-language-model><huggingface-tokenizers>","<p>I fine-tuned my T5-model on a dataset, and when I plot my heat map attention, I see some weird behavior in the tokenizer. For example in the attached picture, the underscore and &quot;eat&quot; are on separate rows as well as _ and &quot;he&quot;, while other underscores and words are on the same rows. What is causing this behavior, is this a bug? <a href=""https://i.sstatic.net/LAK5f.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LAK5f.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"77680618","How to train a customized transformer model with custom dataset formatting","2023-12-18 16:56:58","","4","186","<python><machine-learning><artificial-intelligence><classification><transformer-model>","<p>I am making a custom transformer model that fits specific AI model for converting a user prompt into three differnt classes {&quot;lvl1&quot;, &quot;lvl2&quot;, &quot;lvl3&quot;}, every level goes to a a different decoder that converts each levle text to a seperate task to be done.
I decided to make the classification with a decoder, and takes the output of this decoder and passes it to the other decoders.
the user prompts should be a task to be done like &quot;I Want to create a word doc and write in it an article about globalization and send it to mom in whatsapp as pdf file&quot;
this prompt shuld be classified before it goes to the other decoders.</p>
<p>the problem is that after I made the dataset, the training process goes wrong and I searched a lot in a way to get the classified output but I found nothing.</p>
<p>in the dataset I have a pronpt and the result, then I converted it into the following format:</p>
<pre class=""lang-py prettyprint-override""><code>{
    &quot;prompt&quot;: &quot;Open a music composition software and compose a short piece inspired by nature&quot;,
    &quot;classification&quot;: {
        &quot;lvl1&quot;: {
            &quot;start&quot;: [
                321
            ],
            &quot;sequence&quot;: [
                321,
                269,
                539,
                1131,
                419
            ],
            &quot;text&quot;: &quot;Open a music composition software&quot;
        },
        &quot;lvl2&quot;: {
            &quot;start&quot;: [
                1013
            ],
            &quot;sequence&quot;: [
                1013,
                269,
                493,
                877,
                1072,
                1003,
                1026
            ],
            &quot;text&quot;: &quot;compose a short piece inspired by nature&quot;
        },
        &quot;lvl3&quot;: {
            &quot;start&quot;: [
                -1
            ],
            &quot;sequence&quot;: [],
            &quot;text&quot;: &quot;&quot;
        }
    }
}
</code></pre>
<p>&quot;start&quot; is the first word token
&quot;sequence&quot; is the sequence of tokens for the lvel text
&quot;text&quot; is the text that should I got as a classification</p>
<p>the input is the &quot;prompt&quot;
and the result should be the &quot;classification&quot;</p>
<p>the encoder takes all the &quot;prompts&quot;
but I don't How to give the decoder the &quot;classifications&quot;</p>
<p>encoder code</p>
<pre class=""lang-py prettyprint-override""><code>class EncoderBlock(tf.keras.layers.Layer):
    def __init__(self, dimension_model, num_heads, hidden_dimension, dropout_rate=0.1):
        super(EncoderBlock, self).__init__()

        self.mhsa = MultiHeadSelfAttention(dimension_model, num_heads)
        self.ffn = feed_forward_network(dimension_model, hidden_dimension)

        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)

        self.layernorm1 = tf.keras.layers.LayerNormalization()
        self.layernorm2 = tf.keras.layers.LayerNormalization()
    
    def call(self, x, training, mask):
        mhsa_output, attention_weights = self.mhsa(x, x, x, mask)
        # drop out
        mhsa_output = self.dropout1(mhsa_output, training=training)
        # skip connection
        mhsa_output = self.layernorm1(x + mhsa_output)

        ffn_output = self.ffn(mhsa_output)
        ffn_output = self.dropout2(ffn_output, training=training)
        output = self.layernorm2(mhsa_output + ffn_output)

        return output, attention_weights
class Encoder(tf.keras.layers.Layer):
    def __init__(self, num_blocks, dimension_model, num_heads, hidden_dimension, src_vocab_size, max_seq_len, dropout_rate=0.1):
        super(Encoder, self).__init__()

        self.dimension_model = dimension_model
        self.max_sql_len = max_seq_len

        self.token_embedding = tf.keras.layers.Embedding(src_vocab_size, self.dimension_model)
        self.positonal_embedding = tf.keras.layers.Embedding(max_seq_len, self.dimension_model)

        self.dropout = tf.keras.layers.Dropout(dropout_rate)

        self.blocks = [EncoderBlock(self.dimension_model, num_heads, hidden_dimension, dropout_rate)
                       for _ in range(num_blocks)]
    
    def call(self, input, training, mask):
        token_embeddings = self.token_embedding(input)

        num_pos = input.shape[0] * self.max_sql_len
        positional_index = np.resize(np.arange(self.max_sql_len), num_pos)
        positional_index = np.reshape(positional_index, input.shape)
        positional_embeddings = self.positonal_embedding(positional_index)

        x = self.dropout(token_embeddings + positional_embeddings, training=training)

        for block in self.blocks:
            x, weights = block(x, training, mask)
        
        return x, weights
</code></pre>
<p>decoder code</p>
<pre class=""lang-py prettyprint-override""><code>class DecoderBlock(tf.keras.layers.Layer):
  def __init__(self, d_model, num_heads, hidden_dim, dropout_rate=0.1):
    super(DecoderBlock, self).__init__()

    self.mhsa1 = MultiHeadSelfAttention(d_model, num_heads)
    self.mhsa2 = MultiHeadSelfAttention(d_model, num_heads)

    self.ffn = feed_forward_network(d_model, hidden_dim)

    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)
    self.dropout3 = tf.keras.layers.Dropout(dropout_rate)

    self.layernorm1 = tf.keras.layers.LayerNormalization()
    self.layernorm2 = tf.keras.layers.LayerNormalization()
    self.layernorm3 = tf.keras.layers.LayerNormalization()
  
  # Note the decoder block takes two masks. One for the first MHSA, another
  # for the second MHSA.
  def call(self, encoder_output, target, training, decoder_mask, memory_mask):
    mhsa_output1, attn_weights = self.mhsa1(target, target, target, decoder_mask)
    mhsa_output1 = self.dropout1(mhsa_output1, training=training)
    mhsa_output1 = self.layernorm1(mhsa_output1 + target)

    mhsa_output2, attn_weights = self.mhsa2(mhsa_output1, encoder_output, 
                                            encoder_output, 
                                            memory_mask)
    mhsa_output2 = self.dropout2(mhsa_output2, training=training)
    mhsa_output2 = self.layernorm2(mhsa_output2 + mhsa_output1)

    ffn_output = self.ffn(mhsa_output2)
    ffn_output = self.dropout3(ffn_output, training=training)
    output = self.layernorm3(ffn_output + mhsa_output2)

    return output, attn_weights
class Decoder(tf.keras.layers.Layer):
  def __init__(self, num_blocks, d_model, num_heads, hidden_dim, target_vocab_size,
               max_seq_len, dropout_rate=0.1):
    super(Decoder, self).__init__()

    self.d_model = d_model
    self.max_seq_len = max_seq_len

    self.token_embed = tf.keras.layers.Embedding(target_vocab_size, self.d_model)
    self.pos_embed = tf.keras.layers.Embedding(max_seq_len, self.d_model)

    self.dropout = tf.keras.layers.Dropout(dropout_rate)

    self.blocks = [DecoderBlock(self.d_model, num_heads, hidden_dim, dropout_rate) for _ in range(num_blocks)]

  def call(self, encoder_output, target, training, decoder_mask, memory_mask):
    token_embeds = self.token_embed(target)

    # Generate position indices.
    num_pos = target.shape[0] * self.max_seq_len
    pos_idx = np.resize(np.arange(self.max_seq_len), num_pos)
    pos_idx = np.reshape(pos_idx, target.shape)

    pos_embeds = self.pos_embed(pos_idx)

    x = self.dropout(token_embeds + pos_embeds, training=training)

    for block in self.blocks:
      x, weights = block(encoder_output, x, training, decoder_mask, memory_mask)

    return x, weights
</code></pre>
","transformer-model"
"77677454","When using a transformer model, is numeric data also required for embedding?","2023-12-18 07:10:45","","0","229","<pytorch><time-series><transformer-model>","<p>I am currently learning by looking at the transformer example in the link below.</p>
<p><a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">Transformer tutorial</a></p>
<p>In the example in the link, the alphabet was used as categorical data. For this reason, They used the embedding function in the model to increase the feature dimension.</p>
<p>But I want to apply transformer to numercial time series data. Ultimately, the goal is to predict the class of each point in time series data.<br />
In this case, should embedding be implemented through the Linear function like the alphabet? Or can I apply the raw data directly to the model without the need for embedding?</p>
<p>Thanks for reading.<br />
sincerely.</p>
","transformer-model"
"77671804","How to solve: RuntimeError: CUDA error: device-side assert triggered?","2023-12-16 17:17:20","77674597","-2","2517","<python><pytorch><transformer-model>","<p>I want to use the <em>paraphrase-multilingual-mpnet-base-v2</em> model to build embeddings and I got this error:</p>
<p><strong>RuntimeError: CUDA error: device-side assert triggered</strong></p>
<p>The error occurs by executing <code>string = {k: v.to(device=device) for k, v in string.items()}</code>.</p>
<p><strong>Why do I get the error?</strong></p>
<p>I work in a Google Colab with 12.7 GB RAM and 16 GB GPU-RAM</p>
<p>The goal of the code is to generate sentence embeddings. With some customizing is a chunk-wise execution also possible.</p>
<p>The complete error message:</p>
<pre><code>RuntimeError                              Traceback (most recent call last) &lt;ipython-input-17-8e6bf00d9e24&gt; in &lt;cell line: 104&gt;()
    102     return np.nan
    103 
--&gt; 104 processed_data = processDataRAG(df[5000:], tokenizer, model)

4 frames &lt;ipython-input-17-8e6bf00d9e24&gt; in processDataRAG(data, tokenizer, model)
     10   sents = [str(sentences[0]) for sentences in article_sentences]
     11   number_of_article =[sentences[1] for sentences in article_sentences]
---&gt; 12   embedded_sentencs = [embeddChunkwise(sentence, tokenizer, model, 512) for sentence in tqdm(sents, desc = &quot;Create chunk-wise embeddings&quot;)]
     13   return pd.DataFrame({
     14       &quot;sentences&quot;: sents,

&lt;ipython-input-17-8e6bf00d9e24&gt; in &lt;listcomp&gt;(.0)
     10   sents = [str(sentences[0]) for sentences in article_sentences]
     11   number_of_article =[sentences[1] for sentences in article_sentences]
---&gt; 12   embedded_sentencs = [embeddChunkwise(sentence, tokenizer, model, 512) for sentence in tqdm(sents, desc = &quot;Create chunk-wise embeddings&quot;)]
     13   return pd.DataFrame({
     14       &quot;sentences&quot;: sents,

&lt;ipython-input-17-8e6bf00d9e24&gt; in embeddChunkwise(string, tokenizer, model, chunk_size)
     55     #encoded_input = tokenizer(tokenizer.detokenize(tokenized_chunk))
     56     if len(encoded_chunk) &gt; 0:
---&gt; 57       embedded_chunk = createEmbeddings(
     58           tokenizer(tokenizer.decode(encoded_chunk, skip_special_tokens  = True), return_tensors='pt', add_special_tokens=False),
     59           model

&lt;ipython-input-17-8e6bf00d9e24&gt; in createEmbeddings(string, model)
     77   #print(&quot;Length of input_ids: &quot;, len(string[&quot;input_ids&quot;][0]))
     78   if &quot;

input_ids&quot; in string.keys():
---&gt; 79     string = {k: v.to(device=device) for k, v in string.items()}
     80     with torch.no_grad():
     81 

&lt;ipython-input-17-8e6bf00d9e24&gt; in &lt;dictcomp&gt;(.0)
     77   #print(&quot;Length of input_ids: &quot;, len(string[&quot;input_ids&quot;][0]))
     78   if &quot;input_ids&quot; in string.keys():
---&gt; 79     string = {k: v.to(device=device) for k, v in string.items()}
     80     with torch.no_grad():
     81 

RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
</code></pre>
<p>I run this code:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
import torch
from torch import cuda

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0] #First element of model_output contains all token embeddings
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

# Select device globally
device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

# Load model from HuggingFace Hub
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
model = AutoModel.from_pretrained('sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
                                  device_map = device)
df = pd.read_json(file_path)

def processDataRAG(data, tokenizer, model):
  
  article_sentences = data.content.progress_apply(lambda x: list(nlp_de(x).sents))
  #tokenized_articles = data.content.progress_apply(lambda article: tokenizeChunkwise(article, tokenizer, 512))
  
  article_sentences = [
      (sentences, idx) for idx, article in tqdm(enumerate(list(article_sentences)), desc=&quot;Loop over articles with index&quot;) 
      for sentences in article
      ]
  sents = [str(sentences[0]) for sentences in article_sentences]
  number_of_article =[sentences[1] for sentences in article_sentences]
  embedded_sentencs = [embeddChunkwise(sentence, tokenizer, model, 512) for sentence in tqdm(sents, desc = &quot;Create chunk-wise embeddings&quot;)]
  return pd.DataFrame({
      &quot;sentences&quot;: sents,
      &quot;embeddings&quot;: embedded_sentencs,
      &quot;article&quot;: number_of_article
  })

def embeddChunkwise(string, tokenizer, model, chunk_size):
  decreasing_by_special_tokens = 0 # Because of speical tokens at the beginning and end
  encoded_string = tokenizer(string, add_special_tokens=False)
  if len(encoded_string[&quot;input_ids&quot;])/chunk_size &gt; 1:
    print(&quot;Tokenized_string:&quot;, encoded_string)
    print(&quot;Total tokens: &quot;, str(len(encoded_string[&quot;input_ids&quot;])))
    print(&quot;Tokenized string in chunks: &quot;, str(len(encoded_string[&quot;input_ids&quot;])/chunk_size), &quot; --- &quot; , str(len(encoded_string[&quot;input_ids&quot;])//chunk_size +1))
  embedded_chunks = []
  for idx in list(range(len(encoded_string[&quot;input_ids&quot;])//chunk_size +1 )):
    encoded_chunk=None

    if (chunk_size-decreasing_by_special_tokens)*(idx+1) &lt; len(encoded_string[&quot;input_ids&quot;]): # sentences with 1000 words as instances
      start_idx, end_idx = (chunk_size*idx - decreasing_by_special_tokens*idx, chunk_size*(idx+1) - decreasing_by_special_tokens*(idx+1))

      encoded_chunk = encoded_string[&quot;input_ids&quot;][start_idx:end_idx]

    else: # If it is a sentences with 20 words as instance
      if chunk_size-decreasing_by_special_tokens &gt; len(encoded_string[&quot;input_ids&quot;]):
        encoded_chunk = encoded_string[&quot;input_ids&quot;][chunk_size*(idx) - decreasing_by_special_tokens*(idx):]
      else:
        
        encoded_chunk = encoded_string[&quot;input_ids&quot;][-(chunk_size*(idx) - decreasing_by_special_tokens*(idx)):]

    if len(encoded_chunk) &gt; 0:
      embedded_chunk = createEmbeddings(
          tokenizer(tokenizer.decode(encoded_chunk, skip_special_tokens  = True), return_tensors='pt', add_special_tokens=False), 
          model
          )
      if isinstance(embedded_chunk, list):
        embedded_chunks.append(embedded_chunk[0])
  if len(embedded_chunks) &gt; 1:
    return embedded_chunks
  elif len(embedded_chunks) == 0:
    return np.nan
  else:
    return embedded_chunks[0]

def createEmbeddings(string, model):
  if &quot;input_ids&quot; in string.keys():
    string = {k: v.to(device=device) for k, v in string.items()}
    with torch.no_grad():
      
        try:
          model_output = model(**string)
        except Exception as ex:
          print(&quot;--- Error by creating Embeddings ---&quot;)
          print(&quot;Error: &quot;, str(ex))
          return np.nan
    # Perform pooling. In this case, average pooling
    try:
      sentence_embeddings = mean_pooling(model_output, string['attention_mask'])
    except Exception as ex:
      print(&quot;--- Error by pooling embeddings ---&quot;)
      print(&quot;Model output: &quot;, str(model_output))
      print(&quot;Attention_mask: &quot;, str(string['attention_mask']))
      print(&quot;Error: &quot;, str(ex))
      return np.nan
    sentence_embeddings = sentence_embeddings.detach().cpu().numpy()
    return sentence_embeddings
  else:
    return np.nan

</code></pre>
","transformer-model"
"77659401","Why is the same fc.layer used for q,k,v in xl_net?","2023-12-14 10:22:09","","0","11","<nlp><huggingface-transformers><transformer-model>","<p>Why is the same fc.layer used for q,k,v in xl_net?</p>
<pre><code>def head_projection(h, d_model, n_head, d_head, kernel_initializer, name):
  &quot;&quot;&quot;Project hidden states to a specific head with a 4D-shape.&quot;&quot;&quot;
  proj_weight = tf.get_variable('{}/kernel'.format(name),
                                [d_model, n_head, d_head], dtype=h.dtype,
                                initializer=kernel_initializer)
  head = tf.einsum('ibh,hnd-&gt;ibnd', h, proj_weight)

  return head



def rel_multihead_attn(h, r, r_w_bias, r_r_bias, seg_mat, r_s_bias, seg_embed,
                       attn_mask, mems, d_model, n_head, d_head, dropout,
                       dropatt, is_training, kernel_initializer,
                       scope='rel_attn', reuse=None):
  &quot;&quot;&quot;Multi-head attention with relative positional encoding.&quot;&quot;&quot;

  scale = 1 / (d_head ** 0.5)
  with tf.variable_scope(scope, reuse=reuse):
    if mems is not None and mems.shape.ndims &gt; 1:
      cat = tf.concat([mems, h], 0)
    else:
      cat = h

    # content heads
    q_head_h = head_projection(
        h, d_model, n_head, d_head, kernel_initializer, 'q')
    k_head_h = head_projection(
        cat, d_model, n_head, d_head, kernel_initializer, 'k')
    v_head_h = head_projection(
        cat, d_model, n_head, d_head, kernel_initializer, 'v')
</code></pre>
<p>Is it because of rel_position??<br />
I can't figure out why even if I look at the paper</p>
<p>In the existing transformer model, q, k, and v are all put in separate FC layers, but I don't know why I put them in one FC layer here</p>
","transformer-model"
"77653164","What is the function _transformer_encoder_layer_fwd in pytorch?","2023-12-13 11:20:37","","0","374","<python><pytorch><transformer-model>","<p>I ran into the <code>function _transformer_encoder_layer_fwd</code> of PyTorch in the <code>lib/python3.11/site-packages/torch/_C/_VariableFunctions.pyi</code> file, <a href=""https://i.sstatic.net/HaI5I.png"" rel=""nofollow noreferrer"">the function is called here</a>.</p>
<p>But I didn't find any details about this function. Why could this function be called? And how?</p>
<pre><code>def _transformer_encoder_layer_fwd(src: Tensor, embed_dim:
                                   _int, num_heads: _int,
                                   qkv_weight: Tensor,
                                   qkv_bias: Tensor,
                                   proj_weight: Tensor,
                                   proj_bias: Tensor,
                                   use_gelu: _bool,
                                   norm_first: _bool,
                                   eps: _float,
                                   norm_weight_1: Tensor,
                                   norm_bias_1: Tensor,
                                   norm_weight_2: Tensor,
                                   norm_bias_2: Tensor,
                                   ffn_weight_1: Tensor,
                                   ffn_bias_1: Tensor,
                                   ffn_weight_2: Tensor,
                                   ffn_bias_2: Tensor,
                                   mask: Optional[Tensor] = None,
                                   mask_type: Optional[_int] = None) -&gt; Tensor:
</code></pre>
<p>...</p>
<p>The function is defined in <code>torch/_C/_VariableFunctions.pyi</code></p>
<p>I tried to find any details of this function about how this function be called. But no results.</p>
","transformer-model"
"77649579","Vision transformer is throwing error during training","2023-12-12 23:39:57","","0","42","<transformer-model>","<p>I AM TRYING TO create a sequence to sequence model for license plate text recognition. I have a  data file lpr.csv with license plate image and label. I have 4000 license plate images that i am # #feeding to the model. While training I am getting the following error.</p>
<p>Node: 'gradient_tape/model_53/tf.<strong>operators</strong>.add_531/BroadcastGradientArgs'
Incompatible shapes: [20,0,64] vs. [20,0,32]
[[{{node gradient_tape/model_53/tf.<strong>operators</strong>.add_531/BroadcastGradientArgs}}]]<br />
[Op:__inference_train_function_758198]</p>
<p>`</p>
<pre><code> import tensorflow as tf
   from tensorflow.keras import layers

   # Define Vision Transformer block

   class VisionTransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, mlp_dim, dropout_rate=0.1):
        super(VisionTransformerBlock, self).__init__()
        self.attention = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.mlp = tf.keras.Sequential(
            [layers.Dense(mlp_dim, activation=tf.nn.gelu), layers.Dropout(dropout_rate),  
            layers.Dense(embed_dim)]       
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(dropout_rate)
        self.dropout2 = layers.Dropout(dropout_rate)

    def call(self, inputs, training):
        attn_output = self.attention(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        mlp_output = self.mlp(out1)
        mlp_output = self.dropout2(mlp_output, training=training)
        return self.layernorm2(out1 + mlp_output)

   class PatchEmbedding(layers.Layer):
    def __init__(self, patch_size, embed_dim):
        super(PatchEmbedding, self).__init__()
        self.patch_size = patch_size
        self.projection = layers.Conv2D(embed_dim, kernel_size=patch_size, strides=patch_size,  
          padding=&quot;valid&quot;)
    def call(self, inputs):
        patches = self.projection(inputs)
        return tf.reshape(patches, [tf.shape(patches)[0], -1, tf.shape(patches)[-1]])

    def get_positional_embedding(patch_embeddings, embed_dim):
         num_patches = tf.shape(patch_embeddings)[1]
         positions = tf.range(num_patches, dtype=tf.float32)
         angle_rads = positions[:, tf.newaxis] / tf.pow(10000, tf.range(0, embed_dim, 2,   
         dtype=tf.float32) / embed_dim)   
    sin_vals = tf.math.sin(angle_rads[:, 0::2])
    cos_vals = tf.math.cos(angle_rads[:, 1::2])
    pos_encoding = tf.stack([sin_vals, cos_vals], axis=-1)
    pos_encoding = tf.reshape(pos_encoding, [num_patches, -1])
    pos_encoding = tf.expand_dims(pos_encoding, 0)
    return tf.tile(pos_encoding, [tf.shape(patch_embeddings)[0], 1, 1])

    def VisionTransformer(image_shape, num_classes):
    inputs = layers.Input(shape=image_shape)

    patch_size = 16  # Define your patch size
    embed_dim = 64  # Embedding dimension for each patch
    num_heads = 8
    mlp_dim = 256
    num_layers = 8

    # Patch embeddings
    patch_embeddings = PatchEmbedding(patch_size, embed_dim)(inputs)

    # Positional embeddings
    positional_embeddings = get_positional_embedding(patch_embeddings, embed_dim)
    embeddings = patch_embeddings + positional_embeddings

    # Transformer Encoder Blocks

for _ in range(num_layers):
        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(embeddings, 
        embeddings)
        attn_output = layers.Dropout(0.1)(attn_output)
        attn_output = layers.Add()([embeddings, attn_output])  # Adjusted operation
        attn_output = layers.LayerNormalization(epsilon=1e-6)(attn_output)

        mlp_output = layers.Dense(mlp_dim, activation='relu')(attn_output)
        mlp_output = layers.Dropout(0.1)(mlp_output)
        mlp_output = layers.Dense(embed_dim)(mlp_output)
        mlp_output = layers.LayerNormalization(epsilon=1e-6)(mlp_output)

        embeddings = layers.Add()([mlp_output, attn_output])  # Adjusted operation


    # Global average pooling and classify

    x = layers.GlobalAveragePooling1D()(embeddings)
    x = layers.Dense(256, activation='relu')(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model

   # Create the Vision Transformer model and compile it

    image_shape = (224, 224, 3)
    num_classes = len(train_generator.class_indices)
    vision_transformer = VisionTransformer(image_shape, num_classes)
    vision_transformer.compile(optimizer='adam', loss='categorical_crossentropy', metrics=  
    ['accuracy'])
   # Print model summary
    vision_transformer.summary()

   # Train the model
    vision_transformer.fit(train_generator, epochs=10, validation_data=val_generator)
</code></pre>
","transformer-model"
"77631554","How can I define reconstruction validation in masked point cloud neural networks?","2023-12-09 14:09:22","","1","35","<pytorch><point-clouds><transformer-model><vision-transformer>","<p>I have been using point cloud networks to mask portions of point clouds then reconstruct the missing points. I have noticed though, that these point cloud networks do not define pure accuracy with respect to comparing the ground truth to a reconstructed point cloud in a validation dataset. Ie I want to simply measure how well the model reconstructs on some previously unseen dataset after pretraining. The models I've run across tend to use &quot;testing&quot; or &quot;validation&quot; datasets for downstream tasks such as labeling or segmentation, but I have yet to see accuracy or validation loss purely for reconstruction.</p>
<p>Take <a href=""https://github.com/Pang-Yatian/Point-MAE"" rel=""nofollow noreferrer"">this point cloud network</a> for example (<a href=""https://arxiv.org/abs/2203.06604"" rel=""nofollow noreferrer"">who's paper is here</a>). The authors use the pertained checkpoints for downstream tasks, but do not provide a method to use the pretrained checkpoints to validate <strong>pure reconstruction</strong> on a previously unseen dataset. I've noticed the same trend with other point cloud networks.</p>
<p>I am interested in looking at how well I can train the above point cloud model to reconstruct masked areas of point clouds, training then validation. Ie I am more interested in a loss curve obtained from applying a pretrained model (from the above) to some validation dataset to see how well it performs. I am less interested in downstream tasks. However, the paper and the code do not provide a method to do this. I am somewhat new to point cloud transformers, so my efforts at editing the code at the above repository to achieve my goals has not been fruitful. How would I go about doing this?</p>
","transformer-model"
"77624169","What is the difference between the last transformer layer output and the middle ones?","2023-12-08 03:21:51","","1","36","<transformer-model><semantic-segmentation>","<p>I'm working on a binary segmentation tasks using a Vision Transformer like Network. However, the ViT uses the last layer output to get the final mask while ignoring the middle layers' features. I wonder what is the function of each vision transformer layer, like, extracting the global features? If so, as the number of layers increase, the features will get better or what?</p>
<p>I try to visualize some of the features vector, but it is hard to tell soemthing intuitively. I want to know whether there are some papers which discuss the same problems and what are the results.</p>
","transformer-model"
"77616017","No progression in Training a Transformer-model with pytorch_lightning","2023-12-06 20:08:59","","0","1450","<python><pytorch><transformer-model><pytorch-lightning><pytorch-forecasting>","<p><strong>Hello!</strong></p>
<p>I am new to the field of deep learning and have attempted to create a Transformer model using time series data (weather data) with the help of the reference code (<a href=""https://github.com/CVxTz/time_series_forecasting/tree/main"" rel=""nofollow noreferrer"">https://github.com/CVxTz/time_series_forecasting/tree/main</a>).
I had to customize some aspects to suit my needs.</p>
<p>I am working on this code for my bachelor's thesis to compare Transformer models with LSTM models to determine which is better suited for weather data. I'm unsure whether it's normal for my training, with around 13,000 parameters, to take more than 8 hours or if it's just stuck at some point. I have reduced the model from an initial 8 million parameters to only 13,000 parameters. Additionally, my data has been significantly reduced. I am currently stuck in the training process and need assistance.</p>
<p>Here my Code:</p>
<p><strong>Model:</strong></p>
<pre><code>class TimeSeriesForcasting(pl.LightningModule):
    def __init__(self, n_encoder_inputs, n_decoder_inputs, channels, dropout, lr):
        super().__init__()

        self.save_hyperparameters()

        self.lr = lr
        self.dropout = dropout

        self.input_pos_embedding = torch.nn.Embedding(1024, embedding_dim=channels//2)
        self.target_pos_embedding = torch.nn.Embedding(1024, embedding_dim=channels//2)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=channels,
            nhead=2, #8
            dropout=self.dropout,
            dim_feedforward=4 * channels,
        )
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=channels,
            nhead=2, #8
            dropout=self.dropout,
            dim_feedforward=4 * channels,
        )

        self.encoder = torch.nn.TransformerEncoder(encoder_layer, num_layers=2)#8
        self.decoder = torch.nn.TransformerDecoder(decoder_layer, num_layers=2)#8

        self.input_projection = Linear(n_encoder_inputs, channels //2)
        self.output_projection = Linear(n_decoder_inputs, channels //2)

        self.linear = Linear(channels //2, 1)

        self.do = nn.Dropout(p=self.dropout)

    def encode_src(self, src):
        src_start = self.input_projection(src).permute(1, 0, 2)

        in_sequence_len, batch_size = src_start.size(0), src_start.size(1)
        pos_encoder = (
            torch.arange(0, in_sequence_len, device=src.device)
            .unsqueeze(0)
            .repeat(batch_size, 1)
        )

        pos_encoder = self.input_pos_embedding(pos_encoder).permute(1, 0, 2)

        src = src_start + pos_encoder

        src = self.encoder(src) + src_start

        return src

    def decode_trg(self, trg, memory):

        trg_start = self.output_projection(trg).permute(1, 0, 2)

        out_sequence_len, batch_size = trg_start.size(0), trg_start.size(1)

        pos_decoder = (
            torch.arange(0, out_sequence_len, device=trg.device)
            .unsqueeze(0)
            .repeat(batch_size, 1)
        )
        pos_decoder = self.target_pos_embedding(pos_decoder).permute(1, 0, 2)

        trg = pos_decoder + trg_start

        trg_mask = gen_trg_mask(out_sequence_len, trg.device)

        out = self.decoder(tgt=trg, memory=memory, tgt_mask=trg_mask) + trg_start

        out = out.permute(1, 0, 2)

        out = self.linear(out)

        return out

    def forward(self, x):
        src, trg = x

        src = self.encode_src(src)

        out = self.decode_trg(trg=trg, memory=src)

        return out

    def training_step(self, batch, batch_idx):
        src, trg_in, trg_out = batch

        y_hat = self((src, trg_in))

        y_hat = y_hat.view(-1)
        y = trg_out.view(-1)

        loss = smape_loss(y_hat, y)

        self.log(&quot;train_loss&quot;, loss)

        return loss

    def validation_step(self, batch, batch_idx):
        src, trg_in, trg_out = batch

        y_hat = self((src, trg_in))

        y_hat = y_hat.view(-1)
        y = trg_out.view(-1)

        loss = smape_loss(y_hat, y)

        self.log(&quot;valid_loss&quot;, loss)

        return loss

    def test_step(self, batch, batch_idx):
        src, trg_in, trg_out = batch

        y_hat = self((src, trg_in))

        y_hat = y_hat.view(-1)
        y = trg_out.view(-1)

        loss = smape_loss(y_hat, y)

        self.log(&quot;test_loss&quot;, loss)

        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, patience=10, factor=0.1
        )
        return {
            &quot;optimizer&quot;: optimizer,
            &quot;lr_scheduler&quot;: scheduler,
            &quot;monitor&quot;: &quot;valid_loss&quot;,
        }
</code></pre>
<p><strong>train-function:</strong></p>
<pre><code>def train(
    data_csv_path: str,
    feature_target_names_path: str,
    output_json_path: str,
    log_dir: str,
    model_dir: str ,
    batch_size: int,
    epochs: int ,
    horizon_size: int,
    channels: int,
    lr: float,
    dropout: float
):
    
    data = pd.read_csv(data_csv_path)

    with open(feature_target_names_path) as f:
        feature_target_names = json.load(f)

    data_train = data[~data[feature_target_names[&quot;target&quot;]].isna()]
    grp_by_train = data_train.groupby(by=feature_target_names[&quot;group_by_key&quot;])
    

    full_groups = []
    groups = list(grp_by_train.groups)
    for grp in groups:
        #print(grp_by_train.get_group(grp).shape[0], horizon_size)
        if grp_by_train.get_group(grp).shape[0] &gt; 2 * horizon_size:
            full_groups.append(grp)

    #full_groups = [
    #    grp for grp in groups if grp_by_train.get_group(grp).shape[0] &gt; 2 * horizon_size
    #]



    train_data = Dataset(
        groups=full_groups,
        grp_by=grp_by_train,
        split=&quot;train&quot;,
        features=feature_target_names[&quot;features&quot;],
        target=feature_target_names[&quot;target&quot;],
    )
    val_data = Dataset(
        groups=full_groups,
        grp_by=grp_by_train,
        split=&quot;val&quot;,
        features=feature_target_names[&quot;features&quot;],
        target=feature_target_names[&quot;target&quot;],
    )

    print(&quot;len(train_data)&quot;, len(train_data))
    print(&quot;len(val_data)&quot;, len(val_data))

    train_loader = DataLoader(
        train_data,
        batch_size=batch_size,
        num_workers=12,
        persistent_workers=True,
        shuffle=True,
    )
    val_loader = DataLoader(
        val_data,
        batch_size=batch_size,
        num_workers=12,
        persistent_workers=True,
        shuffle=False,
    )

    model = TimeSeriesForcasting(
        n_encoder_inputs=len(feature_target_names[&quot;features&quot;]) + 1,
        n_decoder_inputs=len(feature_target_names[&quot;features&quot;]) + 1,
        lr=lr,
        dropout=dropout,
        channels=channels
    )

    logger = TensorBoardLogger(
        save_dir=log_dir,
    )

    checkpoint_callback = ModelCheckpoint(
        monitor=&quot;valid_loss&quot;,
        mode=&quot;min&quot;,
        dirpath=model_dir,
        filename=&quot;ts&quot;,
    )

    trainer = pl.Trainer(
        max_epochs=epochs,
        accelerator='auto',
        logger=logger,
        callbacks=[checkpoint_callback],
    )
    trainer.fit(model, train_loader, val_loader)

    result_val = trainer.test(test_dataloaders=val_loader)

    output_json = {
        &quot;val_loss&quot;: result_val[0][&quot;test_loss&quot;],
        &quot;best_model_path&quot;: checkpoint_callback.best_model_path,
    }

    if output_json_path is not None:
        with open(output_json_path, &quot;w&quot;) as f:
            json.dump(output_json, f, indent=4)

    return output_json


</code></pre>
<p><strong>Class-Dataset:</strong></p>
<pre><code>class Dataset(torch.utils.data.Dataset):
    def __init__(self, groups, grp_by, split, features, target):
        self.groups = groups
        self.grp_by = grp_by
        self.split = split
        self.features = features
        self.target = target

    def __len__(self):
        return len(self.groups)

    def __getitem__(self, idx):
        group = self.groups[idx]

        df = self.grp_by.get_group(group)

        src, trg = split_df(df, split=self.split)

        src = src[self.features + [self.target]]

        src = df_to_np(src)

        trg_in = trg[self.features + [f&quot;{self.target}_lag_1&quot;]]

        trg_in = np.array(trg_in)
        trg_out = np.array(trg[self.target])

        src = torch.tensor(src, dtype=torch.float)
        trg_in = torch.tensor(trg_in, dtype=torch.float)
        trg_out = torch.tensor(trg_out, dtype=torch.float)

        return src, trg_in, trg_out


</code></pre>
<p>Other-functions:</p>
<pre><code>
def pad_arr(arr: np.ndarray, expected_size: int = 120):
    &quot;&quot;&quot;
    Pad top of array when there is not enough history
    :param arr:
    :param expected_size:
    :return:
    &quot;&quot;&quot;
    arr = np.pad(arr, [(expected_size - arr.shape[0], 0), (0, 0)], mode=&quot;edge&quot;)
    return arr


def df_to_np(df):
    arr = np.array(df)
    arr = pad_arr(arr)
    return arr

def split_df(
    df: pd.DataFrame, split: str, history_size: int = 120, horizon_size: int = 30
):
    &quot;&quot;&quot;
    Create a training / validation samples
    Validation samples are the last horizon_size rows

    :param df:
    :param split:
    :param history_size:
    :param horizon_size:
    :return:
    &quot;&quot;&quot;
    if split == &quot;train&quot;:
        end_index = random.randint(horizon_size + 1, df.shape[0] - horizon_size)
    elif split in [&quot;val&quot;, &quot;test&quot;]:
        end_index = df.shape[0]
    else:
        raise ValueError

    label_index = end_index - horizon_size
    start_index = max(0, label_index - history_size)

    history = df[start_index:label_index]
    targets = df[label_index:end_index]

    return history, targets


def gen_trg_mask(length, device):
    mask = torch.tril(torch.ones(length, length, device=device)) == 1

    mask = (
        mask.float()
        .masked_fill(mask == 0, float(&quot;-inf&quot;))
        .masked_fill(mask == 1, float(0.0))
    )

    return mask

def smape_loss(y_pred, target):
    loss = 2 * (y_pred - target).abs() / (y_pred.abs() + target.abs() + 1e-8)
    return loss.mean()

</code></pre>
<p><strong>Data i used</strong> <a href=""https://i.sstatic.net/1QsGq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1QsGq.png"" alt=""enter image description here"" /></a></p>
<p><strong>This is my output on Training-Start and also after 8hours of training (i stopped it at 8 Hours):</strong></p>
<pre><code>len(train_data) 5
len(val_data) 5

c:\Users\Luca\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\nn\modules\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)
  warnings.warn(f&quot;enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}&quot;)

GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs

You are using a CUDA device ('NVIDIA GeForce RTX 3080 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]


  | Name                 | Type               | Params
------------------------------------------------------------
0 | input_pos_embedding  | Embedding          | 4.1 K 
1 | target_pos_embedding | Embedding          | 4.1 K 
2 | encoder              | TransformerEncoder | 1.7 K 
3 | decoder              | TransformerDecoder | 2.4 K 
4 | input_projection     | Linear             | 28    
5 | output_projection    | Linear             | 28    
6 | linear               | Linear             | 5     
7 | do                   | Dropout            | 0     
------------------------------------------------------------
12.3 K    Trainable params
0         Non-trainable params
12.3 K    Total params
0.049     Total estimated model params size (MB)
</code></pre>
","transformer-model"
"77605657","How to understand contextualized embeddings in Transformer?","2023-12-05 10:58:42","","2","1305","<nlp><huggingface-transformers><embedding><transformer-model><word-embedding>","<p>As, The input to transformers is essentially a sequence of tokens, each represented as one-hot vectors. These vectors are subsequently multiplied by an embedding matrix (E) to generate the input embeddings (X). This embedding matrix is a learned parameter during the training process. In mathematical terms, this process can be represented as X = E * I, where I stands for the input one-hot vectors.</p>
<p>so if the embedding layer just acts as look-up table to grab a learned vector representation of each token then how the embedding for word <code>left</code> have two different representations in the embedding space for below sentence ?</p>
<p>&quot;I <strong>left</strong> my phone on the <strong>left</strong> side of the table.&quot;</p>
","transformer-model"
"77593800","How to get weights for the embedding layer after training using Pytorch's nn.Transformer?","2023-12-03 10:03:21","","0","260","<python><pytorch><embedding><transformer-model>","<p>I'm a biginner of NLP. Also, I am not an English speaker, so I apologize for my poor English.</p>
<p>I recently try to train a model using Pytorch's nn.Transformer module to get weights for the embedding layer after training.
I'm training on my own data from the Transformer tutorial on Pytorch. (<a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a>)</p>
<p>Currently, I use this code to get weights for the embedding layer after training.
Will this get accurate weights after training?</p>
<pre><code>import time

criterion = nn.CrossEntropyLoss()
lr = 5.0  # learning rate
optimizer = torch.optim.SGD(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)

def train(model: nn.Module) -&gt; None:
    model.train()  # turn on train mode
    total_loss = 0.
    log_interval = 200
    start_time = time.time()

    num_batches = len(train_data) // bptt
    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):
        data, targets = get_batch(train_data, i)
        output = model(data)
        output_flat = output.view(-1, ntokens)
        loss = criterion(output_flat, targets)

        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()

        total_loss += loss.item()
        if batch % log_interval == 0 and batch &gt; 0:
            lr = scheduler.get_last_lr()[0]
            ms_per_batch = (time.time() - start_time) * 1000 / log_interval
            cur_loss = total_loss / log_interval
            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '
                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '
                  f'loss {cur_loss:5.7f}')
            total_loss = 0
            start_time = time.time()

def evaluate(model: nn.Module, eval_data: Tensor) -&gt; float:
    model.eval()  # turn on evaluation mode
    total_loss = 0.
    with torch.no_grad():
        for i in range(0, eval_data.size(0) - 1, bptt):
            data, targets = get_batch(eval_data, i)
            seq_len = data.size(0)
            output = model(data)
            output_flat = output.view(-1, ntokens)
            total_loss += seq_len * criterion(output_flat, targets).item()
    return total_loss / (len(eval_data) - 1)

best_val_loss = float('inf')
epochs = 20

with TemporaryDirectory() as tempdir:
    best_model_params_path = os.path.join(tempdir, &quot;best_model_params.pt&quot;)

    for epoch in range(1, epochs + 1):
        epoch_start_time = time.time()
        train(model)
        val_loss = evaluate(model, val_data)
        elapsed = time.time() - epoch_start_time
        print('-' * 89)
        print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '
            f'valid loss {val_loss:5.7f}')
        print('-' * 89)

        if val_loss &lt; best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), best_model_params_path)

        scheduler.step()
    model.load_state_dict(torch.load(best_model_params_path)) # load best model states

emb=model.embedding.weight
embedding_weight=emb.detach().numpy().copy()
</code></pre>
","transformer-model"
"77592802","KerasTensor to Tensor","2023-12-03 01:49:28","","0","41","<keras><tensor><transformer-model><image-classification>","<p>I am working on a image classification problem using a transformer model with linear self-attention.</p>
<p>I get a type error stating: &quot;TypeError: linear(): argument 'input' (position 1) must be Tensor, not KerasTensor&quot;</p>
<p>How can I convert from KerasTensor to Tensor? This problem is with a standard MNIST dataset.</p>
<p>Code:</p>
<pre><code>def create_vit_classifier2():
    inputs = layers.Input(shape=input_shape)
    # Augment data.
    augmented = data_augmentation(inputs)
    # Create patches.
    patches = Patches(patch_size)(augmented)
    # Encode patches.
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

    # Create multiple layers of the Transformer block.
    for _ in range(transformer_layers):
        # Layer normalization 1.
        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
        # Create a multi-head attention layer.
        attention_output = LinformerSelfAttention(
          dim = 64,
          seq_len = 49,
          heads = 4,
          k = 256,
          one_kv_head = True,
          share_kv = True
        )(x1)
        # attention_output = Padder(attention_output)
        # Skip connection 1.
        x2 = layers.Add()([attention_output, encoded_patches])
        # Layer normalization 2.
        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)
        # MLP.
        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)
        # Skip connection 2.
        encoded_patches = layers.Add()([x3, x2])

    # Create a [batch_size, projection_dim] tensor.
    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = layers.Flatten()(representation)
    representation = layers.Dropout(0.5)(representation)
    # Add MLP.
    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)
    # Classify outputs.
    logits = layers.Dense(num_classes)(features)
    # Create the Keras model.
    model = keras.Model(inputs=inputs, outputs=logits)
    return model
</code></pre>
<p>I have tried to convert between the two such as wrapping X1 in tf.constant()</p>
","transformer-model"
"77588547","Transformers: Cross Attention Tensor Shapes During Inference Mode","2023-12-01 23:19:53","77588763","0","872","<pytorch><transformer-model>","<p>Having been trying to figure this out for a while.  I found <a href=""https://stackoverflow.com/questions/69887535/question-about-tokens-used-in-transformer-decoder-attention-layers-during-infere"">this</a> similar question but I don't think the proposed answer actually addresses the question.</p>
<p>During inference mode of an Encoder/Decoder Transformer, my understanding is we <strong>don't</strong> pre-pad the Decoder input sequence to match the Encoder sequence length (i.e. pass in <code>[start_id,]</code> and not <code>[start_id, pad_id, pad_id, ...]</code>)</p>
<p>I might be missing something but when I don't pre-pad, the attention mechanism cannot correctly compute the matrix multiplication because the Decoder input is of seq_length = 1 while the Encoder seq_length is &gt; 1 (<code>T</code>).  For reference (see attached pic), I identified the tensor shapes during each step and you can see where the last matmul step cannot be performed given incompatible tensor shapes.</p>
<p>What am I missing?  Am I suppose to pre-pad the Decoder input?  Or do I truncate the Encoder output to match the Decoder length?  Something else?</p>
<p><a href=""https://i.sstatic.net/8hBMu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8hBMu.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"77567304","Pretraining BERT Models from scratch vs Further Pretraining","2023-11-28 21:41:47","","1","162","<nlp><arabic><bert-language-model><transformer-model><pre-trained-model>","<p>I want to pretrain an Arabic BERT model on domain-specific data to make it suitable for a specific domain problem, which is the classification of citizen reviews about government services into relevant government sectors. My plan is to pretrain the model on freely available Arabic newspaper articles that specifically tackle the same sectors as the government ones, including education, healthcare, etc. I know these articles are not considered too specific to the target domain, but they are the only suitable data available. I plan to pretrain the model on around 20K articles only since I am limited with time and computational resources. Also, the target dataset contains about 2K citizen reviews provided in Modern Standard Arabic.</p>
<p>So, I have several questions concerning this project:</p>
<ol>
<li><p>Would it be beneficial to pretrain the Arabic BERT model from scratch using this small dataset of 20K samples? or would it be too small to tackle my problem?</p>
</li>
<li><p>Would it be better to apply further pretraining for Arabic BERT model, which means starting with the model initial knowledge (weights) and then further pretraining it on the 20K samples? I am afraid this will lead to model forgetting for the previously learnt knowledge. Also, the combination of general and specific knowledge might affect the model performance on the target dataset of citizen reviews.</p>
</li>
<li><p>Whichever method I choose from above, should I pretrain the model on unlabeled data (unsupervised learning)? or is it better to train it on labeled data to be useful for text classification?</p>
</li>
<li><p>After pretraining the model, should I apply feature extraction or fine-tuning on the target dataset of citizen reviews?</p>
</li>
</ol>
<p>After extensive research, I found that domain-specific models outperform the general ones. Also, it is advised to use pretraining from scartch to make the model specific to the target domain. However, this requires large amount of data and computational power. So, I am not sure if 20K samples are enough. Furthermore, I am not sure if further pretraining will be beneficial for the specific domain target data.</p>
","transformer-model"
"77559832","A given column is not a column of the dataframe","2023-11-27 20:43:47","","2","314","<python><dataframe><function><pipeline><transformer-model>","<p><strong>Hi everyone!</strong></p>
<p>I have a test <code>DataFrame</code> to find out how <code>FunctionTransformer</code> works in a <code>Pipeline</code>.
I created a function that drops one column.
I transform it with <code>FunctionTransformer</code> and put the result into <code>ColumnTransformer</code> constructor and got the error.
The function I created doesn't take any arguments, hence I don't need to call it with some certain values.
But in <code>ColumnTransformer</code> I have to specify the columns.</p>
<p>In the second try I put my <code>FunctionTransformer</code> result into the final <code>Pipeline</code> instead of <code>ColumnTransformer</code> and get the same error.</p>
<p>In addition to that, could someone please explain how data flow in <code>FunctionTransformer</code>, <code>ColumnTransformer</code> and <code>Pipeline</code>?</p>
<p>Why does someone use <code>fit_transform</code> in <code>ColumnTransformer</code>?
Isn't it the same as <code>Pipeline.fit</code>? What's the difference?</p>
<p>In what order should I create columns and <code>DataFrame</code>s for a <code>Pipeline</code> chain?</p>
<p>First, I define my function.
Then split data for <code>x</code> and <code>y</code>, with <code>x</code> data into numerical and categorical columns.
I'd like the <code>Pipeline</code> and <code>ColumnTransformer</code> to use the function and transform my data.</p>
<p>I don't know how data is being sent inside the <code>Pipeline</code> and <code>ColumnTransformer</code>.
I define <code>x</code> and <code>y</code> before the <code>Pipeline</code>, but inside <code>Pipeline</code> it transforms in some different way.</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import FunctionTransformer

a = range(1, 10)
b = range(10, 100, 10)
c = range(11, 110, 11)
d = range(12, 120, 12)

e = ['aa','bb','cc','dd','ee','ff','gg','hh','ii']
f = ['ф','и','с','в','у','а','п','р','ш']
g = ['!','@','#','$','%','^','&amp;','*','(']

test={'a': a, 'b': b, 'c': c, 'd': d, 'e': e, 'f': f, 'g': g}

data = pd.DataFrame(test, columns=['a', 'b', 'c', 'd', 'e', 'f', 'g'])
print(data)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>    a   b   c   d   e   f   g
0   1   10  11  12  aa  ф   !
1   2   20  22  24  bb  и   @
2   3   30  33  36  cc  с   #
3   4   40  44  48  dd  в   $
4   5   50  55  60  ee  у   %
5   6   60  66  72  ff  а   ^
6   7   70  77  84  gg  п   &amp;
7   8   80  88  96  hh  р   *
8   9   90  99  108 ii  ш   (
</code></pre>
<pre class=""lang-py prettyprint-override""><code>def drop_cols(value):
  cols = ['g']
  return data.drop(cols, axis=1)

bad_cols = Pipeline([('bad', FunctionTransformer(drop_cols))])
x = data.drop('f', axis=1)
y = data.f
num_clmns = data.select_dtypes(include='int').columns
cat_clmns = data.select_dtypes(include='object').columns

X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.3, random_state=42)

trans = ColumnTransformer([
    ('bad_cols', bad_cols, data.columns),
    ('num_trans1', StandardScaler(), num_clmns),
    ('cat_trans1', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_clmns)

    ], remainder='passthrough')

pipe = Pipeline([('trans', trans), ('rtc', RandomForestClassifier())])

pipe.fit(X_train, y_train)
</code></pre>
<p>Same error if I put <code>bad_cols</code> into the final <code>Pipeline</code> instead of <code>ColumnTransformer</code>:</p>
<pre class=""lang-py prettyprint-override""><code>trans = ColumnTransformer([
    ('num_trans1', StandardScaler(), num_clmns),
    ('cat_trans1', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), cat_clmns)

    ], remainder='passthrough')

pipe = Pipeline([('bad_cols', bad_cols), ('trans', trans), ('rtc', RandomForestClassifier())])

pipe.fit(X_train, y_train)
</code></pre>
<p>Error:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3801             try:
-&gt; 3802                 return self._engine.get_loc(casted_key)
   3803             except KeyError as err:

12 frames
pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'f'

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
KeyError: 'f'

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py in _get_column_indices(X, key)
    454 
    455         except KeyError as e:
--&gt; 456             raise ValueError(&quot;A given column is not a column of the dataframe&quot;) from e
    457 
    458         return column_indices

ValueError: A given column is not a column of the dataframe
</code></pre>
","transformer-model"
"77559352","Summarizing a small numerical dataframe using FLAN T5","2023-11-27 19:06:55","","0","64","<dataframe><openai-api><transformer-model><huggingface><large-language-model>","<p>I am trying to fine tune the Flan T5 model to summarize a pandas dataframe, which mostly contain numerical values and a date column. I expect it to understand the small dataset along with the dates. How should I feed the data to the model so that it best understands the data?</p>
<p>I am using the following prompt which I cannot change: What do you understand by this data? Respond within 2 Lines.</p>
<p>I am building a pipeline to do prompt tuning later using some labelled data, but the above is the first step.</p>
<p>I am a beginner to LLMs and have tried loading data in json format but it is not showing satisfactory results</p>
","transformer-model"
"77542001","How do I add a channel token embedding in vision Transformer TensorFlow?","2023-11-24 09:20:47","","0","92","<tensorflow><tensor><transformer-model>","<p>I am trying to create a specific version of a Vision Transformer, where <em>patch tokens are generated for each individual channel, utilizing a learnable channel embedding</em> <strong>chn</strong> <em>to preserve channel-specific information. The positional embeddings</em> <strong>pos</strong> <em>and the linear projection</em> <strong>W</strong> <em>are shared across all channels,</em> like shown in the picture below.</p>
<p><a href=""https://i.sstatic.net/cWZ3r.png"" rel=""nofollow noreferrer"">Channel ViT</a></p>
<p>I already have created my simple ViT, but cannot understand how to modify it to get the desired result.</p>
<p>My input is a data tensor (400, 40, 20, 32) (samples, Heigth, Width, Channels) and I have another tensor of 400 corresponding labels.<br />
This is the code that I am using. The important parts for this, are the PatchEncoder and the Create_VisionTransformer functions:</p>
<pre><code>class PatchExtractor(Layer):
    def __init__(self):
        super(PatchExtractor, self).__init__()

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, 5, 5, 1],
            strides=[1, 5, 5, 1],
            rates=[1, 1, 1, 1],
            padding=&quot;VALID&quot;,
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches
    
#%env TF_GPU_ALLOCATOR cuda_malloc_async




class PatchEncoder(Layer):
    def __init__(self, num_patches=16, projection_dim=800):
        super(PatchEncoder, self).__init__()
        self.num_patches = num_patches
        self.projection_dim = projection_dim
        w_init = tf.random_normal_initializer()
        class_token = w_init(shape=(1, projection_dim), dtype=&quot;float32&quot;)
        self.class_token = tf.Variable(initial_value=class_token, trainable=True)
        self.projection = Dense(units=projection_dim)
        self.position_embedding = Embedding(input_dim=num_patches+1, output_dim=projection_dim)

    def call(self, patch):
        batch = tf.shape(patch)[0]
        # reshape the class token embedins
        class_token = tf.tile(self.class_token, multiples = [batch, 1])
        class_token = tf.reshape(class_token, (batch, 1, self.projection_dim))
        # calculate patches embeddings
        patches_embed = self.projection(patch)
        patches_embed = tf.concat([patches_embed, class_token], 1)
        # calcualte positional embeddings
        positions = tf.range(start=0, limit=self.num_patches+1, delta=1)
        positions_embed = self.position_embedding(positions)
        # add both embeddings
        encoded = patches_embed + positions_embed
        return encoded
    




class MLP(Layer):
    def __init__(self, hidden_features, out_features, dropout_rate=0.1):
        super(MLP, self).__init__()
        self.dense1 = Dense(hidden_features, activation=tf.nn.gelu)
        self.dense2 = Dense(out_features)
        self.dropout = Dropout(dropout_rate)

    def call(self, x):
        x = self.dense1(x)
        x = self.dropout(x)
        x = self.dense2(x)
        y = self.dropout(x)
        return y
    





class Block(Layer):
    def __init__(self, projection_dim, num_heads=4, dropout_rate=0.1):
        super(Block, self).__init__()
        self.norm1 = LayerNormalization(epsilon=1e-6)
        self.attn = MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=dropout_rate)
        self.norm2 = LayerNormalization(epsilon=1e-6)
        self.mlp = MLP(projection_dim, projection_dim, dropout_rate)

    def call(self, x):
        # Layer normalization 1.
        x1 = self.norm1(x) # encoded_patches
        # Create a multi-head attention layer.
        attention_output = self.attn(x1, x1)
        # Skip connection 1.
        x2 = Add()([attention_output, x]) #encoded_patches
        # Layer normalization 2.
        x3 = self.norm2(x2)
        # MLP.
        x3 = self.mlp(x3)
        # Skip connection 2.
        y = Add()([x3, x2])
        return y
    
class TransformerEncoder(Layer):
    def __init__(self, projection_dim, num_heads=2, num_blocks=1, dropout_rate=0.1):
        super(TransformerEncoder, self).__init__()
        self.blocks = [Block(projection_dim, num_heads, dropout_rate) for _ in range(num_blocks)]
        self.norm = LayerNormalization(epsilon=1e-6)
        self.dropout = Dropout(0.5)

    def call(self, x):
        # Create a [batch_size, projection_dim] tensor.
        for block in self.blocks:
            x = block(x)
        x = self.norm(x)
        y = self.dropout(x)
        return y




def create_VisionTransformer(num_classes=1, num_heads=3, num_blocks=12, num_patches=16, projection_dim=800, input_shape=(40, 10, 32)):
    inputs = Input(shape=input_shape)
    # Patch extractor
    patches = PatchExtractor()(inputs)
    # Patch encoder
    patches_embed = PatchEncoder(num_patches, projection_dim=projection_dim)(patches)
    # Transformer encoder
    representation = TransformerEncoder(projection_dim = projection_dim, num_heads=num_heads, num_blocks=num_blocks)(patches_embed)
    representation = GlobalAveragePooling1D()(representation)
    # MLP to classify outputs
    #logits = MLP(projection_dim, num_classes, 0.5)(representation)
    logits = Dense(num_classes, activation='sigmoid')(representation)
    # Create model
    model = Model(inputs=inputs, outputs=logits)
            return model
</code></pre>
","transformer-model"
"77540677","Clearing context window of LLM in Huggingface","2023-11-24 04:11:30","77541156","-1","614","<nlp><huggingface-transformers><transformer-model><huggingface><large-language-model>","<p>I want to use inference to ask different questions to LLMs taken from huggingface. But, I want to ask the prompts without the model having info about the previous prompts. Does the model automatically store the previous prompts in context?</p>
<p>Or does it not save any previous information at all and we need to provide all the context in the same prompt?</p>
","transformer-model"
"77540026","How to convert 4D tensor to Dataset with omegaconf DictConfig","2023-11-23 23:18:40","","0","49","<python><pytorch><transformer-model><pytorch-dataloader><omegaconf>","<p>I am trying to use this specific version of a Vision Transformer Model I found on <a href=""https://github.com/insitro/ChannelViT/blob/main/channelvit/backbone/multi_vit.py"" rel=""nofollow noreferrer"">GitHub</a> to train a model with my own dataset. My data is a (400, 3, 224, 224) tensor and my labels are a (400) tensor in pytorch. The problem is the code on gitHub seems to be using cfg: DictConfig as input to the <a href=""https://github.com/insitro/ChannelViT/blob/main/channelvit/main/main_supervised.py"" rel=""nofollow noreferrer"">get_train_loader function</a>, and I frankly have no clue how this works. I have tried to use my tensors as input like this:</p>
<h1>Forward pass</h1>
<pre><code>outputs = model(inputs)
</code></pre>
<p>But i get the following error:</p>
<pre><code>KeyError: 'channels'
</code></pre>
<p>So i assumed I have to somehow convert my data and labels tensor to a dataset with dictConfig. I am a beginner in pytorch and I have never used the omegaconf library soany help would be really apreciated!</p>
<p>Here's the code for the patch encoder:</p>
<pre><code>class PatchEmbedPerChannel(nn.Module):
    &quot;&quot;&quot;Image to Patch Embedding.&quot;&quot;&quot;

    def __init__(
        self,
        img_size: int = 224,
        patch_size: int = 16,
        in_chans: int = 3,
        embed_dim: int = 768,
        enable_sample: bool = True,
    ):
        super().__init__()
        num_patches = (img_size // patch_size) * (img_size // patch_size) * in_chans
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches

        self.proj = nn.Conv3d(
            1,
            embed_dim,
            kernel_size=(1, patch_size, patch_size),
            stride=(1, patch_size, patch_size),
        )  # CHANGED

        self.channel_embed = nn.Embedding(in_chans, embed_dim)
        self.enable_sample = enable_sample

        trunc_normal_(self.channel_embed.weight, std=0.02)

    def forward(self, x, extra_tokens={}):
        # # assume all images in the same batch has the same input channels
        # cur_channels = extra_tokens[&quot;channels&quot;][0]
        # embedding lookup
        cur_channel_embed = self.channel_embed(
            extra_tokens[&quot;channels&quot;]
        )  # B, Cin, embed_dim=Cout
        cur_channel_embed = cur_channel_embed.permute(0, 2, 1)  # B Cout Cin

        B, Cin, H, W = x.shape
        # Note: The current number of channels (Cin) can be smaller or equal to in_chans

        if self.training and self.enable_sample:
            # Per batch channel sampling
            # Note this may be slow
            # Randomly sample the number of channels for this batch
            Cin_new = random.randint(1, Cin)

            # Randomly sample the selected channels
            channels = random.sample(range(Cin), k=Cin_new)
            Cin = Cin_new
            x = x[:, channels, :, :]

            # Update the embedding lookup
            cur_channel_embed = cur_channel_embed[:, :, channels]
            ######

        # shared projection layer across channels
        x = self.proj(x.unsqueeze(1))  # B Cout Cin H W

        # channel specific offsets
        x += cur_channel_embed.unsqueeze(-1).unsqueeze(-1)
        # x += self.channel_embed[:, :, cur_channels, :, :]  # B Cout Cin H W

        # preparing the output sequence
        x = x.flatten(2)  # B Cout CinHW
        x = x.transpose(1, 2)  # B CinHW Cout

        return x, Cin
</code></pre>
<p>And here the get_train_loader mentioned above:</p>
<pre><code>def get_train_loader(cfg: DictConfig):
    # Define the training data loader.
    if len(cfg.train_data) == 1:

        print(&quot;There is only one training data&quot;)
        train_data_cfg = next(iter(cfg.train_data.values()))
        with open_dict(cfg):
            cfg.train_data = train_data_cfg

        train_data = getattr(data, train_data_cfg.name)(
            is_train=True,
            transform_cfg=cfg.train_transformations,
            **train_data_cfg.args,
        )
        train_loader = DataLoader(
            train_data, **train_data_cfg.loader, collate_fn=train_data.collate_fn
        )

        # We also need to pre-compute the number of batches for each epoch.
        # We will use this inforamtion for the learning rate schedule.
        with open_dict(cfg):
            # get number of batches per epoch (many optimizers use this information to schedule
            # the learning rate)
            cfg.train_data.loader.num_batches = (
                len(train_loader) // cfg.trainer.devices + 1
            )

        return train_loader

    else:
        print(&quot;There're more than one training data&quot;)
        train_loaders = {}
        len_loader = None
        batch_size = 0

        for name, train_data_cfg in cfg.train_data.items():
            print(f&quot;Loading {train_data_cfg.name}&quot;)
            train_data = getattr(data, train_data_cfg.name)(
                is_train=True,
                transform_cfg=cfg.train_transformations,
                **train_data_cfg.args,
            )
            train_loader = DataLoader(
                train_data, **train_data_cfg.loader, collate_fn=train_data.collate_fn
            )
            train_loaders[name] = train_loader

            print(f&quot;Dataset {name} has length {len(train_loader)}&quot;)

            if len_loader is None:
                len_loader = len(train_loader)
            else:
                len_loader = max(len_loader, len(train_loader))

            # batch_size += train_data_cfg.loader.batch_size
            batch_size = train_data_cfg.loader.batch_size

        with open_dict(cfg):
            cfg.train_data.loader = {}
            cfg.train_data.loader.num_batches = len_loader // cfg.trainer.devices + 1
            cfg.train_data.loader.batch_size = batch_size

        return train_loaders
</code></pre>
","transformer-model"
"77527264","Suboptimal result of the masked vision transformer","2023-11-22 04:04:35","","0","26","<deep-learning><transformer-model>","<p>I tried to use Huggingface to test a masked autoencoder model using pretrained weights.</p>
<p>I used the following code to inference it.</p>
<pre><code>from torchvision.transforms import ToPILImage
from transformers import AutoImageProcessor, SwinForMaskedImageModeling
import torch
from PIL import Image
import requests
import matplotlib.pyplot as plt
from transformers import AutoImageProcessor, Swinv2ForMaskedImageModeling


url = &quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;
image = Image.open(requests.get(url, stream=True).raw)

image_processor = AutoImageProcessor.from_pretrained(&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;)
model = Swinv2ForMaskedImageModeling.from_pretrained(&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;)

num_patches = (model.config.image_size // model.config.patch_size) ** 2
pixel_values = image_processor(images=image, return_tensors=&quot;pt&quot;).pixel_values
# create random boolean mask of shape (batch_size, num_patches)
bool_masked_pos = torch.randint(low=0, high=2, size=(1, num_patches)).bool()

outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
loss, reconstructed_pixel_values = outputs.loss, outputs.reconstruction
list(reconstructed_pixel_values.shape)

plt.imshow(((pixel_values-pixel_values.min())/(pixel_values.max()-pixel_values.min())).squeeze().permute(2,1,0).detach().numpy())
plt.show()
plt.imshow(((reconstructed_pixel_values-reconstructed_pixel_values.min())/(reconstructed_pixel_values.max()-reconstructed_pixel_values.min())).squeeze().permute(2,1,0).detach().numpy())
plt.show()
</code></pre>
<p>The result I got is not good. The loss is 1.17, and here you can see the original processed image and the output of the network.</p>
<p>[![original][1]][1]
[![results][2]][2]</p>
<p>I tried use the another pretrained model with smaller number of patches. However, the result was blury and not good. I used microsoft/swin-base-simmim-window6-192 as pretrained model.</p>
<p>[![enter image description here][3]][3]</p>
<p>I appreciate if you point to my potential mistakes.
[1]: <a href=""https://i.sstatic.net/M474w.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/M474w.png</a>
[2]: <a href=""https://i.sstatic.net/qkDHw.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/qkDHw.png</a>
[3]: <a href=""https://i.sstatic.net/k3YWA.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/k3YWA.png</a></p>
","transformer-model"
"77487306","How to use 'curvefit' in python to fit a neural network?","2023-11-15 11:23:15","","0","112","<python><curve-fitting><transformer-model>","<pre><code>model = torch.load(.. / .....)

def fit_func(input_param, *params):
    model.eval()
    with torch.no_grad():
        # predict flux using the neural network, params is the values need to be fitted.
        predicted_flux = model(params)
    return predicted_flux.cpu().detach().numpy()

# true parameters and true flux in the dataset
parameters, true_flux = get_input_flux(input_valid, flux_valid)

for i in range(len(validation_set)):
    initial_guess = parameters[i, :]  # use true parameters as initial values
    estimated_params, _, _, mesg, ier = curve_fit(
        fit_func,
        xdata=np.arange(len(flux[i, :])),
        ydata=flux[i, :],
        p0=initial_guess,
        bounds=(-1.5, 1.5),
        method='dogbox',
        full_output=True,
        xtol=1e-10, ftol=1e-10,
        max_nfev=100000
    )
    print(i, mesg, ier, estimated_params[0], estimated_params[1], estimated_params[2])
  
</code></pre>
<p>The function of the neural network is to input parameters and output fluxes. I am trying to use the trained best neural network model to fit the parameters. However, I always obtain results same as the initial values.</p>
","transformer-model"
"77476898","Why replace the masked token with random token in bert?","2023-11-13 21:09:52","","0","494","<nlp><bert-language-model><transformer-model>","<p>Masking in Bert is:</p>
<ul>
<li>Take 15% of all the tokens in the sequence. These are to be used in computing the MLM loss</li>
<li>80% of the time retain the mask</li>
<li>10% of the time replace the mask with the original token</li>
<li>10% of the time replace the mask with a random token</li>
</ul>
<p>I understand why masking is needed. I also understand why the masking is replaced back to the original token. This is because otherwise, the model learns to completely ignore the word itself in deriving its contextual embeddings in the downstream tasks.</p>
<p>What is the need to replace the mask with a random word 10% of the time?</p>
","transformer-model"
"77463361","Training a transformer to copy sequence to identical sequence?","2023-11-10 23:44:24","","0","167","<machine-learning><deep-learning><neural-network><transformer-model><seq2seq>","<p>As part of my learning process I wanted to write a transformer model to copy input sequence to output sequence. I thought it would be relatively straight forward, however the results are less than ideal - loss is higher than I've expected resulting in inaccurate copies of numbers in the sequence.</p>
<p>I would love to get some pointers on where I can improve (so far I tried a lot of hyper param tuning, nothing substantially changed the outcome). Is there something wrong with my architecture? Is there some algorithmic bugs I overlooked?</p>
<pre><code>Sample run
Epoch 0 Loss 1.9481123001017469
Epoch 1 Loss 1.4541001472067325
Epoch 2 Loss 1.2569004525529577
Epoch 3 Loss 1.158278153297749
Epoch 4 Loss 1.1283172952367904
Test Case 0
Expected: [2, 3, 4, 2, 5, 9, 6, 7]
Actual: [2, 6, 4, 9, 3, 2, 7, 5]

Test Case 1
Expected: [8, 2, 5, 6, 7, 9, 4, 8]
Actual: [8, 2, 9, 4, 6, 5, 8, 7]

Test Case 2
Expected: [8, 5, 6, 3, 4, 5, 6, 8]
Actual: [8, 6, 5, 3, 4, 6, 8, 5]

Test Case 3
Expected: [5, 5, 7, 5, 2, 2, 9, 2]
Actual: [2, 5, 2, 5, 9, 7, 2]

Test Case 4
Expected: [4, 5, 9, 5, 3, 5, 7, 8]
Actual: [5, 9, 5, 3, 8, 4, 7]
</code></pre>
<p>My Implementation:</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import math

from torch.utils.data import Dataset, DataLoader

# Set the random seed for reproducibility
torch.manual_seed(0)

# Define the device to run the model on
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Model hyperparameters
input_size = 10  # Size of the input vocabulary (number of unique tokens)
num_layers = 6  # Number of transformer layers
d_model = 512  # The number of expected features in the encoder/decoder inputs
nhead = 8  # The number of heads in the multiheadattention model
dim_feedforward = 2048  # Dimension of the feedforward network model in nn.TransformerEncoder
dropout = 0.01  # Dropout rate
max_seq_length = 10  # Maximum sequence length

# Training hyperparameters
batch_size = 64
num_samples = 3000
epochs = 5
lr = 0.0001  # Learning rate

SOS_token = np.array([0])
EOS_token = np.array([1])


class TransformerModel(nn.Module):
    def __init__(self, input_size, d_model, nhead, num_layers, dim_feedforward, dropout=0.1):
        super(TransformerModel, self).__init__()
        self.model_type = 'Transformer'
        self.tgt_mask = None
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        self.embedding = nn.Embedding(input_size, d_model)
        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers,
                                          dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)
        self.linear = nn.Linear(d_model, input_size)

    def _generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(self, src, tgt):
        tgt_mask = self._generate_square_subsequent_mask(tgt.size(1)).to(device)

        src = self.embedding(src) * math.sqrt(d_model)
        tgt = self.embedding(tgt) * math.sqrt(d_model)
        src = self.pos_encoder(src)
        tgt = self.pos_encoder(tgt)

        output = self.transformer(src, tgt, tgt_mask=tgt_mask)
        output = self.linear(output)
        return output


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=1024):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)




class TrainingDataSet(Dataset):
    def __init__(self, num_tokens, seq_length, size):
        self.num_tokens = num_tokens
        self.seq_length = seq_length
        self.size = size

    def __len__(self):
        return self.size

    def __getitem__(self, index):
        sequence = np.concatenate(
            (SOS_token, torch.randint(2, 10, (self.seq_length,), dtype=torch.long), EOS_token))
        return sequence, sequence  # input and target are the same in the copy task


# Create the dataset and dataloader
dataset = TrainingDataSet(input_size,
                          max_seq_length - 2,  # -2 due to SOS end EOS tokens
                          num_samples)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Initialize the model
model = TransformerModel(input_size, d_model, nhead, num_layers, dim_feedforward, dropout).to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=lr)

# Training loop
model.train()
for epoch in range(epochs):
    total_loss = 0.
    for src, tgt in dataloader:
        src = src.to(device)
        tgt = tgt.to(device)

        tgt_input = tgt[:, :-1]
        tgt_expected = tgt[:, 1:]

        optimizer.zero_grad()
        output = model(src, tgt_input)
        # loss = criterion(output.view(-1, input_size), tgt_expected.view(-1))
        # loss = criterion(output.reshape(-1, input_size), tgt_expected.reshape(-1))

        # permute (N, L, C) -&gt; (N, C, L) where N is batch, L is seq len, C is class/vocab_size
        output = output.permute(0, 2, 1)
        # shape of tgt_expected is (N, L) where N is batch, L is seq len
        loss = criterion(output, tgt_expected)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    print(f'Epoch {epoch} Loss {total_loss / len(dataloader)}')

# Save the model
torch.save(model.state_dict(), 'transformer_copy_task.pth')

def predict(model, input_sequence):
    &quot;&quot;&quot;
    Method from &quot;A detailed guide to Pytorch's nn.Transformer() module.&quot;, by
    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1
    &quot;&quot;&quot;
    model.eval()

    y_input = torch.tensor([SOS_token], dtype=torch.long, device=device)

    for _ in range(max_seq_length+2):
        pred = model(input_sequence, y_input)

        next_item = pred.topk(1)[1].view(-1)[-1].item()  # num with highest probability
        next_item = torch.tensor([[next_item]], device=device)

        # Concatenate previous input with predicted best word
        y_input = torch.cat((y_input, next_item), dim=1)

        # Stop if model predicts end of sentence
        if next_item.view(-1).item() == EOS_token:
            break

    return y_input.view(-1).tolist()


# Test the model
model.eval()

# Here we test some examples to observe how the model predicts
examples = [
    torch.tensor([[0, 2, 3, 4, 2, 5, 9, 6, 7, 1]], dtype=torch.long, device=device),
    torch.tensor([[0, 8, 2, 5, 6, 7, 9, 4, 8, 1]], dtype=torch.long, device=device),
    torch.tensor([[0, 8, 5, 6, 3, 4, 5, 6, 8, 1]], dtype=torch.long, device=device),
    torch.tensor([[0, 5, 5, 7, 5, 2, 2, 9, 2, 1]], dtype=torch.long, device=device),
    torch.tensor([[0, 4, 5, 9, 5, 3, 5, 7, 8, 1]], dtype=torch.long, device=device),
]

for idx, example in enumerate(examples):
    result = predict(model, example)
    print(f&quot;Test Case {idx}&quot;)
    print(f&quot;Expected: {example.view(-1).tolist()[1:-1]}&quot;)
    print(f&quot;Actual: {result[1:-1]}&quot;)
    print()
</code></pre>
","transformer-model"
"77451982","Cache when using pipieline from huggingface","2023-11-09 09:34:57","","0","179","<python><machine-learning><pytorch><huggingface-transformers><transformer-model>","<p>When I load and run a pipeline in huggingface, something is cached and it changes the results in the pipeline.
I would like to free the cache completely. I already tried:
<code>gc.collect()</code> and <code>torch.cuda.empty_cache()</code>
but it still doesn't work.</p>
<p>This is my Code:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer_1 = AutoTokenizer.from_pretrained(model_1)
model_1 = AutoModelForCausalLM.from_pretrained(model_1, device_map=device_1,torch_dtype=torch.bfloat16)

pipe= pipeline(
    &quot;text-generation&quot;,    
    model=model_1,        
    tokenizer=tokenizer_1,   
    torch_dtype=torch.bfloat16,  
    max_new_tokens=500,  
    min_new_tokens=-1, 
    do_sample=True,          
    top_k=50,      
    top_p=1, 
    repetition_penalty = 1.15, 
    temperature=0.01
)

 outcome= pipe(input)
</code></pre>
<p>After I just loaded the model I have 25441MiB on my GPU.</p>
<p>After I run it one or more times, it increases and stays at 26305MiB.</p>
<p>I want to get rid of the extra memory since it seems to affect the outcome of my pipe.</p>
","transformer-model"
"77449999","PyTorch RuntimeError: Invalid Shape During Reshaping for Multi-Head Attention","2023-11-09 01:56:55","","0","126","<python><pytorch><huggingface-transformers><transformer-model><multihead-attention>","<p>I'm implementing a multi-head self-attention mechanism in PyTorch which is part of Text2Image model that I am trying to build and I'm encountering a runtime error when trying to reshape the output of linear transformations before splitting into multiple heads. The text embeddings from my model have a shape of <code>[32, 26, 768]</code>, and I'm using 8 attention heads with an embedding size of 768. However, during reshaping, I get an invalid shape error. Here I am providing various blocks(as screenshots) &amp; overall model definition(pasted below) &amp; error message can you guys help me correcting the error</p>
<ul>
<li><p>Text Encoder Block
<a href=""https://i.sstatic.net/0CLzq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0CLzq.png"" alt=""Text Encoder Block"" /></a></p>
</li>
<li><p>Generator Block
<a href=""https://i.sstatic.net/DnSSy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DnSSy.png"" alt=""Generator Block"" /></a></p>
</li>
<li><p>Discriminator Block
<a href=""https://i.sstatic.net/cSNpI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cSNpI.png"" alt=""Discriminator Block"" /></a></p>
</li>
<li><p>Attention Mechanism
<a href=""https://i.sstatic.net/EhCoe.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EhCoe.png"" alt=""Attention Mechanism"" /></a></p>
</li>
<li><p>Transformer Block
<a href=""https://i.sstatic.net/56sgf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/56sgf.png"" alt=""Transformer Block"" /></a></p>
</li>
<li><p>overall model Architecture:</p>
</li>
</ul>
<pre><code>class Text2ImageModel(nn.Module):
    def __init__(self, image_size, text_embedding_dim, noise_dim, embed_size, heads, dropout, forward_expansion):
        super(Text2ImageModel, self).__init__()

        # Initialize tokenizer from pretrained GPT-2 model
        self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
        self.tokenizer.pad_token = self.tokenizer.eos_token

        # Text encoder (GPT-2 or BERT based) - assuming it outputs a text embedding
        self.text_encoder = TextEncoder()

        # Attention block
        self.attention_block = SelfAttention(embed_size=embed_size, heads=heads, dropout=dropout)

        # Transformer block
        self.transformer_block = TransformerBlock(embed_size=embed_size,
                                                  heads=heads,
                                                  dropout=dropout,
                                                  forward_expansion=forward_expansion)

        # Generator and Discriminator
        self.generator = Generator(text_embedding_dim=text_embedding_dim, z_dim=noise_dim, img_size=image_size)
        self.discriminator = Discriminator(image_size=image_size, text_embedding_dim=text_embedding_dim)

    def forward(self, text_input, images, noise, attention_mask=None):
        # Tokenize and encode the text input
        # tokens = self.tokenizer(text_input, return_tensors='pt', padding=True, truncation=True)
        text_embeddings = self.text_encoder(text_input)
        print(f&quot;Text embeddings shape: {text_embeddings.shape}&quot;) 

        # Apply self-attention to text embeddings
        attention_output = self.attention_block(value=text_embeddings, key=text_embeddings, query=text_embeddings, mask=attention_mask)  
        print(f&quot;Attention output shape: {attention_output.shape}&quot;)
        # Pass the output of attention through the transformer block
        transformer_output= self.transformer_block(value=attention_output, key=attention_output, query=attention_output, mask=attention_mask)
        print(f&quot;Transformer output shape: {transformer_output.shape}&quot;)
        # Generate an image from the transformer output and noise
        generated_images = self.generator(transformer_output, noise)

        # Discriminator takes real images and the corresponding text embeddings
        real_image_discrimination = self.discriminator(images, transformer_output)
        # Discriminator also takes the fake images and text embeddings
        fake_image_discrimination = self.discriminator(generated_images.detach(), transformer_output)

        return generated_images, real_image_discrimination, fake_image_discrimination
</code></pre>
<ul>
<li>the error is popping up while I am trying to pass the text embeddings to attention module</li>
</ul>
<pre><code>Text embeddings shape: torch.Size([32, 27, 768])
Before self attention: torch.Size([32, 27, 768]) torch.Size([32, 27, 768]) torch.Size([32, 27, 768])
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-147-a568d03c70d5&gt; in &lt;cell line: 61&gt;()
     71 
     72         # Forward pass through the model
---&gt; 73         fake_images, real_preds, fake_preds = model(list(captions), images, noise, attention_mask=None)
     74         # Flatten the output for the discriminator
     75         real_preds = real_preds.view(-1)

5 frames
&lt;ipython-input-144-6021d3c7122c&gt; in forward(self, value, key, query, mask)
     24         print(&quot;Before self attention:&quot;, value.shape, key.shape, query.shape)
     25         # Transform and split for multi-head attention
---&gt; 26         values = self.values(value).view(N, value_len, self.heads, self.head_dim)
     27         keys = self.keys(key).view(N, key_len, self.heads, self.head_dim)
     28         queries = self.queries(query).view(N, query_len, self.heads, self.head_dim)

RuntimeError: shape '[32, 27, 8, 768]' is invalid for input of size 663552
</code></pre>
<p>The expected output shape after the linear layer should match [batch_size, seq_len, heads, head_dim], but instead, I am getting an error indicating a mismatch in total elements. I've confirmed that my sequence length is 26, and I'm not sure where the number 27 is coming from in the error message.</p>
<p>Why is my reshaping operation failing, and how can I correct the shape to be compatible with the expected dimensions for multi-head attention?</p>
","transformer-model"
"77449597","Augmenting sequence embeddings with multiclass label encodings","2023-11-08 23:35:36","","0","16","<deep-learning><pytorch><huggingface-transformers><embedding><transformer-model>","<p>I'm using a Transformer to embed windows within a sequence of temporal data. Each data window falls within multiple classes. One of the classes will be the most relevant to interpreting a window, as it influences a signal within the data, but the identity of this class is not known a priori. The number of total classes and the most relevant class can vary between sequences.</p>
<p>I'm looking into ways to adjust the embeddings of windows to relate the windows that share at least one class. This is somewhat analogous to sinusoidal positional encodings, where nearby windows within a sequence have a shorter distance. I've considered using a graph neural network, defining each window as a graph node and drawing edges between windows with the same class. However, I also want the model to potentially be able to recover which windows belong to the 'most relevant class' mentioned above. This means encoding not just that two windows share a class, but also which class they share. I've also looked into multi-hot encoding (where the number of classes would be set to the max possible number of classes), which will then be concatenated/summed with the input. With concatenation, this might drastically increase the input dimensionality. I would appreciate any leads into potential alternatives!</p>
","transformer-model"
"77444485","Using positional encoding in pytorch","2023-11-08 09:57:55","77445896","3","7515","<deep-learning><pytorch><transformer-model>","<p>Is there any built-in positional encoding in pytorch? Basically, I want to be able to specify the dimension of the encoding, and then be able to get the i'th encoding for every i.</p>
","transformer-model"
"77443154","Understanding attention output from generate method in GPT model","2023-11-08 06:06:33","","2","293","<huggingface-transformers><transformer-model><gpt-2>","<pre class=""lang-py prettyprint-override""><code># Importing necessary modules
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Loading pre-trained GPT-2 tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Encoding input text
input_ids = tokenizer.encode(&quot;The dog is running&quot;, return_tensors='pt')

# Generating model output with attention information
output = model.generate(
    input_ids,
    max_length=6,
    num_return_sequences=1,
    no_repeat_ngram_size=2,
    output_attentions=True,
    return_dict_in_generate=True,
)

# Extracting attention tensors
attn = output.attentions
</code></pre>
<p>My observations are as follows.</p>
<ul>
<li>The <code>attn</code> variable is a tuple with two items representing the number of newly generated tokens (because 6 - 4 is 2).</li>
<li>Each item is a tuple of 12 tensors, corresponding to the number of layers in each GPT block.</li>
<li>The shape of the first tensor is [1, 12, 4, 4], and for the second tensor, it's [1, 12, 1, 5].</li>
<li>When visualized, the tensor of shape [1, 12, 4, 4] represents masked attention.</li>
</ul>
<p>Here are my questions.</p>
<ul>
<li>What do tensors with shapes [1, 12, 4, 4] and [1, 12, 1, 5] represent? How are they different?</li>
<li>At what decoding stage do these tensors come from?</li>
</ul>
","transformer-model"
"77442241","tgt_key_padding_mask in pytorch transformers BertModel","2023-11-08 00:40:51","","1","366","<pytorch><padding><mask><bert-language-model><transformer-model>","<p>While going through the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer"" rel=""nofollow noreferrer"">transformer documentation</a> in PyTorch, I see that the tgt_key_padding_mask of shape (batch_size, tgt_seq_len) is used to indicate irrelevance of some parts of tgt because of padding. When I look at Pytorch's <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L865"" rel=""nofollow noreferrer"">BertModel implementation</a> from the transformers library, I don't see an option for such a mask in the forward function. How do I provide the tgt_key_padding_mask when using BertModel?</p>
<p>Notes: The top answer <a href=""https://stackoverflow.com/questions/62170439/difference-between-src-mask-and-src-key-padding-mask"">here </a>explains what tgt_key_padding_mask is.</p>
<p>BertModel has the option to provide a head_mask, which is <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L363"" rel=""nofollow noreferrer"">multiplied with attention_probs</a> (after softmax, but before multiplying with the value). I haven't seen any mention of/ documentation regarding the expected shape of head_mask. It seems like it will be broadcasted to whatever shape necessary to multiply with attention_probs (which if of shape (batch_size, num_heads, query_seq_len=tgt_seq_len, key_seq_len=src_seq_len)), failing which a shape mismatch error will be triggered. I was thinking that I could simply use/ abuse this head_mask by passing my tgt_key_padding_mask via this. But is there another more appropriate way to specify a tgt_key_padding_mask while using BertModel?</p>
","transformer-model"
"77440553","The CodeBert model always generates the same output","2023-11-07 18:14:20","","0","105","<machine-learning><deep-learning><huggingface-transformers><transformer-model><multiclass-classification>","<p>I am trying to use a CodeBert model followed by a CNN for multi-class classification of vulnerabilities in source code. I tried to test the outputs of the CodeBert model on an example.</p>
<pre><code>model_name = &quot;microsoft/codebert-base&quot;
codebert_model = AutoModel.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
codebert_model.to(device)
batch_texts = [&quot;Example &quot;, &quot;OtherExample&quot;, &quot;This is another example &quot;]
batch_encoded = tokenizer.batch_encode_plus(batch_texts, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
prova1 = batch_encoded[&quot;input_ids&quot;]
prova2 = batch_encoded[&quot;attention_mask&quot;]
output_prova =codebert_model(input_ids=prova1.to(device),attention_mask=prova2.to(device)).last_hidden_state
print(output_prova)
</code></pre>
<p>This is the output:</p>
<pre><code>tensor([[[-0.1492,  0.3261,  0.0464,  ..., -0.2080, -0.3364,  0.3195],
         [-0.3576,  0.2826,  0.3289,  ..., -0.0565, -0.7721,  0.1333],
         [ 0.2045, -0.3244,  0.2059,  ...,  0.0232, -0.5977,  0.0806],
         ...,
         [ 0.1698,  0.1308,  0.3582,  ..., -0.2559, -0.0660,  0.3568],
         [ 0.1698,  0.1308,  0.3582,  ..., -0.2559, -0.0660,  0.3568],
         [ 0.1698,  0.1308,  0.3582,  ..., -0.2559, -0.0660,  0.3568]],

        [[-0.1296,  0.3715,  0.0631,  ..., -0.1635, -0.2994,  0.3208],
         [-0.1772,  0.9282,  0.4182,  ...,  0.0147, -0.2448,  0.2903],
         [-0.1238,  0.4330,  0.2896,  ..., -0.0967, -0.6715,  0.5703],
         ...,
         [-0.4116,  0.2910,  0.2929,  ..., -0.6710, -0.2761,  0.3889],
         [-0.4116,  0.2910,  0.2929,  ..., -0.6710, -0.2761,  0.3889],
         [-0.4116,  0.2910,  0.2929,  ..., -0.6710, -0.2761,  0.3889]],

        [[-0.1394,  0.3415,  0.0495,  ..., -0.1913, -0.3411,  0.2758],
         [-0.2208,  0.6902,  0.5890,  ..., -0.1302, -0.5507,  0.3460],
         [-0.1908,  0.7800,  0.4150,  ..., -0.5183, -0.5432,  0.1034],
         ...,
         [-0.1698,  0.2102,  0.5444,  ..., -0.0313, -0.7477,  0.4945],
         [ 0.0994,  0.0209,  0.2717,  ..., -0.0929, -0.6063,  0.3174],
         [-0.1391,  0.3422,  0.0502,  ..., -0.1915, -0.3420,  0.2762]]],
       device='cuda:0', grad_fn=&lt;NativeLayerNormBackward0&gt;)
</code></pre>
<p>I then created the model by inserting the CodeBert model before my CNN</p>
<pre><code>import torch.nn.functional as F
import torch
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer

class MyCNN(nn.Module):
    def __init__(self, codebert_model, input_size, output_size):
        super(MyCNN, self).__init__()

        # Aggiungi il modello Codebert
        self.codebert = codebert_model

        # Aggiungi il layer di convoluzione 1D
        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=200, kernel_size=3)
        self.conv2 = nn.Conv1d(in_channels=input_size, out_channels=200, kernel_size=4)
        self.conv3 = nn.Conv1d(in_channels=input_size, out_channels=200, kernel_size=5)


        # Aggiungi il layer di attivazione ReLU
        self.dropout = nn.Dropout(0.5)

        # Aggiungi il layer di max pooling
        self.fc1 = nn.Linear(200*3,256) #500
        self.fc2 = nn.Linear(256,128)
        self.fc3 = nn.Linear(128,12)

    def forward(self, x , attention_mask):
        # Estrai gli embedding di Codebert
        codebert_output = self.codebert(x, attention_mask=attention_mask).last_hidden_state
        x = codebert_output.permute(0, 2, 1)

        # Esegui la parte di CNN
        #x = codebert_output.permute(0, 2, 1)
        x1 = F.relu(self.conv1(x))
        x2 = F.relu(self.conv2(x))
        x3 = F.relu(self.conv3(x))

        x1 = F.max_pool1d(x1, x1.shape[2])
        x2 = F.max_pool1d(x2, x2.shape[2])
        x3 = F.max_pool1d(x3, x3.shape[2])

        x = torch.cat([x1,x2,x3],dim=1)

        # flatten the tensor
        x = x.flatten(1)

        # apply mean over the last dimension
        #x = torch.mean(x, -1)

        x = self.dropout(x)

        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return(x)
</code></pre>
<p>I built a Dataload that returns the source code and the label associated with the cwe.
And I wrote the training function.</p>
<pre><code>print('Training started.....')
model.to(device)
EPOCHS=20
running_acc = 0
running_loss = 0
dim_batch = 4
BEST_VAL_ACC = 0
timer = time.time()
for e in range(EPOCHS):
  train,val=split_training(train_encodings_119,train_encodings_20,train_encodings_787,train_encodings_125,train_encodings_416,train_encodings_399,train_encodings_200,train_encodings_476,train_encodings_190,train_encodings_264,train_encodings_189,train_encodings_Other)
  train_data = CustomDataset(train)
  val_data  = CustomDataset(val)
  train_iterator = DataLoader(train_data, batch_size=dim_batch, shuffle=False)
  valid_iterator = DataLoader(val_data, batch_size=dim_batch, shuffle=False)
  i=0
  for batch in train_iterator:
    i =  i+1
    code, target = batch
    batch_encoded = tokenizer.batch_encode_plus(code, padding=&quot;max_length&quot;, truncation=True, return_tensors=&quot;pt&quot;, max_length=500)
    input_ids = batch_encoded[&quot;input_ids&quot;].to(device)
    attention_mask = batch_encoded[&quot;attention_mask&quot;].to(device)
    target = target.long().to(device)
    optimizer.zero_grad()
    output = model(input_ids,attention_mask=attention_mask)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()
    acc = multiclass_accuracy(output,target)
    running_acc += acc
    running_loss += loss.item()
  with torch.no_grad():
        model.eval()
        running_acc_val = 0
        running_loss_val = 0
        for batch in valid_iterator:
          code, target = batch
          batch_encoded = tokenizer.batch_encode_plus(code, padding=&quot;max_length&quot;, truncation=True, return_tensors=&quot;pt&quot;, max_length=500)
          input_ids = batch_encoded[&quot;input_ids&quot;].to(device)
          attention_mask = batch_encoded[&quot;attention_mask&quot;].to(device)
          target = target.long().to(device)
          output_val = model(input_ids,attention_mask=attention_mask)
          loss_val = criterion(output_val,target)
          acc_val = multiclass_accuracy(output_val,target)
          running_acc_val += acc_val
          running_loss_val += loss_val.item()
  print_out = &quot;Epoch %d - Training acc: %.4f -Training loss: %.4f - Val acc: %.4f - Val loss: %.4f - Time: %.4fs \n&quot; % (e+1,
  running_acc/len(train_iterator),
  running_loss/len(train_iterator),
  running_acc_val/len(valid_iterator),
  running_loss_val/len(valid_iterator),
  (time.time()-timer))
  if(running_acc_val/len(valid_iterator) &gt; BEST_VAL_ACC):
    BEST_VAL_ACC = running_acc_val/len(valid_iterator)
    model_save_path = &quot;Encodings/model-multiclass.pth&quot;
    torch.save(model.state_dict(), model_save_path)

print('Training completed!')
</code></pre>
<p>I have some pretty strange results. But the strangest thing is that it seems that the CodeBert model always outputs the same values.
In fact, if I try to relaunch the first example of the question, it generates three tensors that are all equal to each other.
This is the output of the example piece of code written at the beginning of the question after launching the training function.</p>
<pre><code>tensor([[[ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         ...,
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111]],

        [[ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         ...,
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111]],

        [[ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         ...,
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111],
         [ 0.0545,  0.1427, -0.0886,  ..., -0.1833,  0.0855,  0.0111]]],
       device='cuda:0', grad_fn=&lt;NativeLayerNormBackward0&gt;)
</code></pre>
<p>I don't understand where the problem is, the input_ids and attention_mask are the correct size and actually change for each batch. However, the output always remains the same.
Can someone help me?
I've tried changing everything but I just can't figure it out.</p>
","transformer-model"
"77431718","Memory and calculation issues running an LLM on GPU in PyTorch","2023-11-06 14:06:59","","0","198","<python><pytorch><gpu><transformer-model><large-language-model>","<p>I am trying to train a local transformer model for a generic sequence modeling task on a 3090 GPU, but I am dealing with a few weird GPU issues I haven’t seen before. The model is just standard PyTorch 2.0, and I am accessing the GPU remotely via ssh.</p>
<p>The first and most pressing issue is that the training function is randomly freezing in the middle of some epochs. When that happens the whole kernel (testing in a notebook) becomes unresponsive, and I have to restart it. The terminal usually shows errors like the following:</p>
<pre><code>Task exception was never retrieved
future: &lt;Task finished name='Task-29973' coro=&lt;WebSocketProtocol13.write_message.&lt;locals&gt;.wrapper() done, defined at /home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py:1090&gt; exception=WebSocketClosedError()&gt;
Traceback (most recent call last):
  File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 1092, in wrapper
    await fut
tornado.iostream.StreamClosedError: Stream is closed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/asyncio/tasks.py&quot;, line 232, in __step
    result = coro.send(None)
  File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 1094, in wrapper
    raise WebSocketClosedError()
tornado.websocket.WebSocketClosedError
[E 20:05:14.638 NotebookApp] Uncaught exception in ZMQStream callback
    Traceback (most recent call last):
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 584, in _run_callback
        f = callback(*args, **kwargs)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 308, in stream_callback
        return callback(self, msg)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/notebook/services/kernels/handlers.py&quot;, line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/notebook/base/zmqhandlers.py&quot;, line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 1061, in _write_frame
        return self.stream.write(frame)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/iostream.py&quot;, line 540, in write
        self._write_buffer.append(data)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/iostream.py&quot;, line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
[E 20:05:14.639 NotebookApp] Uncaught exception in zmqstream callback
    Traceback (most recent call last):
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 634, in _handle_events
        self._handle_recv()
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 663, in _handle_recv
        self._run_callback(callback, msg)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 584, in _run_callback
        f = callback(*args, **kwargs)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 308, in stream_callback
        return callback(self, msg)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/notebook/services/kernels/handlers.py&quot;, line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/notebook/base/zmqhandlers.py&quot;, line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 1061, in _write_frame
        return self.stream.write(frame)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/iostream.py&quot;, line 540, in write
        self._write_buffer.append(data)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/iostream.py&quot;, line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
[E 20:05:14.640 NotebookApp] Exception in callback functools.partial(&lt;function ZMQStream._update_handler.&lt;locals&gt;.&lt;lambda&gt; at 0x7f9f44e875b0&gt;)
    Traceback (most recent call last):
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/ioloop.py&quot;, line 740, in _run_callback
        ret = callback()
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 718, in &lt;lambda&gt;
        self.io_loop.add_callback(lambda: self._handle_events(self.socket, 0))
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 634, in _handle_events
        self._handle_recv()
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 663, in _handle_recv
        self._run_callback(callback, msg)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 584, in _run_callback
        f = callback(*args, **kwargs)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/zmq/eventloop/zmqstream.py&quot;, line 308, in stream_callback
        return callback(self, msg)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/notebook/services/kernels/handlers.py&quot;, line 572, in _on_zmq_reply
        super()._on_zmq_reply(stream, msg)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/notebook/base/zmqhandlers.py&quot;, line 256, in _on_zmq_reply
        self.write_message(msg, binary=isinstance(msg, bytes))
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 339, in write_message
        return self.ws_connection.write_message(message, binary=binary)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 1086, in write_message
        fut = self._write_frame(True, opcode, message, flags=flags)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/websocket.py&quot;, line 1061, in _write_frame
        return self.stream.write(frame)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/iostream.py&quot;, line 540, in write
        self._write_buffer.append(data)
      File &quot;/home/name/anaconda3/envs/setlist/lib/python3.10/site-packages/tornado/iostream.py&quot;, line 157, in append
        b += data  # type: ignore
    BufferError: Existing exports of data: object cannot be re-sized
</code></pre>
<p>The model is running on a 24GB 3090 GPU, and it uses about 18-19GB of RAM.  Anyone understand what could be going on?</p>
<p>The second issue I am trying to understand came from one of my attempts to fix this first problem. I did a little digging and thought it might be due to a memory leak, so I started deleting the input tensors after each forward and backward pass (does this do anything?). And I also put torch.cuda.empty_cache() before each epoch.</p>
<p>I don’t think either of those changes helped, but the second one seems to be causing exploding gradients. Whenever I ran the routine with torch.cuda.empty_cache(), a few batches would randomly introduce loss values of 10^5 or higher, which would rapidly cause the parameters and loss to go to nan. This happened even if I clipped the gradients. It doesn't happen when I delete the empty_cache() function. What is going on here? It seems as though the GPU is being pushed to calculate things incorrectly by the empty_cache command.</p>
","transformer-model"
"77429663","Importing a SequenceClassification model for an NLP taks to Python","2023-11-06 08:15:15","","0","39","<machine-learning><nlp><huggingface-transformers><text-classification><transformer-model>","<p>I'm trying to use a SequenceClassification model for an NLP task and no matter which one I try if can be <code>RoBERTaConvForSequenceClassification</code>,<code>SqueezeBertForSequenceClassification</code>, <code>BARTForSequenceClassification</code> or <code>DistilBertForSequenceClassification</code> the import doesn't work. For example this line:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import DistilBertTokenizer, XLNetForSequenceClassification&quot;  
</code></pre>
<p>gives me a mistake:</p>
<pre><code>RuntimeError: Failed to import transformers.models.xlnet.modeling_xlnet because of the following error (look up to see its traceback):
name '_C' is not defined
</code></pre>
<p>But if I try to only do:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import DistilBertTokenizer
</code></pre>
<p>it works but not for the &quot;SequenceClassification&quot;. I have the latest version of the <code>transformers</code> library and <code>PyTorch</code>.</p>
","transformer-model"
"77428381","Which component in a transformer architecture is actually responsible form mapping a given word into the most likely next word?","2023-11-06 02:03:29","","1","83","<nlp><embedding><transformer-model><attention-model>","<p>I've been trying to gain intuition on how transformers work behind the scenes for language translation. I implemented in <a href=""https://docs.google.com/spreadsheets/d/1HdBN9cM0v7SWPZk_KJrpfMLncDw_ENJ7yFJobevJnak/edit?usp=sharing"" rel=""nofollow noreferrer"">a spreadsheet</a> in order to visualize the math behind and how the embeddings are transformed. But, there's one component that still is not clear to me which is: where the &quot;next word mapping&quot; is really occurring.</p>
<p>For example, let's consider a small inference task from Spanish &quot;yo estoy bien&quot; into English as &quot;I am fine&quot;. Given an input sequence that starts with <code>[&lt;BOS&gt;, 'I', 'am']</code>, which component of the transformer model is tasked with transforming the embedding of 'am' into the embedding for the subsequent word 'fine'? I know that all of them are somewhat involved, but which one is the key one?</p>
<p>Here's my current understanding of possible roles:</p>
<ul>
<li><p>Masked Causal Self-Attention: Maps relationships within words in the target language (e.g., between 'I' and 'am'), but does it aid in choosing 'fine' as the next word?</p>
</li>
<li><p>Cross-Attention: Finds correlations between source and target language words (e.g., 'I' with 'yo' and 'am' with 'sono'), but is it responsible for producing the next word in the sequence?</p>
</li>
<li><p>Feed-Forward Layers: Execute linear transformations of embeddings without considering the context from other words in the sentence. How could they be responsible for selecting 'fine' following 'am' without context from other embeddings?</p>
</li>
</ul>
<p>In short, what I see in the attention mechanisms is that they operate by adjusting an embedding to align more closely with the embeddings they share the greatest similarity with. But, what I would expect in the cross-attention is actually to see the word 'am' having a high similarity with most likely next work, 'bien'. So the new embedding from 'am' would be mapped into 'fine' which is the English equivalent embedding to 'bien'.</p>
<p>I hope I was able to make my question clear. It is hard for me to explain it. But I appreciate anyone who could give me some directions on the right path. Thanks</p>
","transformer-model"
"77427225","Transformer works perfectly during training while works terrible in inference?","2023-11-05 18:47:15","","0","97","<pytorch><compression><transformer-model>","<p>I am adding three layers of transformer in my architecture and the task is compression. The train validation and loss work nicely during training while during the inference it is terrible and the accuracy drops significantly. I am not sure if it has something to do with overfitting or not but the plot at least doesn't show (if I am not wrong). How could it be possible the transformer works great during training but works terribly during the inference?</p>
<pre><code>class Transformer(nn.Module):
            &quot;&quot;&quot; transformer&quot;&quot;&quot;
            def __init__(self, in_dim):
                super(Transformer, self).__init__()
                self.pooling = ME.MinkowskiAvgPooling(kernel_size=3,stride=3,dimension=3)
                self.linear = ME.MinkowskiLinear(in_dim,in_dim)
                self.relu = ME.MinkowskiReLU(inplace=True)
                self.convmlp = ME.MinkowskiConvolution(in_dim,in_dim,kernel_size=1,stride=1,bias=True,dimension=3)
                
            def forward(self, x):

                x = x + self.relu(self.linear(self.pooling(x)))
                x = x + self.relu(self.convmlp(x))
                return x
</code></pre>
<p><a href=""https://i.sstatic.net/0avHs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0avHs.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"77425669","Where to add pos_embedding and pos_drop for two parallet network separately?","2023-11-05 11:08:45","","1","67","<deep-learning><pytorch><huggingface-transformers><transformer-model><pytorch-lightning>","<p>I have a model, which is extracting features from two different networks (<code>SwinTransformer3D()</code> and <code>MyNetwork(...)</code>) in parallel and then concatenates two obtained features from two networks.</p>

<pre class=""lang-py prettyprint-override""><code>class MyModel(nn.Module):
    def __init__(self):
        super(MyModel, self).__init__()
        self.features1 = SwinTransformer3D(pretrained=None,
                 pretrained2d=False,
                 patch_size=self.patch_size, 
                 in_chans=1,
                 embed_dim=dim,
                 depths=[2, 2, 6, 2],
                 num_heads=[3, 6, 12, 24],
                 window_size=window_size, #(20,7,7),
                 mlp_ratio=4.,
                 qkv_bias=True,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.2,
                 norm_layer=torch.nn.LayerNorm,
                 patch_norm=True,
                 frozen_stages=-1,
                 use_checkpoint=False)
        
        self.features2 = MyNetwork(...)
        self.dropout = nn.Dropout(emb_dropout)
        
        self.fc1 = nn.Linear(self.num_features, self.num_features)
        self.fc2 = nn.Linear(self.num_features, self.num_features)
        
        self.fc_out = nn.Linear(2*self.num_features, self.num_features)
        
    def forward(self, x):
        x = self.to_patch_embedding(x)  #ln 1

        **x = x + pos_embed**  
        **x = self.dropout(x)**

        x1 = self.features1(x)   #ln2
        x1 = x1.view(x1.size(0), -1)
        x1 = F.relu(self.fc1(x1))
        
        x2 = self.features2(x)
        x2 = x2.view(x2.size(0), -1)
        x2 = F.relu(self.fc2(x2))

        # Concatenate in dim1 (feature dimension)
        x = torch.cat((x1, x2), 1)
        x = self.fc_out(x)
        return x
</code></pre>
<p>I have a few questions:</p>
<ol>
<li><p>Since <a href=""https://github.com/haofanwang/video-swin-transformer-pytorch/blob/aa1afa07b159f44e29c1cb43d4a7894bb77583ff/video_swin_transformer.py#L131"" rel=""nofollow noreferrer"">SwinTransformer</a> is computing parameters for the position bias, what is <code>pos_drop</code> is for in <a href=""https://github.com/haofanwang/video-swin-transformer-pytorch/blob/aa1afa07b159f44e29c1cb43d4a7894bb77583ff/video_swin_transformer.py#L531C7-L531C48"" rel=""nofollow noreferrer"">this line</a>?</p>
</li>
<li><p>and the second network <code>MyNetwork(...)</code> has</p>
<pre class=""lang-py prettyprint-override""><code> class MyNetwork(nn.Module):
        def __init__(self):
            super(MyNetwork, self).__init__()
            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            self.pos_drop = nn.Dropout(p=drop_rate)
            ...
        def forward_features(self, x):
            ....
            x = x + self.pos_embed
            x = self.pos_drop(x)
            ....
            return x
</code></pre>
</li>
</ol>
<p>and it is added in its <code>forward_features()</code> method, does not mismatch with the position of the token in <code>SwinTransformer()</code> and should I remove it from this class of network?</p>
<ol start=""3"">
<li><p>Since I have the same input patches (x) for both networks, should I add the position embedding to <code>MyModel</code>, <code>forward()</code> method (between ln1 and ln2)and remove the <code>self.pos_embed</code> and <code>self.pos_drop</code> from <strong>MyNetwork()</strong>  and also remove the</p>
<p>self.pos_drop = nn.Dropout(p=drop_rate)</p>
</li>
</ol>
<p>from <strong>SwinTransformer()</strong>?</p>
<p>How this may affect training?</p>
<p>I would really appreciate if you give your expert opinion on this. where should I add <code>pos_embed</code> and <code>pos_drop</code> when we have a model that is combined from two different models (in parallel) and each extracting two different features?</p>
","transformer-model"
"77414028","How to do the fusion of two parallel branch in an encoder design?","2023-11-03 03:26:25","77421988","1","151","<deep-learning><pytorch><neural-network><huggingface-transformers><transformer-model>","<p>It seems I am not designing my encoder correctly, that is why I need the expert opinion on this since I am beginner to transformers and DL model design.</p>
<p>I have two different types of Transformers networks in an encoder as follows:</p>
<p><a href=""https://i.sstatic.net/Lhu8Q.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Lhu8Q.png"" alt=""enter image description here"" /></a></p>
<p>The embedding dimension of each branch is 256 and they are fused by a Linear layer</p>
<pre><code> self.fusion_head = nn.Linear(2*self.num_features, self.num_features) #self_num_features = 256
</code></pre>
<p>I have a forward feature function in my encoder</p>
<pre><code>def transformer_forward(self,x):
    &quot;&quot;&quot;

    :param x: The embeddings + pos_embed
    :return:
    &quot;&quot;&quot;

    x_t1 = self.transformer_type1(x)  # torch.Size([1, 1280, 256])

    x_t2 = self.transformer_type2.forward(x)  # torch.Size([1, 1280, 256])

    # x = x_t1 + x_t2
    x = torch.cat([x_t1,x_t2],dim=2)

    x = self.fusion_head(x)

    return x
</code></pre>
<p>However, after training the models and loading the checkpoints, I realized that the <code>self.fusion_head</code> is place after <code>transformer_type1</code> modules</p>
<p>.
... 3.0.fn.to_qkv.weight', 'module.encoder.transformer_type1.3.layers.3.0.fn.to_out.0.weight', 'module.encoder.transformer_type1.3.layers.3.0.fn.to_out.0.bias', 'module.encoder.transformer_type1.3.layers.3.1.norm.weight', 'module.encoder.transformer_type1.3.layers.3.1.norm.bias', 'module.encoder.transformer_type1.3.layers.3.1.fn.net.0.weight', 'module.encoder.transformer_type1.3.layers.3.1.fn.net.0.bias', 'module.encoder.transformer_type1.3.layers.3.1.fn.net.3.weight', 'module.encoder.transformer_type1.3.layers.3.1.fn.net.3.bias', 'module.encoder.mlp_head.0.weight', 'module.encoder.mlp_head.0.bias', 'module.encoder.mlp_head.1.weight', 'module.encoder.mlp_head.1.bias', <strong>'module.encoder.fusion_head.weight', 'module.encoder.fusion_head.bias',</strong> 'module.encoder.transformer_type2.pos_embed', 'module.encoder.transformer_type2.patch_embed.proj.weight', 'module.encoder.transformer_type2.patch_embed.proj.bias', 'module.encoder.transformer_type2.patch_embed.norm.weight', 'module.encoder.transformer_type2.patch_embed.norm.bias', 'module.encoder.transformer_type2.blocks.0.norm1.weight', 'module.encoder.transformer_type2.blocks.0.norm1.bias', 'module.encoder.transformer_type2.blocks.0.filter.complex_weight', 'module.encoder.transformer_type2.blocks.0.norm2.weight', 'module.encoder.transformer_type2.blocks.0.norm2.bias', 'module.encoder.transformer_type2.blocks.0.mlp.fc1.weight',  ...</p>
<p>Is the placing of this concatenation layer (i.e., <code>fusion_head</code> correct in the forward function? why it is placed after <code>transformet_type1</code>? Should not <code>fusion_head</code> layer be after both <code>transformet_type1</code> and <code>transformer_type2</code> in terms of order?</p>
","transformer-model"
"77409196","Vision transformer for Image as output","2023-11-02 11:29:42","","0","394","<transformer-model>","<p>Normally vision transformer is used for classification. Can it be used for the image where the output is image? If some repository is there, please share the link.</p>
<p>I want to analyze the architecture last layers which result the image as output but most of the architecture is related to classification.</p>
","transformer-model"
"77408994","Train new Word Embedding for mBART","2023-11-02 10:54:58","","0","78","<python><huggingface-transformers><bert-language-model><transformer-model><word-embedding>","<p><strong>TL;DR:</strong> I want to train a (set of) new word embedding(s) for mBART instead of training it for BERT—how do I do that?</p>
<p><strong>Background:</strong></p>
<p>I found an interesting code here: <a href=""https://github.com/tai314159/PWIBM-Putting-Words-in-Bert-s-Mouth/blob/main/get_pseudowords.py"" rel=""nofollow noreferrer"">https://github.com/tai314159/PWIBM-Putting-Words-in-Bert-s-Mouth/blob/main/get_pseudowords.py</a>. This code uses example sentences to generate so called &quot;pseudoword embeddings&quot;. In this form, the code outputs one pseudoword embedding per example sentence (stored here: <a href=""https://github.com/tai314159/PWIBM-Putting-Words-in-Bert-s-Mouth/blob/main/data/queries/single_target/MaPP_all.txt"" rel=""nofollow noreferrer"">https://github.com/tai314159/PWIBM-Putting-Words-in-Bert-s-Mouth/blob/main/data/queries/single_target/MaPP_all.txt</a>).</p>
<p>After adjusting a few lines of the code, it worked for me and I could add the newly-generated embeddings to a vanilla BERT model. I also extended the code so that it only outputs one pseudoword per &quot;group&quot; of example sentences:</p>
<pre class=""lang-py prettyprint-override""><code># [...]

NEW_TOKEN = '#TOKEN#'

# [...]

class Coercion:

# [...]

    def coercion(self,
                 group,
                 k: int = 5):
        model = BertForMaskedLM.from_pretrained(
            'bert-base-cased', return_dict=True)
        model.to('cuda')

        self.builder.tokenizer.add_tokens(NEW_TOKEN)
        model.resize_token_embeddings(len(self.builder.tokenizer))

        new_queries = []
        queries = []
        vec_targets = []

        # Print targets (and their id's) and the query (and its id)
        for entry in group:
            i = 0
            while True:
                i = i + 1
                if ('target' + str(i)) not in entry.keys():
                    break
                print('target ' + str(i) + ': ' + entry[&quot;target&quot; + str(i)] + &quot; , &quot; + str(entry[&quot;target&quot; + str(i) + &quot;_idx&quot;]))
            print('query:' + entry[&quot;query&quot;] + &quot; , &quot; + str(entry[&quot;query_idx&quot;]))

            # Model output
            nlp = FillMaskPipeline(model, self.builder.tokenizer, device=0)
            output = nlp(entry[&quot;query&quot;])
            output = self._format(output)
            print('[MASK]=' + str(output))

            for j in range(1, i):
                vec_targets.append(
                    self._get_target_embed((entry[&quot;target&quot; + str(j)], entry[&quot;target&quot; + str(j) + &quot;_idx&quot;]), model))

            new_query = entry[&quot;query&quot;].split()
            new_query[entry[&quot;query_idx&quot;]] = NEW_TOKEN
            new_query = ' '.join(new_query)
            query = (new_query, entry[&quot;query_idx&quot;])
            print(query)
            new_queries.append(new_query)
            queries.append(query)

        model = self._freeze(model)

        model.eval()

        for i in range(k):
            print('-' * 40)
            print('Random {a}'.format(a=i))

            # Random initialization, same initialization as huggingface
            weight = model.bert.embeddings.word_embeddings.weight.data[-1]
            nn.init.normal_(weight, mean=0.0,
                            std=model.config.initializer_range)

            # Before training
            print('Before training:')
            nlp = FillMaskPipeline(model, self.builder.tokenizer, device=0)

            model = self._train(model, vec_targets, queries)

            print(&quot;*************************************************************************&quot;)
            # After training
            print('After training:')
            nlp = FillMaskPipeline(model, self.builder.tokenizer, device=0)
            for new_query in set(new_queries):  # only view different queries
                print(&quot;query: &quot; + new_query)
                output = nlp(new_query)
                output = self._format(output)
                print('[MASK]=' + str(output))

                outputs_list.append(output)

                output = self._predict_z(model, query)
                output = self._format(output)
                print(NEW_TOKEN + '=' + str(output))
            print(&quot;*************************************************************************&quot;)

    def _train(self, model, vec_targets, queries):
        loss_fct = nn.MSELoss(reduction='mean')  # mean will be computed later
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.3, eps=1e-8)
        epoch = 1000
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=0,
            num_training_steps=epoch)

        max_length = 1 + max([len(self.builder.encode(query[0])[1]) for query in queries])  # possible padding
        input_ids_and_gather_indexes = [self.builder.encode(query[0], max_length=max_length) for query in queries]
        input_ids = torch.cat([input_id for input_id in [i for i, _ in input_ids_and_gather_indexes]], dim=0).to(&quot;cuda&quot;)
        gather_indexes = [gather_index for gather_index in [g for _, g in input_ids_and_gather_indexes]]

        # target_idx is the index of target word in the token list.
        target_idxs = [g[q[1] + 1][0] for g, q in zip(gather_indexes, queries)]
        target_idxs = torch.tensor(target_idxs, device=&quot;cuda&quot;).unsqueeze(-1)
        # token_idx is the index of target word in the vocabulary of BERT
        token_idxs = input_ids.gather(dim=-1, index=target_idxs)
        vocab_size = len(tokenizer.get_vocab())
        min_token_idx = min(token_idxs)
        indices = torch.tensor([i for i in range(vocab_size) if i &lt; min_token_idx], device=&quot;cuda&quot;, dtype=torch.long)

        for _ in trange(epoch):
            model.zero_grad()
            outputs = model(input_ids, output_hidden_states=True)
            z = torch.index_select(outputs.hidden_states[12][0], dim=0, index=target_idxs.squeeze(-1))

            loss = loss_fct(z, torch.stack(vec_targets))

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            model.bert.embeddings.word_embeddings.weight.grad[indices] = 0
            optimizer.step()
            scheduler.step()

        # get the z* for classification
        vec = model.bert.embeddings.word_embeddings(token_idxs).squeeze(1)[0]  # this is z*; [0] because all the same
        vec_array = vec.cpu().detach().numpy()
        z_list.append(vec_array)
        loss_list.append(str(loss.cpu().detach().numpy()))

        # save checkpoints
        try:
            np.save(CACHE + &quot;temp_z_arrays.npy&quot;, np.array(z_list))
            np.save(CACHE + &quot;temp_loss_arrays.npy&quot;, np.array(loss_list))
        except:
            print(&quot;Skip saving this time...&quot;)

        s = 'Final loss={a}'.format(a=str(loss.cpu().detach().numpy()))
        print(s)

        return model

    def _freeze(self, model):
        # Freeze all the parameters except the word embeddings
        for name, param in model.named_parameters():
            param.requires_grad = False
            if name == 'bert.embeddings.word_embeddings.weight':
                param.requires_grad = True

        # Manually break the connection of decoder and embeddings.
        original_weight = model.cls.predictions.decoder.weight
        original_bias = model.cls.predictions.decoder.bias
        decoder = nn.Linear(768, len(tokenizer) - 1, bias=True)
        decoder.weight.requires_grad = False
        decoder.bias.requires_grad = False
        decoder.weight.data.copy_(original_weight.data[:-1])
        decoder.bias.data.copy_(original_bias.data[:-1])
        model.cls.predictions.decoder = decoder

        return model

# [...]

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
builder = DataBuilder(tokenizer)
co = Coercion(builder)
for group in data:
    co.coercion(group)
    print('==' * 40)

result = get_lowest_loss_arrays(z_list, loss_list)

</code></pre>
<p>This also worked reasonably well, so I could append my new embeddings to a fresh model.</p>
<p><strong>My Approach:</strong></p>
<p>Now, I wanted to do something a bit different. I wanted to use the mBART-50 model (<a href=""https://huggingface.co/docs/transformers/model_doc/mbart"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/mbart</a>) to do a similar thing, but masking phrases of variable lengths with <code>&lt;mask&gt;</code>. The new, temporary <code>#TOKEN#</code> embedding, will still only &quot;mask&quot; one word (like in the code above).</p>
<p>mBART has a few attributes, which are different though, so I am not sure which lines could stay the same and which ones need to change. This is my approach so far (lines I am especially unsure with, are marked with &quot;??&quot;):</p>
<pre class=""lang-py prettyprint-override""><code>import csv
import itertools
from typing import List, Tuple, TextIO

from transformers import AutoTokenizer, MBart50Tokenizer, MBartForConditionalGeneration, Text2TextGenerationPipeline
from transformers import get_linear_schedule_with_warmup
from transformers import AdamW
import torch
import torch.nn as nn
import torch.optim
from tqdm import trange, tqdm
import jsonlines
import json
import numpy as np
import os
import sklearn
from sklearn.metrics.pairwise import cosine_similarity
from scipy import spatial
from sklearn.metrics import mean_squared_error
import pickle

NEW_TOKEN = '#TOKEN#'

Item = Tuple[str, int]
Example = Tuple[Item, Item]

# ARGS
QUERIES_PATH = &quot;../../out/CoMaPP_all.json&quot;  # path to queries
DATASET_PATH = &quot;CoMaPP_Dataset.csv&quot;
DIR_OUT = &quot;../../out/&quot;  # path to dir to save the pseudowords
CACHE = &quot;../../out/cache/&quot;  # path to cach directory

################################################

class DataBuilder:
    def __init__(self, tokenizer: AutoTokenizer):
        self.tokenizer = tokenizer

    def encode(self, text: str, max_length=None):
        tokens = text.split()
        # Build token indices
        _, gather_indexes = self._manual_tokenize(tokens)
        # Tokenization
        if max_length:
            encode_dict = self.tokenizer(
                text, return_attention_mask=True,
                return_token_type_ids=False, return_tensors='pt',
                padding='max_length', max_length=max_length)
        else:
            encode_dict = self.tokenizer(
                text, return_attention_mask=True,
                return_token_type_ids=False, return_tensors='pt')
        input_ids = encode_dict['input_ids']
        return input_ids, gather_indexes

    def _manual_tokenize(self, tokens: List[str]):
        split_tokens = []
        gather_indexes = []
        for token in tokens:
            indexs = []
            for sub_token in self.tokenizer.tokenize(token):
                indexs.append(len(split_tokens))
                split_tokens.append(sub_token)
            gather_indexes.append(indexs)

        gather_indexes = [(min(t), max(t) + 1) for t in gather_indexes]

        # Adjust for CLS and SEP
        indices = [(a + 1, b + 1) for a, b in gather_indexes]
        # Add of CLS and SEP
        indices = [(0, 1)] + indices + [(indices[-1][1], indices[-1][1] + 1)]
        return split_tokens, indices


class Coercion:
    def __init__(self, builder: DataBuilder):
        self.builder = builder

    def coercion(self,
                 group,
                 k: int = 5):
        model = MBartForConditionalGeneration.from_pretrained(&quot;facebook/mbart-large-50&quot;, return_dict=True)
        model.to('cuda')

        self.builder.tokenizer.add_tokens(NEW_TOKEN)
        model.resize_token_embeddings(len(self.builder.tokenizer))

        new_queries = []
        queries = []
        vec_targets = []

        # Print targets (and their id's) and the query (and its id)
        for entry in group:
            i = 0
            while True:
                i = i + 1
                if ('target' + str(i)) not in entry.keys():
                    break
                print(f'target {i}: {entry[&quot;target&quot; + str(i)]}, {entry[&quot;target&quot; + str(i) + &quot;_idx&quot;]}')
            print(f'query: {entry[&quot;query&quot;]}, {entry[&quot;query_idx&quot;]}')

            nlp = Text2TextGenerationPipeline(model=model, tokenizer=self.builder.tokenizer, device=0)
            output = nlp(entry[&quot;query&quot;], max_length=30, num_return_sequences=5, num_beams=100)
            output = self._format(output)
            print(f&quot;output: {output}&quot;)
            # print('&lt;mask&gt; = ' + str(outputs))  # TODO Just show the replaced &lt;mask&gt; token

            for j in range(1, i):
                vec_targets.append(
                    self._get_target_embed((entry[&quot;target&quot; + str(j)], entry[&quot;target&quot; + str(j) + &quot;_idx&quot;]), model)
                )

            new_query = entry[&quot;query&quot;].split()
            new_query[entry[&quot;query_idx&quot;]] = NEW_TOKEN
            new_query = ' '.join(new_query)
            query = (new_query, entry[&quot;query_idx&quot;])
            print(query)
            new_queries.append(new_query)
            queries.append(query)

        model = self._freeze(model)

        model.eval()

        for i in range(k):
            print('-' * 40)
            print('Random {a}'.format(a=i))

            # Random initialization, same initialization as huggingface
            weight = model.get_input_embeddings().weight.data[-1]
            nn.init.normal_(weight, mean=0.0, std=model.config.init_std)

            # Before training
            # print('Before training:')
            # We need a Text2TextGeneration here, because mBart is created for translation, originally.
            # Only this way, there can be multiple predicted words for one &lt;mask&gt;.
            # nlp = Text2TextGenerationPipeline(model=model, tokenizer=self.builder.tokenizer, device=0)

            model = self._train(model, vec_targets, queries)

            print(&quot;*************************************************************************&quot;)
            # After training
            print('After training:')
            nlp = Text2TextGenerationPipeline(model=model, tokenizer=self.builder.tokenizer, device=0)
            for new_query in set(new_queries):  # only view different queries
                print(f&quot;query: {new_query}&quot;)
                output = nlp(new_query, max_length=30, num_return_sequences=5, num_beams=100)  # TODO output looks fishy...
                output = self._format(output)
                print(f'output: {output}')

                outputs_list.append(output)

                output = self._predict_z(model, query)
                output = self._format(output)
                print(f'{NEW_TOKEN} {output}')
            print(&quot;*************************************************************************&quot;)

    def _train(self, model, vec_targets, queries):
        loss_fct = nn.MSELoss(reduction='mean')  # mean will be computed later
        optimizer = torch.optim.AdamW(model.parameters(), lr=0.3, eps=1e-8)
        epoch = 1000
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=0,
            num_training_steps=epoch)

        # This snippet, retrieving the possible padding, does the following:
        #  (a) encode each query's text (first [0]),
        #  (b) get the input_ids (second [0]),
        #  (c) count the input_ids (.shape[-1], because the number of input_ids is stored in the second/last dimension).
        # Then, you can take the max to know how much you should pad the rest.
        max_length = max([(self.builder.encode(query[0])[0]).shape[-1] for query in queries])

        input_ids_and_gather_indexes = [self.builder.encode(query[0], max_length=max_length) for query in queries]
        input_ids = torch.cat([input_id for input_id in [i for i, _ in input_ids_and_gather_indexes]], dim=0).to(&quot;cuda&quot;)
        gather_indexes = [gather_index for gather_index in [g for _, g in input_ids_and_gather_indexes]]

        # target_idx is the index of target word in the token list.
        target_idxs = [g[q[1] + 1][0] for g, q in zip(gather_indexes, queries)]
        target_idxs = torch.tensor(target_idxs, device=&quot;cuda&quot;).unsqueeze(-1)
        # token_idx is the index of target word in the vocabulary of BERT
        token_idxs = input_ids.gather(dim=-1, index=target_idxs)
        vocab_size = len(tokenizer.get_vocab())  # can be checked with tokenizer.get_added_vocab()
        min_token_idx = min(token_idxs)
        # Get all indices smaller than the new token_idx:
        indices = torch.tensor([i for i in range(vocab_size) if i &lt; min_token_idx], device=&quot;cuda&quot;, dtype=torch.long)

        for _ in trange(epoch):
            model.zero_grad()
            outputs = model(input_ids, output_hidden_states=True)
            z = torch.index_select(outputs.decoder_hidden_states[12][0], dim=0, index=target_idxs.squeeze(-1))
            # or:
            # outputs = model(input_ids) ??
            # z = torch.index_select(outputs.encoder_last_hidden_state[0], dim=0, index=target_idxs.squeeze(-1)) ??

            loss = loss_fct(z, torch.stack(vec_targets))

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            # or:
            # model.get_input_embeddings().weight.grad.data[indices] = 0 ??
            # model.model.encoder.embed_positions.weight.data[indices] = 0 ??
            # model.model.encoder.embed_tokens.weight.grad[indices] = 0 ??
            model.model.shared.weight.grad[indices] = 0
            optimizer.step()
            scheduler.step()

            # try to fix the feed-forward bug
            outputs = model(input_ids)
            bert_z = torch.index_select(outputs.encoder_last_hidden_state[0], dim=0, index=target_idxs.squeeze(-1))

        # get the z* for classification
        vec = model.get_input_embeddings()(token_idxs).squeeze(1)[0]  # this is z*; [0] because all the same
        vec_array = vec.cpu().detach().numpy()
        z_list.append(vec_array)
        loss_list.append(str(loss.cpu().detach().numpy()))

        # save checkpoints
        np.save(CACHE + &quot;temp_z_arrays_mbart.npy&quot;, np.array(z_list))
        np.save(CACHE + &quot;temp_loss_arrays_mbart.npy&quot;, np.array(loss_list))

        s = 'Final loss={a}'.format(a=str(loss.cpu().detach().numpy()))
        print(s)

        return model

    def _get_target_embed(self, target, model):
        input_ids, gather_indexes = self.builder.encode(target[0])
        target_idx = gather_indexes[target[1] + 1][0]
        model.eval()
        with torch.no_grad():
            # Find the learning target x
            input_ids = input_ids.to('cuda')
            outputs = model(input_ids)
            # encoder relevant for downstream tasks
            x_target = outputs.encoder_last_hidden_state[0, target_idx]
        return x_target

    def _freeze(self, model):
        for name, param in model.named_parameters():
            if 'model.encoder.embed_positions' in name or 'model.decoder.embed_positions' in name:
                param.requires_grad = True
            elif 'model.shared' in name:
                param.requires_grad = True
            else:
                param.requires_grad = False

        return model

    def _format(self, results):  # new format
        reval = []
        for item in results:
            if &quot;generated_text&quot; in item.keys():
                generated_text = item[&quot;generated_text&quot;]
                reval.append(generated_text)
            else:
                token_str = item['token_str']
                score = item['score']
                s = ':'.join([token_str, str(score)])
                reval.append(s)
        return reval

    def _predict_z(self, model, query):
        input_ids, gather_indexes = self.builder.encode(query[0])
        # target_idx is the index of target word in the token list.
        target_idx = gather_indexes[query[1] + 1][0]
        input_ids = input_ids.to('cuda')
        outputs = model(input_ids)
        with torch.no_grad():
            logits = outputs.logits[0, target_idx, :]
        probs = logits.softmax(dim=0)
        values, predictions = probs.topk(5)
        reval = []
        for v, p in zip(values.tolist(), predictions.tolist()):
            s = {
                'score': v,
                'token_str': self.builder.tokenizer.convert_ids_to_tokens(p)
            }
            reval.append(s)
        return reval


def load_data(path: TextIO) -&gt; List[Example]:
    reval = []
    with jsonlines.open(path) as reader:
        for obj in reader:
            target = (obj['target'], obj['target_idx'])
            query = (obj['query'], obj['query_idx'])
            reval.append((target, query))
    return reval


def get_lowest_loss_arrays(z_list, loss_list):
    z_array = np.array(z_list)
    loss_array = np.array(loss_list)

    loss_list = loss_array.tolist()
    z_list = []  # list of arrays

    loss_list = list(map(float, loss_list))

    # print(z_array)
    for vec in z_array:
        # print(&quot;vec.shape&quot;, vec.shape) #(768,)
        z_list.append(vec)

    # empty lists
    z_temp = []
    loss_temp = []

    # 5 initializations
    r = int(len(loss_list) / 5)

    for i in range(r):
        k = 0
        for j in range(5):
            if k == 0:

                k = loss_list[5 * i + j]
                z = z_list[5 * i + j]
            else:
                if loss_list[5 * i + j] &lt; k:
                    k = loss_list[5 * i + j]
                    z = z_list[5 * i + j]
                else:
                    continue

        z_temp.append(z)
        loss_temp.append(k)

    z_temp_array = np.array(z_temp)

    return z_temp_array


if __name__ == '__main__':

    z_list = []
    z_eps_list = []
    loss_list = []
    outputs_list = []

    with open(QUERIES_PATH) as json_file:
        data = json.load(json_file)

    # Group the dataset into a list of lists where the label of the dictionaries is identical:
    data.sort(key=lambda x: x[&quot;label&quot;])
    data = [list(group) for _, group in itertools.groupby(data, key=lambda x: x[&quot;label&quot;])]

    tokenizer = MBart50Tokenizer.from_pretrained(&quot;facebook/mbart-large-50&quot;, src_lang=&quot;de_DE&quot;, tgt_lang=&quot;de_DE&quot;)
    builder = DataBuilder(tokenizer)
    co = Coercion(builder)
    for group in data:
        co.coercion(group)
        print('==' * 40)

    result = get_lowest_loss_arrays(z_list, loss_list)

    np.save(DIR_OUT + 'pseudowords_comapp.npy', result)
</code></pre>
<p>Here is an example part from the json file <code>CoMaPP_all.json</code> I'm using:</p>
<pre class=""lang-json prettyprint-override""><code>[{&quot;label&quot;: &quot;jetzt1631&quot;, &quot;target1&quot;: &quot;«Ich werde nur ganz wenig feiern , denn jetzt heisst es realistisch bleiben .&quot;, &quot;target1_idx&quot;: 8, &quot;query&quot;: &quot;«Ich werde nur ganz wenig feiern , denn jetzt heisst es &lt;mask&gt; .&quot;, &quot;query_idx&quot;: 8}, {&quot;label&quot;: &quot;heisst1631&quot;, &quot;target1&quot;: &quot;«Ich werde nur ganz wenig feiern , denn jetzt heisst es realistisch bleiben .&quot;, &quot;target1_idx&quot;: 9, &quot;query&quot;: &quot;«Ich werde nur ganz wenig feiern , denn jetzt heisst es &lt;mask&gt; .&quot;, &quot;query_idx&quot;: 9}, {&quot;label&quot;: &quot;es1631&quot;, &quot;target1&quot;: &quot;«Ich werde nur ganz wenig feiern , denn jetzt heisst es realistisch bleiben .&quot;, &quot;target1_idx&quot;: 10, &quot;query&quot;: &quot;«Ich werde nur ganz wenig feiern , denn jetzt heisst es &lt;mask&gt; .&quot;, &quot;query_idx&quot;: 10}, {&quot;label&quot;: &quot;jetzt1631&quot;, &quot;target1&quot;: &quot;Aber jetzt heisst es vorwärtsschauen .&quot;, &quot;target1_idx&quot;: 1, &quot;query&quot;: &quot;Aber jetzt heisst es &lt;mask&gt; .&quot;, &quot;query_idx&quot;: 1}, {&quot;label&quot;: &quot;heisst1631&quot;, &quot;target1&quot;: &quot;Aber jetzt heisst es vorwärtsschauen .&quot;, &quot;target1_idx&quot;: 2, &quot;query&quot;: &quot;Aber jetzt heisst es &lt;mask&gt; .&quot;, &quot;query_idx&quot;: 2}, {&quot;label&quot;: &quot;es1631&quot;, &quot;target1&quot;: &quot;Aber jetzt heisst es vorwärtsschauen .&quot;, &quot;target1_idx&quot;: 3, &quot;query&quot;: &quot;Aber jetzt heisst es &lt;mask&gt; .&quot;, &quot;query_idx&quot;: 3}]
</code></pre>
<p><code>CoMaPP_Dataset.csv</code> looks as follows:</p>
<pre><code>label,query,mask,ambigous_word
jetzt1631,&quot;«Ich werde nur ganz wenig feiern , denn jetzt heisst es realistisch bleiben .&quot;,realistisch bleiben,jetzt
heisst1631,&quot;«Ich werde nur ganz wenig feiern , denn jetzt heisst es realistisch bleiben .&quot;,realistisch bleiben,heisst
es1631,&quot;«Ich werde nur ganz wenig feiern , denn jetzt heisst es realistisch bleiben .&quot;,realistisch bleiben,es
jetzt1631,Aber jetzt heisst es vorwärtsschauen .,vorwärtsschauen,jetzt
heisst1631,Aber jetzt heisst es vorwärtsschauen .,vorwärtsschauen,heisst
es1631,Aber jetzt heisst es vorwärtsschauen .,vorwärtsschauen,es
</code></pre>
<p>I have the following problem after training: As soon as I test a <code>new_query</code> in the loop <code>for new_query in set(new_queries):</code>, I get outputs like this:</p>
<pre><code>new_query: '#TOKEN# für &lt;mask&gt; !&quot;&quot; , schrieb der Milliardär am Mittwochmorgen ( Ortszeit ) in dem Kurzbotschaftendienst .'
output: [{'generated_text': '#TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN#'}, {'generated_text': '#TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN#'}, {'generated_text': 'na #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN#'}, {'generated_text': 'con #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN#'}, {'generated_text': 'Con #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN# #TOKEN#'}]
</code></pre>
<p>Using the debugger, I found that the probability of #TOKEN# == 1.0; whereas the other tokens' probabilities are 0.0.</p>
<p>What did I do wrong? I know that BERT and (m)BART are different architectures, but mBART also uses a Masked Language Model as part of its architecture, so I think it should be possible to do something similar to what has been done with BERT in the original code.</p>
","transformer-model"
"77401391","Sequence to sequence prediction transformer : need for normalization","2023-11-01 08:53:22","","0","45","<deep-learning><sequence><normalization><transformer-model>","<p>I am trying to create a transformer model for sequence to sequence prediction using structured data.Do we need to normalize the input data? As a standard transformer architecture already has a layer normalization implemented already.</p>
<p>If we need normalization,how should it be normalized? Only the input to the encoder or the decoder input also needs to be normalized ? But, the decoder input is essentially the output we will be generating.How can I normalize ( local normalization) each sequence with their mean and std ?</p>
","transformer-model"
"77389993","transformer model for Language Modelling in NLP","2023-10-30 15:35:27","","0","55","<pytorch><nlp><transformer-model><large-language-model><simpletransformers>","<p><strong>Purpose of the model</strong></p>
<p>The purpose of the model is to build a small scale LLM (No need to be that better as other LLMs) from scratch to understand the concepts of coding an LLM.</p>
<p><strong>Expected Working</strong></p>
<p>The model is just expected to produce meaningful generations (due to resource constraints).</p>
<p><strong>Problem</strong></p>
<p>The problem with this current model is that it was not able to achieve its task. The generations were not at all meaningful either to the <code>input</code> or to the <code>context</code> and I was not able to understand the actual thing that is causing the problem.</p>
<p><strong>Model</strong></p>
<p>My Model - <a href=""https://www.kaggle.com/code/anirudhmukkamala/llmwithtransformer"" rel=""nofollow noreferrer"">llmwithtransformer</a></p>
<p><strong>My tries</strong></p>
<p>My first mistake was that I have used the default adam optimizer and crossEntropy loss directly. Since transformers require some modifications in it so I have changed it again  with the help of GPT and the available resources from PyTorch and Tensorflow. Although it had made some significant improvement (in generating some random texts which can become a meaningful text with its meaning not at all aligning with the input or context) but still the text is not relational to the input.
I was struck here.</p>
<p><strong>Expecting</strong></p>
<ul>
<li>The points where I have made mistakes with the corrections (not necessarily the code).</li>
<li>Improvement Suggestions</li>
</ul>
","transformer-model"
"77386286","Custom attention function slow when training","2023-10-30 04:58:04","","1","115","<python><optimization><pytorch><transformer-model><attention-model>","<p>I've been trying to implement a custom attention function in a standard gpt-2 style transformer model. I replaced the scaled dot product with negative Euclidean distance and everything seems to be working except that training is extremely slow. With normal dot product attention the model trains in a few minutes. With my implementation it seems like it will take at least a day to train. The dataset I'm using is about 30mb taken from the Pile. I'm not super familiar with Pytorch so I don't know if my implementation is as efficient as it could be.</p>
<p>Below is my attempt at a custom attention function.</p>
<pre><code>def CustomAttention(A: Float[Tensor, &quot;batch posn_q n_heads d_head&quot;], 
                    B: Float[Tensor, &quot;batch posn_k n_heads d_head&quot;]) -&gt; Float[Tensor, &quot;batch n_heads posn_q posn_k&quot;]:
    A_cast = t.permute(A, (0, 2, 1, 3)).unsqueeze(-2)
    B_cast = t.permute(B, (0, 2, 1, 3)).unsqueeze(-3)
    diff = A_cast - B_cast
    square = diff**2
    sum = t.sum(square, dim=-1)
    return -sum
</code></pre>
<p>Basically, the code relies on broadcasting to calculate the elementwise difference for every query-key pair. All of my training is being done locally on a 3080ti and it seems to be using 100% of my gpu as expected. Is there anything I can do to make this run faster?</p>
","transformer-model"
"77382562","Temporal Fusion Transformer model training encountered Gradient Vanishing","2023-10-29 08:40:47","","1","194","<deep-learning><pytorch><transformer-model><seq2seq><multihead-attention>","<p>I am training financial data with Temporal Fusion Transformer. Though this model has skipping connection and residual connection to enhance information. I believe it encountered <strong>gradient vanishing</strong> at least at final output layer. The Temporal Fusion Transformer implementation is inspired from open source, including pytorch-forecasting. I customize it myself for the ease of use. I checked the model multiple times, not finding technical problem so far. Here are some details. I am using MSELoss in place of QuantileLoss to avoid issues at loss function. Can someone help me figuring out what's causing gradient vanishing in terms of training, modelling or the like?</p>
<pre><code># config.py
NUM_GPU = torch.cuda.device_count()
DEVICE = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
BATCH_SIZE = 128
NUM_EPOCHES = 20
NUM_WORKER = 2

# model param
DROPOUT = 0.2
LEARNING_RATE = 0.01
ENCODER_STEPS = 120
DECODER_STEPS = 24+120   # predicting future 24 obs
HIDDEN_LAYER_SIZE = 6
EMBEDDING_DIMENSION = 4
NUM_LSTM_LAYERS = 3
NUM_ATTENTION_HEADS = 2

# train model
criterion = MSELoss(QUANTILES)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

print_every_k = 100
losses = []

for epoch in range(NUM_EPOCHES):
    if epoch==NUM_EPOCHES-1:
        print('debug')

    model.train()
    t0 = time.time()
    logger.debug(f&quot;===== Epoch {epoch+1} =========&quot;)
    train_epoch_loss = 0.0
    train_running_loss = 0.0

    for i, batch in enumerate(train_dataloader):
        labels = batch['outputs'][:,:,0].float().to(DEVICE)

        # Zero the parameter gradients
        optimizer.zero_grad()

        outputs, attention_weights = model(batch)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # Print statistics
        train_running_loss += loss.item()*(labels.shape[0]*labels.shape[1])
        train_epoch_loss += loss.item()*(labels.shape[0]*labels.shape[1])

        if (i+1) % print_every_k == 0:
            logger.debug(f&quot;Mini-batch {i+1} average loss: {round(train_running_loss/len(labels)/(DECODER_STEPS-ENCODER_STEPS)/print_every_k, 5)}&quot;)
            train_running_loss = 0.0

    for name, param in model.named_parameters():
        layer_name = name.split('.')[0]
        others_names = '.'.join(name.split('.')[1:])
        if param.grad is not None:
            layer_name = name.split('.')[0]
            others_names = '.'.join(name.split('.')[1:])
            if layer_name == 'output':
                logger.debug(f'{others_names}: {param.grad}')
</code></pre>
<p>Some of the print is as follows:</p>
<pre><code>| INFO     | __main__:tft_pipe:56 - 18533 model params, 139588 samples.
| DEBUG    | __main__:tft_pipe:79 - ===== Epoch 1 =========
2023-10-28 23:25:55.885 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-0.1970, -0.1778, -0.0316, -0.1864, -0.2094, -0.1853]],
       device='cuda:0')
2023-10-28 23:25:55.891 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.2512], device='cuda:0')
2023-10-28 23:25:55.893 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 184.34 seconds
2023-10-28 23:25:55.894 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00959
2023-10-28 23:26:18.379 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00603
2023-10-28 23:26:18.384 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 2 =========
2023-10-28 23:29:26.948 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[0.3031, 0.2802, 0.0672, 0.1487, 0.1613, 0.1379]], device='cuda:0')
2023-10-28 23:29:26.953 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.1593], device='cuda:0')
2023-10-28 23:29:26.955 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 188.57 seconds
2023-10-28 23:29:26.957 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00492
2023-10-28 23:29:46.950 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00484
2023-10-28 23:29:46.954 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 3 =========
2023-10-28 23:32:57.256 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[ 0.1013,  0.1008,  0.0258, -0.0917, -0.0930, -0.0923]],
       device='cuda:0')
2023-10-28 23:32:57.267 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.5331], device='cuda:0')
2023-10-28 23:32:57.269 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 190.31 seconds
2023-10-28 23:32:57.271 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00474
2023-10-28 23:33:18.197 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00524
2023-10-28 23:33:18.203 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 4 =========
2023-10-28 23:36:28.117 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-0.3200, -0.2964,  0.0014, -0.2076, -0.2188, -0.1801]],
       device='cuda:0')
2023-10-28 23:36:28.124 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([-0.0208], device='cuda:0')
2023-10-28 23:36:28.127 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 189.92 seconds
2023-10-28 23:36:28.129 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.0047
2023-10-28 23:36:48.394 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00477
2023-10-28 23:36:48.399 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 5 =========
2023-10-28 23:39:58.502 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[ 0.1297,  0.1257,  0.0035, -0.0225, -0.0231, -0.0268]],
       device='cuda:0')
2023-10-28 23:39:58.513 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.3566], device='cuda:0')
2023-10-28 23:39:58.515 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 190.12 seconds
2023-10-28 23:39:58.518 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00464
2023-10-28 23:40:18.631 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00512
2023-10-28 23:40:18.636 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 6 =========
2023-10-28 23:43:29.830 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[ 0.0386,  0.0408, -0.0006, -0.0571, -0.0618, -0.0497]],
       device='cuda:0')
2023-10-28 23:43:29.840 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.2770], device='cuda:0')
2023-10-28 23:43:29.843 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 191.21 seconds
2023-10-28 23:43:29.845 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00456
2023-10-28 23:43:50.012 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00599
2023-10-28 23:43:50.017 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 7 =========
2023-10-28 23:47:01.444 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-0.0039, -0.0085,  0.0006,  0.0589,  0.0636,  0.0435]],
       device='cuda:0')
2023-10-28 23:47:01.450 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([-0.2083], device='cuda:0')
2023-10-28 23:47:01.452 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 191.44 seconds
2023-10-28 23:47:01.454 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00447
2023-10-28 23:47:21.836 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00467
2023-10-28 23:47:21.843 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 8 =========
2023-10-28 23:50:32.598 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-3.0020e-02, -2.9020e-02, -1.3130e-05, -1.6233e-02, -1.6929e-02,
         -9.7950e-03]], device='cuda:0')
2023-10-28 23:50:32.606 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([-0.0196], device='cuda:0')
2023-10-28 23:50:32.608 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 190.77 seconds
2023-10-28 23:50:32.610 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00444
2023-10-28 23:50:53.131 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00469
2023-10-28 23:50:53.137 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 9 =========
2023-10-28 23:54:03.649 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[3.5368e-02, 3.3262e-02, 5.8795e-06, 4.1231e-02, 4.7169e-02, 1.7816e-02]],
       device='cuda:0')
2023-10-28 23:54:03.654 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([-0.0711], device='cuda:0')
2023-10-28 23:54:03.656 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 190.52 seconds
2023-10-28 23:54:03.658 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00439
2023-10-28 23:54:23.823 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00494
2023-10-28 23:54:23.828 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 10 =========
2023-10-28 23:57:36.051 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-0.0286, -0.0357, -0.0002,  0.0101,  0.0245,  0.0015]],
       device='cuda:0')
2023-10-28 23:57:36.059 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([-0.2316], device='cuda:0')
2023-10-28 23:57:36.062 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 192.23 seconds
2023-10-28 23:57:36.063 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00436
2023-10-28 23:57:56.124 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00451
2023-10-28 23:57:56.129 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 11 =========
2023-10-29 00:11:24.993 | DEBUG    | __main__:tft_pipe:115 - Mini-batch 1000 average loss: 0.00425
2023-10-29 00:11:43.654 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-1.1735e-03, -2.6554e-03,  4.9117e-07, -1.3693e-04,  2.1181e-03,
         -1.3661e-03]], device='cuda:0')
2023-10-29 00:11:43.662 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([-0.1520], device='cuda:0')
2023-10-29 00:11:43.664 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 190.38 seconds
2023-10-29 00:11:43.666 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00427
2023-10-29 00:12:03.697 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00474
2023-10-29 00:00:01.309 | DEBUG    | __main__:tft_pipe:115 - Mini-batch 700 average loss: 0.00436
2023-10-29 00:00:17.189 | DEBUG    | __main__:tft_pipe:115 - Mini-batch 800 average loss: 0.00441
2023-10-29 00:00:33.102 | DEBUG    | __main__:tft_pipe:115 - Mini-batch 900 average loss: 0.00437
2023-10-29 00:00:49.106 | DEBUG    | __main__:tft_pipe:115 - Mini-batch 1000 average loss: 0.00433
2023-10-29 00:01:07.808 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[ 3.8436e-03,  9.6467e-03,  6.3097e-05, -3.9691e-03, -1.3984e-02,
         -8.0875e-05]], device='cuda:0')
2023-10-29 00:01:07.818 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.1691], device='cuda:0')
2023-10-29 00:01:07.820 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 191.69 seconds
2023-10-29 00:01:07.825 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00433
2023-10-29 00:01:27.898 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00452
2023-10-29 00:01:27.902 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 12 =========
2023-10-29 00:04:39.496 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-2.6736e-03, -3.2901e-04, -2.3421e-03, -8.6103e-05, -7.5090e-03,
         -9.9114e-04]], device='cuda:0')
2023-10-29 00:04:39.502 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.3272], device='cuda:0')
2023-10-29 00:04:39.504 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 191.6 seconds
2023-10-29 00:04:39.505 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00429
2023-10-29 00:05:00.339 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.0053
2023-10-29 00:05:00.343 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 13 =========
2023-10-29 00:08:12.582 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[ 3.5385e-03, -5.6710e-03, -1.4565e-07,  5.9994e-04,  1.7111e-02,
          5.9326e-03]], device='cuda:0')
2023-10-29 00:08:12.593 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.1814], device='cuda:0')
2023-10-29 00:08:12.595 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 192.25 seconds
2023-10-29 00:08:12.596 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00428
2023-10-29 00:08:33.280 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00456
2023-10-29 00:08:33.286 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 14 =========
2023-10-29 00:11:43.654 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-1.1735e-03, -2.6554e-03,  4.9117e-07, -1.3693e-04,  2.1181e-03,
         -1.3661e-03]], device='cuda:0')
2023-10-29 00:11:43.662 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([-0.1520], device='cuda:0')
2023-10-29 00:11:43.664 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 190.38 seconds
2023-10-29 00:11:43.666 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00427
2023-10-29 00:12:03.697 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00474
2023-10-29 00:12:03.701 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 15 =========
2023-10-29 00:15:15.817 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-1.0530e-04, -2.1193e-04, -1.6541e-05, -2.8272e-04, -3.5081e-03,
         -4.4950e-06]], device='cuda:0')
2023-10-29 00:15:15.823 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.1189], device='cuda:0')
2023-10-29 00:15:15.825 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 192.12 seconds
2023-10-29 00:15:15.827 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00427
2023-10-29 00:15:35.848 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00449
2023-10-29 00:15:35.852 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 16 =========
2023-10-29 00:18:47.608 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-5.6954e-04, -7.8295e-04,  8.9046e-06, -2.4048e-05,  4.6034e-03,
         -1.6678e-04]], device='cuda:0')
2023-10-29 00:18:47.621 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.2328], device='cuda:0')
2023-10-29 00:18:47.624 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 191.77 seconds
2023-10-29 00:18:47.626 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00427
2023-10-29 00:19:08.119 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.0045
2023-10-29 00:19:08.123 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 17 =========
2023-10-29 00:22:19.299 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-1.1427e-05,  5.2137e-04,  3.7489e-06,  2.4374e-04,  1.9881e-03,
         -3.9823e-05]], device='cuda:0')
2023-10-29 00:22:19.304 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([-0.2279], device='cuda:0')
2023-10-29 00:22:19.306 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 191.18 seconds
2023-10-29 00:22:19.307 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00425
2023-10-29 00:22:39.591 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00461
2023-10-29 00:22:39.596 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 18 =========
2023-10-29 00:25:52.057 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-5.8683e-05, -8.8668e-04, -5.4194e-06, -6.6186e-05,  1.6081e-03,
          8.8617e-05]], device='cuda:0')
2023-10-29 00:25:52.065 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.1201], device='cuda:0')
2023-10-29 00:25:52.066 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 192.47 seconds
2023-10-29 00:25:52.067 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00425
2023-10-29 00:26:13.248 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00453
2023-10-29 00:26:13.254 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 19 =========
2023-10-29 00:29:23.420 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-4.8534e-04, -2.9827e-04, -4.2051e-06, -2.5660e-04, -6.4457e-03,
         -5.5604e-05]], device='cuda:0')
2023-10-29 00:29:23.428 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.1287], device='cuda:0')
2023-10-29 00:29:23.431 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 190.18 seconds
2023-10-29 00:29:23.433 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00423
2023-10-29 00:29:43.669 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00479
2023-10-29 00:29:43.676 | DEBUG    | __main__:tft_pipe:79 - ===== Epoch 20 =========
2023-10-29 00:32:56.241 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-4.9815e-05, -7.2576e-05,  1.1909e-06, -3.8043e-05, -2.6047e-03,
         -5.8518e-07]], device='cuda:0')
2023-10-29 00:32:56.247 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.1311], device='cuda:0')
2023-10-29 00:32:56.249 | DEBUG    | __main__:tft_pipe:141 - Epoch trained for 192.57 seconds
2023-10-29 00:32:56.251 | DEBUG    | __main__:tft_pipe:142 - Epoch Train loss: 0.00423
2023-10-29 00:33:16.435 | DEBUG    | __main__:tft_pipe:163 - Epoch Val loss: 0.00453
</code></pre>
<p><strong>Updates</strong>:
I could reach a better-normed output gradient with learning_rate=0.0001, which looks like this:</p>
<pre><code>2023-10-30 22:23:57.374 | DEBUG    | __main__:tft_pipe:132 - module.weight: tensor([[-0.1125, -0.1421, -0.1363,  0.1359, -0.1395, -0.1524, -0.1319, 
 0.1499,
         -0.1268, -0.1586]], device='cuda:0')
2023-10-30 22:23:57.381 | DEBUG    | __main__:tft_pipe:132 - module.bias: tensor([0.0319], device='cuda:0')
</code></pre>
<p>Even so, it doesn't resolve the final problem, underfitting. From the loss, one can see that there is almost no reduction upon Epoch 1. What's the more, the prediction result looks no difference from a straight line, meaning the prediction magnitude is far less than the real one. If the cause is not from gradient vanishing, what contributes to such underfitting?</p>
<p>Please note that the raw data is normalized so the scale in figure and scale in training doesn't match, but it doesn't change the underfitting. I can also assure that it is not the case of insufficient factors, as I've tried on synthetic data with future data computed from the past information. Such flat prediction persists. The grey line is attention score, from which style I learned from <em>pytorch-forecasting</em> package.<a href=""https://i.sstatic.net/pyoom.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pyoom.png"" alt=""One prediction result"" /></a></p>
<p><strong>Updates</strong>:Noted that gradient vanishing may not happen on output layer, and that the flat line prediction looks so much like a zero-feature OLS, which could only predict mean, I looked into norm of all layers, gradients of weights and bias. It seems that it is the other layer have norm of magnitude e-5 or less. Could it be gradient vanishing on other layers. What could've been the cause of that?</p>
","transformer-model"
"77373522","How does an instance of pytorch's `nn.Linear()` process a tuple of tensors?","2023-10-27 10:43:27","77373814","0","118","<python><machine-learning><pytorch><nlp><transformer-model>","<p>In the <a href=""http://nlp.seas.harvard.edu/annotated-transformer/"" rel=""nofollow noreferrer"">annotated transformer's implementation of multi-head attention</a>, three tensors (query, key, value) are all passed to a <code>nn.Linear(d_model, d_model)</code>:</p>
<pre><code># some class definition ...
self.linears = clones(nn.Linear(d_model, d_model), 4) # deep-copied list of nn.Linear-modules concatenated via nn.ModuleList
# more code ...
query, key, value = [
  lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)
  for lin, x in zip(self.linears, (query, key, value))
]
</code></pre>
<p>My question: what happens at <code>lin(x)</code>, when an instance of <code>nn.Linear()</code> is called on the tuple <code>(query, key, value)</code>? Is the tuple somehow concatenated to a tensor? If so, how - on which dimension are the tensors concatenated?</p>
","transformer-model"
"77368697","How to get padding mask for cross attention of decoder of transformer","2023-10-26 16:13:51","","0","246","<transformer-model><attention-model><encoder-decoder>","<p>Is the padding mask for cross attention of the decoder of the transformer architecture only derived from the shape of the input tensors from the encoder, so the padding mask is found using this function:</p>
<pre><code>def make_source_mask(source_ids, source_pad_id):
    return (source_ids != source_pad_id).unsqueeze(-2)
</code></pre>
<p>where source_ids is the encoder outputs into the decoder</p>
<p>so here is my complete code for the cross attention module:</p>
<pre><code>class CrossMultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super(CrossMultiHeadAttention, self).__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.kv_layer = nn.Linear(d_model, 2*d_model)
        self.q_layer = nn.Linear(d_model, d_model)
        self.linear_layer = nn.Linear(d_model, d_model)
    def forward(self, x, y, x_source_shape, PADDING_TOKEN):
        batch_size, max_sequence_length, d_model= x.size()
        kv = self.kv_layer(x)
        q = self.q_layer(y)
        kv = kv.reshape(batch_size, max_sequence_length, self.num_heads, self.head_dim*2)
        q = q.reshape(batch_size, max_sequence_length, self.num_heads, self.head_dim)
        kv = kv.permute(0, 2, 1, 3)
        q = q.permute(0, 2, 1, 3)
        k, v = kv.chunk(2, dim=-1)
        values, attention = scaled_dot_product(q, k, v, mask=make_source_mask(x_source_shape, PADDING_TOKEN))
        values = values.permute(0, 2, 1, 3).reshape(batch_size, max_sequence_length, self.num_heads*self.head_dim)
        out = self.linear_layer(values)
        return out
</code></pre>
<p>is the source_mask found wrongly?</p>
","transformer-model"
"77365349","How to save DETR bounding box label and probability?","2023-10-26 08:31:38","","0","98","<deep-learning><resultset><transformer-model><bounding-box>","<p>I have trained on a custom dataset DETR (Detection Transformer) algorithm (Resnet 101 based) thanks to this following Github <a href=""https://github.com/thedeepreader/detr_tutorial"" rel=""nofollow noreferrer"">repo</a>.</p>
<p>The “test.py” file allows to design the output format and by default it consists to save tested images with only detected bounding box display. I would like to modify this script in order to obtain an images and text/JSON file where for each tested image I could obtain/see bounding box, label and confidence (proba).</p>
<p>Here is the initial “Test.py” script :</p>
<pre><code># Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
&quot;&quot;&quot;
Train and eval functions used in main.py
&quot;&quot;&quot;
import math
import os
import cv2
import sys
import argparse
from pathlib import Path
from typing import Iterable
from PIL import Image
import numpy as np

import torch

import util.misc as utils

from models import build_model
from datasets.face import make_face_transforms

import matplotlib.pyplot as plt
import time


def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(1)
    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
         (x_c + 0.5 * w), (y_c + 0.5 * h)]
    return torch.stack(b, dim=1)

def rescale_bboxes(out_bbox, size):
    img_w, img_h = size
    b = box_cxcywh_to_xyxy(out_bbox)
    b = b * torch.tensor([img_w, img_h,
                          img_w, img_h
                          ], dtype=torch.float32)
    return b

def get_images(in_path):
    img_files = []
    for (dirpath, dirnames, filenames) in os.walk(in_path):
        for file in filenames:
            filename, ext = os.path.splitext(file)
            ext = str.lower(ext)
            if ext == '.jpg' or ext == '.jpeg' or ext == '.gif' or ext == '.png' or ext == '.pgm':
                img_files.append(os.path.join(dirpath, file))

    return img_files


def get_args_parser():
    parser = argparse.ArgumentParser('Set transformer detector', add_help=False)
    parser.add_argument('--lr', default=1e-4, type=float)
    parser.add_argument('--lr_backbone', default=1e-5, type=float)
    parser.add_argument('--batch_size', default=6, type=int)
    parser.add_argument('--weight_decay', default=1e-4, type=float)
    parser.add_argument('--epochs', default=300, type=int)
    parser.add_argument('--lr_drop', default=200, type=int)
    parser.add_argument('--clip_max_norm', default=0.1, type=float,
                        help='gradient clipping max norm')

    # Model parameters
    parser.add_argument('--frozen_weights', type=str, default=None,
                        help=&quot;Path to the pretrained model. If set, only the mask head will be trained&quot;)
    # * Backbone
    parser.add_argument('--backbone', default='resnet50', type=str,
                        help=&quot;Name of the convolutional backbone to use&quot;)
    parser.add_argument('--dilation', action='store_true',
                        help=&quot;If true, we replace stride with dilation in the last convolutional block (DC5)&quot;)
    parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'),
                        help=&quot;Type of positional embedding to use on top of the image features&quot;)

    # * Transformer
    parser.add_argument('--enc_layers', default=6, type=int,
                        help=&quot;Number of encoding layers in the transformer&quot;)
    parser.add_argument('--dec_layers', default=6, type=int,
                        help=&quot;Number of decoding layers in the transformer&quot;)
    parser.add_argument('--dim_feedforward', default=2048, type=int,
                        help=&quot;Intermediate size of the feedforward layers in the transformer blocks&quot;)
    parser.add_argument('--hidden_dim', default=256, type=int,
                        help=&quot;Size of the embeddings (dimension of the transformer)&quot;)
    parser.add_argument('--dropout', default=0.1, type=float,
                        help=&quot;Dropout applied in the transformer&quot;)
    parser.add_argument('--nheads', default=8, type=int,
                        help=&quot;Number of attention heads inside the transformer's attentions&quot;)
    parser.add_argument('--num_queries', default=10, type=int,
                        help=&quot;Number of query slots&quot;)
    parser.add_argument('--pre_norm', action='store_true')

    # * Segmentation
    parser.add_argument('--masks', action='store_true',
                        help=&quot;Train segmentation head if the flag is provided&quot;)

    # # Loss
    parser.add_argument('--no_aux_loss', dest='aux_loss', action='store_false',
                        help=&quot;Disables auxiliary decoding losses (loss at each layer)&quot;)
    # * Matcher
    parser.add_argument('--set_cost_class', default=1, type=float,
                        help=&quot;Class coefficient in the matching cost&quot;)
    parser.add_argument('--set_cost_bbox', default=5, type=float,
                        help=&quot;L1 box coefficient in the matching cost&quot;)
    parser.add_argument('--set_cost_giou', default=2, type=float,
                        help=&quot;giou box coefficient in the matching cost&quot;)
    # * Loss coefficients
    parser.add_argument('--mask_loss_coef', default=1, type=float)
    parser.add_argument('--dice_loss_coef', default=1, type=float)
    parser.add_argument('--bbox_loss_coef', default=5, type=float)
    parser.add_argument('--giou_loss_coef', default=2, type=float)
    parser.add_argument('--eos_coef', default=0.1, type=float,
                        help=&quot;Relative classification weight of the no-object class&quot;)

    # dataset parameters
    parser.add_argument('--dataset_file', default='face')
    parser.add_argument('--data_path', type=str)
    parser.add_argument('--data_panoptic_path', type=str)
    parser.add_argument('--remove_difficult', action='store_true')

    parser.add_argument('--output_dir', default='',
                        help='path where to save the results, empty for no saving')
    parser.add_argument('--device', default='cuda',
                        help='device to use for training / testing')
    parser.add_argument('--resume', default='', help='resume from checkpoint')

    parser.add_argument('--thresh', default=0.5, type=float)

    return parser


@torch.no_grad()
def infer(images_path, model, postprocessors, device, output_path):
    model.eval()
    duration = 0
    for img_sample in images_path:
        filename = os.path.basename(img_sample)
        print(&quot;processing...{}&quot;.format(filename))
        orig_image = Image.open(img_sample)
        w, h = orig_image.size
        transform = make_face_transforms(&quot;val&quot;)
        dummy_target = {
            &quot;size&quot;: torch.as_tensor([int(h), int(w)]),
            &quot;orig_size&quot;: torch.as_tensor([int(h), int(w)])
        }
        image, targets = transform(orig_image, dummy_target)
        image = image.unsqueeze(0)
        image = image.to(device)


        conv_features, enc_attn_weights, dec_attn_weights = [], [], []
        hooks = [
            model.backbone[-2].register_forward_hook(
                        lambda self, input, output: conv_features.append(output)

            ),
            model.transformer.encoder.layers[-1].self_attn.register_forward_hook(
                        lambda self, input, output: enc_attn_weights.append(output[1])

            ),
            model.transformer.decoder.layers[-1].multihead_attn.register_forward_hook(
                        lambda self, input, output: dec_attn_weights.append(output[1])

            ),

        ]

        start_t = time.perf_counter()
        outputs = model(image)
        end_t = time.perf_counter()

        outputs[&quot;pred_logits&quot;] = outputs[&quot;pred_logits&quot;].cpu()
        outputs[&quot;pred_boxes&quot;] = outputs[&quot;pred_boxes&quot;].cpu()

        probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]
        # keep = probas.max(-1).values &gt; 0.85
        keep = probas.max(-1).values &gt; args.thresh

        bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], orig_image.size)
        probas = probas[keep].cpu().data.numpy()

        for hook in hooks:
            hook.remove()

        conv_features = conv_features[0]
        enc_attn_weights = enc_attn_weights[0]
        dec_attn_weights = dec_attn_weights[0].cpu()

        # get the feature map shape
        h, w = conv_features['0'].tensors.shape[-2:]

        if len(bboxes_scaled) == 0:
            continue

        img = np.array(orig_image)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        for idx, box in enumerate(bboxes_scaled):
            bbox = box.cpu().data.numpy()
            bbox = bbox.astype(np.int32)
            bbox = np.array([
                [bbox[0], bbox[1]],
                [bbox[2], bbox[1]],
                [bbox[2], bbox[3]],
                [bbox[0], bbox[3]],
                ])
            bbox = bbox.reshape((4, 2))
            cv2.polylines(img, [bbox], True, (0, 255, 0), 2)

        # img_save_path = os.path.join(output_path, filename)
        # cv2.imwrite(img_save_path, img)
        cv2.imshow(&quot;img&quot;, img)
        cv2.waitKey()
        infer_time = end_t - start_t
        duration += infer_time
        print(&quot;Processing...{} ({:.3f}s)&quot;.format(filename, infer_time))

    avg_duration = duration / len(images_path)
    print(&quot;Avg. Time: {:.3f}s&quot;.format(avg_duration))


if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser('DETR training and evaluation script', parents=[get_args_parser()])
    args = parser.parse_args()
    if args.output_dir:
        Path(args.output_dir).mkdir(parents=True, exist_ok=True)

    device = torch.device(args.device)

    model, _, postprocessors = build_model(args)
    if args.resume:
        checkpoint = torch.load(args.resume, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
    model.to(device)
    image_paths = get_images(args.data_path)

    infer(image_paths, model, postprocessors, device, args.output_dir)
</code></pre>
<p>For the image display I’ve already tried to add the probability display with “ probas[keep]” for each bounding box but it wasn’t successful :</p>
<pre><code>img = np.array(orig_image)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        for idx, box in enumerate(bboxes_scaled):
            bbox = box.cpu().data.numpy()
            bbox = bbox.astype(np.int32)
            bbox = np.array([
                [bbox[0], bbox[1]],
                [bbox[2], bbox[1]],
                [bbox[2], bbox[3]],
                [bbox[0], bbox[3]],
                ])
            bbox = bbox.reshape((4, 2))
            cv2.polylines(img, [bbox], probas[keep], True, (0, 255, 0), 2)
</code></pre>
<p>After long researches I couldn’t find script/solutions to obtain proper DETR results presentation. Is there any solution based on that script to obtain such results ?</p>
","transformer-model"
"77360663","How does padding work when using a pytorch TransformerEncoder?","2023-10-25 14:58:03","","0","266","<pytorch><transformer-model><self-attention>","<p>I'm trying to make a TransformerEncoder work with variable length sequences. I understand I can pass a <code>src_key_padding_mask</code> to the <code>forward</code> method.</p>
<p>Here's some example code.</p>
<pre><code>import torch
import torch.nn as nn

embedding_dim = 4
num_heads = 1
ff_dim = 16

encoder = nn.TransformerEncoderLayer(
        d_model=embedding_dim,
        nhead=num_heads,
        dim_feedforward=ff_dim,
        batch_first=True
    )

input_tensor = torch.randn(3, 6, embedding_dim)
input_tensor[0,5,:] = 0
input_tensor[0,4,:] = 0
input_tensor[1,5,:] = 0
print(f&quot;input\n{input_tensor}&quot;)

print(f&quot;no mask\n{encoder(input_tensor)}&quot;)

bool_src_key_padding_mask = torch.tensor(
        [[False, False, False, False, True, True],
         [False, False, False, False, False, True],
         [False, False, False, False, False, False]])

print(f&quot;mask\n{encoder(input_tensor, src_key_padding_mask=bool_src_key_padding_mask)}&quot;)
</code></pre>
<p>I would expect the result of the last line to print out a tensor containing padding tokens (<code>0</code> in this case), but it doesn't. I'm not sure what I'm doing wrong?</p>
","transformer-model"
"77360265","Simple Transformer Question Answer model error showing input should be list of examples","2023-10-25 14:10:41","","0","43","<nlp><transformer-model><nlp-question-answering><simpletransformers>","<p>This is the code for the resume dataset which was in DataFrame format. I tried to make it as json format as required by Simple Transformer.</p>
<p>I have 5 sets of questions which repeats for each row in the dataframe changing the company name, post name and its answers.</p>
<p>counter is just to give unique id to the questions. The dataframe has 22000 rows.
I am not sure why I keep getting this error.
Either the error is the one I have uploaded below or there is error of &quot;bool object is not callable&quot;</p>
<pre class=""lang-py prettyprint-override""><code>for index, value in df_1000_final.items():
    counter = range_till_1000[low:high]   # Slicing the id's
    
    # Taking company name and post name as per each row
    company_name = df_1000_final[index][index_df.loc[index,&quot;company_index&quot;]:index_df.loc[index,&quot;job_role_index&quot;]-4]
    post_name = df_1000_final[index][index_df.loc[index,&quot;title_index&quot;]:index_df.loc[index,&quot;company_index&quot;]-4]
    pay_rate = df_1000_final[index][index_df.loc[index,&quot;payrate_index&quot;]:]
    skills_required = df_1000_final[index][index_df.loc[index,&quot;skills_index&quot;]:index_df.loc[index,&quot;payrate_index&quot;]-3]
    experience_required = df_1000_final[index][index_df.loc[index,&quot;experience_index&quot;]:index_df.loc[index,&quot;skills_index&quot;]-3]
    role = df_1000_final[index][index_df.loc[index,&quot;job_role_index&quot;]:index_df.loc[index,&quot;job_location_index&quot;]-3]

    json_objects.append({
                &quot;context&quot;: value,
                &quot;qas&quot;: [
                    {
                        &quot;id&quot;: str(counter[0]),
                        &quot;is_impossible&quot;: False,
                        &quot;question&quot;: f&quot;What is job provided by {company_name} company?&quot;,
                        &quot;answers&quot;: [
                            {
                                &quot;text&quot;: post_name,
                                &quot;answer_start&quot;: index_df.loc[index,'title_index'],
                            }
                        ],
                    },
                    {
                        &quot;id&quot;: str(counter[1]),
                        &quot;is_impossible&quot;: False,
                        &quot;question&quot;: f&quot;What is salary provided by {company_name} company for {post_name} post?&quot;,
                        &quot;answers&quot;: [
                            {
                                &quot;text&quot;: pay_rate,
                                &quot;answer_start&quot;: index_df.loc[index,'payrate_index'],
                            }
                        ],
                    },
                    {
                        &quot;id&quot;: str(counter[2]),
                        &quot;is_impossible&quot;: False,
                        &quot;question&quot;: f&quot;What are skills required for {post_name} post in {company_name} company?&quot;,
                        &quot;answers&quot;: [
                            {
                                &quot;text&quot;: skills_required,
                                &quot;answer_start&quot;: index_df.loc[index,'skills_index'],
                            }
                        ],
                    },
                    {
                        &quot;id&quot;: str(counter[3]),
                        &quot;is_impossible&quot;: False,
                        &quot;question&quot;: f&quot;What is experience required for {post_name} post in {company_name} company?&quot;,
                        &quot;answers&quot;: [
                            {
                                &quot;text&quot;: experience_required,
                                &quot;answer_start&quot;: index_df.loc[index,'experience_index'],
                            }
                        ],
                    },
                    {
                        &quot;id&quot;: str(counter[4]),
                        &quot;is_impossible&quot;: False,
                        &quot;question&quot;: f&quot;What is role for {post_name} post in {company_name} company?&quot;,
                        &quot;answers&quot;: [
                            {
                                &quot;text&quot;: role,
                                &quot;answer_start&quot;: index_df.loc[index,'job_role_index'],
                            }
                        ],
                    },
                ],
            },
    )
</code></pre>
<pre><code># Configure the model
model_args = QuestionAnsweringArgs()
model_args.train_batch_size = 16
model_args.evaluate_during_training = True

model = QuestionAnsweringModel(
    &quot;roberta&quot;, &quot;roberta-base&quot;, args=model_args
)

# Train the model
model.train_model(train_data, eval_data=False,use_cuda=True)
</code></pre>
<p>This is the error :</p>
<pre><code>'(ReadTimeoutError(&quot;HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)&quot;), '(Request ID: 6d205e22-5a78-4562-ba07-f26af7fbf7fc)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/config.json
WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(&quot;HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)&quot;), '(Request ID: 6d205e22-5a78-4562-ba07-f26af7fbf7fc)')' thrown while requesting HEAD https://huggingface.co/roberta-base/resolve/main/config.json
Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
convert squad examples to features: 100%|██████████| 5000/5000 [01:00&lt;00:00, 82.60it/s]
add example index and unique id: 100%|██████████| 5000/5000 [00:00&lt;00:00, 611182.93it/s]
Epoch 1 of 1: 0%
0/1 [00:56&lt;?, ?it/s]
Epochs 0/1. Running Loss: 0.0076: 100%
322/322 [00:51&lt;00:00, 7.43it/s]
/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(&quot;Detected call of `lr_scheduler.step()` before `optimizer.step()`. &quot;
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-45-03fadae3f855&gt; in &lt;cell line: 11&gt;()
      9 
     10 # Train the model
---&gt; 11 model.train_model(train_data, eval_data=False,use_cuda=True)

5 frames
/usr/local/lib/python3.10/dist-packages/simpletransformers/question_answering/question_answering_utils.py in get_examples(examples_to_process, is_training, version_2_with_negative)
    130 def get_examples(examples_to_process, is_training=True, version_2_with_negative=True):
    131     if not isinstance(examples_to_process, list):
--&gt; 132         raise TypeError(&quot;Input should be a list of examples.&quot;)
    133 
    134     def is_whitespace(c):

TypeError: Input should be a list of examples.
</code></pre>
<p>I did modified the json structure as much as I can but it is still throwing the error</p>
","transformer-model"
"77350222","How make batch size Optimum for inference ASR model on GPU","2023-10-24 07:31:19","","0","94","<gpu><torch><transformer-model><openai-whisper>","<p>I have trained a Whisper ASR model and I have 8 GB GPU memory, my model takes 4 GB of my GPU and I have to calculate the maximum batch size to fit into the memory. My data throughput is around 200k each hour and I have to make it as fast as possible. Total parameters count of my model is <code>763857920</code>.</p>
<p>I tried to use batch size of 10 audios and I got the &quot;CUDA out of ...&quot; error, the data was like this:</p>
<pre><code>Chunk-Size------1.38Mb
chunk-len-------9
GPU-free(Mb)----3777.94MB
gpu-util--------1700.00%
gpu-mem---------200.00%
ERROR-message---Tried to allocate 20.00 MiB
</code></pre>
","transformer-model"
"77347239","Load a TransformerBlock & TokenAndPositionEmbedding keras model after saving it","2023-10-23 17:31:35","","0","106","<python><tensorflow><keras><nlp><transformer-model>","<p>Good morning,
I hope you are well and will help me get out of this situation. Basically, I trained a tensorflow.keras model by having defined a TransformerBlock class as well as a TokenAndPositionEmbedding class as you will see in my code. The problem I have is that when I reload the model I get the following error:</p>
<pre><code>chat = tf.keras.models.load_model(&quot;FAQ_model_2.keras&quot;)
</code></pre>
<p>Error i got :</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-13-a21ba74f84b1&gt; in &lt;cell line: 1&gt;()
----&gt; 1 chat = tf.keras.models.load_model(&quot;FAQ_model_2.keras&quot;)

9 frames
/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py in load_own_variables(self, store)
   3529         all_vars = self._trainable_weights + self._non_trainable_weights
   3530         if len(store.keys()) != len(all_vars):
-&gt; 3531             raise ValueError(
   3532                 f&quot;Layer '{self.name}' expected {len(all_vars)} variables, &quot;
   3533                 &quot;but received &quot;

ValueError: Layer 'dense_13' expected 2 variables, but received 0 variables during loading. Expected: ['dense_13/kernel:0', 'dense_13/bias:0']
</code></pre>
<p>And there is the code i used to register my custom layers :</p>
<pre><code>keras.saving.get_custom_objects().clear()

@keras.saving.register_keras_serializable()
class TransformerBlock(tf.keras.layers.Layer):
  def __init__(self, embed_dim, num_heads, ff_dim, rate=.1, **kwargs):
    super().__init__(**kwargs)

    self.embed_dim = embed_dim
    self.num_heads = num_heads
    self.ff_dim = ff_dim
    self.rate = rate

    self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
    self.ffn = tf.keras.Sequential(
        [tf.keras.layers.Dense(ff_dim, activation=&quot;relu&quot;), tf.keras.layers.Dense(embed_dim),]
    )
    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
    self.dropout1 = tf.keras.layers.Dropout(rate)
    self.dropout2 = tf.keras.layers.Dropout(rate)

  def call(self, inputs, training):
    attn_output = self.att(inputs, inputs)
    attn_output = self.dropout1(attn_output, training=training)
    out1 = self.layernorm1(inputs + attn_output)
    ffn_output = self.ffn(out1)
    ffn_output = self.dropout2(ffn_output, training=training)
    return self.layernorm2(out1 + ffn_output)
  
  def get_config(self):
    config = super().get_config()
    # save constructor args
    config['embed_dim'] = self.embed_dim
    config['num_heads'] = self.num_heads
    config['ff_dim'] = self.ff_dim
    config[&quot;rate&quot;] = self.rate
    return config



@keras.saving.register_keras_serializable()
class TokenAndPositionEmbedding(tf.keras.layers.Layer):
  def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):
    super().__init__(**kwargs)

    self.maxlen = maxlen
    self.vocab_size = vocab_size
    self.embed_dim = embed_dim

    self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
    self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

  def call(self, x):
    maxlen = tf.shape(x)[-1]
    positions = tf.range(start=0, limit=maxlen, delta=1)
    positions = self.pos_emb(positions)
    x = self.token_emb(x)
    return x + positions
  
  def get_config(self):
    config = super().get_config()
    # save constructor args
    config['maxlen'] = self.maxlen
    config['vocab_size'] = self.vocab_size
    config['embed_dim'] = self.embed_dim
    return config


# question
questionX = Input(shape=(29,), name=&quot;userQuestion&quot;)
questionInput = TokenAndPositionEmbedding(29, vocabLen+1, 32)(questionX)
#questionInput = LayerNormalization()(questionInput)
questionInput = TransformerBlock(32, 2, 32)(questionInput)
#questionInput = TransformerBlock(32, 2, 32)(questionInput)
questionInput = tf.keras.layers.GlobalAveragePooling1D()(questionInput)


# class prediction
#classe = tf.keras.layers.Dropout(.2)(questionInput)
classe = tf.keras.layers.Dense(20, activation=&quot;relu&quot;)(questionInput)
classe = tf.keras.layers.Dropout(.1)(classe)
classe = tf.keras.layers.Dense(len(classes), activation=&quot;softmax&quot;, name=&quot;next&quot;)(classe)



# model

chat = Model(inputs=[questionX], outputs=[classe], name=&quot;chat&quot;)
chat.summary()
chat.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.RMSprop(), metrics=[&quot;accuracy&quot;])
</code></pre>
<p>Once the model has been trained i tested, i saved it by using :</p>
<pre><code>chat.save(&quot;FAQ_model_2.keras&quot;)
</code></pre>
<p>And when i reload, i got the error message i provided.</p>
<p>Can you please help me ?</p>
<p>Thanks in advance.</p>
<p>I expect the saved model with TransformerBlock and TokenAndPositionEmbedding to load properly.</p>
","transformer-model"
"77323180","Pytorch Transform to predict numeric sequences","2023-10-19 10:57:05","","0","228","<pytorch><transformer-model>","<p>I'm interested in training a Pytorch Transformer model to predict numeric sequences.  I'm looking for a practical example from which to learn.  Can anyone point me to one?</p>
<p>Something like,</p>
<p>I train it on a sequence of numbers range(1,100).
I give it [22, 23] and it will produce [24, 25, 26, ..]
Later, the samples would be vectors.</p>
<p>I'm new to PyTorch and Transformers, but this can't be hard, right?</p>
","transformer-model"
"77295970","Combine output of ctc distribution with decoder output distribution?","2023-10-15 09:11:51","","0","38","<transformer-model><linear-interpolation><ctc>","<p><a href=""https://i.sstatic.net/zzO9o.jpg"" rel=""nofollow noreferrer"">A visual representation of what I want to do</a>
I have a transformer encoder decoder structure, but want to jointly train with ctc. The encoder outputs(vis softmax) the ctc frame wise probabilities(batch x maxframes x vocab) , and the decoder outputs character probability distribution(batch x maxsequencelength x vocab). I want to combine them (joint decoding), how do I go about doing this?</p>
<p>What I tried:
I tried to linearly combine them using (1-lambda)Pctc + lambda*Pdecoder, but they are of different sizes, I need to decode or collapse the ctc to character probabilities, like remove all the blanks or repetitions but have no clue on how to go about that.</p>
","transformer-model"
"77291396","Error occurs in Pretraining KoBERT using MLM","2023-10-14 02:20:05","","0","59","<python><pytorch><nlp><bert-language-model><transformer-model>","<pre><code>text = &quot;&quot;
inputs = tokenizer(text, return_tensors='pt') #inputs = tokenizer.encode(text, return_tensors='pt')

print(inputs)

rand = torch.rand(inputs.input_ids.shape)

mask_arr = (rand &lt; 0.15) * (inputs.input_ids != 2) * (inputs.input_ids != 3) * (inputs.input_ids != 0)

selection = torch.flatten((mask_arr[0]).nonzero())

import numpy as np

selection_val = np.random.random(len(selection)) 

mask_selection = selection[np.where(selection_val &gt;= 0.2)[0]] 
random_selection = selection[np.where(selection_val &lt; 0.1)[0]]

print(random_selection) # tensor([ 30,  95, 143])
print(mask_selection)

inputs.input_ids[0, mask_selection] = 103
inputs.input_ids[0, random_selection] = torch.randint(0, 30522, size = random_selection.shape)

class MeditationsDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __getitem__(self, idx):
        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
    def __len__(self):
        return len(self.encodings.input_ids)

from transformers import TrainingArguments
from transformers import Trainer

dataset = MeditationsDataset(inputs)


args = TrainingArguments(
    output_dir='result', 
    per_device_train_batch_size=16, 
    num_train_epochs=2 
)

trainer = Trainer(
    model=model, 
    args=args, 
    train_dataset=dataset 
)

trainer.train()
</code></pre>
<p>error code:</p>
<pre><code>RuntimeError Traceback (most recent call last) in &lt;cell line: 25&gt;() 23 ) 24 ---&gt; 25 trainer = Trainer( 26 model=model, 27 args=args,

5 frames /usr/local/lib/python3.10/dist-packages/torch/cuda/random.py in cb() 109 for i in range(device_count()): 110 default_generator = torch.cuda.default_generators[i] --&gt; 111 default_generator.manual_seed(seed) 112 113 _lazy_call(cb, seed_all=True)
</code></pre>
<pre><code>RuntimeError: CUDA error: device-side assert triggered CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. For debugging consider passing CUDA_LAUNCH_BLOCKING=1. Compile with TORCH_USE_CUDA_DSA to enable device-side assertions.
</code></pre>
<p>What was wrong?</p>
<p>I found that device-side assert triggered CUDA kernel errors are triggered by unappropriate class index number, but I can't know where I can specify that.</p>
","transformer-model"
"77281212","Why is the image rotated incorrectly using the Matrix.rotateM method in Media3 Transformer?","2023-10-12 13:52:05","","0","74","<android><matrix><rotation><transformer-model>","<p>I just want to rotate the image in one plane
I'm trying to mess around with the Media3 Transformer demo <a href=""https://developer.android.com/guide/topics/media/transformer/demo-application"" rel=""nofollow noreferrer"">https://developer.android.com/guide/topics/media/transformer/demo-application</a></p>
<pre><code> private OverlayEffect createOverlayEffectFromBundle(Bundle bundle, boolean[] selectedEffects)
      throws PackageManager.NameNotFoundException {
    ImmutableList.Builder&lt;TextureOverlay&gt; overlaysBuilder = new ImmutableList.Builder&lt;&gt;();
    if (selectedEffects[ConfigurationActivity.OVERLAY_LOGO_AND_TIMER_INDEX]) {
      float[] logoPositioningMatrix = GlUtil.create4x4IdentityMatrix();
      Matrix.translateM(
          logoPositioningMatrix, /* mOffset= */ 0, /* x= */ -0.95f, /* y= */ -0.95f, /* z= */ 1);
// here I rotate 45 degrees around the Z axis
      Matrix.rotateM(
          logoPositioningMatrix,0, 45f,0f, 0f,  1f);
      OverlaySettings logoSettings =
          new OverlaySettings.Builder()
              .setMatrix(logoPositioningMatrix)
              .setAnchor(/* x= */ -1f, /* y= */ -1f)
              .build();
</code></pre>
<p>fun from <a href=""https://github.com/androidx/media/blob/release/demos/transformer/src/main/java/androidx/media3/demo/transformer/TransformerActivity.java"" rel=""nofollow noreferrer"">https://github.com/androidx/media/blob/release/demos/transformer/src/main/java/androidx/media3/demo/transformer/TransformerActivity.java</a></p>
<p><a href=""https://i.sstatic.net/ckOXX.jpg"" rel=""nofollow noreferrer"">without rotation</a></p>
<p><a href=""https://i.sstatic.net/5KbPQ.jpg"" rel=""nofollow noreferrer"">after rotation along the Z axis</a></p>
<p><a href=""https://i.sstatic.net/Ymfcu.jpg"" rel=""nofollow noreferrer"">after rotation along the X axis</a></p>
<p><a href=""https://i.sstatic.net/sdO5i.jpg"" rel=""nofollow noreferrer"">after rotation along the Y axis</a></p>
","transformer-model"
"77276708","Positional Encoding of 2D grid","2023-10-11 22:32:51","","0","248","<encoding><transformer-model>","<p>I have a 100 x 100 grid where certain (x,y) coordinates have a 32 dimension feature vector. I want to find the 2D positional encoding for this grid and add that to the feature vector at that position. So, for each x,y coordinate:</p>
<p>new feature = original feature + PE(x,y)</p>
<p>but  I am confused about the implementation from: <a href=""https://github.com/tatp22/multidim-positional-encoding"" rel=""nofollow noreferrer"">https://github.com/tatp22/multidim-positional-encoding</a>.</p>
<p>The input has to be of the form <code>(batch size, x, y, ch)</code> where <code>ch = 32</code> and <code>x, y</code> are the coordinates. But, the calculated PE is also of size <code>(batch size, x, y, ch)</code>. How  do I extract the PE for certain positions in this case?</p>
<p>Thanks a lot</p>
","transformer-model"
"77262279","TypeError: Object of type ViTConfig is not JSON serializable when pushing a custom ViT model to Hugging Face Hub","2023-10-09 23:50:06","","0","338","<python><huggingface-transformers><transformer-model><json-serialization><vision-transformer>","<p>I'm new to hugging face and hugsvision. I'm trying to push a custom Vision Transformer (ViT) model to the Hugging Face Hub. I've defined a custom configuration using ViTConfig, but I get this error when trying to push the feature extractor to the Hub. I want this model to be pushed to hub so I will load it later to fine tune on another dataset.</p>
<p>Here's my code:</p>
<pre><code>from transformers import ViTConfig, ViTFeatureExtractor, ViTForImageClassification

config = ViTConfig (
   _name_or_path= &quot;myViT&quot;,
  architectures= [
    &quot;ViTForImageClassification&quot;
  ],
  id2label= {
    &quot;0&quot;: &quot;Benign&quot;,
    &quot;1&quot;: &quot;Malignant&quot;
  },
  ...
)

model1 = ViTForImageClassification(
    config
)

feature_extractor1 = ViTFeatureExtractor(
    config
)

training_args1 = TrainingArguments(
        ...
)

trainer1 = Trainer(
      ...
)

trainer1.train()

model1.push_to_hub(&quot;my-username/my-custom-vit-model&quot;)
feature_extractor1.push_to_hub(&quot;my-username/my-custom-vit-model&quot;)

</code></pre>
<p>The error message I receive for the last line is:</p>
<pre><code>TypeError: Object of type ViTConfig is not JSON serializable

</code></pre>
<p>This works fine when I use pretrained models.</p>
<p>Any help or suggestions on how to resolve this issue would be greatly appreciated.</p>
<p>I could not find much documentation regarding pushing a custom ViT model to hub. Also, I tried to dump the model.config into json but I did not work.</p>
","transformer-model"
"77259232","How to test nlu model on multiple requests per second?","2023-10-09 13:29:47","","1","33","<python><python-3.x><deep-learning><transformer-model><sentence-transformers>","<p>I am testing nlu model called LaBSE. I want to know how much gpu memory is required when i run it. Here is example how to run it on sentence and turn it into embeddings:</p>
<pre><code>from sentence_transformers import SentenceTransformer
sentences = [&quot;This is an example sentence&quot;]

model = SentenceTransformer('sentence-transformers/LaBSE')
embeddings = model.encode(sentences)
print(embeddings)
</code></pre>
<p>It shows that 1.9GB of gpu memory are used. But i want to know how much gpu memory will be used when i run it with 10, 25, 50 requests per second. How could I do that? How could i run model.encode on 25 requests per second for example? I basically want to know how much gpu and RAM memory is required if I deploy it and there will be multiple requests per second.</p>
","transformer-model"
"77234763","Transformer Just Only Learning to Predict Dots and Padding Tokens in Translation Task","2023-10-05 06:46:39","","0","321","<huggingface-transformers><transformer-model>","<p>I'm new to Natural Language Processing and Transformers. I started to learn about Transformers by implementing one from scratch using PyTorch. This is a translation-level encoder-decoder transformer that translates from English to Malayalam (A South Indian Language).</p>
<p>During the training of the network, even after a prolonged period, it only predicts padding tokens and sometimes the start tokens. To address this issue, I tried a technique where I accumulated text by appending the next sentences to the current ones in both the source and target languages until a maximum length was reached.</p>
<p>Eg:</p>
<p>&quot;This is some text&quot;</p>
<p>&quot;This is some larger text which has more sequence length&quot;</p>
<p>Accumulating,</p>
<p>&quot;This is some text. This is some larger text which has more sequence length .... -&gt; (max-seq-len)&quot;</p>
<p>Then lined up both the source and translation to ensure that each sentence had a comparable length and was aligned with the translation.</p>
<p>Then after training it again, new problems came in, and now the network is only predicting dots &quot;.&quot; since padding tokens are reduced, I am unsure of the reason If there is any problem with the decoder or encoder, or if is this the normal behavior of transformer models as it has not produced any translations even after 5 or 6 epochs. Well, considering the number of epochs is smaller, there might be some kind of progress in each training phase, at least we could see some kind of words emerging, but it is not happening, even in some cases, the loss is not decreasing.</p>
<h2>Things Used</h2>
<p>Here are the things I used,</p>
<p>Dataset: <a href=""https://www.kaggle.com/datasets/sidharthangn/ml-dataset"" rel=""nofollow noreferrer"">https://www.kaggle.com/datasets/sidharthangn/ml-dataset</a></p>
<p>SentencePiece self-trained Tokenizer: <a href=""https://drive.google.com/drive/folders/15ju7heON3szJ2s3DDtNrkjGjXuOFaW-t?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/drive/folders/15ju7heON3szJ2s3DDtNrkjGjXuOFaW-t?usp=sharing</a></p>
<h2>Transformer Code</h2>
<pre class=""lang-py prettyprint-override""><code>import torch
import torch.nn as nn
import math
from math import sqrt
import torch.nn.functional as F


def scaled_dot_product_attention(query, key, value, mask=None):
    dim_k = query.size(-1)
    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) # 32 x 200 x 200
    if mask is not None:
        scores = scores.masked_fill(mask==1, -1e9) # Very small negative number for masking
    attn_weights = F.softmax(scores, dim=-1)
    return attn_weights.bmm(value)


class EncoderPositionalEncoding(nn.Module):
    def __init__(self, config):
        super(EncoderPositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=0.2)

        # Create the positional encoding matrix
        pe = torch.zeros(config['source_contextual_length'], config['embedding_dim'])
        position = torch.arange(0, config['source_contextual_length'], dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0,
                                          config['embedding_dim'],
                                          2).float() * (-torch.log(torch.tensor(10000.0)) / config['embedding_dim']))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        self.pe.requires_grad = False

    def forward(self, x):
        # Add positional encoding to the input
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)


class DecoderPositionalEncoding(nn.Module):
    def __init__(self, config):
        super(DecoderPositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=0.2)

        # Create the positional encoding matrix
        pe = torch.zeros(config['target_contextual_length'], config['embedding_dim'])
        position = torch.arange(0, config['target_contextual_length'], dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0,
                                          config['embedding_dim'],
                                          2).float() * (-torch.log(torch.tensor(10000.0)) / config['embedding_dim']))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
        self.pe.requires_grad = False

    def forward(self, x):
        # Add positional encoding to the input
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)

class EncoderEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_embeddings = nn.Embedding(config['en_vocab_size'], config['embedding_dim'])
        self.layer_norm = nn.LayerNorm(config['embedding_dim'], eps=1e-12)
        self.dropout = nn.Dropout()

    def forward(self, input_ids):
        embeddings = self.token_embeddings(input_ids)
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings

class DecoderEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.token_embeddings = nn.Embedding(config['ml_vocab_size'], config['embedding_dim'])
        self.layer_norm = nn.LayerNorm(config['embedding_dim'], eps=1e-12)
        self.dropout = nn.Dropout()

    def forward(self, input_ids):
        embeddings = self.token_embeddings(input_ids)
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)
        return embeddings

# Encoder Attention Head
class AttentionHead(nn.Module):
    def __init__(self, embed_dim, head_dim):
        super().__init__()
        self.q = nn.Linear(embed_dim, head_dim)
        self.k = nn.Linear(embed_dim, head_dim)
        self.v = nn.Linear(embed_dim, head_dim)

    def forward(self, hidden_state, src_mask):
        attn_outputs = scaled_dot_product_attention(
            self.q(hidden_state),
            self.k(hidden_state),
            self.v(hidden_state),
            mask=src_mask
        )
        return attn_outputs


# Encoder Multi-head attention
class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config['embedding_dim']
        num_heads = config['encoder_attn_heads']
        head_dim = embed_dim // num_heads
        self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])
        self.output_linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, hidden_state, src_mask):
        x = torch.cat([h(hidden_state, src_mask) for h in self.heads], dim=-1)
        x = self.output_linear(x)
        return x

# Position-Wise Feed Forward Layer
class FeedForward(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.linear_1 = nn.Linear(config['embedding_dim'], config['encoder_ffm_dim'])
        self.linear_2 = nn.Linear(config['encoder_ffm_dim'], config['embedding_dim'])
        self.gelu = nn.GELU()
        self.dropout = nn.Dropout(config['dropout'])

    def forward(self, x):
        x = self.linear_1(x)
        x = self.gelu(x)
        x = self.linear_2(x)
        x = self.dropout(x)
        return x

class TransformerEncoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.layer_norm_1 = nn.LayerNorm(config['embedding_dim'])
        self.layer_norm_2 = nn.LayerNorm(config['embedding_dim'])
        self.attention = MultiHeadAttention(config)
        self.feed_forward = FeedForward(config)


    def forward(self, x, src_mask):
        hidden_state = self.layer_norm_1(x)
        # Applying skip connections
        x = x + self.attention(hidden_state, src_mask)
        # Attention with skip connection is passed to feed forward layer
        x = x + self.feed_forward(self.layer_norm_2(x))
        return x

class TransformerEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.encoding = EncoderPositionalEncoding(config)
        self.embeddings = EncoderEmbeddings(config)
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(config) for _ in range(config['encoder_layers'])
        ])

    def forward(self, x, src_mask):
        x = self.embeddings(x)
        x = x + self.encoding(x)
        for layer in self.layers:
            x = layer(x, src_mask)
        return x # Hidden contextual information from Encoder

# Encoder-decoder attention head
class CrossAttentionHead(nn.Module):
    def __init__(self, embed_dim, head_dim):
        super().__init__()
        self.q = nn.Linear(embed_dim, head_dim)
        self.k = nn.Linear(embed_dim, head_dim)
        self.v = nn.Linear(embed_dim, head_dim)

    def forward(self, hidden_state, encoder_out=None):
        attn_outputs = None
            # Cross Attention Encoder-Decoder Attention
        if encoder_out is not None:
            attn_outputs = scaled_dot_product_attention(
                self.q(hidden_state),
                self.k(encoder_out),
                self.v(encoder_out),
            )
        else:
            attn_outputs = scaled_dot_product_attention(
                self.q(hidden_state),
                self.k(hidden_state),
                self.v(hidden_state),
            )
        return attn_outputs

# Encoder decoder multi-head attention
class CrossMultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config['embedding_dim']
        num_heads = config['encoder_attn_heads']
        head_dim = embed_dim // num_heads

        self.heads = nn.ModuleList([CrossAttentionHead(embed_dim, head_dim) for _ in range(num_heads)])
        self.output_linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, hidden_state, encoder_out):
        x = torch.cat([h(hidden_state, encoder_out) for h in self.heads], dim=-1)
        x = self.output_linear(x)
        return x


class MaskedAttentionHead(nn.Module):
    def __init__(self, embed_dim, head_dim):
        super().__init__()
        self.q = nn.Linear(embed_dim, head_dim)
        self.k = nn.Linear(embed_dim, head_dim)
        self.v = nn.Linear(embed_dim, head_dim)

    def forward(self, hidden_state, d_mask):
        attn_outputs = scaled_dot_product_attention(
            self.q(hidden_state),
            self.k(hidden_state),
            self.v(hidden_state),
            mask=d_mask
        )
        return attn_outputs

class MaskedMultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        embed_dim = config['embedding_dim']
        num_heads = config['decoder_attn_heads']
        head_dim = embed_dim // num_heads
        self.heads = nn.ModuleList([
            MaskedAttentionHead(embed_dim, head_dim) for _ in range(num_heads)
        ])

        self.output_linear = nn.Linear(embed_dim, embed_dim)

    def forward(self, hidden_state, d_mask):
        x = torch.cat([h(hidden_state, d_mask) for h in self.heads ], dim=-1)
        x = self.output_linear(x)
        return x


class TransformerDecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.layer_norm_1 = nn.LayerNorm(config['embedding_dim'])
        self.layer_norm_2 = nn.LayerNorm(config['embedding_dim'])
        self.layer_norm_3 = nn.LayerNorm(config['embedding_dim'])

        self.dropout1 = nn.Dropout(config['dropout'])
        self.dropout2 = nn.Dropout(config['dropout'])
        self.dropout3 = nn.Dropout(config['dropout'])

        self.masked_attention = MaskedMultiHeadAttention(config)
        self.attention = CrossMultiHeadAttention(config)
        self.feed_forward = FeedForward(config)

    def forward(self, x, encoder_out, d_mask):
        hidden_state = self.layer_norm_1(x)
        x = x + self.masked_attention(hidden_state, d_mask) # For training, mask is applied
        x = self.dropout1(x)
        hidden_state2 = self.layer_norm_2(x)
        x = x + self.attention(hidden_state2, encoder_out)
        x = self.dropout2(x)
        x = x + self.feed_forward(self.layer_norm_3(x))
        x = self.dropout3(x)
        return x

class TransformerDecoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.encoding = DecoderPositionalEncoding(config)
        self.embeddings = DecoderEmbeddings(config)

        self.layers = nn.ModuleList([
            TransformerDecoderLayer(config) for _ in range(config['decoder_layers'])
        ])

        self.linear_layer = nn.Linear(config['embedding_dim'], config['ml_vocab_size'])

    def forward(self, x, encoder_out, d_mask):
        x = self.embeddings(x)
        x = x + self.encoding(x)
        # Pass the encoder output to the last decoder layer only
        for i, layer in enumerate(self.layers):
            if i == len(self.layers) - 1:
                x = layer(x, encoder_out, d_mask) # IF the last layer, apply encoder output
            else:
                x = layer(x, None, d_mask)  # Pass None for encoder output for other layers
        x = self.linear_layer(x)
        x = F.softmax(x, dim=-1)
        return x # The probability distribution from Decoder


class Transformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.encoder = TransformerEncoder(config)
        self.decoder = TransformerDecoder(config)

    def forward(self, x, y, src_mask=None, d_mask=None):
        # src-mask: For encoder mask - Padding mask
        # d-mask: For decoder mask - Both padding and Look-Ahead mask

        # x: [32, 200]
        # y: [32, 200]
        x = self.encoder(x, src_mask)
        x = self.decoder(y, x, d_mask)
        return x
</code></pre>
<p>Here is the summary of the Transformer,</p>
<ol>
<li><p>Encoder and decoder blocks</p>
</li>
<li><p>The tokenized batch of source language &quot;x&quot; is passed through an encoder with the src_mask (padding mask)</p>
</li>
<li><p>The encoder's self attention takes it, applies masks before calculating scores and returns it.</p>
</li>
<li><p>Then the output of attention is passed to feed forward layer</p>
</li>
<li><p>The tokenized batch of target language &quot;y&quot; is passed to the decoder along with the &quot;x&quot; which is the output from the encoder</p>
</li>
<li><p>The decoder's masked multihead attention with look ahead mask takes it and returns the scores, then it is passed to cross attention, and finally the feed forward layer and applies linear and softmax.</p>
</li>
<li><p>The output from the encoder is only applied to the last decoder depending on how many layers of encoder and decoder is created, It is done inside the &quot;TransformerDecoder&quot; Class</p>
</li>
</ol>
<p>Here are the configurations used</p>
<pre class=""lang-py prettyprint-override""><code>

config = {
    &quot;en_vocab_size&quot;: len(en_vocab_mapping), # The size of the english vocabulary
    &quot;ml_vocab_size&quot;: len(ml_vocab_mapping), # size of malayalam vocabulary
    &quot;embedding_dim&quot;: 512, # Each token is converted into an embdding vector of dimension 512
    &quot;source_contextual_length&quot;: en_tokenized.size(1), # Max_length for source language sequence
    &quot;target_contextual_length&quot;: ml_tokenized.size(1), # Max_length for target langauge sequence
    &quot;encoder_attn_heads&quot;: 8, # Number of attention heads in the encoder
    &quot;decoder_attn_heads&quot;: 8, # Number of attention heads in the decoder
    &quot;encoder_layers&quot;: 4, # Encoder num layers - For more contextual information, you need more encoder layers
    &quot;decoder_layers&quot;: 4, # Decoder num layers - For more contextual information for translation, you need more decoder layers
    &quot;encoder_ffm_dim&quot;: 2048, # Hidden dimension of feed forward layer in encoder
    &quot;decoder_ffm_dim&quot;: 2048, # Hidden dimension of feed forward layer in decoder
    &quot;dropout&quot;: 0.2 # Common Dropout over the network.

}
</code></pre>
<h2>Helper Functions</h2>
<p>Loading the dataset as a Python list containing sentences,</p>
<pre class=""lang-py prettyprint-override""><code># Load Dataset

import multiprocessing

def read_file(file_path):
    with open(file_path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as file:
        return file.readlines()

# Define the file paths
en_file_path = &quot;/content/drive/MyDrive/cu_transformer/Datasets/en_corpus_new.en&quot;
ml_file_path = &quot;/content/drive/MyDrive/cu_transformer/Datasets/ml_corpus_new.ml&quot;

# Create a multiprocessing pool
pool = multiprocessing.Pool(2)  # Adjust the number of processes as needed

# Use parallel processing to read both files
source_language_list, target_language_list = pool.map(read_file, [en_file_path, ml_file_path])

# Close the pool
pool.close()
pool.join()
</code></pre>
<p>Accumulate sentences but adding next sentences in the list of sentences to current one until a max length is reached to avoid excessive number of padding tokens.</p>
<pre class=""lang-py prettyprint-override""><code>import re

def accumulate_text(source_list, target_list, max_sequence_length):
    &quot;&quot;&quot; Accumulating text until a specific length is reached to reduce the number of padding tokens &quot;&quot;&quot;
    new_source_list = []
    new_target_list = []
    current_source_sequence = []
    current_target_sequence = []
    for source_sentence, target_sentence in zip(source_list, target_list):
        if len(current_source_sequence) + len(source_sentence.split()) &lt;= max_sequence_length:
            current_source_sequence.extend(source_sentence.split())
            current_target_sequence.extend(target_sentence.split())
        else:
            new_source_list.append(&quot; &quot;.join(current_source_sequence))
            new_target_list.append(&quot; &quot;.join(current_target_sequence))
            current_source_sequence = source_sentence.split()
            current_target_sequence = target_sentence.split()
    new_source_list.append(&quot; &quot;.join(current_source_sequence))
    new_target_list.append(&quot; &quot;.join(current_target_sequence))
    return new_source_list, new_target_list
</code></pre>
<p>Remove noise:</p>
<pre class=""lang-py prettyprint-override""><code>def remove_noise(text_list, pattern):
    regex_pattern = re.compile(pattern)
    cleaned_text_list = []
    for text in text_list:
        # Use the regex pattern to replace the matched pattern with an empty string
        cleaned_text = re.sub(regex_pattern, '', text)
        cleaned_text_list.append(cleaned_text)
    return cleaned_text_list
</code></pre>
<p>Tokenization:</p>
<pre class=""lang-py prettyprint-override""><code>import torch

def tokenize_and_encode(sentences, tokenizer, add_start_end_tokens=False, max_seq_length=None):
    encoded_sentences = []
    for sentence in sentences:
        encoded_sentence = tokenizer.EncodeAsIds(sentence)
        if add_start_end_tokens:
            encoded_sentence = [tokenizer.PieceToId(&quot;&lt;s&gt;&quot;)] + encoded_sentence + [tokenizer.PieceToId(&quot;&lt;/s&gt;&quot;)]
        if max_seq_length is not None:
            if len(encoded_sentence) &gt; max_seq_length:
                encoded_sentence = encoded_sentence[:max_seq_length]  # Truncate if too long
            else:
                encoded_sentence += [0] * (max_seq_length - len(encoded_sentence))  # Pad if too short
        encoded_sentences.append(encoded_sentence)
    max_length = max(len(sentence) for sentence in encoded_sentences)
    padded_sentences = [sentence + [0] * (max_length - len(sentence)) for sentence in encoded_sentences]
    tensor_sentences = torch.tensor(padded_sentences)
    return tensor_sentences
</code></pre>
<p>Decoding the tokens to see if things are correct,</p>
<pre class=""lang-py prettyprint-override""><code>def decode_tokens(tokenized_tensor, tokenizer):
    decoded_sentences = []
    for tokenized_sentence in tokenized_tensor:
        token_ids = tokenized_sentence.tolist()
        decoded_text = tokenizer.DecodeIds(token_ids)
        decoded_sentences.append(decoded_text)
    return &quot; &quot;.join(decoded_sentences)
</code></pre>
<p>Creating a vocabulary dict,</p>
<pre class=""lang-py prettyprint-override""><code>def create_vocab_mapping(tokenizer):
    vocab_mapping = {tokenizer.IdToPiece(id): id for id in range(tokenizer.GetPieceSize())}
    return vocab_mapping
</code></pre>
<h2>Data Preprocessing</h2>
<p>Loading the trained SentencePeice model for tokenizing</p>
<pre class=""lang-py prettyprint-override""><code>en_model_path = &quot;/content/drive/MyDrive/cu_transformer/en_bpe2.model.model&quot;
ml_model_path = &quot;/content/drive/MyDrive/cu_transformer/ml_bpe2.model.model&quot;


sp_en = spm.SentencePieceProcessor()
sp_en.Load(en_model_path)

sp_ml = spm.SentencePieceProcessor()
sp_ml.Load(ml_model_path)
</code></pre>
<p>Selecting a list of sentences ,</p>
<pre class=""lang-py prettyprint-override""><code>sentences = 32000
en_sentences = [language.strip(&quot;\n&quot;) for language in source_language_list[:sentences]]
ml_sentences = [language.strip(&quot;\n&quot;) for language in target_language_list[:sentences]]
</code></pre>
<p>Text accumulation,</p>
<pre class=""lang-py prettyprint-override""><code>en_accumulated, ml_accumulated = accumulate_text(en_sentences, ml_sentences, 50)
</code></pre>
<p>Creating vocabulary for both source and target language</p>
<pre class=""lang-py prettyprint-override""><code>ml_vocab_mapping = create_vocab_mapping(sp_ml) # English vocabulary
en_vocab_mapping = create_vocab_mapping(sp_en) # Malayalam Vocabulary
</code></pre>
<p>Tokenizing,</p>
<pre class=""lang-py prettyprint-override""><code>en_tokenized = tokenize_and_encode(en_accumulated, sp_en, max_seq_length=70)
ml_tokenized = tokenize_and_encode(ml_accumulated, sp_ml, add_start_end_tokens=True, max_seq_length=70)
</code></pre>
<p>After tokenizing, the tensor contains valid tokens rather than a large number of padding tokens,</p>
<h2>Creating Padding and Look-Ahead Masks</h2>
<pre class=""lang-py prettyprint-override""><code>def create_padding_mask(tokenized_tensor, padding_idx=0):
    # Unsqueezing to match the dimension, I'm not sure if it is correct
    padding_mask = (tokenized_tensor == padding_idx).unsqueeze(1).float() # Convert True False to float
    return padding_mask

def create_look_ahead_mask(batch_size, sequence_length):
    look_ahead_mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1)
    batched_look_ahead_mask = look_ahead_mask.unsqueeze(0).expand(batch_size, -1, -1)
    return batched_look_ahead_mask
</code></pre>
<p>Pytorch Dataset and Dataloader</p>
<pre class=""lang-py prettyprint-override""><code>from torch.utils.data import Dataset, DataLoader

class TranslationDataset(Dataset):
    def __init__(self, en_tokenized, ml_tokenized):
        self.en_tokenized = en_tokenized
        self.ml_tokenized = ml_tokenized
        self.length = len(en_tokenized)

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        en_input = torch.tensor(self.en_tokenized[idx]).clone().detach()
        ml_input = torch.tensor(self.ml_tokenized[idx]).clone().detach()

        return {
            &quot;en_tokenized&quot;: en_input,
            &quot;ml_tokenized&quot;: ml_input
        }

batch_size = 16

dataset = TranslationDataset(en_tokenized, ml_tokenized)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
</code></pre>
<h2>Transformer Training</h2>
<pre class=""lang-py prettyprint-override""><code>import torch.optim as optim

transformer = Transformer(config)
optimizer = optim.Adam(transformer.parameters(), lr=0.00002)
criterion = nn.CrossEntropyLoss()
num_epochs = 10
transformer.to(device)

look_ahead_mask = create_look_ahead_mask(batch_size, ml_tokenized.size(1))
reverse_vocab_mapping = {value : key for key, value in ml_vocab_mapping.items()} # reversing target vocabulary to get characters when using token ID's


# Training 

from tqdm import tqdm

for epoch in range(num_epochs):
    total_loss = 0.0
    data_iterator = tqdm(dataloader, desc=f&quot;Epoch {epoch + 1}/{num_epochs} (DA)&quot;)
    for batch in data_iterator:
        en_batch, ml_batch = (
            batch['en_tokenized'].to(device),
            batch['ml_tokenized'].to(device)
        )
        source_padding_mask = create_padding_mask(en_batch).to(device)
        target_padding_mask = create_padding_mask(ml_batch).to(device)
        look_ahead_mask = look_ahead_mask.to(device)
        decoder_mask = torch.logical_or(look_ahead_mask, target_padding_mask).float()
        optimizer.zero_grad()
        output = transformer(
            en_batch,
            ml_batch,
            src_mask = source_padding_mask,
            d_mask = decoder_mask
        )
        ml_batch_flat = ml_batch.view(-1)
        output_flat = output.contiguous().view(-1, config['ml_vocab_size'])
        loss = criterion(output_flat, ml_batch_flat)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / len(dataloader)
    print(f&quot;Epoch {epoch+1}, Avg. Loss: {avg_loss:.4f}&quot;)

    # Inference 
    if (epoch + 1) % 1 == 0:
        transformer.eval()
        for i, eval_batch in enumerate(dataloader):
            if i == 1:
                break
            en_eval_batch, ml_eval_batch = (
                batch['en_tokenized'].to(device),
                batch['ml_tokenized'].to(device)
            )

            predicted_ids = torch.argmax(output, dim=-1)
            predicted_tokens = [reverse_vocab_mapping[id.item()] for id in predicted_ids[0][1:] if id.item() in reverse_vocab_mapping]
            predicted_sequence = &quot;&quot;.join(predicted_tokens)

            target_ids = ml_eval_batch.view(-1)
            target_tokens = [reverse_vocab_mapping[id.item()] for id in target_ids if id.item() in reverse_vocab_mapping]
            target_sequence = &quot;&quot;.join(target_tokens)

            print(&quot;Actual: &quot;, target_sequence)
            print(&quot;Predicted: &quot;, predicted_sequence + '\n')
</code></pre>
<p>I tried different techniques to preprocess efficiently to make the transformer learn something, just something even some characters or words related to the language which might not be the translation, but at least we can ensure that the model is attending to the language.</p>
<p>But it is not attending to the language, but the dots and padding tokens,</p>
<p>I'm not sure if there are any issues with the architecture or data pre-processing. It appears that something unusual is happening either in the theory or the code. If you could offer any assistance in resolving this matter, it would be greatly appreciated.</p>
<p>Thank You</p>
","transformer-model"
"77231159","RuntimeError: The expanded size of the tensor (10) must match the existing size (11) at non-singleton dimension 1","2023-10-04 15:59:38","","0","700","<python><python-3.x><pytorch><transformer-model>","<p>I am new to implementing transformer model in Pytorch. I have difficulty in implementing the positonal encoder for transformer model. I have input data with shape (128,7,21) where 128 is the batch size, 7 is number of records, and 21 is number of features. The Python code for the positional encoder is written as follow:</p>
<pre><code>class PositionalEncoder(nn.Module):
   def __init__(self, d_model: int, max_seq_len: int=7):
       super(PositionalEncoder, self).__init__()
       self.d_model = d_model

       # Create positional encoding matrix
       pe = self.positional_encoding(max_seq_len, d_model)
       self.register_buffer('pe', pe)

   def positional_encoding(self, max_seq_len, d_model):
       position = torch.arange(0, max_seq_len).unsqueeze(1).float()
       div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))
       pe = torch.zeros(max_seq_len, d_model)
       pe[:, 0::2] = torch.sin(position * div_term)
       pe[:, 1::2] = torch.cos(position * div_term)
       return pe

   def forward(self, x):
       # Calculate the sequence length from the input tensor
       seq_len = x.size(1)

       # Add positional encoding to the input along the sequence dimension
       x = x + self.pe[:, :seq_len]
       return x
</code></pre>
<p>When I execute the above code : <code>pos_encoder = PositionalEncoder(d_model=21)</code>, there is <strong>RuntimeError: The expanded size of the tensor (10) must match the existing size (11) at non-singleton dimension 1.  Target sizes: [7, 10].  Tensor sizes: [7, 11]</strong></p>
<p>The runtime error is generated after <code>pe[:, 1::2] = torch.cos(position * div_term)</code> was executed.</p>
<p>Therefore, how do I solve the runtime error? and I am not sure about what should be the max_seq_len to be set? currently, I am using max_seq_len=7.</p>
<p>Thanks.</p>
","transformer-model"
"77230870","Compatibility of transformers version 4.11.1 with Python 3.11","2023-10-04 15:21:05","","1","139","<python><python-3.x><huggingface-transformers><transformer-model>","<p>I have an issue, while installing transformer version 4.11.1 library. Has anyone encountered it before? It seems like Python library error to me ...</p>
<pre><code>    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 10.8 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 14.0 MB/s eta 0:00:00
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 880.6/880.6 kB 19.1 MB/s eta 0:00:00
 Preparing metadata (setup.py) ... done
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.7/212.7 kB 18.7 MB/s eta 0:00:00
 Installing build dependencies ... done
 Getting requirements to build wheel ... done
 Preparing metadata (pyproject.toml) ... done
 error: subprocess-exited-with-error
 
 × Building wheel for tokenizers (pyproject.toml) did not run successfully.
 │ exit code: 1
 ╰─&gt; See above for output.
 
 note: This error originates from a subprocess, and is likely not a problem with pip.
 Building wheel for tokenizers (pyproject.toml) ... error
 ERROR: Failed building wheel for tokenizers
 Building wheel for sacremoses (setup.py) ... done
ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects
</code></pre>
","transformer-model"
"77226837","How to set the window size for video swin transformer?","2023-10-04 04:53:25","77228013","0","443","<pytorch><huggingface-transformers><transformer-model><pytorch-lightning>","<p>I am confused with the window size setting for the video Swin transformer. I have a data input with the shape <code>(200, 8, 8)</code>, the height and width are 8, and the number of <code>single channel</code> frames is 200. I need each patch to have a dimension of <code>1x1x10</code>.</p>
<p>I refer to <a href=""https://github.com/haofanwang/video-swin-transformer-pytorch/tree/main"" rel=""nofollow noreferrer"">this code</a>, and I created a dummy data <code>dummy_x = torch.rand(1, 1, 200, 8, 8)</code>. The swin transformer that I set is the following tiny swin blocks.</p>
<pre><code>model = SwinTransformer3D(pretrained=None,
                 pretrained2d=True,
                 patch_size=(10,1,1),
                 in_chans=1,
                 embed_dim=96,
                 depths=[2, 2, 6, 2],
                 num_heads=[3, 6, 12, 24],
                 window_size=(20,7,7),
                 mlp_ratio=4.,
                 qkv_bias=True,
                 qk_scale=None,
                 drop_rate=0.,
                 attn_drop_rate=0.,
                 drop_path_rate=0.2,
                 norm_layer=torch.nn.LayerNorm,
                 patch_norm=True,
                 frozen_stages=-1,
                 use_checkpoint=False)
</code></pre>
<p>How to set the window size for the depth (frames)?</p>
<p>For example, window size is set to <code>(7,7)</code> in 2D swin swin. What the number 7 is indicating here? And how to set the window size for the 3D Swin transformer.
In my settings, I set it as <code>(20,7,7)</code>. Is this correct? how it should be?</p>
","transformer-model"
"77219095","How much computation should src_key_padding_mask reduce compared to no mask?","2023-10-03 00:35:31","","0","170","<pytorch><transformer-model><inference><onnxruntime>","<p>I have trained two transformer models in PyTorch and I export them to ONNX where I am doing an inference session. The only difference is that when I export the ONNX model in which I wish to use the transformer's <code>src_key_padding_mask</code> input (in <code>.forward()</code>) , I specify it. Example:</p>
<p>#1 (no mask):
<code>torch.onnx.export(model.cpu(), X.view(1,MAXSEQLEN), &quot;model.onnx&quot;, verbose=False, input_names=['model_inputs'], output_names=['model_outputs'])</code></p>
<p>vs</p>
<p>#2 (with an additional mask input): <code>torch.onnx.export(model.cpu(), (X.view(1,MAXSEQLEN), X_pad_mask.view(1,MAXSEQLEN)), &quot;model-with-padmask.onnx&quot;, verbose=False, input_names=['model_inputs', 'padding_mask'], output_names=['model_outputs'])</code></p>
<p>The issue is that I am not seeing any difference in inference time between the two when I use the padding mask. I can run a test where I sweep the amount of padding in the input from almost full padding to no padding (and obviously use the appropriate mask, where True indicates a corresponding padding token). Expected behavior would be that as I am increasing the amount of padding/masking, inference will execute faster because more of the computation should be getting masked out.</p>
<p>Note when I try exporting I do get a warning--
<code>C:\Users\&lt;user&gt;\Miniconda3\envs\py10\lib\site-packages\torch\onnx\utils.py:620: UserWarning: ONNX Preprocess - Removing mutation from node aten::masked_fill_ on block input: 'src_key_padding_mask'. This changes graph semantics. (Triggered internally at ..\torch\csrc\jit\passes\onnx\remove_inplace_ops_for_onnx.cpp:355.) _C._jit_pass_onnx_remove_inplace_ops_for_onnx(graph, module)</code>
which I can't quite understand.</p>
<hr />
<p>Additional info after more testing today...</p>
<p>I decided to compare three quantities. Sweep max sequence length, and observe the inference times under the following three conditions:</p>
<ul>
<li>(i) feeding both <code>x</code> and <code>x_mask</code> into the model with <code>x_mask</code> all False (ie. the mask input is there, but it is effectively no mask);</li>
<li>(ii) feeding both <code>x</code> and <code>x_mask</code> into the model with <code>x_mask</code> half set to true (ie. mask the second half of <code>x</code> values); and</li>
<li>(iii) ONLY feed <code>x</code> into the model (<code>src_padding_mask</code> = None).</li>
</ul>
<p>In experiments (i) and (ii) inference latencies are roughly similar. In experiment (iii), latencies are roughly similar for smaller lengths, but start to become faster for sequences of larger length... I would have expected the converse to be true: when not using any padding mask, this would hurt latency especially at longer lengths...</p>
","transformer-model"
"77198039","The maximum value of input_ids (50257) must be smaller than the embedding layer's input dimension (50257)","2023-09-28 20:21:36","","0","284","<python><pandas><tensorflow><transformer-model><gpt-2>","<p>I am trying to create a categorical model that produces one of the three result for some data input: True, False, Invalid.</p>
<p>as mentioned in the title I get this error when I try to evaluate the model, here is a code in the
model_tokenizer.py file:</p>
<pre class=""lang-py prettyprint-override""><code>
from transformers import GPT2Tokenizer
import torch
import pandas as pd
import sys


def convert_to_df(file:str):

    df = pd.read_excel(file)
    df.dropna(subset=['Data'],how='all',inplace=True)
    return df


def tokenize_and_encode(df, file_name):


    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    gpt2_tokenizer.add_special_tokens({&quot;pad_token&quot;: &quot;[PAD]&quot;})


    df['Data'].fillna('', inplace=True)

    df['data_tokens'] = df['Data'].apply(
        lambda data: gpt2_tokenizer.tokenize(data))


    df['data_codded'] = df['data_tokens'].apply(
        lambda content_token: gpt2_tokenizer.encode_plus(content_token,
        return_tensors='tf', return_attention_mask=True,max_length=1024, truncation=True,padding='max_length')
    )




    df.to_excel(file_name)
    return df


pd_obj = convert_pandas('datasets/somefile.xlsx') 

data = tokenize_and_encode(pd_obj, 'outputfile.xlsx')

</code></pre>
<p>the code above successfully tokenize and encode the data</p>
<p>the 2nd file is responsible for creating the actual model and training it, in <code>model_creation.py</code> file:</p>
<pre class=""lang-py prettyprint-override""><code>
import tensorflow as tf
import pandas as pd
from transformers import TFGPT2Model
from model_tokenizer import data
import numpy as np

def model_setup(df,):


    df['data_input_ids'] = df['data_codded'].apply(lambda data:data['input_ids'])
    df['data_atten_mask'] = df['data_codded'].apply(lambda data:data['attention_mask'])


    df.to_excel('dataset.xlsx')

    return np.array(df['data_input_ids'].tolist()),np.array(df['data_atten_mask'].tolist())



def data_model(*args):
    try:

        gpt_model = TFGPT2Model.from_pretrained('gpt2')
        data_input_ids_shape = tf.shape(args[0])
        data_attention_mask_shape = tf.shape(args[1])

        data_input_ids = tf.reshape(args[0], (data_input_ids_shape[0], data_input_ids_shape[2]))
        data_attention_mask = tf.reshape(args[1], (data_attention_mask_shape[0], data_attention_mask_shape[2]))

       
        data_inputs = {
            'input_ids': data_input_ids,
            'attention_mask': data_attention_mask
        }
        
        data_embedding = gpt_model([data_inputs]).last_hidden_state
        data_embedding = tf.keras.layers.Flatten()(data_embedding)
    



        concatenated_embed = tf.keras.layers.Concatenate()([data_embedding,])

        output = tf.keras.layers.Dense(10, activation=tf.nn.relu)(concatenated_embed)
        output = tf.keras.layers.Dense(3, activation='softmax', name='veracity_layer')(output)

        model = tf.keras.Model(inputs=[data_input_ids, data_attention_mask, ], outputs=output)


        labels = tf.constant([0,1,2],dtype = tf.int32)
        labels = tf.one_hot(labels, depth=3)

        
        features = tf.data.Dataset.from_tensor_slices((
            (args[0],args[1])
    
    ))
        

        labels = tf.data.Dataset.from_tensor_slices((
            labels
        ))

        dataset = tf.data.Dataset.zip((features,labels))

    
        sampling = len(args[0])
        train_size = int(sampling*0.7)
        dataset = dataset.shuffle(buffer_size=samples, seed=40, reshuffle_each_iteration=False)

        train_dataset = dataset.take(train_size)

        test_dataset = dataset.skip(train_size)

        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        validation_steps = len(test_dataset)
        model.fit(train_dataset, epochs=5, validation_data=test_dataset, validation_steps=validation_steps)

        test_loss, test_accuracy = model.evaluate(test_dataset)

        print(f'The loss is : {test_loss}, The Acc is : {test_accuracy}')
    except Exception as e:
        with open('log.txt','w') as log:
            log.write(str(e))



    



data_input_ids, data_atten_masks = model_setup(data)

data_model(data_input_ids,data_atten_masks)



</code></pre>
<p>and the error i get in <code>log.txt</code> is:</p>
<pre><code>
Exception encountered when calling layer 'transformer' (type TFGPT2MainLayer).

The maximum value of input_ids (50257) must be smaller than the embedding layer's input dimension (50257). The likely cause is some problem at tokenization time.
Condition x &lt; y did not hold.
First 3 elements of x:
[25104  5644   326]
First 1 elements of y:
[50257]

Call arguments received by layer 'transformer' (type TFGPT2MainLayer):
  � input_ids={'input_ids': 'tf.Tensor(shape=(20, 1024), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(20, 1024), dtype=int32)'}
  � past_key_values=None
  � attention_mask=None
  � token_type_ids=None
  � position_ids=None
  � head_mask=None
  � inputs_embeds=None
  � encoder_hidden_states=None
  � encoder_attention_mask=None
  � use_cache=True
  � output_attentions=False
  � output_hidden_states=False
  � return_dict=True
  � training=False

</code></pre>
<p>when i tried to figure out at which line the error happen using <code>sys</code> module, it seemed to happen at this line:</p>
<p><code>  data_embedding = gpt_model([data_inputs]).last_hidden_state</code></p>
<p>any help is appreciated.</p>
","transformer-model"
"77196073","installing problem with MMCV (error: subprocess-exited-with-error)","2023-09-28 14:50:59","","1","1633","<deep-learning><computer-vision><object-detection><transformer-model><openmmlab>","<p>I was trying to do some experiment with Pretrained model ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation on google colab. however, i keep getting error from install mmcv. can you save me here?</p>
<pre><code>!git clone https://github.com/open-mmlab/mmcv.git
%cd /content/mmcv
!git status
!git init
!git checkout v1.3.9
!MMCV_WITH_OPS=1 pip install -e .
</code></pre>
<pre><code>Obtaining file:///content/mmcv
  Preparing metadata (setup.py) ... done
Collecting addict (from mmcv-full==1.3.9)
  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)
Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mmcv-full==1.3.9) (1.23.5)
Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from mmcv-full==1.3.9) (9.4.0)
Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from mmcv-full==1.3.9) (6.0.1)
Collecting yapf (from mmcv-full==1.3.9)
  Downloading yapf-0.40.2-py3-none-any.whl (254 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 254.7/254.7 kB 3.7 MB/s eta 0:00:00
Requirement already satisfied: importlib-metadata&gt;=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf-&gt;mmcv-full==1.3.9) (6.8.0)
Requirement already satisfied: platformdirs&gt;=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf-&gt;mmcv-full==1.3.9) (3.10.0)
Requirement already satisfied: tomli&gt;=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf-&gt;mmcv-full==1.3.9) (2.0.1)
Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata&gt;=6.6.0-&gt;yapf-&gt;mmcv-full==1.3.9) (3.16.2)
Installing collected packages: addict, yapf, mmcv-full
  Running setup.py develop for mmcv-full
    error: subprocess-exited-with-error
    
    × python setup.py develop did not run successfully.
    │ exit code: 1
    ╰─&gt; See above for output.
    
    note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

× python setup.py develop did not run successfully.
│ exit code: 1
╰─&gt; See above for output.

note: This error originates from a subprocess, and is likely not a problem with pip.
</code></pre>
<p>is there a way to solve this problem?</p>
","transformer-model"
"77189885","BERT token vs. embedding","2023-09-27 18:03:54","","5","2532","<token><bert-language-model><embedding><transformer-model>","<p>I understand that WordPiece is used to break text into tokens.  And I understand that, somewhere in BERT, the model maps tokens into token embeddings that represent the meaning of the tokens.  But what I don't understand is where this happens.  Does this all happen in the transformer encoder?  Or is there a separate &quot;pre-encoder&quot; that maps the tokens into embeddings before the transformer encoder starts updating the embeddings using its attention scheme?</p>
<p>Well, I mostly just tried reading about BERT.</p>
","transformer-model"
"77183314","how to save and load tranformer models for further training and text generating? PyTorch","2023-09-26 20:47:49","","0","128","<python><deep-learning><pytorch><save><transformer-model>","<p>please tell me how to save and load tranformer models for furher training?</p>
<p>I came across this blog post - <a href=""https://wingedsheep.com/building-a-language-model"" rel=""nofollow noreferrer"">https://wingedsheep.com/building-a-language-model</a></p>
<p>Here is the code from it - <a href=""https://github.com/wingedsheep/transformer"" rel=""nofollow noreferrer"">https://github.com/wingedsheep/transformer</a></p>
<p>I want to be able to save that model after some training to be able to fine-tune, train, and use it for text generation.</p>
<hr />
<p>That's what the author of the post says about it:</p>
<p><strong>Once you trained the model, it is useful if you can save it, so you don't have to train a new model every time.</strong></p>
<p>To do this we add the following code to the LanguageModel class.</p>
<pre class=""lang-py prettyprint-override""><code>def save_checkpoint(self, path):
    print(f'Saving checkpoint {path}')
    torch.save({
        'number_of_tokens': self.number_of_tokens,
        'max_sequence_length': self.max_sequence_length,
        'embedding_dimension': self.embedding_dimension,
        'number_of_layers': self.number_of_layers,
        'number_of_heads': self.number_of_heads,
        'feed_forward_dimension': self.feed_forward_dimension,
        'dropout_rate': self.dropout_rate,
        'model_state_dict': self.state_dict()
    }, path)

@staticmethod
def load_checkpoint(path) -&gt; 'LanguageModel':
    checkpoint = torch.load(path)
    model = LanguageModel(
        number_of_tokens=checkpoint['number_of_tokens'],
        max_sequence_length=checkpoint['max_sequence_length'],
        embedding_dimension=checkpoint['embedding_dimension'],
        number_of_layers=checkpoint['number_of_layers'],
        number_of_heads=checkpoint['number_of_heads'],
        feed_forward_dimension=checkpoint['feed_forward_dimension'],
        dropout_rate=checkpoint['dropout_rate']
    )
    model.load_state_dict(checkpoint['model_state_dict'])
    return model
</code></pre>
<p>Since we use the AutoregressiveWrapper as convenience class, we can give this wrapper the save and load methods too.</p>
<pre class=""lang-py prettyprint-override""><code>def save_checkpoint(self, path):
    self.model.save_checkpoint(path)

@staticmethod
def load_checkpoint(path) -&gt; 'AutoregressiveWrapper':
    model = LanguageModel.load_checkpoint(path)
    return AutoregressiveWrapper(model)
This makes it possible to easily save and load a trained model using.

model.save_checkpoint('./trained_model')
model = model.load_checkpoint('./trained_model')
</code></pre>
<hr />
<p><strong>But this doesn't work quite well</strong></p>
<p>Script returns some error after it loaded created, trained and saved model, and trying to train it again.</p>
<pre><code>RuntimeError: The size of tensor a (257) must match the size of tensor b (256) at non-singleton dimension 1
</code></pre>
<p>I understand, it, maybe, but just a bit, that while training a model, we need to expand it by one singleton dimension, so if it had size of tensor == 256, it will be 257, and if we try to load 257 in our code, that wants to have a size of 256 - we will get an error.</p>
<pre><code>input_tensor.size() = torch.Size([16, 257]), mask_tensor.size() = torch.Size([16, 257])
RuntimeError: The size of tensor a (257) must match the size of tensor b (256) at non-singleton dimension 1
</code></pre>
<p>(+) If you will try the code yourself - it will be nice to run it on cuda. You can look guide how to do it with that code on part called &quot;Running on GPU&quot;.</p>
<p>Maybe we need to save for example a .txt file with number of tensor size to make variables out of it, but I still don't know what to do... <em>Please help me figure it out...</em></p>
","transformer-model"
"77174370","What should be the data structure before tokenizing the text based data for Tranformer-Based Model?","2023-09-25 16:35:34","","1","21","<nlp><transformer-model><gpt-3>","<p>I have seen some examples where people are mostly getting text-based datasets in the form of JSON, but I don't understand the reason why. Can anyone explain why I need to convert my data into a dictionary before tokenizing it for a Tranfomer-based model i.e (BERT, GPT-3, T5... etc). What I have so far learned, we need to know the dataset, and the attention mask will tell us which token needs to be targeted. But what about before tokenizing, even on HuggingFace no one talks about that, all examples gives one sentence example. I think it will be a great help if there is any explanation.</p>
","transformer-model"
"77170944","Which parallelism technique is used in hugging face accelerate by default if we don't specify any technique in accelerate config but use multiple gpus","2023-09-25 08:16:15","","0","62","<python><nlp><transformer-model><huggingface><accelerate>","<p>Which parallelism technique is used in hugging face accelerate by default if we don't specify any technique in accelerate config but use multiple gpus??</p>
<p>Only want to know the default technique used is it data parallel or distributed data parallel</p>
<pre><code>Model = accelerate.prepare(model)
Model = accelerate.device()
</code></pre>
","transformer-model"
"77169181","Increase pytorch transformer nhead on existing trained model","2023-09-24 22:36:55","","0","60","<pytorch><transformer-model>","<p>I have trained a pytorch transformer with 4 heads, training for over a day:</p>
<pre><code>class MyTransformer(nn.Module):
    def __init__(self, d_model, nhead, num_layers, dropout):
        super(MyTransformer, self).__init__()
        self.input_linear = nn.Linear(1, d_model)
        self.transformer = nn.Transformer(d_model, nhead, num_layers, dropout=dropout)

nhead=4
model = MyTransformer(d_model, nhead, num_layers, dropout=dropout)

#do training

torch.save(model, 'model.pth')

</code></pre>
<p>I was wanting to increase the nhead to 8 with the existing trained model in such a way to not start training from scratch and recommence training. Is this possible?</p>
","transformer-model"
"77140711","How to print Grounding DINO model summary","2023-09-20 08:40:53","","0","335","<deep-learning><pytorch><computer-vision><transformer-model><zeroshot-classification>","<p>I'm trying to obtain the model summary of <a href=""https://github.com/IDEA-Research/GroundingDINO/tree/main"" rel=""nofollow noreferrer"">Grounding DINO</a>. I tried to use the <a href=""https://pypi.org/project/torch-summary/"" rel=""nofollow noreferrer"">torch-summary</a> library to do this, but I'm facing problems with specifying the correct input size, thing that is mandatory to call the summarizing function.</p>
<p>Since Grounding DINO is a multi-modal model (it takes as inputs (image, text) pairs), I'm struggling trying to figure out what sort of input size, and with what format, should I pass to the summary function.</p>
<pre class=""lang-py prettyprint-override""><code>from groundingdino.util.inference import load_model
from torchsummary import summary

model = load_model(CONFIG_PATH, WEIGHTS_PATH)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
model = model.to(device)
summary(model, input_size)
</code></pre>
<p>I tried to pass as input_size parameter:</p>
<ul>
<li>just image size (e.g. (3, 244, 244))</li>
<li>a list containing image size and a text prompt (e.g. [(3, 244, 244), 'some text'])</li>
<li>image size extended with another integer that might represent the length of the textual input (e.g. (3, 244, 244, 10))</li>
<li>a list containing image size and an integer that might represent the length of the textual input (e.g. [(3, 244, 244), 10] and [(3, 244, 244), (10,)])</li>
</ul>
<p>but all of the attempts resulted in an error. For example, the one occurred with the first attempt is:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py in summary(model, input_data, batch_dim, branching, col_names, col_width, depth, device, dtypes, verbose, *args, **kwargs)
    139             with torch.no_grad():
--&gt; 140                 _ = model.to(device)(*x, *args, **kwargs)  # type: ignore[misc]
    141         except Exception as e:

3 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used

/content/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py in forward(self, samples, targets, **kw)
    242         if targets is None:
--&gt; 243             captions = kw[&quot;captions&quot;]
    244         else:

KeyError: 'captions'

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-9-f3881fbb51d4&gt; in &lt;cell line: 9&gt;()
      7 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
      8 model = model.to(device)
----&gt; 9 summary(model, (3, 224, 224))

/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py in summary(model, input_data, batch_dim, branching, col_names, col_width, depth, device, dtypes, verbose, *args, **kwargs)
    141         except Exception as e:
    142             executed_layers = [layer for layer in summary_list if layer.executed]
--&gt; 143             raise RuntimeError(
    144                 &quot;Failed to run torchsummary. See above stack traces for more details. &quot;
    145                 &quot;Executed layers up to: {}&quot;.format(executed_layers)

RuntimeError: Failed to run torchsummary. See above stack traces for more details. Executed layers up to: []
</code></pre>
<p>It seems like it expects some keyword input (captions).
I tried to look at the <a href=""https://github.com/IDEA-Research/GroundingDINO/blob/main/groundingdino/util/inference.py#L68"" rel=""nofollow noreferrer"">predictions-related code</a> in the github repo, as well as to the <a href=""https://github.com/IDEA-Research/GroundingDINO/blob/main/groundingdino/models/GroundingDINO/groundingdino.py#L227"" rel=""nofollow noreferrer"">forward method</a> of the model, but I still wasn't able to fix the problem.</p>
<p>The docs of torch-summary also state that it is possible to directly pass input data in place of input size and let the function infer what it needs to print the summary, so I tried the following:</p>
<pre class=""lang-py prettyprint-override""><code>from groundingdino.util.inference import load_image
image_source, image = load_image(IMG_PATH)
caption='some text'
summary(model, image, caption)
</code></pre>
<p>but it generated the error:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py in summary(model, input_data, batch_dim, branching, col_names, col_width, depth, device, dtypes, verbose, *args, **kwargs)
    139             with torch.no_grad():
--&gt; 140                 _ = model.to(device)(*x, *args, **kwargs)  # type: ignore[misc]
    141         except Exception as e:

4 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)
   1500                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501             return forward_call(*args, **kwargs)
   1502         # Do not call functions when jit is used

/content/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py in forward(self, samples, targets, **kw)
    244         else:
--&gt; 245             captions = [t[&quot;caption&quot;] for t in targets]
    246 

/content/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py in &lt;listcomp&gt;(.0)
    244         else:
--&gt; 245             captions = [t[&quot;caption&quot;] for t in targets]
    246 

TypeError: string indices must be integers

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-7-a5f3a38c6e5a&gt; in &lt;cell line: 9&gt;()
      7 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
      8 model = model.to(device)
----&gt; 9 summary(model, image, TEXT_PROMPT)

/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py in summary(model, input_data, batch_dim, branching, col_names, col_width, depth, device, dtypes, verbose, *args, **kwargs)
    141         except Exception as e:
    142             executed_layers = [layer for layer in summary_list if layer.executed]
--&gt; 143             raise RuntimeError(
    144                 &quot;Failed to run torchsummary. See above stack traces for more details. &quot;
    145                 &quot;Executed layers up to: {}&quot;.format(executed_layers)

RuntimeError: Failed to run torchsummary. See above stack traces for more details. Executed layers up to: []
</code></pre>
<p><code>summary(model, {'image':image, 'captions':[caption]})</code> generated instead the error:</p>
<pre class=""lang-py prettyprint-override""><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-10-5139855142c9&gt; in &lt;cell line: 9&gt;()
      7 device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
      8 model = model.to(device)
----&gt; 9 summary(model, {'image':image, 'captions':[caption]})

1 frames
/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py in summary(model, input_data, batch_dim, branching, col_names, col_width, depth, device, dtypes, verbose, *args, **kwargs)
    134             device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    135 
--&gt; 136         x, input_size = process_input_data(input_data, batch_dim, device, dtypes)
    137         args, kwargs = set_device(args, device), set_device(kwargs, device)
    138         try:

/usr/local/lib/python3.10/dist-packages/torchsummary/torchsummary.py in process_input_data(input_data, batch_dim, device, dtypes)
    217 
    218     else:
--&gt; 219         raise TypeError(
    220             &quot;Input type is not recognized. Please ensure input_data is valid.\n&quot;
    221             &quot;For multiple inputs to the network, ensure input_data passed in is &quot;

TypeError: Input type is not recognized. Please ensure input_data is valid.
For multiple inputs to the network, ensure input_data passed in is a sequence of tensors or a list of tuple sizes. If you are having trouble here, please submit a GitHub issue.
</code></pre>
<p>So, my question is, how can I find the proper input size and format to pass to the summary function? Or, more in general, how can I obtain the summary of such model? (Not necessarily using torch-summary, but I need the same information obtainable through this lib).</p>
<p>Thanks in advance to whoever will be able to help me with this problem.</p>
<p>P.S.
I'm not sure whether it can help, but here is the output of <code>print(model)</code>:</p>
<pre class=""lang-py prettyprint-override""><code>GroundingDINO(
  (transformer): Transformer(
    (encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-5): 6 x DeformableTransformerEncoderLayer(
          (self_attn): MultiScaleDeformableAttention(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Dropout(p=0.0, inplace=False)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout2): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout3): Dropout(p=0.0, inplace=False)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (text_layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (linear1): Linear(in_features=256, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (linear2): Linear(in_features=1024, out_features=256, bias=True)
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.0, inplace=False)
          (dropout2): Dropout(p=0.0, inplace=False)
        )
      )
      (fusion_layers): ModuleList(
        (0-5): 6 x BiAttentionBlock(
          (layer_norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (layer_norm_l): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): BiMultiHeadAttention(
            (v_proj): Linear(in_features=256, out_features=1024, bias=True)
            (l_proj): Linear(in_features=256, out_features=1024, bias=True)
            (values_v_proj): Linear(in_features=256, out_features=1024, bias=True)
            (values_l_proj): Linear(in_features=256, out_features=1024, bias=True)
            (out_v_proj): Linear(in_features=1024, out_features=256, bias=True)
            (out_l_proj): Linear(in_features=1024, out_features=256, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.100)
        )
      )
    )
    (decoder): TransformerDecoder(
      (layers): ModuleList(
        (0-5): 6 x DeformableTransformerDecoderLayer(
          (cross_attn): MultiScaleDeformableAttention(
            (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)
            (attention_weights): Linear(in_features=256, out_features=128, bias=True)
            (value_proj): Linear(in_features=256, out_features=256, bias=True)
            (output_proj): Linear(in_features=256, out_features=256, bias=True)
          )
          (dropout1): Identity()
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (ca_text): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (catext_dropout): Identity()
          (catext_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (dropout2): Identity()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (linear1): Linear(in_features=256, out_features=2048, bias=True)
          (dropout3): Identity()
          (linear2): Linear(in_features=2048, out_features=256, bias=True)
          (dropout4): Identity()
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (ref_point_head): MLP(
        (layers): ModuleList(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Linear(in_features=256, out_features=256, bias=True)
        )
      )
      (bbox_embed): ModuleList(
        (0-5): 6 x MLP(
          (layers): ModuleList(
            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
            (2): Linear(in_features=256, out_features=4, bias=True)
          )
        )
      )
      (class_embed): ModuleList(
        (0-5): 6 x ContrastiveEmbed()
      )
    )
    (tgt_embed): Embedding(900, 256)
    (enc_output): Linear(in_features=256, out_features=256, bias=True)
    (enc_output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (enc_out_bbox_embed): MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
    (enc_out_class_embed): ContrastiveEmbed()
  )
  (bert): BertModelWarper(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (feat_map): Linear(in_features=768, out_features=256, bias=True)
  (input_proj): ModuleList(
    (0): Sequential(
      (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (1): Sequential(
      (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (2): Sequential(
      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
    (3): Sequential(
      (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      (1): GroupNorm(32, 256, eps=1e-05, affine=True)
    )
  )
  (backbone): Joiner(
    (0): SwinTransformer(
      (patch_embed): PatchEmbed(
        (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
      )
      (pos_drop): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): Identity()
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=96, out_features=288, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=96, out_features=96, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.018)
              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=96, out_features=384, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=384, out_features=96, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=384, out_features=192, bias=False)
            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          )
        )
        (1): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.036)
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=192, out_features=576, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=192, out_features=192, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.055)
              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=192, out_features=768, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=768, out_features=192, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=768, out_features=384, bias=False)
            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
        (2): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.073)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.091)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (2): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.109)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (3): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.127)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (4): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.145)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (5): SwinTransformerBlock(
              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=384, out_features=1152, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=384, out_features=384, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.164)
              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=384, out_features=1536, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=1536, out_features=384, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (downsample): PatchMerging(
            (reduction): Linear(in_features=1536, out_features=768, bias=False)
            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
          )
        )
        (3): BasicLayer(
          (blocks): ModuleList(
            (0): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.182)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
            (1): SwinTransformerBlock(
              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (attn): WindowAttention(
                (qkv): Linear(in_features=768, out_features=2304, bias=True)
                (attn_drop): Dropout(p=0.0, inplace=False)
                (proj): Linear(in_features=768, out_features=768, bias=True)
                (proj_drop): Dropout(p=0.0, inplace=False)
                (softmax): Softmax(dim=-1)
              )
              (drop_path): DropPath(drop_prob=0.200)
              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (mlp): Mlp(
                (fc1): Linear(in_features=768, out_features=3072, bias=True)
                (act): GELU(approximate='none')
                (fc2): Linear(in_features=3072, out_features=768, bias=True)
                (drop): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (1): PositionEmbeddingSineHW()
  )
  (bbox_embed): ModuleList(
    (0-5): 6 x MLP(
      (layers): ModuleList(
        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=4, bias=True)
      )
    )
  )
  (class_embed): ModuleList(
    (0-5): 6 x ContrastiveEmbed()
  )
)
</code></pre>
","transformer-model"
"77101152","Understanding Image Resolution in Intermediate Layers of DINO Model (ViT)","2023-09-14 00:31:40","","0","76","<deep-learning><computer-vision><conv-neural-network><transformer-model>","<p>I've been experimenting with the <a href=""https://github.com/facebookresearch/dino"" rel=""nofollow noreferrer"">DINO</a> model from Facebook Research and am trying to understand some specifics about its architecture. I'm aware that the input images are resized to a dimension of 224x224 before being processed by the model.</p>
<p>My main question revolves around the resolution of these images when they reach the intermediate layers, particularly layer 9. Does anyone know the resolution of the images at this layer or any other layers within the model?</p>
<p>Any insights or references would be greatly helpful. Thank you in advance!</p>
","transformer-model"
"77078717","TypeError: TransformerBatchNormEncoderLayer.forward() got an unexpected keyword argument 'is_causal'","2023-09-11 03:06:00","","-1","928","<amazon-web-services><ubuntu><conda><transformer-model>","<p>I am trying to run the github - gzerveas mvts_transformer
After jupyter install -r requirement.txt, it show this error and I do not know how to fix.</p>
<p>I've ask GTP and it suggest this question might due to different version of the environment. So, I opened a terminal in AWS and try to create a new environment based on other similar studies environment. MTSIT-yarkin06-venv.yml(for conda)</p>
<p>But I am not sure whether my solution is right? Coz acutally I just want to fix that 'is_causal' key words.
By the way I also try to find 'is_causual' in github, but I didn't find. Is there a place that I can search all the code in that github program? I appriciate if could provide picutres, as a new learner I am not familiar with the Specific words.
And I am not sure whether the teriminal is keep runing.
Lastly I am not sure when it can finnaly finished preparing, it is that because I am using VPN, or most person will be slow at this stage. Coz I feel I am using AWS-SageMaker and it should not be slow due to my using of VPN.</p>
<p><img src=""https://i.sstatic.net/EXCXs.jpg"" alt=""https://i.sstatic.net/EXCXs.jpg"" />.</p>
","transformer-model"
"77077317","CLIP's Visual Transformer image encoder output","2023-09-10 17:54:57","","0","416","<deep-learning><computer-vision><transformer-model><sentence-transformers><openai-clip>","<p>I was doing some experiments with the CLIP's visual transformer encoder output (<code>clip-ViT-B-32</code>). So basically given the same scene or image, it should output almost same image feature vector given it's a semantics model. But looks like it is very sensitive to illumination and lighting conditions which makes me wonder and the percentage of similarity between the images below are much lower than expected (surprisingly it says 89.45% similar)</p>
<p>Why is that? Is there any ways/models which are less sensitive to illumination changes and are more semantic based?</p>
<pre><code>from sentence_transformers import SentenceTransformer, util
#......
model = SentenceTransformer('clip-ViT-B-32')
encoded_image = model.encode(image, batch_size=128, convert_to_tensor=True, show_progress_bar=True)

# Now we run the clustering algorithm. This function compares images aganist 
# all other images and returns a list with the pairs that have the highest 
# cosine similarity score
processed_images = util.paraphrase_mining_embeddings(encoded_image)
</code></pre>
<p><a href=""https://i.sstatic.net/21a9Im.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/21a9Im.png"" alt=""enter image description here"" /></a> <a href=""https://i.sstatic.net/conHNm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/conHNm.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"77075228","I cannot resolve the error that model.safetensor is not found in execution of my code?","2023-09-10 07:56:05","","0","1157","<python><transformer-model><stable-diffusion><image-generation><safe-tensors>","<p>I can not resolve an error in my code that is to generate an image via text and it keeps on giving an error that model.safetensor not found or unet\diffusion_pytorch_model.safetensors not found and I've tried to download those files and tried to put it in several directries where it could use it but then also it is not executable.</p>
<pre><code> from auth_token import auth_token
    from fastapi import FastAPI, Response
    from fastapi.middleware.cors import CORSMiddleware
    import torch
    from torch import autocast
    from diffusers import StableDiffusionPipeline
    from io import BytesIO
    import base64 
    
    
    app = FastAPI()
    
    app.add_middleware(
        CORSMiddleware, 
        allow_credentials=True, 
        allow_origins=[&quot;*&quot;], 
        allow_methods=[&quot;*&quot;], 
        allow_headers=[&quot;*&quot;]
    )
    
    device = &quot;cuda&quot;
    model_id = &quot;CompVis/stable-diffusion-v1-4&quot;
    pipe = StableDiffusionPipeline.from_pretrained(model_id, revision=&quot;fp16&quot;, torch_dtype=torch.float16, use_auth_token=auth_token)
    pipe.to(device)
    
    @app.get(&quot;/&quot;)
    def generate(prompt: str): 
        with autocast(device): 
            image = pipe(prompt, guidance_scale=8.5).images[0]
    
        image.save(&quot;testimage.png&quot;)
        buffer = BytesIO()
        image.save(buffer, format=&quot;PNG&quot;)
        imgstr = base64.b64encode(buffer.getvalue())
    
        return Response(content=imgstr, media_type=&quot;image/png&quot;)
</code></pre>
","transformer-model"
"77068054","Training difficulties on Transformer seq2seq task using pytorch","2023-09-08 15:14:50","77069904","1","339","<machine-learning><pytorch><nlp><transformer-model><formal-languages>","<p>I am currently employing a seq2seq task using the vanilla <code>torch.nn.Transformer</code>.
My implementation is provided below (SimpleTransformer). I just seem to not be able to make my model output non-trivial sequences and also loss doesn't seem to shrink after flattening. Maybe some more experienced ML researcher could tell me what else to try and what their opinions on those results seem to be?</p>
<h1>Task:</h1>
<p>The task is the following:</p>
<p>Given a sentence belonging to a defined formal grammar give me the parse tree representation of that sentence. E.g:</p>
<ul>
<li>I like apples. -&gt; (S (NP <strong>I</strong>) (VP <strong>like apples</strong>))</li>
<li>John says he likes apples -&gt; (S (NP <strong>John</strong>) (VP <strong>says</strong> (S (NP <strong>he</strong>) (VP <strong>likes apples</strong>))))</li>
</ul>
<p>Brackets will refer to a certain Non-Terminal rule (i.e. a grammatical object / POS).</p>
<p>Each opening and closing bracket type will be a single token. (e.g. S-opening, S-closing, NP-opening, NP-closing, ..., will be seperate tokens). The text will be tokenized using Byte-Pair-Encoding.</p>
<h1>Training params</h1>
<p>I trained my model using Cross-Entropy-Loss, AdamW (0.9, 0.98), learning rates of magnitued e-4 and e-5, with a max token size of 512, a batch size of 4 (as GPU memory wasn't big enough for more), with an data-set of ~70 000 example sentences over 3-4 epochs.</p>
<h1>Results</h1>
<ul>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/275k4oi0"" rel=""nofollow noreferrer"">My loss curve looks like this.</a></li>
<li><a href=""https://wandb.ai/whatfuckingever/Tree%20Bracketing/reports/f1-23-09-08-16-30-50---Vmlldzo1MzQ1MTA5"" rel=""nofollow noreferrer"">My F1 scores during training
like this.</a></li>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/mv6t7cwc"" rel=""nofollow noreferrer"">My evaluation loss is much higher (Note that
evaluation examples will be generally longer than training set and
will be sorted by length)</a></li>
</ul>
<h1>My model:</h1>
<pre><code>class SimpleTransformer(nn.Module):
def __init__(self, vocab_size: int, ntokens=512, d_model=512, num_layers=6, bidirectional=False, device=&quot;cpu&quot;):
    super().__init__()
    self.d_model = d_model
    self.src_embed = nn.Embedding(vocab_size, self.d_model)
    self.tgt_embed = nn.Embedding(vocab_size, self.d_model)
    self.positional_encoder = PositionalEncoding(d_model=self.d_model, max_len=ntokens)
    self.model = Transformer(d_model=self.d_model, batch_first=True, num_encoder_layers=num_layers,
                             num_decoder_layers=num_layers)
    self.bidirectional = bidirectional
    self.generator = Generator(hidden_size=self.d_model, vocab_size=vocab_size) # Just a fc layer
    self.device = device

def forward(self, in_ids, l_ids, in_masks, l_masks):
    in_ids = self.src_embed(in_ids.long()) * math.sqrt(self.d_model)  # scale by sqrt of dmodel
    in_ids = self.positional_encoder(in_ids)

    l_ids = self.tgt_embed(l_ids.long()) * math.sqrt(self.d_model)
    l_ids = self.positional_encoder(l_ids)

    # Create Masks
    src_seq_len = in_ids.size(1)
    tgt_seq_len = l_ids.size(1)
    src_mask = torch.zeros(src_seq_len, src_seq_len, device=self.device).type(torch.bool)
    if not self.bidirectional:
        tgt_mask = torch.triu(torch.full((tgt_seq_len, tgt_seq_len), float('-inf'), device=self.device), diagonal=1)
    else:
        tgt_mask = torch.zeros(tgt_seq_len, tgt_seq_len, device=self.device).type(torch.bool)
    in_masks = in_masks == 0.0 # in_masks will mask special pad_tokens
    l_masks = l_masks == 0.0 # l_masks will mask special pad_tokens

    out = self.model(src=in_ids, tgt=l_ids,
                     src_mask=src_mask, tgt_mask=tgt_mask,
                     src_key_padding_mask=in_masks,
                     tgt_key_padding_mask=l_masks)
    return self.generator(out)
</code></pre>
<p>I tried with</p>
<ul>
<li>num_layers = 6, 3, 1,</li>
<li>d_model = 512</li>
<li>feed-forward dim: 2048 (default)</li>
<li>PositionalEncoding: either sin/cos or none</li>
</ul>
<p>I saw that the outputs will be like:</p>
<pre><code>(s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s (s ...
</code></pre>
<p>No matter the input... I just don't know whether the task is to difficult to handle or whether I made a mistake somewhere in the process. I can't get my model to output non-trivial patterns. I'm quite new in the machine learning business so it could certainly be that it might be a stupid mistake somewhere. Maybe I stopped too early with the training or some hyper-parameters are wrong?</p>
<h1>Comparative Attempts</h1>
<p>Additionally, I used the <code>facebook/fairseq</code> toolkit to train a model on the same task. It's performance was better with loss decreasing significantly:</p>
<ul>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/nl2qwp1t"" rel=""nofollow noreferrer"">Train loss</a></li>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/hl6l0p4i"" rel=""nofollow noreferrer"">Validation Perplexity</a></li>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/q1jwji58"" rel=""nofollow noreferrer"">Validation loss</a></li>
</ul>
<p>I also trained the pre-trained <code>bart-base</code> Model from the <code>huggingface</code> library with the identical training script and training data. It's performce was way better than both of the previous models.</p>
<ul>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/4zpkleru"" rel=""nofollow noreferrer"">Train loss</a> (Valid loss is also around 0.025)</li>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/ro3fjk2b"" rel=""nofollow noreferrer"">Train F1</a></li>
<li><a href=""https://api.wandb.ai/links/whatfuckingever/3avqmhy4"" rel=""nofollow noreferrer"">Eval F1</a></li>
</ul>
","transformer-model"
"77067838","Keras Transformers - Dimensions must be equal","2023-09-08 14:42:33","77124760","1","217","<python><keras><transformer-model>","<p>I wanted to do NER with keras model using transformers. The example was working correctly but I wanted to add some context to each words in order to help the model being more accurate. What I mean by context is &quot;coordinate X&quot;, &quot;coordinate Y&quot;, &quot;width of the word&quot;, &quot;height of the word&quot;, &quot;page index&quot;, ... For example some informations are usually on the top right corner of a document so having the coordinate of the word might help (I'm new to ML so feel free to tell me I'm wrong if it's the case).</p>
<p>In order to have this &quot;context&quot; I've transformed the <code>x_train</code> and <code>x_val</code> in this format:</p>
<pre><code>[
    [
        [pageIndex, wordVocabId, x, y, width, height, ocrScore],
        [pageIndex, wordVocabId, x, y, width, height, ocrScore],
        ...
    ],
    [
        [pageIndex, wordVocabId, x, y, width, height, ocrScore],
        [pageIndex, wordVocabId, x, y, width, height, ocrScore],
        ...
    ],
    ...
]
</code></pre>
<p>Where each array of 2nd level represent a document and each array of 3nd level represent a word with its context.
The 3nd level array is a numpy array of numbers.</p>
<p>Even if I tried to edit the model to make it working I don't think I went in the right direction so I'll post here the model from the example of keras that I try to use and that I would like to adapt to my usecase:</p>
<pre class=""lang-py prettyprint-override""><code>    class TransformerBlock(layers.Layer):
        def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
            super().__init__()
            self.att = keras.layers.MultiHeadAttention(
                num_heads=num_heads, key_dim=embed_dim
            )
            self.ffn = keras.Sequential(
                [
                    keras.layers.Dense(ff_dim, activation=&quot;relu&quot;),
                    keras.layers.Dense(embed_dim),
                ]
            )
            self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)
            self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)
            self.dropout1 = keras.layers.Dropout(rate)
            self.dropout2 = keras.layers.Dropout(rate)

        def call(self, inputs, training=False):
            attn_output = self.att(inputs, inputs)
            attn_output = self.dropout1(attn_output, training=training)
            out1 = self.layernorm1(inputs + attn_output)
            ffn_output = self.ffn(out1)
            ffn_output = self.dropout2(ffn_output, training=training)
            return self.layernorm2(out1 + ffn_output)
        

    class TokenAndPositionEmbedding(layers.Layer):
        def __init__(self, maxlen, vocab_size, embed_dim):
            super().__init__()
            self.token_emb = keras.layers.Embedding(
                input_dim=vocab_size, output_dim=embed_dim
            )
            self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

        def call(self, inputs):
            maxlen = tf.shape(inputs)[-1]
            positions = tf.range(start=0, limit=maxlen, delta=1)
            position_embeddings = self.pos_emb(positions)
            token_embeddings = self.token_emb(inputs)
            return token_embeddings + position_embeddings

    class NERModel(keras.Model):
        def __init__(
            self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32
        ):
            super().__init__()
            self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)
            self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
            self.dropout1 = layers.Dropout(0.1)
            self.ff = layers.Dense(ff_dim, activation=&quot;relu&quot;)
            self.dropout2 = layers.Dropout(0.1)
            self.ff_final = layers.Dense(num_tags, activation=&quot;softmax&quot;)

        def call(self, inputs, training=False):
            x = self.embedding_layer(inputs)
            x = self.transformer_block(x)
            x = self.dropout1(x, training=training)
            x = self.ff(x)
            x = self.dropout2(x, training=training)
            x = self.ff_final(x)
            return x
</code></pre>
<p>Source: <a href=""https://keras.io/examples/nlp/ner_transformers/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/ner_transformers/</a></p>
<p>I try to compile and fit this way:</p>
<pre class=""lang-py prettyprint-override""><code>    print(len(tag_mapping), vocab_size, len(x_train), len(y_train))
    model = NERModel(len(tag_mapping), vocab_size, embed_dim=32, num_heads=4, ff_dim=64)
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(tf.convert_to_tensor(x_train), tf.convert_to_tensor(y_train), validation_data=(x_val, y_val), epochs=10)
    model.save(&quot;model.keras&quot;)
</code></pre>
<p>The result of the <code>print</code> is (I have only 3 tags for now because I first try to make the model working):</p>
<pre><code>3 20000 1000 1000
</code></pre>
<p>The format of my <code>y_train</code> is the follow:</p>
<pre><code>[
    [tagId_document1_word1, tagId_document1_word2, ...],
    [tagId_document2_Word1, tagId_document2_word1, ...]
]
</code></pre>
<p>When I run <code>model.fit</code> I have this error:</p>
<pre><code> ValueError: Dimensions must be equal, but are 516 and 7 for '{{node Equal}} = Equal[T=DT_FLOAT, incompatible_shape_error=true](Cast_1, Cast_2)' with input shapes: [?,516], [?,516,7].
</code></pre>
<p>I hope with all these informations someone can pin me in the right direction because I'm a bit lost here.</p>
<p>Thank you.</p>
","transformer-model"
"77065093","Doubts regarding ELECTRA Paper Implementation","2023-09-08 07:55:14","77130138","-1","36","<bert-language-model><transformer-model><large-language-model>","<p>I am a master's student currently studying NLP. I was reading the ELECTRA paper by Clark et al. I had a few doubts regarding the implementation and training.</p>
<p>I was wondering if you could help me with those.</p>
<ol>
<li>What exactly does the &quot;Step&quot; mean in step count? Does it mean 1 epoch or 1 minibatch?</li>
<li>Also, in the paper I saw (specifically in Table 1), ELECTRA-SMALL and BERT-SMALL both have 14M parameters, how is that possible as ELECTRA should have more parameters because its generator and discriminator module are both BERT-based?</li>
<li>Also, what is the architecture of both the generator and discriminator? Are they both BERT to something else?</li>
<li>Also, we have a sampling step between the generator and the discriminator. How are you back-propagating the gradients through this?</li>
</ol>
<p>Thanks in advance</p>
<p>Well, I tried looking online for answers, but they were not cconclusive. Regarding backpropagating the gradients, i think the gradients in discriminator are not backpropagated to the generator , both are trained separately, although the generated input of current step is put as input to the discriminator.</p>
","transformer-model"
"77061929","Multithreaded Inferences on LLM using GPU's","2023-09-07 18:07:23","","0","880","<machine-learning><pytorch><huggingface-transformers><transformer-model><large-language-model>","<p>If I call the LLM inference (<code>infer()</code> in this case) parallely using multiple threads on a single instance of a model (which consumes all the GPU's), will that work?</p>
<p>Code:</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

checkpoint = &quot;WizardLM/WizardCoder-15B-V1.0&quot;
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;  # &quot;cuda:X&quot; for GPU usage or &quot;cpu&quot; for CPU usage


class Model:
    def __init__(self):
        print(&quot;Running in &quot; + device)
        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)
        self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')

    def infer(self, input_text, token_count):
        inputs = self.tokenizer.encode(input_text, return_tensors=&quot;pt&quot;).to(device)
        outputs = self.model.generate(inputs, max_new_tokens=token_count)
        return self.tokenizer.decode(outputs[0])
</code></pre>
<p>Why I am asking this is as: During the minute long inferences (p3.8x-large EC2) I observe that although GPU memory is consumed fully but the % utilisation is low.</p>
<p>Edit: Calling infer() for same object of Model works and GPU usage is also high, but I am still unclear that if: tokenizer/model are threadsafe?</p>
<p>I found this: <a href=""https://github.com/huggingface/diffusers/issues/3672"" rel=""nofollow noreferrer"">https://github.com/huggingface/diffusers/issues/3672</a>
But this does not confirm anything.</p>
","transformer-model"
"77061898","Incomplete Output with LLM with max_new_tokens","2023-09-07 18:02:00","","2","2576","<machine-learning><huggingface-transformers><transformer-model><huggingface><large-language-model>","<p>I am experimenting with Huggingface LLM models.</p>
<p>And one issue I noticed is that output of the model ends abruptly and I ideally want it to complete the paragraph/sentences/code which it was it between of. (or altogether try to complete the answer within some fixed num of tokens)</p>
<p>Although I have provided max_new_tokens = 300 and also in prompt I write:
&quot;Output should be maximum of 300 words.&quot;</p>
<p>The response is always incomplete and ends abruptly. Any way I can ask for a complete output within desired number of output tokens?</p>
<p>Code:</p>
<pre><code>checkpoint = &quot;HuggingFaceH4/starchat-alpha&quot;
device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; 
class StarCoderModel:
  def __init__(self):
    self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    # make sure `--gpus all` is provided in docker run command if gpu is required
    self.model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map='auto')

  def infer(self, input_text, token_count):
    inputs = self.tokenizer.encode(input_text, return_tensors=&quot;pt&quot;).to(device)
    outputs = self.model.generate(inputs,  max_new_tokens=token_count, pad_token_id=self.tokenizer.eos_token_id)
    return self.tokenizer.decode(outputs[0])[len(input_text):]
</code></pre>
<p>Sample-Output:</p>
<pre><code>private DataType FuntionName(String someId) {
    // TODO: Replace with implementation that utilizes someId to obtain information
    return DataType.Value;
}


The comment:

- If someId is present in the code, use the getAPI from Client with someId as a parameter to obtain some information.
- If the

</code></pre>
","transformer-model"
"77055945","Using the output of the PyTorch Transformer Tutorial","2023-09-07 00:38:09","","0","56","<deep-learning><pytorch><transformer-model>","<p>I want to use the output of the PyTorch transformer tutorial (<a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">tutorial page</a>, <a href=""https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/9cf2d4ead514e661e20d2070c9bf7324/transformer_tutorial.ipynb"" rel=""nofollow noreferrer"">default colab</a>) to do sentence completions.  However, when I try that it just produces nonsense</p>
<pre><code>itos = vocab.get_itos()

def generate_text(model, start_sentence, max_len=50, temperature=1.0):
    &quot;&quot;&quot;
    Generate text using the trained model.
    
    Parameters:
    - model: The trained model
    - start_sentence: The sentence to start the generation
    - max_len: Maximum length of the generated sequence
    - temperature: Determines the randomness of predictions. 
                   Higher values make predictions more random, lower values make it more deterministic.
    
    Returns:
    - generated_text: Generated text
    &quot;&quot;&quot;
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        # Tokenize the starting sentence
        input_tensor = torch.tensor(vocab(tokenizer(start_sentence))).unsqueeze(1).to(device)
        generated_words = [w for w in start_sentence.split()]
        
        # Generation loop
        for _ in range(max_len):
            output = model(input_tensor)
            word_weights = output[-1].squeeze().div(temperature).exp().cpu()
            word_idx = torch.multinomial(word_weights, 1)[0]
            word_tensor = torch.Tensor([[word_idx]]).long().to(device)
            input_tensor = torch.cat([input_tensor, word_tensor], 0)

            # Add generated word to the list
            word = itos[word_idx]
            generated_words.append(word)
            
            # End generation if end-of-sentence token is produced (if it exists in your vocab)
            if word == '&lt;eos&gt;':
                break

        generated_text = ' '.join(generated_words)
        
    return generated_text

# Test the function
start_sentence = &quot;The robot &quot;
generated_sequence = generate_text(model, start_sentence, max_len=100, temperature=2)
print(generated_sequence)
</code></pre>
<p>results in</p>
<pre><code>The robot emulate emulate emulate emulate gaeltacht...
</code></pre>
<p>If I try lower temperatures it just gives 'The robot robot robot robot`</p>
<p>Unlike the tutorial I trained for 10 epochs.  The final epoch gives</p>
<pre><code>-----------------------------------------------------------------------------------------
| epoch   8 |   200/ 2928 batches | lr 2.99 | ms/batch 14.36 | loss  1.20 | ppl     3.33
| epoch   8 |   400/ 2928 batches | lr 2.99 | ms/batch 14.26 | loss  1.23 | ppl     3.44
| epoch   8 |   600/ 2928 batches | lr 2.99 | ms/batch 14.44 | loss  1.16 | ppl     3.20
| epoch   8 |   800/ 2928 batches | lr 2.99 | ms/batch 14.59 | loss  1.18 | ppl     3.25
| epoch   8 |  1000/ 2928 batches | lr 2.99 | ms/batch 14.32 | loss  1.15 | ppl     3.17
| epoch   8 |  1200/ 2928 batches | lr 2.99 | ms/batch 14.29 | loss  1.18 | ppl     3.25
| epoch   8 |  1400/ 2928 batches | lr 2.99 | ms/batch 14.32 | loss  1.17 | ppl     3.24
| epoch   8 |  1600/ 2928 batches | lr 2.99 | ms/batch 14.63 | loss  1.18 | ppl     3.24
| epoch   8 |  1800/ 2928 batches | lr 2.99 | ms/batch 14.31 | loss  1.13 | ppl     3.10
| epoch   8 |  2000/ 2928 batches | lr 2.99 | ms/batch 14.27 | loss  1.18 | ppl     3.27
| epoch   8 |  2200/ 2928 batches | lr 2.99 | ms/batch 14.27 | loss  1.21 | ppl     3.36
| epoch   8 |  2400/ 2928 batches | lr 2.99 | ms/batch 14.64 | loss  1.25 | ppl     3.49
| epoch   8 |  2600/ 2928 batches | lr 2.99 | ms/batch 14.26 | loss  1.24 | ppl     3.45
| epoch   8 |  2800/ 2928 batches | lr 2.99 | ms/batch 14.29 | loss  1.19 | ppl     3.30
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 43.81s | valid loss  0.63 | valid ppl     1.87
-----------------------------------------------------------------------------------------
| epoch   9 |   200/ 2928 batches | lr 2.84 | ms/batch 14.67 | loss  1.09 | ppl     2.97
| epoch   9 |   400/ 2928 batches | lr 2.84 | ms/batch 14.33 | loss  1.09 | ppl     2.97
| epoch   9 |   600/ 2928 batches | lr 2.84 | ms/batch 14.39 | loss  1.04 | ppl     2.84
| epoch   9 |   800/ 2928 batches | lr 2.84 | ms/batch 14.30 | loss  1.13 | ppl     3.10
| epoch   9 |  1000/ 2928 batches | lr 2.84 | ms/batch 14.53 | loss  1.06 | ppl     2.88
| epoch   9 |  1200/ 2928 batches | lr 2.84 | ms/batch 14.30 | loss  1.11 | ppl     3.02
| epoch   9 |  1400/ 2928 batches | lr 2.84 | ms/batch 14.30 | loss  1.09 | ppl     2.97
| epoch   9 |  1600/ 2928 batches | lr 2.84 | ms/batch 14.27 | loss  1.09 | ppl     2.96
| epoch   9 |  1800/ 2928 batches | lr 2.84 | ms/batch 14.49 | loss  1.12 | ppl     3.06
| epoch   9 |  2000/ 2928 batches | lr 2.84 | ms/batch 14.56 | loss  1.11 | ppl     3.02
| epoch   9 |  2200/ 2928 batches | lr 2.84 | ms/batch 14.27 | loss  1.07 | ppl     2.92
| epoch   9 |  2400/ 2928 batches | lr 2.84 | ms/batch 14.29 | loss  1.11 | ppl     3.05
| epoch   9 |  2600/ 2928 batches | lr 2.84 | ms/batch 14.39 | loss  1.09 | ppl     2.99
| epoch   9 |  2800/ 2928 batches | lr 2.84 | ms/batch 14.53 | loss  1.08 | ppl     2.95
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 43.87s | valid loss  0.40 | valid ppl     1.49
-----------------------------------------------------------------------------------------
| epoch  10 |   200/ 2928 batches | lr 2.70 | ms/batch 14.30 | loss  1.02 | ppl     2.78
| epoch  10 |   400/ 2928 batches | lr 2.70 | ms/batch 14.43 | loss  1.06 | ppl     2.89
| epoch  10 |   600/ 2928 batches | lr 2.70 | ms/batch 14.47 | loss  1.03 | ppl     2.79
| epoch  10 |   800/ 2928 batches | lr 2.70 | ms/batch 14.27 | loss  1.00 | ppl     2.72
| epoch  10 |  1000/ 2928 batches | lr 2.70 | ms/batch 14.36 | loss  1.00 | ppl     2.73
| epoch  10 |  1200/ 2928 batches | lr 2.70 | ms/batch 14.35 | loss  1.01 | ppl     2.75
| epoch  10 |  1400/ 2928 batches | lr 2.70 | ms/batch 14.65 | loss  1.03 | ppl     2.80
| epoch  10 |  1600/ 2928 batches | lr 2.70 | ms/batch 14.29 | loss  1.02 | ppl     2.77
| epoch  10 |  1800/ 2928 batches | lr 2.70 | ms/batch 14.30 | loss  1.01 | ppl     2.73
| epoch  10 |  2000/ 2928 batches | lr 2.70 | ms/batch 14.28 | loss  1.02 | ppl     2.77
| epoch  10 |  2200/ 2928 batches | lr 2.70 | ms/batch 14.55 | loss  0.99 | ppl     2.68
| epoch  10 |  2400/ 2928 batches | lr 2.70 | ms/batch 14.26 | loss  1.06 | ppl     2.87
| epoch  10 |  2600/ 2928 batches | lr 2.70 | ms/batch 14.28 | loss  1.03 | ppl     2.80
| epoch  10 |  2800/ 2928 batches | lr 2.70 | ms/batch 14.26 | loss  1.01 | ppl     2.74
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 44.03s | valid loss  0.66 | valid ppl     1.94
-----------------------------------------------------------------------------------------
</code></pre>
<p>One interesting thing is the validation preplexity is lower than the test training perplexity</p>
","transformer-model"
"77036634","Unable to solve RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`","2023-09-04 09:38:29","","2","1393","<pytorch><nlp><huggingface-transformers><bert-language-model><transformer-model>","<p>I am facing this error &quot;RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling <code>cublasCreate(handle)</code>&quot; when trying to fine tune the model &quot;deepset/bert-base-cased-squad2&quot; using the transformers library from hugging face.
I have already tried the following:</p>
<ol>
<li>Changing the vocab size to be equal to the vocab size of the
tokenizer</li>
<li>Using smaller batch sizes (8,4, and even 1)</li>
<li>Updating both pytoch and transformers</li>
</ol>
<p>This is my code</p>
<pre><code>import logging
import os
import time
from transformers import  AutoTokenizer
import json
import pandas as pd
import data_preparation_roberta as data_preparation
import fine_tuning_roberta as fine_tuning
import evaluation_roberta as evaluation
import visualisation
import random


def get_experiment_timestamp():
    return time.strftime(&quot;%Y%m%d_%H%M%S&quot;)

model_name =  &quot;deepset/bert-base-cased-squad2&quot;
data_file = &quot;../data/task1_ExtractiveQA_v5.xlsx&quot;
experiment_timestamp = get_experiment_timestamp()
output_dir = f&quot;../checkpoints/model_checkpoints/{model_name}_{experiment_timestamp}/&quot;

seed = int(random.random()*100)

dataset_dict, tokenized_dataset= data_preparation.prepare_data(data_file,seed)

train_batch_size = 1
eval_batch_size = 1 
num_epochs = 3
learning_rate = 2e-5
max_length = 100
log_history = fine_tuning.train_model(model_name,dataset_dict,tokenized_dataset,
                            train_batch_size, eval_batch_size,
                           num_epochs, learning_rate,  max_length ,output_dir)
</code></pre>
<p>And this is the train_model function:</p>
<pre><code>from transformers import TrainingArguments, Trainer
from transformers import default_data_collator
from transformers import  AutoTokenizer, TrainingArguments
from transformers import AutoModelForQuestionAnswering, Trainer
import torch

def train_model(model_name_hf,dataset_dict,tokenized_dataset, train_batch_size, eval_batch_size, num_epochs, learning_rate, max_length,output_dir):


    #model_name = 
    model = AutoModelForQuestionAnswering.from_pretrained(model_name_hf, torch_dtype=torch.float16)
    tokenizer = AutoTokenizer.from_pretrained(model_name_hf)
    #inputs    = tokenizer(dataset_dict, return_tensors=&quot;pt&quot;)
    #model.cuda()
    #inputs = {k: v.cuda() for k, v in inputs.items()}
    #outputs   = model(**inputs)
    #features  = outputs[0][:,0,:].detach().numpy().squeeze()

    #custom_vocab_size = 50000
    #config = model.config
    #config.vocab_size = tokenizer.vocab_size
    #model = AutoModelForQuestionAnswering.from_pretrained(model_name_hf, torch_dtype=torch.float16, config = config)
    #config.vocab_size = custom_vocab_size
    #nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)
    training_args = TrainingArguments(
        output_dir=output_dir,
        logging_dir='logging_dir',
        evaluation_strategy=&quot;epoch&quot;,
        logging_strategy=&quot;epoch&quot;,
        learning_rate=  learning_rate,  #2e-5,
        per_device_train_batch_size=train_batch_size,
        per_device_eval_batch_size=eval_batch_size,
        num_train_epochs=num_epochs,
        weight_decay=0.01,
        push_to_hub=False,
        #fp16=True,
        #gradient_checkpointing=True,
        #metric_for_best_model = 'f1',
        #report_to=&quot;wandb&quot;,
        report_to=None
        )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset[&quot;train&quot;],
        eval_dataset=tokenized_dataset[&quot;valid&quot;],
        tokenizer=tokenizer,
        #data_collator=default_data_collator
    )

    #Fine tune the model.
    trainer.train()

    #Save the model to output dir.
    trainer.save_model(output_dir)

    #Save training and validation loss
    log_history= trainer.state.log_history
   
    return log_history
</code></pre>
<p>The error:</p>
<pre><code>2023-09-04 09:26:47.427280: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']        
- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|                                                                                                                                                                  | 0/456 [00:00&lt;?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [553,0,0], thread: [96,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [553,0,0], thread: 
../aten/src/ATen/native/cuda/Indexing.cu:1146: indexSelectLargeIndex: block: [553,0,0], thread: [95,0,0] Assertion `srcIndex &lt; srcSelectDimSize` failed.
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /root/Desktop/workspace/Evaluation-results/aut_train_eval/scripts/debug_ft.py:32 in &lt;module&gt;     │
│                                                                                                  │
│   29 num_epochs = 3                                                                              │
│   30 learning_rate = 2e-5                                                                        │
│   31 max_length = 100                                                                            │
│ ❱ 32 log_history = fine_tuning.train_model(model_name,dataset_dict,tokenized_dataset,            │
│   33 │   │   │   │   │   │   │   train_batch_size, eval_batch_size,                              │
│   34 │   │   │   │   │   │      num_epochs, learning_rate,  max_length ,output_dir)              │
│                                                                                                  │
│ /workspace/Evaluation-results/aut_train_eval/scripts/fine_tuning_roberta.py:53 in train_model    │
│                                                                                                  │
│   50 │   )                                                                                       │
│   51 │                                                                                           │
│   52 │   #Fine tune the model.                                                                   │
│ ❱ 53 │   trainer.train()                                                                         │
│   54 │                                                                                           │
│   55 │   #Save the model to output dir.                                                          │
│   56 │   trainer.save_model(output_dir)                                                          │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1555 in train                     │
│                                                                                                  │
│   1552 │   │   │   finally:                                                                      │
│   1553 │   │   │   │   hf_hub_utils.enable_progress_bars()                                       │
│   1554 │   │   else:                                                                             │
│ ❱ 1555 │   │   │   return inner_training_loop(                                                   │
│   1556 │   │   │   │   args=args,                                                                │
│   1557 │   │   │   │   resume_from_checkpoint=resume_from_checkpoint,                            │
│   1558 │   │   │   │   trial=trial,                                                              │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1837 in _inner_training_loop      │
│                                                                                                  │
│   1834 │   │   │   │   │   self.control = self.callback_handler.on_step_begin(args, self.state,  │
│   1835 │   │   │   │                                                                             │
│   1836 │   │   │   │   with self.accelerator.accumulate(model):                                  │
│ ❱ 1837 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)                      │
│   1838 │   │   │   │                                                                             │
│   1839 │   │   │   │   if (                                                                      │
│   1840 │   │   │   │   │   args.logging_nan_inf_filter                                           │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/transformers/trainer.py:2682 in training_step             │
│                                                                                                  │
│   2679 │   │   │   return loss_mb.reduce_mean().detach().to(self.args.device)                    │
│   2680 │   │                                                                                     │
│   2681 │   │   with self.compute_loss_context_manager():                                         │
│ ❱ 2682 │   │   │   loss = self.compute_loss(model, inputs)                                       │
│   2683 │   │                                                                                     │
│   2684 │   │   if self.args.n_gpu &gt; 1:                                                           │
│   2685 │   │   │   loss = loss.mean()  # mean() to average on multi-gpu parallel training        │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/transformers/trainer.py:2707 in compute_loss              │
│                                                                                                  │
│   2704 │   │   │   labels = inputs.pop(&quot;labels&quot;)                                                 │
│   2705 │   │   else:                                                                             │
│   2706 │   │   │   labels = None                                                                 │
│ ❱ 2707 │   │   outputs = model(**inputs)                                                         │
│   2708 │   │   # Save past state if it exists                                                    │
│   2709 │   │   # TODO: this needs to be fixed and made cleaner later.                            │
│   2710 │   │   if self.args.past_index &gt;= 0:                                                     │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501 in _call_impl             │
│                                                                                                  │
│   1498 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │
│   1499 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │
│   1500 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │
│ ❱ 1501 │   │   │   return forward_call(*args, **kwargs)                                          │
│   1502 │   │   # Do not call functions when jit is used                                          │
│   1503 │   │   full_backward_hooks, non_full_backward_hooks = [], []                             │
│   1504 │   │   backward_pre_hooks = []                                                           │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1844 in forward │
│                                                                                                  │
│   1841 │   │   &quot;&quot;&quot;                                                                               │
│   1842 │   │   return_dict = return_dict if return_dict is not None else self.config.use_return  │
│   1843 │   │                                                                                     │
│ ❱ 1844 │   │   outputs = self.bert(                                                              │
│   1845 │   │   │   input_ids,                                                                    │
│   1846 │   │   │   attention_mask=attention_mask,                                                │
│   1847 │   │   │   token_type_ids=token_type_ids,                                                │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501 in _call_impl             │
│                                                                                                  │
│   1498 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │
│   1499 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │
│   1500 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │
│ ❱ 1501 │   │   │   return forward_call(*args, **kwargs)                                          │
│   1502 │   │   # Do not call functions when jit is used                                          │
│   1503 │   │   full_backward_hooks, non_full_backward_hooks = [], []                             │
│   1504 │   │   backward_pre_hooks = []                                                           │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:1022 in forward │
│                                                                                                  │
│   1019 │   │   │   inputs_embeds=inputs_embeds,                                                  │
│   1020 │   │   │   past_key_values_length=past_key_values_length,                                │
│   1021 │   │   )                                                                                 │
│ ❱ 1022 │   │   encoder_outputs = self.encoder(                                                   │
│   1023 │   │   │   embedding_output,                                                             │
│   1024 │   │   │   attention_mask=extended_attention_mask,                                       │
│   1025 │   │   │   head_mask=head_mask,                                                          │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501 in _call_impl             │
│                                                                                                  │
│   1498 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │
│   1499 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │
│   1500 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │
│ ❱ 1501 │   │   │   return forward_call(*args, **kwargs)                                          │
│   1502 │   │   # Do not call functions when jit is used                                          │
│   1503 │   │   full_backward_hooks, non_full_backward_hooks = [], []                             │
│   1504 │   │   backward_pre_hooks = []                                                           │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:612 in forward  │
│                                                                                                  │
│    609 │   │   │   │   │   encoder_attention_mask,                                               │
│    610 │   │   │   │   )                                                                         │
│    611 │   │   │   else:                                                                         │
│ ❱  612 │   │   │   │   layer_outputs = layer_module(                                             │
│    613 │   │   │   │   │   hidden_states,                                                        │
│    614 │   │   │   │   │   attention_mask,                                                       │
│    615 │   │   │   │   │   layer_head_mask,                                                      │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501 in _call_impl             │
│                                                                                                  │
│   1498 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │
│   1499 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │
│   1500 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │
│ ❱ 1501 │   │   │   return forward_call(*args, **kwargs)                                          │
│   1502 │   │   # Do not call functions when jit is used                                          │
│   1503 │   │   full_backward_hooks, non_full_backward_hooks = [], []                             │
│   1504 │   │   backward_pre_hooks = []                                                           │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:497 in forward  │
│                                                                                                  │
│    494 │   ) -&gt; Tuple[torch.Tensor]:                                                             │
│    495 │   │   # decoder uni-directional self-attention cached key/values tuple is at positions  │
│    496 │   │   self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else  │
│ ❱  497 │   │   self_attention_outputs = self.attention(                                          │
│    498 │   │   │   hidden_states,                                                                │
│    499 │   │   │   attention_mask,                                                               │
│    500 │   │   │   head_mask,                                                                    │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501 in _call_impl             │
│                                                                                                  │
│   1498 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │
│   1499 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │
│   1500 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │
│ ❱ 1501 │   │   │   return forward_call(*args, **kwargs)                                          │
│   1502 │   │   # Do not call functions when jit is used                                          │
│   1503 │   │   full_backward_hooks, non_full_backward_hooks = [], []                             │
│   1504 │   │   backward_pre_hooks = []                                                           │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:427 in forward  │
│                                                                                                  │
│    424 │   │   past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,                 │
│    425 │   │   output_attentions: Optional[bool] = False,                                        │
│    426 │   ) -&gt; Tuple[torch.Tensor]:                                                             │
│ ❱  427 │   │   self_outputs = self.self(                                                         │
│    428 │   │   │   hidden_states,                                                                │
│    429 │   │   │   attention_mask,                                                               │
│    430 │   │   │   head_mask,                                                                    │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501 in _call_impl             │
│                                                                                                  │
│   1498 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │
│   1499 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │
│   1500 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │
│ ❱ 1501 │   │   │   return forward_call(*args, **kwargs)                                          │
│   1502 │   │   # Do not call functions when jit is used                                          │
│   1503 │   │   full_backward_hooks, non_full_backward_hooks = [], []                             │
│   1504 │   │   backward_pre_hooks = []                                                           │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:286 in forward  │
│                                                                                                  │
│    283 │   │   past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,                 │
│    284 │   │   output_attentions: Optional[bool] = False,                                        │
│    285 │   ) -&gt; Tuple[torch.Tensor]:                                                             │
│ ❱  286 │   │   mixed_query_layer = self.query(hidden_states)                                     │
│    287 │   │                                                                                     │
│    288 │   │   # If this is instantiated as a cross-attention module, the keys                   │
│    289 │   │   # and values come from an encoder; the attention mask needs to be                 │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1501 in _call_impl             │
│                                                                                                  │
│   1498 │   │   if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks   │
│   1499 │   │   │   │   or _global_backward_pre_hooks or _global_backward_hooks                   │
│   1500 │   │   │   │   or _global_forward_hooks or _global_forward_pre_hooks):                   │
│ ❱ 1501 │   │   │   return forward_call(*args, **kwargs)                                          │
│   1502 │   │   # Do not call functions when jit is used                                          │
│   1503 │   │   full_backward_hooks, non_full_backward_hooks = [], []                             │
│   1504 │   │   backward_pre_hooks = []                                                           │
│                                                                                                  │
│ /opt/conda/lib/python3.8/site-packages/torch/nn/modules/linear.py:114 in forward                 │
│                                                                                                  │
│   111 │   │   │   init.uniform_(self.bias, -bound, bound)                                        │
│   112 │                                                                                          │
│   113 │   def forward(self, input: Tensor) -&gt; Tensor:                                            │
│ ❱ 114 │   │   return F.linear(input, self.weight, self.bias)                                     │
│   115 │                                                                                          │
│   116 │   def extra_repr(self) -&gt; str:                                                           │
│   117 │   │   return 'in_features={}, out_features={}, bias={}'.format(                          │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
  0%|                                                                                           ```

Any help is appreciated.


**EDIT**
I found out later on that I was not passing the correct Tokenizer to the Trainer. It caused the input to the embeddings layers to be in the wrong shape. 
</code></pre>
","transformer-model"
"77035667","TransformerEncoderLayer has nondeterministic random output?","2023-09-04 06:58:38","77036586","0","122","<deep-learning><pytorch><transformer-model>","<pre><code>import torch
from torch import nn
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=1, nhead=1, dim_feedforward=2)
        self.trans = nn.TransformerEncoder(self.encoder_layer, num_layers=1)
    def forward(self, x):
        y = self.trans(x)
        return y
model = MyModel()
model.eval()
x = torch.tensor([[1.],[1.]])
model(x)
</code></pre>
<p>I have a model with just one encoder layer. I set the model to evaluation and each time I run inference I get a different output. Why?</p>
","transformer-model"
"77022710","ValueError: Can't convert non-rectangular Python sequence to Tensor. during fine tuning a falcon 40b model","2023-09-01 11:26:44","","1","144","<tensorflow><machine-learning><huggingface-transformers><transformer-model><fine-tuning>","<p>I have a list of text comments and a list of their labels. I want to fine tune LLM; for this i need to create a tensor dataset below is the code I am using.</p>
<pre><code>#LIST OF ALL LABELS
labels_list = [label_dictionary[category] for category in training_data['Area']]
print('This is label list')
print(labels_list)

#TEXT LIST
text_list = [y for y in training_data['Pain Points']]
print('This is text list')
text_list
</code></pre>
<p>output</p>
<pre><code>This is label list [0, 1, 0, 2, 3, 1, 2, 1]  

This is text list  

['oblems Mentioned in Text:',  'App not working properly, getting errors as soon as contest is completed and unable to change players.',  'Poor user interface and experience making it difficult to navigate and use.',  'Buying reviews for the app.',  'Server glitches taking 2-4 hours to fix problems mentioned above.',  'Unable to complete process due to persistent errors on server being unable to handle huge amount of requests for editing squad information .',  'When trying to login, OTP not received after entering mobile number .',  &quot;Takes years to load score/leaderboard results; 1 hour delayed updates on leaderboards/scores . 8 No information about position when joining a 10 rupees contest; auto joins more expensive contests than desired with no way of changing back 9 Bot opponents don't play, wasting money 10 Crashing often 11 Very poor performance from servers 12 Too many bugs 13 Copied concept 14 Confusing UI 15 Claims rewards 16 Crashed just before first IPL match 17 Andhra Pradesh &amp; Telangana states are unavailable 18 Unable receive OTP 19 Second thing verified PAN status still unverified&quot;]
</code></pre>
<p>now I am tokenizing it</p>
<pre><code>model_name = &quot;tiiuae/falcon-40b-instruct&quot;

tokenizer = AutoTokenizer.from_pretrained(model_name)

tokenized_text = tokenizer(text_list, truncation=True)
</code></pre>
<p>and using tensorflow i am creating a dataset</p>
<pre><code>import tensorflow as tf

dataset = tf.data.Dataset.from_tensor_slices((dict(tokenized_text),labels_list))
</code></pre>
<p>output</p>
<pre><code>TypeError: Could not build aTypeSpec` for [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] with type list 

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
101       dtype = dtypes.as_dtype(dtype).as_datatype_enum
102   ctx.ensure_initialized()
--&gt; 103   return ops.EagerTensor(value, ctx.device_name, dtype)
104
105

ValueError: Can't convert non-rectangular Python sequence to Tensor.
</code></pre>
","transformer-model"
"77018085","How to convert Tensorflow Multi-head attention to PyTorch?","2023-08-31 17:26:32","","0","139","<python><pytorch><tensorflow2.0><transformer-model><multihead-attention>","<p>I'm converting a Tensorflow transformer model to Pytorch equivalent.
In TF multi-head attention part of the code I have:
<code>att = layers.MultiHeadAttention(num_heads=6, key_dim=4)</code>
and the <code>input</code> shape is <code>[None, 136, 4]</code> where None is the batch size, 136 is sequence length and 4 is embedding dimension. <code>num_heads</code> is number of heads and <code>key_dim</code> is dimension of each head.
It gets the input as : <code>att(query=input, value=input)</code></p>
<p>In pyTorch, the MHA is defined as <code>att = nn.MultiheadAttention(embed_dim, num_heads)</code> where <code>embed_dim</code> is the dimension of the model (Not dim of each head) and it must be divisible by <code>num_heads = 6</code>.
Since the input shape is <code>[None, 136, 4]</code> and 4 is not divisible by 6, Pytorch rises an error about divisibility. how should I change my input to be able to use Pytorch instead of TF?</p>
<p>If <code>key_dim</code> in Tf is 4 and it has 6 heads, the whole model's dimension must be 4<em>6. I defined <code>embed_dim</code> in PyTorch equal to 4</em>6 but because of input size of <code>[None, 136, 4]</code>, Pytorch rises error for dimension saying &quot; expected input of size 24 but got 4&quot;.
Is it ok to repeat my input for <code>num_head</code> times to fix the problem?
Does TF feeds the input directly to each head without dividing it to number of heads?
How can I convert TF MHA to Pytorch MHA?</p>
","transformer-model"
"77017536","Swin-Transformer-TF not working with generator","2023-08-31 15:59:31","77022839","0","183","<python><tensorflow><transformer-model><swin-transformer>","<p>Swin-Transformer-TF from <a href=""https://github.com/rishigami/Swin-Transformer-TF/tree/main"" rel=""nofollow noreferrer"">https://github.com/rishigami/Swin-Transformer-TF/tree/main</a>, failed when I use generator for train. I tried many generators, and many output of generators options, lists, Tensor's etc. everything is crashing.</p>
<pre><code>import tensorflow as tf
import numpy as np
!git clone https://github.com/rishigami/Swin-Transformer-TF.git
import sys
sys.path.append('./Swin-Transformer-TF')
from swintransformer import SwinTransformer
</code></pre>
<p>create the model:</p>
<pre><code>img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.imagenet_utils.preprocess_input(tf.cast(data, 
                                                                                                                     tf.float32), 
                                                                                                             mode=&quot;torch&quot;), 
                                          input_shape=[224, 224,3])
pretrained_model = SwinTransformer('swin_tiny_224', num_classes=2, 
                                   include_top=True, pretrained=0, 
                                   use_tpu=0)

model = tf.keras.Sequential([
    img_adjust_layer,
    pretrained_model,
    tf.keras.layers.Dense(2, activation='softmax')
])
    
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-8),
    loss = 'categorical_crossentropy',
    metrics=['accuracy']
)
model.summary()
</code></pre>
<p>Train the model, without generetor, works.</p>
<pre><code>x_train = np.zeros((20,224,224,3))
y_train = np.zeros((20,2))
model.fit(x_train, y_train,
         steps_per_epoch=2, epochs=1)
</code></pre>
<p>Adding a generator (and I tried a few), crashes. The shape is read as <code>(None,None,None,3)</code>:</p>
<pre><code>def __data_generation():
    for i in range(3000):
      yield np.zeros((20,224,224,3)),np.zeros((20,2))
gen=__data_generation()
x,y=next(gen)
print(x.shape,y.shape)
model.fit(gen, epochs=1, steps_per_epoch=4)



AssertionError                            Traceback (most recent call last)
&lt;ipython-input-7-fb53f3a3d1ae&gt; in &lt;cell line: 7&gt;()
      5 x,y=next(gen)
      6 print(x.shape,y.shape)
----&gt; 7 model.fit(gen, epochs=1, steps_per_epoch=4)

4 frames
/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     68             # To get the full stack trace, call:
     69             # `tf.debugging.disable_traceback_filtering()`
---&gt; 70             raise e.with_traceback(filtered_tb) from None
     71         finally:
     72             del filtered_tb

/usr/local/lib/python3.10/dist-packages/keras/engine/training.py in tf__train_function(iterator)
     13                 try:
     14                     do_return = True
---&gt; 15                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16                 except:
     17                     do_return = False

/content/./Swin-Transformer-TF/swintransformer/model.py in tf__call(self, x)
      8                 do_return = False
      9                 retval_ = ag__.UndefinedReturnValue()
---&gt; 10                 x = ag__.converted_call(ag__.ld(self).forward_features, (ag__.ld(x),), None, fscope)
     11 
     12                 def get_state():

/content/./Swin-Transformer-TF/swintransformer/model.py in tf__forward_features(self, x)
      8                 do_return = False
      9                 retval_ = ag__.UndefinedReturnValue()
---&gt; 10                 x = ag__.converted_call(ag__.ld(self).patch_embed, (ag__.ld(x),), None, fscope)
     11 
     12                 def get_state():

/content/./Swin-Transformer-TF/swintransformer/model.py in tf__call(self, x)
      9                 retval_ = ag__.UndefinedReturnValue()
     10                 (B, H, W, C) = ag__.converted_call(ag__.converted_call(ag__.ld(x).get_shape, (), None, fscope).as_list, (), None, fscope)
---&gt; 11                 assert ag__.and_(lambda : ag__.ld(H) == ag__.ld(self).img_size[0], lambda : ag__.ld(W) == ag__.ld(self).img_size[1]), f&quot;Input image size ({ag__.ld(H)}*{ag__.ld(W)}) doesn't match model ({ag__.ld(self).img_size[0]}*{ag__.ld(self).img_size[1]}).&quot;
     12                 x = ag__.converted_call(ag__.ld(self).proj, (ag__.ld(x),), None, fscope)
     13                 x = ag__.converted_call(ag__.ld(tf).reshape, (ag__.ld(x),), dict(shape=[-1, ag__.ld(H) // ag__.ld(self).patch_size[0] * (ag__.ld(W) // ag__.ld(self).patch_size[0]), ag__.ld(self).embed_dim]), fscope)

AssertionError: in user code:

    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 1284, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 1268, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 1249, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 1050, in train_step
        y_pred = self(x, training=True)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py&quot;, line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None
    File &quot;/tmp/__autograph_generated_filegq78ow1y.py&quot;, line 10, in tf__call
        x = ag__.converted_call(ag__.ld(self).forward_features, (ag__.ld(x),), None, fscope)
    File &quot;/tmp/__autograph_generated_filehkbim9xc.py&quot;, line 10, in tf__forward_features
        x = ag__.converted_call(ag__.ld(self).patch_embed, (ag__.ld(x),), None, fscope)
    File &quot;/tmp/__autograph_generated_file3nqntnhc.py&quot;, line 11, in tf__call
        assert ag__.and_(lambda : ag__.ld(H) == ag__.ld(self).img_size[0], lambda : ag__.ld(W) == ag__.ld(self).img_size[1]), f&quot;Input image size ({ag__.ld(H)}*{ag__.ld(W)}) doesn't match model ({ag__.ld(self).img_size[0]}*{ag__.ld(self).img_size[1]}).&quot;

    AssertionError: Exception encountered when calling layer 'swin_tiny_224' (type SwinTransformerModel).
    
    in user code:
    
        File &quot;/content/./Swin-Transformer-TF/swintransformer/model.py&quot;, line 422, in call  *
            x = self.forward_features(x)
        File &quot;/content/./Swin-Transformer-TF/swintransformer/model.py&quot;, line 411, in forward_features  *
            x = self.patch_embed(x)
        File &quot;/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py&quot;, line 70, in error_handler  **
            raise e.with_traceback(filtered_tb) from None
        File &quot;/tmp/__autograph_generated_file3nqntnhc.py&quot;, line 11, in tf__call
            assert ag__.and_(lambda : ag__.ld(H) == ag__.ld(self).img_size[0], lambda : ag__.ld(W) == ag__.ld(self).img_size[1]), f&quot;Input image size ({ag__.ld(H)}*{ag__.ld(W)}) doesn't match model ({ag__.ld(self).img_size[0]}*{ag__.ld(self).img_size[1]}).&quot;
    
        AssertionError: Exception encountered when calling layer 'patch_embed' (type PatchEmbed).
        
        in user code:
        
            File &quot;/content/./Swin-Transformer-TF/swintransformer/model.py&quot;, line 336, in call  *
                assert H == self.img_size[0] and W == self.img_size[1],             f&quot;Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).&quot;
        
            AssertionError: Input image size (None*None) doesn't match model (224*224).
        
        
        Call arguments received by layer 'patch_embed' (type PatchEmbed):
          • x=tf.Tensor(shape=(None, None, None, 3), dtype=float32)
    
    
    Call arguments received by layer 'swin_tiny_224' (type SwinTransformerModel):
      • x=tf.Tensor(shape=(None, None, None, 3), dtype=float32)
</code></pre>
","transformer-model"
"77008549","Very bad Zero Shot classification predictions from ZeroShot Models on Hugging face","2023-08-30 13:34:04","","0","228","<nlp><huggingface-transformers><text-classification><transformer-model><huggingface>","<p>I am currently trying to use two Hugging Face zero shot model for classification. Unfortunately, the results are extremely poor (almost random). Likewise, I get different results when I use the template for the example on the hugging face webpage page, then running it via own code and pipeline. Therefore, I think I am doing something wrong with my code. Additionally, I also get some warnings. Maybe you can help me out 😊</p>
<p>My Code for the first Model:</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;zero-shot-classification&quot;, model=&quot;MoritzLaurer/mDeBERTa-v3-base-mnli-xnli&quot;)
sequence_to_classify = &quot;This is a very cool Video&quot;
candidate_labels = [&quot;Praise&quot;, &quot;Criticism&quot;, &quot;Question&quot;]
output = classifier(sequence_to_classify, candidate_labels, multi_label=False)
print(output)
</code></pre>
<p>Output inkl. warnings:</p>
<pre><code>C:\Users\XXXX\anaconda3\envs\zero-shot-env\lib\site-packages\transformers\convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
{'sequence': 'This is a very cool Video', 'labels': ['Praise', 'Criticism', 'Question'], 'scores': [0.6537378430366516, 0.28773054480552673, 0.05853160098195076]}
</code></pre>
<p>If I use the same input on the <a href=""https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7?candidateLabels=Praise%2C%20Criticism%2C%20Question&amp;multiClass=false&amp;text=This%20is%20a%20very%20cool%20Video"" rel=""nofollow noreferrer"">Hugging face Webpage</a> the result is:</p>
<pre><code>Praise  0.789
Criticism  0.157
Question  0.053
</code></pre>
<p>In this case, it is close together, but in other cases the values between the Hugging Face Page and the code/pipeline are completly different.</p>
<p>I installed <code>sentencepiece</code>, but the warning still appears.</p>
<p>For the second model I use it is the same problem but with a different Warning:</p>
<p>The Code:</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;zero-shot-classification&quot;,
                      model=&quot;joeddav/xlm-roberta-large-xnli&quot;)
sequence_to_classify = &quot;This is a very cool Video&quot;
candidate_labels = [&quot;Praise&quot;, &quot;Criticism&quot;, &quot;Question&quot;]
output = classifier(sequence_to_classify, candidate_labels, multi_label=False)
print(output)
</code></pre>
<p>The Output:</p>
<pre><code>Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
{'sequence': 'This is a very cool Video', 'labels': ['Praise', 'Criticism', 'Question'], 'scores': [0.6573342680931091, 0.24442064762115479, 0.09824512898921967]}
</code></pre>
<p>The Values are also different to the Hugging face webpage. I also do not get what the warning means. Because I use this model in a zero shot setting and did not change the architecture or something. Maybe you know more, very happy for help🙂🙂!</p>
<p>PS: Here is my conda env list</p>
<pre><code># Name                    Version                   Build  Channel
anaconda-client           1.11.2          py310haa95532_0
anaconda-navigator        2.4.1           py310haa95532_0
anaconda-project          0.11.1          py310haa95532_0
attrs                     22.1.0          py310haa95532_0
backports                 1.1                pyhd3eb1b0_0
backports.functools_lru_cache 1.6.4              pyhd3eb1b0_0
backports.tempfile        1.0                pyhd3eb1b0_1
backports.weakref         1.0.post1                  py_1
beautifulsoup4            4.12.2          py310haa95532_0
boltons                   23.0.0          py310haa95532_0
brotlipy                  0.7.0           py310h2bbff1b_1002
bzip2                     1.0.8                he774522_0
ca-certificates           2023.05.30           haa95532_0
certifi                   2023.7.22       py310haa95532_0
cffi                      1.15.1          py310h2bbff1b_3
chardet                   4.0.0           py310haa95532_1003
charset-normalizer        2.0.4              pyhd3eb1b0_0
click                     8.0.4           py310haa95532_0
clyent                    1.2.2           py310haa95532_1
colorama                  0.4.6           py310haa95532_0
conda                     23.7.3          py310haa95532_0
conda-build               3.24.0          py310haa95532_0
conda-content-trust       0.1.3           py310haa95532_0
conda-pack                0.6.0              pyhd3eb1b0_0
conda-package-handling    2.0.2           py310haa95532_0
conda-package-streaming   0.7.0           py310haa95532_0
conda-repo-cli            1.0.41          py310haa95532_0
conda-token               0.4.0              pyhd3eb1b0_0
conda-verify              3.4.2                      py_1
console_shortcut          0.1.1                         4
cryptography              39.0.1          py310h21b164f_0
defusedxml                0.7.1              pyhd3eb1b0_0
filelock                  3.9.0           py310haa95532_0
freetype                  2.12.1               ha860e81_0
future                    0.18.3          py310haa95532_0
giflib                    5.2.1                h8cc25b3_3
glib                      2.69.1               h5dc1a3c_2
glob2                     0.7                pyhd3eb1b0_0
icu                       58.2                 ha925a31_3
idna                      3.4             py310haa95532_0
jinja2                    3.1.2           py310haa95532_0
jpeg                      9e                   h2bbff1b_1
jsonpatch                 1.32               pyhd3eb1b0_0
jsonpointer               2.1                pyhd3eb1b0_0
jsonschema                4.17.3          py310haa95532_0
jupyter_core              5.3.0           py310haa95532_0
krb5                      1.20.1               h5b6d351_1
lerc                      3.0                  hd77b12b_0
libarchive                3.6.2                h2033e3e_1
libclang                  14.0.6          default_hb5a9fac_1
libclang13                14.0.6          default_h8e68704_1
libdeflate                1.17                 h2bbff1b_0
libffi                    3.4.4                hd77b12b_0
libiconv                  1.16                 h2bbff1b_2
liblief                   0.12.3               hd77b12b_0
libpng                    1.6.39               h8cc25b3_0
libpq                     12.15                h906ac69_0
libtiff                   4.5.0                h6c2663c_2
libwebp                   1.2.4                hbc33d0d_1
libwebp-base              1.2.4                h2bbff1b_1
libxml2                   2.10.4               h0ad7f3c_1
libxslt                   1.1.37               h2bbff1b_1
lz4-c                     1.9.4                h2bbff1b_0
m2-msys2-runtime          2.5.0.17080.65c939c               3
m2-patch                  2.7.5                         2
markupsafe                2.1.1           py310h2bbff1b_0
menuinst                  1.4.19          py310h59b6b97_0
msys2-conda-epoch         20160418                      1
navigator-updater         0.4.0           py310haa95532_0
nbformat                  5.7.0           py310haa95532_0
openssl                   1.1.1v               h2bbff1b_0
packaging                 23.0            py310haa95532_0
pathlib                   1.0.1              pyhd3eb1b0_1
pcre                      8.45                 hd77b12b_0
pillow                    9.4.0           py310hd77b12b_0
pip                       22.3.1          py310haa95532_0
pkginfo                   1.9.6           py310haa95532_0
platformdirs              3.10.0          py310haa95532_0
pluggy                    1.0.0           py310haa95532_1
ply                       3.11            py310haa95532_0
powershell_shortcut       0.0.1                         3
psutil                    5.9.0           py310h2bbff1b_0
py-lief                   0.12.3          py310hd77b12b_0
pycosat                   0.6.4           py310h2bbff1b_0
pycparser                 2.21               pyhd3eb1b0_0
pyjwt                     2.4.0           py310haa95532_0
pyopenssl                 23.2.0          py310haa95532_0
pyqt                      5.15.7          py310hd77b12b_0
pyqt5-sip                 12.11.0         py310hd77b12b_0
pyrsistent                0.18.0          py310h2bbff1b_0
pysocks                   1.7.1           py310haa95532_0
python                    3.10.9               h966fe2a_1
python-dateutil           2.8.2              pyhd3eb1b0_0
python-fastjsonschema     2.16.2          py310haa95532_0
python-libarchive-c       2.9                pyhd3eb1b0_1
pytz                      2022.7          py310haa95532_0
pywin32                   305             py310h2bbff1b_0
pyyaml                    6.0             py310h2bbff1b_1
qt-main                   5.15.2               h6072711_9
qt-webengine              5.15.9               h5bd16bc_7
qtpy                      2.2.0           py310haa95532_0
qtwebkit                  5.212                h2bbfb41_5
requests                  2.31.0          py310haa95532_0
requests-toolbelt         1.0.0           py310haa95532_0
ruamel.yaml               0.17.21         py310h2bbff1b_0
ruamel.yaml.clib          0.2.6           py310h2bbff1b_1
ruamel_yaml               0.17.21         py310h2bbff1b_0
setuptools                65.6.3          py310haa95532_0
sip                       6.6.2           py310hd77b12b_0
six                       1.16.0             pyhd3eb1b0_1
soupsieve                 2.4             py310haa95532_0
sqlite                    3.41.2               h2bbff1b_0
tk                        8.6.12               h2bbff1b_0
toml                      0.10.2             pyhd3eb1b0_0
tomli                     2.0.1           py310haa95532_0
toolz                     0.12.0          py310haa95532_0
tornado                   6.3.2           py310h2bbff1b_0
tqdm                      4.65.0          py310h9909e9c_0
traitlets                 5.7.1           py310haa95532_0
tzdata                    2023c                h04d1e81_0
ujson                     5.4.0           py310hd77b12b_0
urllib3                   1.26.16         py310haa95532_0
vc                        14.2                 h21ff451_1
vs2015_runtime            14.27.29016          h5e58377_2
wheel                     0.38.4          py310haa95532_0
win_inet_pton             1.1.0           py310haa95532_0
wincertstore              0.2             py310haa95532_2
xz                        5.4.2                h8cc25b3_0
yaml                      0.2.5                he774522_0
zlib                      1.2.13               h8cc25b3_0
zstandard                 0.19.0          py310h2bbff1b_0
zstd                      1.5.5                hd43e919_0
</code></pre>
","transformer-model"
"77005425","How to implement StoppingCriteria in huggingface transformers v4.33 for code completions?","2023-08-30 06:29:00","","1","503","<python><machine-learning><huggingface-transformers><transformer-model><code-completion>","<p>I'm tryting to get stats of the inference time of different code-completion models on the HumanEval dataset. Since timing is a crucial part of this project, I don't want to time the model when it generates irrelevant tokens. Thus, I hope to implement <strong>StoppingCriteria</strong> on the code-completion models, namely models from the <strong>Codegen, Code LLAMA, and WizardCoder families</strong>.</p>
<p>Currently, when the model generates the full answer but hasn't reached the max number of new tokens (here I set it to 200), it might end with an <code>&lt;|endoftext|&gt;</code> token, but more often it would generate double new lines and continue generating irrelevant text. This largely affects the timing.</p>
<p>Therefore, I hope the generation can stop when it first encounters a <code>&quot;\n\n&quot;</code> token, or two consecutive <code>\n</code> tokens (<code>[**&quot;\n&quot;, &quot;\n&quot;**]</code>). How can I implement this?</p>
<p>To simplify the testing case, here I set the batch size to 1 for each generation. I'd appreciate if it also works when I set num_return_sequences to k, so I can get pass@k stats.</p>
<p>The environment is pulled on 08-29-2023 from the latest huggingface transformers main branch, v4.33. The github repo is provided below:
<a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers</a></p>
<p>The Python environment should be above 3.8.0. To test with various model checkpoints, use the checkpoint names are given in the comments. I recommend to test with smaller models if you don't have enough GPU VRAM.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList
import time
import argparse
import torch

parser = argparse.ArgumentParser()
parser.add_argument(&quot;--checkpoint&quot;, type=str, default=&quot;Salesforce/codegen-2B-mono&quot;, help=&quot;Model path&quot;)
FLAGS = parser.parse_args()

# WizardCoder Family
# WizardLM/WizardCoder-Python-34B-V1.0
# WizardLM/WizardCoder-Python-13B-V1.0
# WizardLM/WizardCoder-15B-V1.0
# WizardLM/WizardCoder-3B-V1.0
# WizardLM/WizardCoder-1B-V1.0

# Code LLAMA 2 Family
# codellama/CodeLlama-7b-hf
# codellama/CodeLlama-13b-hf
# codellama/CodeLlama-34b-hf

# Salesforce Codegen Family
# Salesforce/codegen-350M-mono
# Salesforce/codegen-2B-mono
# Salesforce/codegen-6B-mono
# Salesforce/codegen-16B-mono

stop_words = [&quot;\n\n&quot;]
# HumanEval Q0
prompt_0 = &quot;from typing import List\n\n\ndef has_close_elements(numbers: List[float], threshold: float) -&gt; bool:\n    \&quot;\&quot;\&quot; Check if in given list of numbers, are any two numbers closer to each other than\n    given threshold.\n    &gt;&gt;&gt; has_close_elements([1.0, 2.0, 3.0], 0.5)\n    False\n    &gt;&gt;&gt; has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n    True\n    \&quot;\&quot;\&quot;\n&quot;
# HumanEval Q31
prompt_31 = &quot;\n\ndef is_prime(n):\n    \&quot;\&quot;\&quot;Return true if a given number is prime, and false otherwise.\n    &gt;&gt;&gt; is_prime(6)\n    False\n    &gt;&gt;&gt; is_prime(101)\n    True\n    &gt;&gt;&gt; is_prime(11)\n    True\n    &gt;&gt;&gt; is_prime(13441)\n    True\n    &gt;&gt;&gt; is_prime(61)\n    True\n    &gt;&gt;&gt; is_prime(4)\n    False\n    &gt;&gt;&gt; is_prime(1)\n    False\n    \&quot;\&quot;\&quot;\n&quot;
# HumanEval Q35
prompt_35 = &quot;\n\ndef max_element(l: list):\n    \&quot;\&quot;\&quot;Return maximum element in the list.\n    &gt;&gt;&gt; max_element([1, 2, 3])\n    3\n    &gt;&gt;&gt; max_element([5, 3, -5, 2, -3, 3, 9, 0, 123, 1, -10])\n    123\n    \&quot;\&quot;\&quot;\n&quot;
# HumanEval Q161
prompt_161 = &quot;\ndef solve(s):\n    \&quot;\&quot;\&quot;You are given a string s.\n    if s[i] is a letter, reverse its case from lower to upper or vise versa, \n    otherwise keep it as it is.\n    If the string contains no letters, reverse the string.\n    The function should return the resulted string.\n    Examples\n    solve(\&quot;1234\&quot;) = \&quot;4321\&quot;\n    solve(\&quot;ab\&quot;) = \&quot;AB\&quot;\n    solve(\&quot;#a@C\&quot;) = \&quot;#A@c\&quot;\n    \&quot;\&quot;\&quot;\n&quot;


def main(args):
    # Initialize model and tokenizer
    checkpoint = args.checkpoint
    tokenizer = AutoTokenizer.from_pretrained(checkpoint, device_map=&quot;auto&quot;)
    start_load_model = time.time()
    model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=&quot;auto&quot;)
    print(f&quot;Time to load model {checkpoint} is {time.time() - start_load_model}&quot;)
    
    # Generate the selcted prompts
    for prompt in [prompt_0, prompt_31, prompt_35, prompt_161]:
        input_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_ids
        start_generating = time.time()
        generated_ids = model.generate(
            input_ids,
            use_cache = True,
            pad_token_id = tokenizer.eos_token_id,
            max_new_tokens = 200,
            do_sample = True,
            temperature = 0.8,
            num_beams=1,
            # stopping_criteria=stopping_criteria,
        )
        generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)
        decoded_list = []
        for ids in generated_ids[0]:
            word = tokenizer.decode(int(ids))
            decoded_list.append(word)
        generated_len = len(decoded_list) - len(input_ids[0])
        
        # Print outputs
        print(f&quot;Time to generate is {time.time() - start_generating}&quot;)
        print(f&quot;per token time is {(time.time()-start_generating)/generated_len}&quot;)
        print(f&quot;decoded_list is {decoded_list[:generated_len]}&quot;)
        prompt_ids = tokenizer(prompt, return_tensors=&quot;pt&quot;).input_ids
        prompt = tokenizer.decode(prompt_ids[0])
        print(f&quot;\ngenerated_text is:\n{generated_text[0]}&quot;)

if __name__== &quot;__main__&quot;:
    main(FLAGS)
</code></pre>
","transformer-model"
"76986743","How to resolve this deep learning training error of CUDA？","2023-08-27 10:51:02","","0","83","<pytorch><transformer-model>","<p>During the early stages of network training, the progress bar is functioning as expected. However, towards the end of the training process, an error suddenly occurs.How can I solve it？
main：</p>
<pre><code>            n_o = torch.cuda.IntTensor(n_o)
            p = torch.cuda.FloatTensor(p)
            o = o.cuda()
            o = o.to(torch.int32)
            o = torch.cuda.IntTensor(o)
            p = p.contiguous()
            # o = o.contiguous()
            # n_o = n_o.contiguous()
            idx = pointops.farthest_point_sampling(p, o, n_o)  # (m)
            n_p = p[idx.long(), :]  # (m, 3)
            x, _ = pointops.knn_query_and_group(x, p, offset=o, new_xyz=n_p, new_offset=n_o,
                                                nsample=self.nsample, with_xyz=True)
</code></pre>
<p>FarthestPointSampling：</p>
<pre><code>class FarthestPointSampling(Function):
    @staticmethod
    def forward(ctx, xyz, offset, new_offset):
        &quot;&quot;&quot;
        input: coords: (n, 3), offset: (b), new_offset: (b)
        output: idx: (m)
        &quot;&quot;&quot;
        assert xyz.is_contiguous()
        n, b, n_max = xyz.shape[0], offset.shape[0], offset[0]
        for i in range(1, b):
            n_max = max(offset[i] - offset[i - 1], n_max)
        idx = torch.cuda.IntTensor(new_offset[b - 1].item()).zero_()
        tmp = torch.cuda.FloatTensor(n).fill_(1e10)
        farthest_point_sampling_cuda(b, n_max, xyz, offset.int(), new_offset.int(), tmp, idx)
        del tmp
        return idx
</code></pre>
<p>Error：</p>
<pre><code>/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [3,0,0], thread: [126,0,0] Assertion `index &gt;= -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; &quot;index out of bounds&quot;` failed.
/opt/conda/conda-bld/pytorch_1659484810403/work/aten/src/ATen/native/cuda/IndexKernel.cu:91: operator(): block: [3,0,0], thread: [127,0,0] Assertion `index &gt;= -sizes[i] &amp;&amp; index &lt; sizes[i] &amp;&amp; &quot;index out of bounds&quot;` failed.
Epoch 1/2: 100%|████████████████████▉| 6072/6073 [04:14&lt;00:00, 23.85it/s, Loss=1.61, Accuracy=0.253]
Traceback (most recent call last):
  File &quot;/home/c/Documents/dataparser/HAR/train.py&quot;, line 47, in &lt;module&gt;
    outputs = model(inputs)
  File &quot;/home/c/anaconda3/envs/p/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/c/Documents/dataparser/HAR/point_transformer.py&quot;, line 184, in forward
    p2, x2, o2 = self.enc2([p1, x1, o1])
  File &quot;/home/c/anaconda3/envs/p/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/c/anaconda3/envs/p/lib/python3.8/site-packages/torch/nn/modules/container.py&quot;, line 139, in forward
    input = module(input)
  File &quot;/home/c/anaconda3/envs/p/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/c/Documents/dataparser/HAR/point_transformer_seg.py&quot;, line 93, in forward
    x, _ = pointops.knn_query_and_group(x, p, offset=o, new_xyz=n_p, new_offset=n_o,
  File &quot;/home/c/Documents/dataparser/HAR//pointops/utils.py&quot;, line 17, in knn_query_and_group
    return grouping(idx, feat, xyz, new_xyz, with_xyz), idx
  File &quot;/home/c/Documents/dataparser/HAR/pointops/grouping.py&quot;, line 45, in grouping
    xyz = torch.cat([xyz, torch.zeros([1, 3]).to(xyz.device)], dim=0)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
</code></pre>
<p>Initially, there was another error that would halt at <code>assert xyz.is_contiguous()</code>.</p>
<pre><code>Epoch 1/2:   0%|                                                           | 0/6073 [00:00&lt;?, ?it/s]
Traceback (most recent call last):
  File &quot;/home/c/Documents/dataparser/train.py&quot;, line 47, in &lt;module&gt;
    outputs = model(inputs)
  File &quot;/home/c/anaconda3/envs/p/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/c/Documents/dataparser/transformer.py&quot;, line 184, in forward
    p2, x2, o2 = self.enc2([p1, x1, o1])
  File &quot;/home/c/anaconda3/envs/p/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/c/anaconda3/envs/p/lib/python3.8/site-packages/torch/nn/modules/container.py&quot;, line 139, in forward
    input = module(input)
  File &quot;/home/c/anaconda3/envs/p/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/home/c/Documents/dataparser/HAR/transformer_seg.py&quot;, line 91, in forward
    idx = pointops.farthest_point_sampling(p, o, n_o)  # (m)
  File &quot;/home/c/Documents/dataparser/HAR/sampling.py&quot;, line 14, in forward
    assert xyz.is_contiguous()
AssertionError
</code></pre>
<p>Then, I added a piece of code to the preceding program.</p>
<pre><code>            p = torch.cuda.FloatTensor(p)
            o = o.cuda()
            o = o.to(torch.int32)
            o = torch.cuda.IntTensor(o)
            p = p.contiguous()
</code></pre>
<p>Then it can run normally until it encounters the error mentioned above.</p>
","transformer-model"
"76982108","Extracting intermediate layer of saved model in keras. That intermediate layer is another sub-model defined in main model","2023-08-26 08:16:33","","1","18","<neural-network><tf.keras><keras-layer><transformer-model><pre-trained-model>","<p>Here is my architecture.</p>
<pre><code>def model1():

    inputs = layers.Input(shape=(x1.shape[-2],x1.shape[-1]), name='Input_layer')

    x = layers.Reshape((x1.shape[-2], x1.shape[-1], 1), name='Input_reshape')(inputs)
    #x= layers.Masking(mask_value=0.0, input_shape=(109, 80))(x)

    x = layers.Conv2D(32, 3, strides=2,
                  activation=layers.LeakyReLU(alpha=0.2), 
                  name='convolution_1')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2, name='dropout1_model1')(x)

    x = layers.Conv2D(64, 3, strides=1,
                  activation=layers.LeakyReLU(alpha=0.2),
                  name='convolution_2')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.2, name='dropout2_model1')(x)

    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]), name='conv_reshpae')(x)

    x = layers.Bidirectional(layers.GRU(64, return_sequences=True, activation=layers.LeakyReLU(alpha=0.2), name='GRU_1'))(x)
    x = layers.Dropout(0.2, name='dropout3_model1')(x)

    x = layers.Bidirectional(layers.GRU(64, return_sequences=True, activation=layers.LeakyReLU(alpha=0.2), name='GRU_2'))(x)
    x = layers.Dropout(0.2, name='dropout4_model1')(x)

    outputs=layers.Dense(128, activation=layers.LeakyReLU(alpha=0.2), name='output_1')(x)

    model=keras.Model(inputs=inputs, outputs=outputs, name='audio_model')

    return model

model_audio=model1()
print(model_audio.summary())

def model2():

  inputs=layers.Input(shape=x2.shape[-1])
  x=layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs)
  x=layers.Bidirectional(LSTM(64, return_sequences=True, activation=layers.LeakyReLU(alpha=0.2), name='First_LSTM'))(x)
  x = layers.Dropout(0.2, name='dropout1_model2')(x)
  x=layers.Bidirectional(LSTM(64, return_sequences=True, activation=layers.LeakyReLU(alpha=0.2), name='Second_LSTM'))(x)
  x = layers.Dropout(0.2, name='dropout2_model')(x)
  outputs=layers.Dense(128, activation=layers.LeakyReLU(alpha=0.2), name='output_2')(x)
  model=keras.Model(inputs=inputs,outputs=outputs, name='text_model')
  return model

model_text=model2()
print(model_text.summary())

def model3(model_audio,model_text):

  Input_1 = tf.keras.layers.Input(shape=(x1.shape[-2], x1.shape[-1]), name='Input_1')
  Input_2 = tf.keras.layers.Input(shape=(x2.shape[-1]), name='Input_2')

  output_1 = model_audio(Input_1)
  output_2 = model_text(Input_2)

  layer = layers.MultiHeadAttention(num_heads=2, key_dim=1, name='attention_layer')
  context_vector, attention_weights = layer(output_1, output_2, return_attention_scores=True)
  x = layers.GRU(128, return_sequences=False, activation=layers.LeakyReLU(alpha=0.2), name='GRU')(context_vector)
  x = layers.Dropout(0.2, name='dropout1_modelFinal')(x)
  x = layers.Dense(32, activation=layers.LeakyReLU(alpha=0.2))(x)
  x = layers.Dropout(0.2, name='dropout2_modelFinal')(x)
  outputs = layers.Dense(1,activation='sigmoid', name='final_layer')(x)

  model = keras.Model(inputs=[Input_1, Input_2], outputs=outputs)
  return model

model_final = model3(model_audio,model_text)
print(model_final.summary())
</code></pre>
<p>Here is the model3 (model final) summary</p>
<h1>Layer (type)                Output Shape                 Param #   Connected to</h1>
<p>Input_1 (InputLayer)        [(None, 135, 40)]            0         []</p>
<p>Input_2 (InputLayer)        [(None, 17)]                 0         []</p>
<p>model (Functional)          (None, 65, 128)              553344    ['Input_1[0][0]']</p>
<p>model_1 (Functional)        (None, 17, 128)              287616    ['Input_2[0][0]']</p>
<p>attention_layer (MultiHead  ((None, 65, 128),            1158      ['model[0][0]',<br />
Attention)                   (None, 2, 65, 17))                     'model_1[0][0]']</p>
<p>GRU (GRU)                   (None, 128)                  99072     ['attention_layer[0][0]']</p>
<p>dropout1_modelFinal (Dropo  (None, 128)                  0         ['GRU[0][0]']<br />
ut)</p>
<p>dense (Dense)               (None, 32)                   4128      ['dropout1_modelFinal[0][0]']</p>
<p>dropout2_modelFinal (Dropo  (None, 32)                   0         ['dense[0][0]']<br />
ut)</p>
<p>final_layer (Dense)         (None, 1)                    33        ['dropout2_modelFinal[0][0]']</p>
<p>==================================================================================================
Total params: 945351 (3.61 MB)
Trainable params: 945159 (3.61 MB)
Non-trainable params: 192 (768.00 Byte)</p>
<p>Here I want to extract the <strong>model (Functional)</strong> output which is an another model (mode_audio) defined. It is a submodel fused in main model. actually model_audio output is used an input to model_final</p>
<p>Initially this is what tried</p>
<pre><code>model_output = model.layers[-8].output  # 'model' layer is at index -8 from the summary

model_layer_output = Model(inputs=model.input, outputs=model_output)

intermediate_output = model_layer_output.predict([test_input_1_data, test_input_2_data])
</code></pre>
<p>since layer[-8] is the model_audio layer .i used that as an output. But i was getting the below error</p>
<p><strong>Graph disconnected: cannot obtain value for tensor KerasTensor(type_spec=TensorSpec(shape=(None, 135, 40), dtype=tf.float32, name='Input_layer'), name='Input_layer', description=&quot;created by layer 'Input_layer'&quot;) at layer &quot;Input_reshape&quot;. The following previous layers were accessed without issue: []</strong></p>
<p>So later i tried this. I got the expected output shape. but i am not sure whether it is right</p>
<pre><code>#audio model
input_layer = Input(shape=(135, 40))  
model_audio_output = model.get_layer('model')(input_layer)

submodel = Model(inputs=input_layer, outputs=model_audio_output)
audio_embed=submodel.predict(input_samples)
</code></pre>
<p>can someone suggest how to extract the <strong>model_audio</strong> output from my saved keras model</p>
","transformer-model"
"76965431","How can BERT/Transformer models accept input batches of different sizes?","2023-08-23 23:21:21","","2","761","<nlp><huggingface-transformers><transformer-model><huggingface>","<p>I understand that all inputs in a batch need to be of the same size. However, it seems BERT/Transformers models can accept batches with different sizes as input.</p>
<p>How is that possible? I thought we needed to pad all examples in a batch to <code>model.max_input_size</code>, however, it seems HuggingFace does <code>Dynamic Padding</code> that allows sending batches of different lengths (till the time they are smaller than <code>max_input_size</code>)</p>
<p><a href=""https://i.sstatic.net/zC1WS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zC1WS.png"" alt=""enter image description here"" /></a></p>
<p>Link: <a href=""https://mccormickml.com/2020/07/29/smart-batching-tutorial/"" rel=""nofollow noreferrer"">https://mccormickml.com/2020/07/29/smart-batching-tutorial/</a><br />
Link2: <a href=""https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding</a></p>
","transformer-model"
"76956496","Inputs and Outputs Mismatch of Multi-head Attention Module (Tensorflow VS PyTorch)","2023-08-22 19:52:16","77018020","0","340","<pytorch><transformer-model><attention-model><large-language-model><multihead-attention>","<p>I am trying to convert my tensorflow model for <code>layers.MultiHeadAttention</code> module from <code>tf.keras</code> to <code>nn.MultiheadAttention</code> from <code>torch.nn</code> module. Below are the snippets.</p>
<ol>
<li>Tensorflow Multi-head Attention</li>
</ol>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras import layers

x_sfe_tf = np.random.randn(64, 345, 64)
x_te_tf = np.random.randn(64, 200, 64)

tes_mod_tf = layers.MultiHeadAttention(num_heads=2, key_dim=64)
output_tf = tes_mod_tf(x_sfe_tf, x_te_tf)

print(output_tf.shape)
</code></pre>
<ol start=""2"">
<li>PyTorch Multi-head Attention</li>
</ol>
<pre><code>import torch
import torch.nn as nn

x_sfe_torch = torch.randn(64, 345, 64)
x_te_torch = torch.randn(64, 200, 64)

tes_mod_torch = nn.MultiheadAttention(embed_dim=64, num_heads=2)
output_torch = tes_mod_torch(x_sfe_torch, x_sfe_torch, x_te_torch)
print(output_torch.shape)
</code></pre>
<p>When I run the tensorflow's mha, it successfully returns <code>(64, 345, 64)</code>. But when I run the pytorch's mha, it returns this error:
<code>AssertionError: key shape torch.Size([64, 345, 64]) does not match value shape torch.Size([64, 200, 64])</code></p>
<p>The tensorflow version can return an output with the size of x_sfe, neglecting its size difference from x_te. In the other hand, pytorch version requires that x_sfe and x_te must have the same dimension. I am confused on how actually the tensorflow's Multi-head Attention module works? What is the difference between PyTorch and what is the correct input for the PyTorch? Thanks in advance.</p>
","transformer-model"
"76954009","Why does Cosine Similarity Score of Transformer-Based Model's Embeddings Always Lies Between 70 and 100","2023-08-22 13:52:49","","1","105","<transformer-model><word-embedding><cosine-similarity><variance><dimensionality-reduction>","<p>I have a question in word embeddings using transformer encoder models. Let's create word embeddings using the BERT model.</p>
<ul>
<li><p>Word 1: &quot;cat&quot; (em1)</p>
</li>
<li><p>Word 2: &quot;dog&quot; (em2)</p>
</li>
<li><p>Word 3: &quot;driver&quot; (em3)</p>
</li>
<li><p>Word 4: &quot;lion&quot; (em4)</p>
</li>
</ul>
<p>Let's take the cosine similarity score: (below cosine scores are not real , just for the sake of an example)</p>
<ul>
<li><p>cs(em1, em2) = 0.90</p>
</li>
<li><p>cs(em1, em3) = 0.70</p>
</li>
<li><p>cs(em1, em4) = 0.73</p>
</li>
</ul>
<p>The cosine scores always lie between 70 and 1. They do not go below 70 (for example, the cosine similarity score of &quot;cat&quot; and &quot;driver&quot;).</p>
<p>After applying PCA and reducing the dimension from 768 to 50 (reducing the variance in the vector), the score is now below 70.</p>
<p>My question is: high variance keeps a lot of information about the word, right? But the cosine score always lies between 70 and 100. Can anyone please tell me why this is happening? And why, when I reduce the dimension, the score goes below 70? This problem occurs not only with BERT, but with every transformer-based model.</p>
<p>Thanks in advance for your help!</p>
","transformer-model"
"76949530","Getting an error ""_kwargs = spec_.kwargs.copy()"" saying ""AttributeError: 'NoneType' object has no attribute 'copy'"" when running tensor2tensor","2023-08-22 01:11:18","","0","39","<transformer-model><piano><magenta><tensor2tensor><trax>","<p>I am currently trying to run <a href=""https://colab.research.google.com/notebooks/magenta/piano_transformer/piano_transformer.ipynb"" rel=""nofollow noreferrer"">this</a> google collab notebook. It is a notebook that creates a transformer model and takes input from piano performances and creates newly generated music. I am getting this error when I try importing the tensor2tensor modules.</p>
<p>ERROR ='NoneType' object has no attribute 'copy'</p>
<p>This notebook also somehow only runs on tensorflow v1, as opposed to the most current version.</p>
<p>I believe this has something to do with tensor2tensor currently being depreciated. I see there is a successor library called trax, but it does not have any of the models referenced in the notebook as of yet. Is it possible to migrate the code used in tensor2tensor to trax? I have never migrated github repositories before and any advice or information would be greatly appreciatred.</p>
<p>It seems like there are a million problems with this notebook, but it's such a good source of information that I don't want to give up on it.</p>
<p>Any guidance would be awesome. Thanks</p>
","transformer-model"
"76934291","Split an image into small patches","2023-08-19 08:53:29","76936093","0","900","<python><deep-learning><pytorch><computer-vision><transformer-model>","<p>I want to implement the Vision Transformer model, in the paper they stated that they split the input image into small patches of certain resolution, like if the image 64x64 and the patch resolution is 16x16, it will be split into 16 small patches each of resolution 16x16, so the final shape is (N,P,P,C), where N is the number of patches, P is the resolution, C is the number of channels.</p>
<p>What I tried so the splitting is vectorized :</p>
<pre><code>def image_to_patches_fast(image, res_patch):
    
    (H, W, C) = get_image_shape(image)
    
    
    if C == 1:
        image = image.convert('RGB')
        (H, W, C) = get_image_shape(image)
                    
    P = res_patch
    N = (H*W)//(P**2)
        
    image_tensor = torchvision.transforms.PILToTensor()(image).permute(1,2,0)
    image_patches = image_tensor.view(N,P,P,C)
</code></pre>
<p>the function works, but the output is not as intended, as when I try to visualize the patches, there's something wrong, the patches may not well positioned or I don't know, here's an exmaple:</p>
<p>the input image : <a href=""https://i.sstatic.net/lGy3t.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lGy3t.png"" alt=""input image"" /></a></p>
<p>the visualization of the output patches : <a href=""https://i.sstatic.net/IeXhZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IeXhZ.png"" alt=""patches"" /></a></p>
<p>the function to visualize the patches :</p>
<pre><code>def show_patches(patches):
    
    N,P = patches.shape[0], patches.shape[1]
       
    nrows, ncols = int(N**0.5),int(N**0.5)
    fig, axes = plt.subplots(nrows = nrows, ncols=ncols)
    for row in range(nrows):

        for col in range(ncols):

            idx = col + (row*nrows)
            
            axes[row][col].imshow(patches[idx,:,:,:])
            axes[row][col].axis(&quot;off&quot;)

    plt.subplots_adjust(left=0.1,
                    bottom=0.1,
                    right=0.9,
                    top=0.9,
                    wspace=0.1,
                    hspace=0.1)
    plt.show()
    
</code></pre>
<p>I tried another function to split the image, but it is slower as it uses loops, and it works as expected :</p>
<pre><code>def image_to_patches_slow(image, res_patch):
    
    (H, W, C) = get_image_shape(image)
    
    
    if C == 1:
        image = image.convert('RGB')
        (H, W, C) = get_image_shape(image)
                    
    P = res_patch
    N = (H*W)//(P**2)
    
    nrows, ncols = int(N**0.5), int(N**0.5)
    
    image_tensor = torchvision.transforms.PILToTensor()(image).permute(1,2,0)
    image_patches = torch.zeros((N,P,P,C),dtype = torch.int)
    
    
    for row in range(nrows):
        s_row = row * N
        e_row = (row * N) + N
        for col in range(ncols):

            idx = col + (row*nrows)

            s_col = col*N
            e_col = (col*N) + N
                
            image_patches[idx] = image_tensor[s_row:e_row, s_col:e_col]
    
    return image_patches
</code></pre>
<p>it's output: <a href=""https://i.sstatic.net/skcY3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/skcY3.png"" alt=""slow patches"" /></a></p>
<p>so any help as this slow version bottleneck the training.</p>
","transformer-model"
"76916197","Can't save trained transformer model","2023-08-16 18:32:43","","0","93","<python><tensorflow><keras><nlp><transformer-model>","<p>I have trained on Transformer model but cant save the best model. what is wrong here! Code is running fine and trained.</p>
<pre><code>if not os.path.exists(&quot;asr-checkpoint&quot;):
    os.makedirs(&quot;asr-checkpoint&quot;)

checkpoint_path = '/content/asr-checkpoint'
cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    monitor='val_loss',
    mode='min',
    save_best_only=True,
    verbose=1
)

print(&quot;tf.executing_eagerly():&quot;, tf.executing_eagerly())

history = model.fit(
    ds,
    validation_data=val_ds,
    callbacks=[display_cb],
    initial_epoch=0,
    epochs=1
)
</code></pre>
","transformer-model"
"76910644","Temporal Fusion Transformer: how to join the prediction with the original data","2023-08-16 05:05:54","","0","426","<time-series><transformer-model><pytorch-lightning><multivariate-time-series><pytorch-forecasting>","<p>I'm using Temporal Fusion Transformer to do sales forecast. For validation, how to join the prediction with the original data? I guess I need both the 'time_idx' and the group_ids to join the prediction with the original data. I have get the 'time_idx' from x, how to get the group_ids=['STORE_NUMBER'] information of the predictions.</p>
<p>Thanks</p>
<pre><code>    return TimeSeriesDataSet(
        data,
        time_idx=&quot;time_idx&quot;,
        target=&quot;SALES&quot;,
        group_ids=['STORE_NUMBER'],
        min_encoder_length=1,  # keep encoder length long (as it is in the validation set)
        max_encoder_length=max_encoder_length,
        min_prediction_length=1,
        max_prediction_length=max_prediction_length,
        static_categoricals=['CITY'],  
        static_reals=['STORE_AGE'],        
time_varying_known_categoricals=['day','week','holiday'], 
        time_varying_known_reals=[&quot;time_idx&quot;],
        time_varying_unknown_categoricals=[],
        time_varying_unknown_reals=[&quot;SALES&quot;],
        add_relative_time_idx=True,
        add_target_scales=True,
        add_encoder_length=True,
        allow_missing_timesteps=True,
        },
    )
</code></pre>
<pre><code>print(&quot;Keys in x:&quot;, x.keys())
print(&quot;Keys in raw_predictions:&quot;, raw_predictions.keys())

actuals = x['decoder_target']
times = x['decoder_time_idx']```
</code></pre>
","transformer-model"
"76910000","I was trying to use transformers by parsing the pipeline function for setiment analysis","2023-08-16 01:45:11","","1","448","<nlp><pipeline><huggingface-transformers><sentiment-analysis><transformer-model>","<p>While i was trying to perform sentiment analysis using transformers, then i parsed the pipeline function. Unfortunately, I ran into a runtime error: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):
cannot import name 'type_spec_registry' from 'tensorflow.python.framework'</p>
<p>This is my code:`</p>
<pre><code>from transformers import pipeline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix

</code></pre>
<p>#Basic usage
classifier = pipeline('sentiment-analysis')</p>
<pre><code>
This was the error i got;
ImportError                               Traceback (most recent call last)
File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\transformers\utils\import_utils.py:1099, in _LazyModule._get_module(self, module_name)
   1098 try:
-&gt; 1099     return importlib.import_module(&quot;.&quot; + module_name, self.__name__)
   1100 except Exception as e:

File ~\anaconda3\envs\Asiwajuflow\lib\importlib\__init__.py:127, in import_module(name, package)
    126         level += 1
--&gt; 127 return _bootstrap._gcd_import(name[level:], package, level)

File &lt;frozen importlib._bootstrap&gt;:1030, in _gcd_import(name, package, level)

File &lt;frozen importlib._bootstrap&gt;:1007, in _find_and_load(name, import_)

File &lt;frozen importlib._bootstrap&gt;:986, in _find_and_load_unlocked(name, import_)

File &lt;frozen importlib._bootstrap&gt;:680, in _load_unlocked(spec)

File &lt;frozen importlib._bootstrap_external&gt;:850, in exec_module(self, module)

File &lt;frozen importlib._bootstrap&gt;:228, in _call_with_frames_removed(f, *args, **kwds)

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\transformers\models\distilbert\modeling_tf_distilbert.py:28
     26 import tensorflow as tf
---&gt; 28 from ...activations_tf import get_tf_activation
     29 from ...modeling_tf_outputs import (
     30     TFBaseModelOutput,
     31     TFMaskedLMOutput,
   (...)
     35     TFTokenClassifierOutput,
     36 )

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\transformers\activations_tf.py:107
    105     return tf.keras.activations.gelu(x, approximate=True)
--&gt; 107 gelu = tf.keras.activations.gelu
    108 gelu_new = approximate_gelu_wrap

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\tensorflow\python\util\lazy_loader.py:58, in LazyLoader.__getattr__(self, item)
     57 def __getattr__(self, item):
---&gt; 58   module = self._load()
     59   return getattr(module, item)

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\tensorflow\python\util\lazy_loader.py:41, in LazyLoader._load(self)
     40 # Import the target module and insert it into the parent's namespace
---&gt; 41 module = importlib.import_module(self.__name__)
     42 self._parent_module_globals[self._local_name] = module

File ~\anaconda3\envs\Asiwajuflow\lib\importlib\__init__.py:127, in import_module(name, package)
    126         level += 1
--&gt; 127 return _bootstrap._gcd_import(name[level:], package, level)

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\__init__.py:20
     15 &quot;&quot;&quot;Implementation of the Keras API, the high-level API of TensorFlow.
     16 
     17 Detailed documentation and user guides are available at
     18 [keras.io](https://keras.io).
     19 &quot;&quot;&quot;
---&gt; 20 from keras import distribute
     21 from keras import models

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\distribute\__init__.py:18
     15 &quot;&quot;&quot;Keras' Distribution Strategy library.&quot;&quot;&quot;
---&gt; 18 from keras.distribute import sidecar_evaluator

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\distribute\sidecar_evaluator.py:22
     21 from tensorflow.python.util import deprecation
---&gt; 22 from keras.optimizers.optimizer_experimental import (
     23     optimizer as optimizer_experimental,
     24 )
     25 from tensorflow.python.util.tf_export import keras_export

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\optimizers\__init__.py:25
     24 # Imports needed for deserialization.
---&gt; 25 from keras import backend
     26 from keras.optimizers.legacy import adadelta as adadelta_legacy

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\backend\__init__.py:3
      1 &quot;&quot;&quot;AUTOGENERATED. DO NOT EDIT.&quot;&quot;&quot;
----&gt; 3 from keras.backend import experimental
      4 from keras.src.backend import abs

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\backend\experimental\__init__.py:3
      1 &quot;&quot;&quot;AUTOGENERATED. DO NOT EDIT.&quot;&quot;&quot;
----&gt; 3 from keras.src.backend import disable_tf_random_generator
      4 from keras.src.backend import enable_tf_random_generator

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\src\__init__.py:21
     20 from keras.src import distribute
---&gt; 21 from keras.src import models
     22 from keras.src.engine.input_layer import Input

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\src\models\__init__.py:18
     15 &quot;&quot;&quot;Keras models API.&quot;&quot;&quot;
---&gt; 18 from keras.src.engine.functional import Functional
     19 from keras.src.engine.sequential import Sequential

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\src\engine\functional.py:25
     23 import tensorflow.compat.v2 as tf
---&gt; 25 from keras.src import backend
     26 from keras.src.dtensor import layout_map as layout_map_lib

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\src\backend.py:35
     34 from keras.src.dtensor import dtensor_api as dtensor
---&gt; 35 from keras.src.engine import keras_tensor
     36 from keras.src.utils import control_flow_util

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\src\engine\keras_tensor.py:19
     17 import tensorflow.compat.v2 as tf
---&gt; 19 from keras.src.utils import object_identity
     21 # isort: off

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\src\utils\__init__.py:53
     52 # Preprocessing utils
---&gt; 53 from keras.src.utils.feature_space import FeatureSpace
     55 # Internal

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\src\utils\feature_space.py:20
     19 from keras.src import backend
---&gt; 20 from keras.src.engine import base_layer
     21 from keras.src.saving import saving_lib

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\src\engine\base_layer.py:39
     38 from keras.src.engine import keras_tensor
---&gt; 39 from keras.src.engine import node as node_module
     40 from keras.src.mixed_precision import autocast_variable

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\src\engine\node.py:28
     27 from keras.src.engine import base_layer_utils
---&gt; 28 from keras.src.saving.legacy.saved_model import json_utils
     29 from keras.src.utils import tf_utils

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\keras\src\saving\legacy\saved_model\json_utils.py:38
     37 # isort: off
---&gt; 38 from tensorflow.python.framework import type_spec_registry
     40 _EXTENSION_TYPE_SPEC = &quot;_EXTENSION_TYPE_SPEC&quot;

ImportError: cannot import name 'type_spec_registry' from 'tensorflow.python.framework' (C:\Users\hp\anaconda3\envs\Asiwajuflow\lib\site-packages\tensorflow\python\framework\__init__.py)

The above exception was the direct cause of the following exception:

RuntimeError                              Traceback (most recent call last)
Cell In[9], line 2
      1 #Basic usage
----&gt; 2 classifier = pipeline('sentiment-analysis')

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\transformers\pipelines\__init__.py:788, in pipeline(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
    786 if isinstance(model, str) or framework is None:
    787     model_classes = {&quot;tf&quot;: targeted_task[&quot;tf&quot;], &quot;pt&quot;: targeted_task[&quot;pt&quot;]}
--&gt; 788     framework, model = infer_framework_load_model(
    789         model,
    790         model_classes=model_classes,
    791         config=config,
    792         framework=framework,
    793         task=task,
    794         **hub_kwargs,
    795         **model_kwargs,
    796     )
    798 model_config = model.config
    799 hub_kwargs[&quot;_commit_hash&quot;] = model.config._commit_hash

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\transformers\pipelines\base.py:245, in infer_framework_load_model(model, config, model_classes, task, framework, **model_kwargs)
    243         classes.append(_class)
    244 if look_tf:
--&gt; 245     _class = getattr(transformers_module, f&quot;TF{architecture}&quot;, None)
    246     if _class is not None:
    247         classes.append(_class)

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\transformers\utils\import_utils.py:1090, in _LazyModule.__getattr__(self, name)
   1088 elif name in self._class_to_module.keys():
   1089     module = self._get_module(self._class_to_module[name])
-&gt; 1090     value = getattr(module, name)
   1091 else:
   1092     raise AttributeError(f&quot;module {self.__name__} has no attribute {name}&quot;)

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\transformers\utils\import_utils.py:1089, in _LazyModule.__getattr__(self, name)
   1087     value = self._get_module(name)
   1088 elif name in self._class_to_module.keys():
-&gt; 1089     module = self._get_module(self._class_to_module[name])
   1090     value = getattr(module, name)
   1091 else:

File ~\anaconda3\envs\Asiwajuflow\lib\site-packages\transformers\utils\import_utils.py:1101, in _LazyModule._get_module(self, module_name)
   1099     return importlib.import_module(&quot;.&quot; + module_name, self.__name__)
   1100 except Exception as e:
-&gt; 1101     raise RuntimeError(
   1102         f&quot;Failed to import {self.__name__}.{module_name} because of the following error (look up to see its&quot;
   1103         f&quot; traceback):\n{e}&quot;
   1104     ) from e

RuntimeError: Failed to import transformers.models.distilbert.modeling_tf_distilbert because of the following error (look up to see its traceback):
cannot import name 'type_spec_registry' from 'tensorflow.python.framework' (C:\Users\hp\anaconda3\envs\Asiwajuflow\lib\site-packages\tensorflow\python\framework\__init__.py)
</code></pre>
","transformer-model"
"76909744","how to understand Keras pretrained weight file? (I get non-sense as output)","2023-08-16 00:08:42","","0","75","<python><tensorflow><pytorch><transformer-model><pre-trained-model>","<p>I'm new in both Tensorflow and Pytorch and I'm trying to implement Pytorch version of an available code in Tensorflow.
Pretrained weight file of the TF code is available and I was hoping to be able to load this weights on my Pytorch version. however, I get <em>UnpicklingError: invalid load key, 'H'</em> when I was trying to load this weights using:</p>
<pre><code>weight_file = &quot;pretrained.h5&quot;
model.load_state_dict(torch.load(weight_file ))

</code></pre>
<p>I tried to print weights using:</p>
<pre><code>file = h5py.File(weight_file , 'r')
    for key in file.keys():
        print(key, file[key])
    file.close()
</code></pre>
<p>and I got hundreds of lines in the output and I can't understand it (I couldn't put all of the output here cause it was too long):</p>
<pre><code>add_loss &lt;HDF5 group &quot;/add_loss&quot; (0 members)&gt;
add_loss_1 &lt;HDF5 group &quot;/add_loss_1&quot; (0 members)&gt;
add_loss_10 &lt;HDF5 group &quot;/add_loss_10&quot; (0 members)&gt;
add_loss_11 &lt;HDF5 group &quot;/add_loss_11&quot; (0 members)&gt;
add_loss_12 &lt;HDF5 group &quot;/add_loss_12&quot; (0 members)&gt;
add_loss_13 &lt;HDF5 group &quot;/add_loss_13&quot; (0 members)&gt;
add_loss_14 &lt;HDF5 group &quot;/add_loss_14&quot; (0 members)&gt;
add_loss_15 &lt;HDF5 group &quot;/add_loss_15&quot; (0 members)&gt;
add_loss_16 &lt;HDF5 group &quot;/add_loss_16&quot; (0 members)&gt;
add_loss_17 &lt;HDF5 group &quot;/add_loss_17&quot; (0 members)&gt;
add_loss_18 &lt;HDF5 group &quot;/add_loss_18&quot; (0 members)&gt;
add_loss_19 &lt;HDF5 group &quot;/add_loss_19&quot; (0 members)&gt;
add_loss_2 &lt;HDF5 group &quot;/add_loss_2&quot; (0 members)&gt;
add_loss_20 &lt;HDF5 group &quot;/add_loss_20&quot; (0 members)&gt;
add_loss_21 &lt;HDF5 group &quot;/add_loss_21&quot; (0 members)&gt;
...
concatenate &lt;HDF5 group &quot;/concatenate&quot; (0 members)&gt;
concatenate_1 &lt;HDF5 group &quot;/concatenate_1&quot; (0 members)&gt;
concatenate_2 &lt;HDF5 group &quot;/concatenate_2&quot; (0 members)&gt;
concatenate_3 &lt;HDF5 group &quot;/concatenate_3&quot; (0 members)&gt;
dense_714 &lt;HDF5 group &quot;/dense_714&quot; (1 members)&gt;
dense_715 &lt;HDF5 group &quot;/dense_715&quot; (1 members)&gt;
dense_716 &lt;HDF5 group &quot;/dense_716&quot; (1 members)&gt;
dense_717 &lt;HDF5 group &quot;/dense_717&quot; (1 members)&gt;
dense_718 &lt;HDF5 group &quot;/dense_718&quot; (1 members)&gt;
dense_719 &lt;HDF5 group &quot;/dense_719&quot; (1 members)&gt;
dense_720 &lt;HDF5 group &quot;/dense_720&quot; (1 members)&gt;
dense_721 &lt;HDF5 group &quot;/dense_721&quot; (1 members)&gt;
dense_722 &lt;HDF5 group &quot;/dense_722&quot; (1 members)&gt;
...
dropout_538 &lt;HDF5 group &quot;/dropout_538&quot; (0 members)&gt;
dropout_539 &lt;HDF5 group &quot;/dropout_539&quot; (0 members)&gt;
dropout_540 &lt;HDF5 group &quot;/dropout_540&quot; (0 members)&gt;
dropout_541 &lt;HDF5 group &quot;/dropout_541&quot; (0 members)&gt;
dropout_542 &lt;HDF5 group &quot;/dropout_542&quot; (0 members)&gt;
dropout_543 &lt;HDF5 group &quot;/dropout_543&quot; (0 members)&gt;
dropout_544 &lt;HDF5 group &quot;/dropout_544&quot; (0 members)&gt;
dropout_545 &lt;HDF5 group &quot;/dropout_545&quot; (0 members)&gt;
dropout_546 &lt;HDF5 group &quot;/dropout_546&quot; (0 members)&gt;
...
input_4 &lt;HDF5 group &quot;/input_4&quot; (0 members)&gt;
input_5 &lt;HDF5 group &quot;/input_5&quot; (0 members)&gt;
input_6 &lt;HDF5 group &quot;/input_6&quot; (0 members)&gt;
lambda_10 &lt;HDF5 group &quot;/lambda_10&quot; (0 members)&gt;
lambda_100 &lt;HDF5 group &quot;/lambda_100&quot; (0 members)&gt;
lambda_101 &lt;HDF5 group &quot;/lambda_101&quot; (0 members)&gt;
lambda_102 &lt;HDF5 group &quot;/lambda_102&quot; (0 members)&gt;
lambda_103 &lt;HDF5 group &quot;/lambda_103&quot; (0 members)&gt;
lambda_104 &lt;HDF5 group &quot;/lambda_104&quot; (0 members)&gt;
lambda_105 &lt;HDF5 group &quot;/lambda_105&quot; (0 members)&gt;
lambda_106 &lt;HDF5 group &quot;/lambda_106&quot; (0 members)&gt;
lambda_107 &lt;HDF5 group &quot;/lambda_107&quot; (0 members)&gt;
...
masking_models &lt;HDF5 group &quot;/masking_models&quot; (4 members)&gt;
masking_models_1 &lt;HDF5 group &quot;/masking_models_1&quot; (4 members)&gt;
masking_models_10 &lt;HDF5 group &quot;/masking_models_10&quot; (4 members)&gt;
masking_models_100 &lt;HDF5 group &quot;/masking_models_100&quot; (4 members)&gt;
masking_models_101 &lt;HDF5 group &quot;/masking_models_101&quot; (4 members)&gt;
masking_models_102 &lt;HDF5 group &quot;/masking_models_102&quot; (4 members)&gt;
...
output_0 &lt;HDF5 group &quot;/output_0&quot; (1 members)&gt;
output_1 &lt;HDF5 group &quot;/output_1&quot; (1 members)&gt;
output_10 &lt;HDF5 group &quot;/output_10&quot; (1 members)&gt;
output_11 &lt;HDF5 group &quot;/output_11&quot; (1 members)&gt;
output_12 &lt;HDF5 group &quot;/output_12&quot; (1 members)&gt;
output_13 &lt;HDF5 group &quot;/output_13&quot; (1 members)&gt;
output_14 &lt;HDF5 group &quot;/output_14&quot; (1 members)&gt;
output_15 &lt;HDF5 group &quot;/output_15&quot; (1 members)&gt;
...
position_embedding &lt;HDF5 group &quot;/position_embedding&quot; (1 members)&gt;
position_embedding_1 &lt;HDF5 group &quot;/position_embedding_1&quot; (1 members)&gt;
position_embedding_2 &lt;HDF5 group &quot;/position_embedding_2&quot; (1 members)&gt;
tf.compat.v1.transpose &lt;HDF5 group &quot;/tf.compat.v1.transpose&quot; (0 members)&gt;
tf.compat.v1.transpose_1 &lt;HDF5 group &quot;/tf.compat.v1.transpose_1&quot; (0 members)&gt;
tf.compat.v1.transpose_10 &lt;HDF5 group &quot;/tf.compat.v1.transpose_10&quot; (0 members)&gt;
tf.compat.v1.transpose_100 &lt;HDF5 group &quot;/tf.compat.v1.transpose_100&quot; (0 members)&gt;
tf.compat.v1.transpose_101 &lt;HDF5 group &quot;/tf.compat.v1.transpose_101&quot; (0 members)&gt;
...
tf.expand_dims &lt;HDF5 group &quot;/tf.expand_dims&quot; (0 members)&gt;
tf.expand_dims_1 &lt;HDF5 group &quot;/tf.expand_dims_1&quot; (0 members)&gt;
tf.expand_dims_10 &lt;HDF5 group &quot;/tf.expand_dims_10&quot; (0 members)&gt;
tf.expand_dims_100 &lt;HDF5 group &quot;/tf.expand_dims_100&quot; (0 members)&gt;
tf.expand_dims_101 &lt;HDF5 group &quot;/tf.expand_dims_101&quot; (0 members)&gt;
...
tf.math.reduce_mean &lt;HDF5 group &quot;/tf.math.reduce_mean&quot; (0 members)&gt;
tf.math.reduce_mean_1 &lt;HDF5 group &quot;/tf.math.reduce_mean_1&quot; (0 members)&gt;
tf.math.reduce_mean_10 &lt;HDF5 group &quot;/tf.math.reduce_mean_10&quot; (0 members)&gt;
tf.math.reduce_mean_11 &lt;HDF5 group &quot;/tf.math.reduce_mean_11&quot; (0 members)&gt;
tf.math.reduce_mean_12 &lt;HDF5 group &quot;/tf.math.reduce_mean_12&quot; (0 members)&gt;
tf.math.reduce_mean_13 &lt;HDF5 group &quot;/tf.math.reduce_mean_13&quot; (0 members)&gt;
...
tf.math.square &lt;HDF5 group &quot;/tf.math.square&quot; (0 members)&gt;
tf.math.square_1 &lt;HDF5 group &quot;/tf.math.square_1&quot; (0 members)&gt;
tf.math.square_10 &lt;HDF5 group &quot;/tf.math.square_10&quot; (0 members)&gt;
tf.math.square_11 &lt;HDF5 group &quot;/tf.math.square_11&quot; (0 members)&gt;
tf.math.square_12 &lt;HDF5 group &quot;/tf.math.square_12&quot; (0 members)&gt;
tf.math.square_13 &lt;HDF5 group &quot;/tf.math.square_13&quot; (0 members)&gt;
tf.math.square_14 &lt;HDF5 group &quot;/tf.math.square_14&quot; (0 members)&gt;
tf.math.square_15 &lt;HDF5 group &quot;/tf.math.square_15&quot; (0 members)&gt;
tf.math.square_16 &lt;HDF5 group &quot;/tf.math.square_16&quot; (0 members)&gt;
tf.math.square_17 &lt;HDF5 group &quot;/tf.math.square_17&quot; (0 members)&gt;
...
tf.math.subtract &lt;HDF5 group &quot;/tf.math.subtract&quot; (0 members)&gt;
tf.math.subtract_1 &lt;HDF5 group &quot;/tf.math.subtract_1&quot; (0 members)&gt;
tf.math.subtract_10 &lt;HDF5 group &quot;/tf.math.subtract_10&quot; (0 members)&gt;
tf.math.subtract_11 &lt;HDF5 group &quot;/tf.math.subtract_11&quot; (0 members)&gt;
tf.math.subtract_12 &lt;HDF5 group &quot;/tf.math.subtract_12&quot; (0 members)&gt;
tf.math.subtract_13 &lt;HDF5 group &quot;/tf.math.subtract_13&quot; (0 members)&gt;
tf.math.subtract_14 &lt;HDF5 group &quot;/tf.math.subtract_14&quot; (0 members)&gt;
...
top_level_model_weights &lt;HDF5 group &quot;/top_level_model_weights&quot; (0 members)&gt;
transformer_block &lt;HDF5 group &quot;/transformer_block&quot; (3 members)&gt;
transformer_block_1 &lt;HDF5 group &quot;/transformer_block_1&quot; (3 members)&gt;
transformer_block_2 &lt;HDF5 group &quot;/transformer_block_2&quot; (3 members)&gt;
transformer_block_3 &lt;HDF5 group &quot;/transformer_block_3&quot; (3 members)&gt;
transformer_block_4 &lt;HDF5 group &quot;/transformer_block_4&quot; (3 members)&gt;
zero_padding1d &lt;HDF5 group &quot;/zero_padding1d&quot; (0 members)&gt;
zero_padding1d_1 &lt;HDF5 group &quot;/zero_padding1d_1&quot; (0 members)&gt;
zero_padding1d_10 &lt;HDF5 group &quot;/zero_padding1d_10&quot; (0 members)&gt;
zero_padding1d_100 &lt;HDF5 group &quot;/zero_padding1d_100&quot; (0 members)&gt;
zero_padding1d_101 &lt;HDF5 group &quot;/zero_padding1d_101&quot; (0 members)&gt;
zero_padding1d_102 &lt;HDF5 group &quot;/zero_padding1d_102&quot; (0 members)&gt;
zero_padding1d_103 &lt;HDF5 group &quot;/zero_padding1d_103&quot; (0 members)&gt;
zero_padding1d_104 &lt;HDF5 group &quot;/zero_padding1d_104&quot; (0 members)&gt;
...
</code></pre>
","transformer-model"
"76902580","Python BERTopic 'numpy.float64' object cannot be interpreted as an integer","2023-08-14 22:53:54","","1","94","<python-3.x><bert-language-model><transformer-model><topic-modeling>","<p>I am trying to replicate the Topic Modeling exercise from this article titled <a href=""https://hackernoon.com/nlp-tutorial-topic-modeling-in-python-with-bertopic-372w35l9"" rel=""nofollow noreferrer"">NLP Tutorial: Topic Modeling in Python with BerTopic</a>. The article comes from the website HackerNoon if you'd prefer to find it yourself. All the code used is clearly written in that article. There is also a link to the free data used in the example in the article.</p>
<p>I am using Python 3.10.12 and Jupyter notebook. Following the example:</p>
<pre><code>!pip install bertopic
!pip install bertopic[visualization]

import pandas as pd 
import numpy as np
from bertopic import BERTopic

df = pd.read_csv(&quot;~/tokyo_2020_tweets.csv&quot;, engine='python') 
df = df[0:6000]

model = BERTopic(verbose=True)
docs = df.text.to_list()
topics, probabilities = model.fit_transform(docs)
</code></pre>
<p>When I run that last line, I get the following error:</p>
<blockquote>
<p>TypeError: 'numpy.float64' object cannot be interpreted as an integer</p>
</blockquote>
<p>I am following the steps exactly, so I am not sure what's going wrong (I am new in Python). What causes this error and how do I fix it?</p>
","transformer-model"
"76893626","Python KeyError problem when loading the saved model in pytorch","2023-08-13 13:39:50","","0","347","<python><pytorch><nlp><keyerror><transformer-model>","<p>I am new to deep learning. I am using a transformer model for bengali language stemming process. Now, I trainied the model and test it with some data and it worked fine. So i saved the model. But when i delete the runtime and create a new runtime and load the previously trained model i get a KeyError. I don't know why I'm seeing this. I hope you guys can help.</p>
<p>Here is the notebook (when i run the model within same runtime) : <a href=""https://colab.research.google.com/drive/1TaLOluMViu8jVqCN5F0UbJyP8rZjlhE5?usp=sharing"" rel=""nofollow noreferrer"">Google colab notebook</a></p>
<p>Here you can find the dataset : <a href=""https://drive.google.com/file/d/1-HjpNum2GpgnBe-qGeqfkXjyBHitXJy2/view?usp=sharing"" rel=""nofollow noreferrer"">Dataset</a></p>
<p>Here is what happens when i try to load a trained model : <a href=""https://colab.research.google.com/drive/1q8moMBJpFrMa6EJAegSqYPn7k2wvSj90#scrollTo=AQ423XQWAgc6"" rel=""nofollow noreferrer"">colab link</a></p>
<p>Trained model link : <a href=""https://drive.google.com/file/d/18pqiBHU3MCVZIIxe9jUE4wZ4sA-mdxVe/view?usp=sharing"" rel=""nofollow noreferrer"">model link</a></p>
<p>I think the problem is when I am loading a trained model a 'PAD' token is being added with the given word. But it doesn't happen if i train the model and use it in the same runtime. It only happens when i change the runtime and load a saved model. I've tried to fix it but i can't really find the solution. I hope you guys can find it.</p>
<p>I'll be of great help for me if someone can help. Thank you in advance.</p>
","transformer-model"
"76886837","with torch.no_grad() Changes Sequence Length During Evaluation Mode","2023-08-11 21:50:18","76886958","1","117","<python><deep-learning><pytorch><transformer-model><encoder-decoder>","<p>I built a TransformerEncoder model, and it changes the output's sequence length if I use &quot;with torch.no_grad()&quot; during the evaluation mode.</p>
<p>My model details:</p>
<pre><code>class TransEnc(nn.Module):
    def __init__(self,ntoken: int,encoder_embedding_dim: int,max_item_count: int,encoder_num_heads: int,encoder_hidden_dim: int,encoder_num_layers: int,padding_idx: int,dropout: float = 0.2):
        super().__init__()
        self.encoder_embedding = nn.Embedding(ntoken, encoder_embedding_dim, padding_idx=padding_idx)
        self.pos_encoder = PositionalEncoding(encoder_embedding_dim, max_item_count, dropout)
        encoder_layers = nn.TransformerEncoderLayer(encoder_embedding_dim, encoder_num_heads, encoder_hidden_dim, dropout, batch_first=True) 
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, encoder_num_layers)
        self.encoder_embedding_dim = encoder_embedding_dim

def forward(self,src: torch.Tensor,src_key_padding_mask: torch.Tensor = None) -&gt; torch.Tensor:
        src = self.encoder_embedding(src.long()) * math.sqrt(self.encoder_embedding_dim)
        src = self.pos_encoder(src)
        src = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)
</code></pre>
<p>with</p>
<pre><code>batch_size = 32
ntoken = 4096
encoder_embedding_dim = 256
max_item_count = 64 # max sequence length with padding
encoder_num_heads = 8
encoder_hidden_dim = 256
encoder_num_layers = 4
padding_idx = 0
</code></pre>
<p>I have a tensor (src) containing 32 word-level tokenized sentences (with different paddings) with a shape of (32,64)(batch_size,max_item_count).</p>
<p>When I activate training mode with &quot;model.train()&quot;, set &quot;src_key_padding_mask = src == tokenizer.pad_token_id&quot; and run &quot;logits = model(src = src, src_key_padding_mask = src_key_padding_mask)&quot;, I get logits with an expected shape of (32,64,256)(batch_size,max_item_count,encoder_embedding_dim).</p>
<p>However, when I activate evaluation mode with &quot;model.eval()&quot;, set &quot;src_key_padding_mask = src == tokenizer.pad_token_id&quot; and run with &quot;torch.no_grad(): logits = model(src = src, src_key_padding_mask = src_key_padding_mask)&quot;, I get different logits' shapes every time like (32,31,256), (32,25,256), etc. I want to get logits with a shape of (32,64,256). How can I solve this problem?</p>
<p>OS: Windows 10 x64</p>
<p>Python: 3.10.12</p>
<p>Torch: Tried on both 1.13.1+cu117 and 2.0.1+cu117, but the problem is still the same.</p>
","transformer-model"
"76880211","How to visualize a heatmap in a classification task?","2023-08-11 01:35:38","","0","100","<python><deep-learning><pytorch><classification><transformer-model>","<p>I am now with a classification task using a ViT-like model. And I am trying to create a heatmap like this.</p>
<p><a href=""https://pic1.zhimg.com/v2-33e37193b9a3a6a17b9de380a1b14c70"" rel=""nofollow noreferrer""></a></p>
<p>I have look up the site and found this <a href=""https://stackoverflow.com/questions/45005313/how-to-visualize-an-attention-mechanism-in-a-classification-task""></a> but no solution was given. So i am trying to bring it up again. Help me folks! Thanks a lot.</p>
","transformer-model"
"76854457","cocoeval change the number of keypoints and self.kpt_oks_sigmas into 14 but receive error","2023-08-07 18:51:56","77372345","0","163","<python><evaluation><transformer-model>","<p>I am trying to create keypoints detection using transformer and cocoapi. For evaluation, I use cocoeval and change the &quot;self.kpt_oks_sigmas&quot; from 17 into 14:</p>
<pre><code>self.kpt_oks_sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 
                     1.07, 1.07, .87, .87, .89, .89])/10.0
</code></pre>
<p>into</p>
<pre><code>self.kpt_oks_sigmas = np.array([1.07, .87, .89, 1.07, .87, .89, 1., 1., .79, .72, .62, .79, .72, .62])/10.0
</code></pre>
<p>However, I received error message that says:</p>
<p><a href=""https://i.sstatic.net/hddrK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hddrK.png"" alt=""error message"" /></a></p>
<p>Does anyone knows what should I do to fix this?
Thank you</p>
","transformer-model"
"76828036","Try to fix 'object is not callable' problem","2023-08-03 12:23:43","","0","302","<python><transformer-model><pytorch-lightning><pytorch-dataloader>","<p>I'm new to nn.lightning style, and I'm working on a space-time transformer project. I try to modify the training script to use my own data set for training (the original version is using CIFAR10/MNIST).</p>
<p>I've got a problem with the data module.</p>
<p>Here's the code.</p>
<pre><code>def create_dset(config):
    INV_SCALER = lambda x: x
    SCALER = lambda x: x
    NULL_VAL = None
    PLOT_VAR_IDXS = None
    PLOT_VAR_NAMES = None
    PAD_VAL = None


    if config.dset in [&quot;mnist&quot;, &quot;cifar&quot;]:
        if config.dset == &quot;mnist&quot;:
            config.target_points = 28 - config.context_points
            datasetCls = stf.data.image_completion.MNISTDset
            PLOT_VAR_IDXS = [18, 24]
            PLOT_VAR_NAMES = [&quot;18th row&quot;, &quot;24th row&quot;]
        else:
            config.target_points = 32 * 32 - config.context_points
            datasetCls = stf.data.image_completion.CIFARDset
            PLOT_VAR_IDXS = [0]
            PLOT_VAR_NAMES = [&quot;Reds&quot;]
        DATA_MODULE = stf.data.DataModule(
            datasetCls=datasetCls,
            dataset_kwargs={&quot;context_points&quot;: config.context_points},
            batch_size=config.batch_size,
            workers=config.workers,
            overfit=args.overfit,
        )

        return (
            DATA_MODULE,
            INV_SCALER,
            SCALER,
            NULL_VAL,
            PLOT_VAR_IDXS,
            PLOT_VAR_NAMES,
            PAD_VAL,
        )
    # Try to use my own data set here
    elif config.dset == &quot;custom&quot;:
        data_dir = &quot;./spacetimeformer/mydata&quot;

        # Define data transformations
        transform = transforms.Compose([
            transforms.Resize((256, 256)),  # Resize the images to a specific size
            transforms.ToTensor(),          # Convert images to tensors
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),  # Normalize the pixel values
        ])
        
        # Create the custom image dataset
        dataset = CUSTOMDset(data_dir, context_points=config.context_points, transform=transform)
        
        DATA_MODULE = stf.data.DataModule(
            datasetCls=dataset,
            dataset_kwargs={&quot;context_points&quot;: config.context_points},
            batch_size=config.batch_size,
            workers=config.workers,
            overfit=args.overfit,
        )

        # Rest of the values remain the same as in the &quot;mnist&quot; and &quot;cifar&quot; cases
        PLOT_VAR_IDXS = [0]
        PLOT_VAR_NAMES = [&quot;Reds&quot;]
        PAD_VAL = None
    
        return (
            DATA_MODULE,
            INV_SCALER,
            SCALER,
            NULL_VAL,
            PLOT_VAR_IDXS,
            PLOT_VAR_NAMES,
            PAD_VAL,
        )
</code></pre>
<p>I call <code>create_dset</code> later:</p>
<pre><code>    (
        data_module,
        inv_scaler,
        scaler,
        null_val,
        plot_var_idxs,
        plot_var_names,
        pad_val,
    ) = create_dset(args)
</code></pre>
<p>and I use it in <code>test_dataloader = data_module.test_dataloader()</code>.
**
Here I got an error message: <code>data_module</code> object is not callable.**</p>
<p>I think the problem may be with the <code>CUSTOMDset</code> I defined in another file, so I also enclosed it here.</p>
<pre><code>class CUSTOMDset(Dataset):
    def __init__(self, data_dir, context_points, transform=None):
        self.data_dir = data_dir
        self.context_points = context_points
        self.transform = transform
        self.file_list = os.listdir(self.data_dir)

    def __len__(self):
        return len(self.file_list)

    def __getitem__(self, i):
        img_name = os.path.join(self.data_dir, self.file_list[i])
        img = Image.open(img_name)
        y = img.convert(&quot;RGB&quot;)

        y_c = y.crop((0, 0, self.context_points, y.size[1]))
        y_t = y.crop((self.context_points, 0, y.size[0], y.size[1]))

        x = torch.arange(y.size[1]).view(-1, 1).float() / y.size[1]
        x_c = x[: self.context_points]
        x_t = x[self.context_points:]

        if self.transform:
            y_c = self.transform(y_c)
            y_t = self.transform(y_t)

        return x_c, y_c, x_t, y_t
</code></pre>
<p>I've tried to print <code>data_module</code>. The result is</p>
<pre><code>&lt;spacetimeformer.data.datamodule.DataModule object at 0x7f59cd8f84c0&gt;
</code></pre>
<p>Error message (detailed):</p>
<pre><code>Traceback (most recent call last):
  File &quot;train_my.py&quot;, line 525, in &lt;module&gt;
    main(args)
  File &quot;train_my.py&quot;, line 435, in main
    test_dataloader = data_module.test_dataloader()
  File &quot;/home/spacetimeformer/spacetimeformer/data/datamodule.py&quot;, line 37, in test_dataloader
    return self._make_dloader(&quot;test&quot;, shuffle=shuffle)
  File &quot;/home/spacetimeformer/spacetimeformer/data/datamodule.py&quot;, line 44, in _make_dloader
    self.datasetCls(**self.dataset_kwargs, split=split),
TypeError: 'CUSTOMDset' object is not callable
</code></pre>
<p>The environment: python=3.8, torch=2.0.1</p>
<p><strong>Any idea or suggestion will be highly appreciated! Thanks in advance.</strong></p>
<p>(adding some relevant code--23:46 2023/8/3)</p>
<pre><code>def test_dataloader(self, shuffle=False):
    return self._make_dloader(&quot;test&quot;, shuffle=shuffle)

def _make_dloader(self, split, shuffle=False):
    if self.overfit:
        split = &quot;train&quot;
        shuffle = True
    return DataLoader(
        self.datasetCls(**self.dataset_kwargs, split=split),
        shuffle=shuffle,
        batch_size=self.batch_size,
        num_workers=self.workers,
        collate_fn=self.collate_fn,
    )
</code></pre>
","transformer-model"
"76818252","Transformer handling null values","2023-08-02 09:03:16","76840316","0","239","<nlp><transformer-model><monad-transformers>","<p>user id |    event   | date | amount | duration(min) |<br />
124556.      login     02/23.  N/A.      23.<br />
124556.    withdrawal  02/28.  500       2.<br />
124556.      login     03/02.  N/A.      12.<br />
124556.      p2p       03/03.  234       3.<br />
124556.      p2p       03/12.  213       1.<br />
124556.      deposit   03/23.  511       5.</p>
<p>I am currently trying to use transformer for classification problem.Above is the sample dataframe.I want to encode the event column and convert the dataframe into a time series data.</p>
<p>I want to preserve the time order for each event while keeping the 'amount' and 'duration' columns, but not all events have corresponding 'amount' values. For example, when the event is 'login', the corresponding 'amount' is null. How can I preserve the null values? Can Transformer handle the null values?</p>
","transformer-model"
"76799487","Save trained weights in machine learning code","2023-07-30 20:18:05","76799554","0","107","<python><keras><deep-learning><transformer-model>","<p>i have colab for running machine learning model but when it gets 80 epoch my colab ram crashes and i can not go beind the 80 epochs.i want to somebody to help me save the trained weights somewhere and after the ram crashes i start training the model from that epoch.This is my code, how can i write that purpose code in this python code and where?</p>
<pre><code>for comm_round in range(comms_round):

    global_weights = global_model.get_weights()

    scaled_local_weight_list = list()

    client_names= list(clients_batched.keys())
    random.shuffle(client_names)

    for client in client_names:
        local_model = Transformer
        local_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),
                            optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),
                            metrics='acc')

        local_model.set_weights(global_weights)

        local_model.fit(clients_batched[client], epochs=1, verbose=0, callbacks=[checkpoint_callback])

        scaling_factor = weight_scalling_factor(clients_batched, client)
        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)
        scaled_local_weight_list.append(scaled_weights)

        K.clear_session()

    average_weights = sum_scaled_weights(scaled_local_weight_list)

    global_model.set_weights(average_weights)

    for(X_test, Y_test) in test_batched:
        global_acc, global_loss = test_model(test_x, test_y, global_model, comm_round + 1)
</code></pre>
<p>this code is for final step and for federated learning.</p>
","transformer-model"
"76796590","Deepspeed tensor parallel gets problem in tensor alignment when using tokenizer","2023-07-30 06:32:11","","1","316","<python><pytorch><transformer-model><huggingface><deepspeed>","<p>I tried to use deepspeed to conduct tensor parallel on starcoder as I had multiple small GPUs and each of which cannot singly hold the whole model.</p>
<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer
import os
import torch
import deepspeed

local_rank = int(os.getenv('LOCAL_RANK', '0'))
world_size = int(os.getenv('WORLD_SIZE', '1'))

cache_dir = '/llm-benchmark/starcoder-cache'

os.environ['TRANSFORMERS_CACHE'] = cache_dir

checkpoint = &quot;bigcode/starcoder&quot;
device = &quot;cuda&quot; # for GPU usage or &quot;cpu&quot; for CPU usage

tokenizer = AutoTokenizer.from_pretrained(checkpoint, cache_dir=cache_dir)

# Load model without moving it to device
model = AutoModelForCausalLM.from_pretrained(checkpoint, cache_dir=cache_dir)

ds_engine = deepspeed.init_inference(model, tensor_parallel={'enabled': True, 'tp_size': world_size})
model = ds_engine.module

print('before tokenizing')
inputs = tokenizer.encode(&quot;def print_hello_world():&quot;, return_tensors=&quot;pt&quot;).to(f&quot;{device}&quot;)
print('before generation')
outputs = model.generate(inputs)
print('after generation')
print(tokenizer.decode(outputs[0]))
print('full result')

</code></pre>
<p>When I ran the above code, it seemed that the model had been splitter successfully. However, I got the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;/root/code/starcoder/generate.py&quot;, line 29, in &lt;module&gt;
    outputs = model.generate(inputs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1437, in generate
    return self.greedy_search(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2248, in greedy_search
    outputs = self(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 808, in forward
    transformer_outputs = self.transformer(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 673, in forward
    outputs = block(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 316, in forward
    attn_outputs = self.attn(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 230, in forward
    query, key_value = self.c_attn(hidden_states).split((self.embed_dim, 2 * self.kv_dim), dim=2)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/_tensor.py&quot;, line 803, in split
    return torch._VF.split_with_sizes(self, split_size, dim)
RuntimeError: split_with_sizes expects split_sizes to sum exactly to 1600 (input tensor's size at dimension 2), but got split_sizes=[1536, 256]
Traceback (most recent call last):
  File &quot;/root/code/starcoder/generate.py&quot;, line 29, in &lt;module&gt;
    outputs = model.generate(inputs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/utils/_contextlib.py&quot;, line 115, in decorate_context
    return func(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 1437, in generate
    return self.greedy_search(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/generation/utils.py&quot;, line 2248, in greedy_search
    outputs = self(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 808, in forward
    transformer_outputs = self.transformer(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 673, in forward
    outputs = block(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 316, in forward
    attn_outputs = self.attn(
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py&quot;, line 230, in forward
    query, key_value = self.c_attn(hidden_states).split((self.embed_dim, 2 * self.kv_dim), dim=2)
  File &quot;/root/code/starcoder/lib/python3.10/site-packages/torch/_tensor.py&quot;, line 803, in split
    return torch._VF.split_with_sizes(self, split_size, dim)
RuntimeError: split_with_sizes expects split_sizes to sum exactly to 1600 (input tensor's size at dimension 2), but got split_sizes=[1536, 256]
</code></pre>
<p>It seems that the tokenizer is not aligned with the model. Why is this happening?</p>
","transformer-model"
"76780245","Fine Tuning mT5 Model for QA","2023-07-27 13:27:20","","0","640","<machine-learning><nlp><model><huggingface-transformers><transformer-model>","<p>I want to fine-tune a mT5-small model for QA task.</p>
<p>I loaded and preprocessed the data.</p>
<p>When I use Colab Notebook, I have to do the following:</p>
<pre><code>!pip install transformers
!pip install sentencepiece

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
tokenizer = AutoTokenizer.from_pretrained(&quot;google/mt5-small&quot;, use_fast=False)
</code></pre>
<p>Then I receive the following:</p>
<pre><code>You are using the legacy behaviour of the &lt;class 'transformers.models.t5.tokenization_t5.T5Tokenizer'&gt;. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
</code></pre>
<p>Which I hope is ok. If I don't install <code>sentencepiece</code> and do not set <code>use_fast=False</code>, I get another error that tells me to do that.</p>
<p>Now I don't know how to use the tokenizer.</p>
<p>I have the <code>train_questions</code> array, <code>train_answers</code> array, and <code>train_contexts</code> array.</p>
","transformer-model"
"76780153","Basic transformer calculating loss in python","2023-07-27 13:17:20","","0","296","<python><huggingface-transformers><transformer-model>","<h1>To calculate the loss, we need to pass in a label:</h1>
<pre><code>model_inputs = tokenizer(input_str, return_tensors=&quot;pt&quot;)

labels = ['NEGATIVE', 'POSITIVE']
model_inputs['labels'] = torch.tensor([1])

model_outputs = model(**model_inputs)
</code></pre>
<p>Can anyone explain what does the 3rd line of code does?</p>
","transformer-model"
"76768837","What Can Prevent Time-Series Prediction Model From Learning Trend?","2023-07-26 07:17:34","","0","43","<deep-learning><time-series><prediction><transformer-model><trend>","<p>I am building an encoder-decoder prediction model based on this paper:</p>
<p><a href=""https://www.sciencedirect.com/science/article/pii/S0952197623001483"" rel=""nofollow noreferrer"">https://www.sciencedirect.com/science/article/pii/S0952197623001483</a></p>
<p>It is made of a transformer encoder and a 1D CNN Decoder. The model takes in a input window of length L, and predicts a window of length L shifted by $delta$ time steps.</p>
<p>For now, I am using the model on 1D time series. When I train the model, it seems to be doing quite well as long as there is no trend in the data.</p>
<p>But when there is trend, the model only does well on the training data, and performance is less good on the validation data, and poor on the test data.</p>
<p>See below an example. The first red line shows the end of the training data, and the second the end of the validation data.</p>
<p><a href=""https://i.sstatic.net/CPPVQ.png"" rel=""nofollow noreferrer"">Example</a></p>
<p><strong>So my question is, what are the model parameters or elements of the model structure that might prevent the model from learning beyond data seen in training?</strong></p>
<p>Additional information:</p>
<ul>
<li><p>I fit a sklearn MinMaxScaler on the train data and apply that the the train, validation and test sets. I wonder if seing data outside the [0-1] range after training might be the issue? I have tried using StandardScaler as well but that did not help.</p>
</li>
<li><p>I have tried changind the lookback window, does not help.</p>
</li>
<li><p>I have added dropouts in several parts of the model, improved performance, but did not help in learning trend / learning on data outside the training range.</p>
</li>
</ul>
<p>Thanks you very much for your help!</p>
","transformer-model"
"76750641","Pytorch transformer: What does ""tgt"" parameter do if num_decoder_layers = 0?","2023-07-23 23:08:49","","0","241","<deep-learning><pytorch><transformer-model>","<p>If I make no decoder layers in my transformer like this:</p>
<pre><code>nn.Transformer(nhead=1, num_encoder_layers=1, d_model=1, dim_feedforward=1, num_decoder_layers = 0, dropout=0)
</code></pre>
<p>Why do I need to pass in &quot;the sequence to the decoder&quot; during the forward pass of the model? What does tgt do?</p>
","transformer-model"
"76748117","Transformer model for float sequences","2023-07-23 11:59:56","","1","187","<deep-learning><sequence><transformer-model>","<p>I am trying to apply the Transformer model to my dataset, in which, each item is a pair of float sequences. Traditional Transformer use the sentence to sentence and they require vocabulary sizes. However, my dataset are custom sequences with continuous float values, so that I face difficulty in defining the word vocabulary size for them. Moreover, I don't need tokenize my data, because they are floating sequences already, and my sequence length are also fit to 4.
May I ask how can I apply the Transformer in this cases
Example of my expected Transformer model is like this:</p>
<pre><code>[0.567, 0.787, 0.234, 0.223] =&gt; Transformer model =&gt; [0.673, 0.786, 0.777, 0.898]
</code></pre>
<p>Thank you so much</p>
","transformer-model"
"76734499","What does it do when we call self() inside a class in Python?","2023-07-21 00:57:12","","-2","42","<python><constructor><initialization><transformer-model><sentence-transformers>","<p>I recently came across this piece of code, when trying to implement a transformer using Keras.</p>
<pre><code>class Transformer(keras.Model):
    def __init__(
        self,
        num_hid=64,
        num_head=2,
        num_feed_forward=128,
        source_maxlen=100,
        target_maxlen=100,
        num_layers_enc=4,
        num_layers_dec=1,
        num_classes=60,
    ):
        super().__init__()
    
    ...

    def train_step(self, batch):

        ....

        with tf.GradientTape() as tape:
            preds = self([source, dec_input]) #problematic line
            one_hot = tf.one_hot(dec_target, depth=self.num_classes)
            mask = tf.math.logical_not(tf.math.equal(dec_target, pad_token_idx))
            loss = self.compiled_loss(one_hot, preds, sample_weight=mask)
</code></pre>
<p>Here, how are they using self() inside the class Transformer, in the method train_step.
Which class is it initializing?</p>
<p>Is it initializing class GradientTape? If so, how does it work?</p>
<p>Is it initializing Model class?
If so, Is this allowed in python to initialize parent class inside method of child class using self? How does it work?</p>
<p>I tried this.</p>
<pre><code>class A:
    def __init__(self, x=None, y=None):
        print(&quot;A:&quot;,x,y)

class B(A):
    def __init__(self):
        super().__init__()
    def call(self, x, y):
        c = self(x,y)

b = B()
b.call(1,2)
</code></pre>
<p>Output is:</p>
<pre><code>A: None None


Traceback (most recent call last):
  File &quot;./Playground/file0.py&quot;, line 12, in &lt;module&gt;
    b.call(1,2)
  File &quot;./Playground/file0.py&quot;, line 9, in call
    c = self(x,y)
TypeError: 'B' object is not callable
</code></pre>
<p>As expected it doesn't work. Cannot create new object using self() inside object.</p>
","transformer-model"
"76706128","Difference between Stationary and Non-Stationary time series","2023-07-17 15:44:10","","-2","505","<python><pytorch><time-series><transformer-model>","<p>What is the Difference between Stationary and Non-Stationary time series?</p>
<p>this link :<a href=""https://blog.quantinsti.com/stationarity/#:%7E:text=A%20stationary%20time%20series%20has,to%20the%20long%2Drun%20mean.&amp;text=A%20time%20series%20whose%20statistical%20properties%20change%20over%20time%20is,is%20non%2Dstationary%20in%20nature"" rel=""nofollow noreferrer"">https://blog.quantinsti.com/stationarity/#:~:text=A%20stationary%20time%20series%20has,to%20the%20long%2Drun%20mean.&amp;text=A%20time%20series%20whose%20statistical%20properties%20change%20over%20time%20is,is%20non%2Dstationary%20in%20nature</a>.
explain it
but in a simple way with example, what is it?
I can't understand it because ,I want to find out Why is stationarity one of the key components in time series analysis?
are we have any other type of time series?</p>
","transformer-model"
"76688630","AttributeError: module 'torch.nn.init' has no attribute 'trunc_normal_' While saving trasnformer google/vit-base-patch16-224-in21k model to local","2023-07-14 14:25:32","","1","278","<pytorch><amazon-sagemaker><huggingface-transformers><transformer-model><huggingface>","<p>I am trying to save <code>google/vit-base-patch16-224-in21k</code> locally and then upload it to s3 for hosting this model in the SageMaker environment.</p>
<pre><code>from transformers import AutoImageProcessor, ViTModel
# tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
# model = BertModel.from_pretrained(&quot;bert-base-uncased&quot;)

# processor = AutoImageProcessor.from_pretrained(&quot;google/vit-base-patch16-224-in21k&quot;)
model = ViTModel.from_pretrained(&quot;google/vit-base-patch16-224-in21k&quot;)

model_path = &quot;model/&quot;
code_path = &quot;code/&quot;

if not os.path.exists(model_path):
    os.mkdir(model_path)
    
model.save_pretrained(save_directory=model_path)
# processor.save_pretrained(save_directory=model_path)
</code></pre>
<p>Getting below error while saving</p>
<pre><code>AttributeError: module 'torch.nn.init' has no attribute 'trunc_normal_'
</code></pre>
<p>I tried modifying the transformer library version with no luck. Any help is highly appreciated. Thanks in advance.</p>
<p>Note: I am using <code>conda_amazonei_pytorch_latest_p37</code> kernel in sagemaker.</p>
<p>Reference: <a href=""https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-script-mode/pytorch_bert/deploy_bert_outputs.html"" rel=""nofollow noreferrer"">https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-script-mode/pytorch_bert/deploy_bert_outputs.html</a></p>
","transformer-model"
"76683812","Drop in performance from using nn.Linear(...) to nn.Parameter(torch.tensor(...))","2023-07-13 23:18:22","76763011","0","384","<python><pytorch><transformer-model>","<p>I am doing some experiments on transformer models using pytorch and need to make some modifications that require me to look at different weight matrices seperately. To this end I tried replacing some of the nn.Linear() blocks in the self attention computation with nn.Parameter(torch.tensor()), but I'm finding a substantial drop in performance. Here's the different implementations:</p>
<p>First (with nn.Linear()):</p>
<pre><code>class Attention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = dim_head *  heads
        project_out = not (heads == 1 and dim_head == dim)

        self.heads = heads
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim = -1)
        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)

        self.to_out = nn.Sequential(
            nn.Linear(inner_dim, dim),
            nn.Dropout(dropout)
        ) if project_out else nn.Identity()

    def forward(self, x):
        qkv = self.to_qkv(x).chunk(3, dim = -1)
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h = self.heads), qkv)
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale

        attn = self.attend(dots)


        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -&gt; b n (h d)')
        return self.to_out(out)
</code></pre>
<p>Second(with nn.Parameter(torch.tensor())):</p>
<pre><code>class Attention(nn.Module):
    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):
        super().__init__()
        inner_dim = dim_head *  heads
        project_out = not (heads == 1 and dim_head == dim)
        self.dim_head = dim_head
        self.heads = heads
        self.scale = dim_head ** -0.5

        self.attend = nn.Softmax(dim = -1)
        self.to_q = nn.Parameter(torch.randn(dim, inner_dim))
        self.to_k = nn.Parameter(torch.randn(dim, inner_dim))
        self.to_v = nn.Parameter(torch.randn(dim, inner_dim))
        self.projection = nn.Parameter(torch.randn(inner_dim, dim))
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        q,k,v = x @ self.to_q, x @ self.to_k, x @ self.to_v
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h = self.heads), (q,k,v))
        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale
        attn = self.attend(dots)  
        out = torch.matmul(attn, v)
        out = rearrange(out, 'b h n d -&gt; b n (h d)')
        out = out @ self.projection
        out = self.dropout(out)
        return out
</code></pre>
<p>Any explanation for why these two methods perform differently would help a lot. Thanks.</p>
","transformer-model"
"76669457","UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 15-16: unexpected end of data","2023-07-12 10:11:23","","0","103","<python><utf-8><transformer-model><machine-translation><devanagari>","<pre><code>SOURCE_VOCAB_SIZE = 37_000
TARGET_VOCAB_SIZE = 37_000
DATA_FNAME = 'D:/GPU6/spa-eng/sha5.txt'
import tensorflow as tf
from tensorflow.keras.layers import TextVectorization
def load_data(fname):
    # open the file with utf-8 encoding
    with open(fname, &quot;r&quot;, encoding=&quot;utf-8&quot;,errors= 'ignore') as textFile:
        # the source and the target sentence is demarcated with tab,
        # iterate over each line and split the sentences to get
        # the individual source and target sentence pairs
        lines = textFile.readlines()
        pairs = [line.rstrip('\n').split(&quot;,&quot;) for line in lines]
        # randomly shuffle the pairs
        random.shuffle(pairs)
        # collect the source sentences and target sentences into
        # respective lists
        source = [src for src, _ in pairs]
        target = [trgt for _, trgt in pairs]
    # return the list of source and target sentences
    return (source, target)

def splitting_dataset(source, target):
    # calculate the training and validation size
    trainSize = int(len(source) * 0.6)
    valSize = int(len(source) * 0.2)
    # split the inputs into train, val, and test
    (trainSource, trainTarget) = (source[:trainSize], target[:trainSize])
    (valSource, valTarget) = (
        source[trainSize : trainSize + valSize],
        target[trainSize : trainSize + valSize],
    )
    (testSource, testTarget) = (
        source[trainSize + valSize :],
        target[trainSize + valSize :],
    )
    # return the splits
    return (
        (trainSource, trainTarget),
        (valSource, valTarget),
        (testSource, testTarget),
    )
def tf_split_punct1(text):
    # split accented characters
   
    text = tf_text.normalize_utf8(text, &quot;NFKD&quot;)
    
    # Remove Sharda digits
    text = tf.strings.regex_replace(text, &quot;[^ 𑆃-𑆲.।॥,]&quot;, &quot;&quot;)
    text = tf.strings.regex_replace(text, '𑇆', '')
    text = tf.strings.regex_replace(text, '𑇅', '')
    text = tf.strings.regex_replace(text, '[𑇐𑇑𑇒𑇓𑇔𑇕𑇖𑇗𑇘𑇙𑇑𑇐]', '')
   # text = tf.strings.unicode_decode(text, input_encoding='UTF-8')
    # strip whitespace and add [START] and [END] tokens
    text = tf.strings.strip(text)
    text = tf.strings.join([&quot;[START]&quot;, text, &quot;[END]&quot;], separator=&quot; &quot;)
    
    # return the processed text
    return text

def tf_split_punct2(text):
    # split accented characters
   
    text = tf_text.normalize_utf8(text, &quot;NFKD&quot;)
   
     # Remove double danda punctuation
    text = tf.strings.regex_replace(text, &quot;[^ अ-ह.।॥,]&quot;, &quot;&quot;)
    text = tf.strings.regex_replace(text, '॥', '')
    text = tf.strings.regex_replace(text, '𑇅', '')
    # Remove digits
    text = tf.strings.regex_replace(text, '[०१२३४५६७८९]', '')
    #text = tf.strings.unicode_decode(text, input_encoding='UTF-8')
    # strip whitespace and add [START] and [END] tokens
    text = tf.strings.strip(text)
    text = tf.strings.join([&quot;[START]&quot;, text, &quot;[END]&quot;], separator=&quot; &quot;)
   
    # return the processed text
    return text

print(&quot;[INFO] loading data from {DATA_FNAME}...&quot;)
(source, target) = load_data(fname=DATA_FNAME)
print(&quot;[INFO] splitting the dataset into train, val, and test...&quot;)
(train, val, test) = splitting_dataset(source=source, target=target)
seq_length = 20
# create source text processing layer and adapt on the training
# source sentences
print(&quot;[INFO] adapting the source text processor on the source dataset...&quot;)
sourceTextProcessor = TextVectorization(
    standardize=tf_lower_and_split_punct1, max_tokens=SOURCE_VOCAB_SIZE, split=&quot;whitespace&quot;
)
sourceTextProcessor.adapt(train[0])
sourceTextProcessor.get_vocabulary()
# create target text processing layer and adapt on the training
# target sentences
print(&quot;[INFO] adapting the target text processor on the target dataset...&quot;)
targetTextProcessor = TextVectorization(
    standardize=tf_lower_and_split_punct2, max_tokens=TARGET_VOCAB_SIZE, split=&quot;whitespace&quot;
)
targetTextProcessor.adapt(train[1])
</code></pre>
<p>When this code is used in machine translation using transformer model then it gives the error :
<em>UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 15-16: unexpected end of data</em>.</p>
<p>Here <code>textfile</code> contains Sharada script as source and Devanagari script as target. Please give the solution. When we run function <code>tf_split_punct1</code> separately to remove punctuation from English text it gives same English text without punctuation but when we do the same to remove punctuation from Devanagari or Sharada text it gives this:</p>
<pre><code>tf.Tensor(b'\xf0\x91\x86\xa0\xf0\x91\x87\x80\xf0\x91\x86\xae\xf0\x91\x86\xb3\xf0\x91\x86\x81 \xf0\x91\x86\xa9\xf0\x91\x86\xa4\xf0\x91\x86\xb1\xf0\x91\x86\xb3\xf0\x91\x86\xa2\xf0\x91\x86\xa3\xf0\x91\x86\xb3\xf0\x91\x86\xa9\xf0\x91\x86\xb4 \xf0\x91\x86\xa8\xf0\x91\x86\x93\xf0\x91\x86\xae\xf0\x91\x86\xa2\xf0\x91\x87\x80\xf0\x91\x86\xae\xf0\x91\x86\xb5\xf0\x91\x86\xa0\xf0\x91\x86\xbc \xf0\x91\x86\xa8\xf0\x91\x86\xae\xf0\x91\x86\xa2\xf0\x91\x87\x80\xf0\x91\x86\xae\xf0\x91\x86\xbc\xf0\x91\x86\xb0\xf0\x91\x86\xb4\xf0\x91\x86\x9f\xf0\x91\x86\xb5\xf0\x91\x86\xa9\xf0\x91\x87\x80 \xf0\x91\x87\x86\xf0\x91\x87\x86 \xf0\x91\x86\xa4\xf0\x91\x86\xa9\xf0\x91\x86\xbe\xf0\x91\x86\xb1\xf0\x91\x87\x80\xf0\x91\x86\xa0\xf0\x91\x86\xb6 \xf0\x91\x86\xa0\xf0\x91\x86\xbc', shape=(), dtype=string) 
</code></pre>
","transformer-model"
"76667264","ValueError: Input 0 of layer ""model"" is incompatible with the layer: expected shape=(None, 41671, 43), found shape=(None, 43)","2023-07-12 05:00:46","","0","111","<python><tensorflow><keras><transformer-model>","<p>I am modifying the &quot;Timeseries classification with a Transformer model&quot; found <a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">here</a>, with a dataset that has 41671 rows and 43 columns. Here is the code that I am using:</p>
<pre><code># Model 5: Time-Series Transformer for Classification
import tensorflow as tf
from tensorflow import keras
from keras import layers
import numpy as np

tickers = ['AAPL', 'GOOG', 'MSFT', 'INTC', 'AMZN']

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;relu&quot;)(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res

def build_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
):
    inputs = keras.Input(shape=input_shape)
    print(inputs.shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)
        print(x.shape)

    x = layers.GlobalAveragePooling1D(data_format=&quot;channels_first&quot;)(x)
    print(x.shape)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;relu&quot;)(x)
        print(x.shape)
        x = layers.Dropout(mlp_dropout)(x)
        print(x.shape)
    outputs = layers.Dense(1, activation=&quot;softmax&quot;)(x)
    print(outputs.shape)
    return keras.Model(inputs, outputs)

# Model Training
for i in range(len(tickers)):
    train_tickers = tickers.copy()
    train_tickers.pop(i)
    print(train_tickers)
    df_train = pd.DataFrame()
    for train_ticker in train_tickers:
        df = pd.read_csv(f&quot;Spoofing-Injected DataFrames/{train_ticker}_segmentsummary_spoofed_bidside_{FACTOR}_{INTERVAL_START}_{INTERVAL_END}.csv&quot;, index_col=0)
        df_train = pd.concat([df_train, df], axis=0)
    X_train = df_train.drop(&quot;Classification&quot;, axis=1)
    y_train = df_train[&quot;Classification&quot;]
    df_valid = pd.read_csv(f&quot;Spoofing-Injected DataFrames/{tickers[i]}_segmentsummary_spoofed_bidside_{FACTOR}_{INTERVAL_START}_{INTERVAL_END}.csv&quot;, index_col=0)
    X_valid = df_valid.drop(&quot;Classification&quot;, axis=1)
    y_valid = df_valid[&quot;Classification&quot;]
    # batch_size = None
    model5 = build_model(
        input_shape=X_train.shape,
        head_size=256,
        num_heads=4,
        ff_dim=4,
        num_transformer_blocks=4,
        mlp_units=[128],
        mlp_dropout=0.4,
        dropout=0.25
    )

    model5.compile(
        loss='sparse_categorical_crossentropy',
        optimizer=keras.optimizers.Adam(learning_rate=1e-4),
        metrics=['sparse_categorical_accuracy']
    )

    model5.summary()

    callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]
    history = model5.fit(
        X_train,
        y_train,
        validation_split=0.2,
        epochs=200,
        batch_size=64,
        callbacks=callbacks,
        verbose=0
    )

    model.evaluate(X_valid, y_valid, verbose=1)
    history_df = pd.DataFrame(history.history)
    # Start the plot at epoch 5
    history_df.loc[5:, ['loss', 'val_loss']].plot()
    history_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()

    print((&quot;Best Validation Loss: {:0.4f}&quot; +\
        &quot;\nBest Validation Accuracy: {:0.4f}&quot;)\
        .format(history_df['val_loss'].min(), 
                history_df['val_binary_accuracy'].max()))
</code></pre>
<p>But I am receiving the error that is in the title. It is detecting an input shape of (None, 43) when it should be (None, 41671, 43). I suspect it is related to the Conv1D layers, but I cannot find where it specifies the input shape after those layers.</p>
<p>How can I ensure that the input shape stays consistent while letting the number of rows vary? (I am cross-validating with my data)</p>
<p>I tried to change the dataset into a TensorFlow dataset instead of a Pandas one, but it did not detect the shape of the input at all when I did that.</p>
<p>Here is my model summary</p>
<pre><code>Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 41671, 43)]  0           []                               
                                                                                                  
 layer_normalization (LayerNorm  (None, 41671, 43)   86          ['input_1[0][0]']                
 alization)                                                                                       
                                                                                                  
 multi_head_attention (MultiHea  (None, 41671, 43)   179243      ['layer_normalization[0][0]',    
 dAttention)                                                      'layer_normalization[0][0]']    
                                                                                                  
 dropout (Dropout)              (None, 41671, 43)    0           ['multi_head_attention[0][0]']   
                                                                                                  
 tf.__operators__.add (TFOpLamb  (None, 41671, 43)   0           ['dropout[0][0]',                
 da)                                                              'input_1[0][0]']                
                                                                                                  
 layer_normalization_1 (LayerNo  (None, 41671, 43)   86          ['tf.__operators__.add[0][0]']   
 rmalization)                                                                                     
                                                                                                  
 conv1d (Conv1D)                (None, 41671, 4)     176         ['layer_normalization_1[0][0]']  
                                                                                                  
 dropout_1 (Dropout)            (None, 41671, 4)     0           ['conv1d[0][0]']                 
                                                                                                  
 conv1d_1 (Conv1D)              (None, 41671, 43)    215         ['dropout_1[0][0]']              
                                                                                                  
 tf.__operators__.add_1 (TFOpLa  (None, 41671, 43)   0           ['conv1d_1[0][0]',               
 mbda)                                                            'tf.__operators__.add[0][0]']   
                                                                                                  
 layer_normalization_2 (LayerNo  (None, 41671, 43)   86          ['tf.__operators__.add_1[0][0]'] 
 rmalization)                                                                                     
                                                                                                  
 multi_head_attention_1 (MultiH  (None, 41671, 43)   179243      ['layer_normalization_2[0][0]',  
 eadAttention)                                                    'layer_normalization_2[0][0]']  
                                                                                                  
 dropout_2 (Dropout)            (None, 41671, 43)    0           ['multi_head_attention_1[0][0]'] 
                                                                                                  
 tf.__operators__.add_2 (TFOpLa  (None, 41671, 43)   0           ['dropout_2[0][0]',              
 mbda)                                                            'tf.__operators__.add_1[0][0]'] 
                                                                                                  
 layer_normalization_3 (LayerNo  (None, 41671, 43)   86          ['tf.__operators__.add_2[0][0]'] 
 rmalization)                                                                                     
                                                                                                  
 conv1d_2 (Conv1D)              (None, 41671, 4)     176         ['layer_normalization_3[0][0]']  
                                                                                                  
 dropout_3 (Dropout)            (None, 41671, 4)     0           ['conv1d_2[0][0]']               
                                                                                                  
 conv1d_3 (Conv1D)              (None, 41671, 43)    215         ['dropout_3[0][0]']              
                                                                                                  
 tf.__operators__.add_3 (TFOpLa  (None, 41671, 43)   0           ['conv1d_3[0][0]',               
 mbda)                                                            'tf.__operators__.add_2[0][0]'] 
                                                                                                  
 layer_normalization_4 (LayerNo  (None, 41671, 43)   86          ['tf.__operators__.add_3[0][0]'] 
 rmalization)                                                                                     
                                                                                                  
 multi_head_attention_2 (MultiH  (None, 41671, 43)   179243      ['layer_normalization_4[0][0]',  
 eadAttention)                                                    'layer_normalization_4[0][0]']  
                                                                                                  
 dropout_4 (Dropout)            (None, 41671, 43)    0           ['multi_head_attention_2[0][0]'] 
                                                                                                  
 tf.__operators__.add_4 (TFOpLa  (None, 41671, 43)   0           ['dropout_4[0][0]',              
 mbda)                                                            'tf.__operators__.add_3[0][0]'] 
                                                                                                  
 layer_normalization_5 (LayerNo  (None, 41671, 43)   86          ['tf.__operators__.add_4[0][0]'] 
 rmalization)                                                                                     
                                                                                                  
 conv1d_4 (Conv1D)              (None, 41671, 4)     176         ['layer_normalization_5[0][0]']  
                                                                                                  
 dropout_5 (Dropout)            (None, 41671, 4)     0           ['conv1d_4[0][0]']               
                                                                                                  
 conv1d_5 (Conv1D)              (None, 41671, 43)    215         ['dropout_5[0][0]']              
                                                                                                  
 tf.__operators__.add_5 (TFOpLa  (None, 41671, 43)   0           ['conv1d_5[0][0]',               
 mbda)                                                            'tf.__operators__.add_4[0][0]'] 
                                                                                                  
 layer_normalization_6 (LayerNo  (None, 41671, 43)   86          ['tf.__operators__.add_5[0][0]'] 
 rmalization)                                                                                     
                                                                                                  
 multi_head_attention_3 (MultiH  (None, 41671, 43)   179243      ['layer_normalization_6[0][0]',  
 eadAttention)                                                    'layer_normalization_6[0][0]']  
                                                                                                  
 dropout_6 (Dropout)            (None, 41671, 43)    0           ['multi_head_attention_3[0][0]'] 
                                                                                                  
 tf.__operators__.add_6 (TFOpLa  (None, 41671, 43)   0           ['dropout_6[0][0]',              
 mbda)                                                            'tf.__operators__.add_5[0][0]'] 
                                                                                                  
 layer_normalization_7 (LayerNo  (None, 41671, 43)   86          ['tf.__operators__.add_6[0][0]'] 
 rmalization)                                                                                     
                                                                                                  
 conv1d_6 (Conv1D)              (None, 41671, 4)     176         ['layer_normalization_7[0][0]']  
                                                                                                  
 dropout_7 (Dropout)            (None, 41671, 4)     0           ['conv1d_6[0][0]']               
                                                                                                  
 conv1d_7 (Conv1D)              (None, 41671, 43)    215         ['dropout_7[0][0]']              
                                                                                                  
 tf.__operators__.add_7 (TFOpLa  (None, 41671, 43)   0           ['conv1d_7[0][0]',               
 mbda)                                                            'tf.__operators__.add_6[0][0]'] 
                                                                                                  
 global_average_pooling1d (Glob  (None, 41671)       0           ['tf.__operators__.add_7[0][0]'] 
 alAveragePooling1D)                                                                              
                                                                                                  
 dense (Dense)                  (None, 128)          5334016     ['global_average_pooling1d[0][0]'
                                                                 ]                                
                                                                                                  
 dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              
                                                                                                  
==================================================================================================
Total params: 6,053,369
Trainable params: 6,053,369
Non-trainable params: 0
</code></pre>
<p>My training data shape is (41671, 43) and my label shape is (41671, 1).</p>
","transformer-model"
"76659423","Use of Params in pyspak","2023-07-11 07:03:19","76660505","0","48","<python><pyspark><transformer-model><simpletransformers>","<p>In this example, I am trying to use overrides as a Params object and I want it to be used as a list of strings.</p>
<p>But I am not able to assign its value using the below code.</p>
<pre><code>class _AB(Params):

    overrides = Param(Params._dummy(), &quot;overrides&quot;, &quot;Parameters for environment setup&quot;, typeConverter=TypeConverters.toListString)
    
    def __init__(self, *args):
        super().__init__(*args)
        self._setDefault(overrides=None)
        

class A(_AB):
  @keyword_only
  def __init__(self, overrides):
    super().__init__()
    kwargs = self._input_kwargs
    self.setParams(**kwargs)

  @keyword_only
  def setParams(self, overrides: List[str]):
      kwargs = self._input_kwargs
      print(kwargs)
      return self._set(**kwargs)

  def c(self):
    print(self.overrides.__dict__['typeConverter'].__dict__)
    for i in self.overrides:
       print(i)

a = A(overrides=[&quot;dsfs&quot;, &quot;Sdf&quot;])
a.c()
</code></pre>
<p>It gives me a blank dictionary when I print it inside function <code>c</code>.<br />
It gives me an error:</p>
<pre><code>TypeError: 'Param' object is not iterable
</code></pre>
<p>I guess it's happening because it's not able to assign some value to overrides variable.</p>
","transformer-model"
"76646630","Latent Representation of output sequence in Transformer","2023-07-09 08:57:28","","1","88","<nlp><transformer-model>","<p>I am using a caption-generating transformer. My goal is to train a regression model on the top of the transformer. For this reason I need to have access to the latent representation of the predicted text. I am trying to extract the latent(/ vector representation) of the text while training the transformer. I will store the vector representation of the text and eventually train the regression model. But I am not able to figure out how to extract the vector representation of the final sequence. Can you please help me with that?</p>
<p>I tried splitting the encoder decoedr layer and but couldn't figure out exactly what to fetch.</p>
","transformer-model"
"76645994","How do I get the final node embeddings in a GCN?","2023-07-09 05:24:37","","1","124","<deep-learning><pytorch><transformer-model><graph-neural-network>","<p>I'm building a GCN on the UPFD (Fake News Detection) graph dataset. My code performs graph classification. I need to obtain the final node embeddings for further use in my project.</p>
<p>This is the code so far:</p>
<pre><code>current_file = '.'

train_dataset = UPFD(current_file, 'politifact', 'spacy', 'train', ToUndirected())
val_dataset = UPFD(current_file, 'politifact', 'spacy', 'val', ToUndirected())
test_dataset = UPFD(current_file, 'politifact', 'spacy', 'test', ToUndirected())

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

# before_training = train_dataset[0].x
# print('Feature vector(node embedding) of datapoint #0 (before gtn):\n\t', train_dataset[0].x)


class GraphTransformer(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels,
                 concat=False):
        super().__init__()
        self.concat = concat

        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = TransformerConv(hidden_channels, hidden_channels)
        self.conv3 = TransformerConv(hidden_channels, hidden_channels)

        if self.concat:
            self.lin0 = Linear(in_channels, hidden_channels)
            self.lin1 = Linear(2 * hidden_channels, hidden_channels)

        self.lin2 = Linear(hidden_channels, out_channels)

    def forward(self, x, edge_index, batch):
        h = self.conv1(x, edge_index).relu()
        h = self.conv2(h, edge_index).relu()
        h = self.conv3(h, edge_index).relu()
        h = global_max_pool(h, batch)

        if self.concat:
            # Get the root node (tweet) features of each graph:
            root = (batch[1:] - batch[:-1]).nonzero(as_tuple=False).view(-1)
            root = torch.cat([root.new_zeros(1), root + 1], dim=0)
            news = x[root]

            news = self.lin0(news).relu()
            h = self.lin1(torch.cat([news, h], dim=-1)).relu()

        h = self.lin2(h)
        return h.log_softmax(dim=-1)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GraphTransformer(train_dataset.num_features, 128, train_dataset.num_classes, concat=True).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)

</code></pre>
<p>I tried printing the node embeddings after the model but it gave the same embeddings as before passing through the model. Is a duplicate copy made and modified by the library functions or is the original modified? What do I do? Is there any way to get the final node embeddings?</p>
","transformer-model"
"76634128","GPT2 model training gives, Loss nan during training","2023-07-07 05:19:19","","1","196","<deep-learning><nlp><huggingface-transformers><transformer-model>","<p>I am following <a href=""https://discuss.huggingface.co/t/fine-tuning-gpt2-for-question-answering/31895"" rel=""nofollow noreferrer"">this</a> tutorial of huggingface library. Since I have a large dataset so I have used Dataloader and created batches. But am facing this error. Here is the process of creating batches,</p>
<pre><code> class FeedbackEssentials(Dataset):
def __init__(self, qa_pairs, tokenizer, max_length):
    self.qa_pairs = qa_pairs
    self.tokenizer = tokenizer
    self.max_length = max_length

def __len__(self):
    return len(self.qa_pairs)

def __getitem__(self, idx):
    question = self.qa_pairs[idx][0]
    text = f&quot;{question} {self.tokenizer.eos_token}&quot;
    input_ids = self.tokenizer.encode(text, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True)
    attention_mask = [1] * len(input_ids)  # Assuming all tokens should be attended to

    return {
        'input_ids': torch.tensor(input_ids),
        'attention_mask': torch.tensor(attention_mask)
    }



def text_manipulation(train_dataset):
column1_values = train_dataset['Total Marks'].values
column2_values = train_dataset['Coding'].values
listOfLists = [[pair[0], pair[1]] for pair in zip(column1_values, column2_values)]

text = &quot;&quot;
for feedback in listOfLists:
    text += f&quot;{feedback[0]} {feedback[1]} {tokenizer.eos_token}&quot;
return text

training_dataset = text_manipulation(dataset)
max_length_training = max(len(tokenizer.encode(qa_pair[0],add_special_tokens=True)) for qa_pair in training_dataset)
dataset_training = FeedbackEssentials(training_dataset, tokenizer, max_length_training)
</code></pre>
<p>Here is data loader and batching</p>
<pre><code>batch_size = 4
dataloader = DataLoader(dataset_training, batch_size=batch_size, shuffle=True)

optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)
</code></pre>
<p>I have studied the other deeplearning answers as well. but everything seems on its right place</p>
","transformer-model"
"76606392","Encountering ImportError when trying to import 'BioGptModel' from 'transformers'","2023-07-03 15:55:45","","0","16298","<importerror><transformer-model><huggingface>","<p>I am working with the <code>Transformers</code> library in Python. My goal is to use the <code>BioGptModel</code> model. Here's the code I've written:</p>
<pre><code>from transformers import AutoTokenizer, BioGptModel
import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/biogpt&quot;)
model = BioGptModel.from_pretrained(&quot;microsoft/biogpt&quot;)

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
</code></pre>
<p>Unfortunately, when I run the code I get the following error:</p>
<blockquote>
<p>ImportError: cannot import name 'BioGptModel' from 'transformers'&quot;, tried all solution upgrade transformer and related libraies but still same error</p>
</blockquote>
<p>What am I doing wrong? Is 'BioGptModel' not part of the 'transformers' library, or is there another issue with my code or environment?</p>
","transformer-model"
"76600611","Vision transformer model not training as it should","2023-07-02 20:14:19","","1","360","<python><pytorch><transformer-model><torchvision>","<p>Here is the code for the vision transformer that I built using Pytorch. The model shows a cross entropy of 2.31 and an accuracy of around 10%. This remains the same across all the epochs. Hence, the model is unable to train. Please let me know what I am doing wrong, and if possible, please send a revised code. Thanks in advance!</p>
<p>PS: The model is trained on MNIST</p>
<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Sun Jul  2 14:04:19 2023

@author: Paras
&quot;&quot;&quot;

import torch
from torch import nn
from torchvision import transforms
import torchvision.datasets as datasets
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import math

class Head(nn.Module):
    def __init__(self,num_heads,weight_dimension):
        super(Head, self).__init__()
        self.w1 = nn.Parameter(torch.randn((weight_dimension*num_heads,weight_dimension))).to(device)
        self.w2 = nn.Parameter(torch.randn((weight_dimension*num_heads,weight_dimension))).to(device)
        self.w3 = nn.Parameter(torch.randn((weight_dimension*num_heads,weight_dimension))).to(device)
        
        
    def forward(self,x):
        
        x = x.to(device)
        self.Q = torch.matmul(x,self.w1).to(device)
        self.K = torch.matmul(x,self.w2).to(device)
        self.V = torch.matmul(x,self.w3).to(device)
        
        lnq = nn.LayerNorm(self.Q.size()[1:]).to(device)
        lnk = nn.LayerNorm(self.K.size()[1:]).to(device)
        lnv = nn.LayerNorm(self.V.size()[1:]).to(device)

        self.Q = lnq(self.Q)
        self.K = lnk(self.K)
        self.V = lnv(self.V)
        self.K = torch.transpose(self.K, -2, -1)

        out = torch.matmul(self.Q,self.K)
        out = out/np.sqrt(self.Q.shape[1])
        out = F.softmax(out,dim=-1)
        out = torch.matmul(out,self.V)
        return out
        
    
class MHA(nn.Module):
    def __init__(self,num_heads,weight_dimension):
        super(MHA, self).__init__()
        self.num_heads = num_heads
        self.weight_dimension = weight_dimension
        heads = []
        for i in range(self.num_heads):
            head = Head(self.num_heads,self.weight_dimension)
            heads.append(head)
            
        self.heads = heads
        
    def forward(self,x):
        
        flag=True
        for i in range(self.num_heads):
            if flag:
                out_multihead = self.heads[i](x)
                flag=False
            else:
                out_multihead = torch.cat((out_multihead,self.heads[i](x)),axis=2)
        
        return out_multihead
            
    
class vit_model(nn.Module):

    def __init__(self,img_size,patch_size,embedding_dim,n_heads,hidden_dims_mlp,n_classes,batch_size):
        
        super().__init__()
        self.patch_size = patch_size
        self.n_heads = n_heads
        self.hidden_dims_mlp = hidden_dims_mlp
        self.img_size = img_size
        self.n_classes = n_classes
        self.device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
        self.embedding_dim = embedding_dim
        self.batch_size = batch_size
        
        embedding_rows = self.patch_size*self.patch_size
        embedding_cols = self.embedding_dim
        embedding_cols = int(embedding_cols)
        
        self.embedding_matrix = nn.Parameter(torch.randn((embedding_rows,embedding_cols)))
        self.embedding_matrix.requires_grad_(True)
        
        self.added_class_head = nn.Parameter(torch.randn((1,embedding_cols))) #Normally distributed like nn.init.normal_ (std 10-6)
        self.added_class_head.requires_grad_(True)
        
        self.positional_embeddings = nn.Parameter(self.get_positional_encodings((img_size//patch_size)**2,embedding_cols)) #Trunc Normal distribution
        self.positional_embeddings.requires_grad_(True)
        
        self.weight_dimension = embedding_cols//self.n_heads
        
        self.mha = MHA(self.n_heads,self.weight_dimension)
        
        self.mlp_inside_encoder = nn.Sequential(
            nn.Linear(self.embedding_dim*(self.positional_embeddings.shape[0]+1), self.hidden_dims_mlp),
            nn.GELU(),
            nn.Dropout(0.5),
            nn.Linear(self.hidden_dims_mlp, self.embedding_dim*(self.positional_embeddings.shape[0]+1)),
            nn.GELU(),
            nn.Dropout(0.5)
            )
        
        self.mlp_classification = nn.Sequential(
            nn.Linear(self.embedding_dim, self.n_classes),
            nn.GELU(),
            nn.Dropout(0.5),
            nn.Linear(self.n_classes, self.n_classes),
            nn.GELU(),
            nn.Dropout(0.5)
            )
        
    def divide_image_into_patches(self,imgs,patch_size):
        
        imgs = imgs/255
        
        startx, starty = 0,0
        batch_size, channels, height, width = imgs.shape
        
        flag = True
        for startx in range(0,height,patch_size):
            for starty in range(0,width,patch_size):
                tmat = imgs[:,:,startx:startx+patch_size,starty:starty+patch_size]
                
                tmat = tmat.reshape((batch_size,1,tmat.shape[1]*tmat.shape[2]*tmat.shape[3]))
                if flag:
                    patches_list = tmat
                    flag=False
                else:
                    patches_list = torch.cat((patches_list,tmat),1)

        return patches_list
    

    def get_positional_encodings(self,seq_length, hidden_size):
        position = torch.arange(seq_length).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, hidden_size, 2) * (-math.log(10000.0) / hidden_size))
        encodings = torch.zeros(seq_length, hidden_size)
        encodings[:, 0::2] = torch.sin(position * div_term)
        encodings[:, 1::2] = torch.cos(position * div_term)
        return encodings
    
    
    def forward(self,images):
        
    
        out = self.divide_image_into_patches(images,self.patch_size)
        out = torch.matmul(out,self.embedding_matrix)
        out = out + self.positional_embeddings.unsqueeze(0).expand(self.batch_size, -1, -1)
        out = torch.cat((out,self.added_class_head.expand(self.batch_size, 1, -1)),1)
        out = out.to(self.device)
        ln = nn.LayerNorm(out.size()[1:]).to(self.device)
        out = ln(out)
        layer_norm1 = out.clone()
        out = self.mha(out)
        out = out + layer_norm1
        skip = out.clone()
        out = out.to(self.device)
        ln = nn.LayerNorm(out.size()[1:]).to(self.device)
        out = ln(out)
        out = self.mlp_inside_encoder(out.reshape(out.shape[0],out.shape[1]*out.shape[2]))
        out = skip + out.reshape(self.batch_size,layer_norm1.shape[1],self.embedding_dim)
        out = out[:,-1,:]
        out = self.mlp_classification(out)
        return out

# Define the transform to normalize the data
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])

# Load the training and test datasets
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)

# Create data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, batch_size=1000)
test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=1000)

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

torch.autograd.set_detect_anomaly(True)
model = vit_model(28, 4, 512, 8, 2048, 10, 1000)
model = model.to(device)
print(model)
#num_params = sum(p.numel() for p in model.named_parameters())
for p in model.named_parameters():
    print(p)
#print('Number of parameters:',num_params)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)


epoch_losses = []
epoch_accuracies = []
for epoch in range(100):  # Number of training epochs

    epoch_loss = []
    epoch_acc = []
    model.train()
    for i, (images,labels) in enumerate(train_loader):
        images = images.to(device)
        labels = labels.to(device)
        c = model(images)
        loss = criterion(c,labels)

        with torch.no_grad():
            predictions = torch.argmax(c, dim=-1)

        acc = torch.sum(predictions == labels)/1000
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_loss.append(loss.item())
        epoch_acc.append(acc.cpu().numpy())

    model.eval()
    epoch_losses.append(np.average(epoch_loss))
    epoch_accuracies.append(np.average(epoch_acc))
    print('Epoch loss:',epoch_losses[-1])
    print('Epoch accuracy:',epoch_accuracies[-1])
    

    
</code></pre>
<p>I tried varying learning rates, patch sizes, etc. hyperparameters, but it did not work.</p>
","transformer-model"
"76568933","Using TF model.predict() after model training throws unknown shape error","2023-06-27 22:06:16","","1","779","<python><tensorflow><keras><normalization><transformer-model>","<p>Background: My data has shape of (batch_size, ghost_dim, data_length) and I am learning to train a MultiHeadAttention based model. I am inputting a single batch of same dimension data with model(), model.predict() or model.predict_on_batch() functions to return predictions array after fitting the model and I am getting similar shape related errors on each.</p>
<p>Throws:</p>
<pre><code>2 frames
/usr/local/lib/python3.10/dist-packages/keras/engine/training.py in tf__predict_function(iterator)
     13                 try:
     14                     do_return = True
---&gt; 15                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16                 except:
     17                     do_return = False

ValueError: in user code:

    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 2169, in predict_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 2155, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 2143, in run_step  **
        outputs = model.predict_step(data)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 2111, in predict_step
        return self(x, training=False)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py&quot;, line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None

    **ValueError: Exception encountered when calling layer 'layer_normalization_1' (type LayerNormalization).
    
    Cannot take the length of shape with unknown rank.
    
    Call arguments received by layer 'layer_normalization_1' (type LayerNormalization):
      • inputs=tf.Tensor(shape=&lt;unknown&gt;, dtype=float16)**
</code></pre>
<p>If I used model() function call directly, there is a similar error regarding input shape during MultiHeadAttention layer with input appearing to be 1 dimension instead of the original single batch data that I am inputting, which is:</p>
<pre><code>x_predict = np.random.normal(size=(1, 1, 512)) 

predictions = self.model(x_predict)
predictions = self.model.predict_on_batch(x_predict)
</code></pre>
<p>Not sure what is the origin of all these numpy array shape errors.. Any help/ideas appreciated?</p>
","transformer-model"
"76539975","how to visualize cross-attention maps for checking text-image alignment well?","2023-06-23 12:08:33","","0","652","<pytorch><visualization><transformer-model><attention-model><self-attention>","<p>I was wondering how to visualize cross-attention map of image features a model is looking at given a text query (e.g. sentence).
There are some amazing explainable tools ilke Class Activationi Maps, but they are almost needed 'class' or CNN model (of course, there is vit-attention map too, but for classification problem). <a href=""https://github.com/jacobgil/pytorch-grad-cam"" rel=""nofollow noreferrer"">pytorch-grad-cam</a>, <a href=""https://www.kaggle.com/code/piantic/vision-transformer-vit-visualize-attention-map/notebook"" rel=""nofollow noreferrer"">vit-attention map with classes</a>
But I can't count how many classes words have. This is because each sentence is made up of different words.
How can I visualize a cross-attention encoder output?</p>
<p>PLZ help me.</p>
<p>Thank you. :)</p>
","transformer-model"
"76520092","TF.MultiHeadAttention with 1D Data and Ghost Dimension","2023-06-21 05:04:04","76540668","1","219","<python><tensorflow><machine-learning><keras><transformer-model>","<p>Background:
My data has shape of (batch_size, data_length) and the dimensions seem to be incompatible with the inside MultiHeadAttention operations, especially softmax. Someone kindly suggested that I should use a size 1 ghost dimension as the last dimension.</p>
<p>Error message I got:</p>
<pre><code>(32, 512, 1)
Epoch 1/200

---------------------------------------------------------------------------

ValueError                                Traceback (most recent call last)

&lt;ipython-input-7-870abeaa4b93&gt; in &lt;cell line: 281&gt;()
    279 model.compile(loss=&quot;mean_squared_error&quot;, optimizer=&quot;rmsprop&quot;, metrics=[&quot;accuracy&quot;])
    280 
--&gt; 281 history = model.fit(dataset, epochs=200, validation_data=val_dataset)

1 frames

/usr/local/lib/python3.10/dist-packages/keras/engine/training.py in tf__train_function(iterator)
     13                 try:
     14                     do_return = True
---&gt; 15                     retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
     16                 except:
     17                     do_return = False

ValueError: in user code:

    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 1284, in train_function  *
        return step_function(self, iterator)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 1268, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 1249, in run_step  **
        outputs = model.train_step(data)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/engine/training.py&quot;, line 1050, in train_step
        y_pred = self(x, training=True)
    File &quot;/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py&quot;, line 70, in error_handler
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer 'query' (type EinsumDense).
    
    Dimensions must be equal, but are 1 and 512 for '{{node model/multi_head_attention/query/einsum/Einsum}} = Einsum[N=2, T=DT_HALF, equation=&quot;abc,cde-&gt;abde&quot;](model/Cast, model/multi_head_attention/query/einsum/Einsum/Cast)' with input shapes: [32,512,1], [512,2,512].
    
    Call arguments received by layer 'query' (type EinsumDense):
      • inputs=tf.Tensor(shape=(32, 512, 1), dtype=float16)
</code></pre>
","transformer-model"
"76511182","Tensorflow custom learning rate scheduler gives unexpected EagerTensor type error","2023-06-20 03:08:43","76527098","2","516","<python><tensorflow><machine-learning><deep-learning><transformer-model>","<p>Below is my custom LR Scheduler that subclasses tensorflow.keras.optimizers.schedules.LearningRateSchedule, got error <code>TypeError: Cannot convert -0.5 to EagerTensor of dtype int64</code>. Really baffled as to why Eagertensor is relevant to a simple inverse square calculation for the return call of this custom class..</p>
<pre><code>class lr_schedule(tensorflow.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, dim_embed, warmup_steps):
        self.dim_embed = dim_embed
        self.warmup_steps = warmup_steps
    def __call__(self, step):
        return (self.dim_embed ** -0.5) * min((step ** -0.5), step * (self.warmup_steps ** -1.5))
</code></pre>
<p>Not specifically relevant to this error, but this is a custom LR Scheduler that replicates warmup scheduler that is used at 'Attention is All You Need' paper..</p>
","transformer-model"
"76510809","Sequence to Sequence transformer model for harmonizing a melody: data representation, embedding, and interpretation","2023-06-20 01:07:19","","2","136","<pytorch><midi><word-embedding><transformer-model>","<p>I am developing a seq2seq model using a transformer model and PyTorch. The network should take in a sequence representing a melody and output a sequence of the same length indicating an appropriate harmony at the corresponding time-step. The model is going to be trained on a data set of J.S. Bach chorales, where each time-step has a melody (soprano) and three harmony (alto, tenor, bass) notes. I have been toying with a couple of ways to represent the data:</p>
<ol>
<li>MIDI integer encoding - if the length of a chorale is 640 time-steps, then the entire chorale would be a (640, 4) tensor, and each of the four elements in a row contains the MIDI integer of the note (e.g. middle C = 60).</li>
<li>Many-hot MIDI encoding - the chorale would be a tensor of size (640, R), where R is the range of notes under consideration for input and generation. Each row in the tensor would be a many hot vector where the index corresponding to the MIDI integer of the active notes of each voice would be 1 and the rest 0.</li>
</ol>
<p>I have been able to scan through the dataset and see how many unique melody notes and how many unique harmony combinations there are. I have been assuming that that is the size of my respective vocabularies. I think I have a fundamental misunderstanding of how PyTorch's <code>nn.Embedding</code> works. So here are my questions:</p>
<ol>
<li><p>Inside the model definition, my <code>Embedding</code> layers for the encoder and decoder are set with vocabulary lengths of <code>len(unique_melody_notes) + 3</code> and <code>len(unique_chords) + 3</code> respectively. I add 3 to these in order to include a start-of-sequence, end-of-sequence, and padding token in each vocab. But when training begins when using the many-hot data representation, I get <code>IndexError: index out of range in self</code> exception in the embedding layer. I was under the impression that the <code>nn.Embedding</code> layer is simply a mapping between sparse vectors and dense embeddings determined by the layer. How am I misunderstanding the <code>nn.Embedding</code> layer that these many-hot vectors are not able to be fed to them.</p>
</li>
<li><p>Since the MIDI integer encoding is already a dense representation, do I even need embedding?</p>
</li>
</ol>
<p>I can provide code or data examples if need be. Please help me to understand, and please correct me if I am misunderstanding something.</p>
<p>EDIT: I figured out what my problem was; since the range of the MIDI integers for the melody was {58, 61, ..., 83}, I set the embedding vocab size based on the length of that sequence directly but did not account for shifting the vocab down, so <code>num_embeddings</code> was 27 (length of vocab + padding token) but I was directly passing indices in the range [58-83].</p>
<p>However, now I might be facing a different issue, and this stems from my lack of understanding about what the transformer model is supposed to be doing. I will try to include only necessary code here. I have constructed vocab dictionaries for both the input and target sequences, which have different sizes as there are more unique harmonic combinations than unique melody notes. Here is my data loader:</p>
<pre><code>#functions to retrieve pytorch tensor of vocab indices
def get_harm_index_from_vocab(data):
    t = torch.tensor([harm_dict_seq_2_idx[tuple(seq)] for seq in data])
    return t

def get_mel_index_from_vocab(data):
    t = torch.tensor([mel_dict_note_2_idx[n] for n in data])
    return t

class MusicDataSetMidi(Dataset):
    def __init__(self, data_array_file, sos, eos, transform=None):
        self.data_arr = np.load(data_array_file) #read data into array from file
        self.sos = sos #Not used currently
        self.eos = eos #not used currently
        self.transform = transform
    
    def __len__(self):
        return np.shape(self.data_arr)[0] #first dimension of data tensor represents number of data    
    
    def __getitem__(self, idx):
        chorale = self.data_arr[idx,:,:] #select chorale 'idx' with shape (640, 4)    
        if self.transform:
            chorale = self.transform(chorale)
            
        #Extract first column representing time series of melody in MIDI integer format
        mel = get_mel_index_from_vocab(chorale[:, 0])
        #Extract 2nd through 4th columns representing time series of harmony in MIDI integer format 
        harm = get_harm_index_from_vocab(chorale[:, 1:])
        
        return mel, harm
</code></pre>
<p>Now the input sequence and target sequence look like
[17, 17, 17, 17, 18, 18, ..., 13, 13, 0, 0, ..., 0] and
[3345, 3345, 3345, 3345, 3359, 3359, ..., 1763, 1763, 4960, 4960, ..., 4960] respectively. 0 is a token representing silence (padding) in the input sequence, and 4960 is the token representing the same in the harmony sequence. These are just the keys used when I constructed the vocab dicts since they are of different sizes.</p>
<p>The following two blocks instantiate the positional encoding module and the entire seq2seq transformer model:</p>
<pre><code>class PositionalEncoding(nn.Module):
    def __init__(self, dim_model, dropout_p, max_length):
        super().__init__()
        &quot;&quot;&quot;
        dim_model : dimension of the embedded representations used as inputs to the multi-head attention blocks
        dropout_p : probability of dropout in all dropout layers
        max_len   : how far the position can have an effect on a token (like a window of influence)
        &quot;&quot;&quot;
        self.dropout = nn.Dropout(dropout_p)
        #Encoding from formula
        pos_encoding = torch.zeros(max_length, 1, dim_model)
        positions_list = torch.arange(max_length).unsqueeze(1)
        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)

        #pos_encoding(pos, 2i) = sin(pos/1000^(2i/dim_model))
        pos_encoding[:, 0, 0::2] = torch.sin(positions_list * division_term)
        #pos_encoding(pos, 2i+1) = cos(pos/1000^(2i/dim_model))
        pos_encoding[:, 0, 1::2] = torch.cos(positions_list * division_term)

        #save as buffer (the same as a parameter but without gradients)
        self.register_buffer('pos_encoding', pos_encoding)
            
    def forward(self, token_embedding: torch.tensor) -&gt; torch.tensor:
        #residual connection + positional encoding
        token_plus_pos_embedding = token_embedding + self.pos_encoding[:token_embedding.size(0), :]
        #send through dropout layer and return
        return self.dropout(token_plus_pos_embedding)
</code></pre>
<p>and</p>
<pre><code>class Transformer(nn.Module):
    #Constructor
    def __init__(
        self,
        dim_model,
        num_heads,
        num_encoder_layers,
        num_decoder_layers,
        melody_vocab_size,
        harmony_vocab_size,
        dropout_p,
        pos_enc_max_len
    ):
        super().__init__()
        self.dim_model = dim_model
        # LAYERS
        #Give transformer model an object of our positional encoding class
        self.positional_encoder = PositionalEncoding(
            dim_model=dim_model, dropout_p=dropout_p, max_length=pos_enc_max_len
        )

        #Create embedding layer to turn sequence vectors into dense, continuous vector space
        self.source_embedding = nn.Embedding(melody_vocab_size, dim_model)
        self.target_embedding = nn.Embedding(harmony_vocab_size, dim_model)
        
        #Create transformer layer
        self.transformer = nn.Transformer(
            d_model=dim_model, 
            nhead=num_heads, 
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dropout=dropout_p
        )
        self.out = nn.Linear(dim_model, harmony_vocab_size)
        
    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):
        #src size must be (batch_size, src sequence length)
        #tgt size must be (batch_size, tgt sequence length)
        
        #Embedding + positional encoding: output size = (batch size, sequence length, dim_model)
        src = self.source_embedding(src) * math.sqrt(self.dim_model)
        tgt = self.target_embedding(tgt) * math.sqrt(self.dim_model)
        src = self.positional_encoder(src)
        tgt = self.positional_encoder(tgt)
        
        #Permute embedded/encoded sequences to obtain shape (seqence length, batch size, dim_model)
        src = src.permute(1, 0, 2)
        tgt = tgt.permute(1, 0, 2)
        
        #Pass embedded/encoded sequences to transformer
        transformer_out = self.transformer(
            src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask
        )
        out = self.out(transformer_out)
        
        return out
    
    def get_tgt_mask(self, size) -&gt; torch.tensor:
        #Generates a square matrix where each row allows one more 'word' of sequence to be seen
        mask = torch.tril(torch.ones(size, size) == 1)
        mask = mask.float()
        mask = mask.masked_fill(mask==0, float('-inf'))
        mask = mask.masked_fill(mask==1, float(0.0))
        
        # EX for size=5:
        # [[0., -inf, -inf, -inf, -inf],
        #  [0.,   0., -inf, -inf, -inf],
        #  [0.,   0.,   0., -inf, -inf],
        #  [0.,   0.,   0.,   0., -inf],
        #  [0.,   0.,   0.,   0.,   0.]]
        
        return mask
    
    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -&gt; torch.tensor:
        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is
        # [False, False, False, True, True, True]
        return (matrix == pad_token)
</code></pre>
<p>The following two blocks instantiate the model and runs a sample training loop with a batch size of 1:</p>
<pre><code>#Set compute device dependent on whether GPU is available
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(device)

#--------Arguments for transformer model-----------
 
NUM_MELODY_TOK = len(mel_dict_note_2_idx)
NUM_HARMONY_TOK = len(harm_dict_seq_2_idx)
MODEL_DIM = 512  #size of latent embedded vectors passed to the model. Hyperparameter open to tuning
NUM_HEADS = 8 #number of attention heads in multi headed attention modules
NUM_ENC_LAYERS = 4
NUM_DEC_LAYERS = 4
DROPOUT_PROB = 0.1
MAX_POS_ENC_LENGTH = 640
model = Transformer(dim_model=MODEL_DIM,
                   num_heads=NUM_HEADS,
                   num_encoder_layers=NUM_ENC_LAYERS,
                   num_decoder_layers=NUM_DEC_LAYERS,
                   melody_vocab_size=NUM_MELODY_TOK,
                   harmony_vocab_size=NUM_HARMONY_TOK,
                   dropout_p=DROPOUT_PROB,
                   pos_enc_max_len=MAX_POS_ENC_LENGTH).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.1, betas=(0.9, 0.98))
loss_fcn = nn.CrossEntropyLoss()
</code></pre>
<p>and</p>
<pre><code>def training_loop(model, opt, loss_fn, dataloader):
    
    model.train()
    total_loss = 0
    pred_max_value = []
    pred_argmax_value = []
    
    for input_melody, target_harmony in dataloader:

        input_melody = torch.tensor(input_melody).to(device)
        target_harmony = torch.tensor(target_harmony).to(device)

        #Shift target sequence so model is lined up correctly to predict next target index in sequence?
        target_input = target_harmony[:, :-1]
        target_expected = target_harmony[:, 1:]

        sequence_length = target_input.size(1)
        tgt_mask = model.get_tgt_mask(sequence_length).to(device)
        
        pred = model(input_melody, target_input, tgt_mask)
        #Put back to batch first order for loss function
        pred = pred.permute(1, 2, 0)
        
        #Keep track of which harmony vocab index the model predicts with highest likelihood
        pred_argmax_value.append(torch.argmax(pred[0,:,0]).item())
        #Keep track of value of highest prob. harmony index to see whether model is uncertain about outputs
        pred_max_value.append(torch.max(pred[0,:,0]).item())
        
        loss = loss_fn(pred, target_expected)
        opt.zero_grad()
        loss.backward()
        opt.step()
        total_loss += loss.detach().item()
    return pred, pred_max_value, pred_argmax_value
        
num_epochs = 40
for n in range(num_epochs):   
    pred, mv, amv = training_loop(model, optimizer, loss_fcn, train_dataloader)
</code></pre>
<p>The following plot shows the model the index of the predicted harmonic sequence for the final epoch.
<a href=""https://i.sstatic.net/pYLri.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pYLri.png"" alt=""enter image description here"" /></a></p>
<p>Notice how it keeps predicting the silence/padding token of 4690. Also, I have only one prediction per entire sequence presented to the network (the x-axis of the plot ends at 217 which is the number of pieces in the training data set). Here are my new questions:</p>
<ol>
<li>Is the seq2seq model only predicting the harmonic output for the first time step of each data item? If this is the case, do I need to adjust my training loop to predict the entire accompaniment sequence? I am unsure what the model is predicting exactly.</li>
<li>Come time for inference on yet unseen sequences, do I need to feed the same melody in a loop to generate the entire accompaniment sequence?</li>
<li>The model keeps predicting the silence/padding token. Do I need to use a separate token indicating the end-of-sequence?</li>
<li>Do I need a start of sequence token for the input and/or target sequence? In the training loop I offset the targets, but I'm not really sure what's going on there, I copied it from a tutorial.</li>
</ol>
<p>I would appreciate any help or insight into this problem.</p>
","transformer-model"
"76506435","Facing a GPU memory leak in the training of multi-head attention to generate sentences in an autoregressive way","2023-06-19 12:02:11","","0","171","<deep-learning><nlp><time-series><transformer-model>","<p>I am working on time series forecasting using GPT-like model. The idea is similar to train a language model that, given the beginning part of a sentence, the model can generate the rest of that sentence.</p>
<p>“I’m hungry, I want a hamburger and a cup of cola.”</p>
<p>Input: I’m hungry</p>
<p>Predict: I want a hamburger and a cup of cola.</p>
<p>An autoregressive language model will generate words step by step.</p>
<p>I’m hungry, I
I’m hungry, I want
I’m hungry, I want a
……
I’m hungry, I want a hamburger and a cup of cola.</p>
<p>That is, the newly generated word will be appended to the end of the previous input sequence to construct the new input sequence. During training, I will calculate the loss on the generated content “I want a hamburger and a cup of cola” and use back-propagation to update model parameters. The generation process can be implemented through a for-loop and a “decoder-only” module.</p>
<p>However, the GPU memory usage always spikes in this for-loop.</p>
<p>Do you have any suggestions to optimize the implementation?</p>
<p>Do you think my implementation is the right way to generate word sequences?</p>
<p>My time series forecasting sequence contains around 100 elements, that is, the for-loop repeat operation 100 times.</p>
","transformer-model"
"76482191","CNN for a matrix of numbers","2023-06-15 11:52:55","","-1","310","<machine-learning><matrix><conv-neural-network><artificial-intelligence><transformer-model>","<p>I've been trying to learn more about how CNNs work as I'm trying to implement one inside of a Transformer, but all the examples I've seen use CNNs for images. Since images are technically numbers, would it be possibly to input a matrix of numbers to a CNN? And are there any examples/links that do this?</p>
<p>The part that's confusing me the most is the filters, as those are supposed to look for certain features in the images, but could those be manipulated to look for patterns in the numbers instead?</p>
","transformer-model"
"76459581","OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. on a transformer AI","2023-06-12 19:00:18","","1","79","<python><tensorflow><artificial-intelligence><chatbot><transformer-model>","<p>I want to make a Transformer AI Chatbot so I followed a tutorial on kerases website on how to make a porteguese to english transalator and modified it into a chatbot, and created example sequences. However, I am encountering an error.</p>
<pre><code>import logging
import time

import numpy as np
import matplotlib.pyplot as plt

import tensorflow_datasets as tfds
import tensorflow as tf

from keras_preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences

import tensorflow_text

# Dowloading the dataset and putting it into a tf.data.dataset

tokenizer = Tokenizer()

input_sequences = [&quot;Hello, I'm Dory&quot;]
output_sequences = [&quot;Hey&quot;]

tokenizer.fit_on_texts(output_sequences + input_sequences)

input_sequences = tokenizer.texts_to_sequences(input_sequences)
output_sequences = tokenizer.texts_to_sequences(output_sequences)

input_vocab_size = len(tokenizer.word_index) + 1
output_vocab_size = len(tokenizer.word_index) + 1

MAX_TOKENS = output_vocab_size

input_sequences = pad_sequences(input_sequences, MAX_TOKENS)
output_sequences = pad_sequences(output_sequences, MAX_TOKENS)

input_sequences = np.asarray(input_sequences)
output_sequences = np.asarray(output_sequences)

output_sequences = tf.convert_to_tensor(output_sequences)
input_sequences = tf.convert_to_tensor(input_sequences)

print(f&quot;Word index: {tokenizer.word_index}&quot;)
print(f&quot;Vocab size: {input_vocab_size}&quot;)
print(input_sequences)
print(output_sequences)

def Positional_encoding(length, depth):
    depth = depth/2

    positions = np.arange(length)[:, np.newaxis]
    depths = np.arange(depth)

    angle_rates = 1 / (10000**depths)
    angle_rads = positions * angle_rates

    pos_encoding = np.concatenate(
        [np.sin(angle_rads), np.cos(angle_rads)],
        axis=-1
    )

    return tf.cast(pos_encoding, dtype=tf.float32)

class PositionalEmbedding(tf.keras.layers.Layer):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.d_model = d_model
        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)
        self.pos_encoding = Positional_encoding(length=2048, depth=d_model)

    def compute_mask(self, *args, **kwargs):
        return self.embedding.compute_mask(*args, **kwargs)

    def call(self, x):
        length = tf.shape(x)[1]
        x = self.embedding(x)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x = x + self.pos_encoding[tf.newaxis, :length, :]
        return x

embed_in = PositionalEmbedding(vocab_size=input_vocab_size, d_model=512)
embed_out = PositionalEmbedding(vocab_size=output_vocab_size, d_model=512)

out_emb = embed_out(output_sequences)
in_emb = embed_in(input_sequences)

class BaseAttention(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super().__init__()
        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)
        self.layernorm = tf.keras.layers.LayerNormalization()
        self.add = tf.keras.layers.Add()

class CrossAttention(BaseAttention):
    def call(self, x, context):
        attn_output, attn_scores = self.mha(
            query=x,
            key=context,
            value=context,
            return_attention_scores=True
        )

        self.last_attn_scores = attn_scores

        x = self.add([x, attn_output])
        x = self.layernorm(x)

        return x

class GlobalSelfAttention(BaseAttention):
    def call(self, x):
        attn_output = self.mha(
            query=x,
            value=x,
            key=x
        )

        x = self.add([x, attn_output])
        x = self.layernorm(x)
        return x

class CasualSelfAttention(BaseAttention):
    def call(self, x):
        attn_output = self.mha(
            query=x,
            value=x,
            key=x
        )

        x = self.add([x, attn_output])
        x = self.layernorm(x)
        return x

class FeedForward(tf.keras.layers.Layer):
    def __init__(self, d_model, dff, dropout_rate=0.1):
        super().__init__()
        self.seq = tf.keras.Sequential([
            tf.keras.layers.Dense(dff, activation='relu'),
            tf.keras.layers.Dense(d_model),
            tf.keras.layers.Dropout(dropout_rate)
        ])

        self.add = tf.keras.layers.Add()
        self.layer_norm = tf.keras.layers.LayerNormalization()

    def call(self, x):
        x = self.add([x, self.seq(x)])
        x = self.layer_norm(x)
        return x

class EncoderLayer(tf.keras.layers.Layer):
    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.01):
        super().__init__()

        self.self_attention = GlobalSelfAttention(
            num_heads=num_heads,
            key_dim=d_model,
            dropout=dropout_rate)

        self.ffn = FeedForward(d_model, dff)

    def call(self, x):
        x = self.self_attention(x)
        x = self.ffn(x)
        return x

class Encoder(tf.keras.layers.Layer):
    def __init__(self, *, num_layers, d_model, num_heads,
                 dff, vocab_size, dropout_rate=0.1):
        super().__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.pos_embedding = PositionalEmbedding(
            vocab_size=vocab_size, d_model=d_model
        )

        self.enc_layers = [
            EncoderLayer(d_model=d_model,
                         num_heads=num_heads,
                         dff=dff,
                         dropout_rate=dropout_rate)
            for _ in range(num_layers)]
        self.dropout = tf.keras.layers.Dropout(dropout_rate)

    def call(self, x):

        x = self.pos_embedding(x)

        x = self.dropout(x)

        for i in range(self.num_layers):
            x = self.enc_layers[i](x)

        return x

class DecoderLayer(tf.keras.layers.Layer):
    def __init__(self,
                 x,
                 d_model,
                 num_heads,
                 dff,
                 dropout_rate=0.1):
        super(DecoderLayer, self).__init__()

        self.casual_self_attention = CasualSelfAttention(
            num_heads=num_heads,
            key_dim=d_model,
            dropout=dropout_rate
        )

        self.cross_attention = CrossAttention(
            num_heads=num_heads,
            key_dim=d_model,
            dropout=dropout_rate
        )

        self.ffn = FeedForward(d_model, dff)

    def call(self, x, context):
        x = self.casual_self_attention(x=x)
        x = self.cross_attention(x=x, context=context)

        self.last_attn_scores = self.cross_attention.last_attn_scores

        x = self.ffn(x)
        return x

class Decoder(tf.keras.layers.Layer):
    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,
                 dropout_rate=0.1):
        super().__init__()

        self.d_model = d_model
        self.num_layers = num_layers

        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size, d_model=d_model)
        self.dropout = tf.keras.layers.Dropout(dropout_rate)
        self.dec_layers = [
            DecoderLayer(x=d_model, d_model=d_model, num_heads=num_heads,
                         dff=dff, dropout_rate=dropout_rate)
            for _ in range(num_layers)
        ]

        self.last_attn_scores = None

    def call(self, x, context):
        x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)

        x = self.dropout(x)

        for i in range(self.num_layers):
            x = self.dec_layers[i](x, context)

        self.last_attn_scores = self.dec_layers[-1].last_attn_scores

        # The shape of x is (batch_size, target_seq_len, d_model).
        return x

class Transformer(tf.keras.Model):
  def __init__(self, *, num_layers, d_model, num_heads, dff,
               input_vocab_size, target_vocab_size, dropout_rate=0.1):
    super().__init__()
    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=input_vocab_size,
                           dropout_rate=dropout_rate)

    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,
                           num_heads=num_heads, dff=dff,
                           vocab_size=target_vocab_size,
                           dropout_rate=dropout_rate)

    self.final_layer = tf.keras.layers.Dense(target_vocab_size)


  def call(self, inputs):
    # To use a Keras model with `.fit` you must pass all your inputs in the
    # first argument.
    context, x = inputs

    context = self.encoder(context)  # (batch_size, context_len, d_model)

    x = self.decoder(x, context)  # (batch_size, target_len, d_model)

    # Final linear layer output.
    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)

    try:
      # Drop the keras mask, so it doesn't scale the losses/metrics.
      # b/250038731
      del logits._keras_mask
    except AttributeError:
      pass

    # Return the final output and the attention weights.
    return logits

def lookup(tokenizer, sequence):
    word_index = tokenizer.word_index
    index_word = tokenizer.index_word
    return [index_word.get(idx, &quot;&quot;) for idx in sequence]

num_layers = 4
d_model = 128
dff = 512
num_heads = 8
dropout_rate = 0.1

input_vocab_size = input_vocab_size
target_vocab_size = output_vocab_size

transformer = Transformer(
    num_layers=num_layers,
    d_model=d_model,
    num_heads=num_heads,
    dff=dff,
    input_vocab_size=input_vocab_size,
    target_vocab_size=target_vocab_size,
    dropout_rate=dropout_rate)

output = transformer((input_sequences, output_sequences), training=False)

class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, d_model, warmup_steps=4000):
        super().__init__()

        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)

        self.warmup_steps = warmup_steps

    def __call__(self, step):
        step = tf.cast(step, dtype=tf.float32)
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps ** -1.5)

        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

learning_rate = CustomSchedule(d_model)

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)

def masked_loss(label, pred):
    mask = label != 0
    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=True, reduction='none'
    )
    loss = loss_object(label, pred)

    mask = tf.cast(mask, dtype=loss.dtype)
    loss *= mask

    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)
    return loss

def masked_accuracy(label, pred):
    pred = tf.argmax(pred, axis=2)
    label = tf.cast(label, pred.dtype)
    match = label == pred

    mask = label != 0

    match = match &amp; mask

    match = tf.cast(match, dtype=tf.float32)
    mask = tf.cast(mask, dtype=tf.float32)

    return tf.reduce_sum(match)/tf.reduce_sum(mask)

transformer.compile(
    loss=masked_loss,
    optimizer=optimizer,
    metrics=[masked_accuracy])

transformer.fit(x=input_sequences, y=output_sequences, epochs=5)

class Chatbot(tf.Module):
    def __init__(self, tokenizers, transformer):
        self.tokenizers = tokenizers
        self.transformer = transformer

    def __call__(self, sentence, max_length=MAX_TOKENS):
        assert isinstance(sentence, tf.Tensor)
        if (len(sentence.shape)) == 0:
            sentence = sentence[tf.newaxis]

        sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()

        encoder_input = sentence

        # As the output language is English, initialize the output with the
        # English `[START]` token.
        start_end = self.tokenizers.en.tokenize([''])[0]
        start = start_end[0][tf.newaxis]
        end = start_end[1][tf.newaxis]

        # `tf.TensorArray` is required here (instead of a Python list), so that the
        # dynamic-loop can be traced by `tf.function`.
        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
        output_array = output_array.write(0, start)

        for i in tf.range(max_length):
            output = tf.transpose(output_array.stack())
            predictions = self.transformer([encoder_input, output], training=False)

            # Select the last token from the `seq_len` dimension.
            predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.

            predicted_id = tf.argmax(predictions, axis=-1)

            # Concatenate the `predicted_id` to the output which is given to the
            # decoder as its input.
            output_array = output_array.write(i + 1, predicted_id[0])

            if predicted_id == end:
                break

        output = tf.transpose(output_array.stack())
        # The output shape is `(1, tokens)`.
        text = tokenizer.sequences_to_texts(output)[0]

        tokens = lookup(tokenizer, output)[0]

        # `tf.function` prevents us from using the attention_weights that were
        # calculated on the last iteration of the loop.
        # So, recalculate them outside the loop.
        self.transformer([encoder_input, output[:, :-1]], training=False)
        attention_weights = self.transformer.decoder.last_attn_scores

        return text, tokens, attention_weights

Chatbot = Chatbot(tokenizer, transformer)

def print_transalation(sentence, tokens, ground_truth):
    print(f'{&quot;Input:&quot;:15s}: {sentence}')
    print(f'{&quot;Prediction&quot;:15s}: {tokens.numpy().decode(&quot;utf-8&quot;)}')
    print(f'{&quot;Ground truth&quot;:15s}: {ground_truth}')

class ExportChatbot(tf.Module):
    def __init__(self, chatbot):
        self.chatbot = chatbot

    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])
    def __call__(self, sentence):
        (result,
         tokens,
         attention_weights) = self.transalator(sentence, max_length=MAX_TOKENS)

        return result

Chatbot = ExportChatbot(Chatbot)

tf.saved_model.save(Chatbot, export_dir='Chatbot')


</code></pre>
<p>I run the code and it says:</p>
<pre><code>    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.


Call arguments received by layer 'transformer' (type Transformer):
  • inputs=tf.Tensor(shape=(None, 5), dtype=int32)
</code></pre>
<p>I have tried to turn the sequences into arrays and convert them to tensors but nothing is working</p>
","transformer-model"
"76440501","Handling large inputs in Programming Language Models","2023-06-09 12:58:12","","1","151","<python><machine-learning><nlp><huggingface-transformers><transformer-model>","<p>I am using CodeBERT and CodeT5 models to generate embeddings for my Python code dataset. Specifically, I am using the <code>codebert-base</code> and <code>codet5-small</code> models from HuggingFace. However, these models have a maximum input sequence length of 512 tokens. When tokenizing the Python codes, I find that their length is approximately 10 times larger than the maximum input sequence length. My intention is not to perform any downstream tasks; rather, I want to obtain the embeddings for the purpose of clustering. I am wondering how to handle these lengthy input codes aside from removing less important parts from the inputs.</p>
","transformer-model"
"76434962","How to get transformer Model running on GPU","2023-06-08 19:02:44","","0","661","<machine-learning><deep-learning><pytorch><transformer-model>","<p>Hi so I have a transformer model that only seems to be running on cpu and whenever I try to get it to run on cuda it throws an error saying that two devices are present.</p>
<p>I am using the transformer model found here <a href=""https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb"" rel=""nofollow noreferrer"">https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb</a> .</p>
<p>I have been interfacing with it in order to try and get it to lear off of a pandas dataframe.</p>
<p>this is my code to interact with the model</p>
<pre><code>src_vocab_size = 100160
tgt_vocab_size = 79258
d_model = 512
num_heads = 8
num_layers = 6
d_ff = 2048
max_seq_length = 100
dropout = 0.1

transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)


enctype = &quot;cl100k_base&quot;
encoding = tiktoken.get_encoding(enctype)
df = pd.read_csv('/home/rdrori/geneLocationLookupNetwork/Translation_Admiration/locus.tsv', sep='\t', header = None)
df.columns = ['Column1', 'Column2', 'Column3', 'Column4']
df['Column2'] = df['Column2'].str.cat(df['Column3'], sep=' ', na_rep='')
#print(df.iloc[886, 0])
#print(df.iloc[886, 1])
#print(df.iloc[886, 2])
#src_data = torch.randint(1, 50, (64, 50))
#print(src_data)
source_list = [(encoding.encode(text)) for text in df['Column1']]
trgt_list = [(encoding.encode(text)) for text in df['Column2']]

torch.set_printoptions(sci_mode=False)
source_tensors = []
for sublist in source_list:
    temptensor = torch.tensor(sublist[:16])  # Truncate sublist to length 16
    if len(temptensor) &lt; 16:
        padded_tensor = torch.zeros(16)  # Create a tensor of zeros
        padded_tensor[:len(temptensor)] = temptensor  # Copy sublist elements into the padded tensor
        temptensor = padded_tensor
    source_tensors.append(temptensor)
    
    
target_tensors = []
for slis in trgt_list:
    temporarytens = torch.tensor(slis[:16])  # Truncate sublist to length 16
    if len(temporarytens) &lt; 16:
        padded_tensy = torch.zeros(16)  # Create a tensor of zeros
        padded_tensy[:len(temporarytens)] = temporarytens  # Copy sublist elements into the padded tensor
        temporarytens = padded_tensy
    target_tensors.append(temporarytens)
    
    
print (torch.cat(source_tensors).reshape(41316, 16))
print (torch.cat(target_tensors).reshape(41316, 16))
src_data = (torch.cat(source_tensors).reshape(41316, 16)).long()
tgt_data = (torch.cat(target_tensors).reshape(41316, 16)).long()


transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)




criterion = nn.CrossEntropyLoss(ignore_index=0)
optimizer = optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)

transformer.train()

# Define the batch size and number of splits
batch_size = 32
num_splits = math.ceil(src_data.shape[0] / batch_size)

for epoch in range(313):
    optimizer.zero_grad()
    total_loss = 0.0
    
    for i in range(num_splits):
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size, src_data.shape[0])
        
        src_batch = src_data[start_idx:end_idx]
        tgt_batch = tgt_data[start_idx:end_idx]
        
        output = transformer(src_batch, tgt_batch[:, :-1])
        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_batch[:, 1:].contiguous().view(-1))
        
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    print(f&quot;Epoch: {epoch+1}, Loss: {total_loss / num_splits}&quot;)
</code></pre>
<p>When running that code as it is nothing is outputted because at least as far as I am aware the cpu is much to slow to process the data.</p>
<p>I have tried sending devices to cuda using .to(device) where device is set up to be cuda, I have sent the transformer and any relevant data, I did get this error</p>
<pre><code> File &quot;/home/rdrori/geneLocationLookupNetwork/pivotdevice.py&quot;, line 142, in forward
    src_mask, tgt_mask = self.generate_mask(src, tgt)
  File &quot;/home/rdrori/geneLocationLookupNetwork/pivotdevice.py&quot;, line 138, in generate_mask
    tgt_mask = tgt_mask &amp; nopeak_mask
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and CPU! 
</code></pre>
<p>And have also tried to send tgt_mask, nopeak_mask and src_mask to cuda.</p>
","transformer-model"
"76419472","Fine Tuning Llama on Unlabelled Data","2023-06-07 02:04:50","","1","1157","<nlp><transformer-model><llama-index>","<p>I'd like to use Llama to do a conversational chat bot to answer questions on scientific news articles. Can I use a collection of unlabelled scientific articles to fine tune Llama and to &quot;increase its vocabulary&quot;/&quot;extend its knowledge base&quot;?</p>
<p>I'm not sure how to approach this task without manually creating extra texts as labels for designed elaboration/summarization prompts created by me.</p>
","transformer-model"
"76381190","TypeError: batch_text_or_text_pairs has to be a list (got )","2023-06-01 10:57:25","","1","540","<encoding><nlp><huggingface-transformers><bert-language-model><transformer-model>","<p>I'm getting this error. I'm running this exact code from <a href=""https://www.kaggle.com/code/perevalov540/multiclass-text-classification-with-transformers/notebook"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/perevalov540/multiclass-text-classification-with-transformers/notebook</a></p>
<pre><code>TypeError                                 Traceback (most recent call last)
File :1

Cell In[20], line 5, in regular_encode(texts, tokenizer, maxlen)
      1 def regular_encode(texts, tokenizer, maxlen=512):
      2     &quot;&quot;&quot;
      3     encodes text for a model
      4     &quot;&quot;&quot;
----&gt; 5     enc_di = tokenizer.batch_encode_plus(
      6         texts,
      7         return_token_type_ids=False,
      8         padding='max_length',
      9         max_length=None
     10         
     11     )
     13     return np.array(enc_di['input_ids'])

File ~/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2473, in PreTrainedTokenizerBase.batch_encode_plus(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   2463 # Backward compatibility for 'truncation_strategy', 'pad_to_max_length'
   2464 padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(
   2465     padding=padding,
   2466     truncation=truncation,
   (...)
   2470     **kwargs,
...
   (...)
    382         pad_to_multiple_of=pad_to_multiple_of,
    383     )

TypeError: batch_text_or_text_pairs has to be a list (got )
</code></pre>
<p>This is the function that's giving error.</p>
<pre><code>def regular_encode(texts, tokenizer, maxlen=512):
    &quot;&quot;&quot;
    encodes text for a model
    &quot;&quot;&quot;
    enc_di = tokenizer.batch_encode_plus(
        texts,
        return_token_type_ids=False,
        padding='max_length',
        max_length=None
    )

    return np.array(enc_di['input_ids'])
</code></pre>
","transformer-model"
"76378844","Multi Query Attention implementation","2023-06-01 05:17:25","","0","1817","<python><machine-learning><deep-learning><pytorch><transformer-model>","<p>Im trying to implement the multi query attention as found in Google's PaLM model. However, im not sure if my implementation is correct. If it is, is there a better way to implement this. I can't seem to find any open implementation of it anywhere. Im using PyTorch 2.0.</p>
<pre><code>class CausalAttention(nn.Module):
    def __init__(self, n_embd, n_head, dropout):
        super(CausalAttention, self).__init__()
        assert n_embd % n_head == 0

        self.q_attn = nn.Linear(n_embd, n_embd, bias=False)
        self.k_attn = nn.Linear(n_embd, n_embd // n_head, bias=False)
        self.v_attn = nn.Linear(n_embd, n_embd // n_head, bias=False)

        self.c_proj = nn.Linear(n_embd, n_embd, bias=False)

        self.attn_dropout = nn.Dropout(dropout)
        self.resid_dropout = nn.Dropout(dropout)

        self.n_head = n_head
        self.n_embd = n_embd
        self.dropout = dropout

    def forward(self, x):
        B, T, C = x.shape

        q = self.q_attn(x)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)

        k = self.k_attn(x)
        k = k.view(B, T, 1, C // self.n_head).transpose(1, 2)

        v = self.v_attn(x)
        v = v.view(B, T, 1, C // self.n_head).transpose(1, 2)

        y = F.scaled_dot_product_attention(
            q,
            k,
            v,
            attn_mask=None,
            dropout_p=self.dropout if self.training else 0,
            is_causal=True,
        )

        y = y.transpose(1, 2).contiguous().view(B, T, C)

        y = self.resid_dropout(self.c_proj(y))

        return y
</code></pre>
<p>The code runs fine, just not sure if the implementation is accurate.</p>
","transformer-model"
"76376524","Expected input batch_size (28) to match target batch_size (456), Changing batch size increase the target batch size with GPT2 model","2023-05-31 18:56:43","","1","141","<dataframe><deep-learning><nlp><transformer-model><gpt-2>","<p>I was practising fine-tuning a gpt2 model on a simple question-answer dataset when I encountered this error. I have studied other answers, but my input dataset shapes look fine.</p>
<pre><code>def tokenize_data(total_marks, coding_feeddback):
    inputs = tokenizer(total_marks, truncation=True, padding=True, 
                                                     return_tensors=&quot;pt&quot;)
    labels = tokenizer(coding_feeddback, truncation=True, padding=True, 
                                                    return_tensors=&quot;pt&quot;)['input_ids']
return inputs, labels

*# Prepare the training and validation datasets*

 train_inputs, train_labels = tokenize_data(train_df['Question'].tolist(), 
 train_df['ans'].tolist())
 val_inputs, val_labels = tokenize_data(val_df['Question'].tolist(), 
 val_df['ans'].tolist())

 train_dataset = TensorDataset(train_inputs['input_ids'], train_labels)
 val_dataset = TensorDataset(val_inputs['input_ids'], val_labels)
</code></pre>
<blockquote>
<p><em>Here is the size of the train and validation dataset</em></p>
</blockquote>
<pre><code>print('train input shape:',train_inputs['input_ids'].shape)
print('train label shape: ',train_labels.shape)
print('validation input shape: ',val_inputs['input_ids'].shape)
print('validation label shape: ',val_labels.shape)
</code></pre>
<blockquote>
<p><em>The output of the above lines is as follows.</em></p>
</blockquote>
<pre><code>train input shape: torch.Size([76, 8])
train label shape:  torch.Size([76, 115])
validation input shape:  torch.Size([20, 8])
validation label shape:  torch.Size([20, 98])
</code></pre>
<blockquote>
<p><em>This is how I am using Dataloader.</em></p>
</blockquote>
<pre><code>batch_size = 4
train_dataloader = DataLoader(train_dataset, 
                   batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size)
</code></pre>
<blockquote>
<p>Here is the model.</p>
</blockquote>
<h1>Training loop</h1>
<pre><code>model.train()
for epoch in range(num_epochs):
 for batch in train_dataloader:
    batch = [item.to(device) for item in batch]
    input_ids, labels = batch

    optimizer.zero_grad()
    
    print(&quot;indputIds:&quot;,len(input_ids))
    print(&quot;lebels:&quot;,len(labels))

    outputs = model(input_ids=input_ids, labels=labels)
    loss = outputs.loss
    logits = outputs.logits

    loss.backward()
    optimizer.step()

# Validation
with torch.no_grad():
    model.eval()
    val_loss = 0.0
    for val_batch in val_dataloader:
        val_batch = [item.to(device) for item in val_batch]
        val_input_ids, val_labels = val_batch

        val_outputs = model(input_ids=val_input_ids, labels=val_labels)
        val_loss += val_outputs.loss.item()

    average_val_loss = val_loss / len(val_dataloader)
    print(f&quot;Epoch: {epoch+1}, Validation Loss: {average_val_loss:.4f}&quot;)

model.train()
</code></pre>
<blockquote>
<p>Even the dimension in each batch is the same In train and validation data loader For example:</p>
</blockquote>
<pre><code>Batch 19
Inputs:
  torch.Size([4, 8])
Targets:
  torch.Size([4, 115])
</code></pre>
<blockquote>
<p>Same for validation except in validation target size is [4,8] and [8,98].</p>
</blockquote>
","transformer-model"
"76372007","Trying to install guanaco (pip install guanaco) for a text classification model but getting error","2023-05-31 09:26:21","","1","3630","<python><huggingface-transformers><transformer-model><large-language-model>","<p>I'm trying to install the guanaco language model <a href=""https://arxiv.org/abs/2305.14314"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2305.14314</a> using <code>pip install guanaco</code> for a text classification model but getting error.</p>
<pre><code>Failed to build guanaco
ERROR: Could not build wheels for guanaco, which is required to install pyproject.toml-based projects
</code></pre>
<p><strong>How do I install the language model and use it for classification?</strong></p>
","transformer-model"
"76369451","Unable to train the conformer-rnnt model on tedlium data","2023-05-31 00:45:26","","0","36","<python><huggingface-transformers><transformer-model><distributed-training>","<p>I am trying to train the conformer-rnnt model on tedlium data and am encountering the below error when the command to train is executed.</p>
<pre><code>usage: run_speech_recognition_rnnt.py [-h] (--manifest MANIFEST | --data_file DATA_FILE) --data_root DATA_ROOT [--vocab_size VOCAB_SIZE] [--tokenizer {spe,wpe}]
                                  [--spe_type {bpe,unigram,char,word}] [--spe_character_coverage SPE_CHARACTER_COVERAGE] [--spe_bos] [--spe_eos] [--spe_pad]
                                  [--spe_sample_size SPE_SAMPLE_SIZE] [--spe_train_extremely_large_corpus]
                                  [--spe_max_sentencepiece_length SPE_MAX_SENTENCEPIECE_LENGTH] [--spe_no_split_by_unicode_script] [--no_lower_case] [--log]
run_speech_recognition_rnnt.py: error: the following arguments are required: --data_root
</code></pre>
<p>The command to train the conformer-rnnt model is shown below:</p>
<pre><code>#!/usr/bin/env bash
CUDA_VISIBLE_DEVICES=0 python run_speech_recognition_rnnt.py \
    --config_path=&quot;conf/conformer_transducer_bpe_xlarge.yaml&quot; \
    --model_name_or_path=&quot;stt_en_conformer_transducer_xlarge&quot; \
    --dataset_name=&quot;esc-benchmark/esc-datasets&quot; \
    --tokenizer_path=&quot;tokenizer&quot; \
    --vocab_size=&quot;1024&quot; \
    --max_steps=&quot;100000&quot; \
    --dataset_config_name=&quot;tedlium&quot; \
    --output_dir=&quot;./&quot; \
    --run_name=&quot;rnnt-tedlium-baseline&quot; \
    --wandb_project=&quot;rnnt&quot; \
    --per_device_train_batch_size=&quot;8&quot; \
    --per_device_eval_batch_size=&quot;4&quot; \
    --logging_steps=&quot;50&quot; \
    --learning_rate=&quot;1e-4&quot; \
    --warmup_steps=&quot;500&quot; \
    --save_strategy=&quot;steps&quot; \
    --save_steps=&quot;20000&quot; \
    --evaluation_strategy=&quot;steps&quot; \
    --eval_steps=&quot;20000&quot; \
    --report_to=&quot;wandb&quot; \
    --preprocessing_num_workers=&quot;4&quot; \
    --fused_batch_size=&quot;4&quot; \
    --length_column_name=&quot;input_lengths&quot; \
    --fuse_loss_wer \
    --group_by_length \
    --overwrite_output_dir \
    --do_train \
    --do_eval \
    --do_predict \
    --use_auth_token
</code></pre>
<p>The run_speech_recognition_rnnt.py calls processes which incorporate the arguments given in the error, however it looks like it fails to recognize them. The below code is present in a file named process_asr_text_tokenizer.py and is called by the run_speech_recognition_rnnt.py.</p>
<pre><code>def main():
data_root = args.data_root
manifests = args.manifest
data_file = args.data_file
vocab_size = args.vocab_size
tokenizer = args.tokenizer
spe_type = args.spe_type
spe_character_coverage = args.spe_character_coverage
spe_sample_size = args.spe_sample_size
spe_train_extremely_large_corpus = args.spe_train_extremely_large_corpus
spe_max_sentencepiece_length = args.spe_max_sentencepiece_length
spe_split_by_unicode_script = args.spe_split_by_unicode_script
spe_bos, spe_eos, spe_pad = args.spe_bos, args.spe_eos, args.spe_pad
lower_case = args.lower_case

if not os.path.exists(data_root):
    os.makedirs(data_root)

if args.log:
    logging.basicConfig(level=logging.INFO)

if manifests:
    text_corpus_path = __build_document_from_manifests(data_root, manifests)
else:
    text_corpus_path = data_file
tokenizer_path = __process_data(
    text_corpus_path,
    data_root,
    vocab_size,
    tokenizer,
    spe_type,
    lower_case=lower_case,
    spe_character_coverage=spe_character_coverage,
    spe_sample_size=spe_sample_size,
    spe_train_extremely_large_corpus=spe_train_extremely_large_corpus,
    spe_max_sentencepiece_length=spe_max_sentencepiece_length,
    spe_split_by_unicode_script=spe_split_by_unicode_script,
    spe_bos=spe_bos,
    spe_eos=spe_eos,
    spe_pad=spe_pad,
)

print(&quot;Serialized tokenizer at location :&quot;, tokenizer_path)
logging.info('Done!')
</code></pre>
<p>I am trying to train a conformer-rnnt model on tedlium data and i expect it to train for a while and return certain metrics as results. However, in the course of the experiment I am facing errors running the train command. Although the error seems like a syntactical error, i dont think it is just so and that it would require a more intricate solution.</p>
<p>Please help me resolve this issue.</p>
","transformer-model"
"76365721","How to get Transformer-based recommendation model to predict new movies","2023-05-30 14:09:42","","0","98","<python><tensorflow><keras><neural-network><transformer-model>","<p>I'm trying to run the Behavior Sequence Transformer model in <a href=""https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/structured_data/ipynb/movielens_recommendations_transformers.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/structured_data/ipynb/movielens_recommendations_transformers.ipynb</a> to predict the next movie to watch, based on previous movie ratings.</p>
<p>I've got the code in the Colab notebook running correctly, but can't seem to get any meaningful predictions out of it.</p>
<p>Here's the output of model.input_shape:</p>
<pre><code>{'user_id': (None, 1), 'sequence_movie_ids': (None, 3), 'target_movie_id': (None, 1), 'sequence_ratings': (None, 3), 'sex': (None, 1), 'age_group': (None, 1), 'occupation': (None, 1)}
</code></pre>
<p>And here's the code I'm using for predictions:</p>
<pre><code># Load the latest 3 movies you watched and gave them all a rating of 4
latest_movies = pd.DataFrame({
    'user_id': [0],
    'sequence_movie_ids': [[1, 2, 3]],
    'target_movie_id': [0],
    'sequence_ratings': [[4.0, 4.0, 4.0]],
    'sex': [0],
    'age_group': [0],
    'occupation': [0]
})

# Predict the next movie to watch
next_movie = model.predict(input_data)
print(next_movie)
</code></pre>
<p>However, this leads to the following error message:</p>
<pre><code>Node: 'model/Cast'
Cast int32 to string is not supported
     [[{{node model/Cast}}]] [Op:__inference_predict_function_40501]
</code></pre>
<p>I've also tried to convert input_data to tensors:</p>
<pre><code># Prepare the input data
input_data = {
    'user_id': tf.convert_to_tensor(latest_movies['user_id'], dtype=tf.int32),
    'sequence_movie_ids': tf.convert_to_tensor(latest_movies['sequence_movie_ids'], dtype=tf.int32),
    'target_movie_id': tf.convert_to_tensor(latest_movies['target_movie_id'], dtype=tf.int32),
    'sequence_ratings': tf.convert_to_tensor(latest_movies['sequence_ratings'], dtype=tf.float32),
    'sex': tf.convert_to_tensor(latest_movies['sex'], dtype=tf.int32),
    'age_group': tf.convert_to_tensor(latest_movies['age_group'], dtype=tf.int32),
    'occupation': tf.convert_to_tensor(latest_movies['occupation'], dtype=tf.int32)
}
</code></pre>
<p>But this fails as well, with error message:</p>
<pre><code>ValueError: setting an array element with a sequence.
</code></pre>
<p>How can we get this model to make predictions?</p>
<p>Thanks very much</p>
","transformer-model"
"76356591","How to skip weights init when loading pretrained transformers model?","2023-05-29 10:33:20","","2","878","<python><pytorch><initialization><huggingface-transformers><transformer-model>","<p>I need to find out how to load a pretrained transformer model without initializing weights in the beginning (to save time and memory)?</p>
<ol>
<li>I saw this code example, but this is not elegant:
<pre><code>saved_inits = torch.nn.init.kaiming_uniform_, 
    torch.nn.init.uniform_, 
    torch.nn.init.normal_  # preserving
torch.nn.init.kaiming_uniform_ = skip
torch.nn.init.uniform_ = skip
torch.nn.init.normal_ = skip

model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=args.model_path)

torch.nn.init.kaiming_uniform_, 
    torch.nn.init.uniform_, 
    torch.nn.init.normal_ = saved_inits  # restoring
</code></pre>
</li>
<li>for <code>nn.module</code> subclasses there is <code>torch.nn.utils.skip_init</code>, but it won't work with <code>AutoModelForCausalLM</code></li>
</ol>
<p>Quest: find a way to skip weights initialization in <code>AutoModelForCausalLM</code> (or any similar transformers class)  either using some standard wrapper or parameter.</p>
","transformer-model"
"76309306","AssertionError in generalized_box_iou","2023-05-22 19:21:15","","0","342","<python><transformer-model>","<p>I'm following a <a href=""https://www.youtube.com/watch?v=RkhXoj_Vvr4"" rel=""nofollow noreferrer"">YouTube tutorial</a> to form a DETR model for face detection using same dataset and code. If I get it right, I'm planning to modify it for my project. Right now, I'm getting an AssertionError when I run main.py. The output looked like this:</p>
<pre><code>File &quot;F:\DETR\util\box_ops.py&quot;, line 51, in generalized_box_iou
    assert (boxes1[:, 2:] &gt;= boxes1[:, :2]).all()

AssertionError
</code></pre>
<p>I tried using pdb:</p>
<pre><code>import pdb
def generalized_box_iou(boxes1, boxes2):
    &quot;&quot;&quot;
    Generalized IoU from https://giou.stanford.edu/

    The boxes should be in [x0, y0, x1, y1] format

    Returns a [N, M] pairwise matrix, where N = len(boxes1)
    and M = len(boxes2)
    &quot;&quot;&quot;
    # degenerate boxes gives inf / nan results
    # so do an early check
    pdb.set_trace()
    assert (boxes1[:, 2:] &gt;= boxes1[:, :2]).all()
    assert (boxes2[:, 2:] &gt;= boxes2[:, :2]).all()
    iou, union = box_iou(boxes1, boxes2)

    lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])
    rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])

    wh = (rb - lt).clamp(min=0)  # [N,M,2]
    area = wh[:, :, 0] * wh[:, :, 1]

    return iou - (area - union) / area
</code></pre>
<p>I got following output:</p>
<pre><code>&gt; f:\detr\util\box_ops.py(53)generalized_box_iou()
     51     # so do an early check
     52     pdb.set_trace()
---&gt; 53     assert (boxes1[:, 2:] &gt;= boxes1[:, :2]).all()
     54     assert (boxes2[:, 2:] &gt;= boxes2[:, :2]).all()
     55     iou, union = box_iou(boxes1, boxes2)


p boxes1[:, 2:]
tensor([[0.4649, 0.6003],
        [0.7213, 0.9160],
        [0.6872, 0.9912],
        [0.5093, 0.9677],
        [0.2026, 0.9832],
        [0.6015, 0.9870],
        [0.5790, 0.9843],
        [0.5445, 0.9818],
        [0.1135, 0.9933],
        [0.5385, 1.0058],
        [0.3093, 0.5704],
        [0.6826, 0.7190],
        [0.6489, 0.9788],
        [0.5300, 0.8577],
        [0.3642, 0.9970],
        [0.6232, 0.9240],
        [0.6089, 0.9871],
        [0.6521, 0.9505],
        [0.1352, 1.0018],
        [0.5606, 0.9524]], device='cuda:0')

p boxes1[:, :2]
tensor([[0.2439, 0.1090],
        [0.6295, 0.3856],
        [0.4519, 0.0084],
        [0.3994, 0.5895],
        [0.1331, 0.4344],
        [0.4986, 0.5543],
        [0.4745, 0.5840],
        [0.1148, 0.0825],
        [0.0351, 0.5683],
        [0.3938, 0.1501],
        [0.1885, 0.1104],
        [0.6015, 0.3602],
        [0.4971, 0.0174],
        [0.4392, 0.5872],
        [0.1143, 0.1519],
        [0.5410, 0.4566],
        [0.4996, 0.5593],
        [0.0434, 0.1141],
        [0.0593, 0.3776],
        [0.4481, 0.0401]], device='cuda:0')

p boxes2[:, 2:]
tensor([[0.5703, 0.3911],
        [0.5566, 0.4319]], device='cuda:0')

p boxes2[:, :2]
tensor([[0.5361, 0.3342],
        [0.3488, 0.0721]], device='cuda:0')
</code></pre>
<p>I get different values for boxes1. One time it was p boxes1[:, 2:]</p>
<pre><code>tensor([[nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan],
        [nan, nan]], device='cuda:0')
</code></pre>
<p>The value keeps on changing. I have used the same train and val json files and images from the tutorial.</p>
","transformer-model"
"76306997","How to use a different pre-trained BERT model with bert_score","2023-05-22 14:02:27","","1","332","<bert-language-model><transformer-model>","<p>I want you to use different pretrain bert model embeddings for the bert score. How can I do that?
P, R, F1 = score(cand, ref, lang=&quot;bn&quot;, model_type=&quot;distilbert-base-uncased&quot;,  verbose=True)
In model_type if use my pretain model then it gives a keyError.</p>
","transformer-model"
"76304449","TFAutoModelForSeq2SeqLM requires the TensorFlow library but it was not found in your environment","2023-05-22 08:49:55","","0","632","<python><python-3.x><tensorflow><huggingface-transformers><transformer-model>","<p>For code below:</p>
<pre><code>from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer
from transformers import pipeline
# Load the pre-trained T5 model and tokenizer
model_checkpoint = &quot;t5-small&quot;
model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Define the summarization function


summarizer = pipeline(&quot;summarization&quot;, model=model, tokenizer=tokenizer, framework=&quot;tf&quot;)

summarizer(
   &quot;how are you today? this is something remarkable, the sun is from the west&quot;,
    min_length=1,
    max_length=19,
)
</code></pre>
<p>I got the following error:</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_37988/4253040799.py in &lt;module&gt;
      3 # Load the pre-trained T5 model and tokenizer
      4 model_checkpoint = &quot;t5-small&quot;
----&gt; 5 model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)
      6 tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
      7 

C:\ProgramData\Anaconda3\lib\site-packages\transformers\utils\dummy_tf_objects.py in from_pretrained(cls, *args, **kwargs)
    262     @classmethod
    263     def from_pretrained(cls, *args, **kwargs):
--&gt; 264         requires_backends(cls, [&quot;tf&quot;])
    265 
    266 

C:\ProgramData\Anaconda3\lib\site-packages\transformers\file_utils.py in requires_backends(obj, backends)
    681     name = obj.__name__ if hasattr(obj, &quot;__name__&quot;) else obj.__class__.__name__
    682     if not all(BACKENDS_MAPPING[backend][0]() for backend in backends):
--&gt; 683         raise ImportError(&quot;&quot;.join([BACKENDS_MAPPING[backend][1].format(name) for backend in backends]))
    684 
    685 

ImportError: 
TFAutoModelForSeq2SeqLM requires the TensorFlow library but it was not found in your environment. Checkout the instructions on the
installation page: https://www.tensorflow.org/install and follow the ones that match your environment.
</code></pre>
<p>How should I solve it?</p>
<p>The result of pip list show that I have tensorflow 2.12.0 installed</p>
<pre><code>tensorflow                         2.12.0
</code></pre>
<p>I have seen this <a href=""https://stackoverflow.com/questions/70624869/tfbertforsequenceclassification-requires-the-tensorflow-library-but-it-was-not-f"">TFBertForSequenceClassification requires the TensorFlow library but it was not found in your environment</a> but neither solution within help.</p>
<p>Update for Alvas:
<a href=""https://i.sstatic.net/VFw6F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VFw6F.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"76302971","Question in Pytorch transformer_tutorial about 'NoneType' object has no attribute 'Lock'","2023-05-22 04:04:00","","3","4300","<pytorch><google-colaboratory><transformer-model><torchtext><non-type>","<p>When I run the transformer_tutorial code from Pytorch (<a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a>), I meet a problem in build_vocab_from_iterator.</p>
<pre><code>from torchtext.datasets import WikiText2
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator

train_iter = WikiText2(split='train')
tokenizer = get_tokenizer('basic_english')


vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['&lt;unk&gt;'])
</code></pre>
<pre><code>AttributeError: 'NoneType' object has no attribute 'Lock'
This exception is thrown by __iter__ of _MemoryCellIterDataPipe(remember_elements=1000, source_datapipe=_ChildDataPipe)
</code></pre>
<p>I tried with other torchtext.dataset such as the following codes:</p>
<pre><code>from torchtext.datasets import IMDB

train_iter = IMDB(split='train')

def tokenize(label, line):
    return line.split()

tokens = []
for label, line in train_iter:
    tokens += tokenize(label, line)
</code></pre>
<p>still return the same error. I run all the codes in Google Colab.</p>
<p>I tried to run the codes in different version of pytorch and corresponding pytorchtext, but it failed. I really appreciate it if you could give me some help. Thanks!</p>
","transformer-model"
"76302243","Transformers from scratch - shape '[1, 40, 64]' is invalid for input of size when passing input from encoder to decoder","2023-05-21 22:54:28","","0","624","<python><pytorch><nlp><huggingface-transformers><transformer-model>","<h3>I'm trying to build a transformer model from scratch:</h3>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F

class TransformerEncoder(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_heads, ff_size, dropout):
        super(TransformerEncoder, self).__init__()
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.pos_embedding = nn.Embedding(1000, hidden_size)  # Positional embedding
        self.dropout = nn.Dropout(dropout)
        
        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads)
        self.layer_norm1 = nn.LayerNorm(hidden_size)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(hidden_size, ff_size),
            nn.ReLU(),
            nn.Linear(ff_size, hidden_size)
        )
        self.layer_norm2 = nn.LayerNorm(hidden_size)
        
        self.num_layers = num_layers

    def forward(self, input_seq):
        seq_len, batch_size = input_seq.size()

        # Embedding and positional encoding
        embedded = self.embedding(input_seq)
        pos_ids = torch.arange(seq_len, device=input_seq.device).unsqueeze(0).expand_as(input_seq)
        pos_embedded = self.pos_embedding(pos_ids)
        encoded = self.dropout(embedded + pos_embedded)

        # Transformer encoder layers
        for _ in range(self.num_layers):
            # Self-attention
            attention_output, _ = self.self_attention(encoded, encoded, encoded)
            attention_output = self.layer_norm1(encoded + self.dropout(attention_output))
            
            # Feed-forward
            ff_output = self.feed_forward(attention_output)
            ff_output = self.layer_norm2(attention_output + self.dropout(ff_output))

            encoded = ff_output

        return encoded


class TransformerDecoder(nn.Module):
    def __init__(self, output_size, hidden_size, num_layers, num_heads, ff_size, dropout):
        super(TransformerDecoder, self).__init__()
        self.embedding = nn.Embedding(output_size, hidden_size)
        self.pos_embedding = nn.Embedding(1000, hidden_size)  # Positional embedding
        self.dropout = nn.Dropout(dropout)
        
        self.self_attention = nn.MultiheadAttention(hidden_size, num_heads)
        self.layer_norm1 = nn.LayerNorm(hidden_size)
        
        self.encoder_attention = nn.MultiheadAttention(hidden_size, num_heads)
        self.layer_norm2 = nn.LayerNorm(hidden_size)
        
        self.feed_forward = nn.Sequential(
            nn.Linear(hidden_size, ff_size),
            nn.ReLU(),
            nn.Linear(ff_size, hidden_size)
        )
        self.layer_norm3 = nn.LayerNorm(hidden_size)
        
        self.fc = nn.Linear(hidden_size, output_size)

        self.num_layers = num_layers

    def forward(self, input_seq, encoder_output):
        seq_len, batch_size = input_seq.size()

        # Embedding and positional encoding
        embedded = self.embedding(input_seq)
        pos_ids = torch.arange(seq_len, device=input_seq.device).unsqueeze(0).expand_as(input_seq)
        pos_embedded = self.pos_embedding(pos_ids)
        encoded = self.dropout(embedded + pos_embedded)

        # Transformer decoder layers
        for _ in range(self.num_layers):
            # Self-attention
            self_attention_output, _ = self.self_attention(encoded, encoded, encoded)
            self_attention_output = self.layer_norm1(encoded + self.dropout(self_attention_output))
            
            # Encoder-decoder attention
            encoder_attention_output, _ = self.encoder_attention(self_attention_output, encoder_output, encoder_output)
            encoder_attention_output = self.layer_norm2(self_attention_output + self.dropout(encoder_attention_output))
            
            # Feed-forward
            ff_output = self.feed_forward(encoder_attention_output)
            ff_output = self.layer_norm3(encoder_attention_output + self.dropout(ff_output))

            encoded = ff_output

        output = self.fc(encoded)
        return output


class Transformer(nn.Module):
    def __init__(self, input_size, output_size, hidden_size, num_layers, num_heads, ff_size, dropout):
        super(Transformer, self).__init__()
        self.encoder = TransformerEncoder(input_size, hidden_size, num_layers, num_heads, ff_size, dropout)
        self.decoder = TransformerDecoder(output_size, hidden_size, num_layers, num_heads, ff_size, dropout)

    def forward(self, input_seq, target_seq):
        encoder_output = self.encoder(input_seq)
        output = self.decoder(target_seq, encoder_output)
        return output


# Example parameters
input_size = 50265  # Vocabulary size
output_size = 50265  # Vocabulary size
hidden_size = 256  # Hidden state size of the transformer layers
num_layers = 2  # Number of transformer layers
num_heads = 4  # Number of attention heads
ff_size = 1024  # Feed-forward layer size
dropout = 0.1  # Dropout rate

# Example training loop
input_text = &quot;Hello, how are you?&quot;
output_text = &quot;I am doing well, thank you.&quot;

# Tokenization using Transformers
tokenizer = BartTokenizer.from_pretrained('bart-large-cnn')
tokenizer.add_tokens([&quot;&lt;custom_token_1&gt;&quot;, &quot;&lt;custom_token_2&gt;&quot;])

# Tokenize input and output text
input_tokens = tokenizer.encode(input_text, add_special_tokens=True, truncation=True, padding=True)
output_tokens = tokenizer.encode(output_text, add_special_tokens=True, truncation=True, padding=True)

# Convert tokens to tensors
input_tensor = torch.tensor(input_tokens).unsqueeze(0)  # Add batch dimension
output_tensor = torch.tensor(output_tokens).unsqueeze(0)  # Add batch dimension

# Initialize the transformer model
model = Transformer(input_size, output_size, hidden_size, num_layers, num_heads, ff_size, dropout)

# Example training loop
num_epochs = 10
learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    # Perform forward pass
    output = model(input_tensor, output_tensor)
    
    # Compute loss
    loss = F.cross_entropy(output.view(-1, output_size), output_tensor.view(-1))
    
    # Backpropagation and optimization
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    # Print the loss for monitoring
    print(f&quot;Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}&quot;)

# Test the trained model with input text
test_input_text = &quot;How's the weather today?&quot;
test_input_tokens = tokenizer.encode(test_input_text, add_special_tokens=True, truncation=True, padding=True)
test_input_tensor = torch.tensor(test_input_tokens).unsqueeze(0)  # Add batch dimension

# Set the model to evaluation mode
model.eval()

# Perform forward pass with the test input
test_output = model(test_input_tensor, torch.zeros(1, 1).long())

# Get the predicted class
_, predicted_classes = torch.max(test_output, dim=2)
predicted_classes = predicted_classes.squeeze(0).tolist()

# Convert the predicted class tokens to text
predicted_text = tokenizer.decode(predicted_classes, skip_special_tokens=True)

# Print the predicted output text
print(f&quot;Input: {test_input_text}&quot;)
print(f&quot;Predicted Output: {predicted_text}&quot;)
</code></pre>
<p>The code imports the required libraries and defines a Transformer model with an Encoder and a Decoder. The Encoder and the Decoder consist of multiple transformer layers. The Transformer model is trained using the Adam optimizer and Cross-Entropy loss function. The trained model is then used to predict the output for a test input.</p>
<p>The code also uses the <code>BartTokenizer</code> from the transformers library to tokenize the input and output text.</p>
<h2>Errors</h2>
<p>The following errors are encountered while running the code:</p>
<pre><code>RuntimeError: shape '[1, 40, 64]' is invalid for input of size 2048

</code></pre>
","transformer-model"
"76299944","Machine translation transformer with context","2023-05-21 12:38:57","","0","252","<machine-learning><nlp><transformer-model><machine-translation>","<p>I'm working on a transformer for seq2seq translation that has the typical encoder-decoder structure. I want to include examples of similar sentences and their translations in the prompt (few-shot) but I could only find guides that just translate one sentence to another without other context. Are there any best practices for including examples as context for translation transformers? Should they go in the decoder or encoder?</p>
<p>I currently feed an input sentence <code>source_sentence</code> to the encoder and start generating the translation by feeding a start token <code>&lt;start&gt;</code> to the decoder until an end token <code>&lt;end&gt;</code> is produced.</p>
<p>Should my encoder input be something like this:</p>
<pre><code>example_sentence1
&lt;start&gt; example_translation1 &lt;end&gt;

example_sentence2
&lt;start&gt; example_translation2 &lt;end&gt;

source_sentence
</code></pre>
<p>Or alternatively should my decoder input be:</p>
<pre><code>example_sentence1
&lt;start&gt; example_translation1 &lt;end&gt;

example_sentence2
&lt;start&gt; example_translation2 &lt;end&gt;

source_sentence
&lt;start&gt;
</code></pre>
<p>Neither really feels right. Are there other options?</p>
","transformer-model"
"76269266","How can I compress pytorch transformer model?","2023-05-17 06:53:19","","0","313","<performance><pytorch><compression><transformer-model>","<p>I have a model that contains BERTtokenizer and some transformer blocks.
Is there a way to compress a transformer model or ...?
Our model looks like this:
GPT(
(transformer): ModuleDict(
(wte): Embedding(129600, 768)
(wpe): Embedding(1024, 768)
(drop): Dropout(p=0.0, inplace=False)
(h): ModuleList(
(0-11): 12 x Block(
(ln_1): LayerNorm()
(attn): CausalSelfAttention(
(c_attn): Linear(in_features=768, out_features=2304, bias=False)
(c_proj): Linear(in_features=768, out_features=768, bias=False)
(attn_dropout): Dropout(p=0.0, inplace=False)
(resid_dropout): Dropout(p=0.0, inplace=False)
)
(ln_2): LayerNorm()
(mlp): MLP(
(c_fc): Linear(in_features=768, out_features=3072, bias=False)
(c_proj): Linear(in_features=3072, out_features=768, bias=False)
(dropout): Dropout(p=0.0, inplace=False)
(gelu): GELU(approximate='none')
)
)
)
(ln_f): LayerNorm()
)
(lm_head): Linear(in_features=768, out_features=10048, bias=False)
)</p>
<p>I tried to prune it but it didn't work well.
I hope your detailed answer.</p>
","transformer-model"
"76244907","DeepSpeed config file not found","2023-05-13 21:15:41","","1","502","<huggingface-transformers><transformer-model>","<p>When running my DeepSpeed training script with the command deepspeed --num_gpus=8 client_entry.py --model_id google/flan-t5-xxl --dataset_path data --epochs 3 --per_device_train_batch_size 8 --per_device_eval_batch_size 8 --generation_max_length 129 --lr 1e-4 --deepspeed dsconfig.json, I encountered an assertion error with the message assert config != None, &quot;DeepSpeed requires --deepspeed_config to specify configuration file&quot;. What does this error message mean and how can I resolve i</p>
","transformer-model"
"76237292","My Transformer Encoder / Decoder has the same values for all time steps in eval with PyTorch","2023-05-12 14:26:07","","1","500","<python><pytorch><transformer-model>","<p>I have a model:</p>
<pre><code># model.py
import torch
import torch.nn as nn
import math


class TransformerAutoencoder(nn.Module):
    def __init__(self, d_model, nhead, num_layers, dim_feedforward, dropout=0.0):
        super(TransformerAutoencoder, self).__init__()

        self.encoder = nn.TransformerEncoder(
            encoder_layer=nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout),
            num_layers=num_layers,
        )
        self.relu = nn.ReLU()

        self.bottleneck = nn.Linear(d_model, d_model)

        self.decoder = nn.TransformerDecoder(
            decoder_layer=nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout),
            num_layers=num_layers
        )

        self.d_model = d_model

    def forward(self, src, tgt=None):
        num_time_frames = src.size(1)

        # Generate sinusoidal position embeddings
        position_embeddings_src = self._get_sinusoidal_position_embeddings(num_time_frames, self.d_model).to(src.device)

        # Add position embeddings to input
        src = src + position_embeddings_src

        src = src.transpose(0, 1)  # shape: (T, batch_size, n_mels)

        # Pass the input through the encoder
        memory = self.encoder(src).transpose(0, 1)  # shape: (batch_size, T, n_mels)
        memory = self.relu(memory)

        # Pass the output of the encoder through the bottleneck
        bottleneck = self.bottleneck(memory)  # shape: (batch_size, T, n_mels)
        bottleneck = self.relu(bottleneck)
        bottleneck = bottleneck.mean(dim=1)  # shape: (batch_size, n_mels)

        if tgt is not None:
            # In training mode, we have the target sequence
            # Prepend the bottleneck to the target sequence
            tgt = torch.cat((bottleneck.unsqueeze(1), tgt), dim=1)  # shape: (batch_size, T + 1, n_mels)

            # Generate position embeddings for the new target sequence
            position_embeddings_tgt = self._get_sinusoidal_position_embeddings(
                num_time_frames + 1, self.d_model).to(tgt.device)  # +1 to account for the bottleneck

            tgt = tgt + position_embeddings_tgt

            tgt = tgt.transpose(0, 1)  # shape: (T + 1, batch_size, n_mels)
            output = self.decoder(tgt, memory.transpose(0, 1))  # shape: (T + 1, batch_size, n_mels)

        else:
            # In inference mode, we generate the target sequence step by step
            output = self._generate_sequence(bottleneck, memory.transpose(0, 1), num_time_frames)

        # Transpose output back to (batch_size, T, n_mels)
        output = output.transpose(0, 1)

        return output

    def _generate_sequence(self, bottleneck, memory, max_length):
        # Initialize output with the bottleneck
        output = bottleneck.unsqueeze(0)  # shape: (1, batch_size, n_mels)
        print(&quot;output shape: &quot;, output.shape, output)
        print(&quot;memory shape: &quot;, memory.shape)
        for _ in range(max_length):
            output_step = self.decoder(output, memory)
            print(&quot;output_step shape: &quot;, output_step.shape, output_step)
            output = torch.cat((output, output_step[-1:, :, :]), dim=0)
        # Transpose output back to (batch_size, T, n_mels)
        print(&quot;output shape: &quot;, output.shape)
        return output

    def _get_sinusoidal_position_embeddings(self, num_positions, d_model):
        position_embeddings = torch.zeros(num_positions, d_model)
        positions = torch.arange(0, num_positions, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))

        position_embeddings[:, 0::2] = torch.sin(positions * div_term)
        position_embeddings[:, 1::2] = torch.cos(positions * div_term)
        position_embeddings = position_embeddings.unsqueeze(0)

        return position_embeddings
</code></pre>
<p>Forgetting the sequence generation part, when I run this in eval mode, all the time steps from the encoder are the same. What could I be missing?</p>
","transformer-model"
"76228968","With a PyTorch transformer, how do I account for batches for token-by-token generation?","2023-05-11 14:53:35","","1","305","<pytorch><transformer-model>","<pre><code># model.py
import torch
import torch.nn as nn
import math


class TransformerAutoencoder(nn.Module):
    def __init__(self, d_model, nhead, num_layers, dim_feedforward, bottleneck_size, dropout=0.5):
        super(TransformerAutoencoder, self).__init__()

        self.encoder = nn.TransformerEncoder(
            encoder_layer=nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout),
            num_layers=num_layers
        )

        self.decoder = nn.TransformerDecoder(
            decoder_layer=nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout),
            num_layers=num_layers
        )

        self.bottleneck = nn.Linear(d_model, bottleneck_size)
        self.bottleneck_expansion = nn.Linear(bottleneck_size, d_model)
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model
        self.relu = nn.ReLU()
        self.EOS_token = -1.0  # Define the EOS token as a constant

    def forward(self, src):
        num_time_frames = src.size(1)

        # Generate sinusoidal position embeddings
        position_embeddings = self._get_sinusoidal_position_embeddings(num_time_frames, self.d_model).to(src.device)

        # Add position embeddings to input, shape: (batch_size, num_time_frames, d_model)
        src = src + position_embeddings

        # Pass the input through the encoder, shape: (batch_size, num_time_frames, d_model)
        encoded = self.encoder(src)

        # Pass the encoded output through the bottleneck layer, shape: (batch_size, num_time_frames, bottleneck_size)
        bottleneck_output = self.bottleneck(encoded)
        bottleneck_output = self.dropout(bottleneck_output)

        # Expand the bottleneck output back to the original dimension, shape: (batch_size, num_time_frames, d_model)
        expanded = self.bottleneck_expansion(bottleneck_output)
        expanded = self.dropout(expanded)

        # Pass the expanded output through the decoder, shape: (batch_size, num_time_frames, d_model)
        if self.training:
            decoded = self.decoder(expanded, encoded)
        else:
            decoded = self._decode_token_by_token(expanded, encoded)

        # Apply the ReLU activation to the decoded output
        decoded = self.relu(decoded)

        return decoded, bottleneck_output

    def _decode_token_by_token(self, expanded, encoded):
        batch_size, num_time_frames, _ = expanded.size()
        decoded = torch.full_like(expanded, self.EOS_token)
        for t in range(num_time_frames):
            if t == 0:
                decoder_input = expanded[:, :1]
            else:
                decoder_input = torch.cat([expanded[:, :1], decoded[:, 1:t]], dim=1)
            decoded[:, t] = self.decoder(decoder_input, encoded)[:, t]
        return decoded

    def _get_sinusoidal_position_embeddings(self, num_positions, d_model):
        position_embeddings = torch.zeros(num_positions, d_model)
        positions = torch.arange(0, num_positions, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))

        position_embeddings[:, 0::2] = torch.sin(positions * div_term)
        position_embeddings[:, 1::2] = torch.cos(positions * div_term)
        position_embeddings = position_embeddings.unsqueeze(0)

        return position_embeddings
</code></pre>
<p>This is what I have. But there's a shape mismatch because <code>_decode_token_by_token</code> doesn't appear to iterate over the batch. How can I resolve this?</p>
","transformer-model"
"76226874","How can I use BERT to generate a long paragraph?","2023-05-11 10:57:59","","1","45","<lstm><huggingface-transformers><bert-language-model><transformer-model><mlp>","<p>I am trying to write an image-capturing model, use the CNN model to extract the image feature, and then connect with BERT and MLP to generate a long paragraph However, after training, my model result is really bad. In my MLP part, I should add LSTM into the different feature and then concatenate them, but I don't know where should I add it. Anyone can give me some advice? Thank you!</p>
<p>My BERT decoder part:</p>
<pre><code>class bert_decoder(tf.keras.layers.Layer):

def __init__(self, vocab_size, embed_dim, max_len):
    super().__init__()
    self.bert_model = TFBertModel.from_pretrained(&quot;bert-base-cased&quot;, trainable=False)
    self.ffn_layer1 =  keras.layers.Dense(1000)
    self.ffn_layer2 = keras.layers.Dense(EMBEDDING_DIM * (max_len - 1))
    self.flatten = keras.layers.Flatten()
    self.reshape = keras.layers.Reshape((max_len - 1, embed_dim))
def call(self, input_ids):

    bert_out = self.bert_model(input_ids)[0]
    bert_out_flatten = self.flatten(bert_out)
    bert_out = self.ffn_layer1(bert_out_flatten)
    bert_out = self.ffn_layer2(bert_out)
    bert_out = self.reshape(bert_out)
    return bert_out
</code></pre>
<p>My MLP part:</p>
<pre><code>class TransformerDecoderLayer(tf.keras.layers.Layer):

def __init__(self, embed_dim, units, num_heads):
    super().__init__()
    self.embedding = bert_decoder(
        tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)

    self.attention_1 = tf.keras.layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=embed_dim, dropout=0.1
    )
    self.attention_2 = tf.keras.layers.MultiHeadAttention(
        num_heads=num_heads, key_dim=embed_dim, dropout=0.1
    )

    self.layernorm_1 = tf.keras.layers.LayerNormalization()
    self.layernorm_2 = tf.keras.layers.LayerNormalization()
    self.layernorm_3 = tf.keras.layers.LayerNormalization()

    self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=&quot;relu&quot;)
    self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)

    self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=&quot;softmax&quot;)

    self.dropout_1 = tf.keras.layers.Dropout(0.3)
    self.dropout_2 = tf.keras.layers.Dropout(0.5)


def call(self, input_ids, encoder_output, training, mask=None):

    embeddings = self.embedding(input_ids)
    
    combined_mask = None
    padding_mask = None
    if mask is not None:
        causal_mask = self.get_causal_attention_mask(embeddings)
        padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
        combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
        combined_mask = tf.minimum(combined_mask, causal_mask)
    
    attn_output_1 = self.attention_1(
        query=embeddings,
        value=embeddings,
        key=embeddings,
        attention_mask=combined_mask,
        training=training
    )
    
    out_1 = self.layernorm_1(embeddings + attn_output_1)

    attn_output_2 = self.attention_2(
        query=out_1,
        value=encoder_output,
        key=encoder_output,
        attention_mask=padding_mask,
        training=training
    )

    out_2 = self.layernorm_2(out_1 + attn_output_2)

    ffn_out = self.ffn_layer_1(out_2)
    ffn_out = self.dropout_1(ffn_out, training=training)
    ffn_out = self.ffn_layer_2(ffn_out)

    ffn_out = self.layernorm_3(ffn_out + out_2)
    ffn_out = self.dropout_2(ffn_out, training=training)
    preds = self.out(ffn_out)
    return preds


def get_causal_attention_mask(self, inputs):
    input_shape = tf.shape(inputs)
    batch_size, sequence_length = input_shape[0], input_shape[1]
    i = tf.range(sequence_length)[:, tf.newaxis]
    j = tf.range(sequence_length)
    mask = tf.cast(i &gt;= j, dtype=&quot;int32&quot;)
    mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
    mult = tf.concat(
        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
        axis=0
    )
    return tf.tile(mask, mult)
</code></pre>
","transformer-model"
"76217513","Store intermediate values of pytorch module","2023-05-10 10:50:13","76218173","0","458","<pytorch><hook><transformer-model><self-attention>","<p>I try to plot attention maps for ViT. I know that I can do something like<br />
<code>h_attn = model.blocks[-1].attn.register_forward_hook(get_activations('attention'))</code>
to register a hook that camputres output of some <code>nn.module</code> in my model.<br />
The ViT's attention layer has the following forward structure:</p>
<pre><code>def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)
        
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        
        return x
</code></pre>
<p>Can I somehow attach the hook such that i get the <code>attn</code> value and not the return value of forward (e.g. by using some kind of dummy-module)?</p>
","transformer-model"
"76210061","all-MiniLM-L6-v2 no longer available, any ideas?","2023-05-09 13:47:49","","0","30","<nlp><huggingface-transformers><bert-language-model><transformer-model><topic-modeling>","<p>What happened to some sentence transformers on Hugging Face? Why is all-MiniLM-L6-v2 not available anymore? I want to use it in the BERTopic model but I cannot. Are there any alternative embedding techniques which do not require logining into the Hugging Face.</p>
<p>404 error page appears.</p>
<p><a href=""https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"" rel=""nofollow noreferrer"">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></p>
<p>Error: is not a valid model identifier listed on 'https://huggingface.co/models'</p>
<p>I tried to use it as the default embedding technique in BERTopic but it fails to run.</p>
","transformer-model"
"76197079","Unable to Run Simple Fine-Tuning of GPT-J Model","2023-05-08 02:00:07","","0","540","<python><data-science><artificial-intelligence><huggingface-transformers><transformer-model>","<p>I have been trying to learn to fine tune Large Language Models (LLM), specifically GPT-J from HuggingFace.  I have been struggling to make even simple examples work on custom datasets.  I wanted to train the model to produce google reviews, and was hoping to train it on some publicly available data from here: <a href=""https://jiachengli1995.github.io/google/index.html"" rel=""nofollow noreferrer"">https://jiachengli1995.github.io/google/index.html</a></p>
<p>I cleaned the dataset using pandas, just by reading in the JSON file data.  I removed most of the columns except the text and the humans rating score, and then added the standard GPT &lt;|endoftext|&gt;, so the format in the final output file should have each row look like:</p>
<p><code>&lt;|endoftext|&gt;3: This reestruarunt was pretty OK, middle of the pack&lt;|endoftext|&gt;</code></p>
<p>I did this for 32000 lines and exported it to a .csv file for use as training data, the .csv file is available here: <a href=""https://collidetech-my.sharepoint.com/:x:/p/fred/ERGBiYyLz_JJttt77vk3jp4BpCtgWYuqahC7QZet40P1Yw?e=8pV2dx"" rel=""nofollow noreferrer"">Link to CSV File</a></p>
<p>Finally, I used some tutorial code from HuggingFace to try and build a simple training pipeline in a jupyter notebook.  I have been running the notebook in a Docker container that I SSH into and it appears to run fine in that environment, but when I try and run the final block of code</p>
<pre><code>trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset
)

print(&quot;Starting training&quot;)
trainer.train()
print(&quot;Finished Fine Tuning&quot;)
</code></pre>
<p>I get an error message:
<code>ValueError: The model did not return a loss from the inputs, only the following keys: logits,past_key_values. For reference, the inputs it received are input_ids,attention_mask.</code></p>
<p>This seems to be related to a similar stackoverflow issue: <a href=""https://stackoverflow.com/questions/74014379/how-to-fine-tune-gpt-j-using-huggingface-trainer"">How to fine-tune gpt-j using Huggingface Trainer</a> , and I have tried to modify my code to fit their solution, but the issue does not seem to resolve with the use of the DataCollator function from HuggingFace.  Any help would be much appreciated.</p>
<p>Please find the full code below:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import Trainer, TrainingArguments, AutoModelForCausalLM
from transformers import GPTJForCausalLM, AutoTokenizer
from datasets import load_dataset
import time
import torch
import os
import numpy as np
import evaluate
import transformers

if torch.cuda.is_available():
    print(&quot;Using CUDA!!!&quot;)
    model = GPTJForCausalLM.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;, torch_dtype=torch.float16).cuda()
else:
    print(&quot;OOPS, NO CUDA&quot;)
    model = GPTJForCausalLM.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;, torch_dtype=torch.float16)

tokenizer = AutoTokenizer.from_pretrained(&quot;EleutherAI/gpt-j-6B&quot;)

#functions
def tokenize_function(examples):
    current_tokenizer_result = tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)
    return current_tokenizer_result

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

#Add a file for saving the model weights
tuned_gptj = &quot;./tuned_models/gpt-j-6B&quot;

#Load the dataset, using the load_dataset function
current_dataset = load_dataset(&quot;csv&quot;, data_files=&quot;train.csv&quot;)

model.config.pad_token_id = model.config.eos_token_id
tokenizer.pad_token = tokenizer.eos_token

#tokenize the dataset
tokenized_datasets = current_dataset.map(tokenize_function, batched=True)
small_train_dataset = tokenized_datasets[&quot;train&quot;].select(range(24000))

training_args = TrainingArguments(output_dir=tuned_gptj,
                                 report_to='all',
                                 logging_dir='./logs',
                                 per_device_train_batch_size=1,
                                 label_names=['input_ids','attention_mask'],
                                 num_train_epochs=1,
                                 no_cuda=False,
                                 deepspeed='ds_config_stage1.json'
                                 )
metric = evaluate.load(&quot;accuracy&quot;)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset
)

print(&quot;Starting training&quot;)
trainer.train()
print(f&quot;Finished fine-tuning in {time.time() - start}&quot;)



</code></pre>
","transformer-model"
"76166531","ERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found","2023-05-03 17:21:44","","7","25313","<python><google-colaboratory><huggingface-transformers><bert-language-model><transformer-model>","<p><a href=""https://colab.research.google.com/drive/11u6leEKvqE0CCbvDHHKmCxmW5GxyjlBm?usp=sharing"" rel=""noreferrer"">https://colab.research.google.com/drive/11u6leEKvqE0CCbvDHHKmCxmW5GxyjlBm?usp=sharing</a></p>
<p>setup.py file is in transformers folder(root directory). But this error occurs when I run</p>
<pre><code>!git clone https://github.com/huggingface/transformers
!cd transformers
!pip install -e .
!pip install -r transformers/examples/pytorch/translation/requirements.txt
</code></pre>
<p>Can anyone tell me why is this happening?</p>
","transformer-model"
"76159926","Using RoBERTa-base for QA model outputs the context not an answer","2023-05-03 01:43:27","","2","2049","<python><huggingface-transformers><transformer-model><roberta-language-model>","<p>I'm trying to use this model from <a href=""https://huggingface.co/deepset/roberta-base-squad2"" rel=""nofollow noreferrer"">deepset/roberta-base-squad2</a> to essentially go through a column of work related activities and have it answer the question what are the necessary skills for this job ? However the model is simply handing me back my context or my question+context. I'm not quite sure why it's doing that.</p>
<p>Here's what I'm running,</p>
<pre><code>import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering

# Your DataFrame loading code here
# df = pd.read_csv(&quot;your_data.csv&quot;)

def generate_skills(question, context):
    tokenizer = AutoTokenizer.from_pretrained(&quot;deepset/roberta-base-squad2&quot;)
    model = AutoModelForQuestionAnswering.from_pretrained(&quot;deepset/roberta-base-squad2&quot;)

    inputs = tokenizer(question, context, return_tensors='pt')
    outputs = model(**inputs)

    start_scores = outputs.start_logits
    end_scores = outputs.end_logits

    start_index = torch.argmax(start_scores)
    end_index = torch.argmax(end_scores) + 1

    tokens = inputs['input_ids'][0][start_index:end_index]
    answer = tokenizer.decode(tokens, skip_special_tokens=True)

    return answer

def generate_skills_for_row(row):
    context = row['top_words']
    question = &quot;What are the necessary skills a data scientist should have?&quot;
    skills = generate_skills(question, context)
    return skills

# Create a new column 'skills' based on the 'top_words' column
df['skills'] = df.apply(generate_skills_for_row, axis=1)
</code></pre>
","transformer-model"
"76147663","Getting error when installing basic requirements of transformers notebooks","2023-05-01 14:19:29","","0","268","<python><nlp><google-colaboratory><huggingface-transformers><transformer-model>","<p>I am working on a project and require some installation from transformers notebook this is the code which I ran google colab</p>
<pre><code>!git clone https://github.com/nlp-with-transformers/notebooks.git

import os
os.chdir(&quot;/content/notebooks&quot;)


from install import *
install_requirements()
</code></pre>
<p>Below is the error:-</p>
<pre><code>remote: Enumerating objects: 515, done.
remote: Counting objects: 100% (161/161), done.
remote: Compressing objects: 100% (39/39), done.
remote: Total 515 (delta 139), reused 126 (delta 122), pack-reused 354
Receiving objects: 100% (515/515), 28.61 MiB | 16.21 MiB/s, done.
Resolving deltas: 100% (246/246), done.
⏳ Installing base requirements ...
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
&lt;ipython-input-1-926abb3f024d&gt; in &lt;cell line: 10&gt;()
      8 
      9 from install import *
---&gt; 10 install_requirements()

/content/notebooks/install.py in install_requirements(is_chapter2, is_chapter6, is_chapter7, is_chapter7_v2, is_chapter10, is_chapter11)
     29     process_install = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
     30     if process_install.returncode != 0:
---&gt; 31         raise Exception(&quot;😭 Failed to install base requirements&quot;)
     32     else:
     33         print(&quot;✅ Base requirements installed!&quot;)

Exception: 😭 Failed to install base requirements
</code></pre>
<p>I have tried to separate the code lines but not able to figure it out.</p>
","transformer-model"
"76138944","Loading image captioning (Transformer) model","2023-04-29 23:38:52","","0","124","<nlp><transformer-model>","<p>Hi guys i am using an image captioning model  with transformer and I am having trouble loading the weights of the model in TensorFlow with the commande :<code> model.load_weights('/image_captioning_weights.h5')</code>.
I got this error:</p>
<pre><code>ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.
</code></pre>
<p>Here is how i implemented the model and train it and then saved the weights:</p>
<pre><code>MAX_LENGTH = 40
VOCABULARY_SIZE = 10000
BATCH_SIZE = 64
BUFFER_SIZE = 1000
EMBEDDING_DIM = 512
UNITS = 512

image_augmentation = tf.keras.Sequential(
    [
        tf.keras.layers.RandomFlip(&quot;horizontal&quot;),
        tf.keras.layers.RandomRotation(0.2),
        tf.keras.layers.RandomContrast(0.3),
    ]
)

def CNN_Encoder():
    inception_v3 = tf.keras.applications.InceptionV3(
        include_top=False,
        weights='imagenet'
    )
    inception_v3.trainable = False

    output = inception_v3.output
    output = tf.keras.layers.Reshape(
        (-1, output.shape[-1]))(output)

    cnn_model = tf.keras.models.Model(inception_v3.input, output)
    return cnn_model

class TransformerEncoderLayer(tf.keras.layers.Layer):

    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.layer_norm_1 = tf.keras.layers.LayerNormalization()
        self.layer_norm_2 = tf.keras.layers.LayerNormalization()
        self.attention = tf.keras.layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim)
        self.dense = tf.keras.layers.Dense(embed_dim, activation=&quot;relu&quot;)
    

    def call(self, x, training):
        x = self.layer_norm_1(x)
        x = self.dense(x)

        attn_output = self.attention(
            query=x,
            value=x,
            key=x,
            attention_mask=None,
            training=training
        )

        x = self.layer_norm_2(x + attn_output)
        return x

class Embeddings(tf.keras.layers.Layer):

    def __init__(self, vocab_size, embed_dim, max_len):
        super().__init__()
        self.token_embeddings = tf.keras.layers.Embedding(
            vocab_size, embed_dim)
        self.position_embeddings = tf.keras.layers.Embedding(
            max_len, embed_dim, input_shape=(None, max_len))
    

    def call(self, input_ids):
        length = tf.shape(input_ids)[-1]
        position_ids = tf.range(start=0, limit=length, delta=1)
        position_ids = tf.expand_dims(position_ids, axis=0)

        token_embeddings = self.token_embeddings(input_ids)
        position_embeddings = self.position_embeddings(position_ids)

        return token_embeddings + position_embeddings

class TransformerDecoderLayer(tf.keras.layers.Layer):

    def __init__(self, embed_dim, units, num_heads):
        super().__init__()
        self.embedding = Embeddings(
            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)

        self.attention_1 = tf.keras.layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.1
        )
        self.attention_2 = tf.keras.layers.MultiHeadAttention(
            num_heads=num_heads, key_dim=embed_dim, dropout=0.1
        )

        self.layernorm_1 = tf.keras.layers.LayerNormalization()
        self.layernorm_2 = tf.keras.layers.LayerNormalization()
        self.layernorm_3 = tf.keras.layers.LayerNormalization()

        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=&quot;relu&quot;)
        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)

        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=&quot;softmax&quot;)

        self.dropout_1 = tf.keras.layers.Dropout(0.3)
        self.dropout_2 = tf.keras.layers.Dropout(0.5)
    

    def call(self, input_ids, encoder_output, training, mask=None):
        embeddings = self.embedding(input_ids)

        combined_mask = None
        padding_mask = None
        
        if mask is not None:
            causal_mask = self.get_causal_attention_mask(embeddings)
            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)
            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)
            combined_mask = tf.minimum(combined_mask, causal_mask)

        attn_output_1 = self.attention_1(
            query=embeddings,
            value=embeddings,
            key=embeddings,
            attention_mask=combined_mask,
            training=training
        )

        out_1 = self.layernorm_1(embeddings + attn_output_1)

        attn_output_2 = self.attention_2(
            query=out_1,
            value=encoder_output,
            key=encoder_output,
            attention_mask=padding_mask,
            training=training
        )

        out_2 = self.layernorm_2(out_1 + attn_output_2)

        ffn_out = self.ffn_layer_1(out_2)
        ffn_out = self.dropout_1(ffn_out, training=training)
        ffn_out = self.ffn_layer_2(ffn_out)

        ffn_out = self.layernorm_3(ffn_out + out_2)
        ffn_out = self.dropout_2(ffn_out, training=training)
        preds = self.out(ffn_out)
        return preds


    def get_causal_attention_mask(self, inputs):
        input_shape = tf.shape(inputs)
        batch_size, sequence_length = input_shape[0], input_shape[1]
        i = tf.range(sequence_length)[:, tf.newaxis]
        j = tf.range(sequence_length)
        mask = tf.cast(i &gt;= j, dtype=&quot;int32&quot;)
        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))
        mult = tf.concat(
            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],
            axis=0
        )
        return tf.tile(mask, mult)

class ImageCaptioningModel(tf.keras.Model):

    def __init__(self, cnn_model, encoder, decoder, image_aug=None):
        super().__init__()
        self.cnn_model = cnn_model
        self.encoder = encoder
        self.decoder = decoder
        self.image_aug = image_aug
        self.loss_tracker = tf.keras.metrics.Mean(name=&quot;loss&quot;)
        self.acc_tracker = tf.keras.metrics.Mean(name=&quot;accuracy&quot;)

    def calculate_loss(self, y_true, y_pred, mask):
        loss = self.loss(y_true, y_pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)


    def calculate_accuracy(self, y_true, y_pred, mask):
        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))
        accuracy = tf.math.logical_and(mask, accuracy)
        accuracy = tf.cast(accuracy, dtype=tf.float32)
        mask = tf.cast(mask, dtype=tf.float32)
        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)
    

    def compute_loss_and_acc(self, img_embed, captions, training=True):
        encoder_output = self.encoder(img_embed, training=True)
        y_input = captions[:, :-1]
        y_true = captions[:, 1:]
        mask = (y_true != 0)
        y_pred = self.decoder(
            y_input, encoder_output, training=True, mask=mask
        )
        loss = self.calculate_loss(y_true, y_pred, mask)
        acc = self.calculate_accuracy(y_true, y_pred, mask)
        return loss, acc

    
    def train_step(self, batch):
        imgs, captions = batch

        if self.image_aug:
            imgs = self.image_aug(imgs)
        
        img_embed = self.cnn_model(imgs)

        with tf.GradientTape() as tape:
            loss, acc = self.compute_loss_and_acc(
                img_embed, captions
            )
    
        train_vars = (
            self.encoder.trainable_variables + self.decoder.trainable_variables
        )
        grads = tape.gradient(loss, train_vars)
        self.optimizer.apply_gradients(zip(grads, train_vars))
        self.loss_tracker.update_state(loss)
        self.acc_tracker.update_state(acc)

        return {&quot;loss&quot;: self.loss_tracker.result(), &quot;acc&quot;: self.acc_tracker.result()}
    

    def test_step(self, batch):
        imgs, captions = batch

        img_embed = self.cnn_model(imgs)

        loss, acc = self.compute_loss_and_acc(
            img_embed, captions, training=False
        )

        self.loss_tracker.update_state(loss)
        self.acc_tracker.update_state(acc)

        return {&quot;loss&quot;: self.loss_tracker.result(), &quot;acc&quot;: self.acc_tracker.result()}

    @property
    def metrics(self):
        return [self.loss_tracker, self.acc_tracker]

encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)
decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)

cnn_model = CNN_Encoder()
caption_model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)

cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False, reduction=&quot;none&quot;
)

early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)

caption_model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=cross_entropy
)

history = caption_model.fit(
    train_dataset,
    epochs=10,
    validation_data=val_dataset
)

caption_model.save_weights('imagee_captioning_weights.h5')

</code></pre>
<p>After i saved the model weights i created a instance of the model and tried to load the model where i got this error:</p>
<pre><code>
model = ImageCaptioningModel(
    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,
)
model.compile(
    optimizer=tf.keras.optimizers.Adam(),
    loss=cross_entropy
)
model.load_weights('/imagee_captioning_weights.h5')

</code></pre>
<p>the output:</p>
<p><code>ValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.</code></p>
","transformer-model"
"76078725","How to make transformer encoder and decoder model accept input size of (batch_size, sequence_length)?","2023-04-22 08:35:16","","2","235","<code-generation><transformer-model>","<p>I'm new to ML and i'm trying to make a encoder-decoder model to generate emmet code from screenshot. I have made a dataset consisting of screenshots and its corresponding emmet code(it's some kind of abbreviations of html code). I use a swinTransformer to extract image features from image, and then i have an encoder input of (32, 512)(which is (batch_size, sequnce_length). But i've learnt that the transformer encoder expects an input size of (batch_size, sequnce_length, embeddings). Did i do sth. wrong with the extracting features step or is it possible to modify the transformer encoder to accept my input? Please help me understand this, thank you very much!
My code looks like this：</p>
<pre><code>import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from build_dataset import EmmetDataset 
from swin_transformer_pytorch import SwinTransformer
from transformer_encoder import TransformerEncoder


STModel = SwinTransformer(
    hidden_dim=96,
    layers=(2, 2, 6, 2),
    heads=(3, 6, 12, 24),
    channels=3,
    num_classes=512,
    head_dim=32,
    window_size=4,
    downscaling_factors=(4, 2, 2, 2),
    relative_pos_embedding=True
)
encoder = TransformerEncoder(d_model=512, num_heads=8, num_layers=6)

train_dataset = EmmetDataset('train')
val_dataset = EmmetDataset('val')
test_dataset = EmmetDataset('test')

train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)
num_epochs = 10

for epoch in range(num_epochs):
    for i, (screenshot_tensor, serialized_code_tensor) in enumerate(train_dataloader):
        print(screenshot_tensor.shape) # [32, 3, 768, 768]
        print(serialized_code_tensor.shape) # [32, 512]
        # swinTransformer to extract features
        features = STModel(screenshot_tensor)
        print(features.shape) # [32, 512]
        # Encoder-Decoder
        encoder_output = encoder(features) # the encode expects an input of (batch_size, sequnce_length, embeddings), but i only got an input of (batch_size, sequnce_length)
        print(encoder_output)
        # ... ...
</code></pre>
<p>And my model looks like this:
<a href=""https://i.sstatic.net/bvyBO.png"" rel=""nofollow noreferrer"">my model</a></p>
","transformer-model"
"76077292","Missing Covariates in Darts TimeSeries","2023-04-21 23:24:39","","2","1181","<python><neural-network><time-series><recurrent-neural-network><transformer-model>","<p>I'm putting together a TFT model in the darts library in python and I keep getting the same error.  Here's roughly what I have:</p>
<pre><code>import numpy as np
import pandas as pd
from datetime import timedelta
from tqdm import tqdm_notebook as tqdm

import matplotlib.pyplot as plt

from darts import TimeSeries, concatenate
from darts.dataprocessing.transformers import Scaler
from darts.models import TFTModel
from darts.metrics import mape
from darts.utils.statistics import check_seasonality, plot_acf
from darts.utils.timeseries_generation import datetime_attribute_timeseries
from darts.utils.likelihood_models import QuantileRegression

f_columns = ['Year', 'Month', 'Week', 'DayOfMonth', 'DayOfWeek', 'WeekOfMonth', 'Season', 'Holidays', 'Weekend']
QUANTILES = [0.01, 0.05, 0.1, 0.2, 0.25, 0.5, 0.75, 0.8, 0.9, 0.95, 0.99]
ICL = 60
OCL = 15

time_series = TimeSeries.from_dataframe(data, time_col='CreateDate', value_cols=f_columns + ['Count'], freq = 'D')
train, test = time_series.split_after(SPLIT)

scaler = Scaler()
train_data_scaled = scaler.fit_transform(train)
test_data_scaled = scaler.transform(test)

model = TFTModel(
      input_chunk_length=ICL,
      output_chunk_length=OCL,
      hidden_size=16,
      lstm_layers=2,
      num_attention_heads=1,
      dropout=0.1,
      batch_size=16,
      n_epochs=2,
      likelihood = QuantileRegression(quantiles=QUANTILES),
      )

model.fit(series = train_data_scaled['Count'],
      past_covariates = train_data_scaled[f_columns],
      future_covariates = test_data_scaled[f_columns],
      verbose=True
     )

preds = model.predict(test_data_scaled)
preds = scaler.inverse_transform(preds)
</code></pre>
<p>When I run this I consistently get the error</p>
<pre><code>Missing covariates; could not find past covariates in index value range: 2020-08-22 00:00:00 - 2020-10-20 00:00:00
</code></pre>
<p>However any time I change the input or output chunk lengths these dates change accordingly.  Whenever I look at the data though, there is valid data in whatever time window it claims there are no past covariates.  I'm really unsure what the problem is here.  Let me know how I can amend this issue and get my model working.</p>
<p>This was also a problem someone posted on <a href=""https://github.com/unit8co/darts/issues/1086"" rel=""nofollow noreferrer"">Github</a> but they didn't post a definitive answer and the OP's explanation of how they resolved the issue wasn't detailed enough.</p>
","transformer-model"
"76066313","Fine-tuned MLM based RoBERTa not improving performance","2023-04-20 16:22:58","","1","731","<nlp><huggingface-transformers><bert-language-model><transformer-model><roberta-language-model>","<p>We have lots of domain-specific data (200M+ data points, each document having ~100 to ~500 words) and we wanted to have a domain-specific LM.</p>
<p>We took some sample data points (2M+) &amp; fine-tuned RoBERTa-base (using HF-Transformer) using the Mask Language Modelling (MLM) task.</p>
<p>So far,</p>
<ol>
<li>we did 4-5 epochs (512 sequence length, batch-size=48)</li>
<li>used cosine learning rate scheduler (2-3 cycles/epochs)</li>
<li>We used dynamin masking (masked 15% tokens)</li>
</ol>
<p>Since the RoBERTa model is finetuned on domain-specific data, we do expect this model to perform better than the pre-trained-RoBERTa which is trained on general texts (wiki data, books, etc)</p>
<p>We did perform some tasks like Named Entity Recognition (NER), Text Classification, and Embedding generation to perform cosine similarity tasks. We did this on both finetuned domain-specific RoBERTa and pre-trained-RoBERTa.</p>
<p>Surprisingly, the results are the same (very small difference) for both models. We did try Spacy models too, but the results are the same.</p>
<p>Perplexity scores indicate that finetuned MLM-based RoBERTa has a minimal loss.</p>
<ol>
<li>Can anyone please help us understand why MLM based model is NOT performing better?</li>
<li>Should we go for more data OR more epochs OR both, to see some effect?</li>
<li>are we doing anything wrong here? Let me know if any required details are missing. I will update</li>
</ol>
<p>any suggestions OR any valuable links addressing these concerns would be really helpful</p>
<p>Huggingface discussion page: <a href=""https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913</a></p>
","transformer-model"
"76034630","How do I send an attention-mask ""Mask"" matrix in transformer encoder along with my latent in pytorch's nn.TransformerEncoder?","2023-04-17 11:24:23","","0","2282","<python><machine-learning><deep-learning><pytorch><transformer-model>","<p>I have a transformer_encoder initialized in pytorch as:</p>
<pre><code>transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads), num_layers=num_layers)
</code></pre>
<p>Now I have a latent, which is of shape [8, 320, 512], where 8 is the batch_size, 320 is the sequence length, and 512 is the embedding dimension.</p>
<p>I also have an attention mask of shape [320, 320].</p>
<p>And I want to pass it into transformer encoder like:</p>
<pre><code>latent = torch.rand(8, 320, 512)
mask = torch.rand(320, 320)

output = transformer_encoder(latent, mask)
</code></pre>
<p>But it is giving me error:<br>
&quot;The shape of the 2D attn_mask is torch.Size([320, 320]), but should be (8, 8).&quot;<br><br>
Now I repeated the matrix along batch dimension creating 8 of those (320, 320) matrices, making the shape of 'mask', [8, 320, 320]. When I ran again the above code</p>
<pre><code>latent.shape = [8, 320, 512]
mask.shape = [8, 320, 320]

output = transformer_encoder(latent, mask)
</code></pre>
<p>It gives me this error:<br>
&quot;The shape of the 3D attn_mask is torch.Size([8, 320, 320]), but should be (2560, 8, 8).&quot;</p>
<p>Can someone explain what is the problem here? And how can I solve it? Or the correct way to implement it?</p>
","transformer-model"
"76026095","can not get correct result by pytorch on my MacBook Pro M1","2023-04-16 05:48:44","","0","197","<macos><pytorch><apple-m1><transformer-model><cs231n>","<p>I am learning Transformer_Captioning.ipynb of cs231n assignment3.
According to its guide, I need to run the cell which can test my MultiHeadAttention implementation, and I need to get the relative error should be less than e-3.
After I have finished my code, I got incorrect result:</p>
<pre><code>self_attn_output error:  0.449382070034207
masked_self_attn_output error:  1.0
attn_output error:  1.0
</code></pre>
<p>I even copied other people's code on gtihub which can get correct result, and, I still the same output:</p>
<pre><code>self_attn_output error:  0.449382070034207
masked_self_attn_output error:  1.0
attn_output error:  1.0
</code></pre>
<p>Is there anything else I missed?</p>
<p>My Class：</p>
<pre><code>import torch
import torch.nn as nn
from torch.nn import functional as F
import math

class MultiHeadAttention(nn.Module):
    &quot;&quot;&quot;
    A model layer which implements a simplified version of masked attention, as
    introduced by &quot;Attention Is All You Need&quot; (https://arxiv.org/abs/1706.03762).

    Usage:
      attn = MultiHeadAttention(embed_dim, num_heads=2)

      # self-attention
      data = torch.randn(batch_size, sequence_length, embed_dim)
      self_attn_output = attn(query=data, key=data, value=data)

      # attention using two inputs
      other_data = torch.randn(batch_size, sequence_length, embed_dim)
      attn_output = attn(query=data, key=other_data, value=other_data)
    &quot;&quot;&quot;

    def __init__(self, embed_dim, num_heads, dropout=0.1):
        &quot;&quot;&quot;
        Construct a new MultiHeadAttention layer.

        Inputs:
         - embed_dim: Dimension of the token embedding
         - num_heads: Number of attention heads
         - dropout: Dropout probability
        &quot;&quot;&quot;
        super().__init__()
        assert embed_dim % num_heads == 0

        # We will initialize these layers for you, since swapping the ordering
        # would affect the random number generation (and therefore your exact
        # outputs relative to the autograder). Note that the layers use a bias
        # term, but this isn't strictly necessary (and varies by
        # implementation).
        self.key = nn.Linear(embed_dim, embed_dim)
        self.query = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        self.proj = nn.Linear(embed_dim, embed_dim)

        self.attn_drop = nn.Dropout(dropout)

        self.n_head = num_heads
        self.emd_dim = embed_dim
        self.head_dim = self.emd_dim // self.n_head

    def forward(self, query, key, value, attn_mask=None):
        &quot;&quot;&quot;
        Calculate the masked attention output for the provided data, computing
        all attention heads in parallel.

        In the shape definitions below, N is the batch size, S is the source
        sequence length, T is the target sequence length, and E is the embedding
        dimension.

        Inputs:
        - query: Input data to be used as the query, of shape (N, S, E)
        - key: Input data to be used as the key, of shape (N, T, E)
        - value: Input data to be used as the value, of shape (N, T, E)
        - attn_mask: Array of shape (S, T) where mask[i,j] == 0 indicates token
          i in the source should not influence token j in the target.

        Returns:
        - output: Tensor of shape (N, S, E) giving the weighted combination of
          data in value according to the attention weights calculated using key
          and query.
        &quot;&quot;&quot;
        N, S, E = query.shape
        N, T, E = value.shape
        # Create a placeholder, to be overwritten by your code below.
        output = torch.empty((N, S, E))
        ############################################################################
        # TODO: Implement multiheaded attention using the equations given in       #
        # Transformer_Captioning.ipynb.                                            #
        # A few hints:                                                             #
        #  1) You'll want to split your shape from (N, T, E) into (N, T, H, E/H),  #
        #     where H is the number of heads.                                      #
        #  2) The function torch.matmul allows you to do a batched matrix multiply.#
        #     For example, you can do (N, H, T, E/H) by (N, H, E/H, T) to yield a  #
        #     shape (N, H, T, T). For more examples, see                           #
        #     https://pytorch.org/docs/stable/generated/torch.matmul.html          #
        #  3) For applying attn_mask, think how the scores should be modified to   #
        #     prevent a value from influencing output. Specifically, the PyTorch   #
        #     function masked_fill may come in handy.                              #
        ############################################################################
        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****

        H = self.n_head

        # Compute key, query and value matrices from sequences
        K = self.key(key).view(N, T, H, E // H).moveaxis(1, 2)
        Q = self.query(query).view(N, S, H, E // H).moveaxis(1, 2)
        V = self.value(value).view(N, T, H, E // H).moveaxis(1, 2)

        Y = Q @ K.transpose(2, 3) / math.sqrt(self.head_dim)

        if attn_mask is not None:
            Y = Y.masked_fill(attn_mask == 0, float(&quot;-inf&quot;))

        Y = self.attn_drop(F.softmax(Y, dim=-1)) @ V
        output = self.proj(Y.moveaxis(1, 2).reshape(N, S, E))

        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****
        ############################################################################
        #                             END OF YOUR CODE                             #
        ############################################################################
        return output
</code></pre>
<p>My test code:</p>
<pre><code>import torch
if torch.backends.mps.is_available():
    mps_device = torch.device(&quot;mps&quot;)
    x = torch.ones(1, device=mps_device)
    print (x)
else:
    print (&quot;MPS device not found.&quot;)

torch.manual_seed(231)

# Choose dimensions such that they are all unique for easier debugging:
# Specifically, the following values correspond to N=1, H=2, T=3, E//H=4, and E=8.
batch_size = 1
sequence_length = 3
embed_dim = 8
attn = MultiHeadAttention(embed_dim, num_heads=2)

# Self-attention.
data = torch.randn(batch_size, sequence_length, embed_dim)
self_attn_output = attn(query=data, key=data, value=data)

# Masked self-attention.
mask = torch.randn(sequence_length, sequence_length) &lt; 0.5
masked_self_attn_output = attn(query=data, key=data, value=data, attn_mask=mask)

# Attention using two inputs.
other_data = torch.randn(batch_size, sequence_length, embed_dim)
attn_output = attn(query=data, key=other_data, value=other_data)

expected_self_attn_output = np.asarray([[
[-0.2494,  0.1396,  0.4323, -0.2411, -0.1547,  0.2329, -0.1936,
          -0.1444],
         [-0.1997,  0.1746,  0.7377, -0.3549, -0.2657,  0.2693, -0.2541,
          -0.2476],
         [-0.0625,  0.1503,  0.7572, -0.3974, -0.1681,  0.2168, -0.2478,
          -0.3038]]])

expected_masked_self_attn_output = np.asarray([[
[-0.1347,  0.1934,  0.8628, -0.4903, -0.2614,  0.2798, -0.2586,
          -0.3019],
         [-0.1013,  0.3111,  0.5783, -0.3248, -0.3842,  0.1482, -0.3628,
          -0.1496],
         [-0.2071,  0.1669,  0.7097, -0.3152, -0.3136,  0.2520, -0.2774,
          -0.2208]]])

expected_attn_output = np.asarray([[
[-0.1980,  0.4083,  0.1968, -0.3477,  0.0321,  0.4258, -0.8972,
          -0.2744],
         [-0.1603,  0.4155,  0.2295, -0.3485, -0.0341,  0.3929, -0.8248,
          -0.2767],
         [-0.0908,  0.4113,  0.3017, -0.3539, -0.1020,  0.3784, -0.7189,
          -0.2912]]])

print('self_attn_output error: ', rel_error(expected_self_attn_output, self_attn_output.detach().numpy()))
print('masked_self_attn_output error: ', rel_error(expected_masked_self_attn_output, masked_self_attn_output.detach().numpy()))
print('attn_output error: ', rel_error(expected_attn_output, attn_output.detach().numpy()))
</code></pre>
<p>My configuration:
MacBook Pro M1、Python 3.8.12、conda 4.11.0、torch 2.1.0.dev20230415、macOS Monterey Version 12.5</p>
","transformer-model"
"76019929","pytorch transformer with different dimension of encoder output and decoder memory","2023-04-15 01:20:16","","2","1198","<python><machine-learning><deep-learning><pytorch><transformer-model>","<p>The <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"" rel=""nofollow noreferrer"">Pytorch Transformer</a> takes in a <code>d_model</code> argument</p>
<p>They say <a href=""https://discuss.pytorch.org/t/using-different-feature-size-between-source-and-target-nn-transformer/139525/2?u=noam_salomonski"" rel=""nofollow noreferrer"">in the forums</a> that</p>
<blockquote>
<p>the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">transformer model</a> is not based on encoder and decoder having
different output features</p>
</blockquote>
<p>That is correct, but shouldn't limit the Pytorch implementation to be more generic. Indeed, in the paper all data flows with the same dimension == <code>d_model</code>, but this shouldn't be a theoretical limitation.</p>
<p><strong>I am looking for the reason why Pytorch's transformer isn't generic in this regard, as I am sure there is a good reason</strong></p>
<hr />
<h2>My attempt at understanding this</h2>
<p><a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"" rel=""nofollow noreferrer"">Multi-Head Attention</a> takes in <code>query</code>, <code>key</code> and <code>value</code> matrices which are of orthogonal dimensions.<br />
To mu understanding, that fact alone should allow the transformer model to have one output size for the encoder (the size of its input, due to skip connections) and another for the decoder's input (and output due to skip connections).</p>
<p>Now, looking at the Multi-Head Attention layer in the decoder which takes in Q from the decoder, and K, V from the encoder. I fail to see why K, V can't be of different dimension than Q, even with the skip connection. We could just set <code>d_Q==d_decoder==layer_output_dim</code> and <code>d_K==d_V==encoder_output_dim</code>, and everything would still work, because Multi-Head Attention should be able to take care of the different embedding sizes.</p>
<p><strong>What am I missing, or, how to write a more generic transformer, without breaking Pytorch completely and writing ot all from scratch?</strong></p>
<p><a href=""https://i.sstatic.net/Usett.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Usett.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"75973117","RuntimeError: The size of tensor a (2) must match the size of tensor b (8) at non-singleton dimension 1","2023-04-09 22:04:57","","0","484","<python><pytorch><transformer-model>","<p>I'm trying to build a transformer with positional encoder to create a regression model using PyTorch using the following code:</p>
<pre><code>import math
import torch.nn as nn

class Transformer(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, max_seq_len):
        super(Transformer, self).__init__()
        self.embedding = nn.Linear(input_dim, hidden_dim)
        self.pos_encoding = PositionalEncoding(hidden_dim, max_seq_len)
        self.encoder_layers = nn.ModuleList([EncoderLayer(hidden_dim) for _ in range(num_layers)])
        self.output_layer = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = self.embedding(x)
        x = self.pos_encoding(x)
        for encoder_layer in self.encoder_layers:
            x = encoder_layer(x)
        x = self.output_layer(x.mean(dim=1))
        return x

class PositionalEncoding(nn.Module):
    def __init__(self, hidden_dim, max_seq_len):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=0.1)
        pe = torch.zeros(max_seq_len, hidden_dim)
        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        x = x + self.pe[:, :x.size(1)]
        x = self.dropout(x)
        return x

class EncoderLayer(nn.Module):
    def __init__(self, hidden_dim, dropout=0.1):
        super(EncoderLayer, self).__init__()
        self.feedforward = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Dropout(p=dropout)
        )
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, x):
        x = self.feedforward(x)
        x = self.dropout(x)
        x = self.norm1(x + x)
        x = self.norm2(x + x)
        return x

</code></pre>
<p>And To build it:</p>
<pre><code># Define the hyperparameters
input_dim = X1.shape[1]
hidden_dim = 16
num_layers = 2
num_heads = 8
lr = 1e-3
batch_size = 2
epochs = 1

X_train, X_val, y_train, y_val = train_test_split(X1, y1, test_size=0.2, random_state=42)

# Convert the target variable to NumPy arrays
y_train = y_train.values
y_val = y_val.values

# Create the model and the optimizer
model = Transformer(input_dim, hidden_dim, num_layers, num_heads)
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
criterion = nn.MSELoss()

# Convert the data to PyTorch tensors
X_train = torch.tensor(X_train.values, dtype=torch.float)
X_val = torch.tensor(X_val.values, dtype=torch.float)
y_train = torch.tensor(y_train, dtype=torch.float).unsqueeze(1)
y_val = torch.tensor(y_val, dtype=torch.float).unsqueeze(1)

# Create the data loaders
train_dataset = torch.utils.data.TensorDataset(X_train, y_train)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataset = torch.utils.data.TensorDataset(X_val, y_val)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

# Train the model
for epoch in range(epochs):
    # Train the model on the training set
    model.train()
    train_loss = 0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        y_pred = model(X_batch)
        loss = criterion(y_pred,y_batch.view(1, -1))
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * X_batch.shape[0]
    train_loss /= len(train_dataset)
    print(epochs)

    # Evaluate the model on the validation set
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for X_batch, y_batch in val_loader:
            y_pred = model(X_batch)
            loss = criterion(y_pred,y_batch.view(1, -1))

            val_loss += loss.item() * X_batch.shape[0]
        val_loss /= len(val_dataset)

    # Print the epoch number and the training and validation loss
    print(f&quot;Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}&quot;)
</code></pre>
<p>Then I'm getting the following error:</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-141-7b031268d55a&gt; in &lt;module&gt;
      6     for X_batch, y_batch in train_loader:
      7         optimizer.zero_grad()
----&gt; 8         y_pred = model(X_batch)
      9         loss = criterion(y_pred, y_batch).unsqueeze(1)
     10         loss.backward()

C:\ProgramData\Anaconda3\lib\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)
   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1101                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1102             return forward_call(*input, **kwargs)
   1103         # Do not call functions when jit is used
   1104         full_backward_hooks, non_full_backward_hooks = [], []

&lt;ipython-input-27-b31310f27650&gt; in forward(self, x)
     12     def forward(self, x):
     13         x = self.embedding(x)
---&gt; 14         x = self.pos_encoding(x)
     15         for encoder_layer in self.encoder_layers:
     16             x = encoder_layer(x)

C:\ProgramData\Anaconda3\lib\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)
   1100         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1101                 or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1102             return forward_call(*input, **kwargs)
   1103         # Do not call functions when jit is used
   1104         full_backward_hooks, non_full_backward_hooks = [], []

&lt;ipython-input-27-b31310f27650&gt; in forward(self, x)
     30 
     31     def forward(self, x):
---&gt; 32         x = x + self.pe[:, :x.size(1)]
     33         x = self.dropout(x)
     34         return x

RuntimeError: The size of tensor a (2) must match the size of tensor b (8) at non-singleton dimension 1
</code></pre>
<p>I tried to change the shape of the datframe, but it didn't work. And I'm not sure If I can change to TF instead of PyTorch as well</p>
","transformer-model"
"75964453","Why run_t5_mlm_flax.py does not produces model weight file etc?","2023-04-08 09:41:04","","1","195","<python><nlp><huggingface-transformers><transformer-model><flax>","<p>I was trying to reproduce this Hugging Face tutorial on <a href=""https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling#t5-like-span-masked-language-modeling"" rel=""nofollow noreferrer""><strong>T5-like span masked-language-modeling</strong></a>.</p>
<p>I have the following code <code>tokenizing_and_configing.py</code>:</p>
<pre><code>import datasets

from t5_tokenizer_model import SentencePieceUnigramTokenizer
from transformers import T5Config


vocab_size = 32_000
input_sentence_size = None

# Calculate the total number of samples in the dataset
total_samples = datasets.load_dataset(
    &quot;nthngdy/oscar-mini&quot;, name=&quot;unshuffled_deduplicated_no&quot;, split=&quot;train&quot;
).num_rows

# Calculate one thirtieth of the total samples
subset_samples = total_samples // 30

# Load one thirtieth of the dataset
dataset = datasets.load_dataset(
    &quot;nthngdy/oscar-mini&quot;,
    name=&quot;unshuffled_deduplicated_no&quot;,
    split=f&quot;train[:{subset_samples}]&quot;,
)

tokenizer = SentencePieceUnigramTokenizer(
    unk_token=&quot;&lt;unk&gt;&quot;, eos_token=&quot;&lt;/s&gt;&quot;, pad_token=&quot;&lt;pad&gt;&quot;
)


# Build an iterator over this dataset
def batch_iterator(input_sentence_size=None):
    if input_sentence_size is None:
        input_sentence_size = len(dataset)
    batch_length = 100
    for i in range(0, input_sentence_size, batch_length):
        yield dataset[i : i + batch_length][&quot;text&quot;]


print(&quot;Train Tokenizer&quot;)
# Train tokenizer
tokenizer.train_from_iterator(
    iterator=batch_iterator(input_sentence_size=input_sentence_size),
    vocab_size=vocab_size,
    show_progress=True,
)

# Save files to disk
tokenizer.save(&quot;./models/norwegian-t5-base/tokenizer.json&quot;)

print(&quot;DONE TOKENIZING &quot;)

# CONFIG
config = T5Config.from_pretrained(
    &quot;google/t5-v1_1-small&quot;,
    vocab_size=tokenizer.get_vocab_size()
    # &quot;google/t5-v1_1-base&quot;, vocab_size=tokenizer.get_vocab_size()
)
config.save_pretrained(&quot;./models/norwegian-t5-base&quot;)

print(&quot;DONE SAVING TOKENIZER &quot;)
</code></pre>
<p>The dependency can be found here:</p>
<ul>
<li>📗 <a href=""https://raw.githubusercontent.com/huggingface/transformers/main/examples/flax/language-modeling/t5_tokenizer_model.py"" rel=""nofollow noreferrer""><code>t5_tokenizer_model.py</code></a></li>
</ul>
<p>After <code>tokenizing_and_configing.py</code> is completed. I run this code:</p>
<pre><code>python run_t5_mlm_flax.py \
    --output_dir=&quot;./models/norwegian-t5-base&quot; \
    --model_type=&quot;t5&quot; \
    --config_name=&quot;./models/norwegian-t5-base&quot; \
    --tokenizer_name=&quot;./models/norwegian-t5-base&quot; \
    --dataset_name=&quot;nthngdy/oscar-mini&quot; \
    --dataset_config_name=&quot;unshuffled_deduplicated_no&quot; \
    --max_seq_length=&quot;512&quot; \
    --per_device_train_batch_size=&quot;32&quot; \
    --per_device_eval_batch_size=&quot;32&quot; \
    --adafactor \
    --learning_rate=&quot;0.005&quot; \
    --weight_decay=&quot;0.001&quot; \
    --warmup_steps=&quot;2000&quot; \
    --overwrite_output_dir \
    --logging_steps=&quot;500&quot; \
    --save_steps=&quot;10000&quot; \
    --eval_steps=&quot;2500&quot; \
    --do_train \
    --do_eval

</code></pre>
<p>The full code for <code>run_t5_mlm_flax.py</code> can be found <a href=""https://raw.githubusercontent.com/huggingface/transformers/main/examples/flax/language-modeling/run_t5_mlm_flax.py"" rel=""nofollow noreferrer"">here</a>.</p>
<p>But after <code>run_t5_mlm_flax.py</code> is completed , I can only find these files in <code>./model/norwegian-t5-base</code>:</p>
<pre><code>.
└── norwegian-t5-base
    ├── config.json
    ├── events.out.tfevents.1680920382.ip-172-31-30-81.71782.0.v2
    └── tokenizer.json
    └── eval_results.json
</code></pre>
<p>What's wrong with my process. I expect it to produce more files like these:</p>
<ol>
<li>flax_model.msgpack: This file contains the weights of the fine-tuned Flax model.</li>
<li>tokenizer_config.json: This file contains the tokenizer configuration, such as the vocabulary size and special tokens.</li>
<li>training_args.bin: This file contains the training arguments used during fine-tuning, such as learning rate and batch size.</li>
<li>merges.txt: This file is part of the tokenizer and contains the subword merges.</li>
<li>vocab.json: This file is part of the tokenizer and contains the vocabulary mappings.</li>
<li>train.log: Logs from the training process, including loss, learning rate, and other metrics.</li>
<li>Checkpoint files: If you have enabled checkpoints during training, you will find checkpoint files containing the model weights at specific training steps.</li>
</ol>
<p>Additional note: I don't experience any error messages AT ALL.  Everything completes smoothly without interruption. I'm using Amazon AWS p3.2xlarge;  cuda_11.2.r11.2/compiler.29618528_0</p>
","transformer-model"
"75922638","Hugging Face translation model cross attention layers problem, inconsistent with research","2023-04-03 18:27:10","","1","361","<huggingface-transformers><transformer-model><text-alignment><attention-model><machine-translation>","<p>When I'm inspecting the cross-attention layers from the pretrained transformer translation model (MarianMT model), It is very strange that the cross attention from layer 0 and 1 provide best alignment between input and output. I used bertviz to visualize all heads from all 6 layers, and tried different language, english to german and english to chinese, it all gives the same results, which does not make sense because the last layers should be more accurate according to the paper <em>Jointly Learning to Align and Translate with Transformer Models</em> https://arxiv.org/pdf/1909.02074.pdf</p>
<p>But when I'm looking at the cross attention of model <em>Helsinki-NLP/opus-mt-en-de</em> and <em>Helsinki-NLP/opus-mt-en-zh</em> , the layer 1 gives the best alignment. the code is below:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
import os
os.environ['TRANSFORMERS_CACHE'] = '/data2/hanyings/.cache'

tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-de&quot;)
model = AutoModel.from_pretrained(&quot;Helsinki-NLP/opus-mt-en-de&quot;, output_attentions=True)

encoder_input_ids = tokenizer(&quot;She sees the small elephant.&quot;, return_tensors=&quot;pt&quot;, add_special_tokens=True).input_ids
with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(&quot;Sie sieht den kleinen Elefanten.&quot;, return_tensors=&quot;pt&quot;, add_special_tokens=True).input_ids

outputs = model(input_ids=encoder_input_ids, decoder_input_ids=decoder_input_ids)

encoder_text = tokenizer.convert_ids_to_tokens(encoder_input_ids[0])
decoder_text = tokenizer.convert_ids_to_tokens(decoder_input_ids[0])

from bertviz import model_view
model_view(
    encoder_attention=outputs.encoder_attentions,
    decoder_attention=outputs.decoder_attentions,
    cross_attention=outputs.cross_attentions,
    encoder_tokens= encoder_text,
    decoder_tokens = decoder_text
)
</code></pre>
<p>From the above pictures, I observed that the first 2 layers give the best alignment whereas the last layers do not align the input and output tokens properly. Can you please help me to explain why this happens? and If the alignment of the last layer is not accurate, how does the model provide correct predictions?  Please! It is very important for my research project!</p>
<p>I tried to read the source code of the MarianMT model to see if the layer0 is indeed the top layer(last layer), the order of cross attention layers is reversed. But I did not find anything. So Anyone can help? It is VERY IMPORTANT for my project!!!</p>
","transformer-model"
"75910609","Where did the Transformer embedding numbers come from?","2023-04-02 07:15:39","","0","106","<bert-language-model><embedding><transformer-model><sentence-transformers>","<p>I'm a student studying Transformer. I want to ask, when I will vectorize words with Transformer BERT and get 768 vector dimensions for each word, I'm confused about where these numbers come from, is there a formula to calculate them? Or this vector has been defined according to the token index of each word on Transformer. I need an explanation of the concept. Thank You</p>
<pre><code>array([[-0.6438617 , -0.16065954, -0.5565007 , ..., -0.25163442,
         0.07514413, -0.30617303],
       [-0.3400169 ,  0.10424673, -0.03935281, ..., -0.40216202,
         0.16795622, -0.4955315 ],
       [-0.34042516,  0.5039195 ,  0.02804005, ..., -0.23906936,
        -0.17713265, -0.28009415],
       ...,
       [-0.39777777, -0.84454346,  0.03105666, ..., -0.31586862,
        -0.15702638,  0.08373763],
       [-0.78589696,  0.01650803,  0.02775506, ..., -0.08069627,
         0.07355314, -0.38013673],
       [ 0.22893211,  0.862909  ,  0.06432542, ...,  0.3613814 ,
        -2.1936886 , -0.27759486]], dtype=float32)
</code></pre>
<p>So, I just want to know how to calculate these vectors to produce a number that varies both minuses and positives from -1 to 1. Thank you very much</p>
","transformer-model"
"75876945","CUDA error: CUBLAS_STATUS_INVALID_VALUE error when torch.matmul","2023-03-29 12:02:50","","0","366","<python><pytorch><bert-language-model><transformer-model><matmul>","<p>I am working on text classification task using BERT model.
I have fine-tuned my model and I want load and inference model in various machines.
Most of the time it works just fine, but on one specific environment I am keep getting CUDA error: CUBLAS_STATUS_INVALID_VALUE error.</p>
<p>(torch, numpy, pytorch-transformer, python versions are all setted same)</p>
<p>Even more weird thing is, the error occurs only when I'm working on cuda. (It works fine on cpu)</p>
<p>So I tracked down the error, and I have reached the conclusion that this error comes from torch.matmul(tensor1, tensor2).</p>
<p>When I execute below, It gives me torch.Size([3]), which is correct.</p>
<pre><code>device = 'cpu'
tensor1 = torch.randn(3, 4).to(device)
tensor2 = torch.randn(4).to(device)
torch.matmul(tensor1, tensor2).size()
</code></pre>
<p>But when I execute below, it throws an error</p>
<p>RuntimeError: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling <code>cublasSgemv(handle, op, m, n, &amp;alpha, a, lda, x, incx, &amp;beta, y, incy)</code></p>
<pre><code>device = 'cuda:0'
tensor1 = torch.randn(3, 4).to(device)
tensor2 = torch.randn(4).to(device)
torch.matmul(tensor1, tensor2).size()
</code></pre>
<p>+) I'm currently using python 3.9.16,</p>
<p>pytorch-transformers     1.2.0</p>
<p>torch                    1.13.0</p>
<p>If it helps</p>
","transformer-model"
"75874965","How do I extract features from a torchvision VisitionTransfomer (ViT)?","2023-03-29 08:43:37","75875049","4","3038","<pytorch><computer-vision><feature-extraction><transformer-model><torchvision>","<p>In order to use features from a pretrained <a href=""https://arxiv.org/abs/2010.11929"" rel=""nofollow noreferrer"">VisionTransformer</a> for a downstream task, I'd like to extract features. How do I extract features for example using a <a href=""https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.vit_b_16"" rel=""nofollow noreferrer"">vit_b_16</a> from torchvision? The output should be 768 dimensional features for each image.</p>
<p>Similar as done using CNNs, I was just trying to remove the output layer and pass the input through the remaining layers:</p>
<pre><code>    from torch import nn

    from torchvision.models.vision_transformer import vit_b_16
    from torchvision.models import ViT_B_16_Weights
    
    from PIL import Image as PIL_Image

    vit = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)
    modules = list(vit.children())[:-1]
    feature_extractor = nn.Sequential(*modules)

    preprocessing = ViT_B_16_Weights.DEFAULT.transforms()

    img = PIL_Image.open(&quot;example.png&quot;)
    img = preprocessing(img)

    feature_extractor(img)
</code></pre>
<p>This leads however to an exception:</p>
<pre><code>RuntimeError: The size of tensor a (14) must match the size of tensor b (768) at non-singleton dimension 2
</code></pre>
","transformer-model"
"75855075","Unpatch data of vision transformer","2023-03-27 11:14:14","","0","360","<python><pytorch><transformer-model><vision-transformer>","<p>I have a patch_tensor with the shape: <code>torch.Size([2, 77, 256])</code> and I want to Unpatchify this to <code>(N,H,W,C)</code> or <code>(N,C,H,W)</code>. The original shape of image is (2,4,64,64).</p>
<p>For patch embedding, I am using the <code>PatchEmbed</code> from <code>timm</code> library:</p>
<pre><code>hidden_size = 36 / in_channels = 4 / patch_size = 8 / input_size = 64
</code></pre>
<pre><code>from timm.models.vision_transformer import PatchEmbed
PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True) 
</code></pre>
<p>The encoder block of transformer is similar to the vanilla transformer.</p>
<p>So after extracting patches using  <code>PatchEmbed</code>, the shape of the patched vector is :  <code>torch.Size([2, 64, 36])</code> .</p>
<p>This vector then goes through the Transformer's Encoder and the resulting vector shape is :</p>
<pre><code>torch.Size([2, 77, 256])
</code></pre>
<p>From this point, I am not sure how to start unpatch this vector. Can you please help me.</p>
<p>Thank you in advance.</p>
","transformer-model"
"75840485","Can I use LoRa and Prompt Tuning at the same time for text summarization with GPT?","2023-03-25 08:28:26","","3","1540","<nlp><huggingface-transformers><transformer-model><summarization><huggingface>","<p>LoRA is to insert and learn the rank composition matrix created by dimensionally reducing the weight matrix in the transformer. Prompt Tuning, on the other hand, typically uses a soft prompt that encodes the prompt within the model to learn, rather than a hard prompt that a person gives the task directly. Both are effective in lightening, especially prompt tuning, which is better than hard prompt use.</p>
<p>Both techniques can also be implemented using the peft module.</p>
<pre><code>from peft import get_peft_model, PeftModel, TaskType, LoraConfig, PromptTuningConfig, PromptTuningInit

for path,dirs,files in os.walk('/root/.cache/huggingface/hub/models--kakaobrain--kogpt'):
  for file in files:
    if file.endswith('tokenizer.json'):
      tokenizer_path = path
print(tokenizer_path)

prompt_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM,
    num_virtual_tokens=10,
    prompt_tuning_init=PromptTuningInit.TEXT,
    prompt_tuning_init_text=&quot;Read the following and summarize:&quot;,
    tokenizer_name_or_path=tokenizer_path
)

lora_config = LoraConfig(
    task_type = TaskType.CAUSAL_LM,
    r=8, lora_alpha=32, lora_dropout=0.1,
    target_modules = ['q_proj', 'v_proj'],
    # target_modules = r&quot;.*(q_proj|v_proj)&quot;,
)
</code></pre>
<p>However, the get_feft_model function receives only the model and one peft_config as parameters.</p>
<pre><code>peft_model = get_peft_model(base_model, prompt_config)
</code></pre>
<p>I want to use both techniques at the same time. How shall I do it?</p>
","transformer-model"
"75780500","Does it need a relu function for bert fine tuning?","2023-03-19 06:56:44","","1","219","<pytorch><huggingface-transformers><bert-language-model><transformer-model><fine-tuning>","<p>For example, if it is a multi-class classification, is the following line necessary in the forward function?</p>
<pre><code>final_layer = self.relu(linear_output)
</code></pre>
<p>The class definition is below:</p>
<pre><code>class BertClassifier(nn.Module):

    def __init__(self, dropout=0.5):

        super(BertClassifier, self).__init__()

        self.bert = BertModel.from_pretrained('bert-base-cased')
        self.dropout = nn.Dropout(dropout)
        self.linear = nn.Linear(768, 5)
        self.relu = nn.ReLU()

    def forward(self, input_id, mask):

        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)
        dropout_output = self.dropout(pooled_output)
        linear_output = self.linear(dropout_output)
        #final_layer = self.relu(linear_output)
        return linear_output
</code></pre>
","transformer-model"
"75773706","Huggingface transformer sequence classification inference bug - no attribute 'prepare_inputs_for_generation'","2023-03-18 03:27:20","","1","1529","<pytorch><huggingface-transformers><bert-language-model><transformer-model>","<p>I'm trying to run just basic inference with huggingface bert transformer model based on pytorch. Yet it seems that I'm not calling the inference in the right way. Now I can load the model and the tokenizer nicely, but the inference line gives me error. Note that in the code below, I implemented two ways to do infer, both found from online, yet both gave me the same error. So what am I missing?</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

model_name = &quot;distilbert-base-uncased&quot;
text = &quot;I just had a really nice dinner&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
inputs = tokenizer(text, return_tensors=&quot;pt&quot;)

id2label = {0: &quot;POSITIVE&quot;, 1: &quot;NEGATIVE&quot;}
label2id = {&quot;POSITIVE&quot;: 0, &quot;NEGATIVE&quot;: 1}
model = AutoModelForSequenceClassification.from_pretrained(
    &quot;distilbert-base-uncased&quot;,
    num_labels=2,
    id2label=id2label,
    label2id=label2id
)

inputs = tokenizer(text, return_tensors=&quot;pt&quot;, truncation=True, padding=True)
outputs = model.forward(**inputs)

classifier = pipeline(&quot;sentiment-analysis&quot;, model=model, tokenizer=tokenizer)
outputs = classifier(text)

predicted_label_index = outputs.logits.argmax(-1).item()
predicted_label = id2label[predicted_label_index]
print(f&quot;The predicted label for the text is: {predicted_label}&quot;)
</code></pre>
<p>The error message is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\Owner\Desktop\transformers-v4.25-release\inference\infer_classifier_bert.py&quot;, line 18, in &lt;module&gt;
    outputs = model.forward(**inputs)
  File &quot;C:\Users\Owner\Desktop\transformers-v4.25-release\src\transformers\models\distilbert\modeling_distilbert.py&quot;, line 775, in forward
    logits = self.classifier(pooled_output)  # (bs, num_labels)
  File &quot;C:\Users\Owner\Desktop\transformers-v4.25-release\src\transformers\generation\utils.py&quot;, line 2446, in classifier
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
  File &quot;C:\Users\Owner\AppData\Local\Programs\Python\Python38\lib\site-packages\torch\nn\modules\module.py&quot;, line 1614, in __getattr__
    raise AttributeError(&quot;'{}' object has no attribute '{}'&quot;.format(
AttributeError: 'DistilBertForSequenceClassification' object has no attribute 'prepare_inputs_for_generation'

Process finished with exit code 1

</code></pre>
<p>I looked up the solutions online yet no one had the same error.</p>
","transformer-model"
"75763468","Can't load pyspark.ml.Pipeline model with custom transformer in spark ml pipeline because of missing positional argument for class","2023-03-17 03:17:18","","1","670","<python><apache-spark><databricks><transformer-model><mlflow>","<p>I have a custom transformer python class that I use with a spark mllib pipeline. I want to save the model and load it in another session with spark. I'm able to log the model, but I'm not able to load it after because the confidence transformer requires the labels and it is missing the positional argument. I am using pyspark==3.3.0.</p>
<pre><code>from pyspark.ml import Transformer, PipelineModel
from pyspark.ml.param import Param, Params
from pyspark.ml.util import DefaultParamsWriter, DefaultParamsReader, MLReadable, MLWritable, MLWriter
from pyspark.sql.functions import lit
from pyspark.ml.feature import StringIndexer
from pyspark.ml.classification import LogisticRegression


class Confidence(Transformer, DefaultParamsReadable, DefaultParamsWritable):
    &quot;&quot;&quot;
    A custom Transformer which does some cleanup of the output of the model and creates a column a confidence metric based on a T distribution.
    &quot;&quot;&quot;
    
    labels = Param(
        Params._dummy(),
        &quot;labels&quot;,
        &quot;Count of labels for degrees of freedom&quot;,
        typeConverter=TypeConverters.toInt)

    def __init__(self, labels: int):
        super(Confidence, self).__init__()
        self._setDefault(labels=labels)
        
    def getLabels(self):
        return self.getOrDefault(self.labels)

    def setLabels(self, value):
        self._set(labels=value)

    def _transform(self, df):
        return df.withColumn(&quot;labelCount&quot;, lit(self.getLabels()))
</code></pre>
<pre><code># String Indexer to convert feature column
stringIndexer = StringIndexer(inputCol = &quot;feature&quot;, outputCol = &quot;label&quot;).fit(train)
# Fit model
lr = LogisticRegression()
# Get count of labels from string indexer to pass to confidence
labelCount = len(stringIndexer.labels)
confidence = Confidence(labels = labelCount)

# Create pipeline and fit model
pipeline = Pipeline().setStages([stringIndexer, lr, confidence])
pipeline_model = pipeline.fit(train_df)

basePath = &quot;/tmp/mllib-persistence-example&quot;
pipeline_model.write().overwrite().save(basePath + &quot;/model&quot;)
</code></pre>
<pre><code>model_loaded = Pipeline.load(basePath + &quot;/model&quot;)
</code></pre>
<p>I get this error message:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;command-1923614380916072&gt; in &lt;cell line: 1&gt;()
----&gt; 1 model_loaded = Pipeline.load(basePath + &quot;/model&quot;)

/databricks/spark/python/pyspark/ml/util.py in load(cls, path)
    444     def load(cls, path: str) -&gt; RL:
    445         &quot;&quot;&quot;Reads an ML instance from the input path, a shortcut of `read().load(path)`.&quot;&quot;&quot;
--&gt; 446         return cls.read().load(path)
    447 
    448 

/databricks/spark/python/pyspark/ml/pipeline.py in load(self, path)
    247             return JavaMLReader(cast(Type[&quot;JavaMLReadable[Pipeline]&quot;], self.cls)).load(path)
    248         else:
--&gt; 249             uid, stages = PipelineSharedReadWrite.load(metadata, self.sc, path)
    250             return Pipeline(stages=stages)._resetUid(uid)
    251 

/databricks/spark/python/pyspark/ml/pipeline.py in load(metadata, sc, path)
    437                 stageUid, index, len(stageUids), stagesDir
    438             )
--&gt; 439             stage: &quot;PipelineStage&quot; = DefaultParamsReader.loadParamsInstance(stagePath, sc)
    440             stages.append(stage)
    441         return (metadata[&quot;uid&quot;], stages)

/databricks/spark/python/pyspark/ml/util.py in loadParamsInstance(path, sc)
    727             pythonClassName = metadata[&quot;class&quot;].replace(&quot;org.apache.spark&quot;, &quot;pyspark&quot;)
    728         py_type: Type[RL] = DefaultParamsReader.__get_class(pythonClassName)
--&gt; 729         instance = py_type.load(path)
    730         return instance
    731 

/databricks/spark/python/pyspark/ml/util.py in load(cls, path)
    444     def load(cls, path: str) -&gt; RL:
    445         &quot;&quot;&quot;Reads an ML instance from the input path, a shortcut of `read().load(path)`.&quot;&quot;&quot;
--&gt; 446         return cls.read().load(path)
    447 
    448 

/databricks/spark/python/pyspark/ml/util.py in load(self, path)
    638         metadata = DefaultParamsReader.loadMetadata(path, self.sc)
    639         py_type: Type[RL] = DefaultParamsReader.__get_class(metadata[&quot;class&quot;])
--&gt; 640         instance = py_type()
    641         cast(&quot;Params&quot;, instance)._resetUid(metadata[&quot;uid&quot;])
    642         DefaultParamsReader.getAndSetParams(instance, metadata)

TypeError: __init__() missing 1 required positional argument: 'labels'
</code></pre>
","transformer-model"
"75736575","Vision Transformer for Visual Odometry Regression gives the same output for all inputs","2023-03-14 17:35:47","","1","178","<python><machine-learning><pytorch><transformer-model>","<p>I'm using a Spatio Temporal Vision Transformer known as ViViT, which traditionally is used for video classification, as my model for predicting the change of pose. Pose is the position and orientation of an agent, in this case a camera.</p>
<p>In the training stpe, the model is capable of predicting diverse values and even gives accurate estimates for the path of an agent. In the image below, we see in blue the Ground Truth, and in orange the predicted path:</p>
<p><a href=""https://i.sstatic.net/JmGhK.png"" rel=""nofollow noreferrer"">Odometry plot, ground truth vs predicted in training step</a></p>
<p>However, in the testing fase, the model doesn't show any capabilities of predicting the change of pose.</p>
<p><a href=""https://i.sstatic.net/q8YPg.png"" rel=""nofollow noreferrer"">Odometry plot, ground truth vs predicted in testing step</a></p>
<p>What I feed my model is two images stacked on top of each other. One image corresponds to the instant t and the next one to the instant t+1. I am using the KITTI odometry set.</p>
<pre><code>The shape of my input tensor is the following - [B, C, F, H, W], where:
B = Batches (The size of the batch)
C = Channels (RGB, GrayScale, etc)
F = Number of frames stacked on top of each other (In my particular case this dimension is where I do torch.cat of my images)
H, W = Height, Width
</code></pre>
<p>I wasn't expecting great results but I also wasn't expecting the same output for every single input. If i feed my network a random tensor, like</p>
<pre><code>torch.rand(1, 3, 2, 370, 1241) 
</code></pre>
<p>I get the same output for every diferent tensor.</p>
<p>Also, I have a custom loss function set up:</p>
<p><a href=""https://i.sstatic.net/FVyAu.png"" rel=""nofollow noreferrer"">Custom loss function</a></p>
<p>I am using RMSProp as my optimizer.</p>
<pre><code>For better understanding of the model I am using I have here a pastebin: [Pastebin for the model](https://pastebin.com/3sHjpxng)
</code></pre>
<p>I am stuck with this for the last 3 weeks. Any help would be appreciated.</p>
","transformer-model"
"75721717","Understanding Vision Transformer Implementation in Keras: Issues with Patch Shape and Embedding Layer","2023-03-13 12:12:58","","0","468","<python><tensorflow><keras><transformer-model><vision-transformer>","<p>I'm trying to understand this implementation of vision transformers in keras.</p>
<p>Here is the full <a href=""https://keras.io/examples/vision/image_classification_with_vision_transformer/"" rel=""nofollow noreferrer"">code</a>.</p>
<p>I can't understand why         <code>patches = tf.reshape(patches, [batch_size, -1, patch_dims])</code> is returning a tensor <code>(batch_size,num_patches,patch_dim)</code> with shape of <code>(none,none,108)</code> instead of a tensor of shape <code>(none,144,108)</code>, in this case is returned only one patch and I can</p>
<p>The dimension of <code>patches</code> before being reshaped is <code>(none,12,12,108)</code> in which 12 and 12 are the height and width of all the patches in the image</p>
<pre><code>class Patches(layers.Layer):
    def __init__(self, patch_size):
        super().__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding=&quot;VALID&quot;,
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches
</code></pre>
<p>Later this <code>tensor</code> is then passed to the <code>PatchEncoder()</code> that passes this <code>108 elements patch</code> in a <code>64 dimension dense layer</code> but this should not be done for each of the 144 <code>patches</code> instead of just one(the returned <code>patch</code> of <code>Patches()</code>)?</p>
<p>So that I can have an <code>embedding layer</code> for each of the <code>144 patches</code> I have <code>64 dimension vector elements</code> all different from each other based on the corresponding patch?</p>
<pre><code>class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim):
        super().__init__()
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        encoded = self.projection(patch) + self.position_embedding(positions)
        return encoded
</code></pre>
<p>So I thought that the <code>embedding layer</code> should be something like this in which for each <code>patch</code> I have different values based on the values in the actual patch</p>
<pre><code>**Embedding layer that I think should be returned**
    0.[0 0 0 ... 0]
    1.[1 1 1 ... 1]
    .
    .
    .
    143.[143 143 143 ... 143]
</code></pre>
<p>Instead of this in which all the values in the initial <code>patches</code> are the same because of the <code>shape</code> return in <code>tf.reshape()</code></p>
<pre><code>**Embedding layer that I think is returned but I don't understand if it makes sense**
    0.[0 0 0 ... 0]
    1.[0 0 0 ... 0]
    .
    .
    .
    143.[0 0 0 ... 0]
</code></pre>
<p>My question is how passing a <code>tensor</code> of shape <code>(none,none,108)</code> make sense with this <code>ViT</code> implementation?</p>
<p>Here is also the summary of the model:</p>
<pre><code> input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               
                                )]                                                                
                                                                                                  
 data_augmentation (Sequential)  (None, 72, 72, 3)   7           ['input_3[0][0]']                
                                                                                                  
 patches_2 (Patches)            (None, None, 108)    0           ['data_augmentation[1][0]']      
                                                                                                  
 patch_encoder_2 (PatchEncoder)  (None, 144, 64)     16192       ['patches_2[0][0]']              
</code></pre>
","transformer-model"
"75700651","How to resolve - ""the expanded size of the tensor (1296) must match the existing size (512) at non-singleton dimension 1"" this error?","2023-03-10 20:17:50","","1","893","<deep-learning><pytorch><nlp><huggingface-transformers><transformer-model>","<p>I am trying to classify text with a Bangla data set by a transformer model. To do this, I am using the PyTorch framework to build the neural network model architecture. But I am facing an error with the model. Below I am attaching the code snippets for your better understanding:</p>
<blockquote>
<p>Tokenizing the text</p>
</blockquote>
<pre><code>stemmer = BanglaStemmer()
df[&quot;comment&quot;] = df[&quot;comment&quot;].apply(lambda x: &quot; &quot;.join([stemmer.stem(word) for word in word_tokenize(x)]))
</code></pre>
<blockquote>
<p>Splitting Dataset</p>
</blockquote>
<pre><code>train_text, val_text, train_labels, val_labels = train_test_split(df[&quot;comment&quot;], df[&quot;threat_label&quot;], test_size=0.3, random_state=42)
</code></pre>
<blockquote>
<p>Loading the pre-trained transformer model and tokenize the text using the
tokenizer</p>
</blockquote>
<pre><code>model_class = ppb.BertModel
tokenizer_class = ppb.BertTokenizer
pretrained_weights = 'sagorsarker/bangla-bert-base'

tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)
</code></pre>
<blockquote>
<p>Convert the text into tokens, pad the sequences, and create attention
masks</p>
<p><strong>Train</strong></p>
</blockquote>
<pre><code>   # Tokenize the text
    tokenized_train_text = train_text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))
    # print(tokenized_train_text)
    tokenized_val_text = val_text.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))
    # print(tokenized_val_text)
    
    # Pad the sequences
    max_len = max_length
    padded_train_text = np.array([i + [0]*(max_len-len(i)) for i in tokenized_train_text.values])
    # print(padded_train_text)
    padded_val_text = np.array([i + [0]*(max_len-len(i)) for i in tokenized_val_text.values])
    # print(padded_val_text)
    
    # Create attention masks
    attention_train_masks = np.where(padded_train_text != 0, 1, 0)
    # print(attention_train_masks)
    attention_val_masks = np.where(padded_val_text != 0, 1, 0)
    # print(attention_val_masks)
</code></pre>
<blockquote>
<p>Validation:
This is as same as <strong>Train</strong>.</p>
</blockquote>
<blockquote>
<p>Creating Tensors</p>
</blockquote>
<pre><code># Convert the data into PyTorch tensors
train_text_tensor = torch.tensor(padded_train_text)
# print(train_text_tensor)

val_text_tensor = torch.tensor(padded_val_text)
# print(train_text_tensor)

train_labels_tensor = torch.tensor(padded_train_label)
val_labels_tensor = torch.tensor(padded_val_label)

train_masks_tensor = torch.tensor(attention_train_masks)
val_masks_tensor = torch.tensor(attention_val_masks)
</code></pre>
<blockquote>
<p>Creating a PyTorch DataLoader to feed the data into the model for
training</p>
</blockquote>
<pre><code>from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

batch_size = 32

# Create the DataLoader for training data
train_data = TensorDataset(train_text_tensor, train_masks_tensor, train_labels_tensor)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)
print(len(train_data[0][2]))

# Create the DataLoader for validation data
val_data = TensorDataset(val_text_tensor, val_masks_tensor, val_labels_tensor)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)
</code></pre>
<blockquote>
<p><strong>The model where I am getting the error. The line is <code>outputs = model(batch_text, attention_mask=batch_masks)</code></strong></p>
</blockquote>
<pre><code># Define the hyperparameters
epochs = 4
lr = 2e-5

# Set the device to GPU if available, otherwise, CPU
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

# Freeze the pre-trained parameters
for param in model.parameters():
    param.requires_grad = False

# Add a new classification layer
model.classifier = torch.nn.Sequential(torch.nn.Linear(768, 2), torch.nn.Softmax())

# Move the model to the device
model.to(device)

# Define the optimizer and loss function
optimizer = torch.optim.Adam(model.classifier.parameters(), lr=lr)
loss_fn = torch.nn.CrossEntropyLoss()

# Train the model
for epoch in range(epochs):
    model.train()
    train_loss = 0
    train_accuracy = 0
    for step, batch in enumerate(train_dataloader):
        batch_text = batch[0].to(device)
        # print(batch_text.size())
        batch_masks = batch[1].to(device)
        batch_labels = batch[2].to(device)
        optimizer.zero_grad()
        outputs = model(batch_text, attention_mask=batch_masks)
        loss = loss_fn(outputs[0], batch_labels)
        train_loss += loss.item()
        loss.backward()
        optimizer.step()
        predictions = torch.argmax(outputs[0], dim=1)
        train_accuracy += accuracy_score(predictions.cpu().numpy(), batch_labels.cpu().numpy())
    train_loss /= len(train_dataloader)
    train_accuracy /= len(train_dataloader)
</code></pre>
<blockquote>
<p>Error message is</p>
</blockquote>
<pre><code>/usr/local/lib/python3.9/dist-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
    983             if hasattr(self.embeddings, &quot;token_type_ids&quot;):
    984                 buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]
--&gt; 985                 buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)
    986                 token_type_ids = buffered_token_type_ids_expanded
    987             else:

RuntimeError: The expanded size of the tensor (1296) must match the existing size (512) at non-singleton dimension 1.  Target sizes: [32, 1296].  Tensor sizes: [1, 512]
</code></pre>
<p><strong>Note: This is my first time creating any Neural Network model with PyTorch. I am familiar with TensorFlow.</strong></p>
","transformer-model"
"75687988","Informer: loss always Nan","2023-03-09 17:20:00","75712481","0","168","<deep-learning><nan><loss-function><transformer-model>","<p>I try to use the infomer model to predict my own dataset.But when I change the training dataset to my dataset.Although the program can run, my loss has always been Nan, and there are no predicted values after the training.</p>
<p>I print train_loss,vali_loss and test_loss.The value of them is all nan.</p>
<pre><code>Epoch: 1, Steps: 739 | Train Loss: nan Vali Loss: nan Test Loss: nan
</code></pre>
<p>I looked at the value of test_loss as follows</p>
<pre><code>test_loss = {float32} nan
time_now = {float} 1678378741.1124253
 strides = {tuple: 0} ()
 size = {int} 1
 shape = {tuple: 0} ()
 ndim = {int} 0
 real = {float32} nan
 nbytes = {int} 4
 itemsize = {int} 4
 imag = {float32} 0.0
 flat = {flatiter: 1} &lt;numpy.flatiter object at 0x0000021FA9C06040&gt;
 flags = {flagsobj}   C_CONTIGUOUS : True\n  F_CONTIGUOUS : True\n  OWNDATA : True\n  WRITEABLE : False\n  ALIGNED : True\n  WRITEBACKIFCOPY : False\n
 dtype = {dtype[float32]: 0} float32
 data = {memoryview: 1} &lt;memory at 0x0000021ECC6AFB80&gt;
 base = {NoneType} None
 T = {float32} nan
</code></pre>
<p>You can see that a lot of them are Nan And the MSE and Mae that I output when I finish running are also Nan.</p>
<p>This is my loss calculation code，the ‘pred’ is all nan</p>
<pre><code>epoch_time = time.time()
            for i, (batch_x,batch_y,batch_x_mark,batch_y_mark) in enumerate(train_loader):
                iter_count += 1
                
                model_optim.zero_grad()
                pred, true = self._process_one_batch(
                    train_data, batch_x, batch_y, batch_x_mark, batch_y_mark)
                loss = criterion(pred, true)
                train_loss.append(loss.item())
                ...
            print(&quot;Epoch: {} cost time: {}&quot;.format(epoch+1, time.time()-epoch_time))
            train_loss = np.average(train_loss)
            vali_loss = self.vali(vali_data, vali_loader, criterion)
            test_loss = self.vali(test_data, test_loader, criterion)
</code></pre>
<p>What i confuse is, when I used the dataset that the model originally provided, the program worked fine, all the data was fine, and there was no Nan. And it can also predict the outcome.</p>
<p>here is original dataset column</p>
<pre><code>date Visibility DryBulbFarenheit DryBulbCelsius WetBulbFarenheit DewPointFarenheit DewPointCelsius DewPointCelsius RelativeHumidity WindSpeed WindDirection StationPressure Altimeter  WetBulbCelsius(target)
</code></pre>
<p>And here is my dataset column</p>
<pre><code>date hight wind_speed wind_direction temperature humidity atmospheric_pressure(target)
</code></pre>
<p>I would like to know why the original dataset can be run without error. However, an error occurs when I run my own dataset.where is the problem.Why is my loss always nan and can not predict data.</p>
","transformer-model"
"75684685","Failure to install old versions of transformers in colab","2023-03-09 12:34:01","75688040","0","1626","<python><machine-learning><deep-learning><google-colaboratory><transformer-model>","<p>I recently had a problem installing Transformer version 2.9.0 in colab.</p>
<p><a href=""https://i.sstatic.net/l1bHf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/l1bHf.png"" alt=""Its image"" /></a></p>
<p><a href=""https://i.sstatic.net/Q0mfj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Q0mfj.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"75683051","Pytorch training model with multiple Datasets on multi GPUS","2023-03-09 10:03:36","","0","105","<pytorch><computer-vision><transformer-model><dataloader><multi-gpu>","<p>I have multiple data generators(say 16), each of them are time related, such that I can only generate the data one by one (batch size of each generator are fixed as one). And I have 8 gpus, how can I train the model with all the data simutaneously, something like data generator NO.1 and NO.2 goes to the gpu NO1, data generator NO.3 and NO.4 goes to the gpu NO2 and etc..., something like below in a multiprocess way:</p>
<pre><code># dataloaderpairs = {'device1':(dataloader1, dataloader2), 'device2':(dataloader3, dataloader4).......]}
for gpu_x in my_gpus: 
    for (input1,target1), (input1,target1) in my zip(*dataloaderpairs[gpu_x]):
         model = model.to(gpu_x)
         x  = torch.cat([input1, input2],axis=0).to(gpu_x)
         target = torch.cat([target1, target2],axis=0).to(gpu_x)
         out = model(x)
         losses[gpu_x]+= criterion(out, target)
mloss = merge_the_loss(losses)
mloss.backward()

    
</code></pre>
","transformer-model"
"75682474","nn.Transformer src/tgt/memory masks fail to work","2023-03-09 09:09:44","","1","179","<nlp><torch><transformer-model>","<p>nn.TransformerEncoderLayer produces exactly <strong>same output with same src</strong>,no matter what <strong>src_key_padding_mask</strong> or <strong>src_mask</strong> is.</p>
<p>Likewise,nn.TransformerDecoderLayer output is <strong>not affected</strong> by <strong>any one</strong> of <em>tgt_mask,memory_mask,tgt_key_padding_mask or memory_key_padding_mask</em>.</p>
<p>Does anyone know what’s going wrong?How could I make the masks work in the right way?Thanks a lot.</p>
<pre><code>import torch
import torch.nn as nn

encoder_layer=nn.TransformerEncoderLayer(d_model=6,nhead=2)
encoder_layer.eval()
src=torch.ones((4,3,6))
encoder_layer(src)
</code></pre>
<pre><code>tensor([[[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
</code></pre>
<pre><code>print(encoder_layer(src,src_mask=torch.zeros((4,4)).bool()))
print(encoder_layer(src,src_mask=torch.tensor(
    [[0,1,1,1],
     [0,0,1,1],
     [0,0,0,1],
     [0,0,0,0]]
).bool()))
print(encoder_layer(src,src_mask=torch.tensor(
    [[0,1,0,1],
     [1,0,1,1],
     [0,1,0,1],
     [0,1,1,1]]
).bool()))
</code></pre>
<pre><code>tensor([[[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
tensor([[[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
tensor([[[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]],

        [[ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910],
         [ 0.9927,  0.2251, -1.5199,  1.2508, -1.0397,  0.0910]]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
</code></pre>
<p>All above produces the same result!What's going wrong?</p>
","transformer-model"
"75682297","How to use Huggingface GenerationMixin (or its beam search) with my own model?","2023-03-09 08:54:00","","4","1771","<pytorch><huggingface-transformers><transformer-model><beam-search>","<p>Huggingface's use of a mixin keeps teasing me that this should be possible, but I can't find any clear documentation on exactly what the requirements are, or if the dependencies are just too much to make it worth it. <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/generation/utils.py"" rel=""nofollow noreferrer"">The central module</a> is literally thousands and thousands of lines, and I felt from studying it yesterday that I've learnt more about how to write beam search than I have about GenerationMixin. :-)</p>
<p>From reading the source I think the dependencies are <code>self.config</code> then <code>prepare_inputs_for_generation()</code> and <code>_update_model_kwargs_for_generation()</code>; also implicitly <code>forward()</code>. But I'm not sure that is everything. Nor what each should look like. And I think it may expect <code>forward()</code> to return data in a specific format.</p>
<p>To make the discussion specific, and generally useful, how could Huggingface's beam search be used with <a href=""https://github.com/karpathy/minGPT"" rel=""nofollow noreferrer"">minGPT</a>, which has a <code>forward()</code> function that returns <code>logits,loss</code>. (It actually has its own <code>generate()</code> function that does the equivalent of Huggingface's <code>sample()</code> and <code>greedy_search()</code>, but no beam search support.)  Or <a href=""https://github.com/karpathy/nanoGPT"" rel=""nofollow noreferrer"">nanoGPT</a> if you prefer - they are identical in this area.</p>
<p>In the comments I said <em>It seems everyone's generate/beam search implementation is tied in closely with their transformer implementation...</em> and I still can't really see why everyone reinvents this wheel, and why there is no standalone open source beam search implementation, with a clearly defined interface. Going to throw a bounty at this question, to see if it helps.</p>
","transformer-model"
"75677070","Why didn't the model improve when I used nn.TransformerEncoders?","2023-03-08 18:48:52","","2","73","<python><deep-learning><pytorch><transformer-model>","<p>I'm working on a ViT picture classification task.</p>
<p>This is my code. When I use Transformer model from elsewhere, the model can be trained normally. But when I use nn.Transformer, the loss of the model will not decrease normally.</p>
<p>self.transformer_encoder: something wrong</p>
<p>self.another_transformer_encoder: works well</p>
<pre><code>class ViT(nn.Module):
    def __init__(self, image_size, patch_size, channels, num_classes, dim, depth, heads, mlp_dim):
        super(ViT, self).__init__()

        # Compute number of patches
        self.num_patches = (image_size // patch_size) ** 2

        # Patch embedding layer
        self.patch_embedding = nn.Conv2d(channels, dim, kernel_size=patch_size, stride=patch_size, bias=False)

        # nn.TransformerEncoderLayer and nn.TransformerEncoder
        encoder_layers = TransformerEncoderLayer(
            d_model=dim,
            nhead=heads,
            dim_feedforward=mlp_dim,
            dropout=0.5,
            batch_first=True
        )
        self.transformer_encoder = TransformerEncoder(
            encoder_layer = encoder_layers,
            num_layers=depth
        )
        # this is taken from elsewhere
        self.another_transformer_encoder = Transformer(dim, depth, heads, mlp_dim)

        # Position embedding layer
        self.pos_encoder = PositionalEncoding(dim)

        # Classification head
        self.cls = nn.Parameter(torch.randn(1, 1, dim))
        self.classification_head = nn.Linear(dim, num_classes)

    def forward(self, x):
        # Split input image into patches
        patches = self.patch_embedding(x)    # (batch_size, dim, num_patches_h, num_patches_w)
        patches = patches.flatten(2)   # (batch_size, num_patches, dim)
        patches = patches.transpose(1, 2)    # (batch_size, num_patches, dim)

        # Add position embedding to patches
        patches = self.pos_encoder(patches)

        # Add CLS token
        cls_token = self.cls.expand(x.shape[0], -1, -1)    # (batch_size, 1, dim)
        patches = torch.cat([cls_token, patches], dim=1)   # (batch_size, num_patches+1, dim)

        # Pass patches through transformer encoder layers
        patches = self.transformer_encoder(patches) # bad
        # patches = self.another_transformer_encoder(patches) # good

        # Extract CLS token and pass through classification head
        cls_token = patches[:, 0]
        output = self.classification_head(cls_token)
        return output
</code></pre>
<p>This is the model call part.</p>
<pre><code>model = model = ViT(image_size=28,
                    patch_size=7,
                    channels=1,
                    num_classes=10,
                    dim=64,
                    depth=6,
                    heads=8,
                    mlp_dim=128
                    ).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
</code></pre>
<p>I added <code>batch_first=True</code> in <code>TransformerEncoderLayer</code>, but it didn't work.</p>
","transformer-model"
"75667984","Transformer is only predicting End of Sequence token during inference","2023-03-07 23:00:17","","1","700","<pytorch><nlp><transformer-model>","<h1>Question</h1>
<hr />
<p>I am trying to code a translator from English to Kannada (a South Indian language). The training seems to happen just fine. However, when trying to infer / translate a given English sentence to Kannada, I just get the <code>END_TOKEN</code> as the translation no matter what the actual English sentence input is. I need assistance to see why this is the case.</p>
<h1>Details</h1>
<hr />
<p>I have prodded different parts of the network, but have not seen anything unusual. The dataset I used is from <a href=""https://paperswithcode.com/paper/samanantar-the-largest-publicly-available"" rel=""nofollow noreferrer"">Samanatar</a>. This is a collection of sentence pairs of 11 Indian languages along with their English translations.The following is the code for the transformer stored in a file called <code>transformer.py</code></p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
import math
from torch import nn
import torch.nn.functional as F

def get_device():
    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

def scaled_dot_product(q, k, v, mask=None):
    d_k = q.size()[-1]
    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)
    if mask is not None:
        scaled = scaled.permute(1, 0, 2, 3) + mask
        scaled = scaled.permute(1, 0, 2, 3)
    attention = F.softmax(scaled, dim=-1)
    values = torch.matmul(attention, v)
    return values, attention

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_sequence_length):
        super().__init__()
        self.max_sequence_length = max_sequence_length
        self.d_model = d_model

    def forward(self):
        even_i = torch.arange(0, self.d_model, 2).float()
        denominator = torch.pow(10000, even_i/self.d_model)
        position = (torch.arange(self.max_sequence_length)
                          .reshape(self.max_sequence_length, 1))
        even_PE = torch.sin(position / denominator)
        odd_PE = torch.cos(position / denominator)
        stacked = torch.stack([even_PE, odd_PE], dim=2)
        PE = torch.flatten(stacked, start_dim=1, end_dim=2)
        return PE

class SentenceEmbedding(nn.Module):
    &quot;For a given sentence, create an embedding&quot;
    def __init__(self, max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN):
        super().__init__()
        self.vocab_size = len(language_to_index)
        self.max_sequence_length = max_sequence_length
        self.embedding = nn.Embedding(self.vocab_size, d_model)
        self.language_to_index = language_to_index
        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)
        self.dropout = nn.Dropout(p=0.1)
        self.START_TOKEN = START_TOKEN
        self.END_TOKEN = END_TOKEN
        self.PADDING_TOKEN = PADDING_TOKEN
    
    def batch_tokenize(self, batch, start_token, end_token):

        def tokenize(sentence, start_token, end_token):
            sentence_word_indicies = [self.language_to_index[token] for token in list(sentence)]
            if start_token:
                sentence_word_indicies.insert(0, self.language_to_index[self.START_TOKEN])
            if end_token:
                sentence_word_indicies.append(self.language_to_index[self.END_TOKEN])
            for _ in range(len(sentence_word_indicies), self.max_sequence_length):
                sentence_word_indicies.append(self.language_to_index[self.PADDING_TOKEN])
            return torch.tensor(sentence_word_indicies)

        tokenized = []
        for sentence_num in range(len(batch)):
           tokenized.append( tokenize(batch[sentence_num], start_token, end_token) )
        tokenized = torch.stack(tokenized)
        return tokenized.to(get_device())
    
    def forward(self, x, start_token, end_token): # sentence
        x = self.batch_tokenize(x, start_token, end_token)
        x = self.embedding(x)
        pos = self.position_encoder().to(get_device())
        x = self.dropout(x + pos)
        return x


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.qkv_layer = nn.Linear(d_model , 3 * d_model)
        self.linear_layer = nn.Linear(d_model, d_model)
    
    def forward(self, x, mask):
        batch_size, sequence_length, d_model = x.size()
        qkv = self.qkv_layer(x)
        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)
        qkv = qkv.permute(0, 2, 1, 3)
        q, k, v = qkv.chunk(3, dim=-1)
        values, attention = scaled_dot_product(q, k, v, mask)
        values = values.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)
        out = self.linear_layer(values)
        return out


class LayerNormalization(nn.Module):
    def __init__(self, parameters_shape, eps=1e-5):
        super().__init__()
        self.parameters_shape=parameters_shape
        self.eps=eps
        self.gamma = nn.Parameter(torch.ones(parameters_shape))
        self.beta =  nn.Parameter(torch.zeros(parameters_shape))

    def forward(self, inputs):
        dims = [-(i + 1) for i in range(len(self.parameters_shape))]
        mean = inputs.mean(dim=dims, keepdim=True)
        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)
        std = (var + self.eps).sqrt()
        y = (inputs - mean) / std
        out = self.gamma * y + self.beta
        return out

  
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, hidden, drop_prob=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, hidden)
        self.linear2 = nn.Linear(hidden, d_model)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(p=drop_prob)

    def forward(self, x):
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        return x


class EncoderLayer(nn.Module):
    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):
        super(EncoderLayer, self).__init__()
        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)
        self.norm1 = LayerNormalization(parameters_shape=[d_model])
        self.dropout1 = nn.Dropout(p=drop_prob)
        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)
        self.norm2 = LayerNormalization(parameters_shape=[d_model])
        self.dropout2 = nn.Dropout(p=drop_prob)

    def forward(self, x, self_attention_mask):
        residual_x = x
        x = self.attention(x, mask=self_attention_mask)
        x = self.dropout1(x)
        x = self.norm1(x + residual_x)
        residual_x = x
        x = self.ffn(x)
        x = self.dropout2(x)
        x = self.norm2(x + residual_x)
        return x
    
class SequentialEncoder(nn.Sequential):
    def forward(self, *inputs):
        x, self_attention_mask  = inputs
        for module in self._modules.values():
            x = module(x, self_attention_mask)
        return x

class Encoder(nn.Module):
    def __init__(self, 
                 d_model, 
                 ffn_hidden, 
                 num_heads, 
                 drop_prob, 
                 num_layers,
                 max_sequence_length,
                 language_to_index,
                 START_TOKEN,
                 END_TOKEN, 
                 PADDING_TOKEN):
        super().__init__()
        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)
        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)
                                      for _ in range(num_layers)])

    def forward(self, x, self_attention_mask, start_token, end_token):
        x = self.sentence_embedding(x, start_token, end_token)
        x = self.layers(x, self_attention_mask)
        return x


class MultiHeadCrossAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.kv_layer = nn.Linear(d_model , 2 * d_model)
        self.q_layer = nn.Linear(d_model , d_model)
        self.linear_layer = nn.Linear(d_model, d_model)
    
    def forward(self, x, y, mask):
        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention
        kv = self.kv_layer(x)
        q = self.q_layer(y)
        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)
        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)
        kv = kv.permute(0, 2, 1, 3)
        q = q.permute(0, 2, 1, 3)
        k, v = kv.chunk(2, dim=-1)
        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!
        values = values.reshape(batch_size, sequence_length, d_model)
        out = self.linear_layer(values)
        return out


class DecoderLayer(nn.Module):
    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):
        super(DecoderLayer, self).__init__()
        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)
        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])
        self.dropout1 = nn.Dropout(p=drop_prob)

        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)
        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])
        self.dropout2 = nn.Dropout(p=drop_prob)

        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)
        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])
        self.dropout3 = nn.Dropout(p=drop_prob)

    def forward(self, x, y, self_attention_mask, cross_attention_mask):
        _y = y
        y = self.self_attention(y, mask=self_attention_mask)
        y = self.dropout1(y)
        y = self.layer_norm1(y + _y)

        _y = y
        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)
        y = self.dropout2(y)
        y = self.layer_norm2(y + _y)

        _y = y
        y = self.ffn(y)
        y = self.dropout3(y)
        y = self.layer_norm3(y + _y)
        return y


class SequentialDecoder(nn.Sequential):
    def forward(self, *inputs):
        x, y, self_attention_mask, cross_attention_mask = inputs
        for module in self._modules.values():
            y = module(x, y, self_attention_mask, cross_attention_mask)
        return y

class Decoder(nn.Module):
    def __init__(self, 
                 d_model, 
                 ffn_hidden, 
                 num_heads, 
                 drop_prob, 
                 num_layers,
                 max_sequence_length,
                 language_to_index,
                 START_TOKEN,
                 END_TOKEN, 
                 PADDING_TOKEN):
        super().__init__()
        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)
        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])

    def forward(self, x, y, self_attention_mask, cross_attention_mask, start_token, end_token):
        y = self.sentence_embedding(y, start_token, end_token)
        y = self.layers(x, y, self_attention_mask, cross_attention_mask)
        return y


class Transformer(nn.Module):
    def __init__(self, 
                d_model, 
                ffn_hidden, 
                num_heads, 
                drop_prob, 
                num_layers,
                max_sequence_length, 
                kn_vocab_size,
                english_to_index,
                kannada_to_index,
                START_TOKEN, 
                END_TOKEN, 
                PADDING_TOKEN
                ):
        super().__init__()
        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, english_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)
        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, kannada_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)
        self.linear = nn.Linear(d_model, kn_vocab_size)
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

    def forward(self, 
                x, 
                y, 
                encoder_self_attention_mask, 
                decoder_self_attention_mask, 
                decoder_cross_attention_mask,
                enc_start_token=False,
                enc_end_token=False,
                dec_start_token=False, # We should make this true
                dec_end_token=False): # x, y are batch of sentences
        x = self.encoder(x, encoder_self_attention_mask, start_token=enc_start_token, end_token=enc_end_token)
        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask, start_token=dec_start_token, end_token=dec_end_token)
        out = self.linear(out)
        return out
</code></pre>
<p>The following is the code to get the dataset and initialize the model.</p>
<pre class=""lang-py prettyprint-override""><code>from transformer import Transformer
english_file = 'drive/MyDrive/translation_en_kn/train.en'
kannada_file = 'drive/MyDrive/translation_en_kn/train.kn'

def is_valid_tokens(sentence, vocab):
    for token in list(set(sentence)):
        if token not in vocab:
            return False
    return True

def is_valid_length(sentence, max_sequence_length):
    return len(list(sentence)) &lt; (max_sequence_length - 1) # need to re-add the end token so leaving 1 space


# Generated this by filtering Appendix code

START_TOKEN = '&lt;START&gt;'
PADDING_TOKEN = '&lt;PADDING&gt;'
END_TOKEN = '&lt;END&gt;'

kannada_vocabulary = [START_TOKEN, ' ', '!', '&quot;', '#', '$', '%', '&amp;', &quot;'&quot;, '(', ')', '*', '+', ',', '-', '.', '/', 
                      '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '&lt;', '=', '&gt;', '?', 'ˌ', 
                      'ँ', 'ఆ', 'ఇ', 'ా', 'ి', 'ీ', 'ు', 'ూ', 
                      'ಅ', 'ಆ', 'ಇ', 'ಈ', 'ಉ', 'ಊ', 'ಋ', 'ೠ', 'ಌ', 'ಎ', 'ಏ', 'ಐ', 'ಒ', 'ಓ', 'ಔ', 
                      'ಕ', 'ಖ', 'ಗ', 'ಘ', 'ಙ', 
                      'ಚ', 'ಛ', 'ಜ', 'ಝ', 'ಞ', 
                      'ಟ', 'ಠ', 'ಡ', 'ಢ', 'ಣ', 
                      'ತ', 'ಥ', 'ದ', 'ಧ', 'ನ', 
                      'ಪ', 'ಫ', 'ಬ', 'ಭ', 'ಮ', 
                      'ಯ', 'ರ', 'ಱ', 'ಲ', 'ಳ', 'ವ', 'ಶ', 'ಷ', 'ಸ', 'ಹ', 
                      '಼', 'ಽ', 'ಾ', 'ಿ', 'ೀ', 'ು', 'ೂ', 'ೃ', 'ೄ', 'ೆ', 'ೇ', 'ೈ', 'ೊ', 'ೋ', 'ೌ', '್', 'ೕ', 'ೖ', 'ೞ', 'ೣ', 'ಂ', 'ಃ', 
                      '೦', '೧', '೨', '೩', '೪', '೫', '೬', '೭', '೮', '೯', PADDING_TOKEN, END_TOKEN]

english_vocabulary = [START_TOKEN, ' ', '!', '&quot;', '#', '$', '%', '&amp;', &quot;'&quot;, '(', ')', '*', '+', ',', '-', '.', '/', 
                        '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',
                        ':', '&lt;', '=', '&gt;', '?', '@', 
                        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 
                        'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 
                        'Y', 'Z',
                        '[', '\\', ']', '^', '_', '`', 
                        'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',
                        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 
                        'y', 'z', 
                        '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN]

index_to_kannada = {k:v for k,v in enumerate(kannada_vocabulary)}
kannada_to_index = {v:k for k,v in enumerate(kannada_vocabulary)}
index_to_english = {k:v for k,v in enumerate(english_vocabulary)}
english_to_index = {v:k for k,v in enumerate(english_vocabulary)}

with open(english_file, 'r') as file:
    english_sentences = file.readlines()
with open(kannada_file, 'r') as file:
    kannada_sentences = file.readlines()

# Limit Number of sentences
TOTAL_SENTENCES = 100000
english_sentences = english_sentences[:TOTAL_SENTENCES]
kannada_sentences = kannada_sentences[:TOTAL_SENTENCES]
english_sentences = [sentence.rstrip('\n') for sentence in english_sentences]
kannada_sentences = [sentence.rstrip('\n') for sentence in kannada_sentences]

max_sequence_length = 200
valid_sentence_indicies = []
for index in range(len(kannada_sentences)):
    kannada_sentence, english_sentence = kannada_sentences[index], english_sentences[index]
    if is_valid_length(kannada_sentence, max_sequence_length) \
      and is_valid_length(english_sentence, max_sequence_length) \
      and is_valid_tokens(kannada_sentence, kannada_vocabulary):
        valid_sentence_indicies.append(index)

kannada_sentences = [kannada_sentences[i] for i in valid_sentence_indicies]
english_sentences = [english_sentences[i] for i in valid_sentence_indicies]

d_model = 512
batch_size = 30
ffn_hidden = 2048
num_heads = 8
drop_prob = 0.1
num_layers = 1
max_sequence_length = 200
kn_vocab_size = len(kannada_vocabulary)

transformer = Transformer(d_model, 
                          ffn_hidden,
                          num_heads, 
                          drop_prob, 
                          num_layers, 
                          max_sequence_length,
                          kn_vocab_size,
                          english_to_index,
                          kannada_to_index,
                          START_TOKEN, 
                          END_TOKEN, 
                          PADDING_TOKEN)
</code></pre>
<p>We then create the dataset loader</p>
<pre class=""lang-py prettyprint-override""><code>from torch import nn
from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):

    def __init__(self, english_sentences, kannada_sentences):
        self.english_sentences = english_sentences
        self.kannada_sentences = kannada_sentences

    def __len__(self):
        return len(self.english_sentences)

    def __getitem__(self, idx):
        return self.english_sentences[idx], self.kannada_sentences[idx]


criterian = nn.CrossEntropyLoss(ignore_index=kannada_to_index[PADDING_TOKEN],
                    reduction='none')

# When computing the loss, we are ignoring cases when the label is the padding token
for params in transformer.parameters():
    if params.dim() &gt; 1:
        nn.init.xavier_uniform_(params)

optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
</code></pre>
<p>Here is the function to create the look ahead and padding masks for the encoder and decoder. I use <code>-1e9</code> instead of negative infinity so the softmax performed during scaled dot product attention is stable.</p>
<pre class=""lang-py prettyprint-override""><code>
NEG_INFTY = -1e9

def create_masks(eng_batch, kn_batch):
    num_sentences = len(eng_batch)
    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)
    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)
    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)
    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)
    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)

    for idx in range(num_sentences):
      eng_sentence_length, kn_sentence_length = len(eng_batch[idx]), len(kn_batch[idx])
      eng_chars_to_padding_mask = np.arange(eng_sentence_length + 1, max_sequence_length)
      kn_chars_to_padding_mask = np.arange(kn_sentence_length + 1, max_sequence_length)
      encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True
      encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True
      decoder_padding_mask_self_attention[idx, :, kn_chars_to_padding_mask] = True
      decoder_padding_mask_self_attention[idx, kn_chars_to_padding_mask, :] = True
      decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True
      decoder_padding_mask_cross_attention[idx, kn_chars_to_padding_mask, :] = True

    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)
    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)
    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)
    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask
</code></pre>
<p>Here is the trainer code. Note for every 100 iterations, we print an example English Sentence from the current batch, it's actual Kannada Translation and the corresponding prediction the model made at the time.</p>
<pre class=""lang-py prettyprint-override""><code>transformer.train()
transformer.to(device)
total_loss = 0
num_epochs = 10

for epoch in range(num_epochs):
    print(f&quot;Epoch {epoch}&quot;)
    iterator = iter(train_loader)
    for batch_num, batch in enumerate(iterator):
        eng_batch, kn_batch = batch
        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, kn_batch)
        optim.zero_grad()
        kn_predictions = transformer(eng_batch,
                                     kn_batch,
                                     encoder_self_attention_mask.to(device), 
                                     decoder_self_attention_mask.to(device), 
                                     decoder_cross_attention_mask.to(device),
                                     enc_start_token=False,
                                     enc_end_token=False,
                                     dec_start_token=True,
                                     dec_end_token=True)
        labels = transformer.decoder.sentence_embedding.batch_tokenize(kn_batch, start_token=False, end_token=True)
        loss = criterian(
            kn_predictions.view(-1, kn_vocab_size).to(device),
            labels.view(-1).to(device)
        ).to(device)
        valid_indicies = torch.where(labels.view(-1) == kannada_to_index[PADDING_TOKEN], False, True)
        loss = loss.sum() / valid_indicies.sum()
        loss.backward()
        optim.step()
        #train_losses.append(loss.item())
        if batch_num % 100 == 0:
            print(f&quot;Iteration {batch_num} : {loss.item()}&quot;)
            print(f&quot;English: {eng_batch[0]}&quot;)
            print(f&quot;Kannada Translation: {kn_batch[0]}&quot;)
            kn_sentence_predicted = torch.argmax(kn_predictions[0], axis=1)
            predicted_sentence = &quot;&quot;
            for idx in kn_sentence_predicted:
              if idx == kannada_to_index[END_TOKEN]:
                break
              predicted_sentence += index_to_kannada[idx.item()]
            print(f&quot;Kannada Prediction: {predicted_sentence}&quot;)
            print(&quot;-------------------------------------------&quot;)
</code></pre>
<p>You'll notice the the loss certainly decreases and the prediction though not amazing, are starting to actually take shape. I ran this for about 2,000 iterations for now.</p>
<pre><code>Epoch 0
Iteration 0 : 5.444823265075684
English: Hes a scientist.
Kannada Translation: ಇವರು ಸಂಶೋಧಕ ಸ್ವಭಾವದವರು.
Kannada Prediction: ಬँಗಗಗಗಗ೫ಗ೫ు :೬೬೬೬೬ಗೃಗಥಥ ೈೈ$$ೠೠೠೠೠೠೠೠೠೠೠರೠೠೠీೠರೠೠೠೠ---+ಬ+++ಬಬಏ+ಬಬಿ+೮ಬ೧೮ಳ +ಳ+ఇఇँೕೌೌఇఇఇೌಚಥ್ఇಓ೫ಣೕ4ఇೀఇఇ೫ಔಌˌాಗಸಜಶಶాಸಸದಶ+ಷాాಸಷˌ೦೫ಷಪಷ೭ೞ#೧ँ?ಷಷಚಥಣಥಥ?ಷ?ಥಥಚಚಥಚ%ಚಚ*ಬಫಗಗಬˌಗ೧&amp;ಗಬಗಬ೮೮.ಗಗˌಗಗಗಗಗಌ%%ಷಷಷಚಷಚ?+0ಣ++ೋೋಗಗಘಚೋ&lt;ಇಚ
-------------------------------------------
Iteration 100 : 3.5337600708007812
English: She ate it.
Kannada Translation: ಅವಳು ಅವನಿಗೆ ಊಟ ಹಾಕಿದಳೂ.
Kannada Prediction: ಆದ್       ್            ್   ್್್  ್     
-------------------------------------------
Iteration 200 : 3.3461103439331055
English: Caste and religion were unknown.
Kannada Translation: ಜಾತಿ, ಬೇಧ ಎಂಬುದೇ ಗೊತ್ತಿರಲಿಲ್ಲ.
Kannada Prediction: ಇದರ್  ೆ  ಿ   ಿ    ಿ ್್್ ್
-------------------------------------------
Iteration 300 : 3.1086130142211914
English: Seeing this, ruler was elated and told his son that the strength of the rabbit is due to the valour of the region's citizenry.
Kannada Translation: ಇದನ್ನು ನೋಡಿ, ಆಡಳಿತಗಾರನು ಉತ್ಸಾಹದಿಂದ ಮತ್ತು ಮೊಲದ ಬಲವು ಪ್ರದೇಶದ ನಾಗರಿಕರ ಶೌರ್ಯದ ಕಾರಣ ಎಂದು ತನ್ನ ಮಗನಿಗೆ ತಿಳಿಸಿದನು.
Kannada Prediction: ಆದ್ುದ  ಸ್ದ   ಮ ಿರ ್್ದ್್ ಸಸ್ರುರಿ  ದ್ ಿ್ ್ ಕಾ ್ುಸಗ್  ಸಾತು್  ಿಮಿನ್ ದಾಿಮ್ ುನ ುಸ್ ್್ದರರಿ ತ್ಿದುಮಾಾ್ದಿ ುಿರ್ ಿ    
-------------------------------------------
Iteration 400 : 3.0160765647888184
English: I also had such a feeling.
Kannada Translation: ನನಗಂತೂ ಅಂಥ ಅನುಭೂತಿಯೇ ಆಗಿದ್ದು.
Kannada Prediction: ಇಾ್ೆತ್ ಮಲದ ಮವ್ ಾ ್   ವಗಿ ುಲು 
-------------------------------------------
Iteration 500 : 3.020336627960205
English: What if its too late?
Kannada Translation: ದೀರ್ಘಕಾಲ ಇದ್ದರೆ ನಾಟ್ ಏನಾಗುತ್ತದೆ?
Kannada Prediction: ಇಾನ ಿಲ ್ರಾಪ  ರುು ಹೆಗ್ರಮನುರಳ ್ತ್ೆ.
-------------------------------------------
Iteration 600 : 2.7896459102630615
English: I am happy that our principals and teachers are enthusiastically participating in this campaign to implement the National Education Policy.
Kannada Translation: ರಾಷ್ಟ್ರೀಯ ಶಿಕ್ಷಣ ನೀತಿಯನ್ನು ಜಾರಿಗೆ ತರುವ ಈ ಅಭಿಯಾನದಲ್ಲಿ ನಮ್ಮ ಪ್ರಾಂಶುಪಾಲರು ಮತ್ತು ಶಿಕ್ಷಕರು ಉತ್ಸಾಹದಿಂದ ಭಾಗವಹಿಸುತ್ತಿರುವುದು ನನಗೆ ಸಂತೋಷವಾಗಿದೆ.
Kannada Prediction: ಆಿರ್ ್ಲ್ಯ ಪ್ ್ರ ್ಪ್ರ್ಯಿ್ಯ್ ಪ್ರ್ ಿ ಮ್ಿ  ಮ ಮನ   ರ್್್ರ್ದಹಿ  ಾನ್ರ್ ದ್ ್ಗ್ು ಹಾ್ನ್ ಮ ಸಾಯ್್ಿ ಕಂ್ಯುಗುೆದದೆಪ್ರಿಿ್ ಿ ಿ ್ಯ್ ಿ ್ ಸಿ್ಿ ಪಿತ್ತ್ಿಗಿ ್ 
-------------------------------------------
Iteration 700 : 2.8551084995269775
English: This will cause heartburn.
Kannada Translation: ಇದು ಎದೆಯುರಿಗೆ ಕಾರಣವಾಗುತ್ತದೆ.
Kannada Prediction: ಅದ್ ಕಂ  ಾ ್ ಳ ಪ್ರ್  ರಳ ್ಲ್ .
-------------------------------------------
Iteration 800 : 2.7453389167785645
English: The government is sinking into debt.
Kannada Translation: ಸಾಲದ ಬಲೆಗೆ ಸರ್ಕಾರ ಸಿಕ್ಕಿಕೊಂಡಿದೆ.
Kannada Prediction: ಆುರ್ುಕ್್ ಿಗಕಿುಲ್ರ್ಮಿ  ಲ್ದ್ಂದಿದೆ.
-------------------------------------------
Iteration 900 : 2.7385761737823486
English: Rajeshwari Devi and others participated in this lively programme.
Kannada Translation: ರಾಜೇಶ್ವರಿ ದೇವಿ ಮೊದಲಾದವರು ಉಪಸ್ಥಿತರಿದ್ದರು.
Kannada Prediction: ಮಿವ್ ್ಲಿ್ ಸ ಕಾ ಸಾರರ್ರ ುೆ ಮತ್್ನಿದ್್ಸ ತುಿ.
-------------------------------------------
Iteration 1000 : 2.6645731925964355
English: The virus can be passed on through contact with contaminated surfaces.
Kannada Translation: ಕಲುಷಿತ ಮೇಲ್ಮೈಗಳ ಮೂಲಕ ವೈರಸ್ ಹರಡಬಹುದು.
Kannada Prediction: ಸಾ್ ್ ್ಸಾ ್ಲಾ ಳಿಸ  ಿ್ಸ  ್ಿರನಾಾೆ್ಾ ಿ.
-------------------------------------------
Iteration 1100 : 2.7321813106536865
English: &quot;&quot;&quot;I don't know too much more.&quot;
Kannada Translation: &quot;&quot;&quot;ನನಗೆ ಹೆಚ್ಚೇನು ತಿಳಿದಿಲ್ಲ.&quot;
Kannada Prediction: ಅ&quot;ನಕುಿೆ ಹಾ ್ರ್ನ್ ನ್ದಿದೆದಿತ್
-------------------------------------------
</code></pre>
<p>The issue however creeps in the model inference time. I pass an empty Kannada sentence (which should be encoded to a <code>START_TOKEN</code>), but all I get is the <code>END_TOKEN</code> for the translation.</p>
<pre class=""lang-py prettyprint-override""><code>transformer.eval()
kn_sentence = (&quot;&quot;,)
eng_sentence = (&quot;Should we go to the mall?&quot;,)

for word_counter in range(max_sequence_length):
    encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, kn_sentence)
    predictions = transformer(eng_sentence,
                              kn_sentence,
                              encoder_self_attention_mask.to(device), 
                              decoder_self_attention_mask.to(device), 
                              decoder_cross_attention_mask.to(device),
                              enc_start_token=False,
                              enc_end_token=False,
                              dec_start_token=True,
                              dec_end_token=False)
    next_token_prob_distribution = predictions[0][word_counter] # not actual probs
    next_token_index = torch.argmax(next_token_prob_distribution).item()
    next_token = index_to_kannada[next_token_index]
    kn_sentence = (kn_sentence[0] + next_token, )
    print(f&quot;next_token: {next_token}&quot;)
    if next_token == END_TOKEN:
      break
 
print(f&quot;translation: {kn_sentence}&quot;)
</code></pre>
<p>Running the code above, I get this</p>
<pre><code>next_token: &lt;END&gt;
translation: ('&lt;END&gt;',)
</code></pre>
<p>Thanks for reading until this point. Any assistance is appreciated.</p>
","transformer-model"
"75646742","Reshape matrix before matrix multiplication","2023-03-06 03:05:31","","0","97","<python><performance><pytorch><matrix-multiplication><transformer-model>","<p>I have a snippet of the <code>forward</code> step of &quot;Transformer Neural Network&quot; using Pytorch.</p>
<ul>
<li>Source: <a href=""https://github.com/pbloem/former/blob/master/former/transformers.py"" rel=""nofollow noreferrer"">https://github.com/pbloem/former/blob/master/former/transformers.py</a></li>
<li>Simplified to below code by me.</li>
</ul>
<p>With:</p>
<ul>
<li><code>b: batch_size</code>, <code>t: input sequence length</code>, <code>k: embedding length</code>, <code>self.num_vocabs: output classes</code></li>
<li><code>self.toprobs(x)</code>: a <code>nn.Linear</code> layer with in/output features <code>(k, num_vocabs)</code>.</li>
</ul>
<pre><code>    def forward(self, x):
        tokens = self.token_embedding(x)
        b, t, k = tokens.size()

        x = self.transformer_block(tokens)

        x = x.view(b * t, k)
        x = self.toprobs(x)
        x = x.view(b, t, self.num_vocabs)

        output = F.log_softmax(x, dim=2)

        return output
</code></pre>
<p>Given: <code>b = 2, t = 2, k = 3, self.num_vocabs = 256</code>.</p>
<ul>
<li>The output shape of <code>x</code> after <code>x = self.transformer_block(tokens)</code> is <code>(2, 2, 3)</code>.</li>
<li>Reshape <code>x</code> to <code>(b * t, k) -&gt; (4, 3)</code>, then pass throught <code>self.toprobs(x)</code> I got <code>(4, 256)</code>, then reshape again back to <code>(2, 2, 256)</code>.</li>
</ul>
<p>Question:</p>
<ul>
<li>Why <code>x</code> need to reshape to <code>(b * t, k)</code>? If I keep <code>x</code> shape at <code>(2, 2, 3)</code> and pass throght <code>self.toprobs(x)</code>, I stil have the same results and shape <code>(2, 2, 256)</code>.</li>
<li>Is there any beneficial in accelerate or memory usage of matrix multiplication step?</li>
</ul>
<pre><code>    def forward(self, x):
        tokens = self.token_embedding(x)
        b, t, k = tokens.size()

        x = self.transformer_block(tokens)

        # Same result without matrix reshape
        x = self.toprobs(x)

        output = F.log_softmax(x, dim=2)

        return output
</code></pre>
","transformer-model"
"75642865","How can modify ViT Pytorch transformer model for regression task","2023-03-05 14:17:00","","1","1048","<python><regression><transformer-model>","<p>How can modify ViT Pytorch transformer model for regression task on datasets such as:</p>
<ol>
<li>Stock Prediction Dataset</li>
<li>Real Estate Price Prediction</li>
</ol>
<p>Anyone have code or worked on transformer model for regression?</p>
","transformer-model"
"75609325","Pass image of any size to Pytorch ViT pretrained model","2023-03-01 21:44:14","","1","1019","<deep-learning><pytorch><transformer-model>","<pre><code>import torch
from torchvision import models

model = models.vit_b_32(pretrained=True ,image_size=320)
model.eval()
</code></pre>
<p>The above piece of code is failing at Line 3 with the below error:</p>
<pre><code>ValueError: The parameter 'image_size' expected value 224 but got 320 instead. 
</code></pre>
<p>So does Pytorch's pre-trained Vision Transformer model take only a fixed shape input image size unlike pre-trained ResNet's which are flexible with the image size ?</p>
<p>I am shying a bit from downsizing my image as I am trying to perform Crack-detection on some samples. After downsizing to 224, the crack pixels become far too small which I believe my affect my model's performance.
When I train my model on ResNets, I get the optimal performance for image shapes &gt; 400px</p>
","transformer-model"
"75599347","Copy Stage on IBM Data Stage","2023-03-01 03:33:14","","0","248","<transformer-model><datastage>","<p>I found a strange thing while using Copy Data to insert data into a table.
All columns are processed in a transformer and there are two special columns in the transformer.</p>
<p>Column A uses index function to perform LIKE operation in a string.
Column B is the string to be used column A and it is sorted by alphabetical order in one column.</p>
<p>The following code is using index function to assign a value of Column A.</p>
<pre><code>IF index(COL_B, &quot;ABC&quot;, 1) &gt;0 THEN 'ABC' ELSE COL_B
</code></pre>
<p>My expected result on the target table is Column A to have 'ABC' or the original string of Column B.
When I checked out the table, column A was not changed at all.</p>
<p><em><strong>BTW, after placing one transformer between COPY stage and Target table, the column A is updated.</strong></em></p>
<p>I couldn't find the reason why it happened or any clue/explanation by googling. Is it normal result when we use a COPY stage? If so, our team should know it can happen all the time.</p>
<ol>
<li>Original flow - not update column A and B</li>
</ol>
<p><a href=""https://i.sstatic.net/1lCoD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1lCoD.png"" alt=""enter image description here"" /></a></p>
<ol start=""2"">
<li>Added a transformer between COPY and Target table - updated column A and B</li>
</ol>
<p><a href=""https://i.sstatic.net/1HJWv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1HJWv.png"" alt=""enter image description here"" /></a></p>
<p>======================================================</p>
<p><strong>[Updates]</strong></p>
<ol>
<li><p>This error is not reproduced regardless of the creation a new job by copying the original job and a target table on the Oracle.</p>
</li>
<li><p>The original job and the copied job are sending the correct dataflow without Stage Variables once the dataflow was corrected by one more transformer even though the 2nd transformer was removed after the correction.</p>
</li>
<li><p>Due to #1 updates, Stage Variables in the transformer in the original job works well, but I couldn't have the exact testing against the incorrect dataflow.</p>
</li>
</ol>
","transformer-model"
"75585069","Why is BERT Storing cache even after Caching is disabled?","2023-02-27 20:04:16","75601400","0","494","<caching><huggingface-transformers><torch><bert-language-model><transformer-model>","<p>I am trying to extract hidden state features from a fine-tuned BERT model, but each text entry consumes memory and does not free it up after the next call. I can only run 20-30 sentences with 24 GB of ram memory.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertModel
import numpy as np

data = pd.read_csv('https://docs.google.com/spreadsheets/d/' + 
                   '1cFyUJdpFC3gpQsqjNc4D8ZCxBAMd_Pcpu8SlrsjAv-Q' +
                   '/export?gid=0&amp;format=csv',
                  )
data = data.MESSAGES


# I will be using my own fine-tuned model, but with
# bert-base-uncased, I get the same problem
tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = BertModel.from_pretrained(
    &quot;bert-base-cased&quot;,
    from_tf=True,
    output_hidden_states=True,
    use_cache=False)

sentences = data[0:].tolist()
inputs = tokenizer(sentences, return_tensors='pt', padding=True,truncation=True)
featuresINeed = model(inputs['input_ids'])['pooler_output']
</code></pre>
<p>In the case above, I run out of memory. I tried breaking it into chunks and using <code>torch.cuda.empty_cache()</code>, but it doesn't seem to clear all the memory. I tried both with and without GPU. In my case, I am using a dataset of size 60,000 (possibly larger in the future) and using a fine-tuned model of BERT large. I will have a 24GB GPU available for me.</p>
<p>Any suggestions?</p>
<p>To keep in mind, my main goal is to have 1 Language Model predict the next token and extract features of the current sentence.</p>
","transformer-model"
"75549817","Transformer train a new tokenizer base on existing one","2023-02-23 20:06:42","","2","592","<python><nlp><tokenize><transformer-model><huggingface-tokenizers>","<p>In the following code</p>
<pre><code>from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-chinese&quot;)
tokenizer_new = tokenizer.train_new_from_iterator(training_corpus, 50000, new_special_tokens = ['健康','医学','试剂盒',....])
</code></pre>
<p>where training_corpus is an iterator generating lines of text from hard drive chinese_medical.txt file</p>
<p>For readers who may not familiar with &quot;bert-base-chinese&quot;, it is a single character tokenizer with &quot;wordpiece&quot; model as default</p>
<p>My question is</p>
<p><strong>tokenizer_new</strong> has totally different indexing from <strong>tokenizer</strong>, for example</p>
<pre><code>tokenizer.vocab['先'] #1044
tokenizer_new.vocab['先'] #5151
</code></pre>
<p>this makes continue training (i.e. train a Chinese medical specific BERT model) on model</p>
<pre><code>model = AutoModelForMaskedLM.from_pretrained(&quot;bert-base-chinese&quot;)
</code></pre>
<p>impossible. Because the indexing id for the same character is totally different in <strong>tokenizer_new</strong></p>
<p>How to make the indexing for the same character fixed?</p>
","transformer-model"
"75548271","Supervised fine tuning in pre-trained language model","2023-02-23 17:26:03","","0","153","<nlp><transformer-model><language-model><fine-tuning>","<p>Supervised find turning adds a extra output layer to the pre-trained model.</p>
<p>Does this extra layer alter the probability of words that are not related to the fine tune data?</p>
","transformer-model"
"75538600","Core Data using NSArray does not preserve any data after program termination","2023-02-22 22:04:13","","0","36","<swift><xcode><core-data><nsarray><transformer-model>","<p>I'm attempting to configure the transformer to preserve array objects. It was successful to create new data after refreshing data, however when I killed the program and then reopened it, the data was gone. What did I overlook?</p>
<p>Here my startup code:</p>
<pre><code>func createDatabase() {
    ValueTransformer.setValueTransformer(UUIDTransformer(), forName: NSValueTransformerName(&quot;UUIDTransformer&quot;))
    ValueTransformer.setValueTransformer(TheCategoriesTransformer(), forName: NSValueTransformerName(&quot;TheCategoriesTransformer&quot;))
    
    do { theDailyData = try contextDataCore.fetch(The_daily_data.fetchRequest()) } catch { }
    do { theHabitList = try contextDataCore.fetch(The_habit_list.fetchRequest()) } catch { }
    do { thePhotoData = try contextDataCore.fetch(The_photos.fetchRequest()) } catch { }
}
</code></pre>
<p>Here my transformer code:</p>
<pre><code>class UUIDTransformer: ValueTransformer {
    override func transformedValue(_ value: Any?) -&gt; Any? {
        guard let uuid = value as? [UUID] else { return nil }

        do {
            let data = try NSKeyedArchiver.archivedData(withRootObject: uuid, requiringSecureCoding: true)
            return data
        } catch {
            return nil
        }

    }

    override func reverseTransformedValue(_ value: Any?) -&gt; Any? {
        guard let data = value as? Data else { return nil }

        do {
            let uuid = try NSKeyedUnarchiver.unarchivedObject(ofClass: NSArray.self, from: data) as! [UUID]
            return uuid
        } catch {
            return nil
        }

    }
}

class TheCategoriesTransformer: ValueTransformer {
    override func transformedValue(_ value: Any?) -&gt; Any? {
        guard let uuid = value as? [String] else { return nil }

        do {
            let data = try NSKeyedArchiver.archivedData(withRootObject: uuid, requiringSecureCoding: true)
            return data
        } catch {
            return nil
        }

    }

    override func reverseTransformedValue(_ value: Any?) -&gt; Any? {
        guard let data = value as? Data else { return nil }

        do {
            let categories = try NSKeyedUnarchiver.unarchivedObject(ofClass: NSArray.self, from: data) as! [String]
            return categories
        } catch {
            return nil
        }

    }
}
</code></pre>
<p>Create new data:</p>
<pre><code>func newJournalData(theDate: Date, theHabitID: UUID? = nil, theSubject: String? = nil, theJournal: String? = nil, theMind: String? = nil, theWeather: String = &quot;NA&quot;) {
    let repairDate = Calendar.current.dateComponents([.year, .month, .day], from: theDate)
    let freshDate = Calendar.current.date(from: repairDate)
    
    var newHabitID: [UUID] = []
    if theHabitID != nil { newHabitID.append(theHabitID!) }
    
    var theImpactCount = 0
    var theImpactTotal = 0
    
    let findHabitImpact = theHabitList.first(where: { $0.theID == theHabitID })
    
    for countX in theHabitList {
        switch countX.impact {
        case 0: theImpactTotal += 1
        case 1: theImpactTotal += 5
        case 2: theImpactTotal += 10
        default: theImpactTotal += 0
        }
    }
    
    if findHabitImpact != nil {
        switch findHabitImpact?.impact {
        case 0: theImpactCount += 1
        case 1: theImpactCount += 5
        case 2: theImpactCount += 10
        default: theImpactCount += 0
        }
    }
    
    let finalImpact = Int((Double(theImpactCount) / Double(theImpactTotal)) * 100)
    
    let newJournalData = The_daily_data(context: contextDataCore)
    newJournalData.date = freshDate
    newJournalData.habitData = newHabitID
    newJournalData.theSubject = theSubject
    newJournalData.journal = theJournal
    newJournalData.yourMind = theMind
    newJournalData.theWeather = theWeather
    newJournalData.percent = Int16(finalImpact)
    
    do { try contextDataCore.save(); createDatabase() } catch { print(&quot;Error&quot;) }
}
</code></pre>
<p>Screenshot:</p>
<p><a href=""https://i.sstatic.net/VFifw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VFifw.png"" alt=""enter image description here"" /></a>   <a href=""https://i.sstatic.net/EwYWz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EwYWz.png"" alt=""enter image description here"" /></a></p>
<p>I can see the array from habitData without any problems when I create the data. When the program is closed and reopened, just the habitData data is lost; all other data remains intact. After restarting the program, habitData always returns nil.</p>
","transformer-model"
"75522496","JOLT Transformation - Data grouping based on child array objects field","2023-02-21 15:22:32","75523274","0","149","<json><transformation><jolt><transformer-model>","<p>I received a json format and wanted to transform it in a format that will be easier to display in the UI whereby the grouping will be based on <code>levelOne</code> and <code>levelTwo</code> fields, which are inside the child array object.  I have tried as much as i can but nothing came close to the desired output I wanted.</p>
<p><strong>Input:</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;id&quot;: &quot;101&quot;,
  &quot;myLevel&quot;: &quot;Beginner&quot;,
  &quot;riskProducts&quot;: [
    {
      &quot;id&quot;: &quot;1&quot;,
      &quot;product&quot;: {
        &quot;id&quot;: &quot;2&quot;,
        &quot;levelOne&quot;: &quot;Equities&quot;,
        &quot;levelTwo&quot;: &quot;Global Developed Large Cap Equity&quot;,
        &quot;productName&quot;: &quot;Allianz Thematica Fund&quot;,
        &quot;proposedAmountToInvest&quot;: &quot;null&quot;,
        &quot;proposedHoldings&quot;: &quot;2&quot;,
        &quot;unitPrice&quot;: &quot;111.44&quot;
      }
    },
    {
      &quot;id&quot;: &quot;2&quot;,
      &quot;product&quot;: {
        &quot;id&quot;: &quot;1&quot;,
        &quot;levelOne&quot;: &quot;Equities&quot;,
        &quot;levelTwo&quot;: &quot;Global Developed Large Cap Equity&quot;,
        &quot;productName&quot;: &quot;AB Low Volatility Equity Portfolio&quot;,
        &quot;proposedAmountToInvest&quot;: &quot;null&quot;,
        &quot;proposedHoldings&quot;: &quot;2&quot;,
        &quot;unitPrice&quot;: &quot;150.18&quot;
      }
    },
    {
      &quot;id&quot;: &quot;4&quot;,
      &quot;product&quot;: {
        &quot;id&quot;: &quot;4&quot;,
        &quot;levelOne&quot;: &quot;Equities&quot;,
        &quot;levelTwo&quot;: &quot;US Large Cap Equity&quot;,
        &quot;productName&quot;: &quot;Franklin Technology Fund&quot;,
        &quot;proposedAmountToInvest&quot;: &quot;null&quot;,
        &quot;proposedHoldings&quot;: &quot;4&quot;,
        &quot;unitPrice&quot;: &quot;93.02&quot;
      }
    },
    {
      &quot;id&quot;: &quot;5&quot;,
      &quot;product&quot;: {
        &quot;id&quot;: &quot;5&quot;,
        &quot;levelOne&quot;: &quot;Fixed Income&quot;,
        &quot;levelTwo&quot;: &quot;Global Aggregate Bonds&quot;,
        &quot;productName&quot;: &quot;PIMCO GIS Income Fund HKD Inc&quot;,
        &quot;proposedAmountToInvest&quot;: &quot;null&quot;,
        &quot;proposedHoldings&quot;: &quot;7&quot;,
        &quot;unitPrice&quot;: &quot;11.93&quot;
      }
    }
  ]
}
</code></pre>
<p><strong>Desired Output:</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;myLevel&quot;: &quot;Beginner&quot;,
  &quot;levelOne&quot;: [
    {
      &quot;id&quot;: &quot;Equities&quot;,
      &quot;levelTwo&quot;: [
        {
          &quot;id&quot;: &quot;Global Developed Large Cap Equity&quot;,
          &quot;product&quot;: [
            {
              &quot;id&quot;: &quot;2&quot;,
              &quot;productName&quot;: &quot;Allianz Thematica Fund&quot;,
              &quot;proposedHoldings&quot;: &quot;2&quot;,
              &quot;unitPrice&quot;: &quot;111.44&quot;
            },
            {
              &quot;id&quot;: &quot;1&quot;,
              &quot;productName&quot;: &quot;AB Low Volatility Equity Portfolio&quot;,
              &quot;proposedHoldings&quot;: &quot;2&quot;,
              &quot;unitPrice&quot;: &quot;150.18&quot;
            }
          ],
          &quot;sPercentage&quot;: &quot;0&quot;,
          &quot;tPercentage&quot;: &quot;0&quot;
        },
        {
          &quot;id&quot;: &quot;US Large Cap Equity&quot;,
          &quot;product&quot;: [
            {
              &quot;id&quot;: &quot;4&quot;,
              &quot;productName&quot;: &quot;Franklin Technology Fund&quot;,
              &quot;proposedHoldings&quot;: &quot;4&quot;,
              &quot;unitPrice&quot;: &quot;93.02&quot;
            }
          ],
          &quot;sPercentage&quot;: &quot;0&quot;,
          &quot;tPercentage&quot;: &quot;0&quot;
        }
      ]
    },
    {
      &quot;id&quot;: &quot;Fixed Income&quot;,
      &quot;levelTwo&quot;: [
        {
          &quot;id&quot;: &quot;Global Aggregate Bonds&quot;,
          &quot;product&quot;: [
            {
              &quot;id&quot;: &quot;5&quot;,
              &quot;productName&quot;: &quot;PIMCO GIS Income Fund HKD Inc&quot;,
              &quot;proposedAmountToInvest&quot;: &quot;null&quot;,
              &quot;proposedHoldings&quot;: &quot;7&quot;,
              &quot;unitPrice&quot;: &quot;11.93&quot;
            }
          ],
          &quot;sPercentage&quot;: &quot;0&quot;,
          &quot;tPercentage&quot;: &quot;0&quot;
        }
      ]
    }
  ]
}
</code></pre>
<p>Appreciate the help and please do recommend a tutorial site where I would be able to understand better how the transformation works.</p>
","transformer-model"
"75520288","Jolt Transformer Grouping based on the fields inside the object","2023-02-21 12:05:00","75520751","1","38","<json><transform><jolt><transformer-model>","<p>I wanted to group the input array objects based on the fields <code>profileName</code>, <code>assetLevel1</code>, and <code>assetLevel2</code>.</p>
<p><strong>Input:</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;product&quot;: [
    {
      &quot;id&quot;: &quot;id1&quot;,
      &quot;entity&quot;: &quot;entity1&quot;,
      &quot;productID&quot;: &quot;productID1&quot;,
      &quot;productName&quot;: &quot;productName1&quot;,
      &quot;unitPrice&quot;: &quot;unitPrice1&quot;,
      &quot;assetLevel1&quot;: &quot;Equities&quot;,
      &quot;assetLevel2&quot;: &quot;US Large Cap Equity&quot;,
      &quot;profileName&quot;: &quot;Beginner Level&quot;
    },
    {
      &quot;id&quot;: &quot;id3&quot;,
      &quot;entity&quot;: &quot;entity3&quot;,
      &quot;productID&quot;: &quot;productID3&quot;,
      &quot;productName&quot;: &quot;productName3&quot;,
      &quot;unitPrice&quot;: &quot;unitPrice3&quot;,
      &quot;assetLevel1&quot;: &quot;Fixed Income&quot;,
      &quot;assetLevel2&quot;: &quot;Global Aggregate Funds&quot;,
      &quot;profileName&quot;: &quot;Novice Level&quot;
    },
    {
      &quot;id&quot;: &quot;id2&quot;,
      &quot;entity&quot;: &quot;entity2&quot;,
      &quot;productID&quot;: &quot;productID2&quot;,
      &quot;productName&quot;: &quot;productName2&quot;,
      &quot;unitPrice&quot;: &quot;unitPrice2&quot;,
      &quot;assetLevel1&quot;: &quot;Equities&quot;,
      &quot;assetLevel2&quot;: &quot;US Large Cap Equity&quot;,
      &quot;profileName&quot;: &quot;Beginner Level&quot;
    }
  ]
}
</code></pre>
<p><strong>My current spec:</strong></p>
<pre class=""lang-json prettyprint-override""><code>[
  {
    &quot;operation&quot;: &quot;shift&quot;,
    &quot;spec&quot;: {
      &quot;product&quot;: {
        &quot;*&quot;: &quot;@profileName.@assetLevel1[]&quot;
      }
    }
  }
]
</code></pre>
<p><strong>Desired output:</strong></p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;Beginner Level&quot;: {
    &quot;Equities&quot;: [
      {
        &quot;US Large Cap Equity&quot;: [
          {
            &quot;assetLevel1&quot;: &quot;Equities&quot;,
            &quot;assetLevel2&quot;: &quot;US Large Cap Equity&quot;,
            &quot;entity&quot;: &quot;entity1&quot;,
            &quot;id&quot;: &quot;id1&quot;,
            &quot;productID&quot;: &quot;productID1&quot;,
            &quot;productName&quot;: &quot;productName1&quot;,
            &quot;profileName&quot;: &quot;Beginner Level&quot;,
            &quot;unitPrice&quot;: &quot;unitPrice1&quot;
          },
          {
            &quot;assetLevel1&quot;: &quot;Equities&quot;,
            &quot;assetLevel2&quot;: &quot;US Large Cap Equity&quot;,
            &quot;entity&quot;: &quot;entity2&quot;,
            &quot;id&quot;: &quot;id2&quot;,
            &quot;productID&quot;: &quot;productID2&quot;,
            &quot;productName&quot;: &quot;productName2&quot;,
            &quot;profileName&quot;: &quot;Beginner Level&quot;,
            &quot;unitPrice&quot;: &quot;unitPrice2&quot;
          }
        ]
      }
    ]
  },
  &quot;Novice Level&quot;: {
    &quot;Fixed Income&quot;: [
      {
        &quot;Global Aggregate Funds&quot;: [
          {
            &quot;assetLevel1&quot;: &quot;Fixed Income&quot;,
            &quot;assetLevel2&quot;: &quot;Global Aggregate Funds&quot;,
            &quot;entity&quot;: &quot;entity3&quot;,
            &quot;id&quot;: &quot;id3&quot;,
            &quot;productID&quot;: &quot;productID3&quot;,
            &quot;productName&quot;: &quot;productName3&quot;,
            &quot;profileName&quot;: &quot;Novice Level&quot;,
            &quot;unitPrice&quot;: &quot;unitPrice3&quot;
          }
        ]
      }
    ]
  }
}
</code></pre>
<p>Can anyone help?</p>
<p>I have tried the above but unable to proceed as I am a newbie in this.</p>
","transformer-model"
"75498310","Are masks in Tensorflow automatically consumed by loss and metric?","2023-02-19 06:04:57","","2","222","<tensorflow><keras><mask><transformer-model><loss>","<p>This <a href=""https://stackoverflow.com/a/47060797/1516331"">answer</a> says:</p>
<blockquote>
<p>If there's a mask in your model, it'll be propagated layer-by-layer
and eventually applied to the loss. So if you're padding and masking
the sequences in a correct way, the loss on the padding placeholders
would be ignored.</p>
</blockquote>
<p>However in <a href=""https://www.tensorflow.org/text/tutorials/transformer#set_up_the_loss_and_metrics"" rel=""nofollow noreferrer"">TensorFlow's tutorial on Transformers</a>, the author has implemented custom <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/losses/Loss"" rel=""nofollow noreferrer"">loss</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric"" rel=""nofollow noreferrer"">metric</a> where masks are computed and applied internally. Is this necessary?</p>
<p>Note in the <a href=""https://www.tensorflow.org/text/tutorials/transformer#the_transformer"" rel=""nofollow noreferrer"">code of the Transformer model</a>, the author has deleted the keras mask:</p>
<pre><code>    ....
    ....

    try:
      # Drop the keras mask, so it doesn't scale the losses/metrics.
      # b/250038731
      del logits._keras_mask
    except AttributeError:
      pass

    # Return the final output and the attention weights.
    return logits
</code></pre>
<p>Do we need to implement a custom loss and metric with mask, or we can use the built-in ones?</p>
","transformer-model"
"75496527","Keras Transformer - Test Loss Not Changing","2023-02-18 21:34:32","","2","230","<python><tensorflow><machine-learning><keras><transformer-model>","<p>I'm trying to create a small transformer model with Keras to model stock prices, based off of <a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">this tutorial</a> from the Keras docs. The problem is, my test loss is massive and barely changes between epochs, unsurprisingly resulting in severe underfitting, with my outputs all the same arbitrary value.</p>
<p>My code is below:</p>
<pre><code>def transformer_encoder_block(inputs, head_size, num_heads, filters, dropout=0):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=filters, kernel_size=1, activation=&quot;relu&quot;)(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


data = ...

input = np.array(
    keras.preprocessing.sequence.pad_sequences(data[&quot;input&quot;], padding=&quot;pre&quot;, dtype=&quot;float32&quot;))

output = np.array(
    keras.preprocessing.sequence.pad_sequences(data[&quot;output&quot;], padding=&quot;pre&quot;, dtype=&quot;float32&quot;))

# Input shape: (723, 36, 22)
# Output shape: (723, 36, 1)

# Train data
train_features = input[100:]
train_labels = output[100:]

train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=3)

# Test data
test_features = input[:100]
test_labels = output[:100]

test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=3)



inputs = keras.Input(shape=(None,22), dtype=&quot;float32&quot;, name=&quot;inputs&quot;)

# Ignore padding in inputs
x = layers.Masking(mask_value=0)(inputs)

x = transformer_encoder_block(x, head_size=64, num_heads=16, filters=3, dropout=0.2)

# Multiclass = Softmax (decrease, no change, increase)
outputs = layers.TimeDistributed(layers.Dense(3, activation=&quot;softmax&quot;, name=&quot;outputs&quot;))(x)

# Create model
model = keras.Model(inputs=inputs, outputs=outputs)

# Compile model
model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=(tf.keras.optimizers.Adam(learning_rate=0.005)), metrics=['accuracy'])

# Train model
history = model.fit(train_features, train_labels, epochs=10, batch_size=32)

# Evaluate on the test data
test_loss = model.evaluate(test_features, test_labels, verbose=0)
print(&quot;Test loss:&quot;, test_loss)

out = model.predict(test_features)
</code></pre>
<p>After padding, <code>input</code> is of shape <code>(723, 36, 22)</code>, and <code>output</code> is of shape <code>(723, 36, 1)</code> (before converting output to one hop, after which there are 3 output classes).</p>
<p>Here's an example output for ten epochs (trust me, more than ten doesn't make it better):</p>
<pre><code>Epoch 1/10
20/20 [==============================] - 2s 62ms/step - loss: 10.7436 - accuracy: 0.3335
Epoch 2/10
20/20 [==============================] - 1s 62ms/step - loss: 10.7083 - accuracy: 0.3354
Epoch 3/10
20/20 [==============================] - 1s 60ms/step - loss: 10.6555 - accuracy: 0.3392
Epoch 4/10
20/20 [==============================] - 1s 62ms/step - loss: 10.7846 - accuracy: 0.3306
Epoch 5/10
20/20 [==============================] - 1s 60ms/step - loss: 10.7600 - accuracy: 0.3322
Epoch 6/10
20/20 [==============================] - 1s 59ms/step - loss: 10.7074 - accuracy: 0.3358
Epoch 7/10
20/20 [==============================] - 1s 59ms/step - loss: 10.6569 - accuracy: 0.3385
Epoch 8/10
20/20 [==============================] - 1s 60ms/step - loss: 10.7767 - accuracy: 0.3314
Epoch 9/10
20/20 [==============================] - 1s 61ms/step - loss: 10.7346 - accuracy: 0.3341
Epoch 10/10
20/20 [==============================] - 1s 62ms/step - loss: 10.7093 - accuracy: 0.3354
Test loss: [10.073813438415527, 0.375]
4/4 [==============================] - 0s 22ms/step
</code></pre>
<p>Using the same data on a simple LSTM model with the same shape yielded a desirable prediction with a constantly decreasing loss.</p>
<p>Tweaking the learning rate appears to have no effect, nor does stacking more <code>transformer_encoder_block()</code>s.</p>
<p>If anyone has any suggestions for how I can solve this, please let me know.</p>
","transformer-model"
"75474086","How to train FLAN-T5 to summarization task with a custom dataset of legal documents in pt-br?","2023-02-16 14:58:59","75538540","0","3615","<python><nlp><transformer-model><summarization>","<p>So, I would like to create a small proof-of-concept using (already extracted in txt files) +- 4.000 legal text divided in:</p>
<ol>
<li>2.000 initial petitions / complaints *.txt files</li>
<li>2.000 summaries of each initial petition (txt files too)</li>
</ol>
<p>PS.: <strong>all text files are in brazilian portuguese (pt-br)</strong></p>
<p>So how can I use these txt files to train a new transformer able to generate new summaries (using flan-t5) ?</p>
","transformer-model"
"75432369","In the original T5 paper, what does 'step' mean?","2023-02-13 06:09:36","","0","289","<nlp><bert-language-model><transformer-model>","<p>I have been reading the original T5 paper 'Exploring the limits of transfer learning with a unified text-to-text transformer.' On page 11, it says &quot;We pre-train each model for 2^19=524,288 <strong>steps</strong> on C4 before fine-tuning.&quot;</p>
<p>I am not sure what the '<strong>steps</strong>' mean. Is it the same as epochs? Or the number of iterations per epoch?</p>
<p>I guess 'steps'='iterations' in a single epoch.</p>
","transformer-model"
"75391230","How to access the value projection at MultiHeadAttention layer in Pytorch","2023-02-08 20:10:14","","1","168","<python><pytorch><transformer-model><self-attention><multihead-attention>","<p>I'm making an own implementation for the <a href=""https://arxiv.org/abs/2106.05234"" rel=""nofollow noreferrer"">Graphormer</a> architecture. Since this architecture needs to add an edge-based bias to the output for the key-query multiplication at the self-attention mechanism I am adding that bias by hand and doing the matrix multiplication for the data with the attention weights outside the attention mechanism:</p>
<pre><code>import torch as th
from torch import nn


# Variable inicialization
B, T, C, H = 2, 3, 4, 2
self_attn = nn.MultiheadAttention(C, H, batch_first = True)

# Tensors
x = th.randn(B, T, C)
attn_bias = th.ones((B, T, T))

#  Self-attention mechanism
_, attn_wei = self_attn(query=x, key=x, value=x)

# Adding attention bias
if attn_bias is not None:
    attn_wei = attn_wei + attn_bias

x = attn_wei @ x # TODO use value(x) instead of x

print(x)
</code></pre>
<p>This works, but for using the full potential of self-attention, the last matrix multiplication should be like <code>x = attn_wei @ value(x)</code> but I am not able to get the value projector from the <code>selt_attn</code> object as it should have something like that inside of it.</p>
<p>How could I do this?</p>
","transformer-model"
"75374973","How to retrieve attention weight alignment for tokens using transformer (BERT) model?","2023-02-07 14:45:28","","2","1733","<pytorch><huggingface-transformers><bert-language-model><text-classification><transformer-model>","<p>I am working on text classification with transformer models (PyTorch, Huggingface, running on GPU).</p>
<p>I have already my model and my training loop and it works fine but to better understand wrong predictions of the model, I want to &quot;dig a little deeper&quot; and look at the attention weights the model gave the tokens of the misclassified predictions (for evaluation during training and later for predictions on the test set).</p>
<p>I tried already so many things and I already found a way the model outputs attention weights (will attach an example), BUT a) it outputs too many attention weights for each misclassified example (can it be because it gives the weights for several layers?) and b) the attention weights are not connected to the tokens. I want it in a more interpretable way, like coloring the tokens in a darker color the more weight is on that token.</p>
<p>This is my code so far (irrelevant snippets left out):</p>
<pre><code>model = AutoModelForSequenceClassification.from_pretrained(checkpoint,
                                                      num_labels=len(label_dict),
                                                      output_attentions=True,
                                                      output_hidden_states=True)
</code></pre>
<pre><code>def get_wrong_predictions(predictions, true_vals):
    wrong_predictions = []
    for i in range(len(true_vals)):
        if np.argmax(predictions[i]) != true_vals[i]:
            wrong_predictions.append((true_vals[i], np.argmax(predictions[i])))
    return wrong_predictions
</code></pre>
<pre><code>seed_val = 17
random.seed(seed_val)
np.random.seed(seed_val)
torch.manual_seed(seed_val)
torch.cuda.manual_seed_all(seed_val)

def evaluate(dataloader_val):

    model.eval()
    
    loss_val_total = 0
    predictions, true_vals = [], []
    attentions = []
    
    for batch in dataloader_val:
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }

        with torch.no_grad():        
            outputs = model(**inputs)
            
        loss = outputs[0]
        logits = outputs[1]
        attention_scores = outputs.attentions[-1].detach().cpu().numpy() # extract the attention scores from the last layer
        loss_val_total += loss.item()

        logits = logits.detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)
        attentions.append(attention_scores)
    
    loss_val_avg = loss_val_total/len(dataloader_val) 
    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
    attentions = np.concatenate(attentions, axis=0)
    
    preds = np.argmax(predictions, axis=1)
    misclassified = np.where(preds != true_vals)[0]

    texts = df[&quot;description&quot;].tolist()
    
    global misclassified_examples
    misclassified_examples = []
    #misclassified = []

    for idx in misclassified[:min(len(attention_scores), len(misclassified))]:
        text = texts[idx]
        true_label = true_vals[idx]
        pred_label = preds[idx]
        attention_weights = attentions[idx] # get the attention weights for the misclassified examples
        misclassified_examples.append({
            'text': text,
            'true_label': true_label,
            'pred_label': pred_label,
            'attention_weights' : attention_weights
        })
            
    return loss_val_avg, predictions, true_vals

    
train_losses = [] #to plot later
val_losses = []
overall_accuracy = []
    
for epoch in tqdm(range(1, epochs+1)):
    
    model.train()
    
    loss_train_total = 0

    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)
    for batch in progress_bar:

        model.zero_grad()
        
        batch = tuple(b.to(device) for b in batch)
        
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2],
                 }       

        outputs = model(**inputs)
        
        loss = outputs[0]
        loss_train_total += loss.item()
        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        optimizer.step()
        scheduler.step()
        
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})
    
    loss_train_avg = loss_train_total/len(dataloader_train) #to plot later
    train_losses.append(loss_train_avg) # to plot later

    
        
    tqdm.write(f'\nEpoch {epoch}')
    
    loss_train_avg = loss_train_total/len(dataloader_train)            
    tqdm.write(f'Training loss: {loss_train_avg}')
    
    val_loss, predictions, true_vals = evaluate(dataloader_validation)
    val_f1 = f1_score_func(predictions, true_vals)
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (Weighted): {val_f1}')
    
    val_overallaccuracy = overallaccuracy(true_vals, predictions)
    tqdm.write(f'Overall Accuracy: {val_overallaccuracy}')
    
    val_mcc = mcc_score_func(true_vals, predictions)
    tqdm.write(f'MCC: {val_mcc}')
    
    val_acc = accuracy_per_class(predictions, true_vals)
    tqdm.write(f'Accuracy per class: {val_acc}')
    
    val_losses.append(val_loss)
    overall_accuracy.append(val_overallaccuracy)

</code></pre>
<p>The image shows exemplarily what kind of output I get, but that is &quot;too much&quot; attention as I expected one number for one token and then of course some way to relate that number of attention weight back to that token.</p>
<p>[<a href=""https://i.sstatic.net/hTc6A.png"" rel=""nofollow noreferrer"">enter image description here</a>](<a href=""https://i.sstatic.net/ICmvp.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/ICmvp.png</a>)</p>
<p>How  could I adjust my code to include attention weight alignment for the tokens?</p>
","transformer-model"
"75305169","Decoding hidden layer embeddings in T5","2023-02-01 02:49:47","77034052","4","1662","<python><machine-learning><nlp><huggingface-transformers><transformer-model>","<p>I'm new to NLP (pardon the very noob question!), and am looking for a way to perform vector operations on sentence embeddings (e.g., randomization in embedding-space in a uniform ball around a given sentence) and then decode them. I'm currently attempting to use the following strategy with T5 and Huggingface Transformers:</p>
<ol>
<li>Encode the text with <code>T5Tokenizer</code>.</li>
<li>Run a forward pass through the encoder with <code>model.encoder</code>. Use the last hidden state as the embedding. (I've tried <code>.generate</code> as well, but it doesn't allow me to use the decoder separately from the encoder.)</li>
<li>Perform any desired operations on the embedding.</li>
<li><strong>The problematic step: Pass it through <code>model.decoder</code> and decode with the tokenizer.</strong></li>
</ol>
<p>I'm having trouble with (4). My sanity check: I set (3) to do nothing (no change to the embedding), and I check whether the resulting text is the same as the input. So far, that check always fails.</p>
<p>I get the sense that I'm missing something rather important (something to do with the lack of beam search or some other similar generation method?). I'm unsure of whether what I think is an embedding (as in (2)) is even correct.</p>
<p><strong>How would I go about encoding a sentence embedding with T5, modifying it in that vector space, and then decoding it into generated text?</strong> Also, might another model be a better fit?</p>
<p>As a sample, below is my incredibly broken code, based on <a href=""https://huggingface.co/blog/encoder-decoder#encoder-decoder"" rel=""nofollow noreferrer"">this</a>:</p>
<pre class=""lang-py prettyprint-override""><code>t5_model = transformers.T5ForConditionalGeneration.from_pretrained(&quot;t5-large&quot;)
t5_tok = transformers.T5Tokenizer.from_pretrained(&quot;t5-large&quot;)
text = &quot;Foo bar is typing some words.&quot;
input_ids = t5_tok(text, return_tensors=&quot;pt&quot;).input_ids
encoder_output_vectors = t5_model.encoder(input_ids, return_dict=True).last_hidden_state
# The rest is what I think is problematic:
decoder_input_ids = t5_tok(&quot;&lt;pad&gt;&quot;, return_tensors=&quot;pt&quot;, add_special_tokens=False).input_ids
decoder_output = t5_model.decoder(decoder_input_ids, encoder_hidden_states=encoder_output_vectors)
t5_tok.decode(decoder_output.last_hidden_state[0].softmax(0).argmax(1))
</code></pre>
","transformer-model"
"75258263","Transformer Positional Encoding -- What is maxlen used for","2023-01-27 12:43:07","75258482","0","313","<python><machine-learning><pytorch><transformer-model>","<pre><code>class PositionalEncoding(nn.Module):
    def __init__(self,
    emb_size: int,
    dropout: float,
    maxlen: int = 5000):
        super(PositionalEncoding, self).__init__()
        den = torch.exp(- torch.arange(0, emb_size, 2)\* math.log(10000) / emb_size)
        pos = torch.arange(0, maxlen).reshape(maxlen, 1)
        pos_embedding = torch.zeros((maxlen, emb_size))
        pos_embedding\[:, 0::2\] = torch.sin(pos \* den)
        pos_embedding\[:, 1::2\] = torch.cos(pos \* den)
        pos_embedding = pos_embedding.unsqueeze(-2)

        self.dropout = nn.Dropout(dropout)
        self.register_buffer('pos_embedding', pos_embedding)
    
    def forward(self, token_embedding: Tensor):
        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])
</code></pre>
<p>This code is in <a href=""https://pytorch.org/tutorials/beginner/translation_transformer.html"" rel=""nofollow noreferrer"">here</a></p>
<p>I know what positional encoding is used for, but is maxlen a constant value? Or does it vary depending on the batch size or the length of the data?</p>
<p>An example from NLP:</p>
<pre><code>  [data_lenght, bacth_size]

  [256, 64]

  64*256 = 16,384 variables are obtained.
</code></pre>
<p>What I don't understand here is the 5000 maxlen value used in positional encoding has something to do with it.</p>
<p>Am I using it wrong?</p>
<p>Should maxlen be changed according to the example I gave?</p>
","transformer-model"
"75247437","ValueError: text input must of type `str` (single example)","2023-01-26 14:24:45","","2","9453","<python><pytorch><huggingface-transformers><transformer-model>","<p>I am trying to run the evaluation of both MCLIP and ItalianCLIP on zero-shot learning task found this notebook <a href=""https://colab.research.google.com/drive/1zfWeVWY79XXH63Ci-pk8xxx3Vu_RRgW-?usp=sharing"" rel=""nofollow noreferrer"">colab</a>. when running the below prediction cell I get the below error</p>
<pre><code>---------------------------------------------------------------------------

/opt/conda/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py in encode(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)
    159         for start_index in trange(0, len(sentences), batch_size, desc=&quot;Batches&quot;, disable=not show_progress_bar):
    160             sentences_batch = sentences_sorted[start_index:start_index+batch_size]
--&gt; 161             features = self.tokenize(sentences_batch)
    162             features = batch_to_device(features, device)
    163 

/opt/conda/lib/python3.7/site-packages/sentence_transformers/SentenceTransformer.py in tokenize(self, texts)
    317         Tokenizes the texts
    318         &quot;&quot;&quot;
--&gt; 319         return self._first_module().tokenize(texts)
    320 
    321     def get_sentence_features(self, *features):

/opt/conda/lib/python3.7/site-packages/sentence_transformers/models/CLIPModel.py in tokenize(self, texts)
     69             images = None
     70 
---&gt; 71         inputs = self.processor(text=texts_values, images=images, return_tensors=&quot;pt&quot;, padding=True)
     72         inputs['image_text_info'] = image_text_info
     73         return inputs

/opt/conda/lib/python3.7/site-packages/transformers/models/clip/processing_clip.py in __call__(self, text, images, return_tensors, **kwargs)
     97 
     98         if text is not None:
---&gt; 99             encoding = self.tokenizer(text, return_tensors=return_tensors, **kwargs)
    100 
    101         if images is not None:

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in __call__(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   2521             if not self._in_target_context_manager:
   2522                 self._switch_to_input_mode()
-&gt; 2523             encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
   2524         if text_target is not None:
   2525             self._switch_to_target_mode()

/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in _call_one(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)
   2580         if not _is_valid_text_input(text):
   2581             raise ValueError(
-&gt; 2582                 &quot;text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) &quot;
   2583                 &quot;or `List[List[str]]` (batch of pretokenized examples).&quot;
   2584             )

ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).

</code></pre>
<p>How can this be fixed?</p>
","transformer-model"
"75242958","SetFit Model training on a custom dataset","2023-01-26 06:55:12","","0","384","<python><nlp><huggingface-transformers><transformer-model>","<p>I've a dataframe with 3 columns and I'm trying to perform few shot text classification using SetFit model</p>
<p>Dataframe (df)</p>
<pre><code>           A                   B                      C
0    Lorem ipsum ta      lorem ipsum                  Yes
1    Excepteur sint      occaecat excepteur           No
2    Duis aute irure     aute irure                   Yes
</code></pre>
<p>The traditional SetFit model accepts two inputs i.e. Text and label</p>
<p>I want to train the model with an extra input as well i.e. 2 inputs (A,B) to predict the label (C).</p>
<p>And I'm not sure how to apply the SetFit model from Transformers library to do this</p>
<p>I am trying to search how to do this and trying different code pieces but in the meanwhile I would appreciate any guidance, Thank you in advance!</p>
","transformer-model"
"75227030","Getting an embedded output from huggingface transformers","2023-01-24 20:37:22","75239038","1","498","<nlp><huggingface-transformers><transformer-model><roberta-language-model>","<p>To compare different paragraphs, I am trying to use a transformer model, fitting each paragraph onto the model and then in the end I intend to compare the outputs and see which paragraph has the most similarity.</p>
<p>For this purpose, I am using Roberta-base model. I first used roberta tokenizer on a paragraph. Then I used the roberta model on that tokenized output. But the process is failing due to lack of memory. Even 25GB ram is not enough to complete the process for the paragraphs with 1324 lines.</p>
<p>Any idea how can I make it better or any suggestion what mistakes i might be doing?</p>
<pre><code>from transformers import RobertaTokenizer, RobertaModel
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
tokenizer = RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;)

model = RobertaModel.from_pretrained(&quot;roberta-base&quot;).to(device)

inputs = tokenizer(dict_anrika['Anrika'], return_tensors=&quot;pt&quot;, truncation=True, 
padding=True).to(device)
outputs = model(**inputs)
</code></pre>
","transformer-model"
"75224660","Visualisation Attentions Transformer-based Architecture","2023-01-24 16:48:44","","1","129","<visualization><transformer-model>","<p>I have a transformer based architecture, specifically I have both an encoder using Vision Transformers and a decoder using Language Models based on Transformers.
We know that Transformers have a specific attention mechanism named 'Self-attention'.
I was wondering, having a trained model (I am using PyTorch), what should I visualise to understand what part of the input is the most important? I suppose I have to visualise the weights of the Self-attention layer to understand what is more important for the prediction.</p>
<p>Here I report a part of the model that is explicative:</p>
<pre><code>ViTLayer(
          (attention): ViTAttention(
            (attention): ViTSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=False)
              (key): Linear(in_features=768, out_features=768, bias=False)
              (value): Linear(in_features=768, out_features=768, bias=False)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): ViTSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (intermediate): ViTIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): ViTOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
   )
</code></pre>
<p>I suppose I should visualise the <code>VitSelfOutput</code> Linear layer weights to understand what part of the input image is useful for the prediction,
am I right?</p>
<p>Do you have any suggestions?</p>
<p>Thanks for the help,
S</p>
","transformer-model"
"75215801","PyTorch Transformer Encoder masking sequence values","2023-01-23 22:43:01","","0","817","<python><pytorch><transformer-model>","<p>It was my understanding that in the PyTorch <code>TransformerEncoder</code> I can pass a mask which would then stop certain features being attended to. By doing so, the model learns to &quot;fill in the gaps&quot; and becomes more resilient to noise and overfitting.</p>
<p>In the documentation for <code>TransformerEncoder</code> which I'm using, it is described as a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html"" rel=""nofollow noreferrer""><code>mask</code></a> without any details. The other class I use <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html"" rel=""nofollow noreferrer"">TransformerEncoderLayer</a> also doesn't describe it in detail, only the page about <a href=""https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html"" rel=""nofollow noreferrer"">torch.nn.Transformer</a> actually goes into some detail.</p>
<p>If I pass a mask which is <code>[B, S]</code> where <code>B</code> is batch, and <code>S</code> is the sequence, I'd be masking <em>which</em> part of the sequence to mask.</p>
<p>Alas, trying that ends in an error which says that the mask needs to be <code>[S,S]</code> which I don't get why. Going through the internet, this doesn't seem to be asked much, and only this <a href=""https://stats.stackexchange.com/questions/598239/how-is-padding-masking-considered-in-the-attention-head-of-a-transformer"">question on cross-validated</a> seems to get into it, but without the information I needed.</p>
<p>Aside from using the <code>padding</code> mask (which I'd normally use to avoid attending to padded or null values of my Sequence) how can I realistically pass a mask which will tell the model to ignore certain parts of the input sequence (per Batch or per Sequence)?</p>
","transformer-model"
"75189397","Getting the query, key and value matrices from PyTorch with self_attn.in_proj_weight","2023-01-20 21:15:20","","1","945","<pytorch><tensor><transformer-model>","<p>We have implemented a transformer based on the tutorial <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">here</a>.</p>
<p>We need to access the weights of the query, key and value matrices and were planning on doing this with <code>model.state_dict()</code>. However the model stores these matrices as a concatenation in this shared matrix.</p>
<pre><code>model.state_dict()['transformer_encoder.layers.0.self_attn.in_proj_weight']
</code></pre>
<p>We would assume that they are concatenated in the order query, key, value. If so we can just split the tensor manually. However, we were unable to verify in the PyTorch documentation whether this is the actual order. Is there an easy way to verify whether this is the case? Or any other way to get the query, key and value matrices individually for this transformer model?</p>
","transformer-model"
"75168973","ValueError Getting Emission from Wav2Vec2 PyTorch Pipeline Model","2023-01-19 07:33:37","","0","110","<pytorch><speech-recognition><speech-to-text><transformer-model><torchaudio>","<p>When calling</p>
<pre><code>model = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H.get_model()
emission = model(data)
</code></pre>
<p>This is to get the emission probabilities from the model.</p>
<p>but I get</p>
<pre><code>File &quot;XXX\lib\site-packages\torch\nn\modules\module.py&quot;, line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;XXX\lib\site-packages\torchaudio\models\wav2vec2\model.py&quot;, line 119, in forward
    x, lengths = self.feature_extractor(waveforms, lengths)
  File &quot;XXX\lib\site-packages\torch\nn\modules\module.py&quot;, line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;XXX\lib\site-packages\torchaudio\models\wav2vec2\components.py&quot;, line 135, in forward
    raise ValueError(&quot;Expected the input Tensor to be 2D (batch, time), &quot; &quot;but received {list(x.shape)}&quot;)
ValueError: Expected the input Tensor to be 2D (batch, time), but received {list(x.shape)}
</code></pre>
","transformer-model"
"75151382","Transformer Neural Network architecture question - query, key and value matrices","2023-01-17 19:29:24","75226097","0","257","<matrix><neural-network><transformer-model>","<p>please help me to understand query, key and value matrices in transformer architecture. How many query, key and value matrices should be in one encoder? For example I have only one head of attention and ten embeddings, does it mean that it should be only one set of the matrices (saying &quot;set&quot; I mean three matrices - query, key and value matrix), or it means that each of ten embedding shoud have its own set of the matrices?</p>
<p>I tried to create encoder on &quot;processing&quot; programming language with one set of the matrices, the results of the output are strange (looks like the resulting vectors are almost identical, but they should be different), so I came here to clarify the theory, maybe I'm doing something wrong.</p>
","transformer-model"
"75144329","what is the difference between sklearn.model_selection.train_test_split and torch.utils.data.random_split?","2023-01-17 09:30:22","","1","294","<python><scikit-learn><pytorch><transformer-model>","<p>I am using <strong>transformer</strong> to do a speech classification task.</p>
<p>I used two methods to split <strong>my_dataset</strong> into training set and test set.</p>
<p>The first is torch.utils.data.random_split:</p>
<pre><code>train_len = int(0.9 * len(my_dataset))

lengths = [train_len , len(my_dataset) - train_len]

train_set, valid_set = random_split(my_dataset, lengths)
</code></pre>
<p>The second is sklearn.model_selection.train_test_split:</p>
<pre><code>train_set, valid_set = train_test_split(my_dataset, test_size=0.1)
</code></pre>
<p>I have tries many times. When I use the first method, the accuracy rate is always 60%, but when I use the second method, the accuracy rate is only 55%.</p>
<p>So what is the difference between sklearn.model_selection.train_test_split and torch.utils.data.random_split?</p>
<p><strong>The two methods are only different in the way the data set is divided, and the others are the same.</strong></p>
","transformer-model"
"75120799","Can I apply both absolute and relative positional encoding to a Transformer model?","2023-01-14 19:49:30","","0","551","<deep-learning><encoding><embedding><transformer-model>","<p>I'm trying to make my own Transformer model. I found that the currently popular models have abandoned absolute position encoding and adopted relative position encoding. However, others told me that I could use both methods of position encoding. Is this OK? Will this hurt the model? Thanks!</p>
<p>I tried to find relevant information online, but there was no apparent result.</p>
","transformer-model"
"75103200","I lose microseconds when Timestamp passes through my IRestResponseTransformer in Java SpringBoot","2023-01-12 23:01:51","","1","76","<java><json><timestamp><format><transformer-model>","<p>i will try to be more clear as possible. My project uses 2 containers to execute a service and get data from my DB2 . The first container passes input data through an IRestRequestTransformer that convert my data to be readable by the second container. The second container execute a query on my DB2 and extract the data I need. This data returns to the first container through an IRestResponseTransformer. In my case the data i'm trying to get is a Timestamp with microseconds precision that for example will be &quot;2022-09-23 11.25.52.660135&quot; . I can see that this Timestamp is correct when the query is executed on the second container :</p>
<p><a href=""https://i.sstatic.net/ETmeG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ETmeG.png"" alt=""The parameter is called timestampInsB03 in this case"" /></a></p>
<p>but when it passes through the IRestResponseTransformer it gets cut and on my first container i only get &quot;2022-09-23 11.25.52.660&quot; without microseconds:</p>
<p><a href=""https://i.sstatic.net/vHr4i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vHr4i.png"" alt=""timestampInsB03 here too"" /></a></p>
<p>The model of the table contains the mapping of all the columns with getter and setter like this :</p>
<pre><code>@Column(name = &quot;TIMESTAMP_INS_B03&quot;)
private Timestamp timestampB03;

public Timestamp getTimestampInsB03() {
        return timestampInsB03;
    }

    public void setTimestampInsB03(Timestamp timestampInsB03) {
        this.timestampInsB03 = timestampInsB03;
    }

</code></pre>
<p>I work with json and I wonder if exist an annotation like JsonFormat to specify that I need to keep microseconds or something similar. If you need more informations tell me and I will answer.</p>
","transformer-model"
"75070730","Masking the input to model as a graph","2023-01-10 13:32:45","","2","56","<huggingface-transformers><transformer-model>","<p>I was wondering if it is possible to mask the input in order to get attention like in GNNs. Specifically, if there was a way to send an attention mask in 2D we could tell a token to attend just to its neighbours and not the other tokens. Is this behaviour possible for example using a pretrained language model from Huggingface?</p>
","transformer-model"
"75059015","Problem with trained model and load model","2023-01-09 14:52:15","75068230","0","132","<speech-recognition><huggingface-transformers><transformer-model>","<p>I'm trying to create model from <code>wav2vec2</code> with <code>facebook/wav2vec2-base-960h</code> pretrained model and this is my <code>training_args</code></p>
<pre><code>from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir=save_dir,
    group_by_length=True,
    per_device_train_batch_size=10,
    per_device_eval_batch_size=10,
    gradient_accumulation_steps=2,
    evaluation_strategy=&quot;steps&quot;,
    num_train_epochs=0.5,
    fp16=True,
    save_steps=10,
    eval_steps=10,
    logging_steps=10,
    learning_rate=1e-4,
    warmup_steps=500,
    save_total_limit=2,
)
</code></pre>
<p>and this is my <code>trainer</code></p>
<pre><code>from transformers import Trainer

trainer = Trainer(
    model=model,
    data_collator=data_collator,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=_common_voice_train,
    eval_dataset=_common_voice_test,
    tokenizer=processor.feature_extractor,
)
</code></pre>
<p>now when the training part is over and model trained the <code>trainer.evaluate()</code> part show me the good result like this</p>
<blockquote>
<p>reference: &quot;شما امروز صبوری بفرمایین ثبت شده تا امروز با شما هماهنگی انجام بشه&quot;
<br/>predicted: &quot;شما امروز سبوری بفرمای سبز شده تا امروز با شما همهمنگی انجام باشه&quot;</p>
</blockquote>
<p>but when I'm trying to load and use the model I got this</p>
<blockquote>
<p>رچسصجپ هدثج یو تو یتنپ هر وغسهروغج سچ ثزتسه شتذس صمرجچو</p>
</blockquote>
<p>I load my model like this</p>
<pre><code>sample_rate = 16_000

model = Wav2Vec2ForCTC.from_pretrained(&quot;/content/drive/MyDrive/model&quot;)
processor = Wav2Vec2Processor.from_pretrained(&quot;/content/drive/MyDrive/model&quot;)
audio_input, sample_rate = librosa.load(&quot;/content60_L4.wav&quot;, sr=sample_rate)
input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=&quot;pt&quot;).input_values
logits = model(input_values).logits
predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.decode(predicted_ids[0])
</code></pre>
<p>I can't find my mistake</p>
","transformer-model"
"75055543","AWS Sagemaker T5 or huggingface Model training issue","2023-01-09 09:51:52","","0","93","<amazon-web-services><amazon-sagemaker><huggingface-transformers><transformer-model>","<p>I am trying to train a t5 conditional Generation model in Sagemaker, its running fine when I am passing the arguments directly in notebook but its not learning anything when I am passing estimator and train.py script, I followed the documentation provided by hugging face as well as AWS. But still we are facing issue it is saying training is completed and saving model with in 663 seconds what ever might be the size of dataset. Kindly give suggestions for this.</p>
","transformer-model"
"75042987","Time-Series Transformer Model Prediction Accuracy","2023-01-07 18:55:31","","0","1000","<tensorflow><keras><deep-learning><time-series><transformer-model>","<p>I have created a transformer model for multivariate time series predictions for a linear regression problem.</p>
<p><strong>Details about the Dataset</strong></p>
<p>I have the hourly varying data i.e., single feature (lagged energy use data). The model improvement could be done by increasing the number of lagged energy use data, which provide more information to the model)  to predict the time sequence (energy consumption of a building). So my input has the shape <code>X.shape = (8783, 168, 1) </code>i.e., 8783 time sequences, each sequence contains lagged energy use data of one week i.e., <code>24*7 =168</code> hourly entries/vectors and each vector contains lagged energy use data as input. My output has the shape <code>Y.shape = (8783,1) </code>i.e., 8783 sequences each containing 1 output value (i.e., building energy consumption value after every hour).</p>
<p><strong>Model Details</strong></p>
<p>I took as a model an example from the official keras site. It is created for classification problems, I modified it for my regression problem by changing the activation of last output layer from sigmoid to relu. Input shape <code>(train_f) = (8783, 168, 1)</code> Output shape <code>(train_P) = (8783,1)</code> When I trained the model for 100 no. of epochs it converges very well for less number of epochs as compared to my reference models (i.e., LSTMs and LSTMS with self attention). After training, when the model is asked to make prediction by feeding in the test data, the prediction performance is also good as compare to the reference models.</p>
<p>For the same model predicting well, in order to improve its performance now I am feeding in the lagged data of energy use of 1 month i.e., <code>168*4 = 672</code> hourly entries/vectors and each vector contains lagged energy use data as input. So my input going into the model now has the shape <code>X.shape = (8783, 672, 1)</code>. Both the training and prediction accuracy drops in comparison to weekly input data as seen below.</p>
<pre><code>**lagged energy use data for 1 week i.e., X.shape = (8783, 168, 1)**
            **MSE    RMSE     MAE   R-Score**
Training data   1.0489  1.0242  0.6395  0.9707
Testing data    0.6221  0.7887  0.5648  0.9171

**lagged energy use data for 1 week i.e., X.shape = (8783, 672, 1)**
                **MSE    RMSE     MAE   R-Score**
Training data   1.6424  1.2816  0.7326  0.9567
Testing  data   1.4991  1.2244  0.9233  0.6903
</code></pre>
<p>I believe that providing more information to the model should result in better predictions. Any suggestions, how to improve the model prediction/test accuracy? Is there something wrong with the model?</p>
<pre><code>
df_energy = pd.read_excel(&quot;/content/drive/MyDrive/Architecture Topology/Building_energy_consumption_record.xlsx&quot;)
extract_for_normalization = list(df_energy)[1]

df_data_float = df_energy[extract_for_normalization].astype(float)
df_data_array = df_data_float.to_numpy()
df_data_array_1 = df_data_array.reshape(-1,1)

from sklearn.model_selection import train_test_split
train_X, test_X = train_test_split(df_data_array_1, train_size = 0.7, shuffle = False)
scaler = MinMaxScaler(feature_range=(0, 1))

scaled_train_X=scaler.fit_transform(train_X)


**Converting train_X into required shape (inputs,sequences, features)**

train_f = []  #features input from training data
train_p = [] # prediction values

n_future = 1 #number of days we want to predict into the future
n_past = 672 # no. of time series input features to be considered for training

for val in range(n_past, len(scaled_train_X) - n_future+1):
    train_f.append(scaled_train_X[val - n_past:val, 0:scaled_train_X.shape[1]])
    train_p.append(scaled_train_X[val + n_future - 1:val + n_future, -1])
    

train_f, train_p = np.array(train_f), np.array(train_p)

**Transformer Model**

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;relu&quot;)(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def build_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
):
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format=&quot;channels_first&quot;)(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(train_p.shape[1])(x)
    return keras.Model(inputs, outputs)

input_shape = (train_f.shape[1], train_f.shape[2])

model = build_model(
    input_shape,
    head_size=256,
    num_heads=4,
    ff_dim=4,
    num_transformer_blocks=4,
    mlp_units=[128],
    mlp_dropout=0.4,
    dropout=0.25,
)


model.compile(loss=tf.keras.losses.mean_absolute_error, 
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
              metrics=[&quot;mse&quot;])


model.summary()

history = model.fit(train_f, train_p, epochs=100, batch_size = 32, validation_split = 0.25, verbose = 1)


trainYPredict = model.predict(train_f)

**Inverse transform the prediction and keep the last value(output)**

trainYPredict1 = np.repeat(trainYPredict, scaled_train_X.shape[1], axis = -1)
trainYPredict_actual = scaler.inverse_transform(trainYPredict1)[:, -1]

train_p_actual = np.repeat(train_p, scaled_train_X.shape[1], axis = -1)

train_p_actual1 = scaler.inverse_transform(train_p_actual)[:, -1]



Prediction_mse=mean_squared_error(train_p_actual1 ,trainYPredict_actual)
print(&quot;Mean Squared Error of prediction is:&quot;, str(Prediction_mse))

Prediction_rmse =sqrt(Prediction_mse)
print(&quot;Root Mean Squared Error of prediction is:&quot;, str(Prediction_rmse))

prediction_r2=r2_score(train_p_actual1 ,trainYPredict_actual)
print(&quot;R2 score of predictions is:&quot;, str(prediction_r2))

prediction_mae=mean_absolute_error(train_p_actual1 ,trainYPredict_actual)
print(&quot;Mean absolute error of prediction is:&quot;, prediction_mae)

**Testing of model**

scaled_test_X = scaler.transform(test_X)
test_q = []
test_r = []

for val in range(n_past, len(scaled_test_X) - n_future+1):
    test_q.append(scaled_test_X[val - n_past:val, 0:scaled_test_X.shape[1]])
    test_r.append(scaled_test_X[val + n_future - 1:val + n_future, -1])
    

test_q, test_r = np.array(test_q), np.array(test_r)

testPredict = model.predict(test_q)
</code></pre>
","transformer-model"
"74996994","Do BERT word embeddings change depending on context?","2023-01-03 17:58:21","75113048","1","857","<nlp><huggingface-transformers><bert-language-model><embedding><transformer-model>","<p>Before answering &quot;yes, of course&quot;, let me clarify what I mean:</p>
<p>After BERT has been trained, and I want to use the pretrained embeddings for some other NLP task, can I once-off extract all the word-level embeddings from BERT for all the words in my dictionary, and then have a set of static key-value word-embedding pairs, from where I retrieve the embedding for let's say &quot;bank&quot;, or will the embeddings for &quot;bank&quot; change depending on whether the sentence is &quot;Trees grow on the river bank&quot;, or &quot;I deposited money at the bank&quot; ?</p>
<p>And if the latter is the case, how do I practically use the BERT embeddings for another NLP task, do I need to run every input sentence through BERT before passing it into my own model?</p>
<p>Essentially - do embeddings stay the same for each word / token after the model has been trained, or are they dynamically adjusted by the model weights, based on the context?</p>
","transformer-model"
"74979359","How is position wise feed forward neural network implemented for transformers?","2023-01-02 05:59:25","","6","4362","<machine-learning><pytorch><neural-network><transformer-model>","<p>I am having hard time understanding position wise feed forward neural network in transformers architecture.</p>
<p><a href=""https://i.sstatic.net/HfjMB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HfjMB.png"" alt=""enter image description here"" /></a></p>
<p>Lets take example as Machine translation task, where inputs are sentences. From the figure I understand that for each word, different feed forward neural network is used to the output of self attention sub-layer. The feed forward layer apply similar Linear transformations but actual weights and biases for each transformations are different because they are two different feed forward neural network.</p>
<p>refering to <a href=""https://nlp.seas.harvard.edu/2018/04/03/attention.html"" rel=""nofollow noreferrer"">Link</a>, Here is the class for <code>PositionWiseFeedForward</code> neural network</p>
<pre><code>class PositionwiseFeedForward(nn.Module):
    &quot;Implements FFN equation.&quot;
    def __init__(self, d_model, d_ff, dropout=0.1):
        super(PositionwiseFeedForward, self).__init__()
        self.w_1 = nn.Linear(d_model, d_ff)
        self.w_2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        return self.w_2(self.dropout(F.relu(self.w_1(x))))
</code></pre>
<p>My question is:</p>
<p>I don't see anything position-wise about this. This is simple fully connected neural network with two layers. assuming  <code>x</code> to be list of embedding of each word in a sentence, each word in a sentence is transformed by above layer using same set of weight and biases.(correct me if i am wrong)</p>
<p>I was expecting to find something like passing each word embedding to separate <code>Linear</code> layer which will have different weight and biases to achieve something similar to what is shown in the picture.</p>
","transformer-model"
"74977873","the evaluation section leds to bug","2023-01-01 22:55:42","74978671","1","57","<tensorflow><google-colaboratory><transformer-model>","<p>there is a problem in running  code
hi,</p>
<p>pop out an error and can not generate the result,
Error:
ValueError: Input 2 is incompatible with layer model_2: expected shape=(None, 50), found shape=(None, 51)</p>
<p>so is there any solution for?
Much obliged</p>
<p>the full part that triggers the bug droped below
full error:</p>
<pre><code>ValueError: in user code:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *
    return step_function(self, iterator)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **
    outputs = model.distribute_strategy.run(run_step, args=(data,))
/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:540 run
    return self.extended.tpu_run(fn, args, kwargs, options)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1296 tpu_run
    return func(args, kwargs)
/opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/tpu_strategy.py:1364 tpu_function
    xla_options=tpu.XLAOptions(use_spmd_for_xla_partitioning=False))
/opt/conda/lib/python3.7/site-packages/tensorflow/python/tpu/tpu.py:968 replicate
    xla_options=xla_options)[1]
/opt/conda/lib/python3.7/site-packages/tensorflow/python/tpu/tpu.py:1439 
</code></pre>
","transformer-model"
"74939384","Parallelising Transformer model inference due to large batch sizes","2022-12-28 10:58:31","","1","478","<parallel-processing><pytorch><huggingface-transformers><transformer-model><huggingface>","<p>I have trained a text classification model using the HuggingFace library. The task is to classify bank transaction descriptions into one of 16 categories.</p>
<p>During training, my batch size was 32. However, now at inference time, all transactions for a single bank account must be classified at a time. Some accounts have thousands of transactions, meaning at inference, the model is recieiving batches much larger than it did during training (1000s as opposed to 32). This is making inference much slower than we would like (~15s per 1000 transactions). To make matters worse, right now I cannot deploy the service on a GPU instance due to cost constraints, so the model is only running on CPU(s).</p>
<p>Are there any options for improving inference time? Is it possible to split each bank accounts' transactions into smaller batches and pass each batch to copies of the model in parallel?</p>
","transformer-model"
"74927061","While taking a nn.Module (which consists of other nn.Modules) to cuda, aren't all the internal nn.Modules supposed to go to CUDA too?","2022-12-27 08:20:17","","0","45","<machine-learning><deep-learning><pytorch><transformer-model>","<p>I have this module VideoSegmentationNetwork which consists of 3 nn.Modules</p>
<ol>
<li>CNNEncoder(nn.Module)</li>
<li>TransformerEncoder(nn.Module)</li>
<li>CNNDecoder(nn.Module)</li>
</ol>
<pre><code>class VideoSegmentationNetwork(nn.Module):
    def __init__(self):
        super(VideoSegmentationNetwork, self).__init__()
        self.cnnencoder = CNN_Encoder()
        self.transenc = TransformerEncoder(input_dim=EMBEDDED_DIMENSION, num_layers=2, num_heads=2)
        self.cnndecoder = CNN_Decoder()
</code></pre>
<p>I have defined this network as:</p>
<pre><code>   model = VideoSegmentationNetwork()
   model.to(&quot;cuda:0&quot;)
</code></pre>
<p>While doing this, all three Classes are in CUDA when I check.</p>
<p><strong>&quot;SO WHERE IS THE PROBLEM? o.O&quot;</strong> - You might ask.</p>
<p>Well, it comes inside TransformerEncoder(nn.Module) class, which is another class. It has a custom made MultiHeadAttention(nn.Module) class. The error exactly pops in, the MultiHeadAttention(nn.Module) class.</p>
<p>The MultiHeadAttention class is like this:</p>
<pre><code>class MultiheadAttention(nn.Module):
    def __init__(self, input_dim, num_heads):
        super(MultiheadAttention, self).__init__()
        self.query_layer = nn.Linear(input_dim, input_dim)
        self.key_layer = nn.Linear(input_dim, input_dim)
        self.value_layer = nn.Linear(input_dim, input_dim)
        self.output_layer = nn.Linear(input_dim, input_dim)
</code></pre>
<p>The error exactly pops in forward(self, x) method in the class. The error exactly says this:</p>
<p><a href=""https://i.sstatic.net/HCDVy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HCDVy.png"" alt=""enter image description here"" /></a></p>
<p>So I was wondering, doesn't taking VideoSegmentationNetwork to cuda, take everything inside it to cuda too? Or I am missing something here, and you guys have some suggestions on what I should do?</p>
","transformer-model"
"74914230","How to import Transformers with Tensorflow","2022-12-25 15:30:53","74914295","1","2858","<python><tensorflow><keras><transformer-model>","<p>After installing Transformers using</p>
<pre><code>pip install Transformers
</code></pre>
<p>I get version 4.25.1 , but when I try to import Transformer by</p>
<pre><code>from tensorflow.keras.layers import Transformer
# or
from tensorflow.keras.layers.experimental  import Transformer
</code></pre>
<p>I get this error:</p>
<pre><code>ImportError: cannot import name 'Transformer' from 'tensorflow.keras.layers'
</code></pre>
<p>I am using <code>Tenserflow 2.10</code> and <code>python 3.7</code>.</p>
","transformer-model"
"74908748","Transformer Model isn't fitting well on Time Series Data","2022-12-24 15:56:26","","1","396","<tensorflow><keras><deep-learning><time-series><transformer-model>","<p>I have created two models LSTM, LSTM with Self-Attention. Now I am working on to create my first transformer model. I created it for multivariate time series predictions (many-to-one classification model).</p>
<p>I have the hourly varying data i.e., 8 different features (hour, month, temperature, humidity, windspeed, solar radiations concentration etc.) and with them I am trying to predict the time sequence (energy consumption of a building. So my input has the shape <code>X.shape = (8783, 168, 8)</code> i.e., 8783 time sequences, each sequence contains 168 hourly entries/vectors and each vector contains 8 features. My output has the shape <code>Y.shape = (8783,1)</code> i.e., 8783 sequences each containing 1 output value (i.e., building energy consumption value after every hour).</p>
<p>I took as a model an example from the official keras site. It is created for classification problems, I converted my output to classes  <code>n_classes = len(np.unique(Y_train))</code> = 156
<code>Input shape (X_train) = (8783, 168, 8) Output shape (Y_train) = (8783,1) n_classes = 156 </code>
In the softmax activation layer, I set the output to <code>n_classes</code> but it's not fitting well. Below I attach the model and I would like to know:</p>
<p>I)- Have I done something wrong in the model? Is the model architecture is fine? Are there maybe some other parts of the code I need to change for it to work for my problem?</p>
<p>II)- Also, can a transformer at all work on multivariate problems of my kind (8 features input, 1 feature output) or do transformers only work on univariate problems?</p>
<pre><code>
def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks,
                            mlp_units, dropout=0, mlp_dropout=0):

    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):

        # Normalization and Attention
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        x = layers.MultiHeadAttention(
            key_dim=head_size, num_heads=num_heads, dropout=dropout
        )(x, x)
        x = layers.Dropout(dropout)(x)
        res = x + inputs

        # Feed Forward Part
        x = layers.LayerNormalization(epsilon=1e-6)(res)
        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(dropout)(x)
        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
        x = x + res

    x = layers.GlobalAveragePooling1D(data_format=&quot;channels_first&quot;)(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(mlp_dropout)(x)
    x = layers.Dense(n_classes, activation=&quot;softmax&quot;)(x)
    return keras.Model(inputs, x)


model_tr = build_transformer_model(input_shape=(X_train.shape[1], X_train.shape[2]), head_size=256,
                                   num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], mlp_dropout=0.4, dropout=0.2)
model_tr.compile(loss=&quot;sparse_categorical_crossentropy&quot;,
    optimizer=keras.optimizers.Adam(learning_rate=0.0001),
    metrics=[&quot;sparse_categorical_accuracy&quot;],
)
plot_model(model_tr, to_file='model_plot.png', show_shapes=True, show_layer_names=True)
model_tr.summary()
m_tr_history = model_tr.fit(x=X_train, y=Y_train, validation_split=0.15, batch_size=64, epochs=100, verbose = 1)
model_tr.save('halka_transformer.h5')
</code></pre>
<p><a href=""https://i.sstatic.net/nAned.jpg"" rel=""nofollow noreferrer"">No. of epochs</a></p>
<p>Thanks in advance for this valuable assistance.</p>
","transformer-model"
"74898307","Is there any language model that is able to generate text given both beginning and ending sequences?","2022-12-23 09:54:10","","1","43","<nlp><huggingface-transformers><bert-language-model><transformer-model>","<p>Generative language models like GPT-2 can generate text given beginning sequences, and <a href=""https://arxiv.org/abs/2012.08561"" rel=""nofollow noreferrer"">cloze Transformer</a> can fill in the one-word blank between two sequence. Is there any generative language model that can generate multi-word text that connects given beginning and ending sequences as an extension to both models like GPT-2 and cloze transformer?</p>
<p>Input:</p>
<pre><code>Beginning sequence: What is Love?
Ending sequence: Love is an emotional state, not a physical one.
</code></pre>
<p>Output:</p>
<pre><code>[What is Love?] Love is a state of mind, a feeling, 
an emotion, or a mental state. It is the feeling 
of being in love with someone or something. The 
word &quot;love&quot; is derived from the Latin word for 
&quot;to love,&quot; &quot;lēgēre,&quot; which means to feel, to be 
attracted to, and to love. [Love is an emotional 
state, not a physical one.]
</code></pre>
","transformer-model"
"74897010","I have a tensor of shape [5, 2, 18, 4096]. I want to stack the 0th dimension along the 2nd dimension. How can I do it?","2022-12-23 07:08:03","74897074","2","90","<python><machine-learning><deep-learning><pytorch><transformer-model>","<p>The shape of the tensor is <code>[5, 2, 18, 4096]</code>. I want to take each tensor along 0th dimension of size <code>[2, 18, 4096]</code> and stack it on top of another tensor which is of shape from the same tensor <code>[2, 18, 4096]</code> and do it for all tensors along the 0th dimension. The final tensor should be <code>[2, 90, 4096]</code>.</p>
","transformer-model"
"74894820","What is the correct way to penalize one prediction more over another?","2022-12-22 23:31:54","","0","725","<machine-learning><pytorch><loss-function><transformer-model><cross-entropy>","<p>I have a BERT-based sequence classification model that takes as an input 4 strings and out 2 labels for each one:</p>
<pre><code>my_input = [string_1, string_2, string_3, string_4]
out_logits = model(my_input).logits
out_softmax = torch.softmax(out_logits)
out_softmax 
&gt;&gt;&gt; tensor([[0.8666, 0.1334],
        [0.8686, 0.1314],
        [0.8673, 0.1327],
        [0.8665, 0.1335]], device='cuda:0', grad_fn=&lt;SoftmaxBackward0&gt;)
</code></pre>
<p>My loss function is <code>nn.CrossEntropyLoss()</code> and my labels are tensors with indices corresponding to the correct labels: <code>tensor([0., 0., 0., 1.])</code>. Note that every label except for one is <code>1</code>.</p>
<pre><code>loss = loss_fun(out_softmax, labels_tensor)
# step
optim.zero_grad()
loss.backward()
optim.step()
</code></pre>
<p>The issue I'm having as appearing above, is that the model learns to just predict one class (e.g., the first column above). Not entirely sure why it's happening, but I thought that penalizing more the prediction that should be <code>1</code> might help.</p>
<p>How can I penalize more that prediction?</p>
","transformer-model"
"74774840","Make nn.Transformer work for Text Generation","2022-12-12 17:15:34","","0","507","<pytorch><nlp><transformer-model>","<p>I am trying to make a Transformer work for paraphrase generation but the generations are not useful (the same everytime, full of BOS tokens or &quot;?&quot; tokens).
I followed <a href=""https://pytorch.org/tutorials/beginner/translation_transformer.html"" rel=""nofollow noreferrer"">this tutorial</a> for reference. My implementation is embedded into a framework which requires an Encoder and a Decoder:</p>
<p>The encoder is like this:</p>
<pre><code>class TransformerEncoder(nn.Module):
    def __init__(
        self,
        vocab_size,
        pad_token_id=None,
        embedding_size=256,
        num_heads=8,
        num_layers=3,
        ffnn_size=512,
        dropout=0.1,
    ):
        super(TransformerEncoder, self).__init__()
        self.vocab_size = vocab_size
        self.pad_token_id = pad_token_id

        self.embedding_size = embedding_size
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.ffnn_size = ffnn_size

        self.embed_tokens = TokenEmbedding(vocab_size, embedding_size)
        self.embed_positions = PositionalEmbedding(embedding_size, dropout=dropout)

        encoder_layer = nn.TransformerEncoderLayer(
            embedding_size,
            num_heads,
            ffnn_size,
            dropout,
        )
        encoder_norm = nn.LayerNorm(embedding_size)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers, encoder_norm)

    def forward(
        self,
        input_ids,
    ):

        # seq_len = input_ids.shape[1]
        # device = next(self.parameters()).device

        embedded_tokens = self.embed_positions(self.embed_tokens(input_ids))
        # B x T x C -&gt; T x B x C
        embedded_tokens = embedded_tokens.transpose(0, 1)

        memory = self.encoder(embedded_tokens)

        return (memory,)
</code></pre>
<p>The decoder is like this:</p>
<pre><code>class TransformerDecoder(nn.Module):
    def __init__(
        self,
        vocab_size,
        pad_token_id=None,
        embedding_size=256,
        num_heads=8,
        num_layers=3,
        ffnn_size=512,
        dropout=0.1,
    ):

        super(TransformerDecoder, self).__init__()
        self.vocab_size = vocab_size
        self.pad_token_id = pad_token_id

        self.embedding_size = embedding_size
        self.num_heads = num_heads
        self.num_layers = num_layers
        self.ffnn_size = ffnn_size

        self.dropout_module = nn.Dropout(p=dropout)

        self.embed_tokens = TokenEmbedding(vocab_size, embedding_size)
        self.embed_positions = PositionalEmbedding(embedding_size, dropout=dropout)

        decoder_layer = nn.TransformerDecoderLayer(
            embedding_size, num_heads, ffnn_size, dropout
        )
        decoder_norm = nn.LayerNorm(embedding_size)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers, decoder_norm)
        self.fc_out = nn.Linear(embedding_size, vocab_size)

    def forward(
        self,
        input_ids,
        encoder_out,
    ):
        seq_len = input_ids.shape[1]

        device = next(self.parameters()).device
        mask = generate_square_subsequent_mask(seq_len).to(device)

        embedded_tokens = self.embed_positions(self.embed_tokens(input_ids))

        # B x T x C -&gt; T x B x C
        embedded_tokens = embedded_tokens.transpose(0, 1)

        output = self.decoder(embedded_tokens, encoder_out[0], tgt_mask=mask)

        # T x B x C -&gt; B x T x C
        output = output.transpose(1, 0)

        return (self.fc_out(output),)
</code></pre>
<p>TokenEmbedding and PositionalEmbedding are as in the tutorial.
The main model just invokes encoder and decoder like:</p>
<pre><code>        encoder_outputs = self.encoder(input_ids=input_ids, **kwargs)

        decoder_outputs = self.decoder(
            input_ids=decoder_input_ids,
            encoder_out=encoder_outputs,
            **kwargs,
        )
</code></pre>
<p>The labels are shifted one token to the right to be fed to the decoder using:</p>
<pre><code>def shift_tokens_right(self, input_ids: torch.Tensor, decoder_start_token_id: int):
   shifted_input_ids = input_ids.new_zeros(input_ids.shape)
   shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()
   shifted_input_ids[:, 0] = decoder_start_token_id
   return shifted_input_ids
</code></pre>
<p>The loss is calculated as:</p>
<pre><code>loss_fct = nn.CrossEntropyLoss(ignore_index=self.pad_token_id)
loss = loss_fct(logits.reshape(-1, logits.shape[-1]), targets.reshape(-1))
</code></pre>
<p>The loss is going down, but the generations are real bad. Following is an example of the generations:
<strong>Source:</strong> &lt; s &gt; Can I jailbreak iOS 10 ? &lt; /s &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt;
<strong>Preds:</strong> &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt; &lt; s &gt;
<strong>Target:</strong> &lt; s &gt; Can you jailbreak iOS 10 ? &lt; /s &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt; &lt; pad &gt;</p>
<p>As you can see, the predictions in this case are only BOS tokens. The output of the decoder on each decoder step is always almost the same for every iteration. The model does not seem to be learning. I have tried learning rates from 0.1 to 1e-4. For a brief moment at the second or third epoch, there were produced intelligible sentences, but quickly after that the generations reverted back to just BOS or PAD tokens.</p>
<p>Do you have an intuition on what might be wrong? Sorry for the question not being self-contained. Thanks in advance for any help you can provide.</p>
","transformer-model"
"74773355","How to avoid target ""leaking"" into the training process with PyTorch's TimeSeriesDataSet and TemporalFusionTransformer?","2022-12-12 15:20:42","","1","179","<deep-learning><pytorch><prediction><transformer-model>","<p>Basically, I have a time series dataset that wants to predict the price 14 days from now. There are dozens of features and everything is being trained with TemporalFusionTransformer (PyTorch version).</p>
<p>Originally, I trained to predict price every day for the next 14 days, however these gave fairly poor results. So, I want to switch to predict ONLY the price 14 days from now. For a conventional dataset I would just lag the price variable 14 days (dataset['price_lagged'] = dataset['price'].shift(-13) and set max_prediction_length to 1 to predict the 14th day) and set this as a target variable.</p>
<p>Current code that still predicts well (and it shouldn't):</p>
<pre><code>training = TimeSeriesDataSet(
    dataset[lambda x: x.day &lt;= training_cutoff], 
    time_idx=&quot;day&quot;,
    target=&quot;price_lagged&quot;,
    group_ids=[&quot;group_id&quot;]
    min_encoder_length=max_encoder_length ,
    max_encoder_length=max_encoder_length,
    max_prediction_length=max_prediction_length,
    static_categoricals=[],
    static_reals=[],
    time_varying_known_categoricals=[],
    time_varying_unknown_reals=[
        #I removed EVERYTHING from here
    ],
    time_varying_unknown_categoricals=[
],
    time_varying_known_reals=[  
],
    target_normalizer=EncoderNormalizer(),
    #lags=lags,
    add_relative_time_idx=True,
    allow_missing_timesteps=False,
    add_target_scales=True,
)
</code></pre>
<p>However, the model seems to use the target as a variable as well. I can see this if I literally remove every other feature, I still get a fairly good prediction. How can I explicitly have the model NOT use the target as a (obvious) feature? Can TFT work like this?</p>
","transformer-model"
"74768655","Receptive Field in Swin Transformer","2022-12-12 08:52:20","","1","167","<conv-neural-network><transformer-model><attention-model><self-attention><receptive-field>","<p>I want to ask if is it true that the receptive field of swin transformer is just in the local window where we compute the self-attention? And is there any way to increase the receptive field when using swin transformer?</p>
<p>I know that when we use consecutive convolution layers, we will get a bigger receptive field as result. So, can we do the same with swin transformer?</p>
","transformer-model"
"74740685","Mt5 language translation - Language in parameters","2022-12-09 08:45:58","","1","225","<nlp><huggingface-transformers><transformer-model><machine-translation><huggingface>","<p>I'm trying to train a Arabic to English language translation model, and want to know if there is any option where I can mention the input and target language in the code.</p>
<p>I'm using the below code, where df1 is the dataframe with columns (prefix,input_text,target_text)</p>
<pre><code>from simpletransformers.t5 import T5Model, T5Args
model = T5Model(&quot;mt5&quot;, &quot;google/mt5-small&quot;, args=model_args, use_cuda=False)
model.train_model(df1,eval_data=df2)
results = model.eval_model(df2, verbose=True)
</code></pre>
","transformer-model"
"74728925","Using EluetherAPI GPT models for NLP tasks","2022-12-08 10:36:42","","3","514","<python><nlp><huggingface-transformers><transformer-model>","<p>EluetherAPI released many GPT models based on the PILE dataset, which is equivalent to original GPT models. As they are trained on a larger dataset, we can perform multiple NLP tasks on the same model without retraining the model, with just a few prompts, or by providing some context using few-shot learning.</p>
<p>I am trying to achieve the same. But the problem is the return text is sometimes too large or too short. Here is my example code:</p>
<pre><code>generator = pipeline('text-generation', model='EleutherAI/gpt-neo-1.3B', device=0)
prompt= &quot;&quot;&quot;[Original]: The diplomatic spat came days after France cut the number of visas it issues for citizens of Algeria and other North African countries.
[Paraphrase]: &quot;&quot;&quot;
result = generator(prompt, do_sample=True, min_length=10, max_new_tokens=50, top_p=0.9, temperature=1)
</code></pre>
<p>the result gave me this:</p>
<pre><code>France has been forced to temporarily remove two of its citizens who are on a tourist visa from Algeria and Morocco, which have had a long and acrimonious history over the past decade.
[Original]: The two visa holders, who
</code></pre>
<p>As you can see, it gives me a result with the input text included, I removed the input text, and it works fine but at the end, it still shows the [Original]: prompt. How to remove it and give the exact same results?</p>
<p>I tried multiple times, even providing it context, but it works fine sometime and sometime not. I even tried few-shot learning with data as:</p>
<pre><code>&quot;&quot;&quot;[Original]: Algeria recalled its ambassador to Paris on Saturday and closed its airspace to French military planes a day later after the French president made comments about the northern Africa country. 
[Paraphrase]: Last Saturday, the Algerian government recalled its ambassador and stopped accepting French military airplanes in its airspace. It happened one day after the French president made comments about Algeria.
###
[Original]: President Macron was quoted as saying the former French colony was ruled by a &quot;political-military system&quot; with an official history that was based not on truth, but on hatred of France.
[Paraphrase]: Emmanuel Macron said that the former colony was lying and angry at France. He also said that the country was ruled by a &quot;political-military system&quot;.
###
[Original]: The diplomatic spat came days after France cut the number of visas it issues for citizens of Algeria and other North African countries.
[Paraphrase]: Diplomatic issues started appearing when France decided to stop granting visas to Algerian people and other North African people.
###
[Original]: After a war lasting 20 years, following the decision taken first by President Trump and then by President Biden to withdraw American troops, Kabul, the capital of Afghanistan, fell within a few hours to the Taliban, without resistance.
[Paraphrase]:&quot;&quot;
</code></pre>
<p>I want to know if is there any way to pass the end_sequence so that it will stop generating after that, also the parameters top_p and temperature to get good results.</p>
","transformer-model"
"74727256","SequenceClassifierOutput has generator as loss instead of a tensor","2022-12-08 08:09:10","","0","654","<pytorch><huggingface-transformers><transformer-model><roberta>","<p>I'm doing Distillation from a Roberta with an Adapter, I'm following <a href=""https://towardsdatascience.com/distillation-of-bert-like-models-the-code-73c31e8c2b0a"" rel=""nofollow noreferrer"">this</a> tutorial</p>
<p>and in the function <code>distill_roberta_weights()</code> I just change <code>teacher_model.config.to_dict()</code>
to <code>student.load_state_dict(teacher.state_dict(), strict=False)</code>, so the student model has the adapter too.</p>
<p>But when I am training the distillation using the</p>
<pre><code>DistillationTrainer
</code></pre>
<p>from <a href=""https://www.philschmid.de/knowledge-distillation-bert-transformers"" rel=""nofollow noreferrer"">here</a>
I get the following error
<a href=""https://i.sstatic.net/bxe1N.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bxe1N.png"" alt=""enter image description here"" /></a></p>
<p>Do you have any idea of what is the problem?
The student_output has a loss generator instead the tensor, the part of the cross entropy does not have any problem as it uses the logits from the outputs.</p>
<p>EDIT:</p>
<p>I am adding more information</p>
<pre class=""lang-py prettyprint-override""><code>def distill_weights(teacher, student):
   &quot;&quot;&quot;
   Recursively copies the weights of the (teacher) to the (student).
   This function is meant to be first called on a RobertaFor... model, but is then called on every children of that model recursively.
   The only part that's not fully copied is the encoder, of which only half is copied.
   &quot;&quot;&quot;
   # If the part is an entire RoBERTa model or a RobertaFor..., unpack and iterate
   if isinstance(teacher, RobertaModel) or type(teacher).__name__.startswith('RobertaFor'):
       for teacher_part, student_part in zip(teacher.children(), student.children()):
           distill_weights(teacher_part, student_part)
   # Else if the part is an encoder, copy one out of every layer
   elif isinstance(teacher, RobertaEncoder):
           teacher_encoding_layers = [layer for layer in next(teacher.children())]
           student_encoding_layers = [layer for layer in next(student.children())]
           for i in range(len(student_encoding_layers)):
               student_encoding_layers[i].load_state_dict(teacher_encoding_layers[2*i].state_dict())
   # Else the part is a head or something else, copy the state_dict
   else:
       student.load_state_dict(teacher.state_dict(), strict=False)


def distill_roberta_based(teacher_model):
   &quot;&quot;&quot;
   Distilates a RoBERTa (teacher_model) like would DistilBERT for a BERT model.
   The student model has the same configuration, except for the number of hidden layers, which is // by 2.
   The student layers are initilized by copying one out of two layers of the teacher, starting with layer 0.
   The head of the teacher is also copied.
   &quot;&quot;&quot;
   # Set student configuration
   configuration = teacher_model.config.to_dict()
   configuration['num_hidden_layers'] //= 2
   configuration = RobertaConfig.from_dict(configuration)
   
   # create student model
   student_model = type(teacher_model)(configuration)
   distill_weights(teacher=teacher_model, student=student_model)

   return student_model
#function for train the Distillated model
class DistillationTrainer(Trainer):
   def __init__(self, *args, teacher_model=None, **kwargs):
       super().__init__(*args, **kwargs)
       
       self.teacher = teacher_model
       # place teacher on same device as student
       self._move_model_to_device(self.teacher,self.model.device)
       self.teacher.eval()

   
   def compute_loss(self, model, inputs, return_outputs = False)  :
       &quot;&quot;&quot;
       The distillation loss for distilating a BERT-like model.
       The loss takes the (teacher_logits), (student_logits) and (labels) for various losses.
       The (temperature) can be given, otherwise it's set to 1 by default.
       &quot;&quot;&quot;
       outputs_student =  model(**inputs)
       print(outputs_student)
       student_loss    = outputs_student.loss
       
       
       # compute teacher output
       with torch.no_grad():
         outputs_teacher = self.teacher(**inputs)
       
       # assert size
       assert outputs_student.logits.size() == outputs_teacher.logits.size()
                                

       # Classification loss (problem-specific loss)
       loss_function = CrossEntropyLoss()
       
       # Temperature and sotfmax
       student_logits = F.softmax (outputs_student.logits / self.args.temperature, dim=-1)
       teacher_logits = F.softmax (outputs_teacher.logits / self.args.temperature, dim=-1)
       loss_logits = loss_function(student_logits, teacher_logits)

       # Return weighted student loss
       loss = self.args.alpha * student_loss + (1. - self.args.alpha) * loss_logits
       return (loss, outputs_student) if return_outputs else loss

#create the student 
student_model_adapter = distill_roberta_based(teacher_model)
#activate adapter 
student_model_adapter.set_active_adapters('parallel')
student_model_adapter.train_adapter('parallel')  

trainer = DistillationTrainer(
   student_model_adapter,
   training_args,
   teacher_model=teacher_model,
   train_dataset=tokenized_datasets[&quot;train&quot;],
   eval_dataset=tokenized_datasets[&quot;validation&quot;],
   data_collator=data_collator,
   tokenizer=tokenizer,
   compute_metrics=compute_metrics,
)
trainer.args._n_gpu = 4
</code></pre>
<p>So, the desired output of <code>outputs_student</code> should be like</p>
<pre><code>SequenceClassifierOutput(loss=tensor([0.6899, 0.6902, 0.6926, 0.6913, 0.6906, 0.6904, 0.6922, 0.6917],
       device='cuda:0', grad_fn=&lt;GatherBackward&gt;), logits=tensor([[-1.2512e-03, -9.7885e-03],
        [ 6.2714e-03, -5.7755e-03],.....])
</code></pre>
<p>But instead the output is</p>
<pre><code>SequenceClassifierOutput(loss=&lt;generator object gather.&lt;locals&gt;.gather_map.&lt;locals&gt;.&lt;genexpr&gt; at 0x7f5bb4fbe9d0&gt;, logits=tensor([[-0.0150,  0.0075],
        [-0.0122,  0.0181],...
</code></pre>
","transformer-model"
"74716393","Keep getting ""EOFError: Ran out of input"" error","2022-12-07 12:03:14","","0","309","<python-3.x><machine-learning><transformer-model><pytorch-lightning>","<p>So basicly i am trying to use a transformer model to classify ecg heartbeats. I got error after running the following script:</p>
<pre><code># Import Necessary Dependencies

import os
import copy
import pandas as pd
import math


# Pytorch
import torch
import torch.nn as nn
from torch.utils.data.dataset import Dataset
from torch.utils.data.dataloader import DataLoader
from torch.optim import Adam

# Pytorch Lightning
from pytorch_lightning import LightningModule, LightningDataModule
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint


# Torchmetric for computing accuracy
from torchmetrics.functional import accuracy

# Plotting
import matplotlib.pyplot as plt


# Dataset

def load_list(root, filename):
    filepath = os.path.join(root, filename)
    output = pd.read_csv(filepath, header=None)
    return output

class MITBIHArrhythmia(Dataset):
    def __init__(self, root, subset=None):
        assert subset is None or subset in [&quot;training&quot;, &quot;validation&quot;, &quot;testing&quot;], (
            &quot;When `subset` not None, it must take a value from &quot; + &quot;{'training', 'validation', 'testing'}.&quot;
        )
        self.root = root
        file_dict = {
            &quot;training&quot;: &quot;mitbih_train.csv&quot;,
            &quot;validation&quot;: &quot;mitbih_test.csv&quot;,
            &quot;testing&quot;: &quot;mitbih_test.csv&quot;
        }
        self._walker = load_list(self.root, file_dict[subset])
    def __len__(self):
        return len(self._walker)
    def __getitem__(self, n:int):
        row = self._walker.loc[n, :].values.tolist()
        label = row.pop()
        return row, label

## Data Visualization

ds = MITBIHArrhythmia(&quot;C:/Users/Samet/Desktop/ECG&quot;, &quot;training&quot;)

CLASSES_DICT = {
    0: &quot;Normal beat (N)&quot;,
    1: &quot;Supraventricular premature beat (S)&quot;,
    2: &quot;Premature ventricular contraction (V)&quot;,
    3: &quot;Fusion of ventricular and normal beat (F)&quot;,
    4: &quot;Unclassifiable beat (Q)&quot;
}
class_to_dsindx = {0:0, 1:72471, 2:74694, 3:80482, 4:81123}
# 0 starts at index 0, 1 starts at index 72471, 2 starts at index 74694
# 3 starts at index 80482, 4 starts at index 81123

fig, axes = plt.subplots(2, 3, figsize=(20, 10))

for cat, value in CLASSES_DICT.items():
    ax = axes[int(cat/3), cat%3]
    for j in range(5):
        indx = class_to_dsindx[cat]
        ax.plot(ds[indx+j][0])
    ax.set(title=f&quot;1-beat ECG for {CLASSES_DICT[cat]}&quot;)
    ax.set(ylabel=&quot;Amplitude&quot;)
    ax.set(xlabel=&quot;Time (ms)&quot;)
plt.show()

# Transformer Model
#The Transformer used below is a modified version of the original transformer. Since the application need not translation, the decoder block was completely removed. The positional embeddings were also removed since adding this layer reduces accuracy (I don't know why). The Positional Feed Forward was also changed to a simple Feed Forward Network. A fully connected was attached to the end of the network for classition. A softmax layer was not added since we used we used CrossEntropyLoss() for calculating the loss.

#Essentially, our model was composed of
#Encoder block (N duplicates) -&gt; Fully Connected Network (nn.Linear)

#The Encoder block is composed of two SublayerConnections (Attention and Feed-Forward Network).
#The codes below were copied from http://nlp.seas.harvard.edu/annotated-transformer/ (GitHub: https://github.com/harvardnlp/annotated-transformer/).

def clones(module, N):
    &quot;Produce N identical layers.&quot;
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])


#### ATTENTION
def attention(query, key, value, mask=None, dropout=None):
    &quot;Compute 'Scaled Dot Product Attention'&quot;
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    p_attn = scores.softmax(dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn

class MultiHeadedAttention(nn.Module):
    def __init__(self, h, d_model, dropout=0.0):
        &quot;Take in model size and number of heads.&quot;
        super(MultiHeadedAttention, self).__init__()
        assert d_model % h == 0
        # We assume d_v always equals d_k
        self.d_k = d_model // h
        self.h = h
        self.linears = clones(nn.Linear(d_model, d_model), 4)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward(self, query, key, value, mask=None):
        if mask is not None:
            mask = mask.unsqueeze(1) # Same mask applied to all h heads.

        # 1) Do all the linear projections in batch from d_model =&gt; h x d_k
        query, key, value = [
            lin(x).view( -1, self.h, self.d_k).transpose(1, 2)
            for lin, x in zip(self.linears, (query, key, value))
        ]

        # 2) Apply attention on all the projected vectors in batch.
        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)

        # 3) &quot;Concat&quot; using a view and apply a final linear.
        x = (x.transpose(1, 2).contiguous().view( -1, self.h * self.d_k))
        
        del query
        del key
        del value
        out = self.linears[-1](x)
        return out

## BLOCKING
class LayerNorm(nn.Module):
    &quot;Construct a layernorm module.&quot;
    def __init__(self, features, eps=1e-6):
        super().__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps
    
    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2

class SublayerConnection(nn.Module):
    &quot;A residual connection followed by a layer norm.&quot;
    def __init__(self, size, dropout):
        super().__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, sublayer):
        &quot;Apply residual connection to any sublayer with the same size.&quot;
        return x + self.dropout(sublayer(self.norm(x)))

class EncoderBlock(nn.Module):
    &quot;Encoder is made up of self-attn and feed forward&quot;
    def __init__(self, size, self_attn, feed_forward, dropout):
        super().__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size

    def forward(self, x, mask):
        &quot;Follow Figure 1 (left) for connections.&quot;
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        x = self.sublayer[1](x, self.feed_forward)
        return x

class Encoder(nn.Module):
    &quot;Core encoder is a stack of N layers&quot;
    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, mask=None):
        &quot;Pass the input (and mask) through each layer in turn.&quot;
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)

class FeedForward(nn.Module):
    &quot;Construct a FeedForward network with one hidden layer&quot;
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(d_model, d_ff)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

class Transformer(nn.Module):
    &quot;Transformer Model&quot;
    def __init__(self, input_size, num_classes, num_heads=8, N=6, d_ff=256, dropout=0.0):
        super().__init__()
        c = copy.deepcopy
        attn = MultiHeadedAttention(num_heads, input_size)
        ff = FeedForward(input_size, d_ff, dropout)
        self.encoder = Encoder(EncoderBlock(input_size, c(attn), c(ff), dropout), N)
        self.fc = nn.Linear(input_size, num_classes)
    
    def forward(self, x):
        x = self.encoder(x)
        x = self.fc(x)
        return x

# Pytorch Ligthning Modules

class LitMITBIH(LightningDataModule):
    def __init__(self, root, batch_size, num_workers, length=200):
        super().__init__()
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.path = root
        self.length = length
    
    def prepare_data(self):
        self.train_dataset = MITBIHArrhythmia(self.path, &quot;training&quot;)
        self.val_dataset = MITBIHArrhythmia(self.path, &quot;validation&quot;)
        self.test_dataset = MITBIHArrhythmia(self.path, &quot;testing&quot;)
    
    def setup(self, stage=None):
        self.prepare_data()

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=True,
            pin_memory=True,
            collate_fn=self.collate_fn
        )
    
    def val_dataloader(self):
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=False, 
            pin_memory=True,
            collate_fn=self.collate_fn
        )
    
    def test_dataloader(self):
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            num_workers=self.num_workers,
            shuffle=False, 
            pin_memory=True,
            collate_fn=self.collate_fn
        )

    def collate_fn(self, batch):
        labels = []
        heartbeats = []
        for sample in batch:
            waveform, label = sample
            if len(waveform) &lt; self.length:
                padsize = self.length - len(waveform)
                waveform += [0]*padsize

            labels.append(torch.tensor(label).type(torch.int64))
            heartbeats.append(torch.tensor(waveform))

        labels = torch.stack(labels)
        heartbeats = torch.stack(heartbeats)
        return heartbeats, labels

class LitTransformer(LightningModule):
    def __init__(self, input_size, num_classes, num_heads, depth, max_epochs, lr,  dropout=0.1, d_ff=256):
        super().__init__()
        self.save_hyperparameters()
        self.model = Transformer(input_size, num_classes, num_heads, depth, d_ff, dropout)
        self.loss = torch.nn.CrossEntropyLoss()
        self.reset_parameters()

    def reset_parameters(self):
        for p in self.model.parameters():
            if p.dim() &gt; 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, x):
        return self.model(x)

    def configure_optimizers(self):
        optimizer = Adam(self.parameters(), lr=self.hparams.lr)
        # scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)
        return optimizer #[optimizer], [scheduler]
    
    def training_step(self, batch, batch_idx):
        wavs, labels = batch
        preds = self(wavs)
        loss = self.loss(preds, labels)
        self.log('train_loss', loss)
        return {&quot;loss&quot;: loss}
    
    def training_epoch_end(self, outputs):
        avg_loss = torch.stack([x[&quot;loss&quot;] for x in outputs]).mean()
        self.log(&quot;train_loss&quot;, avg_loss, on_epoch=True)
    
    def validation_step(self, batch, batch_idx):
        return self.test_step(batch, batch_idx)

    def validation_epoch_end(self, outputs):
        return self.test_epoch_end(outputs)
    
    def test_step(self, batch, batch_idx):
        wavs, labels = batch
        preds = self(wavs)
        loss = self.loss(preds, labels)
        acc = accuracy(preds, labels) * 100.
        return {&quot;preds&quot;: preds, 'test_loss': loss, 'test_acc': acc}

    def test_epoch_end(self, outputs):
        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()
        avg_acc = torch.stack([x['test_acc'] for x in outputs]).mean()
        self.log(&quot;test_loss&quot;, avg_loss, on_epoch=True, prog_bar=True)
        self.log(&quot;test_acc&quot;, avg_acc, on_epoch=True, prog_bar=True)

# TRAINING

path = &quot;C:/Users/Samet/Desktop/ECG&quot;
input_size = 200
batch_size = 128
num_workers = 20
lr = 1e-4
max_epochs = 20
num_heads = 5
depth = 6
num_classes = 5
dropout = 0.0

datamodule = LitMITBIH(path, batch_size, num_workers, length=input_size)
datamodule.setup()

model = LitTransformer(input_size, num_classes, num_heads, depth, max_epochs, lr, dropout)
print(model)



save_path = &quot;C:/Users/Samet/Desktop/ECG/working&quot;
ckpt_name = &quot;ecg-transformer&quot;
model_checkpoint = ModelCheckpoint(
    dirpath=os.path.join(save_path, &quot;checkpoints&quot;),
    filename=ckpt_name,
    save_top_k=1,
    verbose=True,
    monitor='test_acc',
    mode='max',
)

if __name__ == '__main__':
    trainer = Trainer(accelerator=&quot;gpu&quot;, devices=1,
                    max_epochs=max_epochs,
                    logger=None,
                    callbacks=[model_checkpoint]
                )
    trainer.fit(model, datamodule=datamodule)

# Testing
model = model.load_from_checkpoint(
    os.path.join(save_path, &quot;checkpoints&quot;, ckpt_name+&quot;.ckpt&quot;)
)

trainer.test(model, datamodule=datamodule)
</code></pre>
<pre><code>C:\Users\Samet\anaconda3\lib\site-packages\pytorch_lightning\callbacks\model_checkpoint.py:604: UserWarning: Checkpoint directory C:\Users\Samet\Desktop\ECG\working\checkpoints exists and is not empty.
  rank_zero_warn(f&quot;Checkpoint directory {dirpath} exists and is not empty.&quot;)

EOFError: Ran out of input
</code></pre>
<p>First i was getting &quot;An attempt has been made to start a new process before the current process has finished its bootstrapping phase.&quot; error so i put the &quot;if _<em>name</em>_ == '_<em>main</em>_':&quot; line now i get Ran out of input error. I dont know what to do next.</p>
","transformer-model"
"74654341","How to Answer Subjective/descriptive types of lQuestions using BERT Model?","2022-12-02 10:34:26","","0","91","<nlp><huggingface-transformers><bert-language-model><transformer-model><gpt-2>","<p>I am trying to implement BERT Model for Question Answering tasks, but Its a little different from the existing Q&amp;A models,
The Model will be given some text(3-4 pages) and will be asked questions based on the text, and the expected  answer may be asked in short or descriptive subjective type</p>
<p>I tried to implement BERT, for this task.</p>
<p><strong>The Problems I am facing:</strong>
The input token limit for BERT is 512.
How to get the answer in long form, which can describe any instance, process, event, etc.</p>
","transformer-model"
"74652127","problem for change batch size in my model","2022-12-02 07:22:10","","0","496","<out-of-memory><transformer-model><hyperparameters><pytorch-lightning><batchsize>","<p>When I train my model(my model is a transformer that its input is featured extracted from T5 model and Vit )
I have problem for set batch_size more than 2 number</p>
<pre><code>number of image is 25000 for training.
GPU is GTX 3090(24 gpu ram).
24 core multithreading CPU.

number of total parameter =363M
seq_len=512
max-step=100000/2
iter=100000
img:torch.Size([3, 384, 500])
tokens:torch.Size([512])
</code></pre>
<p>I want to increase batch_size from 2 to 3,4,... but I can't. and I see error
for example when I set batch_size=4, I have this error
CUDA out of memoryTried to allocat....
(I attache image for error)
But when I decrease to 2 I have not this error .
What's I wrong?
<a href=""https://i.sstatic.net/uBKcH.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
","transformer-model"
"74629194","NLG | T5 Paraphrase results are not creative enough","2022-11-30 14:13:05","","0","140","<nlp><huggingface-transformers><transformer-model><sentence-transformers><nlg>","<p>I've tried to use pre-trained model to create paraphrases of sentences,
Even after trying to correlate top_p and top_k the result paraphrases were almost the same as the original sentence.
I would like to get results that look completely different (eg. third person talking about this topic or to point an event that illustrates this topic.)</p>
<p>Below is an example of three sentences that express the same idea in different words:</p>
<ol>
<li>Looking within is the best way to start solving the challenges in our lives.</li>
<li>People who are looking for solutions to their problems in the world around them will always continue to look for solutions.</li>
<li>The first step in healing our pain lies in our ability to &quot;look in the mirror&quot;.
Do you think it possible is to achieve those results with more effort on fine tuning or correlation?</li>
</ol>
<p>models I've tried:</p>
<p>&quot;ramsrigouthamg/t5_paraphraser&quot;: <a href=""https://huggingface.co/ramsrigouthamg/t5_paraphraser"" rel=""nofollow noreferrer"">https://huggingface.co/ramsrigouthamg/t5_paraphraser</a></p>
<p>&quot;Vamsi/T5_Paraphrase_Paws&quot;: <a href=""https://huggingface.co/Vamsi/T5_Paraphrase_Paws"" rel=""nofollow noreferrer"">https://huggingface.co/Vamsi/T5_Paraphrase_Paws</a></p>
","transformer-model"
"74574350","Validation loss never decreases during training","2022-11-25 14:36:37","","0","106","<deep-learning><training-data><transformer-model><machine-translation><seq2seq>","<p><a href=""https://i.sstatic.net/GJ3Ei.jpg"" rel=""nofollow noreferrer"">enter image description here</a>I encountered one perplexing situation: I trained a Transformer model for conditional music generation, finding the training loss decrease while the validation loss keeps increasing from the beginning, which is quite strange.</p>
<p>I attached a tensorboard screenshot for your reference. Here is the data allocation of about 2.7k songs: train:valid:test = 7:2:1.</p>
<p>I even try changing the learning rate from 0.01 to 0.00001, resulting nonetheless the same situation. On this above, shall I use more data for training or are there any other possible solutions regarding this?</p>
","transformer-model"
"74543465","Why using tf.transpose for transformer on my own data, made problems in model.fit function?","2022-11-23 08:03:37","","0","161","<python><tensorflow><artificial-intelligence><training-data><transformer-model>","<p>I want to do fine tuning for a transformer model, on my data. I based on <a href=""https://github.com/sayakpaul/TF-2.0-Hacks/blob/master/data2vec_vision_image_classification.ipynb"" rel=""nofollow noreferrer"">that</a> code but the difference, it is my data. I tried to load the data as tf.data.Dataset.from_generator, like explained <a href=""https://www.tensorflow.org/guide/data"" rel=""nofollow noreferrer"">here</a></p>
<p>Then by performing transform in this way:</p>
<pre><code>def make_dataset(dataset: tf.data.Dataset, train: bool, image_size: int = IMAGE_SIZE):
def preprocess(image, label):
    # for training, do augmentation
    if train:
        if tf.random.uniform(shape=[]) &gt; 0.5:
            image = tf.image.flip_left_right(image)
    image = tf.image.resize(image, size=image_size, method=&quot;bicubic&quot;)
    return image, label

if train:
    dataset = dataset.shuffle(BATCH_SIZE * 10)

dataset = dataset.map(preprocess, AUTO).batch(BATCH_SIZE)
# Transpose because the `transformers` model has a leading channel dimension.
dataset = dataset.map(lambda x, y: (tf.transpose(x, [0, 3, 1, 2]), y), AUTO)**
return dataset.prefetch(AUTO)
</code></pre>
<p>I get the following error:</p>
<pre><code>    ValueError                                Traceback (most recent call last)
/tmp/ipykernel_2703355/4289617362.py in &lt;module&gt;
----&gt; 1 train_dataset = make_dataset(train_dataset, True)
      2 val_dataset = make_dataset(val_dataset, False)

/tmp/ipykernel_2703355/2956852849.py in make_dataset(dataset, train, image_size)
     17     dataset = dataset.map(preprocess, AUTO).batch(BATCH_SIZE)
     18     # Transpose because the `transformers` model has a leading channel dimension.
---&gt; 19     dataset = dataset.map(lambda x, y: (tf.transpose(x, [0, 3, 1, 2]), y), AUTO)
     20     return dataset.prefetch(AUTO)

~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py in map(self, map_func, num_parallel_calls, deterministic, name)
   2016       return MapDataset(self, map_func, preserve_cardinality=True, name=name)
   2017     else:
-&gt; 2018       return ParallelMapDataset(
   2019           self,
   2020           map_func,

~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)
   5232     self._input_dataset = input_dataset
   5233     self._use_inter_op_parallelism = use_inter_op_parallelism
-&gt; 5234     self._map_func = structured_function.StructuredFunctionWrapper(
   5235         map_func,
   5236         self._transformation_name(),
...
    File &quot;/tmp/ipykernel_2703355/2956852849.py&quot;, line 19, in None  *
        lambda x, y: (tf.transpose(x, [0, 3, 1, 2]), y)

    ValueError: Dimension must be 5 but is 4 for '{{node transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](args_0, transpose/perm)' with input shapes: [?,32,224,224,3], [4].
</code></pre>
<p>I do not understand what that error is about. Who needs 5 dimensions, and how to fix it?</p>
<p>Edit:</p>
<p>The shape of data:</p>
<pre><code>train_dataset.element_spec

    (TensorSpec(shape=(32, 256, 256, 3), dtype=tf.float32, name=None),
 TensorSpec(shape=(32, 5), dtype=tf.float32, name=None))
</code></pre>
<p>and:</p>
<pre><code>val_dataset.element_spec
</code></pre>
<p>gave me the same.</p>
<p><a href=""https://colab.research.google.com/drive/1IhV06R7VTQ0nQ1gs4PqThrKo18lHShIC#scrollTo=Vz_kqZWrgykI"" rel=""nofollow noreferrer"">link</a> to my code</p>
","transformer-model"
"74517147","AutoTokenizer.from_pretrained('google/byt5-base') giving error: OSError: Can't load config and internet connection broken by 'NewConnectionError('","2022-11-21 10:01:00","","1","294","<nlp><huggingface-transformers><transformer-model><huggingface>","<pre><code>`tokenizer = AutoTokenizer.from_pretrained('google/byt5-base')
</code></pre>
<p>OSError: Can't load config for 'google/byt5-base'. Make sure that:</p>
<ul>
<li><p>'google/byt5-base' is a correct model identifier listed on 'https://huggingface.co/models'</p>
</li>
<li><p>or 'google/byt5-base' is the correct path to a directory containing a config.json file</p>
</li>
</ul>
<p>Edit:</p>
<p>Also was getting error below while upgrading transformer</p>
<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('
</code></pre>
<p>I refereed <a href=""https://huggingface.co/google/byt5-base"" rel=""nofollow noreferrer"">this</a> article and checked correct path in the  model repository as well but no luck</p>
<p>Any help highly appreciated! Thanks.</p>
","transformer-model"
"74455494","Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)`","2022-11-16 05:15:03","","1","312","<python><keras><tf.keras><transformer-model>","<p>while implementing the transformer for internet traffic forecasting i am getting this error &quot;ValueError: Unexpected result of <code>train_function</code> (Empty logs). Please use <code>Model.compile(..., run_eagerly=True)</code>, or <code>tf.config.run_functions_eagerly(True)</code> for more information of where went wrong, or file a issue/bug to <code>tf.keras</code>.&quot;</p>
<p><a href=""https://colab.research.google.com/drive/1NEHpa_o6OsNBlG2a5IzfjympPc5mUXyU?usp=sharing"" rel=""nofollow noreferrer"">The notebook link</a></p>
<p>Parameter values</p>
<pre><code>{
    &quot;look_back&quot; : 7,
    &quot;n_features&quot; : 1,
    &quot;horizon&quot; : 7,

    &quot;n_layers_init&quot; : 2,
    &quot;n_layers_end&quot; : 6,

    &quot;n_units_init&quot; : 8,
    &quot;n_units_end&quot; : 512,

    &quot;unit&quot; : &quot;lstm&quot;,

    &quot;log_dir&quot; : &quot;log&quot;,
    &quot;checkpoint_dir&quot; : &quot;checkpoint&quot;

}
</code></pre>
<p>what to run this code</p>
","transformer-model"
"74454598","TabTransformer Multiclass classification","2022-11-16 02:53:21","","1","358","<python><tensorflow><deep-learning><tabular><transformer-model>","<p>TabTransformer: Is there any documentation or example implementation of multiclass tabular data classifiation using TabTransformer in Tensorflow?</p>
<p>Following reference from keras is for binary classification only:
<a href=""https://keras.io/examples/structured_data/tabtransformer/"" rel=""nofollow noreferrer"">https://keras.io/examples/structured_data/tabtransformer/</a></p>
<p>And converting the sigmoid to softmax and increasing the output layer nodes to C(total classes) comes out with errors.</p>
","transformer-model"
"74433941","Node: 'IteratorGetNext' - INVALID_ARGUMENT: Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [5], [batch]: [0]","2022-11-14 15:18:24","","1","471","<python><tensorflow><keras><nlp><transformer-model>","<p>I'm trying to work on the Kaggle Getting Started <a href=""https://www.kaggle.com/competitions/nlp-getting-started"" rel=""nofollow noreferrer"">Natural Language Processing with Disaster Tweets</a> competition as an exam project for my uni deep learning course.</p>
<p>I am trying to solve the problem using a multi-input network, where the keyword and location columns are handled by two separate Conv1D networks, and the text column by a TransformerEncoder. I have the Conv1D networks working, but the TransformerEncoder is giving me the error in the title. I am using word embeddings (tried using both trained from scratch and with GloVe embeddings, but both give the same error) and positional encoding, and these are based on the <a href=""https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb"" rel=""nofollow noreferrer"">implementation</a> (TransformerEncoder and PositionalEncoding classes) in the Chollet Deep Learning with Python, 2nd edition book.</p>
<p>This is how I process the dataset:</p>
<ol>
<li><p>I import the .csv as a Pandas DataFrame and after some pre-processing, make a training and validation split</p>
</li>
<li><p>I convert the DataFrames into tf.data tensor datasets:</p>
</li>
</ol>
<pre><code>train_text = data.Dataset.from_tensor_slices((train_data['text'].values.astype(str), train_data['target'].values.astype(bool)))
train_keywords = data.Dataset.from_tensor_slices((train_data['keyword'].values.astype(str), train_data['target'].values.astype(bool)))
train_loc = data.Dataset.from_tensor_slices((train_data['location'].values.astype(str), train_data['target'].values.astype(bool)))

val_text = data.Dataset.from_tensor_slices((validation_data['text'].values.astype(str), validation_data['target'].values.astype(bool)))
val_keywords = data.Dataset.from_tensor_slices((validation_data['keyword'].values.astype(str), validation_data['target'].values.astype(bool)))
val_loc = data.Dataset.from_tensor_slices((validation_data['location'].values.astype(str), validation_data['target'].values.astype(bool)))
</code></pre>
<p>I have also tried using methods that resemble the methods in <a href=""https://www.tensorflow.org/tutorials/load_data/pandas_dataframe#a_dataframe_as_a_dictionary"" rel=""nofollow noreferrer"">Load a pandas DataFrame</a> more, but I got the same results.</p>
<ol start=""3"">
<li>I perform tokenization, standardization, and vectorization:</li>
</ol>
<pre><code>text_vectorization = TextVectorization(
    max_tokens=MAX_TOKENS_TEXT,
    output_sequence_length=max_text_length,
    standardize=standardize_text,
    output_mode='int'
)
keyword_vectorization = TextVectorization(
    max_tokens=MAX_TOKENS_KEYWORDS,
    output_sequence_length=MAX_KEYWORD_LENGTH,
    standardize=standardize_keywords,
    output_mode='int'
)
loc_vectorization = TextVectorization(
    max_tokens=MAX_TOKENS_KEYWORDS,
    output_sequence_length=MAX_LOC_LENGTH,
    standardize=standardize_loc,
    output_mode='int'
)

text_vectorization.adapt(train_text.map(lambda x, y: x))
keyword_vectorization.adapt(train_keywords.map(lambda x, y: x))
loc_vectorization.adapt(train_loc.map(lambda x, y: x))

train_text_vectorized = train_text.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=-1   # According to the documentation, -1 means auto
).batch(BATCH_SIZE)
train_loc_vectorized = train_loc.map(
    lambda x, y: (loc_vectorization(x), y),
    num_parallel_calls=-1
).batch(BATCH_SIZE)
train_keywords_vectorized = train_keywords.map(
    lambda x, y: (keyword_vectorization(x), y),
    num_parallel_calls=-1
).batch(BATCH_SIZE)

val_text_vectorized = val_text.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=-1
).batch(BATCH_SIZE)
val_loc_vectorized = val_loc.map(
    lambda x, y: (loc_vectorization(x), y),
    num_parallel_calls=-1
).batch(BATCH_SIZE)
val_keywords_vectorized = val_keywords.map(
    lambda x, y: (keyword_vectorization(x), y),
    num_parallel_calls=-1
).batch(BATCH_SIZE)
</code></pre>
<p>Here, I have also tried the following, with the same result:</p>
<ul>
<li>Using a batch size of 1 instead of 32</li>
<li>Not using .batch at all</li>
<li>Using .padded_batch instead of .batch</li>
</ul>
<ol start=""4"">
<li>Based on a recommendation <a href=""https://github.com/keras-team/keras/issues/16016#issuecomment-1312666517"" rel=""nofollow noreferrer"">here</a>, I zip up the data into a format that the network will accept as input:</li>
</ol>
<pre><code>def dataset_zipper(loc, text, keyword):
    return (loc[0], text[0], keyword[0]), text[1]

train_full_vectorized = data.Dataset.zip((train_loc_vectorized, train_text_vectorized, train_keywords_vectorized))
train_full_vectorized = train_full_vectorized.map(dataset_zipper, num_parallel_calls=-1)

val_full_vectorized = data.Dataset.zip((val_loc_vectorized, val_text_vectorized, val_keywords_vectorized))
val_full_vectorized = val_full_vectorized.map(dataset_zipper, num_parallel_calls=-1)
</code></pre>
<p>Now I build the network:
1.</p>
<pre><code>loc_input = Input(shape=(MAX_TOKENS_KEYWORDS,), dtype='int64', name='location')
keyword_input = Input(shape=(MAX_TOKENS_KEYWORDS,), dtype='int64', name='keyword')
text_input = Input(shape=(MAX_TOKENS_TEXT,), dtype=&quot;int64&quot;, name='text')

full_network = concatenate([
    generate_convnet(loc=True, input_layer=loc_input),
    generate_transformer(input_layer=text_input),
    generate_convnet(loc=False, input_layer=keyword_input)
])
full_network = Dropout(0.3)(full_network)
full_network = Dense(1, activation='sigmoid')(full_network)  # This is the classifier - since this is binary classification, I will use sigmoid activation
</code></pre>
<ol start=""2"">
<li></li>
</ol>
<pre><code>model = Model(inputs=[loc_input, text_input, keyword_input], outputs=full_network)
model.compile(loss='binary_crossentropy',
              optimizer=Adam(learning_rate=0.001),
              metrics=['binary_accuracy'])
</code></pre>
<ol start=""3"">
<li></li>
</ol>
<pre><code>callbacks = [
    ModelCheckpoint('twitter_disasters_v1.h5', save_best_only=True),
    EarlyStopping(monitor='val_loss', patience=5, mode='min')]

results = model.fit(x=train_full_vectorized, validation_data=val_full_vectorized, class_weight=class_weights, callbacks=callbacks, epochs=100)
</code></pre>
<p>And here's where I get the error:</p>
<pre><code>Node: 'IteratorGetNext'
2 root error(s) found.
  (0) INVALID_ARGUMENT:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [5], [batch]: [0]
     [[{{node IteratorGetNext}}]]
     [[gradient_tape/model_1/transformer_encoder_1/multi_head_attention_1/query/einsum/Einsum/_144]]
  (1) INVALID_ARGUMENT:  Cannot add tensor to the batch: number of elements does not match. Shapes are: [tensor]: [5], [batch]: [0]
     [[{{node IteratorGetNext}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_117129]
</code></pre>
","transformer-model"
"74428413","Understanding dimensions in MultiHeadAttention layer of Tensorflow","2022-11-14 07:50:28","74430271","2","4376","<tensorflow><nlp><transformer-model><attention-model>","<p>I'm learning multi-head attention with <a href=""https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853"" rel=""nofollow noreferrer"">this article</a>.
As the writer claimed, the structure of MHA (by the original paper) is as follows:</p>
<p><a href=""https://i.sstatic.net/hwDQq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hwDQq.png"" alt=""enter image description here"" /></a></p>
<p>But the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention"" rel=""nofollow noreferrer""><code>MultiHeadAttention</code></a> layer of Tensorflow seems to be more flexible:</p>
<ol>
<li>It does not require <code>key_dim * num_heads = embed_dim</code>. Like:</li>
</ol>
<pre><code>layer = tf.keras.layers.MultiHeadAttention(num_heads = 2, key_dim = 4)
x = tf.keras.Input(shape=[3, 5])
layer(x, x)
# no error
</code></pre>
<p>Does the depth of the weight matrix in <code>tf.MHA</code> layer set to <code>key_dim * num_heads</code> regardless of <code>embed_dim</code>? So that Q/K/V can still be properly split by <code>num_heads</code>.</p>
<ol start=""2"">
<li>However, the output depth of tf.MHA layer is (by default) guaranteed to be <code>embed_dim</code>. So there is a final dense layer with <code>embed_dim</code> nodes to ensure the dimension？</li>
</ol>
","transformer-model"
"74415426","How to stop data shuffling while training the HuggingFace BERT model?","2022-11-12 18:16:53","","4","2770","<nlp><huggingface-transformers><bert-language-model><transformer-model>","<p>I want to train a BERT transformer model using the <code>HuggingFace</code> implementation/library. During training, <code>HuggingFace</code> shuffles the training data for each epoch, but I don't want to shuffle the data. For example, if I have 5 training data and the batch size = 2, then I want the training data to be presented as [1, 2], [2, 3], [3, 4] and [4, 5]. I cannot find any resources that show how to disable the default shuffling.</p>
","transformer-model"
"74360282","BOS token for encoder decoder models","2022-11-08 11:52:42","74386139","2","1358","<deep-learning><huggingface-transformers><transformer-model><huggingface-tokenizers>","<p>I’m using T5-base for my model, and it seems to be generating something reasonable when I do <code>model.generate</code>. But my question is how?</p>
<p>The decoder part of this model needs a starting token to start decoding doesn’t it? How does it figure out what the very first token is supposed to look like?</p>
<p>Or am I doing the training wrong where I should have included a token?</p>
<p>If needed <a href=""https://www.kaggle.com/code/sachin/t5-for-grammar-correction"" rel=""nofollow noreferrer"">here is a code sample</a> where I used <code>model.generate</code>.</p>
<h2>Edit 1</h2>
<p>Apart from the answer below, I found that there was a <code>model.config.decoder_start_token_id</code>. This is not necessarily <code>&lt;bos&gt;</code>. In the case of <code>T5/Flan-T5</code> it ended up being <code>&lt;pad&gt;</code>.</p>
","transformer-model"
"74330016","Multihead Attention with for loop","2022-11-05 17:31:57","","1","152","<deep-learning><pytorch><nlp><transformer-model><attention-model>","<p>According to the Attention is All You Need paper</p>
<blockquote>
<p>Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively</p>
</blockquote>
<p>What I understand is there should be <code>n_heads</code> <em>different</em> linear layers so that each learns a different projection. Thus, I implemented the logic as follows</p>
<pre class=""lang-py prettyprint-override""><code>class Attention(nn.Module):
    def __init__(self, embed_size=512, out_feat=64) -&gt; None:
        super().__init__()
        self.embed_size = embed_size
        self.out_feat   = out_feat
        self.value_fc   = nn.Linear(embed_size, out_feat)
        self.query_fc   = nn.Linear(embed_size, out_feat)
        self.key_fc     = nn.Linear(embed_size, out_feat)

    def forward(self, value, key, query, mask=None) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        value: torch.Tensor of shape (N, seq_len, embed_size)
        key: torch.Tensor of shape (N, seq_len, embed_size)
        query: torch.Tensor of shape (N, seq_len, embed_size)
        mask: torch.Tensor of shape (N, seq_len, seq_len)

        returns torch.Tensor of shape (N, seq_len, out_feat)
        &quot;&quot;&quot;
        value = self.value_fc(value)  # N, seq_len, out_feat
        key = self.key_fc(key)
        query = self.query_fc(query)
        weights = torch.bmm(query, torch.transpose(key, 1, 2))
        weights /= math.sqrt(self.out_feat)  # N, query_len, key_len
        if mask != None:
            weights += mask
        weights = torch.softmax(weights, dim=2)
        return torch.bmm(weights, value)


class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size=512, n_heads=8) -&gt; None:
        super().__init__()
        assert embed_size % n_heads == 0, &quot;Input feat. dim must be div. by n_heads&quot;

        self.embed_size = embed_size
        self.n_heads = n_heads
        self.out_feat = embed_size // n_heads
        self.attention_layers = nn.ModuleList(
            [Attention(self.embed_size, self.out_feat) for _ in range(self.n_heads)]
        )
        self.fc = nn.Linear(embed_size, embed_size)

    def forward(self, value, key, query, mask=None):
        values = [
            attention(value, key, query, mask) for attention in self.attention_layers
        ]
        return self.fc(torch.cat(values, dim=2))
</code></pre>
<p>However, all the (official) implementations I have seen use a single Attention layer but reshapes the keys, queries and values into <code>(batch_size, len, n_heads, d_model // n_heads)</code> (then they apply transpose before computing the attention weights). Although this is more efficient to compute, there is only one linear layer for each key, query and value rather than <code>n_heads</code> linear layers. I think this contradicts with what the paper says. I know this increases the amount of learnable parameters a lot but isn't this the correct way according to the paper?</p>
","transformer-model"
"74290497","How to handle sequences longer than 512 tokens in layoutLMV3?","2022-11-02 14:06:32","","4","3028","<transformer-model><huggingface-tokenizers><huggingface>","<p>How to work with sequences longer than 512 tokens. I don't wanted to use truncates =True. But actually wanted to handle the longer sequences</p>
","transformer-model"
"74278898","How to mask inputs with variable size in transformer model when the batches needs to be masked differently?","2022-11-01 16:25:50","74329767","0","1307","<python><numpy><tensorflow><keras><transformer-model>","<p>I'm making a transformer using <code>tensorflow.keras</code> and having issues understanding how the <code>attention_mask</code> works for a <code>MultiHeadAttention</code> layer.</p>
<p>My input is 3-dimensional data. For example, let's assume my whole dataset has 10 elements, each one with length no more than 4:</p>
<pre class=""lang-py prettyprint-override""><code># whole data
[
  # first item
  [
    [     1,      2,      3],
    [     1,      2,      3],
    [np.nan, np.nan, np.nan],
    [np.nan, np.nan, np.nan],
  ],
  # second item
  [
    [     1,      2,      3],
    [     5,      8,      2],
    [     3,      7,      8],
    [     4,      6,      2],
  ],
  ... # 8 more items
]
</code></pre>
<p>So, my mask looks like:</p>
<pre class=""lang-py prettyprint-override""><code># assume this is a numpy array
mask = [
  [
    [1, 1, 1],
    [1, 1, 1],
    [0, 0, 0],
    [0, 0, 0],
  ],
  [
    [1, 1, 1],
    [1, 1, 1],
    [1, 1, 1],
    [1, 1, 1],
  ],
  ...
]
</code></pre>
<p>So the shape of the mask til now is <code>[10, 4, 3]</code>. Let's say I use <code>batch_size = 5</code>. Now, according documentation, <code>attention_mask</code> shape should be <code>[B, T, S]</code> (batch_size, query_size, key_size). In the example case should be <code>[5, 4, 4]</code>?</p>
<h2>Question</h2>
<p><strong>If the mask is calculated only once, what 5 items should I give as a mask? This sounds counterintuitive to me. How should I build the mask?</strong></p>
<p>According <a href=""https://stackoverflow.com/questions/67805117/multiheadattention-attention-mask-keras-tensorflow-example"">this</a> answer, head_size should be also taken in account, so they also do:</p>
<pre class=""lang-py prettyprint-override""><code>mask = mask[:, tf.newaxis, tf.newaxis, :]
</code></pre>
<h2>What I've tested</h2>
<p>The only time I manage to run the transformer successfully using the <code>attention_mask</code> is when I do:</p>
<pre class=""lang-py prettyprint-override""><code>mask = np.ones((batch_size, data.shape[1], data.shape[2]))
mask = mask[:, tf.newaxis, tf.newaxis, :]
</code></pre>
<p>Obviously that mask makes no sense, because it is all ones, but it was just to test if it had the correct shape.</p>
<h2>The model</h2>
<p>I'm using practically the same code from the <code>keras</code> <a href=""https://keras.io/examples/timeseries/timeseries_classification_transformer/"" rel=""nofollow noreferrer"">example</a> transformer for time series classification</p>
<pre class=""lang-py prettyprint-override""><code>def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.0, mask=None):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x, attention_mask=mask)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;relu&quot;)(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def build_model(
    n_classes,
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0.0,
    mlp_dropout=0.0,
    input_mask=None,
) -&gt; keras.Model:
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout, input_mask)

    x = layers.GlobalAveragePooling2D(data_format=&quot;channels_first&quot;)(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(n_classes, activation=&quot;softmax&quot;)(x)
    return keras.Model(inputs, outputs)
</code></pre>
","transformer-model"
"74266829","print shape of tensors in tensor2tensor","2022-10-31 17:07:52","","0","41","<tensorflow><transformer-model><tensor2tensor>","<p>I am trying to print the shape of the tensor (running the basic transformer code) using tensor2tensor. I see &quot;?&quot; in the shape.
I tried doing the following:</p>
<pre><code>    #with tf.Session() as sess:
    #  print(sess.run(x).shape)
</code></pre>
<p>But this gives me an error</p>
<pre><code>cannot assign a device for operation transformer/parallel_0/transformer/Identity: {{node transformer/parallel_0/transformer/Identity}} was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.
         [[transformer/parallel_0/transformer/Identity]]

</code></pre>
<p>I also tried to use tf.print(tf.shape(x)) but that doesn't print anything</p>
<p>I am running the command :</p>
<pre><code>t2t-decoder \
  --data_dir=$DATA_DIR \
  --problem=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --output_dir=$TRAIN_DIR \
  --decode_hparams=&quot;beam_size=$BEAM_SIZE,alpha=$ALPHA&quot; \
  --decode_from_file=$DECODE_FILE \
  --decode_to_file=translation.en
</code></pre>
<p>I don't know much about tensorflow, but I see that these are being used?</p>
<pre><code>tf.compat.v1.train.MonitoredSession(
            session_creator=tf.compat.v1.train.ChiefSessionCreator(
</code></pre>
<p>How Can I print shape of tensor in such a scenario? Please help!!!</p>
<pre><code></code></pre>
","transformer-model"
"74259529","Why accuracy of finetune transformer model is less when evaluated after loading from disk, than during training?","2022-10-31 06:36:22","","3","348","<python><pytorch><huggingface-transformers><transformer-model>","<p>I am finetuning a transformer model and during the training cycle, evaluating it at each epoch. The best model is selected based on the highest evaluation accuracy among all epochs. Once the training cycle is completed and the best model is dumped to the disk, I try to regenerate that validation accuracy. I am unable to regenerate the exact validation accuracy reported by the training phase. I am getting a 3% to 4% drop in accuracy on the same evaluation data.</p>
<p>(For regeneration, I am calling the same evaluation function and passing it model and dataset. Nothing else changed for evaluation accuracy regeneration)</p>
","transformer-model"
"74223324","Using huggingface transformers trainer method for hugging face datasets","2022-10-27 13:58:41","","1","2299","<python><nlp><huggingface-transformers><transformer-model><huggingface>","<p>I am trying to train a transformer(Salesforce codet5-small) using the huggingface trainer method and on a hugging face Dataset (namely, &quot;eth_py150_open&quot;). However, I'm encountering a number of issues.</p>
<p>Here is the relevant code snippet:</p>
<pre><code>import torch
import transformers
from datasets import load_dataset_builder
from datasets import load_dataset

corpus=load_dataset(&quot;eth_py150_open&quot;, split='train')

training_args = transformers.TrainingArguments( #general training arguments
    per_device_train_batch_size = 8,
    warmup_steps = 0,
    weight_decay = 0.01,
    learning_rate = 1e-4,
    num_train_epochs = 12,
    output_dir = './runs/run2/output/',
    logging_dir = './runs/run2/logging/',
    logging_steps = 50,
    save_steps= 10000,
    remove_unused_columns=False,
)

model = transformers.T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-small').cuda()

trainer = transformers.Trainer(
    model = model,
   args = training_args,
    train_dataset = corpus,
)

</code></pre>
<p>However, when running trainer.train(), I get the following error:</p>
<pre><code>***** Running training *****
  Num examples = 74749
  Num Epochs = 12
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed &amp; accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 112128
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-28-3435b262f1ae&gt; in &lt;module&gt;
----&gt; 1 trainer.train()

3 frames
/usr/local/lib/python3.7/dist-packages/transformers/trainer.py in _prepare_inputs(self, inputs)
   2414         if len(inputs) == 0:
   2415             raise ValueError(
-&gt; 2416                 &quot;The batch received was empty, your model won't be able to train on it. Double-check that your &quot;
   2417                 f&quot;training dataset contains keys expected by the model: {','.join(self._signature_columns)}.&quot;
   2418             )

TypeError: can only join an iterable
</code></pre>
<p>I have tried converting corpus to a torch Dataset object, but can't seem to figure out how to do this. I'd really appreciate any help!</p>
","transformer-model"
"74191069","How do I implement a knowledge base in a Huggingface model?","2022-10-25 08:21:53","","2","426","<bert-language-model><transformer-model><knowledge-graph>","<p>I made a knowledge base using <code>COMET</code> on the Atomic knowledge graph, using this <a href=""https://github.com/atcbosselut/comet-commonsense/tree/acf320a13456310bff6a29b7221a2dce552b3e2b"" rel=""nofollow noreferrer"">tutorial</a>.</p>
<p>I would like to include this knowledge in a regular pre-trained <code>BERT</code> model from <a href=""https://huggingface.co/bert-base-cased?text=Paris%20is%20the%20%5BMASK%5D%20of%20France."" rel=""nofollow noreferrer"">HuggingFace</a> to see how the model with access to this knowledge performs on a different task (sentiment analysis).</p>
<p>I saved the generated tuples from <code>COMET</code> in a pickle file.</p>
<p>Thanks!</p>
","transformer-model"
"74137023","Does NLP Transformer has backpropagation and how BERT has its word embedding?","2022-10-20 08:54:54","","0","430","<deep-learning><nlp><bert-language-model><word-embedding><transformer-model>","<p>I was reading Attention all you need papers and i have not get any idea how the weights are updated in the Transformer base architecture is there any Backpropagation ? normally yes for the model to learn and update his weights but could any one confirm me that and explain it to me if possible ?</p>
<p>I know about the sum up of the 3 embeddings in the input of the Transformer (sentece embedding, postional embedding and wordpiece embedding) howerver, could any one explain to me what'is exactly a wordpiece embedding ? all i know it has 30k vocabulary tokens but i did not know how it trained, is it trained by a Transformer?
Thanks !</p>
","transformer-model"
"74118363","How to generate vision transformer attention maps for 3D grayscale MRI data","2022-10-18 23:14:09","","0","782","<deep-learning><pytorch><computer-vision><transformer-model><self-attention>","<p>How can I generate attention maps for 3D grayscale MRI data after training with vision transformer for a classification problem?</p>
<p>My data shape is (120,120,120) and the model is 3D ViT. For example:</p>
<pre><code>img = nib.load()
img = torch.from_numpy(img)
model = torch.load...
model.eval()

output, attn = model(img)
</code></pre>
<p>After this, because I have 6 transformer layers and 12 heads, so the attn I got the shape that is</p>
<pre><code>(6,12,65,65)
</code></pre>
<p>Then I don't know how to apply this to original 3D grayscale images. I got several examples online that only deal with images from ImageNet.</p>
<p>For example:</p>
<p><a href=""https://github.com/tczhangzhi/VisionTransformer-Pytorch"" rel=""nofollow noreferrer"">https://github.com/tczhangzhi/VisionTransformer-Pytorch</a></p>
<p><a href=""https://github.com/jacobgil/vit-explain"" rel=""nofollow noreferrer"">https://github.com/jacobgil/vit-explain</a></p>
<p>Can anyone help me with this?</p>
","transformer-model"
"74109833","How to set learning rate 0.2 when training transformer with Noam decay?","2022-10-18 10:58:32","","0","491","<python><deep-learning><transformer-model><learning-rate>","<p>I am training small transformer encoder - transformer decoder translation model using small datasets.</p>
<p>Size of my dataset is less than 200k.</p>
<p>When training transformer with low resource datasets, below 2 papers suggests to use learning rate 2 (reference 2), or 0.2 (reference 1) respectively with Noam decay.</p>
<p>However, I dont know how to set learning rate 2 or 0.2 when I use Noam decay scheduler.</p>
<p>Because as far as I know, when I use Noam decay scheduler, learning rate is determined by <strong>model dimension, step number, and warmup step size</strong>. so I dont know how to set learning rate 2 or 0.2, and what it means.</p>
<p>When I modified Noam decay scheduler and made linear warm up and square root decay, with peak 0.2, the model does not converge.</p>
<p>Thanks in advance.</p>
<p><strong>reference papers</strong></p>
<ol>
<li><a href=""https://aclanthology.org/C18-1054.pdf"" rel=""nofollow noreferrer"">https://aclanthology.org/C18-1054.pdf</a></li>
<li><a href=""https://aclanthology.org/2021.mtsummit-research.5.pdf"" rel=""nofollow noreferrer"">https://aclanthology.org/2021.mtsummit-research.5.pdf</a></li>
</ol>
","transformer-model"
"74095947","What does num_labels actually do?","2022-10-17 10:33:37","","0","1066","<pytorch><huggingface-transformers><transformer-model><pytorch-lightning>","<p>When training a BERT based model one can set <code>num_labels</code></p>
<p><code>AutoConfig.from_pretrained(BERT_MODEL_NAME, num_labels=num_labels)</code></p>
<p>So for example if we want to have a prediction of 3 values we may use <code>num_labels=3</code>.</p>
<p>My question is what does it do internally? Is it just connecting a <code>nn.Linear</code> to the last embedding layer?</p>
<p>Thanks</p>
","transformer-model"
"74075650","Transformer Model only predict Start or End Tokens","2022-10-14 23:27:26","","0","1033","<pytorch><transformer-model><cross-entropy>","<p>So I've been trying to build and train a Transformer Model from scratch for empathetic dialogue generation tasks and currently I'm struggling with the training process since the model only seems to predict START and END tokens in the final output layer irrespective of the target token given to the Transformer decoder. I've gone through the implementation multiple times and spotted and corrected some issues (mostly with the MultiHead Attention Layer and tokenization), however still haven't had any luck.</p>
<p>I am using <code>F.cross_entropy</code> to compute the cross entropy between the final logits outputted from the transformer <code>out[:, :-1:, :]</code> and the target sequence in my dataset <code>target[:, 1:]</code>. The shifts are of course necessary since each output of the transformer corresponds to the next predicted token. I tried removing the START and END tokens in this loss function (i.e., <code>out[:, :-2:, :] and target[:, 1:-1]</code>) but this didn't help either. The logits and targets are all shaped according to PyTorch documentation i.e., (batch_size, classes, sequence_length) and (batch_size, sequence_length) respectively with the target containing the class indices (the padding index is hence ignored). The training output looks something like this.</p>
<pre><code>Epoch 0:   1%|          | 1/180 [00:05&lt;16:28,  5.53s/it, loss=11, v_num=3, train_loss=11.00]
Epoch 0:   1%|          | 2/180 [00:25&lt;37:55, 12.78s/it, loss=11, v_num=3, train_loss=11.00]
...
Epoch 5:  90%|█████████ | 162/180 [00:58&lt;00:06,  2.77it/s, loss=5.54, v_num=3, train_loss=5.520]
Epoch 5:  90%|█████████ | 162/180 [00:58&lt;00:06,  2.77it/s, loss=5.53, v_num=3, train_loss=5.430]
</code></pre>
<p>As seen above, the loss decays to a constant loss value between 5-6 and stays constant (even up to the 50th epoch). I printed out the probability tensors at each training step by softmax-ing the logits. Highest probabilities are attributed to the START and END tokens irrespective of the target token into the transformer decoder.</p>
<p>To confirm this behavior, I wrote a script to predict a response from the trained model (using beam search) given a context sequence and setting the first target token to [START]. No matter what context sequence I input into the model or what beam width I use, the next target token is always predicted to be [END]. I'm not sure if this has something to do with tokenization or some weights in the model exploding but I can't seem to get rid of this behaviour. I even included dropout layers to eliminate the latter problem and still not luck. This issue persists even if I remove the emotional embeddings I am adding in the decoder.</p>
<p>Here is the full implementation of the Model for reference:</p>
<pre><code>class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size: int, heads: int) -&gt; None:
        super().__init__()

        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = self.embed_size // self.heads

        assert self.head_dim * self.heads == self.embed_size

        self.values = nn.Linear(self.embed_size, self.embed_size, bias=False)
        self.keys = nn.Linear(self.embed_size, self.embed_size, bias=False)
        self.queries = nn.Linear(self.embed_size, self.embed_size, bias=False)

        self.fc_out = nn.Linear(self.embed_size, self.embed_size, bias=False)

    def forward(
        self,
        keys: torch.Tensor, 
        values: torch.Tensor, 
        queries: torch.Tensor, 
        mask: torch.Tensor
    ) -&gt; torch.Tensor:

        N = queries.shape[0]
        keys_len, values_len, queries_len = keys.shape[1], values.shape[1], queries.shape[1]

        values = self.values(values).reshape(N, values_len, self.heads, self.head_dim)
        keys = self.keys(keys).reshape(N, keys_len, self.heads, self.head_dim)
        queries = self.queries(queries).reshape(N, queries_len, self.heads, self.head_dim)

        scores = torch.einsum(&quot;nqhd,nkhd-&gt;nhqk&quot;, [queries, keys])

        # Apply mask to attention scores if specified
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float(&quot;-1e20&quot;))

        # Normalise with respect to all keys
        attention = F.softmax(scores / (self.embed_size ** 0.5), dim=-1)

        out = torch.einsum(&quot;nhqk,nvhd-&gt;nqhd&quot;, [attention, values])
        out = self.fc_out(out.reshape(N, queries_len, self.embed_size))

        return out


class TransformerBlock(nn.Module):
    def __init__(
        self,
        embed_size: int, 
        heads: int, 
        dropout: float, 
        forward_expansion: int
    ) -&gt; None:

        super().__init__()

        self.attention = MultiHeadAttention(embed_size, heads)

        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)

        self.dropout = nn.Dropout(dropout)

        self.ff = nn.Sequential(
            nn.Linear(embed_size, embed_size * forward_expansion),
            nn.ReLU(),
            nn.Linear(embed_size * forward_expansion, embed_size)
        )

    def forward(
        self,
        keys: torch.Tensor, 
        values: torch.Tensor, 
        queries: torch.Tensor, 
        mask: torch.Tensor
    ) -&gt; torch.Tensor:

        attention = self.attention(keys, values, queries, mask)

        contextualised = self.dropout(self.norm1(attention + queries))
        forward = self.ff(contextualised)
        out = self.dropout(self.norm2(forward + contextualised))

        return out

class Encoder(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        padding_idx: int,
        num_layers: int,
        embed_size: int,
        heads: int,
        dropout: float, 
        forward_expansion: int,
        max_seq_len: int,
        num_of_emo_labels: int
    ) -&gt; None:

        super().__init__()

        self.word_embeddings = nn.Embedding(
            vocab_size + 1, embed_size, padding_idx=padding_idx)
        self.pos_embeddings = nn.Embedding(max_seq_len, embed_size)
        self.ds_embeddings = nn.Embedding(2 + 1, embed_size, padding_idx=0)

        self.layers = nn.ModuleList(
            [TransformerBlock(embed_size, heads, dropout, forward_expansion)
             for _ in range(num_layers)]
        )

        self.dropout = nn.Dropout(dropout)
    
    def forward(
        self, 
        context: torch.Tensor, 
        context_ds_state: torch.Tensor,
        mask: torch.Tensor,
        emotion_label: torch.Tensor
    ) -&gt; torch.Tensor:

        N, seq_len = context.shape
        positions = torch.arange(0, seq_len, device=context.device).expand(N, seq_len)

        word_embeddings = self.word_embeddings(context)
        pos_embeddings = self.pos_embeddings(positions)
        ds_embeddings = self.ds_embeddings(context_ds_state)

        out = self.dropout(word_embeddings + pos_embeddings + ds_embeddings)

        for layer in self.layers:
            out = layer(out, out, out, mask)
        
        return out

class DecoderBlock(nn.Module):
    def __init__(
        self,
        embed_size: int,
        heads: int,
        dropout: float,
        forward_expansion: int
    ) -&gt; None:

        super().__init__()

        self.attention = MultiHeadAttention(embed_size, heads)
        self.norm = nn.LayerNorm(embed_size)
        self.transformer_block = TransformerBlock(
            embed_size,
            heads, 
            dropout, 
            forward_expansion
        )
        self.dropout = nn.Dropout(dropout)

    def forward(
        self,
        x: torch.Tensor,
        keys: torch.Tensor,
        values: torch.Tensor,
        target_mask: torch.Tensor,
        input_mask: torch.Tensor
    ) -&gt; torch.Tensor:
        
        attention = self.attention(x, x, x, target_mask)
        queries = self.dropout(self.norm(attention + x))
        out = self.transformer_block(keys, values, queries, input_mask)

        return out

class Decoder(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        padding_idx: int,
        num_layers: int,
        embed_size: int,
        heads: int,
        dropout: float, 
        forward_expansion: int,
        max_seq_len: int,
        num_of_emo_labels: int
    ) -&gt; None:

        super().__init__()

        self.word_embeddings = nn.Embedding(
            vocab_size + 1, embed_size, padding_idx=padding_idx)
        self.pos_embeddings = nn.Embedding(max_seq_len, embed_size)
        self.ds_embeddings = nn.Embedding(2 + 1, embed_size, padding_idx=0)
        self.emotion_embedding = nn.Embedding(num_of_emo_labels, embed_size)

        self.layers = nn.ModuleList(
            [DecoderBlock(embed_size, heads, dropout, forward_expansion)
             for _ in range(num_layers)]
        )

        self.dropout = nn.Dropout(dropout)

        self.fc_out = nn.Linear(embed_size, vocab_size)

    def forward(
        self,
        target: torch.Tensor,
        target_ds_state: torch.Tensor,
        encoder_out: torch.Tensor,
        target_mask: torch.Tensor,
        input_mask: torch.Tensor,
        emotion_label: torch.Tensor
    ) -&gt; torch.Tensor:

        N, seq_len = target.shape
        positions = torch.arange(0, seq_len, device=target.device).expand(N, seq_len)

        word_embeddings = self.word_embeddings(target)
        pos_embeddings = self.pos_embeddings(positions)
        ds_embeddings = self.ds_embeddings(target_ds_state)

        out = self.dropout(word_embeddings + pos_embeddings + ds_embeddings)
        
        for layer in self.layers:
            out = layer(out, encoder_out, encoder_out, target_mask, input_mask)
        
        emotion_embedding = self.emotion_embedding(
            emotion_label).unsqueeze(1).expand(-1, seq_len, -1)
        
        out = self.fc_out(out + emotion_embedding)

        return out

class Transformer(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        num_of_emo_labels: int,
        max_seq_len: int,
        padding_idx: int,
        num_layers: int = 6,
        embed_size: int = 256,
        heads: int = 8,
        dropout: float = 0.5, 
        forward_expansion: int = 4
    ) -&gt; None:

        super().__init__()

        self.padding_idx = padding_idx
        self.encoder = Encoder(
            vocab_size,
            padding_idx,
            num_layers, 
            embed_size, 
            heads,
            dropout, 
            forward_expansion, 
            max_seq_len,
            num_of_emo_labels
        )

        self.decoder = Decoder(
            vocab_size,
            padding_idx,
            num_layers, 
            embed_size, 
            heads,
            dropout, 
            forward_expansion, 
            max_seq_len,
            num_of_emo_labels
        )

    def create_padding_mask(self, batch_seq):
        N = batch_seq.size(dim=0)
        padding_mask = (batch_seq != self.padding_idx).unsqueeze(1).unsqueeze(2)
        return padding_mask
    
    def create_lookahead_mask(self, batch_seq):
        N, seq_len = batch_seq.shape
        lookahead_mask = torch.tril(torch.ones(
            N, 1, seq_len, seq_len, device=batch_seq.device))
        return lookahead_mask
    
    def forward(
        self,
        context: torch.Tensor,
        target: torch.Tensor,
        context_ds_state: torch.Tensor,
        target_ds_state: torch.Tensor,
        emotion_label: torch.Tensor
    ) -&gt; None:

        input_mask = self.create_padding_mask(context)
        target_mask = torch.minimum(
            self.create_lookahead_mask(target), 
            self.create_padding_mask(target)
        )

        encoder_out = self.encoder(
            context, 
            context_ds_state, 
            input_mask,
            emotion_label
        )
        out = self.decoder(
            target, 
            target_ds_state,
            encoder_out, 
            target_mask, 
            input_mask, 
            emotion_label
        )

        return out
</code></pre>
<p>I have used both Adam and AdamW as my optimizers with a StepLR scheduler if that's relevant. I've been stuck on this problem for a while now so any help would be appreciated. Thanks in advance :)</p>
","transformer-model"
"74069064","Solving an Algebraic Loop in Simulink using an Initial Value","2022-10-14 12:15:07","","0","1229","<matlab><simulink><transformer-model>","<p>I am building a circuit model for a transformer which models the effects of hysteresis. It does so using the Matlab function block on the right, and works successfully when tested in isolation. However, the value of the magnetising inductance Lm depends on calculations requiring the value of Im. But Simulink cannot determine the value of Im without the value of Lm, thus forming an algebraic loop.</p>
<p>However, I have the initial value for the inductance, Lm_initial loaded into the workspace. With this, I should be able to solve for the first Im value, which can be used to determine the next Lm, and so on. However, specifying Lm_initial in the variable inductor's properties doesn't work; Simulink tries to evaluate Lm with the nonexistent 'phi' and 'Im' values rather than trying to solve for an initial Im using the value of the initial inductance.</p>
<p>I have tried solutions involving commenting/uncommenting blocks and implementing further subsystems which activate/deactivate depending on the time step, as well as unit delays, but these run into issues regarding tracking time for calculating the derivatives or output very incorrect/noisy waveforms.</p>
<p>Is there a relatively simple solution for this case? The problem appears as if it'd be relatively simple to solve, but I cannot seem to find a workaround for this.</p>
<p><a href=""https://i.sstatic.net/5JbuH.png"" rel=""nofollow noreferrer"">Transformer Equivalent Model</a></p>
","transformer-model"
"74056019","Inconsistent shapes between outputs and targets","2022-10-13 12:45:18","","0","137","<python><pytorch><huggingface-transformers><transformer-model>","<p>I have the following code to train a transformer-based model (huggingface) for a multi-head regression task (I call it multi-head because the model predicts multiple output scores, not only one).</p>
<pre><code># select device
device = 'cuda' if cuda.is_available() else 'cpu'
print(&quot;DEVICE: &quot;, device)
MAX_LEN = 512
TRAIN_BATCH_SIZE = 8
VALID_BATCH_SIZE = 8
TRAIN_EPOCHS = 1
TEST_EPOCHS = 1
LEARNING_RATE = 2e-05
REG_DROPOUT = 0.1
DISPLAY_STEP_THRESHOLD = 100
MODEL_NAME_CHECKPOINT = 'bert-base-uncased'
MODEL_FOLDER = 'autotune_multihead_regression_model'
# *************** DATA *********************
# load data
train_data = pd.read_csv(f&quot;derived_data/traindev_fold{args.fold}.csv&quot;)
test_data = pd.read_csv(f&quot;derived_data/test_fold{args.fold}.csv&quot;)

train_data['labels'] = train_data[train_data.columns[2:]].values.tolist()
test_data['labels'] = test_data[test_data.columns[2:]].values.tolist()

print(&quot;train data shape: &quot;, train_data.shape)
print(&quot;test data shape: &quot;, test_data.shape)

# make datasets
train_dataset = Dataset.from_pandas(train_data)
test_dataset = Dataset.from_pandas(test_data)

dataset = DatasetDict({
    'train': train_dataset,
    'test': test_dataset
})

# initialize tokenizer
tokenizer = BertTokenizer.from_pretrained(MODEL_NAME_CHECKPOINT)


def preprocess_function(examples):
    return tokenizer(examples[&quot;full_text&quot;], truncation=True, padding=&quot;max_length&quot;)


# apply the preprocessing on the entire dataset
encoded_dataset = dataset.map(preprocess_function, batched=True)


# ******************** FINETUNING ********************#

class BERTClass(torch.nn.Module):
    def __init__(self):
        super(BERTClass, self).__init__()
        self.l0 = transformers.BertModel.from_pretrained(MODEL_NAME_CHECKPOINT)
        self.l1 = torch.nn.Linear(768, 6)

    def forward(self, input_ids, attention_mask, labels):
        &quot;&quot;&quot;&quot;Override the function forward. Note that keys not appearing here will be removed by trainer.
        It does not matter if trainer is not used.
        &quot;&quot;&quot;
        # _, output_0 = self.l0(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)
        _, output_0 = self.l0(input_ids, attention_mask=attention_mask, return_dict=False)
        output = self.l1(output_0)
        return output


def model_init():
    model = BERTClass()
    return model


args = TrainingArguments(
    MODEL_FOLDER,
    evaluation_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=TRAIN_BATCH_SIZE,
    per_device_eval_batch_size=VALID_BATCH_SIZE,
    num_train_epochs=5,
    weight_decay=0.01,
    load_best_model_at_end=True,
    # this is an advantage in the sense that the last model is not necessarily the best one.
    metric_for_best_model=&quot;mean_rmse&quot;,
    logging_strategy=&quot;steps&quot;,
    logging_steps=100,
    push_to_hub=False
)


def mean_rmse(outputs, targets):
    &quot;&quot;&quot;&quot;
    :param
        outputs: 2D list
        targets: 2D list
    :returns
        a scalar real number
    &quot;&quot;&quot;
    delta = outputs - targets
    delta = torch.sqrt((delta ** 2).mean(axis=0))
    return delta.mean()


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    return {&quot;mean_rmse&quot;: mean_rmse(predictions, labels)}


class RegressionTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get(&quot;labels&quot;)
        outputs = model(**inputs)
        loss = torch.nn.MSELoss()(outputs.squeeze(), labels.squeeze())
        return (loss, outputs) if return_outputs else loss

temp_dataset = encoded_dataset[&quot;train&quot;].select(range(100))

trainer = RegressionTrainer(
    model_init(),
    args,
    train_dataset=encoded_dataset[&quot;train&quot;],
    eval_dataset=temp_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()
</code></pre>
<p>In the code, I use a customized model because I want to have the flexibility of the head. Also, I use Trainer to train the model because I want to use <code>hyperparameter_search</code>.
The target (<code>labels</code>) of the dataset is a (row) vector of 6 variables.
Now the training seems going well when I can see the loss decreasing.
However, the code crashes when it starts the evaluation.
In the code above, I use part of the training set for evaluation, and it throws:</p>
<pre><code>***** Running Evaluation *****
  Num examples = 100
  Batch size = 8
                                                                                                                                                                                                                                  Traceback (most recent call last):██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00&lt;00:00, 13.35it/s]
  File &quot;autotune_multiregression_head_bert.py&quot;, line 152, in &lt;module&gt;
    trainer.train()
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/transformers/trainer.py&quot;, line 1504, in train
    ignore_keys_for_eval=ignore_keys_for_eval,
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/transformers/trainer.py&quot;, line 1834, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/transformers/trainer.py&quot;, line 2052, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/transformers/trainer.py&quot;, line 2781, in evaluate
    metric_key_prefix=metric_key_prefix,
  File &quot;/home/ubuntu/anaconda3/envs/pytorch_p37/lib/python3.7/site-packages/transformers/trainer.py&quot;, line 3059, in evaluation_loop
    metrics = self.compute_metrics(EvalPrediction(predictions=all_preds, label_ids=all_labels))
  File &quot;autotune_multiregression_head_bert.py&quot;, line 130, in compute_metrics
    return {&quot;mcrmse&quot;: mcrmse_fn(predictions, labels)}
  File &quot;autotune_multiregression_head_bert.py&quot;, line 123, in mcrmse_fn
    delta = outputs - targets
ValueError: operands could not be broadcast together with shapes (87,6) (100,6)
 20%|
</code></pre>
<p>I got the same error when evaluating on the test set. I assume some examples got failed when evaluating and are not added to the final result, hence the final shape of <code>outputs</code> is not consistent with <code>targets</code>? (I debugged and see that <code>outputs</code> has 87 examples while <code>targets</code> has 100).
What has gone wrong?</p>
","transformer-model"
"74034385","Transformer with multi input","2022-10-11 21:37:40","","2","1225","<csv><tensorflow><neural-network><transformer-model><sequential>","<p>I would like to ask does anyone have idea how to provide two sequences within a transformer model( multi head attention to make it work like cross attention), I tried many times but doesnt understand how to perform two same kind of inputs ( type csv : numerical data, dimension for both csv 128,32 and same for other one)</p>
<pre><code>Model = sequential()

Input1 = tf.keras.input(shape=[128,32])

Input2 = tf.keras.input(shape=[128,32])

Mha = tf.keras.layers.MultiheadAttention(num_heads=2)
Output_tensor = Mha(Input1,Input2)

Retune Model
</code></pre>
<p>This is just a dummy code that I understood from tensorflow, If someone can provide a better example, It would be very helpful, I am trying to perform cross attenion over two inputs with multi head attention</p>
<p>Thank you in advance</p>
","transformer-model"
"74021838","How to reduce the difference between training and validation in the loss curve?","2022-10-11 00:35:40","","-1","2102","<deep-learning><pytorch><time-series><transformer-model><overfitting-underfitting>","<p>I have used the Transformer model to train the time series dataset, but there is always a gap between training and validation in my loss curve. I have tried using different learning rates, batch sizes, dropout, heads, dim_feedforward, and layers, but they don't work. Can anyone give me some ideas on reducing the gap between them?</p>
<p><a href=""https://i.sstatic.net/u5bfc.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/u5bfc.jpg"" alt=""enter image description here"" /></a></p>
<p>I also tried to ask the question on the Pytorch forum but didn't get any reply.
<a href=""https://discuss.pytorch.org/t/how-to-design-a-decoder-for-time-series-regression-in-transformer/162000"" rel=""nofollow noreferrer"">How to design a decoder for time series regression in Transformer?</a></p>
","transformer-model"
"73993625","How to get YoloS predicted bounding boxes ordered: from top part of image to lower","2022-10-08 00:36:08","73996537","0","362","<computer-vision><object-detection><yolo><transformer-model><yolov5>","<p>I am training an object detection to identify lines of handwritten texts (following this <a href=""https://github.com/NielsRogge/Transformers-Tutorials/blob/master/YOLOS/Fine_tuning_YOLOS_for_object_detection_on_custom_dataset_(balloon).ipynb"" rel=""nofollow noreferrer"">notebook</a>, then I can crop each line detected for further processing. However, the bounding boxes are not in order as each lines appear in the original input image. Can someone help or point me to a useful resource?</p>
<p>Thanks</p>
","transformer-model"
"73903642","How to understand Object Queries in DETR?","2022-09-30 03:10:47","","1","199","<deep-learning><transformer-model>","<p>Its hard to understand the role of Object Queries in DETR, and how it be trained?</p>
","transformer-model"
"73889309","Transformer classification model for float data","2022-09-29 01:20:16","","0","762","<pytorch><transformer-model>","<p>I'm trying make a transformer classification model by PyTorch and the input is <code>torch.FloatTensor(float data)</code>. But, I'm getting hard time with dealing embedding layer since the data is float tensor, it's hard to choose vocal size. Moreover, even I use embedding layer by passing the input as long type, during training <code>CUDA error: device-side assert triggered</code> occurs which seems to be from out of range of the input. Is there any way to build transformer classifier that can get float data as input?</p>
","transformer-model"
"73870056","FastBert TypeError :forward() got an unexpected keyword argument 'masked_lm_labels'","2022-09-27 15:22:35","","1","696","<python><huggingface-transformers><bert-language-model><transformer-model><sentence-transformers>","<p>I am following <a href=""https://medium.com/@vitalshchutski/french-nlp-entamez-le-camembert-avec-les-librairies-fast-bert-et-transformers-14e65f84c148"" rel=""nofollow noreferrer"">this tutorial </a> and I have an error in this step:</p>
<pre><code>lm_learner.fit(epochs=30,
        lr=1e-4,
        validate=True,
        schedule_type=&quot;warmup_cosine&quot;,
        optimizer_type=&quot;adamw&quot;)
</code></pre>
<p>And this is how I implement the pretrainedmodel:</p>
<pre><code>lm_learner = BertLMLearner.from_pretrained_model(
                        dataBunch=databunch_lm,
                        pretrained_path='camembert-base',
                        output_dir=MODEL_PATH,
                        metrics=[],
                        device=device_cuda,
                        logger=logger,
                        multi_gpu=False,
                        logging_steps=50,
                        fp16_opt_level=&quot;O2&quot;
                        )
</code></pre>
<p>The exact error is :
<a href=""https://i.sstatic.net/KGXI8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KGXI8.png"" alt=""enter image description here"" /></a></p>
<p>I only found old answers and it doesn't help me.</p>
","transformer-model"
"73847435","How to predict <unk> token for neural machine translation","2022-09-25 19:50:27","","0","444","<text><nlp><transformer-model><fairseq>","<p>For example, if I have the words MKIK or &quot;牛逼&quot; (which is artificially created) how can we tell neural networks (transformer model) to keep the same output?</p>
<p>The problem is with using the transformer model on fairseq.</p>
<p>I found fairseq has <code>--replace-unk</code> parameters, but it doesn't seem to work on transformer model or it has a bug</p>
","transformer-model"
"73835386","How to understand the self-attention mask implementation in google transformer tutorial","2022-09-24 07:14:34","","1","1569","<tensorflow><transformer-model><self-attention>","<p>I am reading <a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">google's transformer tutorial</a>, and the part why the <code>attention_mask</code> for multi-head attention can be built via <code>mask1 &amp; mask2</code> was unclear to me. Any help would be great!</p>
<pre><code>  def call(self, x, training, mask):

    # A boolean mask.
    if mask is not None:
      mask1 = mask[:, :, None]
      mask2 = mask[:, None, :]
      attention_mask = mask1 &amp; mask2          # &lt;= here 
    else:
      attention_mask = None

    # Multi-head self-attention output (`tf.keras.layers.MultiHeadAttention `).
    attn_output = self.mha(
        query=x,  # Query Q tensor.
        value=x,  # Value V tensor.
        key=x,  # Key K tensor.
        attention_mask=attention_mask, # A boolean mask that prevents attention to certain positions.
        training=training, # A boolean indicating whether the layer should behave in training mode.
        )
</code></pre>
<p>toy example breakdown</p>
<pre><code>input = tf.constant([
    [[1, 0, 3, 0], [1, 2, 0, 0]]
])

mask = tf.keras.layers.Embedding(2,2, mask_zero=True).compute_mask(input)
print(mask)
mask1 = mask[:, :, None]   # same as tf.expand_dims(mask, axis = 2)
print(mask1)
mask2 = mask[:, None, :]
print(mask2)

print(mask1 &amp; mask2)

&gt;

tf.Tensor(
[[[ True False  True False]
  [ True  True False False]]], shape=(1, 2, 4), dtype=bool)

tf.Tensor(
[[[[ True False  True False]]

  [[ True  True False False]]]], shape=(1, 2, 1, 4), dtype=bool)

tf.Tensor(
[[[[ True False  True False]
   [ True  True False False]]]], shape=(1, 1, 2, 4), dtype=bool)

&lt;tf.Tensor: shape=(1, 2, 2, 4), dtype=bool, numpy=               # &lt;= why built mask like this?
array([[[[ True, False,  True, False],
         [ True, False, False, False]],

        [[ True, False, False, False],
         [ True,  True, False, False]]]])&gt;
</code></pre>
","transformer-model"
"73834605","NLP: transformer learning weights","2022-09-24 03:46:18","","0","446","<nlp><embedding><transformer-model><attention-model><self-attention>","<p>The softmax function obtains the weights and then MatMul with V.
Are the weights stored anywhere? Or how the learning process happened if the weights are not stored or used on the next round?
Moreover, the linear transformation does not use the weights!</p>
<p>Source code: <a href=""https://github.com/fawazsammani/chatbot-transformer/blob/master/models.py"" rel=""nofollow noreferrer"">https://github.com/fawazsammani/chatbot-transformer/blob/master/models.py</a></p>
","transformer-model"
"73830228","Why is transformer decoder always generating output of same length as gold labels?","2022-09-23 15:50:53","","0","1263","<deep-learning><nlp><huggingface-transformers><transformer-model><encoder-decoder>","<p>I am generating some summaries using a fine-tuned BART model, and I've noticed something strange. If I feed the labels to the model, it will always generate summaries of the same length of the label, whereas if I do not pass the labels to the model, it generates outputs of length 1024 (max BART seq length). This is unexpected, so I'm trying to understand if there is any problem / bug with the reproducible example below</p>
<pre><code>from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
model=AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn')
tokenizer=AutoTokenizer.from_pretrained('facebook/bart-large-cnn')


sentence_to_summarize = ['This is a text to summarise. I just went for a walk in the park and saw very large crowds gathering to watch an impromptu football match']
encoded_dict = tokenizer.batch_encode_plus(sentence_to_summarize, return_tensors='pt', max_length=1024, padding='max_length')
input_ids = encoded_dict['input_ids']
attention_mask = encoded_dict['attention_mask']
label = tokenizer.encode('I went to the park', return_tensors='pt')
</code></pre>
<p>Notice the following two cases.<br />
Case 1:</p>
<pre><code>output = model(input_ids=input_ids, attention_mask=attention_mask)
print(output['logits'].shape)
</code></pre>
<p>shape printed is <code>torch.Size([1, 1024, 50264])</code></p>
<p>Case 2</p>
<pre><code>output = model(input_ids=input_ids, attention_mask=attention_mask, labels=label)
print(output['logits'].shape)
</code></pre>
<p>shape printed is <code>torch.Size([1, 7, 50264])</code> where 7 is the length of the label <code>'I went to the park'</code> (including start and end tokens).<br />
Ideally the summarization model would learn when to generate the EOS token, but this should not always lead to summaries of identical length of the gold output (i.e. the label). Why is the label length influencing the model output in this way?</p>
<p>I would expect the only difference between cases 1 and 2 being that in the second case the output also contains the loss value, but I wouldn't expect this to influence the logits in any way</p>
","transformer-model"
"73735121","PyTorch AttributeError: 'NoneType' object has no attribute 'size'","2022-09-15 17:17:06","","0","1352","<python><transformer-model><class-transformer>","<p>I'm building two parallel convolutional neural networks (CNN) in parallel with a Transformer encoder network to classify image data. I'm working on a dataset of faces to classify emotions from one of 7 classes.
But when I ran the model, I got this error:</p>
<blockquote>
<p>AttributeError: 'NoneType' object has no attribute 'size'.
The model is as follows:</p>
</blockquote>
<pre><code>class parallel_all_you_want(nn.Module):
    # Define all layers present in the network
    def __init__(self,num_emotions):
        super().__init__() 
        
        ################ TRANSFORMER BLOCK #############################
        # maxpool the input feature map/tensor to the transformer 
        # a rectangular kernel worked better here for the rectangular input spectrogram feature map/tensor
        self.transformer_maxpool = nn.MaxPool2d(kernel_size=[1,4], stride=[1,4])
        
        transformer_layer = nn.TransformerEncoderLayer(
            d_model=40, # input feature (frequency) dim after maxpooling 40*282 -&gt; 40*70 (MFC*time)
            nhead=4, # 4 self-attention layers in each multi-head self-attention layer in each encoder block
            dim_feedforward=512, # 2 linear layers in each encoder block's feedforward network: dim 40--&gt;512---&gt;40
            dropout=0.4, 
            activation='relu' # ReLU: avoid saturation/tame gradient/reduce compute time
        )
        
        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=4)
        
        ############### 1ST PARALLEL 2D CONVOLUTION BLOCK ############
        # 3 sequential conv2D layers: (1,40,282) --&gt; (16, 20, 141) -&gt; (32, 5, 35) -&gt; (64, 1, 8)
        self.conv2Dblock1 = nn.Sequential(
            
            # 1st 2D convolution layer
            nn.Conv2d(
                in_channels=1, # input volume depth == input channel dim == 1
                out_channels=16, # expand output feature map volume's depth to 16
                kernel_size=3, # typical 3*3 stride 1 kernel
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(16), # batch normalize the output feature map before activation
            nn.ReLU(), # feature map --&gt; activation map
            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size
            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training
            
            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel
            nn.Conv2d(
                in_channels=16, 
                out_channels=32, # expand output feature map volume's depth to 32
                kernel_size=3,
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters
            nn.Dropout(p=0.3), 
            
            # 3rd 2D convolution layer identical to last except output dim
            nn.Conv2d(
                in_channels=32,
                out_channels=64, # expand output feature map volume's depth to 64
                kernel_size=3,
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=4, stride=4),
            nn.Dropout(p=0.3),
        )
        ############### 2ND PARALLEL 2D CONVOLUTION BLOCK ############
        # 3 sequential conv2D layers: (1,40,282) --&gt; (16, 20, 141) -&gt; (32, 5, 35) -&gt; (64, 1, 8)
        self.conv2Dblock2 = nn.Sequential(
            
            # 1st 2D convolution layer
            nn.Conv2d(
                in_channels=1, # input volume depth == input channel dim == 1
                out_channels=16, # expand output feature map volume's depth to 16
                kernel_size=3, # typical 3*3 stride 1 kernel
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(16), # batch normalize the output feature map before activation
            nn.ReLU(), # feature map --&gt; activation map
            nn.MaxPool2d(kernel_size=2, stride=2), #typical maxpool kernel size
            nn.Dropout(p=0.3), #randomly zero 30% of 1st layer's output feature map in training
            
            # 2nd 2D convolution layer identical to last except output dim, maxpool kernel
            nn.Conv2d(
                in_channels=16, 
                out_channels=32, # expand output feature map volume's depth to 32
                kernel_size=3,
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=4, stride=4), # increase maxpool kernel for subsequent filters
            nn.Dropout(p=0.3), 
            
            # 3rd 2D convolution layer identical to last except output dim
            nn.Conv2d(
                in_channels=32,
                out_channels=64, # expand output feature map volume's depth to 64
                kernel_size=3,
                stride=1,
                padding=1
                      ),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=4, stride=4),
            nn.Dropout(p=0.3),
        )

        self.fc1_linear = nn.Linear(512*2+40,num_emotions) 
        
        self.softmax_out = nn.Softmax(dim=1) # dim==1 is the freq embedding
        
    # define one complete parallel fwd pass of input feature tensor thru 2*conv+1*transformer blocks
    def forward(self,x):
        print(x.shape)
     
        conv2d_embedding1 = self.conv2Dblock1(x) # x == N/batch * channel * freq * time
        
        conv2d_embedding1 = torch.flatten(conv2d_embedding1, start_dim=1) 
        
        conv2d_embedding2 = self.conv2Dblock2(x) # x == N/batch * channel * freq * time
       
        conv2d_embedding2 = torch.flatten(conv2d_embedding2, start_dim=1) 
     
        x_maxpool = self.transformer_maxpool(x)

        x_maxpool_reduced = torch.squeeze(x_maxpool,1)
        
        x = x_maxpool_reduced.permute(2,0,1) 
        
        transformer_output = self.transformer_encoder(x)
      
        transformer_embedding = torch.mean(transformer_output, dim=0) # dim 40x70 --&gt; 40
        
        complete_embedding = torch.cat([conv2d_embedding1, conv2d_embedding2,transformer_embedding], dim=1)  

        output_logits = self.fc1_linear(complete_embedding)  
        
        output_softmax = self.softmax_out(output_logits)
        
        return output_logits, output_softmax  
</code></pre>
<p>This is the implementation of Keras' model.summary method:</p>
<pre><code>from torchsummary import summary

# need device to instantiate model
device = 'cuda'

# instantiate model for 8 emotions and move to GPU 
model = parallel_all_you_want(len(emotion_labels)).to(device)

# include input feature map dims in cemotions_dictall to summary()
summary(model, input_size=(1,40,282))
</code></pre>
","transformer-model"
"73734172","AttributeError: 'TFBertModel' object has no attribute 'parameters'","2022-09-15 15:56:16","","0","896","<python><nlp><attributeerror><bert-language-model><transformer-model>","<p>Hello I am trying to train a Bert Model for a tokenizer I had trained. I imported</p>
<pre><code>from transformers import TFBertModel

model = TFBertModel.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<p>Now I am trying to initialise the training by defining the Optimizer like this</p>
<pre><code>from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)
</code></pre>
<p>However I am getting the following error</p>
<hr />
<p>AttributeError                            Traceback (most recent call last)
 in 
1 from transformers import AdamW
2
----&gt; 3 optimizer = AdamW(model.parameters(), lr=5e-5)</p>
<p>AttributeError: 'TFBertModel' object has no attribute 'parameters'</p>
<p>Following a tutorial on Youtube</p>
<p><a href=""https://www.youtube.com/watch?v=04oZ2P0uvp0&amp;t=1501s&amp;ab_channel=code_your_own_AI"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=04oZ2P0uvp0&amp;t=1501s&amp;ab_channel=code_your_own_AI</a></p>
","transformer-model"
"73704193","HuggingFace Summarization: effect of specifying both `do_sample` and `num_beams`","2022-09-13 13:50:59","73723518","0","2496","<nlp><huggingface-transformers><transformer-model><summarization><beam-search>","<p>I am using a HuggingFace summarization pipeline to generate summaries using a fine-tuned model. The <code>summarizer</code> object is initialised as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline

summarizer = pipeline(
    &quot;summarization&quot;, 
    model=model, 
    tokenizer=tokenizer, 
    num_beams=5, 
    do_sample=True, 
    no_repeat_ngram_size=3,
    max_length=1024,
    device=0,
    batch_size=8
)
</code></pre>
<p>According to the documentation, setting <code>num_beams=5</code> means that the top 5 choices are retained when a new token in the sequence is generated based on a language model, and the model moves forward discarding all other possibilities, and repeating this after every new token is generated. However, this option seems to be apparently incompatible with <code>do_sample=True</code> which seems to activate a behaviour where new tokens are picked based on some random strategy (which doesn't have to be uniformly random of course, but I don't know the details of this process). Could anyone explain clearly how  <code>num_beams=5</code> and <code>do_sample=True</code> would work together (no error is raised so I assume this is a valid <code>summarizer</code> configuration)?</p>
","transformer-model"
"73703192","Error when importing torchtext.data.datasets_utils","2022-09-13 12:37:24","","0","723","<pytorch><transformer-model><torchtext>","<p>Im trying to import torchtext but got errors even after trying with different version.</p>
<pre><code>from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence
import math
from torch.nn import Transformer
import torch.nn as nn
import torch
from torch import Tensor
from torchtext.vocab import build_vocab_from_iterator
from typing import Iterable, List
from torchtext.data.datasets_utils import _RawTextIterableDataset
from torchtext.data.datasets_utils import _read_text_iterator
</code></pre>
<p>I got error as shown below.</p>
<pre><code>---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
     13 from typing import Iterable, List
     14 # from torchtext.data.datasets_utils import _RawTextIterableDataset
---&gt; 15 from torchtext.data.datasets_utils import _read_text_iterator
     16 import os
     17 import csv

File c:\Users\anaconda3\envs\torch_env1\lib\site-packages\torchtext\data\datasets_utils.py:7, in &lt;module&gt;
      4 import os
      6 from torch.utils.data import functional_datapipe, IterDataPipe
----&gt; 7 from torch.utils.data.datapipes.utils.common import StreamWrapper
      9 try:
     10     import defusedxml.ElementTree as ET

ImportError: cannot import name 'StreamWrapper' from 'torch.utils.data.datapipes.utils.common'
</code></pre>
<p>My torch and torchtext version are as below.</p>
<pre><code>Successfully installed torch-1.12.1 torchtext-0.13.1
</code></pre>
","transformer-model"
"73700156","Why LSTM+transformer is not working well?","2022-09-13 08:56:15","","0","107","<python><lstm><transformer-model>","<p>I'm trying to use LSTM and transformer to do binary-classification, but it does not improve the performance than normal LSTM model, sometimes it will go even worse. Input shape of training data is (3014, 48, 178), input data is time-series medical data, the following code is for transformer.</p>
<pre><code>class TokenAndPositionEmbedding(layers.Layer):
def __init__(self, maxlen, vocab_size, embed_dim):
    super(TokenAndPositionEmbedding, self).__init__()
    self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
    self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

def call(self, x):
    maxlen = tf.shape(x)[-1]
    positions = tf.range(start=0, limit=maxlen, delta=1)
    positions = self.pos_emb(positions)
    x = self.token_emb(x)
    return x + positions


class TransformerBlock(layers.Layer):
def __init__(self, embed_dim, num_heads, ff_dim, rate=0.001):
    super(TransformerBlock, self).__init__()
    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
    self.ffn = Sequential(
        [layers.Dense(ff_dim, activation=&quot;relu&quot;),layers.Dense(embed_dim),]
    )
    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
    self.dropout1 = layers.Dropout(rate)
    self.dropout2 = layers.Dropout(rate)

def call(self, inputs, training):
    attn_output = self.att(inputs, inputs)
    attn_output = self.dropout1(attn_output, training=training)
    out1 = self.layernorm1(inputs + attn_output)
    ffn_output = self.ffn(out1)
    ffn_output = self.dropout2(ffn_output, training=training)
    return self.layernorm2(out1 + ffn_output)

class PositionEmbeddingFixedWeights(layers.Layer):
def __init__(self, sequence_length, output_dim, **kwargs):
    super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)
    position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                         
    self.position_embedding_layer = layers.Embedding(
        input_dim=sequence_length, output_dim=output_dim,
        weights=[position_embedding_matrix],
        trainable=False
    )
     

def get_position_encoding(self, seq_len, d, n=10000):
    P = np.zeros((seq_len, d))
    for k in range(seq_len):
        for i in np.arange(int(d/2)):
            denominator = np.power(n, 2*i/d)
            P[k, 2*i] = np.sin(k/denominator)
            P[k, 2*i+1] = np.cos(k/denominator)
    return P


def call(self, inputs):
    position_indices = tf.range(tf.shape(inputs)[-2])
    embedded_indices = self.position_embedding_layer(position_indices)
    return embedded_indices
</code></pre>
<p>Model code is</p>
<pre><code>model = Sequential([tf.keras.Input(shape=(48,178)),
                BatchNormalization(),
                
                tf.keras.layers.GRU(units = 128,recurrent_dropout=0.5,activation='tanh', dropout=0.5,return_sequences = True,activity_regularizer=regularizers.L2(0.01)),

                TransformerBlock(128, 48, 178),
                tf.keras.layers.GlobalAveragePooling1D(),

                Dense(60,activation='tanh'),
                Dense(1,activation='sigmoid')])
</code></pre>
<p>It had troubled me for a long time. I'm trying to use LSTM to dealing with the time-series feature, and transformer to learn the importance between features, but it seems not working.</p>
","transformer-model"
"73636196","Masking layer vs attention_mask parameter in MultiHeadAttention","2022-09-07 13:14:12","73679598","5","1520","<python><tensorflow><keras><transformer-model>","<p>I use <code>MultiHeadAttention</code> layer in my transformer model (my model is very similar to the named entity recognition models). Because my data comes with different lengths, I use padding and <code>attention_mask</code> parameter in <code>MultiHeadAttention</code> to mask padding. If I would use the <code>Masking</code> layer before <code>MultiHeadAttention</code>, will it have the same effect as <code>attention_mask</code> parameter? Or should I use both: <code>attention_mask</code> and <code>Masking</code> layer?</p>
","transformer-model"
"73599290","Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel:","2022-09-04 12:11:59","","0","1204","<python><pytorch><huggingface-transformers><bert-language-model><transformer-model>","<p>My PC is Windows 11 pro x64, NVIDIA card, Python 3.10 , pip . I follow guide at <a href=""https://huggingface.co/docs/transformers/main/en/model_doc/phobert"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/main/en/model_doc/phobert</a></p>
<p>my command</p>
<pre><code>pip install jupyterlab
pip list
python -m jupyterlab
pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116
pip install transformer
</code></pre>
<p>my code</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoModel, AutoTokenizer

phobert = AutoModel.from_pretrained(&quot;vinai/phobert-base&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/phobert-base&quot;)
line = &quot;Tôi là sinh_viên trường đại_học Công_nghệ .&quot;
input_ids = torch.tensor([tokenizer.encode(line)])
with torch.no_grad():
    features = phobert(input_ids)
</code></pre>
<p>error</p>
<pre class=""lang-py prettyprint-override""><code>Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</code></pre>
<p><a href=""https://i.sstatic.net/EMHpg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EMHpg.png"" alt=""enter image description here"" /></a></p>
<p>How to run success?</p>
","transformer-model"
"73596536","How to implement pair-wise calculation of attention within a batch?","2022-09-04 03:05:36","","1","236","<nlp><pytorch><transformer-model><attention-model>","<p>Suppose I now have the following code to calculate source-target attention for two variable, x and y:</p>
<pre><code>class MultiHeadedAttention(nn.Module):
    &quot;&quot;&quot;Multi-Head Attention layer

    :param int n_head: the number of head s
    :param int n_feat: the number of features
    :param float dropout_rate: dropout rate
    &quot;&quot;&quot;

    def __init__(self, n_head: int, n_feat: int, dropout_rate: float):
        super(MultiHeadedAttention, self).__init__()
        assert n_feat % n_head == 0
        self.d_k = n_feat // n_head
        self.h = n_head
        self.linear_q = nn.Linear(n_feat, n_feat)
        self.linear_k = nn.Linear(n_feat, n_feat)
        self.linear_v = nn.Linear(n_feat, n_feat)
        self.linear_out = nn.Linear(n_feat, n_feat)
        self.dropout = nn.Dropout(p=dropout_rate)

    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        mask: Optional[torch.Tensor] = None,
    ) -&gt; torch.Tensor:
        &quot;&quot;&quot;Compute 'Scaled Dot Product Attention'

        :param torch.Tensor query: (batch, x_len, size)
        :param torch.Tensor key: (batch, y_len, size)
        :param torch.Tensor value: (batch, y_len, size)
        :param torch.Tensor mask: (batch, x_len, y_len)
        :param torch.nn.Dropout dropout:
        :return torch.Tensor: attentined and transformed `value` (batch, x_len, depth)
             weighted by the query dot key attention (batch, head, x_len, y_len)
        &quot;&quot;&quot;
        n_batch = query.size(0)
        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)
        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)
        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)
        q = q.transpose(1, 2)  # (batch, head, x_len, d_k)
        k = k.transpose(1, 2)  # (batch, head, x_len, d_k)
        v = v.transpose(1, 2)  # (batch, head, y_len, d_k)
        
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(
            self.d_k
        )  # (batch, head, x_len, y_len)
        if mask is not None:
            mask = mask.unsqueeze(1).eq(0)  # (batch, 1, x_len, y_len)
            mask = mask.to(device=scores.device)
            scores = scores.masked_fill_(mask, -np.inf)
            attn = torch.softmax(scores, dim=-1).masked_fill(
                mask, 0.0
            )  # (batch, head, x_len, y_len)
        else:
            attn = torch.softmax(scores, dim=-1)  # (batch, head, x_len, y_len)

        p_attn = self.dropout(attn)
        x = torch.matmul(p_attn, v)  # (batch, head, x_len, d_k)
        x = (
            x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)
        )  # (batch, x_len, depth)
        return self.linear_out(x)  # (batch, x_len, depth)
</code></pre>
<p>So this class calculate the attention of batch size=B pairs of (x, y)_i, gives output of dim (batch, x_len, depth). So far so good. <br /><br />
The question is: What if I wanted to extend this class to calculate NOT ONLY (x1, y1), (x2, y2)..., but also all combination of xy, i.e. (x1, y2), (x1, y3)... within the batch, so that I will get an output of dim (batch, batch, x_len, depth) WITHOUT LOOPING. <br /><br />How would you implement this? Any recommendation, suggestion, example is appreciated.</p>
<hr />
<p>EDITED<br /><br />
I just came up with an idea which does the desired job at the expense of extra memory use. Just simply copy X and Y along the batch dimension so that the represent all the pairs of x_i and y_i. Specifically:</p>
<pre><code>b = torch.tensor(list(range(batch_size)))
comb = torch.cartesian_prod(b, b)
x = x[comb[:, 0], :, :]
y = y[comb[:, 1], :, :]
</code></pre>
<p>and then after the calculation, view or reshape the first dimension and it will return output which is of dim=(batch_size, batch_size, x_len, depth).</p>
<p>I have tested using toy example and quite sure it does do the job.
However, unfortunately, for my case it got CUDA out of memory.<br />
What would you do under this situation? Should I give up on parallelism and just use loop to make it works?</p>
","transformer-model"
"73591224","Transformers training vs fine-tuning for a specific task","2022-09-03 09:46:01","","-1","328","<tensorflow><machine-learning><nlp><transformer-model>","<p>I am looking at the below tensorflow transformers implementation.</p>
<p><a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<p>I am not sure I understood correctly. When initialising a transformers model it need to be trained on a lot of raw text in an unsupervised way so that it learns the language and then you can fit it to a particular task.</p>
<p>In this example, I am not sure if the training data is used to train the transformers model itself? It look like there is only one &quot;fitting&quot; procedure. Is this correct?</p>
","transformer-model"
"73584599","Trying to weight columns in ColumnTransformer using FunctionTransformer, Value Error","2022-09-02 15:03:13","","0","127","<python><scikit-learn><transformer-model>","<p>I am trying to apply weights to ColumnTransformer for further classification (weights among columns). Given that it is not possible to do so direcly in the ColumnTransformer class (to my knowledge), I tried duplicating the values using numpy.</p>
<pre><code>from numpy import repeat,ones
...
trf_list = [
  (
    feature,
    Pipeline([
      ('vectorizer', TfidfVectorizer(strip_accents='unicode', lowercase=True, stop_words='english', vocabulary=self.voc[feature])),
      ('weighter', FunctionTransformer(lambda row: repeat(row, weight)))
    ]),
    feature
  ) for feature, weight in zip(features, weights)]
trf = ColumnTransformer(transformers = trf_list, **kwargs)

test_data = np.array([[1, 9, 4], [2, 7, 1]])

expected_results = np.array([[1, 1, 9, 9, 4, 4], [2, 2, 7, 7, 1, 1]]) # with weight=2
</code></pre>
<p>Unfortunately, the output data is not flat, triggering a ValueError:</p>
<pre><code>ValueError: The output of the 'description' transformer should be 2D (scipy matrix, array, or pandas DataFrame).
ValueError                                Traceback (most recent call last)
in engine
----&gt; 1 m.fit(x_train)

/home/cdsw/src/V3/model.py in fit(self, train_set)
     65   
     66   def fit(self, train_set):
---&gt; 67     self.pipeline.fit(train_set)
     68     
     69     

/home/cdsw/.local/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    339         &quot;&quot;&quot;
    340         fit_params_steps = self._check_fit_params(**fit_params)
--&gt; 341         Xt = self._fit(X, y, **fit_params_steps)
    342         with _print_elapsed_time('Pipeline',
    343                                  self._log_message(len(self.steps) - 1)):

/home/cdsw/.local/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self, X, y, **fit_params_steps)
    305                 message_clsname='Pipeline',
    306                 message=self._log_message(step_idx),
--&gt; 307                 **fit_params_steps[name])
    308             # Replace the transformer of the step with the fitted
    309             # transformer. This is necessary when loading the transformer

/home/cdsw/.local/lib/python3.6/site-packages/joblib/memory.py in __call__(self, *args, **kwargs)
    347 
    348     def __call__(self, *args, **kwargs):
--&gt; 349         return self.func(*args, **kwargs)
    350 
    351     def call_and_shelve(self, *args, **kwargs):

/home/cdsw/.local/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    752     with _print_elapsed_time(message_clsname, message):
    753         if hasattr(transformer, 'fit_transform'):
--&gt; 754             res = transformer.fit_transform(X, y, **fit_params)
    755         else:
    756             res = transformer.fit(X, y, **fit_params).transform(X)

/home/cdsw/.local/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py in fit_transform(self, X, y)
    525 
    526         self._update_fitted_transformers(transformers)
--&gt; 527         self._validate_output(Xs)
    528 
    529         return self._hstack(list(Xs))

/home/cdsw/.local/lib/python3.6/site-packages/sklearn/compose/_column_transformer.py in _validate_output(self, result)
    414                 raise ValueError(
    415                     &quot;The output of the '{0}' transformer should be 2D (scipy &quot;
--&gt; 416                     &quot;matrix, array, or pandas DataFrame).&quot;.format(name))
    417 
    418     def _log_message(self, name, idx, total):

ValueError: The output of the 'description' transformer should be 2D (scipy matrix, array, or pandas DataFrame).
</code></pre>
<p>( The input to the ColumnTransformer is a pandas dataframe if that helps)</p>
","transformer-model"
"73498648","Unable to run Tensorflow's official Tensor2Tensor colab notebook","2022-08-26 09:09:15","","1","80671","<python><tensorflow><nlp><google-colaboratory><transformer-model>","<p>I have zero experience in Tensorflow and recently started studying about NLP. Came across the Tensorflow implementation of the Transformer based on <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention is All You Need</a> paper.</p>
<p><a href=""https://github.com/tensorflow/tensor2tensor"" rel=""nofollow noreferrer"">Tensor2Tensor</a> package has a Quick Start section which has a colab link</p>
<blockquote>
<p>Quick Start<br />
<a href=""https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb"" rel=""nofollow noreferrer"">This iPython notebook</a> explains T2T and runs in your
browser using a free VM from Google, no installation needed.</p>
</blockquote>
<p>I wanted to run this and it gives the error</p>
<pre class=""lang-py prettyprint-override""><code>ValueError: Tensorflow 1 is unsupported in Colab.

Your notebook should be updated to use Tensorflow 2.
See the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2.
</code></pre>
<p>Have no clue on what to change.</p>
<p>I'm I not supposed to run the colab but just observe the already printed results. Is there a Tensorflow 2 version which I can run and see.</p>
","transformer-model"
"73459106","Why heads share same KQV weights(matrix) in transformer?","2022-08-23 12:54:46","73479517","0","365","<pytorch><transformer-model>","<pre><code>    self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
    self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
    self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
    self.fc_out = nn.Linear(heads * self.head_dim, embed_size)

    def forward(self, values, keys, query, mask):
    # Get number of training examples
    N = query.shape[0]

    value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

    # Split the embedding into self.heads different pieces
    values = values.reshape(N, value_len, self.heads, self.head_dim)
    keys = keys.reshape(N, key_len, self.heads, self.head_dim)
    query = query.reshape(N, query_len, self.heads, self.head_dim)

    values = self.values(values)  # (N, value_len, heads, head_dim)
    keys = self.keys(keys)  # (N, key_len, heads, head_dim)
    queries = self.queries(query)  # (N, query_len, heads, heads_dim)

    # Einsum does matrix mult. for query*keys for each training example
    # with every other training example, don't be confused by einsum
    # it's just how I like doing matrix multiplication &amp; bmm

    energy = torch.einsum(&quot;nqhd,nkhd-&gt;nhqk&quot;, [queries, keys])
</code></pre>
<p>I have noticed that many implementations of multi-headed attention are similar to the above code.
But I am confused as to why, here, the KQV projections for the different heads seem to be shared.</p>
<p>Is it because in back propagation they receive the same signal?</p>
","transformer-model"
"73454858","How to use map() on a DataLoader dataset?","2022-08-23 07:42:45","","1","2626","<python><deep-learning><pytorch><computer-vision><transformer-model>","<p>I'm  trying to train a pretrained visual transformer (ViT) on a new dataset.
The dataset is made up of jpg images sorted into folders (train, val, test) and has 4 calsses.
I want to use map() on the dataset for preprocessing.
I added '<strong>getitem</strong>'  and '<strong>len</strong>' so that it'll be a map-style dataset.
But I still get the error:</p>
<pre><code>AttributeError: 'DataLoader' object has no attribute 'map'
</code></pre>
<p>Here's the code:</p>
<pre><code>from torch.utils.data import DataLoader 
class MyDataset(Dataset):
    def __init__(self, path, transform):
        self.files = glob.glob(path)
        print(type(self.files))
        self.transform = transform
        self.labels = [filepath.split('/')[-2] for filepath in self.files]
    def __getitem__(self, item):
        file = self.files[item]
        label = self.labels[item]
        file = Image.open(file)
        file = self.transform(file)
        return file, label
    def __len__(self):
        return len(self.files)
    
    
    transform=transforms.Compose([transforms.ToTensor()])
    
    train_data = MyDataset(train_path, transform)
    val_data = MyDataset(val_path, transform)
    test_data = MyDataset(test_path, transform)
    
    train = DataLoader(train_data , batch_size=1, shuffle=True, num_workers=3)
    val = DataLoader(val_data , batch_size=1, shuffle=True)
    test = DataLoader(test_data , batch_size=1, shuffle=True)
    
    
    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')
    data_collator = default_data_collator
    
    
    def preprocess_images(examples):
        images = examples['img']
        images = [np.array(image, dtype=np.uint8) for image in images]
        images = [np.moveaxis(image, source=-1, destination=0) for image in images]
        inputs = feature_extractor(images=images)
        examples['pixel_values'] = inputs['pixel_values']
    
        return examples
    
    
    features = Features({
        'label': ClassLabel(
            names=['class1', 'class2', 'class3', 'class4']),
        'img': Array3D(dtype=&quot;int64&quot;, shape=(3, 32, 32)),
        'pixel_values': Array3D(dtype=&quot;float32&quot;, shape=(3, 224, 224)),
    })
    
    preprocessed_train_ds = train.map(preprocess_images, batched=True, features=features)
    preprocessed_val_ds = val.map(preprocess_images, batched=True, features=features)
    preprocessed_test_ds = test.map(preprocess_images, batched=True, features=features)
</code></pre>
<p>What else can I do?</p>
","transformer-model"
"73413237","ValueError: The following `model_kwargs` are not used by the model: ['encoder_outputs'] (note: typos in the generate arguments will also show up","2022-08-19 07:28:04","","3","10694","<pytorch><ocr><transformer-model><encoder><encoder-decoder>","<p>When I try to run my  code for Donut for DocVQA model, I got the following error</p>
<pre><code>&quot;&quot;&quot;Test&quot;&quot;&quot;
from donut import DonutModel
from PIL import Image
import torch

model = DonutModel.from_pretrained(
    &quot;naver-clova-ix/donut-base-finetuned-cord-v2&quot;)
if torch.cuda.is_available():
    model.half()
    device = torch.device(&quot;cuda&quot;)
    model.to(device)
else:
    model.encoder.to(torch.bfloat16)
model.eval()
image = Image.open(
    &quot;./src/png-report-page_capone_v1_August_2017_GMS_Balance_Sheet_0.png&quot;).convert(&quot;RGB&quot;)
output = model.inference(image=image, prompt=&quot;&lt;s_cord-v2&gt;&quot;)

print(output)
</code></pre>
<p>The error`</p>
<pre><code>ValueError: The following `model_kwargs` are not used by the model: ['encoder_outputs'] (note: typos in the generate arguments will also show up in this list)
</code></pre>
","transformer-model"
"73405232","Transformer summariser pipeline giving different results on same model with fixed seed","2022-08-18 14:54:35","73407592","0","1550","<deep-learning><nlp><huggingface-transformers><transformer-model><summarization>","<p>I am using a HuggingFace summariser pipeline and I noticed that if I train a model for 3 epochs and then at the end run evaluation on all 3 epochs with fixed random seeds, I get a different results based on whether I restart the python console 3 times or whether I load the different model (one for every epoch) on the same summariser object in a loop, and I would like to understand why we have this strange behaviour.</p>
<p>While my results are based on ROUGE score on a large dataset, I have made this small reproducible example to show this issue. Instead of using the weights of the same model at different training epochs, I decided to demonstrate using two different summarization models, but the effect is the same. Grateful for any help.</p>
<p>Notice how in the first run I firstly use the <code>facebook/bart-large-cnn</code> model and then the <code>lidiya/bart-large-xsum-samsum</code> model without shutting the python terminal. In the second run I only use <code>lidiya/bart-large-xsum-samsum</code> model and get different output (which should not be the case).</p>
<p>NOTE: this reproducible example won't work on a CPU machine as it doesn't seem sensitive to <code>torch.use_deterministic_algorithms(True)</code> and it might give different results every time when run on a CPU, so should be reproduced on a GPU.</p>
<p>FIRST RUN</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer,  AutoModelForSeq2SeqLM, pipeline
import torch

# random text taken from UK news website
text = &quot;&quot;&quot;
The veteran retailer Stuart Rose has urged the government to do more to shield the poorest from double-digit inflation, describing the lack of action as “horrifying”, with a prime minister “on shore leave” leaving a situation where “nobody is in charge”.
Responding to July’s 10.1% headline rate, the Conservative peer and Asda chair said: “We have been very, very slow in recognising this train coming down the tunnel and it’s run quite a lot of people over and we now have to deal with the aftermath.”
Attacking a lack of leadership while Boris Johnson is away on holiday, he said: “We’ve got to have some action. The captain of the ship is on shore leave, right, nobody’s in charge at the moment.”
Lord Rose, who is a former boss of Marks &amp; Spencer, said action was needed to kill “pernicious” inflation, which he said “erodes wealth over time”. He dismissed claims by the Tory leadership candidate Liz Truss’s camp that it would be possible for the UK to grow its way out of the crisis.
&quot;&quot;&quot;

seed = 42
torch.cuda.manual_seed_all(seed)
torch.use_deterministic_algorithms(True)
tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/bart-large-cnn&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;facebook/bart-large-cnn&quot;)
model.eval()
summarizer = pipeline(
    &quot;summarization&quot;, model=model, tokenizer=tokenizer, 
    num_beams=5, do_sample=True, no_repeat_ngram_size=3, device=0
)

output = summarizer(text, truncation=True)

tokenizer = AutoTokenizer.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model.eval()
summarizer = pipeline(
    &quot;summarization&quot;, model=model, tokenizer=tokenizer, 
    num_beams=5, do_sample=True, no_repeat_ngram_size=3, device=0
)

output = summarizer(text, truncation=True)
print(output)
</code></pre>
<p>output from <code>lidiya/bart-large-xsum-samsum</code> model should be</p>
<pre><code>[{'summary_text': 'The UK economy is in crisis because of inflation. The government has been slow to react to it. Boris Johnson is on holiday.'}]
</code></pre>
<p>SECOND RUN (you must restart python to conduct the experiment)</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer,  AutoModelForSeq2SeqLM, pipeline
import torch

text = &quot;&quot;&quot;
The veteran retailer Stuart Rose has urged the government to do more to shield the poorest from double-digit inflation, describing the lack of action as “horrifying”, with a prime minister “on shore leave” leaving a situation where “nobody is in charge”.
Responding to July’s 10.1% headline rate, the Conservative peer and Asda chair said: “We have been very, very slow in recognising this train coming down the tunnel and it’s run quite a lot of people over and we now have to deal with the aftermath.”
Attacking a lack of leadership while Boris Johnson is away on holiday, he said: “We’ve got to have some action. The captain of the ship is on shore leave, right, nobody’s in charge at the moment.”
Lord Rose, who is a former boss of Marks &amp; Spencer, said action was needed to kill “pernicious” inflation, which he said “erodes wealth over time”. He dismissed claims by the Tory leadership candidate Liz Truss’s camp that it would be possible for the UK to grow its way out of the crisis.
&quot;&quot;&quot;

seed = 42
torch.cuda.manual_seed_all(seed)
torch.use_deterministic_algorithms(True)

tokenizer = AutoTokenizer.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model = AutoModelForSeq2SeqLM.from_pretrained(&quot;lidiya/bart-large-xsum-samsum&quot;)
model.eval()
summarizer = pipeline(
    &quot;summarization&quot;, model=model, tokenizer=tokenizer, 
    num_beams=5, do_sample=True, no_repeat_ngram_size=3, device=0
)

output = summarizer(text, truncation=True)
print(output)
</code></pre>
<p>output should be</p>
<pre><code>[{'summary_text': 'The government has been slow to deal with inflation. Stuart Rose has urged the government to do more to shield the poorest from double-digit inflation.'}]
</code></pre>
<p>Why is the first output different from the second one?</p>
","transformer-model"
"73386000","Simple BERT implementation on R side","2022-08-17 09:26:52","","1","550","<r><tensorflow><bert-language-model><transformer-model><reticulate>","<p>I would like to implement a simple text classification using BERT model on <strong>R</strong> side. On my local side, I have no progress because of <a href=""https://stackoverflow.com/questions/73323839/how-should-i-install-keras-bert-to-use-properly-on-r-side"">this issue</a>. Therefore, I moved to kaggle platform. Also, I have started to replicate <a href=""https://www.kaggle.com/code/pehahn/basic-bert-with-r/"" rel=""nofollow noreferrer"">this kaggle kernel</a>. However, I could not succeed to call <code>transformer$TFBertModel</code> because this line throws me the below error. As you might guess that there are few tutorials BERT on R side, so I am also appreciated for any simple BERT text classification tutorial based on R (over kaggle).</p>
<pre><code>ERROR
Error in py_get_attr_impl(x, name, silent): RuntimeError: Failed to import transformer...
</code></pre>
<p>as:</p>
<pre><code>library(tidyverse)
library(reticulate)
library(LiblineaR)
library(tidymodels)
library(rsample)

reticulate::py_install('transformers', pip = TRUE)

transformer = reticulate::import('transformers')
tf = reticulate::import('tensorflow')
builtins &lt;- import_builtins()

tokenizer &lt;- transformer$AutoTokenizer$from_pretrained('bert-base-uncased')

BERT = transformer$TFBertModel$from_pretrained(&quot;bert-base-uncased&quot;)

print(&quot;completed...&quot;)
</code></pre>
","transformer-model"
"73357526","Transformer: Which parameters are learned during training process?","2022-08-15 06:27:04","","0","726","<transformer-model>","<p>I got some questions when tried to read and learn the Transformer paper &quot;Attention is all you need&quot;:</p>
<ol>
<li>Which parameters exactly are Tranformer model learned during training process since the attention weight matrix is temporarily calculated from &quot;softmax(QK<sup>T</sup>/√d<sub>k</sub>)&quot;? The only trained parameters i know are the linear transformation factor applied on input before entering Multi-head Attention and factors inside FFN. Is there any parameter else? I wish to have a clear and unambiguous summary please.</li>
<li>What is the role of FFN in this model? How does it process the data and why we need it? I wish to have a simple and direct explanation please.</li>
</ol>
<p>Please forgive my grammar mistakes since English is not my native language. Thank you so much.</p>
","transformer-model"
"73340805","Huggingface generating chatbot response using GPT-J","2022-08-13 00:20:27","","2","631","<python><chatbot><huggingface-transformers><transformer-model><huggingface>","<p>I’m using EleutherAI/gpt-j-6B for a chatbot. I’m using the following prompt and the following code:</p>
<pre><code>prompt = &quot;person alpha:\nhi! how are you doing?\n\nperson beta:I am fine, thank you. What are you doing?\n\nperson alpha:\nI am at home watching tv.\n\nperson beta:\nThat sounds like a lot of fun. What are you watching?\n\nperson alpha:\n&quot;

model = GPTJForCausalLM.from_pretrained(
                sEleutherAI/gpt-j-6B,
                revision=&quot;float16&quot;,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True,
                use_cache=False,
                gradient_checkpointing=True
        )
tokenizer = transformers.AutoTokenizer.from_pretrained(self.hf_name, pad_token='&lt;|endoftext|&gt;',eos_token='&lt;|endoftext|&gt;')

resp_len = 128
resp_temp = 0.72
prompt = self.tokenizer(text, return_tensors='pt')
prompt = {key: value.to('cuda') for key, value in prompt.items()}
ex_min = len(prompt) + resp_len
out = self.model.generate(**prompt,
                           min_length=ex_min,
                           max_length=ex_min + resp_len,
                           do_sample=True,
                           top_k=35,
                           top_p=0.90,
                           temperature=resp_temp,
                           #repetition_penalty=1.5,
                           #length_penalty=1.2,
                           no_repeat_ngram_size=4,
                           clean_up_tokenization_spaces=True,
)

res = self.tokenizer.decode(out[0])
</code></pre>
<p>The output of the model is then the following:</p>
<pre><code>person alpha:
hi! how are you doing?

person beta:
I am fine, thank you. What are you doing?

person alpha:
I am at home watching tv.

person beta:
That sounds like a lot of fun. What are you watching?

person alpha:
A movie called ‘The Hobbit’. It is a good movie.

Person beta:
Ah, I have not seen that movie. I will look for it.

The above conversation is an example of how the system can handle more than two participants. The conversation can continue with any number of participants.

If person alpha has not been added to the system, the conversation will go through the normal steps of finding the user and adding the person to the system. In the above example, the conversation would continue with person beta.

Figure 4.2: Conversation with more than two people

The conversation can be continued in a number of ways. For example, person beta can start the conversation again. Person beta can start a conversation with person alpha, or person alpha can start a new conversation with person beta, or person beta can join a conversation with someone else.

4.2.2. Conversation and Media
The Conversations system allows media to be used in a conversational context. In this section we will look at how media can be used in the Conversations system.

In the Conversations user interface, media is added to a conversation by dragging and dropping the media file onto the conversation.

When a media file is added to the conversation, the file is added as an attachment to the conversation. This means that the media file can be used by the participants in the conversation. For example the person alpha can add the audio file to the conversation and play it for the person beta. Person beta could also add the video file and play it.
The Conversions system does not use any of the media files for anything other than the conversation. In the Conversations application, the media is just used as a medium for conversation.
The media file is not deleted when the conversation is finished. The file remains on the server and can be used later.

You can add more than one media file to a conversation.
For example, if you
</code></pre>
<p>The response of person alpha <code>A movie called ‘The Hobbig’. It is a good movie</code> makes totally sense but then the model continues with person beta and adds much more useless text.</p>
<p>What I want is just the response for person alpha and then the model should stop producing text.</p>
<p>How can this be done? The amount of produced text should also be variable because an answer is not always of same length.</p>
","transformer-model"
"73330538","AttributeError: module 'torch' has no attribute '_transformer_encoder_layer_fwd'","2022-08-12 07:17:31","","1","3377","<python><python-3.x><pytorch><transformer-model>","<p>I got an error corresponding to the title when <code>batch_first</code> is set to True in the Pytorch transformer code. When <code>batch_first = False</code>, such an error does not appear.
<code>Torch version 1.9.0+cu111</code> and <code>Python version 3.7.12</code> are used.</p>
<p>The code below belongs to the <code>TransformerEncoderLayer</code> class, where the error corresponding to the title occurs.
<a href=""https://i.sstatic.net/1YmzL.png"" rel=""nofollow noreferrer"">A code with an error</a></p>
<p>I can't find the <code>torch._transformer_encoder_layer_fwd</code> function even in the pytorch official documentation.</p>
","transformer-model"
"73314467","Output logits from T5 model for text generation purposes","2022-08-11 01:54:08","","2","4131","<nlp><pytorch><huggingface-transformers><transformer-model><autoregressive-models>","<p>I am using the T5 model found on Hugging Face for text summarization. How can I output the logits of the T5 model directly given a text input for generation purposes (not training)?</p>
<p>I want to generate the outputs token by token so that I can calculate the entropy of each output token, respectively. It does not seem like the .generate() method will work for this.</p>
<p>I effectively want to create my own generate function but I need to obtain the logits of the model to be able to do this.</p>
","transformer-model"
"73310991","Is AllenNLP biased towards BERT?","2022-08-10 18:17:28","","1","261","<bert-language-model><transformer-model><hyperparameters><allennlp><roberta>","<p>At my University's research group we have been pre-training a RoBERTa model for Portuguese and also a domain-specific one, also based on RoBERTa. We have been conducting a series of benchmarks using <a href=""https://github.com/huggingface/transformers/"" rel=""nofollow noreferrer"">huggingface's transformers library</a>, and the RoBERTa models are performing better than the <a href=""https://huggingface.co/neuralmind/bert-base-portuguese-cased"" rel=""nofollow noreferrer"">existing Portuguese BERT model</a> for almost all datasets and tasks.</p>
<p>One of the tasks we're focusing on is NER, and since AllenNLP supports a <a href=""https://github.com/allenai/allennlp-models/blob/main/allennlp_models/tagging/models/crf_tagger.py"" rel=""nofollow noreferrer"">CRF-based NER model</a>, we were looking forward to seeing if we would get even greater improvements using these new RoBERTa models combined with AllenNLP's crf_tagger. We used the same jsonnet config we were using for BERT, only switching to RoBERTa, and ran a grid search on some hyperparameters to look for the best model. We tested hyperparameters such as weight decay and learning rate (for huggingface_adamw optimizer) and dropout (for crf_tagger), using 3 different seeds. To our surprise, the RoBERTa models weren't getting better results than the existing BERT model, which contradicted the experiments using transformers. It wasn't even a tie, the BERT model was much better (90.43% for the best BERT x 89.27% for the best RoBERTa).</p>
<p>This made us suspicious that AllenNLP could be somehow biased towards BERT, then we decided to run an English-specific standard benchmark (<a href=""https://huggingface.co/datasets/conll2003"" rel=""nofollow noreferrer"">CoNLL 2003</a>) for NER using transformers and AllenNLP, and the results we got enforced this suspicion. For AllenNLP, we ran a grid search keeping the exact jsonnet config, changing only the learning rate (from 8e-6 to 7e-5), the learning rate scheduler (slanted_triangular and linear_with_warmup with 10% and 3% of the steps with warmup) and the model, of course (bert-base-cased and roberta-base). The results we got for AllenNLP were surprising: absolutely all models trained with bert-base-cased were better than all roberta-base models (best BERT was 91.65% on the test set and best RoBERTa was 90.63%).</p>
<p>For transformers, we did almost the same thing, except we didn't change the learning rate scheduler there, we kept the default one, which is linear with warmup, using 10% warmup ratio. We tested the same learning rates, and also applied 3 different seeds. The results we got for transformers were exactly the opposite: all roberta-base models were better than all bert-base-cased models (best RoBERTa was 92.46% on the test set and best BERT was 91.58%).</p>
<p>Is there something in AllenNLP framework that could be making these trained NER models biased towards BERT, and underperforming for RoBERTa? Where could we start looking for possible issues? Doesn't look like a hyperparameter issue, since we tested so many combinations with grid search so far.</p>
<p>Thanks!</p>
","transformer-model"
"73303113","Extracting specific blocks from a module list","2022-08-10 08:30:56","73303546","0","155","<python><pytorch><transformer-model>","<p>I'm using a <a href=""https://github.com/facebookresearch/TimeSformer/blob/main/timesformer/models/vit.py#L291"" rel=""nofollow noreferrer"">pretrained model</a> in which there are several self_attentions sequentially stacked each one after another and the number of them is 12. I need to extract the output of the fourth and 10th blocks of this sequential layers. In the following script, the <code>BLock</code> represents each self-attention layer:</p>
<pre><code>dpr = [x.item() for x in torch.linspace(0, 0.1, 12)]  # stochastic depth decay rule
    
    self.blocks = nn.ModuleList([
        Block(
            dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
            drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, attention_type=self.attention_type)
        for i in range(12)])
</code></pre>
<p>The self-attention layers (stack of <code>Block</code>) are as follows:</p>
<pre><code>## Attention blocks
    for blk in self.blocks:
        x = blk(x, B, T, W)
</code></pre>
<p>How can I extract the fourth and the 10th layers' output?</p>
","transformer-model"
"73261021","Multi-instance classification using tranformer model","2022-08-06 15:24:03","73297488","3","328","<python><tensorflow><keras><deep-learning><transformer-model>","<p>I use the transformer from this <a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">Keras documentation example</a> for multi-instance classification. The class of each instance depends on other instances that come in one bag. I use transformer model because:</p>
<blockquote>
<p>It makes no assumptions about the temporal/spatial relationships across the data. This is ideal for processing a set of objects</p>
</blockquote>
<p>For example, each bag may have maximal 5 instances and there are 3 features per instance.</p>
<pre><code># Generate data
max_length = 5
x_lst = []
y_lst = []
for _ in range(10):
    num_instances = np.random.randint(2, max_length + 1)
    x_bag = np.random.randint(0, 9, size=(num_instances, 3))
    y_bag = np.random.randint(0, 2, size=num_instances)
    
    x_lst.append(x_bag)
    y_lst.append(y_bag)
</code></pre>
<p>Features and labels of first 2 bags (with 5 and 2 instances):</p>
<pre><code>x_lst[:2]

[array([[8, 0, 3],
        [8, 1, 0],
        [4, 6, 8],
        [1, 6, 4],
        [7, 4, 6]]),
 array([[5, 8, 4],
        [2, 1, 1]])]

y_lst[:2]

[array([0, 1, 1, 1, 0]), array([0, 0])]
</code></pre>
<p>Next, I pad features with zeros and targets with -1:</p>
<pre><code>x_padded = []
y_padded = []

for x, y in zip(x_lst, y_lst):
    x_p = np.zeros((max_length, 3))
    x_p[:x.shape[0], :x.shape[1]] = x
    x_padded.append(x_p)

    y_p = np.negative(np.ones(max_length))
    y_p[:y.shape[0]] = y
    y_padded.append(y_p)

X = np.stack(x_padded)
y = np.stack(y_padded)
</code></pre>
<p>where <code>X.shape</code> is equal to <code>(10, 5, 3)</code> and <code>y.shape</code> is equal to <code>(10, 5)</code>.</p>
<p>I made two changes to the original model: added the Masking layer
after the Input layer and set the number of units in the last Dense layer to the maximal size of the bag (plus 'sigmoid' activation):</p>
<pre><code>def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Attention and Normalization
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;relu&quot;)(res)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    return x + res

def build_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
):
    inputs = keras.Input(shape=input_shape)
    inputs = keras.layers.Masking(mask_value=0)(inputs) # ADDED MASKING LAYER
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format=&quot;channels_first&quot;)(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(5, activation='sigmoid')(x) # CHANGED ACCORDING TO MY OUTPUT
    return keras.Model(inputs, outputs)

input_shape = (5, 3)

model = build_model(
    input_shape,
    head_size=256,
    num_heads=4,
    ff_dim=4,
    num_transformer_blocks=4,
    mlp_units=[128],
    mlp_dropout=0.4,
    dropout=0.25,
)

model.compile(
    loss=&quot;binary_crossentropy&quot;,
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    metrics=[&quot;binary_accuracy&quot;],
)
model.summary()
</code></pre>
<p>It looks like my model doesn't learn much. If I use the number of true values for each bag (<code>y.sum(axis=1)</code> and <code>Dense(1)</code>) as a target instead of classification of each instance, the model learns good. Where is my error? How should I build the output layer in this case? Do I need a custom lost function?</p>
<p>UPDATE:
I made a custom loss function:</p>
<pre><code>def my_loss_fn(y_true, y_pred):
    mask = tf.cast(tf.math.not_equal(y_true, tf.constant(-1.)), tf.float32)
    y_true, y_pred = tf.expand_dims(y_true, axis=-1), tf.expand_dims(y_pred, axis=-1)
    bce = tf.keras.losses.BinaryCrossentropy(reduction='none')
    return tf.reduce_sum(tf.cast(bce(y_true, y_pred), tf.float32) * mask)

mask = (y_test != -1).astype(int)
pd.DataFrame({'n_labels': mask.sum(axis=1), 'preds': ((preds * mask) &gt;= .5).sum(axis=1)}).plot(figsize=(20, 5))
</code></pre>
<p>And it looks like the model learns:
<a href=""https://i.sstatic.net/CosLJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CosLJ.png"" alt=""enter image description here"" /></a></p>
<p>But it predicts all nonmasked labels as 1.
<a href=""https://i.sstatic.net/44d8B.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/44d8B.png"" alt=""enter image description here"" /></a></p>
<p>@thushv89 This is <a href=""https://stats.stackexchange.com/questions/584029/predict-parallel-time-intervals"">my problem</a>. I take 2 time points: t1 and t2 and look for all vehicles that are in maintenance at the time t1 and for all vehicles that are planned to be in maintenance at the time t2. So, this is my bag of items. Then I calculate features like how much time t1 vehicles have already spent in maintenance, how much time from t1 to the plan start for t2 vehicle etc. My model learns well if I try to predict the number of vehicles in maintenance at the time t2, but I would like to predict which of them will leave and which of them will come in (3 vs [True, False, True, True] for 4 vehicles in the bag).</p>
","transformer-model"
"73243093","How to customize the positional embedding?","2022-08-04 23:10:48","","2","1296","<deep-learning><huggingface-transformers><transformer-model><huggingface-tokenizers><huggingface>","<p>I am using the Transformer model from Hugging face for machine translation. However, my input data has relational information as shown below:</p>
<p><a href=""https://i.sstatic.net/gdek9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gdek9.png"" alt=""enter image description here"" /></a></p>
<p>I want to craft a graph like the like the following:</p>
<pre><code> ________
 |       |
 |      \|/
He ended his meeting on Tuesday night.
/|\ |         |          /|\
 |  |         |           | 
 |__|         |___________|  
</code></pre>
<p>Essentially each <code>token</code> in the sentence is a node and there could be an <code>edge</code> embedded between the tokens.</p>
<p>In a normal transformer, the tokens are processed into token embeddings, also there is an encoding of each position which resulted into positional embeddings.</p>
<p>How could I do something similar with the edge information?</p>
<p>Theoretically I could take the edge type and the positional encoding of a node and output an embedding. The embeddings of all the edges can be added to the positional embeddings for the corresponding nodes.</p>
<p>Ideally, I would like to implement this with the hugging face transformer.</p>
<p>I am struggling to understand how could I update the positional embedding here:</p>
<p><a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/longformer/modeling_longformer.py#L453"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/models/longformer/modeling_longformer.py#L453</a></p>
<pre><code>        self.position_embeddings = nn.Embedding(
            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx
        )
</code></pre>
","transformer-model"
"73238155","Sliding Transformer model into longer sequence","2022-08-04 15:00:51","","1","197","<deep-learning><huggingface-transformers><transformer-model>","<p>I have very long genome sequences where I have to do some classification stuff on top. What I want to try is to use a transformer to predict the next token from the 512 chunk sequence and slide this transformer to the whole sequence and use those to work on top of the whole sequence.</p>
<p>Let’s say an example: Imagine I have a 250.000 token sequence, I should slide the transformer 488 times producing 488 tokens. Concatenate this output to obtain a summary array of the sequence and build a classifier on top of it.</p>
<p>I’m trying to find any examples that could guide me in this direction but I hardly can find any of them. Does someone think that’s a good idea? Where could I look for some near examples of sliding a transformer/LSTM over a longer sequence?</p>
<p><a href=""https://i.sstatic.net/vzwVs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vzwVs.png"" alt=""conceptual example"" /></a></p>
<p>Thank you very much, I’ll appreciate everything!</p>
","transformer-model"
"73216050","How does NLP model know the output length during translation tasks?","2022-08-03 03:36:49","73236839","-1","115","<nlp><huggingface-transformers><bert-language-model><transformer-model><sentence-transformers>","<p>Translating English to French, we may have this:</p>
<p>Input: &quot;Please help me translate this sentence&quot;            6 tokens
Output: &quot;Merci de m'aider à traduire cette phrase&quot;         7 tokens</p>
<p>We have 7 tokens in the output. How does Bert model know this length during the network processing? Which hyperparameters are involved?</p>
","transformer-model"
"73194976","Do I need training data in multiple languages for a multilingual transformer?","2022-08-01 14:07:37","","0","29","<machine-learning><nlp><classification><transformer-model>","<p>I am attempting to train a transformer which can categorize sentences into one of <em>n</em> categories. This model should be able to work with a number of different languages - English and Arabic in my case.</p>
<p>Do I need to have labelled training data in both English and Arabic to fine tune a pretrained transformer, such as <a href=""https://huggingface.co/docs/transformers/model_doc/bloom"" rel=""nofollow noreferrer"">BLOOM</a>, or can I fine tune the model using only English samples, and then the model should also work well on Arabic samples for my classificaiton task, since the fine tuning only trained the classification head?</p>
<p>My thoughts are that the pretraining of this model should allow it to transform the same input texts in English and Arabic to the same (or similar) embedding, which the classification head would have learned to then predict these embeddings accurately through the fine tuning.</p>
","transformer-model"
"73183103","Clustering based on semantic similarity returning no values","2022-07-31 11:56:17","73218870","3","876","<python-3.x><pandas><numpy><word-embedding><transformer-model>","<p>I have 'Key_Phrases' as a column in pandas dataframe df. The objective is to cluster them on semantic similarity. I am using SentenceTransformer model.</p>
<pre><code> df['Key Phrases'] is as follows

                'Key_Phrases'

0              ['BYD' 'Daiwa Capital Markets analyst' 'NIO' 'Order flows'\n 'consumer preferences' 'cost pressures' 'raw materials'\n 'regulatory pressure' 'sales cannibalization' 'sales volume growth'\n 'vehicle batteries']
1              ['CANADA' 'Canada' 'Global Carbon Pricing Challenge'\n 'Major Economies Forum' 'climate finance commitment'\n 'developing countries' 'energy security' 'food security'\n 'international shipping' 'pollution pricing']
2              ['Clean Power Plan' 'EPA' 'Environmental Protection Agency'\n 'Supreme Court' 'Supreme Court decision' 'Virginia' 'West Virginia'\n 'renewable energy' 'tax subsidies']
3              ['BlueOvalSK' 'Ford' 'Ford Motor' 'Kathleen Valley' 'LG Energy' 'Liontown'\n 'Liontown Resources' 'SK Innovation' 'SK On' 'Tesla' 'battery metals'\n 'joint venture' 'lithium spodumene concentrate'\n 'lithium supply agreement']
4              ['Emissions Trading System' 'European Commission' 'European Parliament'\n 'ICIS' 'carbon border adjustment mechanism' 'carbon leakage']
5              ['Digital Industries' 'MG Motor India' 'MindSphere'\n 'Plant Simulation software' 'Siemens' 'carbon footprints'\n 'digitalisation' 'experience' 'intelligent manufacturing'\n 'production efficiency' 'strategic collaborations']
6              ['Malaysia' 'Mosti' 'NTIS' 'National Technology and Innovation Sandbox'\n 'National Urbanisation Policy' 'Sunway Innovation Labs'\n 'Sunway iLabs Super Accelerator' 'economic growth'\n 'memorandum of understanding' 'quality of life' 'safe environment'\n 'smart cities' 'smart city sandbox' 'urban management' 'urban population']
7              ['Artificial Intelligence' 'Electricity and Water Authority'\n 'Green Mobility' 'Grid Automation' 'Internet of Things' 'Smart Dubai'\n 'Smart Energy Solutions' 'Smart Grid' 'Smart Water'\n 'artificial intelligence' 'blockchain' 'connected services'\n 'energy storage' 'integrated systems' 'interoperability' 'smart city'\n 'smart grid' 'sustainability' 'water network']
8              ['Artificial Intelligence' 'Clean Energy Strategy 2050'\n 'Dubai Electricity and Water Authority' 'Green Mobility'\n 'Grid Automation' 'Internet of Things' 'Smart Dubai'\n 'Smart Energy Solutions' 'Smart Grid' 'Smart Water'\n 'Zero Carbon Emissions Strategy' 'artificial intelligence' 'blockchain'\n 'clean energy sources' 'connected services' 'energy storage'\n 'integrated systems' 'interoperability' 'smart city' 'smart grid'\n 'sustainability']

Key_Phrases_list_1 = df['Key Phrases'].tolist()
from sentence_transformers import SentenceTransformer, util
import numpy as np

model = SentenceTransformer('distilbert-base-nli-stsb-quora-ranking')    
#Encoding is done with one simple step
embeddings = model.encode(Key_Phrases_list_1, show_progress_bar=True, convert_to_numpy=True)
</code></pre>
<p>Then the following function is created:</p>
<pre><code>def detect_clusters(embeddings, threshold=0.90, min_community_size=20):
    # Compute cosine similarity scores
    cos_scores = util.pytorch_cos_sim(embeddings, embeddings)

    #we filter those scores according to the minimum community size we specified earlier
    # Minimum size for a community
    top_k_values, _ = cos_scores.topk(k=min_community_size, largest=True)
    # Filter for rows &gt;= min_threshold
    extracted_communities = []
    for i in range(len(top_k_values)):
        if top_k_values[i][-1] &gt;= threshold:
            new_cluster = []

    # Only check top k most similar entries
            top_val_large, top_idx_large = cos_scores[i].topk(k=init_max_size, largest=True)
            top_idx_large = top_idx_large.tolist()
            top_val_large = top_val_large.tolist()
            
            if top_val_large[-1] &lt; threshold:
                for idx, val in zip(top_idx_large, top_val_large):
                    if val &lt; threshold:
                        break
                        new_cluster.append(idx)
            else:
                # Iterate over all entries (slow)
                for idx, val in enumerate(cos_scores[i].tolist()):
                    if val &gt;= threshold:
                        new_cluster.append(idx)
                        
            extracted_communities.append(new_cluster)

    unique_communities = []
    extracted_ids = set()
        
    for community in extracted_communities:
        add_cluster = True
        for idx in community:
            if idx in extracted_ids:
                add_cluster = False
                break
        if add_cluster:
            unique_communities.append(community)
            for idx in community:
                extracted_ids.add(idx)
    return unique_communities
</code></pre>
<p>Then the function is called:</p>
<pre><code>clusters = detect_clusters(embeddings, min_community_size=6, threshold=0.75)
</code></pre>
<p>I am getting no values in return. Am I missing anything in the detect_clusters function.</p>
","transformer-model"
"73159747","Transformers (Attention is all you need) with Word2Vec or GloVe?","2022-07-28 22:15:00","","1","1940","<python><stanford-nlp><word2vec><word-embedding><transformer-model>","<p>I am new to NLP and i am confused about the embedding.
Is it possible, if i already have trained GloVe embeddings / or Word2Vec embeddings and send these into Transformer? Or does the Transformer needs raw data and do its own embedding?
(Language: python, keras)</p>
","transformer-model"
"73155719","Do weights of the [PAD] token have a function?","2022-07-28 15:52:21","73158625","2","1151","<huggingface-transformers><word-embedding><transformer-model><huggingface-tokenizers><huggingface>","<p>When looking at the weights of a transformer model, I noticed that the embedding weights for the padding token <code>[PAD]</code> are nonzero. I was wondering whether these weights have a function, since they are ignored in the multi-head attention layers.</p>
<p>Would it make sense to set these weights to zeros? The weights can be seen using <code>model.base_model.embeddings.word_embeddings.weight[PAD_ID]</code> where typically <code>PAD_ID=0</code>.</p>
","transformer-model"
"73113261","The essence of learnable positional embedding? Does embedding improve outcomes better?","2022-07-25 17:37:50","73118125","7","7630","<deep-learning><pytorch><bert-language-model><transformer-model>","<p>I was recently reading the bert source code from the hugging face project. I noticed that the so-called &quot;learnable position encoding&quot; seems to refer to a specific nn.Parameter layer when it comes to implementation.</p>
<pre class=""lang-py prettyprint-override""><code>def __init__(self):
    super()
    positional_encoding = nn.Parameter()
def forward(self, x):
    x += positional_encoding
</code></pre>
<p>↑ Could be this feeling, then performed the learnable position encoding. Whether that means it's that simple or not, I'm not sure I understand it correctly, I want to ask someone with experience.</p>
<p>In addition, I noticed a classic bert structure whose location is actually coded only once at the initial input. Does this mean that the subsequent bert layers, for each other, lose the ability to capture location information?</p>
<pre class=""lang-py prettyprint-override""><code>BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0): BertLayer(...)
      ...
  (pooler): BertPooler(...)
</code></pre>
<p>Would I get better results if the results of the previous layer were re-positional encoded before the next BERT layer?</p>
","transformer-model"
"73111496","Pytorch-Lightning Misconfiguration Exception; The closure hasn't been executed","2022-07-25 15:10:24","","1","3221","<python><transformer-model><pytorch-lightning>","<p>I have been trying to train a <code>torch.nn.TransformerEncoderLayer</code> using the standard Pytorch-Lightning <code>Trainer</code> class. Before the first epoch even starts, I face the following error:</p>
<p><strong>MisconfigurationException</strong>: The closure hasn't been executed. HINT: did you call <code>optimizer_closure()</code> in your <code>optimizer_step</code> hook? It could also happen because the <code>optimizer.step(optimizer_closure)</code> call did not execute it internally.</p>
<p>I have very properly defined the <code>configure_optimizers()</code> method in the trainer and it works for every other model (say, LSTM, GRU, MultiHeadAttention). If I replace them with the TransformerEncoder, the aforementioned error pops up.</p>
<hr />
<p>Here is the model code I am using:</p>
<pre><code>class PositionalEncoder(nn.Module):
    def __init__(self, d_model=512, max_seq_len=512):
        super().__init__()
        self.d_model = d_model
        pe = torch.zeros(max_seq_len, 
                         d_model)
        
        for pos in range(max_seq_len):
            for i in range(0, d_model, 2):
                pe[pos, i] = sin(pos / (10000 ** ((2 * i)/d_model)))
                pe[pos, i+1] = cos(pos / (10000 ** ((2 * (i + 1))/d_model)))
                
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
 
    def forward(self, x):
        x *= sqrt(self.d_model)
        x += self.pe[:,:x.size(1)]
        return x
</code></pre>
<pre><code>class TRANSFORMER(pl.LightningModule):
    def __init__(self, 
                 input_dim,
                 d_model=512,
                 nhead=8,
                 num_layers=6,
                 dropout=0.5,
                 use_scheduler=True,
                 num_tags=len(TAG2IDX),
                 total_steps=1024,
                 train_dataset=None,
                 val_dataset=None,
                 test_dataset=None):
        
        super().__init__()
        self.crf = CRF(num_tags=num_tags, batch_first=True)
        self.fc = nn.Linear(d_model, num_tags)
        self.use_scheduler = use_scheduler
        
        self.embedding = nn.Embedding(num_embeddings=input_dim, 
                                      embedding_dim=d_model, 
                                      padding_idx=0)
        
        self.pos_encoder = PositionalEncoder(d_model=d_model)
        
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,
                                                        nhead=nhead,
                                                        dropout=dropout,
                                                        activation=&quot;gelu&quot;,
                                                        batch_first=True)
        
        self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layer,
                                             num_layers=num_layers)
        ## Hyperparameters ##
        self.learning_rate = LEARNING_RATE
        self.weight_decay = WEIGHT_DECAY
        self.total_steps = total_steps
        self.batch_size = BATCH_SIZE
        ## Datasets ##
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.test_dataset = test_dataset
        ## steps ##
        if self.use_scheduler: 
            self.total_steps = len(train_dataset) // self.batch_size


    # create the dataloaders
    # add shuffle only for train_dataloader
    # make sure num_workers is set appropriately and drop_last is set to False
    def train_dataloader(self):
        return DataLoader(self.train_dataset, 
                          batch_size=self.batch_size,
                          num_workers=N_JOBS,
                          shuffle=True,
                          drop_last=False)


    def val_dataloader(self):
        return DataLoader(self.val_dataset, 
                          batch_size=self.batch_size,
                          num_workers=N_JOBS,
                          shuffle=False,
                          drop_last=False)


    def test_dataloader(self):
        return DataLoader(self.test_dataset, 
                          batch_size=self.batch_size,
                          num_workers=N_JOBS,
                          shuffle=False,
                          drop_last=False)
    

    def forward(self, input_ids, masks):
        out = self.embedding(input_ids)
        out = self.pos_encoder(out)
        out = self.encoder(out, src_key_padding_mask=~masks)
        out = self.fc(out)
        return out

    
    def _shared_evaluation_step(self, batch, batch_idx):
        ids, masks, lbls = batch
        emissions = self(ids, masks)
        loss = -self.crf(emissions, lbls, mask=masks)
        pred = self.crf.decode(emissions, mask=masks)
        r, p, f1 = f1score(lbls, pred)
        return loss, r, p, f1


    def training_step(self, batch, batch_idx):
        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)
        self.log(&quot;train_loss&quot;, loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log(&quot;train_recall&quot;, r, on_step=False, on_epoch=True, prog_bar=True)
        self.log(&quot;train_precision&quot;, p, on_step=False, on_epoch=True, prog_bar=True)
        self.log(&quot;train_f1score&quot;, f1, on_step=False, on_epoch=True, prog_bar=True)
        return loss


    def validation_step(self, batch, batch_idx):
        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)
        self.log(&quot;val_loss&quot;, loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log(&quot;val_recall&quot;, r, on_step=False, on_epoch=True, prog_bar=True)
        self.log(&quot;val_precision&quot;, p, on_step=False, on_epoch=True, prog_bar=True)
        self.log(&quot;val_f1score&quot;, f1, on_step=False, on_epoch=True, prog_bar=True)

    
    def test_step(self, batch, batch_idx):
        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)
        self.log(&quot;test_loss&quot;, loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log(&quot;test_recall&quot;, r, on_step=False, on_epoch=True, prog_bar=True)
        self.log(&quot;test_precision&quot;, p, on_step=False, on_epoch=True, prog_bar=True)
        self.log(&quot;test_f1score&quot;, f1, on_step=False, on_epoch=True, prog_bar=True)


    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        ids, masks, _ = batch 
        return self.crf.decode(self(ids, masks), mask=masks)
    
    
    def configure_optimizers(self):           
        optimizer = Ranger(self.parameters(), 
                           lr=self.learning_rate,
                           weight_decay=self.weight_decay)

        if self.use_scheduler:
            scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,
                                                        num_warmup_steps=1,
                                                        num_training_steps=self.total_steps)
            lr_scheduler = {
                'scheduler': scheduler, 
                'interval': 'epoch', 
                'frequency': 1
            }
            return [optimizer], [lr_scheduler]
        else:
            return [optimizer]
</code></pre>
<hr />
<p>and here is how I am using the trainer class:</p>
<pre><code>trainer = pl.Trainer(accelerator=&quot;gpu&quot;,
                     max_epochs=EPOCHS,
                     precision=32,
                     log_every_n_steps=1,
                     callbacks=[earlystopping_callback, 
                                checkpoint_callback])
</code></pre>
","transformer-model"
"73096601","Problems encountered in implementing Bert-like models","2022-07-24 07:43:36","","1","120","<python><machine-learning><pytorch><bert-language-model><transformer-model>","<p>I'm currently trying to learn while implementing a bert-like network myself. I try to use it to perform cv feature extraction -&gt; classification of the Mnist dataset. I believe the task is simple and therefore suitable for learning. The transformer's encoder may not be the best in image feature extraction, but it should be powerful enough for me to reach a conclusion.</p>
<p>The details of my implementation can be briefly outlined as follows:</p>
<ol>
<li>Using the Mnist dataset provided by <code>tochvision</code>, if the batch_size is set to 128, the dataloader returns 28*28 single channel images of shape <code>[128, 1, 28, 28].</code></li>
<li>Cut each image into 16 equal parts (4 equal parts in each direction) so that the data of an image can be interpreted as a sequence of 16 lengths and <code>(28/4)**2=49</code> dimensions. -&gt; <code>[128, 16, 49]</code></li>
<li>Transformed to hidden dimension with dense layer -&gt; <code>[128, 16, 64]</code></li>
<li>Repeats after 6 transformer's encoder layer, which is structurally very similar to the bert layer.</li>
<li>Output the final classification result with MLP. -&gt;<code>[128, 10]</code></li>
</ol>
<p>I will include the full implementation code at the end of this article, for me, this implementation I have consulted a lot of online code, but there are a lot of details that I can't understand, I would really appreciate if anyone can answer any of these:</p>
<ol>
<li>The sample code starts with the following transformation of the Mnist dataset, I wonder why this transformation is taking place, is it because of the requirement of performing positional embedding on the original data's distribution?</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>transform_mnist = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize((0.1307,), (0.3081,))
])
</code></pre>
<ol start=""2"">
<li>The sample code provides a <code>cls_token</code> layer that changes the raw data from <code>[128, 16, 64]</code> into <code>[128, 17, 64]</code>. I don't quite understand what this is used for.</li>
<li>Whether to add dropout layer in multi head self attention. I quote a lot of code on the Internet, but they're implemented in different ways. Some code adds dropout after the attention layer, some add dropout after the final output (of self-attention block), and some don't add it at all. I would like to know if the dropout is used in the bert structure (and its various modifications) that is commonly used in production environments today.</li>
<li>What should be the recommended order for layernorm, dropout, and the residual connection layer after the end of attention? I noticed that some code puts LayerNorm in front of the self-attention block, which is different from most others and confused me.</li>
<li>I noticed that the positional encoding is only performed once in the sample code (instead of each encoder block performing its own positional encoding), does this mean that subsequent blocks lack the ability to capture the position?</li>
<li>I noticed that the input to the final MLP, which should theoretically be the features extracted by the previous network, uses <code>x[:, 0, :]</code> , and I wonder why, is it because we're doing a classification task? If our task type is trying to regress into a continuous field of values, should we also use <code>x[:, 0, :]</code> as MLP's input?</li>
</ol>
<p>Thank you all. Here is my full training code, and unfortunately, while it works, there seems to be some problem with the output and the loss that keep piling up. I don't know what caused it.</p>
<pre class=""lang-py prettyprint-override""><code>import torch
import numpy as np
import torchvision
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from einops import rearrange

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
download_path = './data'
batch_size = 128

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, hidden_dim, heads, dropout_rate, bias=False):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.heads = heads
        self.d_k = hidden_dim // heads
        self.scale = np.sqrt(self.d_k)
        self.W_q, self.W_k, self.W_v, self.W_o = [nn.Linear(hidden_dim, hidden_dim, bias=bias) for _ in range(4)]
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x, mask=None):
        batch_size, seq_len, hidden_dim = x.shape
        # q, k, v -&gt; [batch_size, 8, 17, 8]
        q = self.W_q(x).reshape(batch_size, seq_len, self.heads, self.d_k).permute(0, 2, 1, 3)
        k = self.W_k(x).reshape(batch_size, seq_len, self.heads, self.d_k).permute(0, 2, 1, 3)
        v = self.W_v(x).reshape(batch_size, seq_len, self.heads, self.d_k).permute(0, 2, 1, 3)

        # attention score
        attn = torch.matmul(q, k.transpose(2, 3)) / self.scale # -&gt; [batch_size, 8, 17, 17]
        if mask is not None:
            ... # masking
        attn = attn.softmax(dim=-1)
        v = torch.matmul(attn, v).permute(0, 2, 1, 3).reshape(batch_size, seq_len, self.hidden_dim) # -&gt; [batch_size, 8, 17, 8]
        return self.dropout(self.W_o(v)) # -&gt; [batch_size, 17, 64]

class PositionWiseFeedForward(nn.Module):
    def __init__(self, hidden_dim, mlp_dim, dropout_rate):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.mlp_dim = mlp_dim
        self.dense1 = nn.Linear(hidden_dim, mlp_dim)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(mlp_dim, hidden_dim)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        return self.dropout(self.dense2(self.relu(self.dense1(x))))

class EncoderBlock(nn.Module):
    def __init__(self, hidden_dim, heads, mlp_dim, dropout_rate):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.heads = heads
        self.self_attention_layer = MultiHeadSelfAttention(hidden_dim, heads, dropout_rate)
        self.ffn = PositionWiseFeedForward(hidden_dim, mlp_dim, dropout_rate)
        self.norm_layer1 = nn.LayerNorm(hidden_dim)
        self.norm_layer2 = nn.LayerNorm(hidden_dim)

    def forward(self, x):
        y = self.self_attention_layer(x)
        z = self.norm_layer1(x + y)
        return self.norm_layer1(z + self.ffn(z))

class MnistTransformerClsNet(nn.Module):
    def __init__(self, image_size, patch_div_num, class_num, depth, heads, hidden_dim, mlp_dim, channels=1):
        super().__init__()
        assert image_size % patch_div_num == 0, 'image dimensions must be divisible by the patch size'
        self.image_size = image_size
        self.patch_div_num = patch_div_num
        self.patch_size = image_size // patch_div_num

        self.convert_layer = nn.Linear(channels * self.patch_size ** 2, hidden_dim, bias=True)
        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_dim))
        self.pos_embedding = nn.Parameter(torch.randn(1, patch_div_num**2 + 1, hidden_dim))
        self.blocks = nn.Sequential()
        for _ in range(depth):
            self.blocks.add_module(f'block_{_}', EncoderBlock(hidden_dim, heads, mlp_dim, 0.1))
        self.mlp_layer = nn.Sequential(
            nn.Linear(hidden_dim, mlp_dim),
            nn.GELU(),
            nn.Linear(mlp_dim, class_num)
        )

    def forward(self, x): # input: x -&gt; [batch_size, 1, 28, 28]
        p = self.patch_size
        x = rearrange(x, 'b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)', p1=p, p2=p) # -&gt; [batch_size, 16, 49]
        x = self.convert_layer(x) # -&gt; [batch_size, 16, 64]

        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1) # -&gt; [batch_size, 1, 64]
        x = torch.cat((cls_tokens, x), dim=1) # -&gt; [batch_size, 17, 64]
        x += self.pos_embedding # -&gt; [batch_size, 17, 64]
        for _, block in enumerate(self.blocks):
            x = block(x) # -&gt; [batch_size, 17, 64]
        return self.mlp_layer(x[:, 0]) # -&gt; [batch_size, 64] -&gt; [batch_size, 10]

def train():
    transform_mnist = torchvision.transforms.Compose([
        torchvision.transforms.ToTensor(),
        torchvision.transforms.Normalize((0.1307,), (0.3081,))
    ])

    train_set = torchvision.datasets.MNIST(download_path, download=True, train=True, transform=transform_mnist)
    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)

    net = MnistTransformerClsNet(image_size=28, patch_div_num=4, class_num=10, depth=6, heads=8, hidden_dim=64, mlp_dim=128).to(device)
    optimizer = torch.optim.Adam(net.parameters(), lr=0.002)

    for epoch in range(100):
        # train
        for step, (x, y) in enumerate(train_loader):
            x, y = x.to(device), y.to(device)
            output = net(x).to(device)
            optimizer.zero_grad()
            loss = F.nll_loss(output, y)
            loss.backward()
            optimizer.step()
            if step % 100 == 0:
                print('[' + '{:5}'.format(step * len(x)) + '/' + '{:5}'.format(len(train_loader.dataset)) +
                      ' (' + '{:3.0f}'.format(100 * step / len(train_loader)) + '%)]  Loss: ' +
                      '{:6.4f}'.format(loss.item()))

if __name__ == '__main__':
    train()
</code></pre>
","transformer-model"
"73095449","Patch Encoder for ViT implementation in Python","2022-07-24 02:46:54","","0","503","<python><transformer-model>","<p>I am learning about the visual transformers from this <a href=""https://analyticsindiamag.com/hands-on-guide-to-using-vision-transformer-for-image-classification/"" rel=""nofollow noreferrer"">link</a>. I couldn't understand the implementation <strong>Step 2.3: Patch encoder</strong> in that which is:</p>
<pre><code>    def __init__(self, num_patches, projection_dim):
        super(PatchEncoder, self).__init__()
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )
 
    def call(self, patch):
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        encoded = self.projection(patch) + self.position_embedding(positions)
        return encoded

</code></pre>
<p>Anyone please help me to understand what exactly this function is doing</p>
","transformer-model"
"73090827","Pytorch's Transformer decoder accuracy fluctuation","2022-07-23 12:39:10","","1","271","<pytorch><huggingface-transformers><transformer-model><self-attention>","<p>I have a sequence to sequence POS tagging model which uses Transformer decoder to generate target tokens.
My implementation of Pytorch's Transformer decoder is as follows:</p>
<p>in the initialization:</p>
<pre class=""lang-py prettyprint-override""><code>    self.decoder_layer = nn.TransformerDecoderLayer(d_model=ENV_HIDDEN_SIZE, nhead=2,batch_first=True,dim_feedforward=300 ,activation=&quot;relu&quot;)
    self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=2)
</code></pre>
<p>and in the forward function:</p>
<pre class=""lang-py prettyprint-override""><code>    if infer==False: # for training
        embedded=embedded*math.sqrt(ENV_HIDDEN_SIZE)
        embedded = self.pos_encoder(embedded)
        zol = self.transformer_decoder(tgt=embedded,memory=newtensor
                                       ,memory_mask=self.transformer_mask
                                       ,memory_key_padding_mask=x_mask
                                       ,tgt_mask=self.transformer_mask)

        scores = self.slot_trans(self.dropout3(zol))
    else: #for inferrence
        bos = Variable(torch.LongTensor([[tag2index['&lt;BOS&gt;']]*batch_size])).cuda().transpose(1,0)
        bos = self.embedding(bos)
        tokens=bos
        for i in range(length):
            temp_embedded=tokens*math.sqrt(ENV_HIDDEN_SIZE)
            temp_embedded = self.pos_encoder(temp_embedded)
            zol = self.transformer_decoder(tgt=temp_embedded,
                                           memory=newtensor,
                                           tgt_mask=self.transformer_mask[:i+1,:i+1],
                                           memory_key_padding_mask=x_mask,
                                           memory_mask=self.transformer_mask[:i+1,:]
                                           )
            scores = self.slot_trans(self.dropout3(zol))
            softmaxed = self.softmax(scores)
            _,input = torch.max(softmaxed,2)
            newtok = self.embedding(input)
            tokens=torch.cat((bos,newtok),dim=1)
</code></pre>
<p>and the memory_mask is generated by the function &quot;generate_square_subsequent_mask&quot; as given:</p>
<pre class=""lang-py prettyprint-override""><code>    def generate_square_subsequent_mask(sz: int) :
        &quot;&quot;&quot;Generates an upper-triangular matrix of -inf, with zeros on diag.&quot;&quot;&quot;
        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)
</code></pre>
<p>I am observing something weird. If I <strong>do not</strong> feed the memory_mask with generate_subsequent_mask  -which I should not according to <a href=""https://discuss.pytorch.org/t/how-to-get-memory-mask-for-nn-transformerdecoder/60414/4"" rel=""nofollow noreferrer"">this post</a>-, the accuracy severely decreases. Furthermore, accuracy of the model fluctuates between 50% and 90% on each epoch randomly on the test set but not the training set.
if I <strong>do</strong> feed the memory_mask, everything is fine, and model accuracy steadily increases to 95% on the test set. Moreover, the final accuracy takes a hit when <strong>not feeding</strong> the memory_mask.
Things I tried:</p>
<ol>
<li>Without memory_mask: Tuning the learning rate.</li>
<li>Without memory_mask: Increasing the nhead and num_layers.</li>
<li>Using a simple linear layer.</li>
</ol>
<p>At the end-note, using a simple linear layer instead of the transformer decoder provides a better accuracy. Any ideas as to why this is happening?</p>
","transformer-model"
"73041042","How to go around truncating long sentences with Hugginface Tokenizers?","2022-07-19 17:13:18","","2","731","<nlp><huggingface-transformers><transformer-model><huggingface-tokenizers>","<p>I am new to tokenizers. My understanding is that the <code>truncate</code> attribute just cuts the sentences. But I need the whole sentence for context.</p>
<p>For example, my sentence is :</p>
<pre><code>&quot;Ali bin Abbas'ın  Kitab Kamilü-s Sina adlı eseri daha sonra 980 yılında nasıl adlandırılmıştır?  Ali bin Abbas'ın eseri Rezi'nin hangi isimli eserinden daha özlü ve daha sistematikdir?  Ali bin Abbas'ın Kitab Kamilü-s Sina adlı eseri İbn-i Sina'nın hangi isimli eserinden daha uygulamalı bir biçimde yazılmıştır? Kitab el-Maliki Avrupa'da Constantinus Africanus tarafından hangi dile çevrilmiştir? Kitab el-Maliki'nin ilk bölümünde neye ağırlık verilmiştir?
</code></pre>
<p>But when I use <code>max_length=64</code>, <code>truncation=True</code> and <code>pad_to_max_length=True</code> for my encoder(as suggested in the internet), half of sentence is being gone:</p>
<pre><code>▁Ali', '▁bin', '▁Abbas', &quot;'&quot;, 'ın', '▁Kitab', '▁Kami', 'lü', '-', 's', '▁Sina', '▁ad', 'lı', '▁es', 'eri', '▁daha', '▁sonra', '▁980', '▁yıl', 'ında', '▁na', 'sıl', '▁adlandır', 'ılmıştır', '?', '▁', '&lt;sep&gt;', '▁Ali', '▁bin', '▁Abbas', &quot;'&quot;, 'ın', '▁es', 'eri', '▁Rez', 'i', &quot;'&quot;, 'nin', '▁', 'hangi', '▁is', 'imli', '▁es', 'erinden', '▁daha', '▁', 'özlü', '▁ve', '▁daha', '▁sistema', 'tik', 'dir', '?', '▁', '&lt;sep&gt;', '▁Ali', '▁bin', '▁Abbas', &quot;'&quot;, 'ın', '▁Kitab', '▁Kami', 'lü', '&lt;/s&gt;']
</code></pre>
<p>And when I increase max length, CUDA is running out of memory of course. What should be my approach for long texts in the dataset?</p>
<p>My code for encoding:</p>
<pre class=""lang-py prettyprint-override""><code>input_encodings = tokenizer.batch_encode_plus(
    example_batch['context'], 
    max_length=512, 
    add_special_tokens=True,
    truncation=True, 
    pad_to_max_length=True)

target_encodings = tokenizer.batch_encode_plus(
    example_batch['questions'],
    max_length=64, 
    add_special_tokens=True,
    truncation=True,
    pad_to_max_length=True)
</code></pre>
","transformer-model"
"73030486","I have a question about who we should do testing phase in transformers for time series forecasting","2022-07-19 02:31:05","","0","29","<transformer-model>","<p>In this article <a href=""https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04"" rel=""nofollow noreferrer"">https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04</a> the author used an empty sequence with start of the token as first element for feeding to the decoder in inference phase. For empty sequence I considered a sequence with the start token as first element and zeros as remaining elements. For instance if the output window is in size of 3, I considered a sequence with length of three ( the first element is start of the token and the remaining is zero) then these zero will be replaced with new token generated in each step. It will be continued till our sequence fed to the decoder has been filled with all new tokens. I want to know if I am in a right path? The empty sequence that I considered is a right thing ?</p>
","transformer-model"
"73026144","Visualizing ViT Attention maps after fine tuning on medical dataset","2022-07-18 17:13:47","","1","1617","<python><deep-learning><transformer-model><fine-tuning><self-attention>","<p>I have imported the Vit-b32 model and fine-tuned it to perform classification task on echo images. Now I want to visualize the attention maps so that I can know on which part of the image the model is focusing for doing the classification task. But I am unable to do it and I am getting an error when I am trying to visualize the attention maps after fine-tuning the model.
Below is the code:</p>
<pre><code>!pip install --quiet vit-keras
from vit_keras import vit
vit_model = vit.vit_b32(
        image_size = IMAGE_SIZE,
        activation = 'softmax',
        pretrained = True,
        include_top = False,
        pretrained_top = False,
        classes = 3)
</code></pre>
<p>When I try yo visualize the attention map without any finetuning then it is working without any error:</p>
<pre><code>from vit_keras import visualize

    x = test_gen.next()
    image = x[0]
    
    attention_map = visualize.attention_map(model = vit_model, image = image)
    
    # Plot results
    fig, (ax1, ax2) = plt.subplots(ncols = 2)
    ax1.axis('off')
    ax2.axis('off')
    ax1.set_title('Original')
    ax2.set_title('Attention Map')
    _ = ax1.imshow(image)
    _ = ax2.imshow(attention_map)
</code></pre>
<p>Now in the below code I have added some classification layers to the model and fine-tuned it:</p>
<pre><code>model = tf.keras.Sequential([
        vit_model,
        tf.keras.layers.Flatten(),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(11, activation = tfa.activations.gelu),
        tf.keras.layers.BatchNormalization(),
        tf.keras.layers.Dense(3, 'softmax')
    ],
    name = 'vision_transformer')

model.summary()
</code></pre>
<p>Below is the output of the above cell:</p>
<pre><code>&gt; Model: &quot;vision_transformer&quot;
&gt; _________________________________________________________________ Layer (type)                 Output Shape              Param #   
&gt; ================================================================= vit-b32 (Functional)         (None, 768)               87455232  
&gt; _________________________________________________________________ flatten_1 (Flatten)          (None, 768)               0         
&gt; _________________________________________________________________ batch_normalization_2 (Batch (None, 768)               3072      
&gt; _________________________________________________________________ dense_2 (Dense)              (None, 11)                8459      
&gt; _________________________________________________________________ batch_normalization_3 (Batch (None, 11)                44        
&gt; _________________________________________________________________ dense_3 (Dense)              (None, 3)                 36        
&gt; ================================================================= Total params: 87,466,843 Trainable params: 87,465,285 Non-trainable
&gt; params: 1,558
&gt; _________________________________________________________________
</code></pre>
<p>Now I have trained the model on my own medical dataset:</p>
<pre><code>learning_rate = 1e-4

optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)

model.compile(optimizer = optimizer, 
              loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.2), 
              metrics = ['accuracy'])

STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size
STEP_SIZE_VALID = valid_gen.n // valid_gen.batch_size

reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_accuracy',
                                                 factor = 0.2,
                                                 patience = 2,
                                                 verbose = 1,
                                                 min_delta = 1e-4,
                                                 min_lr = 1e-6,
                                                 mode = 'max')

earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy',
                                                 min_delta = 1e-4,
                                                 patience = 5,
                                                 mode = 'max',
                                                 restore_best_weights = True,
                                                 verbose = 1)

checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath = './model.hdf5',
                                                  monitor = 'val_accuracy', 
                                                  verbose = 1, 
                                                  save_best_only = True,
                                                  save_weights_only = True,
                                                  mode = 'max')

callbacks = [earlystopping, reduce_lr, checkpointer]

model.fit(x = train_gen,
          steps_per_epoch = STEP_SIZE_TRAIN,
          validation_data = valid_gen,
          validation_steps = STEP_SIZE_VALID,
          epochs = EPOCHS,
          callbacks = callbacks)

model.save('model.h5', save_weights_only = True)
</code></pre>
<p>After training when I am trying to visualize the attention map of the model, it is showing error:</p>
<pre><code>from vit_keras import visualize

x = test_gen.next()
image = x[0]

attention_map = visualize.attention_map(model = model, image = image)

# Plot results
fig, (ax1, ax2) = plt.subplots(ncols = 2)
ax1.axis('off')
ax2.axis('off')
ax1.set_title('Original')
ax2.set_title('Attention Map')
_ = ax1.imshow(image)
_ = ax2.imshow(attention_map)
</code></pre>
<p>Below is the following error:</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-13-f208f2d2b771&gt; in &lt;module&gt;
      4 image = x[0]
      5 
----&gt; 6 attention_map = visualize.attention_map(model = model, image = image)
      7 
      8 # Plot results

/opt/conda/lib/python3.7/site-packages/vit_keras/visualize.py in attention_map(model, image)
     14     &quot;&quot;&quot;
     15     size = model.input_shape[1]
---&gt; 16     grid_size = int(np.sqrt(model.layers[5].output_shape[0][-2] - 1))
     17 
     18     # Prepare the input

TypeError: 'NoneType' object is not subscriptable
</code></pre>
<p>Please suggest some way to rectify the above error and visualize the attention maps of the fine-tuned model</p>
","transformer-model"
"73023729","BEiT large example on Huggingface raises warnings","2022-07-18 14:09:43","","0","200","<python><artificial-intelligence><warnings><huggingface-transformers><transformer-model>","<p>I run the example code found at: <a href=""https://huggingface.co/microsoft/beit-large-patch16-512"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/beit-large-patch16-512</a> of the BEiT (large-sized model, fine-tuned on ImageNet-1k), with no changes:</p>
<pre><code>from transformers import BeitFeatureExtractor, BeitForImageClassification
from PIL import Image
import requests
url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)
feature_extractor = BeitFeatureExtractor.from_pretrained('microsoft/beit-large-patch16-512')
model = BeitForImageClassification.from_pretrained('microsoft/beit-large-patch16-512')
inputs = feature_extractor(images=image, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
logits = outputs.logits
# model predicts one of the 1000 ImageNet classes
predicted_class_idx = logits.argmax(-1).item()
print(&quot;Predicted class:&quot;, model.config.id2label[predicted_class_idx])
</code></pre>
<p>The code runs correctly but it raises the following warnings:</p>
<pre><code>/home/name/anaconda3/lib/python3.8/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755853042/work/aten/src/ATen/native/TensorShape.cpp:2228.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
    /home/name/anaconda3/lib/python3.8/site-packages/transformers/feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow.
Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor.
(Triggered internally at  /opt/conda/conda-bld/pytorch_1646755853042/work/torch/csrc/utils/tensor_new.cpp:210.)
</code></pre>
<p>Are these warnings safe to ignore? If not how to fix them? Should I just signal this as a bug on the Huggingface page?</p>
","transformer-model"
"73012695","Does nn.Transformer in PyTorch include the PositionalEncoding() process so far?","2022-07-17 14:36:51","","1","584","<pytorch><python-3.8><transformer-model>","<p>I was trying to solve a seq2seq problem with Transformer Module. According my understanding to the source code from <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py"" rel=""nofollow noreferrer"">pytoch's github</a>, I thought PositionalEncoding() is not yet included. (Correct me if i'm wrong). But I saw soooo many codes (including the one written by one of my tutors) using the default nn.Transformer to model seq2seq problems which really confused me.</p>
","transformer-model"
"72999740","Cannot get DataCollator to prepare tf dataset","2022-07-15 21:25:05","","0","1443","<python><tensorflow><named-entity-recognition><transformer-model><huggingface>","<p>I’m trying to follow this tutorial to fine-tune bert for a NER task using my own dataset. <a href=""https://www.philschmid.de/huggingface-transformers-keras-tf"" rel=""nofollow noreferrer"">https://www.philschmid.de/huggingface-transformers-keras-tf</a>.  Below is my shortened code, and the error due to the last line of the code. I’m new to all these, and thank you in advance for helping out!</p>
<pre><code># load dataset, 
df_converters = {'tokens': ast.literal_eval, 'labels': ast.literal_eval}
train_df = pd.read_csv(&quot;train_df_pretokenization.csv&quot;, converters=df_converters)
train_df = train_df.head(10)

# model and pretrained tokenizer
model_ckpt = &quot;indobenchmark/indobert-base-p2&quot;
tokenizer = AutoTokenizer.from_pretrained(model_ckpt) 

# tokenization, and align labels 
def tokenize_and_align_labels(batch): 

    tag2int = {'B-POI':0, 'B-STR':1, 'E-POI':2, 'E-STR':3, 'I-POI':4,
           'I-STR':5, 'S-POI':6, 'S-STR':7, 'O':8}
           
    #tokenized_inputs = tokenizer(batch['tokens'], is_split_into_words=True, truncation = True, padding = True)
    tokenized_inputs = tokenizer(batch['tokens'], is_split_into_words=True, truncation = True)
    labels=[]
    for idx, label in enumerate(batch['labels']):
        word_ids = tokenized_inputs.word_ids(batch_index = idx)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids: 
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:
                label_ids.append(tag2int[label[word_idx]])
            else: 
                label_ids.append(tag2int[label[word_idx]])
            previous_word_idx = word_idx
        labels.append(label_ids)
    tokenized_inputs['tags'] = labels

    return tokenized_inputs

def encode_dataset(ds):
    return ds.map(tokenize_and_align_labels, batched= True, batch_size=10, remove_columns=['labels','tokens', 'index'])
    
train_ds = Dataset.from_pandas(train_df)
train_ds_encoded = encode_dataset(train_ds)

# prepare model input 
data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=&quot;tf&quot;)

tf_train_dataset = train_ds_encoded.to_tf_dataset(
    columns= ['input_ids', 'token_type_ids', 'attention_mask', 'tags'],
    shuffle=False,
    batch_size=5,
    collate_fn=data_collator
)
</code></pre>
<blockquote>
<p>ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.</p>
</blockquote>
<p>I thought data collator is supposed to take care of the padding work given the requested batch size, and I don’t understand why feeding in sequences of different lengths will cause this error. Indeed, the tutorial runs fine without specifying padding or truncation. My code will run if I add padding = True to the tokenizer in the function (the line I commented out in the function). But I don’t think it is the right place to add paddings.</p>
","transformer-model"
"72986749","How to get token or code embedding using Codex API?","2022-07-14 21:16:14","73096314","-1","620","<python><transformer-model><openai-api><language-model>","<p>For a given code snippet, how to get embedding using the Codex API?</p>
<pre><code>import os
import openai
import config


openai.api_key = config.OPENAI_API_KEY

def runSomeCode():
    response = openai.Completion.create(
      engine=&quot;code-davinci-001&quot;,
      prompt=&quot;\&quot;\&quot;\&quot;\n1. Get a reputable free news api\n2. Make a request to the api for the latest news stories\n\&quot;\&quot;\&quot;&quot;,
      temperature=0,
      max_tokens=1500,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0)

    if 'choices' in response:
        x = response['choices']
        if len(x) &gt; 0:
            return x[0]['text']
        else:
            return ''
    else:
        return ''



answer = runSomeCode()
print(answer)
</code></pre>
<p>But I want to figure out given a python code block like the following, can I get the embedding from codex?</p>
<p>Input:</p>
<pre><code>import Random
a = random.randint(1,12)
b = random.randint(1,12)
for i in range(10):
    question = &quot;What is &quot;+a+&quot; x &quot;+b+&quot;? &quot;
    answer = input(question)
    if answer = a*b
        print (Well done!)
    else:
        print(&quot;No.&quot;)
</code></pre>
<p>Output:</p>
<ul>
<li>Embedding of the input code</li>
</ul>
","transformer-model"
"72985464","how to make colab use GPU for spacy training NER model","2022-07-14 18:59:57","","1","3596","<tensorflow><nlp><spacy><named-entity-recognition><transformer-model>","<p>I have <strong>40,000</strong> records and I the training process is very <em>slow</em>, this is the line I use in colab for training</p>
<pre><code>! python -m spacy train config.cfg --output /content/ --paths.train /content/training_data.spacy --paths.dev /content/training_data.spacy
</code></pre>
<p>when I run this cell, this show up</p>
<blockquote>
<p>ℹ Saving to output directory: /content
<br/>ℹ Using CPU
<br/>ℹ To switch to GPU 0, use the option: --gpu-id 0</p>
</blockquote>
<p>I want to use the <code>GPU</code> but when I use the <code>--gpu-id 0</code> in end of cell command, I got <code>ERROR</code></p>
<p>and I changed runtime of colab to GPU</p>
","transformer-model"
"72973591","SkLearn DecisionTree Doesn't Include Numerical Features After Fitting","2022-07-13 23:12:42","72973622","0","54","<python><machine-learning><scikit-learn><transformer-model>","<p>I'm trying to fit a dataframe with SkLearn DecisionTree with the following code.  But I get a error <code>Length of feature_names, 9 does not match number of features, 8</code>.  The DecisionTree seems to have only fitted categorical features after transformed by onehotencoding, not the numerical feature.  How can I include the numerical feature in the decisiontree model?</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import tree
from matplotlib import pyplot as plt
import graphviz 
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder,StandardScaler
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.linear_model import LinearRegression

df = pd.DataFrame({'brand'      : ['aaaa', 'asdfasdf', 'sadfds', 'NaN'],
                   'category'   : ['asdf','asfa','asdfas','as'], 
                   'num1'       : [1, 1, 0, 0] ,
                   'target'     : [1,0,0,1]})

df

dtarget=df['target']
dfeatures=df.drop('target', axis=1)


num = dfeatures.select_dtypes(include=[&quot;int64&quot;]).columns.tolist()
cat = dfeatures.select_dtypes(include=[&quot;object&quot;]).columns.tolist()


transformer = ColumnTransformer(
    transformers=[
        (&quot;cat&quot;, OneHotEncoder(),  cat),
    ]
)


clf= DecisionTreeClassifier(criterion=&quot;entropy&quot;, max_depth = 5)


pipe = Pipeline(steps=[
                ('onehotenc', transformer),
                ('decisiontree', clf)
                ])

#Fit the training data to the pipeline
pipe.fit(dfeatures, dtarget)



pipe.named_steps['onehotenc'].get_feature_names_out().tolist(), 

dot_data= tree.export_graphviz(clf,
                     out_file=None,
                     feature_names = num + pipe.named_steps['onehotenc'].get_feature_names_out().tolist(), 
                     class_names= ['1', '0'],
                     filled = True)
</code></pre>
","transformer-model"
"72972655","How to extract the keywords on which universal sentence encoder was trained on?","2022-07-13 21:06:54","","1","125","<tensorflow><nlp><transformer-model><sentence-similarity><penn-treebank>","<p>I am using Universal sentence encoder to encode some documents into a 512 dimensional embeddings. These are then used to find similar items to a search query which is also encoded using USE. USE works pretty well on general english words in search query and documents but performs really bad when the search query contains rare keywords such as people's name etc. I am thinking of enabling a reranker over the search results that takes into account the number of rare words present in the search query and the document retrieved. This should boost the scores of documents which contain known words while reduce the score of documents that contain unknown words.</p>
<p>My question is How do I get the grammar of Universal sentence encoder to implement such re-ranker?</p>
","transformer-model"
"72930221","Why i get Generator object at xxxxxx?","2022-07-10 16:45:51","","0","506","<sensors><esp8266><transformer-model><micropython>","<p>Hey i wrote this code from the library CT sensor written for micropython, this is the link to the library:</p>
<p><a href=""https://github.com/alisonsalmeida/emonlib-micropython"" rel=""nofollow noreferrer"">https://github.com/alisonsalmeida/emonlib-micropython</a></p>
<pre><code>from machine import Pin, ADC
import utime
from emonlib import Emonlib
from time import sleep
adc = ADC(0)
ct = Emonlib
ct.current(object, adc, 10)
cd=  ct.calc_current_rms(10,2)
while True:
  
  print(cd)
  sleep(0.5)
</code></pre>
<p>i can not figure out how to get the value of the current RMS, i am quite new to python,</p>
<p><strong>Could you help me to learn how to work on that library?</strong>
<em>i have not found anything yet online, i am trying since weeks :(</em></p>
","transformer-model"
"72909864","I'm trying to convert a keras transformer model to pythorch, but the number of model parameters are different","2022-07-08 10:10:30","","2","156","<python><keras><torch><transformer-model>","<p>Why does the pytorch model only have ~3.7 million parameters while the keras model has over 30 million? Is there something that I did wrong or is there such a big difference beetween the keras and the pytorch implementation of the multiheadattention layer?</p>
<p>Is there a difference how keras and torch handles the sequential layer after the multiheadattention?</p>
<p><strong>Tensorflow</strong></p>
<pre><code>def get_angles(pos, i, d_model):
    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))
    return pos * angle_rates

def positional_encoding(position, d_model):
    angle_rads = get_angles(np.arange(position)[:, np.newaxis],
                            np.arange(d_model)[np.newaxis, :],
                            d_model)

    # apply sin to even indices in the array; 2i
    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])

    # apply cos to odd indices in the array; 2i+1
    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])

    pos_encoding = angle_rads[np.newaxis, ...]

    return tf.cast(pos_encoding, dtype=tf.float32)

class PositionEmbedding(layers.Layer):
    def __init__(self, max_len, embed_dim):
        super(PositionEmbedding, self).__init__()
        self.pos_encoding = positional_encoding(max_len,
                                                embed_dim)

    def call(self, x):
        seq_len = tf.shape(x)[1]
        x += self.pos_encoding[:, :seq_len, :]
        return x

class TransformerBlock(tf.keras.Model):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation=&quot;relu&quot;), layers.Dense(embed_dim), ]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

def transformer_classifer(embed_dim, ff_dim, max_len, num_heads, dropout=0.1):
    inputs = layers.Input(shape=(max_len, embed_dim))
    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
    embedding_layer = PositionEmbedding(1024, embed_dim)
    x = embedding_layer(inputs)
    x = transformer_block(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Dense(32, activation=&quot;relu&quot;)(x)
    x = layers.Dropout(dropout)(x)
    outputs = layers.Dense(2, activation=&quot;softmax&quot;)(x)
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model

 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 75, 768)]         0         
                                                                 
 position_embedding (Positio  (None, 75, 768)          0         
 nEmbedding)                                                     
                                                                 
 transformer_block_1 (Transf  (None, 75, 768)          31491584  
 ormerBlock)                                                     
                                                                 
 global_average_pooling1d (G  (None, 768)              0         
 lobalAveragePooling1D)                                          
                                                                 
 dropout_41 (Dropout)        (None, 768)               0         
                                                                 
 dense_4 (Dense)             (None, 32)                24608     
                                                                 
 dropout_42 (Dropout)        (None, 32)                0         
                                                                 
 dense_5 (Dense)             (None, 2)                 66        
                                                                 
=================================================================
Total params: 31,516,258
Trainable params: 31,516,258
Non-trainable params: 0
</code></pre>
<p><strong>Pythorch</strong></p>
<pre><code>class TransformerBlock(nn.Module):
    def __init__(self, embed_dim, num_heads, ff_dim, max_len, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.embed_dim = embed_dim
        self.max_len = max_len
        self.att = nn.MultiheadAttention(num_heads=num_heads, embed_dim=embed_dim, batch_first=True)

        self.sequential = nn.Sequential(
            nn.Linear(in_features=embed_dim, out_features=ff_dim),
            nn.ReLU(inplace=True),
            nn.Linear(in_features=ff_dim, out_features=embed_dim)
        )

        self.layernorm1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)
        self.layernorm2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)
        self.dropout1 = nn.Dropout(rate)
        self.dropout2 = nn.Dropout(rate)

    def forward(self, inputs, training=True):
        #print(&quot;inputs shape&quot;, inputs.shape)
        attn_output, _ = self.att(inputs, inputs, inputs)
        attn_output = self.dropout1(attn_output)
        out1 = self.layernorm1(inputs + attn_output)
        seq_output = self.sequential(out1)
        seq_output = self.dropout2(seq_output)
        return self.layernorm2(out1 + seq_output)


class transformer_classifer(nn.Module):
    def __init__(self, embed_dim, ff_dim, max_len, num_heads, dropout=0.1):
        super(transformer_classifer, self).__init__()
        self.max_len = max_len
        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, max_len)
        self.embedding_layer = PositionEmbedding(max_len, embed_dim)
        self.dropout = nn.Dropout(dropout)
        self.linear1 = nn.Linear(in_features=embed_dim, out_features=32)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(in_features=32, out_features=2)
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, inputs, training=True):
        x = self.embedding_layer(inputs, self.max_len, training=training)
        x = self.transformer_block(x, training=training)
        x = torch.mean(x, dim=1)
        x = self.dropout(x)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.linear2(x)
        #x = self.softmax(x)
        return x

================================================================================
Layer (type:depth-idx)                                  Param #
================================================================================
├─TransformerBlock: 1-1                                 --
|    └─MultiheadAttention: 2-1                          --
|    |    └─NonDynamicallyQuantizableLinear: 3-1        590,592
|    └─Sequential: 2-2                                  --
|    |    └─Linear: 3-2                                 1,574,912
|    |    └─ReLU: 3-3                                   --
|    |    └─Linear: 3-4                                 1,573,632
|    └─LayerNorm: 2-3                                   1,536
|    └─LayerNorm: 2-4                                   1,536
|    └─Dropout: 2-5                                     --
|    └─Dropout: 2-6                                     --
├─PositionEmbedding: 1-2                                --
├─Dropout: 1-3                                          --
├─Linear: 1-4                                           24,608
├─ReLU: 1-5                                             --
├─Linear: 1-6                                           66
├─Softmax: 1-7                                          --
================================================================================
Total params: 3,766,882
Trainable params: 3,766,882
Non-trainable params: 0
================================================================================
</code></pre>
","transformer-model"
"72889808","What is the range of BERT CLS values?","2022-07-06 21:01:13","","0","888","<nlp><classification><bert-language-model><transformer-model>","<p>As you can see in my title, I am interested in the value range of BERT.
I read through the BERT paper but it is all still a little confusing for me. At the end of a Classification, BERT has the calculated CLS Token (Classification Token) which is a 768 long float array.</p>
<p>But what are the maximum/minimum possible values in that array?</p>
","transformer-model"
"72882799","HuggingFace - Why does the T5 model shorten sentences?","2022-07-06 11:27:25","","2","840","<python><huggingface-transformers><transformer-model><huggingface-tokenizers><huggingface>","<p>I wanted to train the model for spell correction. I trained two models allegro/plt5-base with polish sentences and google/t5-v1_1-base with english sentences. Unfortunately, I don't know for what reason, but both models shorten the sentences.
Example:</p>
<pre><code>phrases = ['The name of the man who was kild was Jack Robbinson he has black hair brown eyes blue Jacket and blue Jeans.']
encoded = tokenizer(phrases, return_tensors=&quot;pt&quot;, padding=True, max_length=512, truncation=True)
print(encoded)
# {'input_ids': tensor([[   37,   564,    13,     8,   388,   113,    47,     3,   157,   173,
#             26,    47,  4496,  5376,  4517,   739,     3,    88,    65,  1001,
#           1268,  4216,  2053,  1692, 24412,    11,  1692,  3966,     7,     5,
#              1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
#          1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}

encoded.to('cuda')
translated = model.generate(**encoded)
print(translated)
# tensor([[   0,   37,  564,   13,    8,  388,  113,   47, 2170,   47, 4496, 5376,
#          4517,  739,    3,   88,   65, 1001, 1268, 4216]], device='cuda:0')

tokenizer.batch_decode(translated, skip_special_tokens=True)
#['The name of the man who was born was Jack Robbinson he has black hair brown']
</code></pre>
<p>And something like this happens in almost every longer sentence. I tried to check if the model has any maximum sentence length set based on the documentation: <a href=""https://huggingface.co/transformers/v3.1.0/model_doc/t5.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/v3.1.0/model_doc/t5.html</a>. But the config of this model has no such field:
<code>n_positions – The maximum sequence length that this model might ever be used with. Typically set this to something large just in case (e.g., 512 or 1024 or 2048). n_positions can also be accessed via the property max_position_embeddings.</code>
This is the entire config of the model:</p>
<pre><code>T5Config {
  &quot;_name_or_path&quot;: &quot;final_model_t5_800_000&quot;,
  &quot;architectures&quot;: [
    &quot;T5ForConditionalGeneration&quot;
  ],
  &quot;d_ff&quot;: 2048,
  &quot;d_kv&quot;: 64,
  &quot;d_model&quot;: 768,
  &quot;decoder_start_token_id&quot;: 0,
  &quot;dropout_rate&quot;: 0.1,
  &quot;eos_token_id&quot;: 1,
  &quot;feed_forward_proj&quot;: &quot;gated-gelu&quot;,
  &quot;initializer_factor&quot;: 1.0,
  &quot;is_encoder_decoder&quot;: true,
  &quot;layer_norm_epsilon&quot;: 1e-06,
  &quot;model_type&quot;: &quot;t5&quot;,
  &quot;num_decoder_layers&quot;: 12,
  &quot;num_heads&quot;: 12,
  &quot;num_layers&quot;: 12,
  &quot;output_past&quot;: true,
  &quot;pad_token_id&quot;: 0,
  &quot;relative_attention_max_distance&quot;: 128,
  &quot;relative_attention_num_buckets&quot;: 32,
  &quot;tie_word_embeddings&quot;: false,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.18.0&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 32128
}
</code></pre>
<p>What can be done to make the model return whole sentences?</p>
<h4>Update</h4>
<p>I looked in the old documentation earlier. But in the new one I don't see a field in the config at all about the maximum sentence length. <a href=""https://huggingface.co/docs/transformers/main/en/model_doc/t5#transformers.T5Config"" rel=""nofollow noreferrer"">new documentation</a></p>
","transformer-model"
"72881589","How to use fairseq interactive with multiple gpu?","2022-07-06 10:07:41","","0","799","<deep-learning><pytorch><transformer-model><fairseq>","<p>I am trying to generate new prediction for the model, but I found it is not that intuitive to use fairseq. I found <code>fairseq-interactive</code> could help to generate with a good settings of batch_size, however, it seems that it will use 1 GPU at a time, I wonder if it is possible to use multiple GPU? Hope someone can kindly help!</p>
<p>Many thanks :)</p>
","transformer-model"
"72848458","How do I fix ValueError: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets?","2022-07-03 17:10:14","","0","261","<python><nlp><sentiment-analysis><bert-language-model><transformer-model>","<p>I am currently learning how sentiment analysis and machine learning works. I am following this <a href=""https://medium.com/mlearning-ai/twitter-sentiment-analysis-with-deep-learning-using-bert-and-hugging-face-830005bcdbbf"" rel=""nofollow noreferrer"">tutorial</a> and this is the <a href=""https://github.com/baotramduong/Twitter-Sentiment-Analysis-with-Deep-Learning-using-BERT/blob/main/Notebook.ipynb"" rel=""nofollow noreferrer"">Github</a> source code. When it comes to training the model, I am facing this error:</p>
<pre><code>ValueError: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets
</code></pre>
<p>Upon researching solutions on the internet, I found that it could be due to the presence of continuous values. I have tried to change this line but to no avail:<br />
<code>val_f1 = f1_score_func(predictions, true_vals)</code><br />
to<br />
<code>val_f1 = f1_score_func(predictions, np.round(true_vals))</code></p>
<p>Here is the relevant part of the codes:</p>
<pre><code>def evaluate(dataloader_val):

    #evaluation mode disables the dropout layer 
    model.eval()
    
    #tracking variables
    loss_val_total = 0
    predictions, true_vals = [], []
    
    for batch in tqdm(dataloader_val):
        
        #load into GPU
        batch = tuple(b.to(device) for b in batch)
        
        #define inputs
        inputs = {'input_ids':      batch[0],
                  'attention_mask': batch[1],
                  'labels':         batch[2]}

        #compute logits
        with torch.no_grad():        
            outputs = model(**inputs)
        
        #compute loss
        loss = outputs[0]
        logits = outputs[1]
        loss_val_total += loss.item()

        #compute accuracy
        logits = logits.detach().cpu().numpy()
        label_ids = inputs['labels'].cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)
    
    #compute average loss
    loss_val_avg = loss_val_total/len(dataloader_val) 
    
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
            
    return loss_val_avg, predictions, true_vals


for epoch in tqdm(range(1, epochs+1)):

    #set model in train mode
    model.train()

    #tracking variable
    loss_train_total = 0
    
    #set up progress bar
    progress_bar = tqdm(dataloader_train, 
                        desc='Epoch {:1d}'.format(epoch), 
                        leave=False, 
                        disable=False)
    
    for batch in progress_bar:
        #set gradient to 0
        model.zero_grad()

        #load into GPU
        batch = tuple(b.to(device) for b in batch)

        #define inputs
        inputs = {'input_ids': batch[0],
                  'attention_mask': batch[1],
                  'labels': batch[2]}
        
        outputs = model(**inputs)
        loss = outputs[0] #output.loss
        loss_train_total +=loss.item()

        #backward pass to get gradients
        loss.backward()
        
        #clip the norm of the gradients to 1.0 to prevent exploding gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        #update optimizer
        optimizer.step()

        #update scheduler
        scheduler.step()
        
        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})     
    
    tqdm.write('\nEpoch {epoch}')
    
    #print training result
    loss_train_avg = loss_train_total/len(dataloader_train)
    tqdm.write(f'Training loss: {loss_train_avg}')
    
    #evaluate
    val_loss, predictions, true_vals = evaluate(dataloader_val)
    
    #f1 score
    #val_f1 = f1_score_func(predictions, true_vals) #old
    val_f1 = f1_score_func(predictions, np.round(true_vals)) #new
    tqdm.write(f'Validation loss: {val_loss}')
    tqdm.write(f'F1 Score (weighted): {val_f1}')
</code></pre>
<p>I am new to this and I really wanna know the actual reasoning and working solution behind the error I am facing. Thank you for your help.</p>
","transformer-model"
"72845537","Confusion Matrix interpretation data perfectly balanced","2022-07-03 09:57:25","","1","95","<nlp><classification><bert-language-model><confusion-matrix><transformer-model>","<p>I have trained a transformer based classifier with 2 classes (0,1) reaching a 91 % accuracy on a perfectly balanced dataset. I printed out the confusion matrix on validation data after had tuned the threshold on them and those are the results but they are perfectly balanced. Makes sense in your opinion?</p>
<pre><code>09:29:30 root INFO:*** EVALUATION ON VALIDATION DATA ***
09:29:30 root INFO:AUC: 0.9708
09:29:30 root INFO:Tuned Threshold: 0.3104
09:29:31 root INFO:Matthews Correlation Coefficient computed after applying the tuned/selected threshold : 0.8230210619188743
09:29:31 root INFO:Accuracy: 91.15%
09:29:32 root INFO:--Classification report for VAL DATA--
09:29:32 root INFO:              precision    recall  f1-score   support

          0       0.91      0.91      0.91     88406
          1       0.91      0.91      0.91     88406

   accuracy                           0.91    176812
  macro avg       0.91      0.91      0.91    176812
weighted avg       0.91      0.91      0.91    176812

        pred:0  pred:1
true:0   80583    7823
true:1    7823   80583
</code></pre>
<p>Thanks for the advice.</p>
<p>UPDATE:</p>
<p>confusion matrix on test set using the same threshold:</p>
<pre><code>        pred:0  pred:1
true:0   81714    9968
true:1    9612   82070
</code></pre>
","transformer-model"
"72839824","Call spring-integration transformer method","2022-07-02 14:27:37","","0","409","<spring-integration><transformer-model><spring-integration-dsl><spring-integration-http>","<p>I have a <code>SpringBoot 2.2.6</code> webapp and the customer ask me to use <code>spring-integration-http</code> for the endpoint.</p>
<p>Now, my goal is to be able to profiling <code>@Transformers</code>.</p>
<p>For example if I design an <code>interface</code> like follow:</p>
<pre><code>public interface CommonTransformer {
   public Integer transform(AObject t, @Header String some);

   public Integer transform(BObject t, @Header String some);
}
</code></pre>
<p>And a class like follow:</p>
<pre><code>@Component
@Profile(&quot;unicredit&quot;)
public class TestTransformer implements CommonTransformer {
    
  @Override
  @Transformer
  public Integer transform(AObject t, @Header(&quot;case&quot;) String caso) {
    return t.getId();
  }

  @Override
  @Transformer
  public Integer transform(BObject t, @Header(&quot;case&quot;) String caso) {
    return t.getId();
  }
}
</code></pre>
<p>And the following endpoint:</p>
<pre><code>@Bean
public IntegrationFlow test(Jackson2JsonObjectMapper obj, CommonTransformer transformer) {
    return IntegrationFlows.from(Http.inboundGateway(&quot;/api/test/{id_a}/{id_b}&quot;)
            .requestMapping(m -&gt; m.methods(HttpMethod.GET))
                .replyChannel(&quot;reply&quot;)
                .payloadExpression(&quot;new it.integration.http.bean.AObject(#pathVariables.id_a, #pathVariables.id_b)&quot;)
                .requestPayloadType(AObject.class))
            .enrichHeaders(h -&gt; h.header(&quot;case&quot;, &quot;test1&quot;))
            .transform(transformer)
            .transform(new ObjectToJsonTransformer(obj))
            .channel(&quot;httpRequest&quot;)
            .get();
    
}

@Bean
public IntegrationFlow test2(Jackson2JsonObjectMapper obj, CommonTransformer transformer) {
    return IntegrationFlows.from(Http.inboundGateway(&quot;/api/test2/{id_b}&quot;)
            .requestMapping(m -&gt; m.methods(HttpMethod.GET))
                .replyChannel(&quot;reply&quot;)
                .payloadExpression(&quot;new it.integration.http.bean.BObject(#pathVariables.id_b)&quot;)
                .requestPayloadType(BObject.class))
            .enrichHeaders(h -&gt; h.header(&quot;case&quot;, &quot;test2&quot;))
            .transform(transformer)
            .transform(new ObjectToJsonTransformer(obj))
            .channel(&quot;httpRequest&quot;)
            .get();
    
}
</code></pre>
<p>This way, if I call the first or the second endpoint the correct <code>@Transformer</code> will be called just specifying the interface as endpoint parameter.</p>
<p>This allow me to create for example another implementation like:</p>
<pre><code>@Component
@Profile(&quot;ubs&quot;)
public class TestTransformer implements CommonTransformer {

  UBSSOAPClient client;

  @Override
  @Transformer
  public Integer transform(AObject t, @Header(&quot;case&quot;) String caso) {
    Object obj = client.getSome();
    return someMapper.mapToAObject(obj);
  }

  @Override
  @Transformer
  public Integer transform(BObject t, @Header(&quot;case&quot;) String caso) {
    Object obj = client.getSome();
    return someMapper.mapToBObject(obj);
  }
}
</code></pre>
<p>This kind of profiling is vital for the project I'm developing, but if I have two endpoint like this:</p>
<pre><code>@Bean
public IntegrationFlow test(Jackson2JsonObjectMapper obj, CommonTransformer transformer) {
    return IntegrationFlows.from(Http.inboundGateway(&quot;/api/test/{id}&quot;)
            .requestMapping(m -&gt; m.methods(HttpMethod.GET))
                .replyChannel(&quot;reply&quot;)
                .payloadExpression(&quot;#pathVariables.id)
                .requestPayloadType(Integer.class))
            .transform(transformer)
            .transform(new ObjectToJsonTransformer(obj))
            .channel(&quot;httpRequest&quot;)
            .get();
}

@Bean
public IntegrationFlow test(Jackson2JsonObjectMapper obj, CommonTransformer transformer) {
    return IntegrationFlows.from(Http.inboundGateway(&quot;/api/test2/{id}&quot;)
            .requestMapping(m -&gt; m.methods(HttpMethod.GET))
                .replyChannel(&quot;reply&quot;)
                .payloadExpression(&quot;#pathVariables.id)
                .requestPayloadType(Integer.class))
            .transform(transformer)
            .transform(new ObjectToJsonTransformer(obj))
            .channel(&quot;httpRequest&quot;)
            .get();
}
</code></pre>
<p>The trick doesn't work because they have the same <code>Integer</code> parameter within the payload.
Therefore the question is can I have an interface like:</p>
<pre><code>public interface CommonTransformer {
 public Integer transformA(AObject t, @Header String some, Other params);

 public Integer transformB(BObject t, @Header String some);
 .......
 other methods
}
</code></pre>
<p>Several profiled implementation and calling the appropriate method in some way at this point:</p>
<pre><code>@Bean
public IntegrationFlow test(CommonTransformer transformer) {
     .....
     .....
     .transform(transformer::transformA) // or something like that??
}
</code></pre>
<p>Clearly I want to pass to the method the payload the headers I enriched (or setted with <code>#pathVariables</code> or <code>#requestParams</code> expression).</p>
<p>Maybe the question (and the code I have written) may seem dummy but I'm absolutely new to <code>spring-integration</code> and I have to do so much thing in a very short time..</p>
<p>Any help is appreciated.</p>
","transformer-model"
"72833114","tf.keras.layers.MultiHeadAttention's argument key_dim sometimes not matches to paper's example","2022-07-01 18:03:52","73874012","4","2923","<tensorflow><tf.keras><transformer-model><attention-model>","<p>For example, I have input with shape (1, 1000, 10) (so, <code>src.shape</code> wil be <code>(1, 1000, 10)</code>, which means the sequence length is 1000, and the dimension is 10. Then:</p>
<ul>
<li>This works (random <code>num_head</code> and <code>key_dim</code>):</li>
</ul>
<pre><code>class Model(tf.keras.Model):
        def __init__(self):
            super(Model, self).__init__()
            self.attention1 = tf.keras.layers.MultiHeadAttention(num_heads=20, key_dim=9)
            self.dense = tf.keras.layers.Dense(10, activation=&quot;softmax&quot;)

        def call(self, src) :
            output = self.attention1(src, src)
            output = tf.reshape(output, [1, 10000])
            output = self.dense(output)
            return output
</code></pre>
<ul>
<li>And this works too (random <code>num_head</code> and <code>key_dim</code>) :</li>
</ul>
<pre><code>class Model(tf.keras.Model):
        def __init__(self):
            super(Model, self).__init__()
            self.attention1 = tf.keras.layers.MultiHeadAttention(num_heads=123, key_dim=17)
            self.dense = tf.keras.layers.Dense(10, activation=&quot;softmax&quot;)

        def call(self, src):
            output = self.attention1(src, src)
            output = tf.reshape(output, [1, 10000])
            output = self.dense(output)
            return output
</code></pre>
<p>So, this layer works with whatever <code>num_heads</code> and <code>key_dim</code>, which does not match the paper idea. (It works because no error report, and it able to train)</p>
<p>In the paper, 'attention is all you need', it says <code>key_dim</code> is the dimension of key for each head, not the original head dimension, and thus <code>key_dim</code> should equal to <code>embed_dim</code>/<code>head_num</code>. So, if we want to have a <code>head_num</code> of 5, the <code>key_dim</code> has to be 2, if <code>embedding_dim</code> is 10.</p>
<p><a href=""https://i.sstatic.net/4MUe3.png"" rel=""nofollow noreferrer"">the screen shot from the paper</a></p>
<p>Also, from the keras attention class discription, the <code>key_dim</code> is Size of each attention head for query and key, which matches to the paper idea.</p>
<p><a href=""https://i.sstatic.net/eRRGN.png"" rel=""nofollow noreferrer"">the screen shot from the class discription</a></p>
<p>Therefore, why <code>tf.keras.layers.MultiHeadAttention</code> able to take unmatched dimension. When it takes the unmatching dimension, how does it work internally with these extra weight parameters?</p>
<ul>
<li>Some of the question descriptions are cite from <a href=""https://stackoverflow.com/questions/70034327/understanding-key-dim-and-num-heads-in-tf-keras-layers-multiheadattention"">here</a>.</li>
</ul>
","transformer-model"
"72824754","What machine specs do you need in training of DETR?(End-to-End Object Detection with Transformers)","2022-07-01 05:29:50","","0","405","<python><deep-learning><transformer-model>","<p>I am researching the machine specs needed for DETR training.
However, I only have a geforce 1660 super and I got an &quot;out of memory&quot; error. So, please let me know how much machine specs you have to use to complete the DETR training.
Please help me with my research.
DETR(<a href=""https://github.com/facebookresearch/detr"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/detr</a>)</p>
","transformer-model"
"72815200","Does HuggingFace's Trainer automatically ignore features not required by the model?","2022-06-30 11:44:40","72855272","0","1313","<deep-learning><huggingface-transformers><bert-language-model><transformer-model>","<p>I am a new user of Transformers and I have successfully fine-tuned a BERT model following the tutorial.</p>
<p>However, I have one question about the features I send to the Trainer and those accepted by the BERT model.</p>
<p>Specifically, my original dataset contains two columns named “text” and “label”. After tokenizing the “text”, the dataset object now has three more columns named “input_ids”, “token_type_ids”, and “attention_mask”. I understand that these three columns are required by the BERT model, but I didn’t drop the original “text” column when I fed the dataset to the Trainer API.</p>
<p>So my question is, does BERT automatically ignore non-relevant features? (maybe this is achieved quietly by the Trainer API) Or should I remove these columns, leaving only “input_ids”, “token_type_ids”, and “attention_mask”?</p>
<p>For example, below is my dataset object:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 6851
    })
    test: Dataset({
        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 762
    })
})
</code></pre>
<p>And I fed it and my model to the Trainer:</p>
<pre><code>trainer = Trainer(
    model,
    training_args,
    train_dataset=data[&quot;train&quot;],
    eval_dataset=data[&quot;test&quot;],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
</code></pre>
<p>What happened to the &quot;text&quot; feature?</p>
","transformer-model"
"72804316","How to duplicate records check between the source file and target table in Datastage","2022-06-29 15:46:23","","0","108","<transformer-model><datastage>","<p>I want to do two types of duplicate checking</p>
<ol>
<li>If we already have loaded A file With That name previously.</li>
</ol>
<p>For instance, file A is loaded into the target table, and subsequent run, if we receive the file A, this time sequence should be aborted because it's already loaded.</p>
<ol start=""2"">
<li>If we have already loaded a with the identical records</li>
</ol>
<p>For instance, file A is already in the target table, and next time we receive file B in this file B, those already loaded in the target with file A should not be loaded, and the job should be aborted</p>
<p>Can anyone help me with this scenario?</p>
<p>Thanks
Venkat.</p>
","transformer-model"
"72776335","ValueError: Unable to create dataset (name already exists) when using ModelCheckpoint to save my model","2022-06-27 17:44:49","","4","10863","<python><tensorflow><keras><transformer-model>","<p>I am trying to run the Keras official code example &quot;Image classification with Swin Transformers&quot;.</p>
<p>The code works fine at first, but after I added a <code>ModelCheckpoint</code> to save the hdf5 model in the callbacks argument of the <code>model.fit</code> method { i.e. <code>model.fit(..., callbacks=[ModelCheckpoint(...)], ..., )</code> }, I get the following error:</p>
<blockquote>
<p>ValueError: Unable to create dataset (name already exists)</p>
</blockquote>
<p>What does <code>name</code> refer to here? How should I solve this problem?</p>
<p>I ran the code on my local device (Windows 10, tensorflow2.8.0) and Google Colab (tensorflow2.8.2) and both results in the above error.</p>
<p>The full code example can be found here [https://keras.io/examples/vision/swin_transformers/] ,The only difference between my code and code example is that I added a line of code for <code>ModelCheckpoint</code>. The location of the added code and the error message are shown below.</p>
<p>Code fragment:</p>
<pre><code>model = keras.Model(input, output)
model.compile(
    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),
    optimizer=tfa.optimizers.AdamW(
        learning_rate=learning_rate, weight_decay=weight_decay
    ),
    metrics=[
        keras.metrics.CategoricalAccuracy(name=&quot;accuracy&quot;),
        keras.metrics.TopKCategoricalAccuracy(5, name=&quot;top-5-accuracy&quot;),
    ],
)

history = model.fit(
    x_train,
    y_train,
    batch_size=batch_size,
    epochs=num_epochs,
    validation_split=validation_split,
    # 👇 I added one line of code
    callbacks = keras.callbacks.ModelCheckpoint('lowest_loss.hdf5', monitor='loss', verbose=0, save_best_only=True, save_weights_only=True)
)
</code></pre>
<p>This is the error I got:</p>
<blockquote>
<p>ValueError<br />
Traceback (most recent call last)<br />
 in ()<br />
18     validation_split=validation_split,<br />
19     # 👇 I added one line of code<br />
20     callbacks = keras.callbacks.ModelCheckpoint('lowest_loss.hdf5', monitor='loss', verbose=0, save_best_only=True, save_weights_only=True)<br />
21 )</p>
<p>2 frames<br />
/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)<br />
65     except Exception as e:  # pylint: disable=broad-except<br />
66       filtered_tb = _process_traceback_frames(e.<strong>traceback</strong>)<br />
67       raise e.with_traceback(filtered_tb) from None<br />
68     finally:<br />
69       del filtered_tb</p>
<p>/usr/local/lib/python3.7/dist-packages/h5py/_hl/group.py in create_dataset(self, name, shape, dtype, data, **kwds)<br />
146                     group = self.require_group(parent_path)<br />
147<br />
148             dsid = dataset.make_new_dset(group, shape, dtype, data, name, **kwds)<br />
149             dset = dataset.Dataset(dsid)<br />
150             return dset</p>
<p>/usr/local/lib/python3.7/dist-packages/h5py/_hl/dataset.py in make_new_dset(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, allow_unknown_filter)<br />
135<br />
136<br />
137     dset_id = h5d.create(parent.id, name, tid, sid, dcpl=dcpl)<br />
138<br />
139     if (data is not None) and (not isinstance(data, Empty)):</p>
<p>h5py/_objects.pyx in h5py._objects.with_phil.wrapper()</p>
<p>h5py/_objects.pyx in h5py._objects.with_phil.wrapper()</p>
<p>h5py/h5d.pyx in h5py.h5d.create()</p>
<p>ValueError: Unable to create dataset (name already exists)</p>
</blockquote>
","transformer-model"
"72758187","OpenAI GPT-3 API: Fine tune a fine tuned model?","2022-06-26 00:35:25","","8","3409","<transformer-model><openai-api><fine-tuning><gpt-3>","<p>The OpenAI documentation for the <code>model</code> attribute in the fine-tune API states a bit confusingly:</p>
<blockquote>
<p><strong>model</strong></p>
<p>The name of the base model to fine-tune. You can select one of &quot;ada&quot;, &quot;babbage&quot;, &quot;curie&quot;, &quot;davinci&quot;, or a fine-tuned model created after 2022-04-21.</p>
</blockquote>
<p>My question: is it better to fine-tune a base model or a fine-tuned model?</p>
<p>I created a fine-tune model from <code>ada</code> with file <code>mydata1K.jsonl</code>:</p>
<pre><code>ada + mydata1K.jsonl --&gt; ada:ft-acme-inc-2022-06-25
</code></pre>
<p>Now I have a bigger file of samples <code>mydata2K.jsonl</code> that I want to use to improve the fine-tuned model.
In this second round of fine-tuning, is it better to fine-tune <code>ada</code> again or to fine-tune my fine-tuned model <code>ada:ft-acme-inc-2022-06-25</code>?  I'm assuming this is possible because my fine tuned model is created after 2022-04-21.</p>
<pre><code>ada + mydata2K.jsonl --&gt; better-model
</code></pre>
<p>or</p>
<pre><code>ada:ft-acme-inc-2022-06-25 + mydata2K.jsonl --&gt; even-better-model?
</code></pre>
","transformer-model"
"72737771","Can transformer network be used as one-to-many structure?","2022-06-24 00:08:33","","1","633","<networking><transformer-model>","<p>I want to replace one-to-many LSTM into transformer network, so I am studying it right now.
But as far as I studied, the transformer network uses Encoder-Decoder structure, which is mapping the input components with the output components.</p>
<p>Is it possible to use this encoder-decoder structure with one-to-many structure like LSTM?
If this is possible, then we are connecting all the &quot;many&quot; parts with the &quot;one&quot; vector and training them. Is it correct?</p>
","transformer-model"
"72714050","Matrix operation in an Attention Mechanism","2022-06-22 10:42:09","","-1","96","<machine-learning><math><matrix><transformer-model>","<p>I am reading an article dealing with Transformer Machine Learning models applied to finance. I am trying to understand the math behind the architecture, but I failed to understand this part :</p>
<p><a href=""https://i.sstatic.net/QKjx3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QKjx3.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/7s3Im.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7s3Im.png"" alt=""enter image description here"" /></a></p>
<p>Especially, I don't get why the dimensions are not matching between the operations.
According to my comprehension:</p>
<ol>
<li><strong>step (8)</strong> : <em>u</em> should be <em>M(d_model, 1)</em></li>
<li><strong>step (9)</strong> : this should not be possible as the matrix    multiplication dimensions does not match to perform the operation:
<em>M(d_model, K) . M(1,d_model)</em></li>
</ol>
<p>Here is the full part of the study :</p>
<p><a href=""https://i.sstatic.net/aDUun.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aDUun.png"" alt=""enter image description here"" /></a></p>
<p>I guess, I am missunderstanding something with this notation <a href=""https://i.sstatic.net/u761f.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/u761f.png"" alt=""enter image description here"" /></a>
or with the <em>&quot;non-linearly project the matrix M to u&quot;</em> sentence.</p>
<p>Can someone enlighten me about this, please ?</p>
<blockquote>
<p><strong>Transformer-based attention network for stock movement prediction</strong>, 2022,
<em>Qiuyue Zhang, Chao Qin, Yunfeng Zhang , Fangxun Bao, CaimingZhang, Peide Liu</em></p>
</blockquote>
","transformer-model"
"72709138","issue with loading a model in Keras in Colab","2022-06-22 02:49:11","","0","333","<tensorflow><keras><deep-learning><google-colaboratory><transformer-model>","<p>I've saved my model as follows:
model.save('/content/model')</p>
<p>When my model is saved: I got this WARNING:</p>
<pre><code>WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 430). These functions will not be directly callable after loading.
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Assets written to: /content/model/assets
INFO:tensorflow:Assets written to: /content/model/assets
WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x7f95000fac10&gt; has the same name 'LSTMCell' as a built-in Keras object. Consider renaming &lt;class 'keras.layers.recurrent.LSTMCell'&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.
WARNING:absl:&lt;keras.layers.recurrent.LSTMCell object at 0x7f950018eb50&gt; has the same name 'LSTMCell' as a built-in Keras object. Consider renaming &lt;class 'keras.layers.recurrent.LSTMCell'&gt; to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.
</code></pre>
<p>Now,when I try to load the model I got error message:<br />
model_CSP = keras.models.load_model('/content/model')</p>
<p>I got an error message as follows:</p>
<pre><code>2022-06-22 01:57:09.702116: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
Traceback (most recent call last):
  File &quot;Do_predection_Combined_model_.py&quot;, line 730, in &lt;module&gt;
    model_CSP = keras.models.load_model('/content/CSP_model')
  File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py&quot;, line 573, in assert_same_structure
    % (str(e), str1, str2))
ValueError: The two structures don't have the same nested structure.

First structure: type=tuple str=(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None)},), {'training': False})

Second structure: type=tuple str=((TensorSpec(shape=(None, 128), dtype=tf.int32, name='inputs'),), {'token_type_ids': TensorSpec(shape=(None, 128), dtype=tf.int32, name='token_type_ids'), 'training': False, 'attention_mask': TensorSpec(shape=(None, 128), dtype=tf.int32, name='attention_mask')})

More specifically: Substructure &quot;type=dict str={'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None)}&quot; is a sequence, while substructure &quot;type=TensorSpec str=TensorSpec(shape=(None, 128), dtype=tf.int32, name='inputs')&quot; is not
Entire first structure:
(({'input_ids': .},), {'training': .})
Entire second structure:
((.,), {'token_type_ids': ., 'training': ., 'attention_mask': .})

'''
</code></pre>
<p>I don't know what is wrong? I follow the best practice in saving and loading a model. but I still get this error.</p>
<p>Edit the question:</p>
<p>I think I make a big mistake when I choose Keras API to train my model. After I spend a lot of time designing and training my model. I can't save and reload again. I don't know why the ML experiments recommend this API for beginners. I tried all practice for saving and loading my model and  All of them failed. get tired of this</p>
","transformer-model"
"72707540","How to create an iterable DataPipe with PyTorch using txt files","2022-06-21 21:56:07","72711183","0","898","<python><nlp><pytorch><transformer-model><torchtext>","<p>I have two text files to train a transformer model. However, instead of using PyTorch's own datasets, I'm using something I downloaded from the internet.</p>
<pre><code>source = open('./train_de.de', encoding='utf-8').read().split('\n')
target = open('./train_en.en', encoding='utf-8').read().split('\n')
</code></pre>
<p>With the code above, I have some Danish sentences in a list named &quot;source&quot;, and their translation in English sentences in another list named &quot;target&quot;.</p>
<p>My question is, how can I make an iterable DataPipe with PyTorch such that when I write something like:</p>
<pre><code>source, target = next(iter(train_iter))
</code></pre>
<p>this will give me the Danish sentence with it's corresponding English translation in seperate strings?</p>
","transformer-model"
"72695297","Difference between from_config and from_pretrained in HuggingFace","2022-06-21 04:20:10","72744660","2","3537","<huggingface-transformers><transformer-model><distilbert>","<pre class=""lang-py prettyprint-override""><code>num_labels = 3 if task.startswith(&quot;mnli&quot;) else 1 if task==&quot;stsb&quot; else 2
preconfig = DistilBertConfig(n_layers=6)
    
model1 = AutoModelForSequenceClassification.from_config(preconfig)
model2 = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)
</code></pre>
<p>I am modifying <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=545PP3o8IrJV"" rel=""nofollow noreferrer"">this code</a> (modified code is provided above) to test DistilBERT transformer layer depth size via <code>from_config</code> since from my knowledge <code>from_pretrained</code> uses 6 layers because in the paper section 3 they said:</p>
<blockquote>
<p>we initialize the student from the teacher by taking one layer out of two</p>
</blockquote>
<p>While what I want to test is various sizes of layers. To test whether both are the same, I tried running the <code>from_config</code>
with <code>n_layers=6</code> because based on the documentation <a href=""https://huggingface.co/docs/transformers/model_doc/distilbert#transformers.DistilBertConfig"" rel=""nofollow noreferrer"">DistilBertConfig</a> the <code>n_layers</code> is used to determine the transformer block depth. However as I run <code>model1</code> and <code>model2</code> I found that with  SST-2 dataset, in accuracy:</p>
<ul>
<li><code>model1</code> achieved only <code>0.8073</code></li>
<li><code>model2</code> achieved <code>0.901</code></li>
</ul>
<p>If they both behave the same I expect the result to be somewhat similar but 10% drop is a significant drop, therefore I believe there ha to be a difference between the functions. Is there a reason behind the difference of the approach (for example <code>model1</code> has not yet applied hyperparameter search) and is there a way to make both functions behave the same? Thank you!</p>
","transformer-model"
"72673637","The decoder part in a transformer model","2022-06-19 00:49:14","72747878","3","4950","<nlp><transformer-model><decoder>","<p>I'm fairly new to NLP and I was reading a blog explaining the transformer model. I was quite confused about the input/output for the decoder block (attached below). I get that y_true is fed into the decoder during the training step to combine with the output of the encoder block. What I don't get is, if we already know y_true, why run this step to get the output probability? I just don't quite get the relationship between the bottom right &quot;Output Embedding&quot; and the top right &quot;Output Probabilities&quot;. When we use the model, we wouldn't really have y_true, do we just use y_pred and feed them into the decoder instead? This might be a noob question. Thanks in advance.</p>
<p><a href=""https://i.sstatic.net/nQ2f5.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nQ2f5.png"" alt=""The Decoder Block of the Transformer Architecture
Taken from “Attention Is All You Need“"" /></a></p>
","transformer-model"
"72639699","How to fine tune a masked language model?","2022-06-16 02:08:32","","2","520","<machine-learning><pytorch><huggingface-transformers><transformer-model><huggingface-datasets>","<p>I'm trying to follow the <a href=""https://huggingface.co/course/chapter7/3?fw=pt"" rel=""nofollow noreferrer"">huggingface tutorial</a> on fine tuning a masked language model (masking a set of words randomly and predicting them). But they assume that the dataset is in their system (can load it with <code>from datasets import load_dataset; load_dataset(&quot;dataset_name&quot;))</code>. However, my input dataset is a long string:</p>
<pre><code>text = &quot;This is an attempt of a great example. &quot;
dataset = text * 3000
</code></pre>
<p>I followed their approach and tokenized each it:</p>
<pre><code>from transformers import AutoTokenizer
from transformers import AutoModelForMaskedLM
import torch
from transformers import DataCollatorForLanguageModeling

model_checkpoint = &quot;distilbert-base-uncased&quot;
model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

def tokenize_long_text(tokenizer, long_text):
    individual_sentences = long_text.split('.')
    tokenized_sentences_list = tokenizer(individual_sentences)['input_ids']
    tokenized_sequence = [x for xs in tokenized_sentences_list for x in xs]
    return tokenized_sequence 

tokenized_sequence = tokenize_long_text(tokenizer, long_text)
</code></pre>
<p>Following by chunking it into equal length segments:</p>
<pre><code>def chunk_long_tokenized_text(tokenizer_text, chunk_size):
    # Compute length of long tokenized texts
    total_length = len(tokenizer_text)
    # We drop the last chunk if it's smaller than chunk_size
    total_length = (total_length // chunk_size) * chunk_size
    
    return [tokenizer_text[i : i + chunk_size] for i in range(0, total_length, chunk_size)]

chunked_sequence = chunk_long_tokenized_text(tokenized_sequence, 30)
</code></pre>
<p>Created a data collator for random masking:</p>
<pre><code>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15) # expects a list of dicts, where each dict represents a single chunk of contiguous text
</code></pre>
<p>Example of how it works:</p>
<pre><code>d = {}
d['input_ids'] = chunked_sequence[0]
d

&gt;&gt;&gt;{'input_ids': [101,
  2023,
  2003,
  1037,
  2307,
  103,...

for chunk in data_collator([ d ])[&quot;input_ids&quot;]:
    print(f&quot;\n'&gt;&gt;&gt; {tokenizer.decode(chunk)}'&quot;)
&gt;&gt;&gt;'&gt;&gt;&gt; [CLS] this is a great [MASK] [SEP] [CLS] this is a great [MASK] [SEP] [CLS] this is a great [MASK] [SEP] [CLS] this is a great [MASK] [SEP] [CLS] this'
</code></pre>
<p>However, the remaining steps (which I believe is just the training component) seem to only work using their <code>trainer</code> method, which can only take their dataset.</p>
<p>How can this work with a dataset in the form of a string?</p>
","transformer-model"
"72623363","PyTorch TransformerEncoderLayer different input order gets different results","2022-06-14 21:06:59","72688889","2","1629","<python><pytorch><transformer-model>","<p>Before I start, I’m very new to Transformers, and sorry for by bad sentence structure, I have a fever right now.</p>
<p>Any time I use nn.TransformerEncoderLayer in anyway with a saved model if the data is in a different order I get different results. Is there a way to save the Encode table (or whatever this would be), this would be in the MultiheadAttention part of the TransformerEncoderLayer right?</p>
<p>Just using TransformerEncoderLayer and save the model and then use np.random.permutation() to shuffle the input data. And running the input data through the model after calling model.eval. This always gives me different results unless I use the same order every time.</p>
<p>I have this layer in my model like this</p>
<pre><code>self.transformer = nn.TransformerEncoderLayer()
</code></pre>
<p>I save the model like so</p>
<pre><code>torch.save(model, path)
</code></pre>
<p>Does this not save the nn.TransformerEncoderLayer() or something?</p>
","transformer-model"
"72620563","What's the use of residual connections in neural networks?","2022-06-14 16:39:03","","0","682","<machine-learning><neural-network><transformer-model><self-attention>","<p>I've recently been learning about self-attention transformers and the &quot;Attention is All You Need&quot; paper. When describing the architecture of the neural network used in the paper, one breakdown of the paper included this explanation for residual connections:</p>
<p>&quot;Residual layer connections are used (of course) in both encoder and decoder blocks&quot;
(origin: <a href=""https://www.kaggle.com/code/residentmario/transformer-architecture-self-attention/notebook"" rel=""nofollow noreferrer"">https://www.kaggle.com/code/residentmario/transformer-architecture-self-attention/notebook</a>)</p>
<p>This was, unfortunately, not obvious to me. What is the purpose of residual connections, and why should this be standard practice?</p>
","transformer-model"
"72572232","How to preserve column order after applying sklearn.compose.ColumnTransformer on numpy array","2022-06-10 09:45:32","72572491","0","874","<python><scikit-learn><numpy-ndarray><scaling><transformer-model>","<p>I want to use <code>Pipeline</code> and <code>ColumnTransformer</code> modules from sklearn library to apply scaling on numpy array. Scaler is applied on some of the columns. And, I want to have the output with same column order of input.</p>
<p>Example:</p>
<pre><code>import numpy as np
from sklearn.compose import ColumnTransformer 
from sklearn.preprocessing import  MinMaxScaler


X = np.array ( [(25, 1, 2, 0),
                (30, 1, 5, 0),
                (25, 10, 2, 1),
                (25, 1, 2, 0),
                (np.nan, 10, 4, 1),
                (40, 1, 2, 1) ] )



column_trans = ColumnTransformer(
    [ ('scaler', MinMaxScaler(), [0,2]) ], 
     remainder='passthrough') 
      
X_scaled = column_trans.fit_transform(X)
</code></pre>
<p>The problem is that <code>ColumnTransformer</code> changes the order of columns. How can I preserve the original order of columns?</p>
<p>I am aware of this <a href=""https://stackoverflow.com/questions/68874492/preserve-column-order-after-applying-sklearn-compose-columntransformer"">post</a>. But, it is for pandas DataFrame. For some reasons, I cannot use DataFrame and I have to use numpy array in my code.</p>
<p>Thanks.</p>
","transformer-model"
"72556640","How to save/load a model checkpoint with several losses in Pytorch?","2022-06-09 07:56:44","72558011","1","2235","<python><pytorch><loss-function><embedding><transformer-model>","<p>Using Ubuntu 20.04, Pytorch 1.10.1.</p>
<p>I am trying to solve a music generation task with a transformer architecture and multi-embeddings, for processing tokens with several characteristics.</p>
<p>In each training iteration, I have to calculate the loss of each token characteristic and store it in a vector, then I suppose that I should store in a checkpoint a vector containing all of them (or something similar), instead of what I'm doing now which is saving the total loss. I would like to know how to store all losses in the checkpoint (be able to keep training when loading it), or if it isn't needed at all.</p>
<p>The epochs loop:</p>
<pre class=""lang-py prettyprint-override""><code>for epoch in range(0, epochs):
    
    print('Epoch: ', epoch)
    
    loss = trfrmr.train(epoch+1, model, train_loader, train_loss_func, opt, lr_scheduler, num_iters=-1)
    loss_train.append(loss)
    
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': opt.state_dict(),
        'loss': loss,
        }, &quot;model_pop909_checkpoint.pth&quot;)
</code></pre>
<p>The training loop:</p>
<pre class=""lang-py prettyprint-override""><code>for batch_num, batch in enumerate(dataloader):
    time_before = time.time()

    opt.zero_grad()

    x = batch[0].to(get_device())
    tgt = batch[1].to(get_device())

    # x is the input sequence (N,T,Z), that should be input into the transformer forward function as (T,N,Z)
    y = model(x.permute(1, 0, 2))

    # tgt is the real output sequence, of shape (N,T,Z), T is sequence length, N batch size, Z the different token types
    # y are the output logits, is a list of Z tensors of shape (T,N,C*) where C is the vocabulary size, and will vary depending on the token type (pitch, velocity etc...)
    losses = []
    for j in range(LEN_VOCAB):
        aux_loss = loss.forward(y[j].permute(1, 2, 0),
                                        tgt[..., j])  # shapes (N,C,T) and (N,T), see Pytorch cross-entropy for details
        losses.append(aux_loss)

    losses_sum = sum(losses)  # here we sum, but we could also have mean for instance

    losses_sum.backward()
    opt.step()

    if lr_scheduler is not None:
         lr_scheduler.step()

    lr = opt.param_groups[0]['lr']
                
    loss_hist.append(losses_sum)
    if batch_num == num_iters:
       break
</code></pre>
","transformer-model"
"72548986","Masked self-attention in tranformer's decoder","2022-06-08 16:12:15","","1","1093","<transformer-model><attention-model><encoder-decoder><self-attention>","<p>I'm writing my thesis about attention mechanisms. In the paragraph in which I explain the decoder of transformer I wrote this:</p>
<blockquote>
<p>The first sub-layer is called masked self-attention, in which the masking operation consists in preventing the decoder from paying attention to subsequent words.
That is to say, while training a transformer for translation purposes, it is possible to access the target translation; on the other hand, during the inference, that is the translation of new sentences, it is not possible to access the target translation. Therefore, when calculating the probabilities of the next word in the sequence, the network must not access that word. Otherwise, the translation task would be banal and the network would not learn to predict the translation correctly.</p>
</blockquote>
<p>I don't know if I said something wrong also in the previous part, but my professor thinks I made mistakes in the following part:</p>
<blockquote>
<p>To understand in a simple way the functioning of the masked self-attention level, let's go back to the example “Orlando Bloom loves Miranda Kerr” (x1 x2 x3 x4 x5).
If we consider the inputs as vectors x1, x2. x3. x4. x5 and we want to translate the word x3 corresponding to &quot;loves&quot;, you need to make sure that the following words x4 and x5 do not influence the translation y3. To prevent this influence, masking sets the weights of x4 and x5 to zero. Then a normalization of the weights is performed so that the sum of the elements of each column in the matrix is equal to 1. The result is a matrix with normalized weights in each column.</p>
</blockquote>
<p>Can someone please tell me where the miskates are?</p>
","transformer-model"
"72519845","huggingface transformer issue","2022-06-06 15:21:15","","1","1037","<python><bert-language-model><transformer-model><huggingface>","<p>I used huggingface transformer, but I got some issues like below.
How can I handle this problem?</p>
<pre><code>training_args = TrainingArguments(
    output_dir='./.checkpoints',
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.batch_size_per_device,
    per_device_eval_batch_size=config.batch_size_per_device,
    warmup_steps=n_warmup_steps,
    weight_decay=0.01,
    fp16=True,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    logging_steps=n_total_iterations // 100,
    save_steps=n_total_iterations // config.n_epochs,
    load_best_model_at_end=True, )
</code></pre>
<hr />
<pre><code>  File &quot;finetune_plm_hftrainer.py&quot;, line 134, in main
    load_best_model_at_end=True,
  File &quot;&lt;string&gt;&quot;, line 90, in __init__
  File &quot;/usr/local/lib/python3.6/dist-packages/transformers/training_args.py&quot;, line 813, in __post_init__
    raise ValueError(f&quot;logging strategy {self.logging_strategy} requires non-zero --logging_steps&quot;)
ValueError: logging strategy IntervalStrategy.STEPS requires non-zero --logging_steps
</code></pre>
","transformer-model"
"72515538","Why not train RNN with x=data[0:n] and y=data[n+1] (but instead x=data[0:n] and y=data[1:n+1])?","2022-06-06 09:18:01","","0","162","<python><deep-learning><time-series><recurrent-neural-network><transformer-model>","<p>This is a general and I think very basic/elementary question about how to set up recurrent neural networks. For the sake of it, let's assume we're training an autoregressive language model that tries to predict the next character in some text.</p>
<p>When I look at existing implementations that train an RNN, what I usually find is that the data fed to these RNNs are snippets of a certain length where the input and output are of the same shape, but shifted relative to each other by one sample (such that the predictor x is data[0:n] and the predictee y is data[1:n+1]).
At the end of such an RNN, we'd usually find some sort of mapping (+softmax) from the hidden state h_t to the number of classes (here: characters). Usually, this seems to produce a class label for each of the input time samples, i.e., we get a class label prediction for each output sample. In other words, we predict y[0] from predictor[0] using the hidden state after it has seen x[0] (h_t=0), we predict y[1] from x[1] using (h_t=1), and so on and so forth.</p>
<p>I find this surprising, because I thought the whole point of RNNs was to &quot;build up&quot; / &quot;develop&quot; a hidden state over a certain amount of time. I.e., in a case as described above, I would expect that we should only generate a single prediction for data[n+1] (i.e. y[n]), using h_t=n (where h is thus integrating and thus exploiting the entire history length as defined by the data loader).
If a hidden state is initiated randomly, and then only sees 1 single sample of data, I would naively assume it should perform a lot worse vs when it has seen n data points. If the mapping from hidden state to output is trained with such suboptimal &quot;premature&quot; states, shouldn't it converge on some compromise between compensating for the premature states and ideally exploiting information in a &quot;mature&quot; hidden state?</p>
<p>However, usually, when I try to set an RNN up in the way I thought it should make more sense, it doesn't work as well (I reach worse validation losses). This could be for several reasons (given that I am not providing my code), but can someone maybe enlighten me if there is a good theoretical motivation why RNNs (and transformers) aren't usually fed with pairs of predictor=data[0:n] and predictee=data[n+1]?</p>
","transformer-model"
"72437297","how can we apply masked language modelling on the images using multimodal models? How can we implement such a thing and get MLM scores?","2022-05-30 16:17:05","","1","64","<image-processing><huggingface-transformers><bert-language-model><transformer-model><multimodal>","<p>It might not be clear from the question what I want to say, but how can we apply masked language modelling with the text and image given using multimodal models like lxmert. For example, if there is some text given (This is a MASK) and we mask some word in it, and there is an image given (maybe of a cat), how can we apply MML to predict the word as cat? How can we implement such a thing and get MLM scores out of it using huggingface library api? A snippet of code explaining such will be great. If anyone can help, it would help in better understanding.</p>
","transformer-model"
"72414634","how can we get the attention scores of multimodal models via hugging face library?","2022-05-28 09:51:10","72415815","3","717","<image-processing><huggingface-transformers><bert-language-model><transformer-model><attention-model>","<p>I was wondering if we could get the attention scores of any multimodal model using the api provided  by the hugging face library, as it's relatively easier to get such scores of normal language bert model, but what about lxmert? If anyone can answer this, it would help the understanding of such models.</p>
","transformer-model"
"72390209","MultiHeadAttention masking with tensorflow","2022-05-26 10:15:35","","2","571","<tensorflow><masking><transformer-model><self-attention>","<p>I have been trying to make a custom mask for targetted combinations of queries and keys for my MultiHeadAttention layer but can not figure out the way to use this layer masking.</p>
<p>Here is an example with a dummy dataset (batch size 1) :</p>
<pre><code>key     = tf.ones([1, 32 , 128])
mask    = tf.concat([
    tf.concat([tf.zeros([16 , 16]) , tf.zeros([16 , 16]) ] , 0) ,
    tf.concat([tf.zeros([16 , 16]) , tf.ones([16 , 16])  ] , 0) ] , 1)
mask    = mask[tf.newaxis, tf.newaxis, : , : ]


# key shape  -&gt; ( 1 , 32 , 128 )
# mask shape -&gt; ( 1 , 1,  32 , 32 )
</code></pre>
<p>when I print <code>mask[0][0].numpy()</code> I get :</p>
<p><a href=""https://i.sstatic.net/5uRnU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5uRnU.png"" alt=""masking"" /></a></p>
<p>Now using the foolowing layer ( 1 head , self-attention ) :</p>
<pre><code>mha_layer =  tf.keras.layers.MultiHeadAttention( num_heads=1, key_dim=128 )
attention_output, attention_scores = mha_layer(  key , key , attention_mask=mask  ,  return_attention_scores=True)
</code></pre>
<p>I get the folowing attention scores (<code>attention_scores[0][0].numpy()</code>) :</p>
<p><a href=""https://i.sstatic.net/mmB6s.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mmB6s.png"" alt=""attention scores"" /></a></p>
<p>Here the dark-violet color stands for 0.0 , yellow for 0.06 and green-blue for 0.03</p>
<p>I would expect to have expected the green-blue part to be 0.0s because of the masking.</p>
<p>Am I using the masking wrong ? or it is not possible to mask entire queries/keys ?</p>
<p>I hope my question makes sense 😅 and that it is not too obvious.
Thank you in advance, if you can help :)</p>
","transformer-model"
"72362223","Adding more custom entities into pretrained custom NER Spacy3","2022-05-24 11:37:38","","1","338","<vector><nlp><named-entity-recognition><transformer-model><spacy-3>","<p>I've a huge amount of textual data and wanted to add around 50 different entities. Initially when I started working with it, I was getting memory error. As we know spacy can handle 1,00,000 tokens per GB and maximum up to 10,00,000. So I chunked my dataset into 5 sets and using annotator created multiple JSON file for the same. Now I started with one JSON and successfully completed creating the model and now I want to add more data into it so that I don't miss out any tags and there's a good variety of data is used while training in the model. Please guide me how to proceed next.</p>
","transformer-model"
"72323634","Issue in tensor value assignment. What can be wrong in script?","2022-05-20 18:54:23","72430915","0","72","<python><pipeline><tensor><transformer-model>","<p>I  am getting error too many indices for tensor of dimension 1. Can anyone please help how it can be rectified?</p>
<p>The function extract_triplets is a function to parse the generated text and extract the triplets</p>
<pre><code>from transformers import pipeline

triplet_extractor = pipeline('text2text-generation', model='Babelscape/rebel-large', tokenizer='Babelscape/rebel-large')
# We need to use the tokenizer manually since we need special tokens.
extracted_text = triplet_extractor.tokenizer.batch_decode(triplet_extractor(&quot;Punta Cana is a resort town in the municipality of Higuey, in La Altagracia Province, the eastern most province of the Dominican Republic&quot;, return_tensors=True, return_text=False)[0][&quot;generated_token_ids&quot;][&quot;output_ids&quot;])
print(extracted_text[0])
# Function to parse the generated text and extract the triplets
def extract_triplets(text):
    triplets = []
    relation, subject, relation, object_ = '', '', '', ''
    text = text.strip()
    current = 'x'
    for token in text.replace(&quot;&lt;s&gt;&quot;, &quot;&quot;).replace(&quot;&lt;pad&gt;&quot;, &quot;&quot;).replace(&quot;&lt;/s&gt;&quot;, &quot;&quot;).split():
        if token == &quot;&lt;triplet&gt;&quot;:
            current = 't'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
                relation = ''
            subject = ''
        elif token == &quot;&lt;subj&gt;&quot;:
            current = 's'
            if relation != '':
                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
            object_ = ''
        elif token == &quot;&lt;obj&gt;&quot;:
            current = 'o'
            relation = ''
        else:
            if current == 't':
                subject += ' ' + token
            elif current == 's':
                object_ += ' ' + token
            elif current == 'o':
                relation += ' ' + token
    if subject != '' and relation != '' and object_ != '':
        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})
    return triplets
extracted_triplets = extract_triplets(extracted_text[0])
print(extracted_triplets)
</code></pre>
","transformer-model"
"72313812","Tried to export a function which references 'untracked' resource Tensor(""272554:0"", shape=(), dtype=resource)","2022-05-20 05:07:08","","1","432","<python><machine-learning><deep-learning><transformer-model><transfer-learning>","<p>I'm currently using CoAtNet0 for this project, and I can't seem to save the model. Hope someone can guide me how to fix the error or is there another way to save the model? The error for the code is:</p>
<blockquote>
<p>AssertionError: Tried to export a function which references
'untracked' resource Tensor(&quot;272554:0&quot;, shape=(), dtype=resource).
TensorFlow objects (e.g. tf.Variable) captured by functions must be
'tracked' by assigning them to an attribute of a tracked object or
assigned to an attribute of the main object directly.</p>
</blockquote>
<p>Here's the code for the model.</p>
<pre><code># CoAtNet
class MBConv(tf.keras.layers.Layer):
def __init__(self, filters, kernel_size, strides = 1, expand_ratio = 1, se_ratio = 4, residual = True, momentum = 0.9, epsilon = 0.01, convolution = tf.keras.layers.Conv2D, activation = tf.nn.swish, kernel_initializer = &quot;he_normal&quot;, **kwargs):
    super(MBConv, self).__init__(**kwargs)
    self.filters = filters
    self.kernel_size = kernel_size
    self.strides = strides
    self.expand_ratio = expand_ratio
    self.se_ratio = se_ratio
    self.residual = residual
    self.momentum = momentum
    self.epsilon = epsilon
    self.convolution = convolution
    self.activation = activation
    self.kernel_initializer = kernel_initializer
    self.model_layer = layers.LayerNormalization()

def build(self, input_shape):
    self.layers = []
    self.post = []
    if self.expand_ratio != 1:
        conv = self.convolution(input_shape[-1] * self.expand_ratio, 1, use_bias = False, kernel_initializer = self.kernel_initializer)
        norm = tf.keras.layers.BatchNormalization(momentum = self.momentum, epsilon = self.epsilon)
        act = tf.keras.layers.Activation(self.activation)
        input_shape = input_shape[:-1] + (input_shape[-1] * self.expand_ratio,)
        self.layers += [conv, norm, act]
    
    #Depthwise Convolution
    conv = self.convolution(input_shape[-1], self.kernel_size, strides = self.strides, groups = input_shape[-1], padding = &quot;same&quot;, use_bias = False, kernel_initializer = self.kernel_initializer)
    norm = tf.keras.layers.BatchNormalization(momentum = self.momentum, epsilon = self.epsilon)
    act = tf.keras.layers.Activation(self.activation)
    self.layers += [conv, norm, act]
    
    #Squeeze and Excitation layer, if desired
    axis = list(range(1, len(input_shape) - 1))
    gap = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis = axis, keepdims = True))
    squeeze = self.convolution(max(1, int(input_shape[-1] / self.se_ratio)), 1, use_bias = True, kernel_initializer = self.kernel_initializer)
    act = tf.keras.layers.Activation(self.activation)
    excitation = self.convolution(input_shape[-1], 1, use_bias = True, kernel_initializer = self.kernel_initializer)
    se = lambda x: x * tf.nn.sigmoid(excitation(act(squeeze(gap(x)))))
    self.layers += [se]
    
    #Output Phase
    conv = self.convolution(self.filters, 1, use_bias = False, kernel_initializer = self.kernel_initializer)
    norm = tf.keras.layers.BatchNormalization(momentum = self.momentum, epsilon = self.epsilon)
    self.layers += [conv, norm]
    
    #Residual
    if self.residual:
        if 1 &lt; self.strides:
            pool = tf.keras.layers.MaxPool2D(pool_size = self.strides + 1, strides = self.strides, padding = &quot;same&quot;)
            self.post.append(pool)
        if input_shape[-1] != self.filters:
            resample = self.convolution(self.filters, 1, use_bias = False, kernel_initializer = self.kernel_initializer)
            self.post.append(resample)
    
def call(self, x):
    out = x
    for layer in self.layers:
        out = layer(out)
        
    if self.residual:
        for layer in self.post:
            x = layer(x)
        out = out + x
    return out
    
def get_config(self):
    config = super(MBConv, self).get_config()
    config[&quot;filters&quot;] = self.filters
    config[&quot;kernel_size&quot;] = self.kernel_size
    config[&quot;expand_ratio&quot;] = self.expand_ratio
    config[&quot;se_ratio&quot;] = self.se_ratio
    config[&quot;residual&quot;] = self.residual
    config[&quot;momentum&quot;] = self.momentum
    config[&quot;epsilon&quot;] = self.epsilon
    config[&quot;convolution&quot;] = self.convolution
    config[&quot;activation&quot;] = self.activation
    config[&quot;kernel_initializer&quot;] = self.kernel_initializer
    return config

class MultiHeadSelfAttention(tf.keras.layers.Layer):
def __init__(self, emb_dim = 768, n_head = 12, out_dim = None, relative_window_size = None, dropout_rate = 0., kernel_initializer = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.01), **kwargs):
    #ScaledDotProductAttention
    super(MultiHeadSelfAttention, self).__init__(**kwargs)
    self.emb_dim = emb_dim
    self.n_head = n_head
    if emb_dim % n_head != 0:
        raise ValueError(&quot;Shoud be embedding dimension % number of heads = 0.&quot;)
    if out_dim is None:
        out_dim = self.emb_dim
    self.out_dim = out_dim
    if relative_window_size is not None and np.ndim(relative_window_size) == 0:
        relative_window_size = [relative_window_size, relative_window_size]
    self.relative_window_size = relative_window_size
    self.projection_dim = emb_dim // n_head
    self.dropout_rate = dropout_rate
    self.query = tf.keras.layers.Dense(emb_dim, kernel_initializer = kernel_initializer)
    self.key = tf.keras.layers.Dense(emb_dim, kernel_initializer = kernel_initializer)
    self.value = tf.keras.layers.Dense(emb_dim, kernel_initializer = kernel_initializer)
    self.combine = tf.keras.layers.Dense(out_dim, kernel_initializer = kernel_initializer)
    
def build(self, input_shape):
    if self.relative_window_size is not None:
        self.relative_position_bias_table = self.add_weight(&quot;relative_position_bias_table&quot;, shape = [((2 * self.relative_window_size[0]) - 1) * ((2 * self.relative_window_size[1]) - 1), self.n_head], trainable = self.trainable)
        coords_h = np.arange(self.relative_window_size[0])
        coords_w = np.arange(self.relative_window_size[1])
        coords = np.stack(np.meshgrid(coords_h, coords_w, indexing = &quot;ij&quot;)) #2, Wh, Ww
        coords = np.reshape(coords, [2, -1])
        relative_coords = np.expand_dims(coords, axis = -1) - np.expand_dims(coords, axis = -2) #2, Wh * Ww, Wh * Ww
        relative_coords = np.transpose(relative_coords, [1, 2, 0]) #Wh * Ww, Wh * Ww, 2
        relative_coords[:, :, 0] += self.relative_window_size[0] - 1 #shift to start from 0
        relative_coords[:, :, 1] += self.relative_window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.relative_window_size[1] - 1
        relative_position_index = np.sum(relative_coords, -1)
        self.relative_position_index = tf.Variable(tf.convert_to_tensor(relative_position_index), trainable = False, name= &quot;relative_position_index&quot;)
    
def attention(self, query, key, value, relative_position_bias = None):
    score = tf.matmul(query, key, transpose_b = True)
    n_key = tf.cast(tf.shape(key)[-1], tf.float32)
    scaled_score = score / tf.math.sqrt(n_key)
    if relative_position_bias is not None:
        scaled_score = scaled_score + relative_position_bias
    weight = tf.nn.softmax(scaled_score, axis = -1)
    if 0 &lt; self.dropout_rate:
        weight = tf.nn.dropout(weight, self.dropout_rate)
    out = tf.matmul(weight, value)
    return out

def separate_head(self, x):
    out = tf.keras.layers.Reshape([-1, self.n_head, self.projection_dim])(x)
    out = tf.keras.layers.Permute([2, 1, 3])(out)
    return out

def call(self, inputs):
    query = self.query(inputs)
    key = self.key(inputs)
    value = self.value(inputs)
    
    query = self.separate_head(query)
    key = self.separate_head(key)
    value = self.separate_head(value)
    
    relative_position_bias = None
    if self.relative_window_size is not None:
        relative_position_bias = tf.gather(self.relative_position_bias_table, tf.reshape(self.relative_position_index, [-1]))
        relative_position_bias = tf.reshape(relative_position_bias, [self.relative_window_size[0] * self.relative_window_size[1], self.relative_window_size[0] * self.relative_window_size[1], -1]) #Wh * Ww,Wh * Ww, nH
        relative_position_bias = tf.transpose(relative_position_bias, [2, 0, 1]) #nH, Wh * Ww, Wh * Ww
        relative_position_bias = tf.expand_dims(relative_position_bias, axis = 0)
    attention = self.attention(query, key, value, relative_position_bias)
    attention = tf.keras.layers.Permute([2, 1, 3])(attention)
    attention = tf.keras.layers.Reshape([-1, self.emb_dim])(attention)
    
    out = self.combine(attention)
    return out
    
def get_config(self):
    config = super(MultiHeadSelfAttention, self).get_config()
    config[&quot;emb_dim&quot;] = self.emb_dim
    config[&quot;n_head&quot;] = self.n_head
    config[&quot;out_dim&quot;] = self.out_dim
    config[&quot;relative_window_size&quot;] = self.relative_window_size
    config[&quot;projection_dim&quot;] = self.projection_dim
    config[&quot;dropout_rate&quot;] = self.dropout_rate
    return config

class ConvTransformer(tf.keras.layers.Layer):
def __init__(self, emb_dim = 768, n_head = 12, strides = 1, out_dim = None, epsilon = 1e-5, dropout_rate = 0., activation = tf.keras.activations.gelu, kernel_initializer = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.01), **kwargs):
    super(ConvTransformer, self).__init__(**kwargs)
    self.emb_dim = emb_dim
    self.n_head = n_head
    self.strides = strides
    self.out_dim = out_dim if out_dim is not None else emb_dim
    self.epsilon = epsilon
    self.dropout_rate = dropout_rate
    self.activation = activation
    self.kernel_initializer = kernel_initializer
    
def build(self, input_shape):
    self.attention = []
    self.residual = []
    
    #Attention
    shape = input_shape[1:3]
    if 1 &lt; self.strides:
        shape = np.divide(np.add(shape, (self.strides - 1)), self.strides).astype(int)
        pool = tf.keras.layers.MaxPool2D(pool_size = self.strides + 1, strides = self.strides, padding = &quot;same&quot;)
        self.attention.append(pool)
        self.residual.append(pool)
    if input_shape[-1] != self.out_dim:
        resample = tf.keras.layers.Conv2D(self.out_dim, 1, padding = &quot;same&quot;, use_bias = False, kernel_initializer = &quot;he_normal&quot;)
        self.residual.append(resample)
    pre_reshape = tf.keras.layers.Reshape([-1, input_shape[-1]])
    mhsa = MultiHeadSelfAttention(emb_dim = self.emb_dim, n_head = self.n_head, out_dim = self.out_dim, relative_window_size = shape, dropout_rate = self.dropout_rate)
    post_reshape = tf.keras.layers.Reshape([*shape, self.out_dim])
    self.attention += [pre_reshape, mhsa, post_reshape]
    
    self.ffn = []
    #Feed Forward Network
    norm = tf.keras.layers.LayerNormalization(epsilon = self.epsilon)
    dense1 = tf.keras.layers.Dense(self.out_dim, kernel_initializer = self.kernel_initializer)
    act = tf.keras.layers.Activation(self.activation)
    dense2 = tf.keras.layers.Dense(self.out_dim, kernel_initializer = self.kernel_initializer)
    self.ffn = [norm, dense1, act, dense2]

def call(self, inputs):
    out = inputs
    for layer in self.attention:
        out = layer(out)
    for layer in self.residual:
        inputs = layer(inputs)
    out = out + inputs
    
    for layer in self.ffn:
        out = layer(out)
    return out
    
def get_config(self):
    config = super(ConvTransformer, self).get_config()
    config[&quot;emb_dim&quot;] = self.emb_dim
    config[&quot;n_head&quot;] = self.n_head
    config[&quot;strides&quot;] = self.strides
    config[&quot;out_dim&quot;] = self.out_dim
    config[&quot;epsilon&quot;] = self.epsilon
    config[&quot;dropout_rate&quot;] = self.dropout_rate
    config[&quot;activation&quot;] = self.activation
    config[&quot;kernel_initializer&quot;] = self.kernel_initializer
    return config

def coatnet(x, n_class = 1000, include_top = True, n_depth = [2, 2, 6, 14, 2], n_feature = [64, 96, 192, 384, 768], block = [&quot;C&quot;, &quot;M&quot;, &quot;M&quot;, &quot;T&quot;, &quot;T&quot;], stage_stride_size = 2, expand_ratio = 4, se_ratio = 4, dropout_rate = 0., activation = tf.keras.activations.gelu, name = &quot;&quot;):
#block : S &gt; Stem, C &gt; MBConv, T &gt; Transformer
if 0 &lt; len(name):
    name += &quot;_&quot;
if isinstance(stage_stride_size, int):
    stage_stride_size = [stage_stride_size] * len(block)
    
out = x
for i, (_n_depth, _n_feature, _block, _stage_stride_size) in enumerate(zip(n_depth, n_feature, block, stage_stride_size)):
    for j in range(_n_depth):
        stride_size = 1 if j != 0 else _stage_stride_size
        residual = out
        if _block.upper() == &quot;C&quot;:# i == 0:
            out = tf.keras.layers.Conv2D(_n_feature, 1 if i != 0 else 3, strides = stride_size, padding = &quot;same&quot;, use_bias = False, kernel_initializer = &quot;he_normal&quot;, name = &quot;{0}stage{1}_conv{2}&quot;.format(name, i, j + 1))(out)
            out = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5, name = &quot;{0}stage{1}_norm{2}&quot;.format(name, i, j + 1))(out)
            out = tf.keras.layers.Activation(activation, name = &quot;{0}stage{1}_act{2}&quot;.format(name, i, j + 1))(out)
        elif _block.upper() == &quot;M&quot;:
            out = tf.keras.layers.BatchNormalization(momentum = 0.9, epsilon = 1e-5, name = &quot;{0}stage{1}_pre_norm{2}&quot;.format(name, i, j + 1))(out)
            out = MBConv(_n_feature, 3, strides = stride_size, expand_ratio = expand_ratio, se_ratio = se_ratio, residual = True, momentum = 0.9, epsilon = 1e-5, activation = activation, name = &quot;{0}stage{1}_mbconv{2}&quot;.format(name, i, j + 1))(out)
        elif _block.upper() == &quot;T&quot;:
            out = tf.keras.layers.LayerNormalization(epsilon = 1e-5, name = &quot;{0}stage{1}_pre_norm{2}&quot;.format(name, i, j + 1))(out)
            out = ConvTransformer(32 * 8, 8, strides = stride_size, out_dim = _n_feature, epsilon = 1e-5, activation = activation, name = &quot;{0}stage{1}_transformer{2}&quot;.format(name, i, j + 1))(out)

if include_top:
    out = tf.keras.layers.GlobalAveragePooling2D(name = &quot;{0}gap&quot;.format(name))(out)
    if 0 &lt; dropout_rate:
        out = tf.keras.layers.Dropout(dropout_rate, name = &quot;{0}dropout&quot;.format(name))(out)
    out = tf.keras.layers.Dense(n_class, kernel_initializer = tf.keras.initializers.RandomNormal(mean = 0, stddev = 0.01), name = &quot;{0}logits&quot;.format(name))(out)
return out

def coatnet0(input_tensor = None, input_shape = None, classes = 1000, include_top = True, weights = None):
    if input_tensor is None:
        img_input = tf.keras.layers.Input(shape = input_shape)
    else:
        if not tf.keras.backend.is_keras_tensor(input_tensor):
            img_input = tf.keras.layers.Input(tensor = input_tensor, shape = input_shape)
        else:
            img_input = input_tensor

out = coatnet(img_input, classes, include_top, n_depth = [2, 2, 3, 5, 2], n_feature = [64, 96, 192, 384, 768], block = [&quot;C&quot;, &quot;M&quot;, &quot;M&quot;, &quot;T&quot;, &quot;T&quot;], stage_stride_size = 2, expand_ratio = 4, se_ratio = 4, dropout_rate = 0., activation = tf.keras.activations.gelu)
model = tf.keras.Model(img_input, out)

if weights is not None:
    model.load_weights(weights)
return model

def get_model():
model = coatnet0(input_shape = (224, 224, 3), include_top = False)

for layer in model.layers[:-1]:
    layer.trainable = False

#adding layers


x = tf.keras.layers.Flatten()(model.output)
#x = tf.keras.layers.BatchNormalization()(x)
#x = tf.keras.layers.Dense(500, activation = tf.keras.activations.gelu)(x)
x = tf.keras.layers.Dense(500, activation = tf.keras.activations.gelu, kernel_initializer=tf.keras.initializers.VarianceScaling()`)(x)`

    #x = tf.keras.layers.Dropout(0.2)(x)
    #x = tf.keras.layers.Dense(500, activation = tf.keras.activations.gelu)(x)
    x = tf.keras.layers.Dense(500, activation = tf.keras.activations.gelu, kernel_initializer=tf.keras.initializers.VarianceScaling()
)(x)
    prediction = tf.keras.layers.Dense(2, activation = 'softmax', kernel_initializer=tf.keras.initializers.VarianceScaling()
)(x)
    model = tf.keras.Model(model.input, prediction)

model.summary()

loss = tf.keras.losses.binary_crossentropy
opt = tf.keras.optimizers.Adam(learning_rate=0.00001)

metric = ['accuracy']
#weights = compute_class_weight(class_weight = &quot;balanced&quot;, classes = np.unique(train_batches.classes), y = train_batches.classes)
#cw = dict(zip(np.unique(train_batches.classes), weights))


callbacks = [
    #tf.keras.callbacks.ModelCheckpoint(&quot;covid_classifier_model.h1&quot;, save_best_only=True, verbose = 0),
    tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss', mode = &quot;auto&quot;, verbose=1),
    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='auto')
    
]

model.compile(optimizer = opt, loss = loss,
              metrics=metric)
return model

model.save(&quot;my_model&quot;)
</code></pre>
","transformer-model"
"72303447","Loss Not Decreasing for a Bert from Scratch PyTorch Model","2022-05-19 11:06:47","","1","478","<pytorch><bert-language-model><transformer-model>","<p>I followed Aladdin Persson's <a href=""https://www.youtube.com/watch?v=U0s0f995w14"" rel=""nofollow noreferrer"">Youtube video</a> to code up just the encoder portion of the transformer model in PyTorch, except I just used the Pytorch's multi-head attention layer. The model seems to produce the correct shape of data. However, during training, the training loss does not drop and the resulting model always predicts the same output of 0.4761. Dataset used for training is from the <a href=""https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection"" rel=""nofollow noreferrer"">Sarcasm Detection Dataset</a> from Kaggle. Would appreciate any help you guys can give on errors that I have made.</p>
<pre><code>import pandas as pd
from transformers import BertTokenizer
import torch.nn as nn
import torch
from sklearn.model_selection import train_test_split
from torch.optim.lr_scheduler import ReduceLROnPlateau
import math

df = pd.read_json(&quot;Sarcasm_Headlines_Dataset_v2.json&quot;, lines=True)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
encoded_input = tokenizer(df['headline'].tolist(), return_tensors='pt',padding=True)

X = encoded_input['input_ids']
y = torch.tensor(df['is_sarcastic'].values).float()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)

device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
print(device)
torch.cuda.empty_cache()

class TransformerBlock(nn.Module):
    def __init__(self,embed_dim, num_heads, dropout, expansion_ratio):
        super(TransformerBlock, self).__init__()
        self.attention = nn.MultiheadAttention(embed_dim, num_heads)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.feed_forward = nn.Sequential(
            nn.Linear(embed_dim, expansion_ratio*embed_dim),
            nn.ReLU(),
            nn.Linear(expansion_ratio*embed_dim,embed_dim)
        )
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, value, key, query):
        attention, _ = self.attention(value, key, query)
        x=self.dropout(self.norm1(attention+query))
        forward = self.feed_forward(x)
        out=self.dropout(self.norm2(forward+x))
        return out

class Encoder(nn.Module):
    #the vocab size is one more than the max value in the X matrix.
    def __init__(self,vocab_size=30109,embed_dim=128,num_layers=1,num_heads=4,device=&quot;cpu&quot;,expansion_ratio=4,dropout=0.1,max_length=193):
        super(Encoder,self).__init__()
        
        self.device = device
        self.word_embedding = nn.Embedding(vocab_size,embed_dim)
        self.position_embedding = nn.Embedding(max_length,embed_dim)
        self.layers = nn.ModuleList(
            [
                TransformerBlock(embed_dim,num_heads,dropout,expansion_ratio) for _ in range(num_layers)
            ]
        )
        
        self.dropout = nn.Dropout(dropout)
        self.classifier1 = nn.Linear(embed_dim,embed_dim)
        self.classifier2 = nn.Linear(embed_dim,1)
        self.relu = nn.ReLU()
    
    def forward(self,x):
        N, seq_length = x.shape
        positions = torch.arange(0,seq_length).expand(N, seq_length).to(self.device)
        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))
        
        for layer in self.layers:
            #print(out.shape)
            out = layer(out,out,out)
        
        #Get the first output for classification
        #Pooled output from hugging face is: Last layer hidden-state of the first token of the sequence (classification token) further processed by a Linear layer and a Tanh activation function.
        #Pooled output from hugging face will be different from out[:,0,:], which is the output from the CLS token.
        out = self.relu(self.classifier1(out[:,0,:]))
        out = self.classifier2(out)
        
        return out

torch.cuda.empty_cache()
net = Encoder(device=device)
net.to(device)

batch_size = 32
num_train_samples = X_train.shape[0]
num_val_samples = X_test.shape[0]

criterion = nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(net.parameters(),lr=1e-5)
scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)

val_loss_hist=[]
loss_hist=[]
epoch = 0
min_val_loss = math.inf

print(&quot;Training Started&quot;)

patience = 0

for _ in range(100):
    
    epoch += 1
        
    net.train()
    epoch_loss = 0
    
    permutation = torch.randperm(X_train.size()[0])
    
    for i in range(0,X_train.size()[0], batch_size):
        
        indices = permutation[i:i+batch_size]
        
        features=X_train[indices].to(device)
        labels=y_train[indices].reshape(-1,1).to(device)
        
        output = net.forward(features)
        loss = criterion(output, labels)

        optimizer.zero_grad() 
        loss.backward()
        optimizer.step()
        
        epoch_loss+=loss.item()
        
    epoch_loss = epoch_loss / num_train_samples * num_val_samples
    loss_hist.append(epoch_loss)
    
    #print(&quot;Eval&quot;)
    net.eval()
    epoch_val_loss = 0
    
    permutation = torch.randperm(X_test.size()[0])
    
    for i in range(0,X_test.size()[0], batch_size):
        
        indices = permutation[i:i+batch_size]
        
        features=X_test[indices].to(device)
        labels = y_test[indices].reshape(-1,1).to(device)
        
        output = net.forward(features)
        loss = criterion(output, labels)        

        epoch_val_loss+=loss.item()
    
    val_loss_hist.append(epoch_val_loss)
    
    scheduler.step(epoch_val_loss)
    
    #if epoch % 5 == 0:
    print(&quot;Epoch: &quot; + str(epoch) + &quot; Train Loss: &quot; + format(epoch_loss, &quot;.4f&quot;) + &quot;. Val Loss: &quot; + format(epoch_val_loss, &quot;.4f&quot;) + &quot; LR: &quot; + str(optimizer.param_groups[0]['lr']))
            
    if epoch_val_loss &lt; min_val_loss:
        min_val_loss = epoch_val_loss
        torch.save(net.state_dict(), &quot;torchmodel/weights_best.pth&quot;)
        print('\033[93m'+&quot;Model Saved&quot;+'\033[0m')
        patience = 0
        
    else:
        patience += 1
    
    if (patience == 10):
        break
        
print(&quot;Training Ended&quot;)
</code></pre>
","transformer-model"
"72280030","how to resolve Transformer model DistilBert error got an unexpected keyword argument 'special_tokens_mask'","2022-05-17 20:02:50","72285941","1","1880","<python><huggingface-transformers><transformer-model>","<p>I am using</p>
<p>Apple Mac M1</p>
<p>OS: MacOS Monterey</p>
<p>Python 3.10.4</p>
<p>I am trying to implement a vector search with DistilBERT and Weaviate by following this <a href=""https://towardsdatascience.com/a-sub-50ms-neural-search-with-distilbert-and-weaviate-4857ae390154"" rel=""nofollow noreferrer"">tutorial</a></p>
<p>below is the code setup</p>
<pre><code>import nltk
import os
import random
import time
import torch
import weaviate
from transformers import AutoModel, AutoTokenizer
from nltk.tokenize import sent_tokenize

torch.set_grad_enabled(False)

# udpated to use different model if desired
MODEL_NAME = &quot;distilbert-base-uncased&quot;
model = AutoModel.from_pretrained(MODEL_NAME)
model.to('cuda') # remove if working without GPUs
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# initialize nltk (for tokenizing sentences)
nltk.download('punkt')

# initialize weaviate client for importing and searching
client = weaviate.Client(&quot;http://localhost:8080&quot;)

def get_post_filenames(limit_objects=100):
    file_names = []
    i=0
    for root, dirs, files in os.walk(&quot;./data/20news-bydate-test&quot;):
        for filename in files:
            path = os.path.join(root, filename)
            file_names += [path]
        
    random.shuffle(file_names)
    limit_objects = min(len(file_names), limit_objects)
      
    file_names = file_names[:limit_objects]

    return file_names

def read_posts(filenames=[]):
    posts = []
    for filename in filenames:
        f = open(filename, encoding=&quot;utf-8&quot;, errors='ignore')
        post = f.read()
        
        # strip the headers (the first occurrence of two newlines)
        post = post[post.find('\n\n'):]
        
        # remove posts with less than 10 words to remove some of the noise
        if len(post.split(' ')) &lt; 10:
               continue
        
        post = post.replace('\n', ' ').replace('\t', ' ').strip()
        if len(post) &gt; 1000:
            post = post[:1000]
        posts += [post]

    return posts       


def text2vec(text):
    tokens_pt = tokenizer(text, padding=True, truncation=True, max_length=500, add_special_tokens = True, return_tensors=&quot;pt&quot;)
    tokens_pt.to('cuda') # remove if working without GPUs
    outputs = model(**tokens_pt)
    return outputs[0].mean(0).mean(0).detach()

def vectorize_posts(posts=[]):
    post_vectors=[]
    before=time.time()
    for i, post in enumerate(posts):
        vec=text2vec(sent_tokenize(post))
        post_vectors += [vec]
        if i % 100 == 0 and i != 0:
            print(&quot;So far {} objects vectorized in {}s&quot;.format(i, time.time()-before))
    after=time.time()
    
    print(&quot;Vectorized {} items in {}s&quot;.format(len(posts), after-before))
    
    return post_vectors

def init_weaviate_schema():
    # a simple schema containing just a single class for our posts
    schema = {
        &quot;classes&quot;: [{
                &quot;class&quot;: &quot;Post&quot;,
                &quot;vectorizer&quot;: &quot;none&quot;, # explicitly tell Weaviate not to vectorize anything, we are providing the vectors ourselves through our BERT model
                &quot;properties&quot;: [{
                    &quot;name&quot;: &quot;content&quot;,
                    &quot;dataType&quot;: [&quot;text&quot;],
                }]
        }]
    }

    # cleanup from previous runs
    client.schema.delete_all()

    client.schema.create(schema)

def import_posts_with_vectors(posts, vectors, batchsize=256):
    batch = weaviate.ObjectsBatchRequest()

    for i, post in enumerate(posts):
        props = {
            &quot;content&quot;: post,
        }
        batch.add(props, &quot;Post&quot;, vector=vectors[i])
        
        # when either batch size is reached or we are at the last object
        if (i !=0 and i % batchsize == 0) or i == len(posts) - 1:
            # send off the batch
            client.batch.create(batch)
            
            # and reset for the next batch
            batch = weaviate.ObjectsBatchRequest() 
    

def search(query=&quot;&quot;, limit=3):
    before = time.time()
    vec = text2vec(query)
    vec_took = time.time() - before

    before = time.time()
    near_vec = {&quot;vector&quot;: vec.tolist()}
    res = client \
        .query.get(&quot;Post&quot;, [&quot;content&quot;, &quot;_additional {certainty}&quot;]) \
        .with_near_vector(near_vec) \
        .with_limit(limit) \
        .do()
    search_took = time.time() - before

    print(&quot;\nQuery \&quot;{}\&quot; with {} results took {:.3f}s ({:.3f}s to vectorize and {:.3f}s to search)&quot; \
          .format(query, limit, vec_took+search_took, vec_took, search_took))
    for post in res[&quot;data&quot;][&quot;Get&quot;][&quot;Post&quot;]:
        print(&quot;{:.4f}: {}&quot;.format(post[&quot;_additional&quot;][&quot;certainty&quot;], post[&quot;content&quot;]))
        print('---')

# run everything
init_weaviate_schema()
posts = read_posts(get_post_filenames(4000))
vectors = vectorize_posts(posts)
import_posts_with_vectors(posts, vectors)

search(&quot;the best camera lens&quot;, 1)
search(&quot;which software do i need to view jpeg files&quot;, 1)
search(&quot;windows vs mac&quot;, 1)

</code></pre>
<p>the fuction below trigger errors</p>
<pre><code>

def text2vec(text):
    # tokens_pt = tokenizer(text, padding=True, truncation=True, max_length=500, add_special_tokens = True, return_tensors=&quot;pt&quot;)
    tokens_pt = tokenizer.encode_plus(text, add_special_tokens = True,    truncation = True, padding = &quot;max_length&quot;, return_attention_mask = True, return_tensors = &quot;pt&quot;)

    tokens_pt.to('cuda') # remove if working without GPUs
    outputs = model(**tokens_pt)
    return outputs[0].mean(0).mean(0).detach()
</code></pre>
<blockquote>
<blockquote>
<p>error 1</p>
<blockquote>
<p>tokens_pt.to('cuda') # remove if working without GPUs
AttributeError: 'dict' object has no attribute 'to'</p>
</blockquote>
</blockquote>
</blockquote>
<p>when I comment out the GPU</p>
<pre><code>#tokens_pt.to('cuda')
</code></pre>
<p>and run the code. I get this error</p>
<blockquote>
<blockquote>
<p>error 2</p>
<blockquote>
<p>outputs = model(**tokens_pt)
File &quot;/opt/homebrew/Caskroom/miniforge/base/envs/py310a/lib/python3.10/site-packages/torch/nn/modules/module.py&quot;, line 1110, in _call_impl
return forward_call(*input, **kwargs)
TypeError: DistilBertModel.forward() got an unexpected keyword argument 'special_tokens_mask'</p>
</blockquote>
</blockquote>
</blockquote>
<p>what is causing this errors and how can I fix it ?</p>
","transformer-model"
"72278254","create_padding_mask in Transformer code uses encoder input sequence for creating padding mask in 2nd attention block of the decoder","2022-05-17 17:20:55","","0","1001","<tensorflow><transformer-model>","<p>I am going through the Transformer code on tensorflow.org - <a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<pre><code>def create_masks(self, inp, tar):
    # Encoder padding mask (Used in the 2nd attention block in the decoder too.)
    padding_mask = create_padding_mask(inp)

    # Used in the 1st attention block in the decoder.
    # It is used to pad and mask future tokens in the input received by
    # the decoder.
    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])
    dec_target_padding_mask = create_padding_mask(tar)
    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)

    return padding_mask, look_ahead_mask
</code></pre>
<p>Transformer class has a method called create_masks which creates padding and look ahead mask.
I understand that padding mask for encoder should take input sequence(input to the encoder) for creating padding mask. However, what I do not understand is why should the input sequence to the encoder should be used for creating padding mask for second attention block of the decoder(first line of the code). I think the padding mask for decoder should be created using the target sequence(which is fed to the decoder).</p>
<p>Please help me understand why this is done.</p>
","transformer-model"
"72270342","running temporal fusion transformer default dataset shape error","2022-05-17 08:08:27","","2","256","<tensorflow><time-series><forecasting><transformer-model>","<p>I ran default code of Temporal fusion transformer in google colab which downloaded at <a href=""https://github.com/greatwhiz/tft_tf2"" rel=""nofollow noreferrer"">github</a>.</p>
<p>After clone, when I ran the step 2, there's no way to test training.</p>
<pre><code>python3 -m script_train_fixed_params volatility outputs yes 
</code></pre>
<p>The problem is shape error in the below.</p>
<pre><code>Computing best validation loss
Computing test loss
/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  updates=self.state_updates,
Traceback (most recent call last):
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 193, in _run_module_as_main
    &quot;__main__&quot;, mod_spec)
  File &quot;/usr/lib/python3.7/runpy.py&quot;, line 85, in _run_code
    exec(code, run_globals)
  File &quot;/content/drive/MyDrive/tft_tf2/script_train_fixed_params.py&quot;, line 239, in &lt;module&gt;
    use_testing_mode=True)  # Change to false to use original default params
  File &quot;/content/drive/MyDrive/tft_tf2/script_train_fixed_params.py&quot;, line 156, in main
    targets = data_formatter.format_predictions(output_map[&quot;targets&quot;])
  File &quot;/content/drive/MyDrive/tft_tf2/data_formatters/volatility.py&quot;, line 183, in format_predictions
    output[col] = self._target_scaler.inverse_transform(predictions[col])
  File &quot;/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_data.py&quot;, line 1022, in inverse_transform
    force_all_finite=&quot;allow-nan&quot;,
  File &quot;/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py&quot;, line 773, in check_array
    &quot;if it contains a single sample.&quot;.format(array)
ValueError: Expected 2D array, got 1D array instead:
array=[-1.43120418  1.58885804  0.28558148 ... -1.50945972 -0.16713021
 -0.57365613].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>
<p>I've tried to modify code which is predict dataframe shpae of 'data_formatters/volatility.py&quot;, line 183, in format_predictions' because I guessed that's where the problem arises.), but I can't handle that.</p>
","transformer-model"
"72184014","AttributeError: 'tuple' object has no attribute 'dim'","2022-05-10 09:40:45","","0","819","<python><pytorch><time-series><transformer-model>","<blockquote>
<p>I am trying to build a transformer network by <strong>PyTorch library</strong>. The data set that I use is historical financial market data.</p>
</blockquote>
<pre><code> x_train= torch.from_numpy(x_train_tfr)
 x_test= torch.from_numpy(x_test_tfr)
 y_train_tfr = torch.from_numpy(y_train_tfr)
 y_test_tfr = torch.from_numpy(y_test_tfr)
</code></pre>
<p>After data preparation, I use the below code to split x_train and y-train into 12 chunks:</p>
<pre><code>x_train_split=torch.split(x_train_tfr,12, dim=0)
y_train_split=torch.split(y_train_tfr,12, dim=0)
</code></pre>
<blockquote>
<p>and then I use the below code to train my model:</p>
</blockquote>
<pre><code>transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)
src = x_train_split
tgt = y_train_split
out, state = transformer_model(src, tgt)
</code></pre>
<blockquote>
<p>but the result is as below :</p>
</blockquote>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-64-769f9734fa98&gt; in &lt;module&gt;()
  3 src = x_train_split
  4 tgt = y_train_split
  ----&gt; 5 out, state = transformer_model(src, tgt)

           1 frames
 /usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py in forward(self, src,tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask,memory_key_padding_mask)
   134         &quot;&quot;&quot;
   135 
   --&gt; 136         is_batched = src.dim() == 3
   137         if not self.batch_first and src.size(1) != tgt.size(1) and is_batched:
   138             raise RuntimeError(&quot;the batch number of src and tgt must be equal&quot;)

  AttributeError: 'tuple' object has no attribute 'dim'
</code></pre>
<blockquote>
<p>How could I solve this error? Do I have to do anything extra before my model trainin?</p>
</blockquote>
","transformer-model"
"72174251","IndexError: too many indices for tensor of dimension 2: When adding custom layer on HuggingFace model","2022-05-09 15:10:41","","2","629","<python><pytorch><huggingface-transformers><bert-language-model><transformer-model>","<p>I've tried to add custom layers to HuggingFace Transformer model on binary classification task. As an absolute beginner, I tried to follow this <a href=""https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd"" rel=""nofollow noreferrer"">tutorial</a></p>
<p>Here's the custom model</p>
<pre><code>class CustomModel(nn.Module):
  def __init__(self,checkpoint,num_labels): 
    super(CustomModel,self).__init__() 
    self.num_labels = num_labels 

    #Load Model with given checkpoint and extract its body
    self.model =  AutoModelForSequenceClassification.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))
    self.dropout = nn.Dropout(0.1) 
    self.classifier = nn.Linear(768,num_labels) # load and initialize weights

  def forward(self, input_ids=None, attention_mask=None,labels=None):
    #Extract outputs from the body
    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)

    #Add custom layers
    sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state

    logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses
    
    loss = None
    if labels is not None:
      loss_fct = nn.CrossEntropyLoss()
      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
    
    return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)

</code></pre>
<p>Unfortunately, it generated an error:</p>
<pre><code>&lt;ipython-input-32-5d5e07952b71&gt; in forward(self, input_ids, attention_mask, labels)
     19     sequence_output = self.dropout(outputs[0]) #outputs[0]=last hidden state
     20 
---&gt; 21     logits = self.classifier(sequence_output[:,0,:].view(-1,768)) # calculate losses
     22 
     23     loss = None

IndexError: too many indices for tensor of dimension 2
</code></pre>
<p>Also, you can find all of the code <a href=""https://colab.research.google.com/drive/1GJBR5lFGO3xb9lERHcISohtXZZ067imU?usp=sharing"" rel=""nofollow noreferrer"">here</a>.</p>
","transformer-model"
"72139255","M2M100Tokenizer.from_pretrained 'NoneType' object is not callable","2022-05-06 09:33:44","","0","1264","<huggingface-transformers><nonetype><transformer-model><huggingface-tokenizers>","<p>I have the following chunk of code from this <a href=""https://huggingface.co/facebook/m2m100_418M"" rel=""nofollow noreferrer"">link</a>:</p>
<pre><code>from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer

hi_text = &quot;जीवन एक चॉकलेट बॉक्स की तरह है।&quot;
chinese_text = &quot;生活就像一盒巧克力。&quot;

model = M2M100ForConditionalGeneration.from_pretrained(&quot;facebook/m2m100_418M&quot;)
tokenizer = M2M100Tokenizer.from_pretrained(&quot;facebook/m2m100_418M&quot;)
</code></pre>
<p>Which gives me the following error:</p>
<pre><code>TypeError                                 Traceback (most recent call last)

&lt;ipython-input-13-c56f34229c4a&gt; in &lt;module&gt;()
      5 
      6 model = M2M100ForConditionalGeneration.from_pretrained(&quot;facebook/m2m100_418M&quot;)
----&gt; 7 tokenizer = M2M100Tokenizer.from_pretrained(&quot;facebook/m2m100_418M&quot;)

TypeError: 'NoneType' object is not callable
</code></pre>
<p>I'm using Google Colab, but funnily enough it works perfectly fine in VSCode.</p>
","transformer-model"
"72135234","why Vision Transformers key and query linar layer do not combine into one matrix","2022-05-06 01:31:43","","0","381","<python><deep-learning><pytorch><conv-neural-network><transformer-model>","<p>I study some vision transformers code (e.g. vit-pytorch)
and found in attention module:</p>
<pre><code>#x is input
key=nn.Linear(...,bias=False)(x)
query=nn.Linear(...,bias=False)(x)
similar_matrix=torch.matmul(query,key.transpose(...))
</code></pre>
<p>because Linear can be considered as a matrix, I get</p>
<pre><code>key=K^T @ x
query=Q^T @ x
similar_matrix = query^T @ key = x^T @ (Q @ K^T) @ x
(K,Q means learnable matrix, @ means matmul, ^T means transpose)
</code></pre>
<p>here Q @ K^T , I think they can be combined into a matrix in order to reduce the amount of parameters and calculation</p>
<p>why not do this? is it because the training effect is not good?</p>
","transformer-model"
"72119452","Bert tokenizer wont work with tensor format (tensorflow)","2022-05-04 21:03:16","","0","651","<tensorflow><transformer-model>","<p>This may be a silly question but im new using tf. I have the following code but the tokenizer wont use the strings inside the tensor.</p>
<pre><code>import tensorflow as tf

docs = tf.data.Dataset.from_tensor_slices([['hagamos que esto funcione.'], [&quot;por fin funciona!&quot;]])

from transformers import AutoTokenizer, DataCollatorWithPadding
import numpy as np

checkpoint = &quot;dccuchile/bert-base-spanish-wwm-uncased&quot;
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize(review):
    return tokenizer(review)


tokens = docs.map(tokenize)
</code></pre>
<p>I get the folowing output:</p>
<pre><code>ValueError: in user code:

    File &quot;&lt;ipython-input-54-3272cedfdcab&gt;&quot;, line 13, in tokenize  *
        return tokenizer(review)
    File &quot;/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py&quot;, line 2429, in __call__  *
        raise ValueError(

    ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).
</code></pre>
<p>while my expected output is something like this:</p>
<pre><code>tokenizer('esto al fin funciona!')

{'input_ids': [4, 1202, 1074, 1346, 4971, 1109, 5], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}
</code></pre>
<p>Any idea how to make it work?</p>
","transformer-model"
"72108772","HugginFace dataset error: RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same or","2022-05-04 06:30:30","","1","504","<nlp><pytorch><huggingface-transformers><transformer-model>","<p>I have taken code from many sources regarding Common Voice dataset. The only modifications I did was to change the language from Turkish to Persian.
I try to run the codes. However, I encounter this error when the line trainer.train() runs:</p>
<pre><code>RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor
</code></pre>
<p>This is my code. You can copy paste it in Google Colab and run it (where I have run):</p>
<pre><code>!pip install datasets==1.13.3
!pip install transformers==4.11.3
!pip install huggingface_hub==0.0.19
!pip install torchaudio
!pip install librosa
!pip install jiwer
!apt install git-lfs
!pip install hazm
!pip install pydub
!pip install pythainlp
import os
import re
#from typing import List, Dict, Tuple
import pandas as pd
from scipy.io import wavfile
from pythainlp.tokenize import word_tokenize
#from spell_correction import correct_sentence
import matplotlib.pyplot as plt
import numpy as np
from tqdm.auto import tqdm
from pydub import AudioSegment
from pythainlp.tokenize import word_tokenize, syllable_tokenize
from datasets import load_dataset, load_from_disk, load_metric
import hazm
import string
import torch
import os
#os.environ['CUDA_VISIBLE_DEVICES']='2, 3'

torch.cuda.empty_cache()

#print(torch.cuda.memory_summary(device=None, abbreviated=False))

print(torch.cuda.is_available())




_normalizer = hazm.Normalizer()

chars_to_ignore = [
    &quot;,&quot;, &quot;?&quot;, &quot;.&quot;, &quot;!&quot;, &quot;-&quot;, &quot;;&quot;, &quot;:&quot;, '&quot;&quot;', &quot;%&quot;, &quot;'&quot;, '&quot;', &quot;�&quot;,
    &quot;#&quot;, &quot;!&quot;, &quot;؟&quot;, &quot;?&quot;, &quot;«&quot;, &quot;»&quot;, &quot;،&quot;, &quot;(&quot;, &quot;)&quot;, &quot;؛&quot;, &quot;'ٔ&quot;, &quot;٬&quot;,'ٔ', &quot;,&quot;, &quot;?&quot;, 
    &quot;.&quot;, &quot;!&quot;, &quot;-&quot;, &quot;;&quot;, &quot;:&quot;,'&quot;',&quot;“&quot;, &quot;%&quot;, &quot;‘&quot;, &quot;”&quot;, &quot;�&quot;, &quot;–&quot;, &quot;…&quot;, &quot;_&quot;, &quot;”&quot;, '“', '„',
    'ā', 'š',
#     &quot;ء&quot;,
]

# In case of farsi
chars_to_ignore = chars_to_ignore + list(string.ascii_lowercase + string.digits)

chars_to_mapping = {
    'ك': 'ک', 'دِ': 'د', 'بِ': 'ب', 'زِ': 'ز', 'ذِ': 'ذ', 'شِ': 'ش', 'سِ': 'س', 'ى': 'ی',
    'ي': 'ی', 'أ': 'ا', 'ؤ': 'و', &quot;ے&quot;: &quot;ی&quot;, &quot;ۀ&quot;: &quot;ه&quot;, &quot;ﭘ&quot;: &quot;پ&quot;, &quot;ﮐ&quot;: &quot;ک&quot;, &quot;ﯽ&quot;: &quot;ی&quot;,
    &quot;ﺎ&quot;: &quot;ا&quot;, &quot;ﺑ&quot;: &quot;ب&quot;, &quot;ﺘ&quot;: &quot;ت&quot;, &quot;ﺧ&quot;: &quot;خ&quot;, &quot;ﺩ&quot;: &quot;د&quot;, &quot;ﺱ&quot;: &quot;س&quot;, &quot;ﻀ&quot;: &quot;ض&quot;, &quot;ﻌ&quot;: &quot;ع&quot;,
    &quot;ﻟ&quot;: &quot;ل&quot;, &quot;ﻡ&quot;: &quot;م&quot;, &quot;ﻢ&quot;: &quot;م&quot;, &quot;ﻪ&quot;: &quot;ه&quot;, &quot;ﻮ&quot;: &quot;و&quot;, 'ﺍ': &quot;ا&quot;, 'ة': &quot;ه&quot;,
    'ﯾ': &quot;ی&quot;, 'ﯿ': &quot;ی&quot;, 'ﺒ': &quot;ب&quot;, 'ﺖ': &quot;ت&quot;, 'ﺪ': &quot;د&quot;, 'ﺮ': &quot;ر&quot;, 'ﺴ': &quot;س&quot;, 'ﺷ': &quot;ش&quot;,
    'ﺸ': &quot;ش&quot;, 'ﻋ': &quot;ع&quot;, 'ﻤ': &quot;م&quot;, 'ﻥ': &quot;ن&quot;, 'ﻧ': &quot;ن&quot;, 'ﻭ': &quot;و&quot;, 'ﺭ': &quot;ر&quot;, &quot;ﮔ&quot;: &quot;گ&quot;,
        
    # &quot;ها&quot;: &quot;  ها&quot;, &quot;ئ&quot;: &quot;ی&quot;,
    &quot;۱۴ام&quot;: &quot;۱۴ ام&quot;,
        
    &quot;a&quot;: &quot; ای &quot;, &quot;b&quot;: &quot; بی &quot;, &quot;c&quot;: &quot; سی &quot;, &quot;d&quot;: &quot; دی &quot;, &quot;e&quot;: &quot; ایی &quot;, &quot;f&quot;: &quot; اف &quot;,
    &quot;g&quot;: &quot; جی &quot;, &quot;h&quot;: &quot; اچ &quot;, &quot;i&quot;: &quot; آی &quot;, &quot;j&quot;: &quot; جی &quot;, &quot;k&quot;: &quot; کی &quot;, &quot;l&quot;: &quot; ال &quot;,
    &quot;m&quot;: &quot; ام &quot;, &quot;n&quot;: &quot; ان &quot;, &quot;o&quot;: &quot; او &quot;, &quot;p&quot;: &quot; پی &quot;, &quot;q&quot;: &quot; کیو &quot;, &quot;r&quot;: &quot; آر &quot;,
    &quot;s&quot;: &quot; اس &quot;, &quot;t&quot;: &quot; تی &quot;, &quot;u&quot;: &quot; یو &quot;, &quot;v&quot;: &quot; وی &quot;, &quot;w&quot;: &quot; دبلیو &quot;, &quot;x&quot;: &quot; اکس &quot;,
    &quot;y&quot;: &quot; وای &quot;, &quot;z&quot;: &quot; زد &quot;,
    &quot;\u200c&quot;: &quot; &quot;, &quot;\u200d&quot;: &quot; &quot;, &quot;\u200e&quot;: &quot; &quot;, &quot;\u200f&quot;: &quot; &quot;, &quot;\ufeff&quot;: &quot; &quot;,
}


def multiple_replace(text, chars_to_mapping):
    pattern = &quot;|&quot;.join(map(re.escape, chars_to_mapping.keys()))
    return re.sub(pattern, lambda m: chars_to_mapping[m.group()], str(text))

def remove_special_characters(text, chars_to_ignore_regex):
    text = re.sub(chars_to_ignore_regex, '', text).lower() + &quot; &quot;
    return text

def normalizer(text, chars_to_ignore=chars_to_ignore, chars_to_mapping=chars_to_mapping):
    chars_to_ignore_regex = f&quot;&quot;&quot;[{&quot;&quot;.join(chars_to_ignore)}]&quot;&quot;&quot;
    text = text.lower().strip()

    text = _normalizer.normalize(text)
    text = multiple_replace(text, chars_to_mapping)
    text = remove_special_characters(text, chars_to_ignore_regex)
    text = re.sub(&quot; +&quot;, &quot; &quot;, text)
    _text = []
    for word in text.split():
        try:
            word = int(word)
            _text.append(words(word))
        except:
            _text.append(word)
            
    text = &quot; &quot;.join(_text) + &quot; &quot;
    text = text.strip()

    if not len(text) &gt; 0:
        return None
    
    return text + &quot; &quot;

data_dir = &quot;cv-corpus-9.0-2022-04-27/fa&quot;

from datasets import load_dataset, load_metric, Audio

common_voice_train = load_dataset(&quot;common_voice&quot;, &quot;fa&quot;, split=&quot;train&quot;)
common_voice_train = common_voice_train.select(range(500))
common_voice_dev = load_dataset(&quot;common_voice&quot;, &quot;fa&quot;, split=&quot;validation&quot;)
common_voice_dev = common_voice_dev.select(range(50))
common_voice_test = load_dataset(&quot;common_voice&quot;, &quot;fa&quot;, split=&quot;test&quot;)
common_voice_test = common_voice_test.select(range(50))

print(common_voice_train)
print(common_voice_dev)
print(common_voice_test)

from datasets import ClassLabel
import random
import pandas as pd
#from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples &lt;= len(dataset), &quot;Can't pick more elements than there are in the dataset.&quot;
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    print(df.head())

#show_random_elements(common_voice_train.remove_columns([&quot;path&quot;]), num_examples=20)

def normalizer(batch, chars_to_ignore=chars_to_ignore, chars_to_mapping=chars_to_mapping):
    chars_to_ignore_regex = f&quot;&quot;&quot;[{&quot;&quot;.join(chars_to_ignore)}]&quot;&quot;&quot;
    text = batch[&quot;sentence&quot;].lower().strip()

    text = _normalizer.normalize(text)
    text = multiple_replace(text, chars_to_mapping)
    text = remove_special_characters(text, chars_to_ignore_regex)
    text = re.sub(&quot; +&quot;, &quot; &quot;, text)
    _text = []
    for word in text.split():
        try:
            word = int(word)
            _text.append(words(word))
        except:
            _text.append(word)
            
    text = &quot; &quot;.join(_text) + &quot; &quot;
    text = text.strip()

    if not len(text) &gt; 0:
        return None

    if len(text) &gt;= 32:
        text = text[:30]
    
    batch[&quot;sentence&quot;] = text
    
    return batch

#print(common_voice_train[0][&quot;sentence&quot;])
#print(common_voice_dev[0][&quot;sentence&quot;])
#print(common_voice_test[0][&quot;sentence&quot;])

common_voice_train = common_voice_train.map(normalizer, fn_kwargs={&quot;chars_to_ignore&quot;: chars_to_ignore, &quot;chars_to_mapping&quot;: chars_to_mapping})
common_voice_dev = common_voice_dev.map(normalizer, fn_kwargs={&quot;chars_to_ignore&quot;: chars_to_ignore, &quot;chars_to_mapping&quot;: chars_to_mapping})
common_voice_test = common_voice_test.map(normalizer, fn_kwargs={&quot;chars_to_ignore&quot;: chars_to_ignore, &quot;chars_to_mapping&quot;: chars_to_mapping})

#print(common_voice_train[0][&quot;sentence&quot;])
#print(common_voice_dev[0][&quot;sentence&quot;])
#print(common_voice_test[0][&quot;sentence&quot;])

def extract_all_chars(batch):
    all_text = &quot; &quot;.join(batch[&quot;sentence&quot;])
    vocab = list(set(all_text))
    return {&quot;vocab&quot;: [vocab], &quot;all_text&quot;: [all_text]}

vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=4, keep_in_memory=True, remove_columns=common_voice_train.column_names)
vocab_dev = common_voice_dev.map(extract_all_chars, batched=True, batch_size=4, keep_in_memory=True, remove_columns=common_voice_train.column_names)
vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=4, keep_in_memory=True, remove_columns=common_voice_test.column_names)

vocab_list = list(sorted(set(vocab_train[&quot;vocab&quot;][0]) | set(vocab_dev[&quot;vocab&quot;][0]) | set(vocab_test[&quot;vocab&quot;][0])))
vocab_list = [vocab for vocab in vocab_list if vocab not in [&quot; &quot;, &quot;\u0307&quot;]]
print(len(vocab_list))
print(vocab_list)

vocab_list = list(sorted(set(vocab_train[&quot;vocab&quot;][0]) | set(vocab_dev[&quot;vocab&quot;][0]) | set(vocab_test[&quot;vocab&quot;][0])))
vocab_list = [vocab for vocab in vocab_list if vocab not in [&quot; &quot;, &quot;\u0307&quot;]]
print(len(vocab_list))
print(vocab_list)

special_vocab = [&quot;&lt;pad&gt;&quot;, &quot;&lt;s&gt;&quot;, &quot;&lt;/s&gt;&quot;, &quot;&lt;unk&gt;&quot;, &quot;|&quot;]
vocab_dict = {v: k for k, v in enumerate(special_vocab + vocab_list)}
print(len(vocab_dict))
print(vocab_dict)

for name, age in vocab_dict.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)
    if age == 5:
        k1 = name
    elif age == 8:
        k2=name

del vocab_dict[k1]
del vocab_dict[k2]

import json
with open('vocab.json', 'w') as vocab_file:
    json.dump(vocab_dict, vocab_file)

from transformers.trainer_utils import get_last_checkpoint

save_dir = &quot;model checkpoints/&quot;

last_checkpoint = None
if os.path.exists(save_dir):
    last_checkpoint = get_last_checkpoint(save_dir)

print(last_checkpoint if last_checkpoint else str(None))

from transformers import Wav2Vec2CTCTokenizer

tokenizer = Wav2Vec2CTCTokenizer(
        &quot;vocab.json&quot;, 
        bos_token=&quot;&lt;s&gt;&quot;,
        eos_token=&quot;&lt;/s&gt;&quot;,
        unk_token=&quot;&lt;unk&gt;&quot;,
        pad_token=&quot;&lt;pad&gt;&quot;,
        word_delimiter_token=&quot;|&quot;,
        do_lower_case=False,
        max_length=31
    )

text = &quot;از مهمونداری کنار بکشم&quot;
print(&quot; &quot;.join(tokenizer.tokenize(text)))
print(tokenizer.decode(tokenizer.encode(text)))

from transformers import Wav2Vec2FeatureExtractor

feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)

from transformers import Wav2Vec2Processor

processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)

if len(processor.tokenizer.get_vocab()) == len(processor.tokenizer):
    print(len(processor.tokenizer))

if not os.path.exists(save_dir):
    print(&quot;Saving ...&quot;)
    processor.save_pretrained(save_dir)
    print(&quot;Saved!&quot;)

import torchaudio
import librosa


target_sampling_rate = 16_000

def speech_file_to_array_fn(batch):
    speech_array, sampling_rate = torchaudio.load(batch[&quot;path&quot;])
    speech_array = speech_array.squeeze().numpy()
    speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, target_sampling_rate)

    batch[&quot;speech&quot;] = speech_array
    batch[&quot;sampling_rate&quot;] = target_sampling_rate
    batch[&quot;duration_in_seconds&quot;] = len(batch[&quot;speech&quot;]) / target_sampling_rate
    batch[&quot;target_text&quot;] = batch[&quot;sentence&quot;]
    return batch

common_voice_train = common_voice_train.map(speech_file_to_array_fn, remove_columns=common_voice_train.column_names)
common_voice_dev = common_voice_dev.map(speech_file_to_array_fn, remove_columns=common_voice_dev.column_names)
common_voice_test = common_voice_test.map(speech_file_to_array_fn, remove_columns=common_voice_test.column_names)

#print(common_voice_train[0][&quot;sampling_rate&quot;])
#print(common_voice_test[0][&quot;sampling_rate&quot;])


min_duration_in_seconds = 5.0
max_duration_in_seconds = 10.0

def filter_by_max_duration(batch):
    return min_duration_in_seconds &lt;= batch[&quot;duration_in_seconds&quot;] &lt;= max_duration_in_seconds

print(f&quot;Split sizes [BEFORE]: {len(common_voice_train)} train and {len(common_voice_test)} validation.&quot;)



_common_voice_train = common_voice_train.filter(filter_by_max_duration)
_common_voice_dev = common_voice_dev
_common_voice_test = common_voice_test
# _common_voice_test = common_voice_test.filter(filter_by_max_duration, num_proc=4)

print(f&quot;Split sizes [AFTER]: {len(_common_voice_train)} train and {len(_common_voice_test)} validation.&quot;)

# check that all files have the correct sampling rate
def prepare_dataset(batch):
    assert (
        len(set(batch[&quot;sampling_rate&quot;])) == 1), f&quot;Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.&quot;

    batch[&quot;input_values&quot;] = processor(batch[&quot;speech&quot;], sampling_rate=batch[&quot;sampling_rate&quot;][0]).input_values

    with processor.as_target_processor():
        batch[&quot;labels&quot;] = processor(batch[&quot;target_text&quot;]).input_ids

    return batch

_common_voice_train = _common_voice_train.map(prepare_dataset, remove_columns=_common_voice_train.column_names, batch_size=4, batched=True)
_common_voice_dev = _common_voice_dev.map(prepare_dataset, remove_columns=_common_voice_dev.column_names, batch_size=4, batched=True)
_common_voice_test = _common_voice_test.map(prepare_dataset, remove_columns=_common_voice_test.column_names, batch_size=4, batched=True)

#_common_voice_train.set_format(type='torch', columns=['input_values', 'labels'])
#_common_voice_dev.set_format(type='torch', columns=['input_values', 'labels'])
#_common_voice_test.set_format(type='torch', columns=['input_values', 'labels'])


###############################################################################################################

#torch.cuda.empty_cache()

#print(torch.cuda.memory_summary(device=None, abbreviated=False))

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

@dataclass
class DataCollatorCTCWithPadding:
    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    max_length_labels: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    pad_to_multiple_of_labels: Optional[int] = None

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -&gt; Dict[str, torch.Tensor]:
        input_features = [{&quot;input_values&quot;: feature[&quot;input_values&quot;]} for feature in features]
        label_features = [{&quot;input_ids&quot;: feature[&quot;labels&quot;]} for feature in features]

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors=&quot;pt&quot;,
            )
        with self.processor.as_target_processor():
            labels_batch = self.processor.pad(
                label_features,
                padding=self.padding,
                max_length=self.max_length_labels,
                #max_length=64,
                pad_to_multiple_of=self.pad_to_multiple_of_labels,
                return_tensors=&quot;pt&quot;,
                )

        labels = labels_batch[&quot;input_ids&quot;].masked_fill(labels_batch.attention_mask.ne(1), -100)

        batch[&quot;labels&quot;] = labels

        return batch

data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)

wer_metric = load_metric(&quot;wer&quot;)

import random


def compute_metrics(pred):
    pred_logits = pred.predictions
    pred_ids = np.argmax(pred_logits, axis=-1)

    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.batch_decode(pred_ids)

    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

    if isinstance(label_str, list):
        if isinstance(pred_str, list) and len(pred_str) == len(label_str):
            for index in random.sample(range(len(label_str)), 3):
                print(f'reference: &quot;{label_str[index]}&quot;')
                print(f'predicted: &quot;{pred_str[index]}&quot;')
        else:
            for index in random.sample(range(len(label_str)), 3):
                print(f'reference: &quot;{label_str[index]}&quot;')
                print(f'predicted: &quot;{pred_str}&quot;')

    wer = wer_metric.compute(predictions=pred_str, references=label_str)

    return {&quot;wer&quot;: wer}


from transformers import Wav2Vec2ForCTC, Wav2Vec2Config

configuration = Wav2Vec2Config(hidden_size=256, num_hidden_layers=6, num_attention_heads=6, intermediate_size=1024)

model_args ={}

print('haaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa')
print(len(processor.tokenizer.get_vocab()))
print('haaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa')

model = Wav2Vec2ForCTC.from_pretrained(
    &quot;facebook/wav2vec2-large-xlsr-53&quot; if not last_checkpoint else last_checkpoint, 
    #model_name_or_path if not last_checkpoint else last_checkpoint,
    attention_dropout=0.1,
    #hidden_size=256,
    #num_hidden_layers=8,
    #num_attention_heads=2,
    #intermediate_size=256,
    hidden_dropout=0.1,
    feat_proj_dropout=0.0,
    mask_time_prob=0.05,
    layerdrop=0.1,
    gradient_checkpointing=True, 
    ctc_loss_reduction=&quot;mean&quot;, 
    ctc_zero_infinity=True,
    bos_token_id=processor.tokenizer.bos_token_id,
    eos_token_id=processor.tokenizer.eos_token_id,
    pad_token_id=processor.tokenizer.pad_token_id,
    vocab_size=len(processor.tokenizer.get_vocab())
    #vocab_size=64
)

model.config = configuration


model.freeze_feature_extractor()

model = model.to(torch.device(&quot;cuda&quot;))

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir=save_dir,
    group_by_length=True,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    evaluation_strategy=&quot;steps&quot;,
    num_train_epochs=0.5,
    fp16=True,
    #save_steps=10,
    #eval_steps=10,
    #logging_steps=10,
    learning_rate=1e-4,
    #warmup_steps=500,
    #save_total_limit=2,
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    data_collator=data_collator,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=_common_voice_train,
    eval_dataset=_common_voice_test,
    tokenizer=processor.feature_extractor,
)

torch.cuda.empty_cache()

train_result = trainer.train()


metrics = train_result.metrics
max_train_samples = len(_common_voice_train)
metrics[&quot;train_samples&quot;] = min(max_train_samples, len(_common_voice_train))

trainer.save_model()

trainer.log_metrics(&quot;train&quot;, metrics)
trainer.save_metrics(&quot;train&quot;, metrics)
trainer.save_state()
</code></pre>
<p>I’d be really thankful for anyone who can solve my problem. Please help me as this is driving me mad.
P.S.: There’s a line which tries to set the format of _common_dataset to torch tensor. However, even when I run it, i still encounter errors like I mentioned.</p>
","transformer-model"
"72081963","Approximation Softmax Kernel using Random Fourier Features & Performers","2022-05-02 02:59:35","","2","190","<math><nlp><transformer-model><kernel-density><approximation>","<p>I read the work titled '<a href=""https://arxiv.org/pdf/2009.14794.pdf"" rel=""nofollow noreferrer"">Rethinking Attention with Performers</a>'. This is a seminal contribution to handling the quadratic time complexity of self-attention used in Transformer with strong theoretical guarantees.
However, I am stuck with the following equation (equation 5 on paper) to approximate the non-linear shift-invariant kernel.</p>
<p><a href=""https://i.sstatic.net/ySLtK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ySLtK.png"" alt=""refer to Equation 5"" /></a></p>
<p>I am unable to prove the above equation, especially the deterministic function h(x) used in the equation. However, literature (cited in the paper and others) has the following form of Random Fourier Feature functions. This \phi function doesn't contain the deterministic function h(x).</p>
<p><a href=""https://i.sstatic.net/KanMw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KanMw.png"" alt=""Image "" /></a></p>
<p>Find a few references for the above equation</p>
<ul>
<li><a href=""https://people.eecs.berkeley.edu/%7Ebrecht/papers/07.rah.rec.nips.pdf"" rel=""nofollow noreferrer"">Random Features for Large-Scale Kernel Machines</a></li>
<li><a href=""https://papers.nips.cc/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf"" rel=""nofollow noreferrer"">Orthogonal Random Features</a></li>
<li><a href=""https://arxiv.org/pdf/2006.03555.pdf"" rel=""nofollow noreferrer"">Linearly Scalable Long-Context Transformers</a></li>
</ul>
<p>It seems that equation 5 is a generalization of the above equation. However, I am unable to drive equation 5 from this equation. Kindly help me out to get equation 5 of 'Rethinking Attention with Performers'.</p>
<p>There is a blog for the work titled '<a href=""https://gregorygundersen.com/blog/2019/12/23/random-fourier-features/#a1-gaussian-kernel-derivation"" rel=""nofollow noreferrer"">Random Features for Large-Scale Kernel Machines</a>' for a better understanding of low-dimensional approximation of kernel function.</p>
","transformer-model"
"72048488","How to write a generation function for text pytorch transformer?","2022-04-28 18:15:08","","3","545","<python><machine-learning><nlp><pytorch><transformer-model>","<p>Following this <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">pytorch tutorial</a>, I'm able to create and train a transformer model on a custom dataset. The problem is, I've scoured the web and have found no clear answers... How do I use this model to generate text? I took a stab at it, by encoding my SOS and seed text and passing it through the model's forward method... But this only produces repeating garbage. The src_mask doesn't appear to be the right size or functioning at all.</p>
<pre><code>def generate(model: nn.Module, src_text:str):
    src=BeatleSet.encode(src_text.lower()) # encodes seed text
    SOS=BeatleSet.textTokDict['&lt;sos&gt;'] ; EOS=BeatleSet.textTokDict['&lt;eos&gt;'] # obtain eos and sos tokens
    model.eval(); entry=[SOS]+src
    y_input=torch.tensor([entry], dtype=torch.long, device=device) # convert entry to tensor
    num_tokens=len(BeatleSet)
    for i in range(50):
        src_mask=generate_square_subsequent_mask(y_input.size(0)).to(device) #create a mask of size 1,1 (???)
        pred=model(y_input, src_mask) # passed through forward method
        next_item = pred.topk(1)[1].view(-1)[-1].item() # selecting the most probable next token (I think)
        next_item = torch.tensor([[next_item]], device=device)
        y_input=torch.cat((y_input, next_item), dim=1) # added to inputs to be run again
        if next_item.view(-1).item() == EOS:
            break
    return &quot; &quot;.join(BeatleSet.decode(y_input.view(-1).tolist()))
    
print(generate(model, &quot;Oh yeah I&quot;))
</code></pre>
<p>For the record, I'm following the architecture to the letter. This should be reproducible with the wikidata set that is used in the tutorial. Please advise, I've been banging my head on this one.</p>
","transformer-model"
"72026063","Vision Transformer attention map by keypoint location - TensorFlow","2022-04-27 09:01:36","72043571","0","417","<tensorflow><pytorch><computer-vision><transformer-model><pose-estimation>","<p>I have trained a ViT model on TensorFlow for keypoint estimation based on <a href=""https://github.com/yangsenius/TransPose"" rel=""nofollow noreferrer"">https://github.com/yangsenius/TransPose</a> and I would like to simulate the attention maps of each keypoint like this: <a href=""https://raw.githubusercontent.com/yangsenius/TransPose/main/attention_map_image_dependency_transposeh_thres_0.00075.jpg"" rel=""nofollow noreferrer"">https://raw.githubusercontent.com/yangsenius/TransPose/main/attention_map_image_dependency_transposeh_thres_0.00075.jpg</a></p>
<p>I have found the code on Pytorch but I have no idea about how to simulate it on TensorFlow:
<a href=""https://github.com/yangsenius/TransPose/blob/dab9007b6f61c9c8dce04d61669a04922bbcd148/visualize.py#L128"" rel=""nofollow noreferrer"">https://github.com/yangsenius/TransPose/blob/dab9007b6f61c9c8dce04d61669a04922bbcd148/visualize.py#L128</a></p>
","transformer-model"
"72014025","Unknown task text-classification, available tasks are ['feature-extraction', 'sentiment-analysis',","2022-04-26 12:31:51","72014229","3","3942","<python><huggingface-transformers><transformer-model>","<p>I am trying to use Transformers for the first time based on this model:</p>
<p><a href=""https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion?text=I+like+you.+I+love+you"" rel=""nofollow noreferrer"">https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion?text=I+like+you.+I+love+you</a></p>
<p>The sample code provided here its:</p>
<pre><code>from transformers import pipeline
classifier = pipeline(&quot;text-classification&quot;,model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)
prediction = classifier(&quot;I love using transformers. The best part is wide range of support and its easy to use&quot;, )
print(prediction)
</code></pre>
<p>However I get this error:</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-6-74ca6189abbe&gt; in &lt;module&gt;
      1 from transformers import pipeline
----&gt; 2 classifier = pipeline(&quot;text-classification&quot;,model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)
      3 prediction = classifier(&quot;I love using transformers. The best part is wide range of support and its easy to use&quot;, )
      4 print(prediction)

/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)
    340     &quot;&quot;&quot;
    341     # Retrieve the task
--&gt; 342     targeted_task, task_options = check_task(task)
    343 
    344     # Use default model/config/tokenizer for the task if no model is provided

/anaconda/envs/azureml_py38/lib/python3.8/site-packages/transformers/pipelines/__init__.py in check_task(task)
    234         raise KeyError(f&quot;Invalid translation task {task}, use 'translation_XX_to_YY' format&quot;)
    235 
--&gt; 236     raise KeyError(
    237         f&quot;Unknown task {task}, available tasks are {list(SUPPORTED_TASKS.keys()) + ['translation_XX_to_YY']}&quot;
    238     )

KeyError: &quot;Unknown task text-classification, available tasks are ['feature-extraction', 'sentiment-analysis', 'ner', 'question-answering', 'table-question-answering', 'fill-mask', 'summarization', 'translation', 'text2text-generation', 'text-generation', 'zero-shot-classification', 'conversational', 'translation_XX_to_YY']&quot;
</code></pre>
<p>And Yes I Installed transfomers first</p>
","transformer-model"
"71984994","While training BERT variant, getting IndexError: index out of range in self","2022-04-24 02:13:35","","0","7409","<python><pytorch><huggingface-transformers><bert-language-model><transformer-model>","<p>While training <a href=""https://huggingface.co/docs/transformers/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"" rel=""nofollow noreferrer""><code>XLMRobertaForSequenceClassification</code></a>:</p>
<pre><code>xlm_r_model(input_ids = X_train_batch_input_ids
            , attention_mask = X_train_batch_attention_mask
            , return_dict = False
           )
</code></pre>
<p>I faced following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;string&gt;&quot;, line 3, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py&quot;, line 1218, in forward
    return_dict=return_dict,
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py&quot;, line 849, in forward
    past_key_values_length=past_key_values_length,
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/roberta/modeling_roberta.py&quot;, line 132, in forward
    inputs_embeds = self.word_embeddings(input_ids)
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py&quot;, line 160, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File &quot;/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py&quot;, line 2044, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
</code></pre>
<p>Below are details:</p>
<ol>
<li><p>Creating model</p>
<pre><code>config = XLMRobertaConfig() 
config.output_hidden_states = False
xlm_r_model = XLMRobertaForSequenceClassification(config=config)
xlm_r_model.to(device) # device is device(type='cpu')
</code></pre>
</li>
<li><p>Tokenizer</p>
<pre><code>xlmr_tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')

MAX_TWEET_LEN = 402

&gt;&gt;&gt; df_1000.info() # describing a data frame I have pre populated
&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 1000 entries, 29639 to 44633
Data columns (total 2 columns):
#    Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
0    text    1000 non-null   object
1    class   1000 non-null   int64 
dtypes: int64(1), object(1)
memory usage: 55.7+ KB

X_train = xlmr_tokenizer(list(df_1000[:800].text), padding=True, max_length=MAX_TWEET_LEN+5, truncation=True) # +5: a head room for special tokens / separators
&gt;&gt;&gt; list(map(len,X_train['input_ids']))  # why its 105? shouldn't it be MAX_TWEET_LEN+5 = 407?
[105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, 105, ...]

&gt;&gt;&gt; type(train_index) # describing (for clarity) training fold indices I pre populated
&lt;class 'numpy.ndarray'&gt;

&gt;&gt;&gt; train_index.size 
640

X_train_fold_input_ids = np.array(X_train['input_ids'])[train_index]
X_train_fold_attention_mask = np.array(X_train['attention_mask'])[train_index]

&gt;&gt;&gt; i # batch id
0
&gt;&gt;&gt; batch_size
16

X_train_batch_input_ids = X_train_fold_input_ids[i:i+batch_size]
X_train_batch_input_ids = torch.tensor(X_train_batch_input_ids,dtype=torch.long).to(device)

X_train_batch_attention_mask = X_train_fold_attention_mask[i:i+batch_size]
X_train_batch_attention_mask = torch.tensor(X_train_batch_attention_mask,dtype=torch.long).to(device)

&gt;&gt;&gt; X_train_batch_input_ids.size()
torch.Size([16, 105]) # why 105? Shouldnt this be MAX_TWEET_LEN+5 = 407?

&gt;&gt;&gt; X_train_batch_attention_mask.size()
torch.Size([16, 105]) # why 105? Shouldnt this be MAX_TWEET_LEN+5 = 407?
</code></pre>
</li>
</ol>
<p>After this I make the call <code>xlm_r_model(...)</code> as stated at the beginning of this question and ending up with the specified error.</p>
<p>Noticing all these details, I am still not able to get why I am getting the specified error. Where I am doing it wrong?</p>
","transformer-model"
"71974438","Training New AutoTokenizer Hugging Face","2022-04-22 20:43:56","","2","2741","<python><nlp><data-science><huggingface-transformers><transformer-model>","<blockquote>
<p>Getting this error: AttributeError: 'GPT2Tokenizer' object has no
attribute 'train_new_from_iterator'</p>
</blockquote>
<p>Very similar to hugging face documentation. I changed the input and that's it (shouldn't affect it). It worked once. Came back to it 2 hrs later and it doesn't... nothing was changed NOTHING. Documentation states train_new_from_iterator only works with 'fast' tokenizers and that AutoTokenizer is supposed to pick a 'fast' tokenizer by default. My best guess is, it is having some trouble with this. I also tried downgrading transformers and reinstalling to no success. df is just one column of text.</p>
<pre><code>from transformers import AutoTokenizer
import tokenizers

def batch_iterator(batch_size=10, size=5000):
    for i in range(100): #2264
        query = f&quot;select note_text from cmx_uat.note where id &gt; {i * size} limit 50;&quot;
        df = pd.read_sql(sql=query, con=cmx_uat)

        for x in range(0, size, batch_size):
            yield list(df['note_text'].loc[0:5000])[x:x + batch_size]

old_tokenizer = AutoTokenizer.from_pretrained('roberta')
training_corpus = batch_iterator()
new_tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 32000)
</code></pre>
","transformer-model"
"71910237","Why do I get the error: ZeroDivisionError: 0.0 cannot be raised to a negative power error when loading deit model from Timm","2022-04-18 09:50:51","71911151","0","261","<machine-learning><pytorch><computer-vision><transformer-model>","<p>I am trying to make inference on the DeiT small variant from <a href=""https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/deit.py"" rel=""nofollow noreferrer"">timm</a>.</p>
<pre><code>from timm.models import create_model
model = create_model('deit_small_patch16_224', pretrained=True)
</code></pre>
<p>But I get the error:</p>
<pre><code>self.scale = head_dim ** -0.5
ZeroDivisionError: 0.0 cannot be raised to a negative power.
</code></pre>
<p>However, creating a different model with <code>model = create_model('deit_tiny_patch16_224', pretrained=True)</code>, I can make inference successfully and it works pretty fine. I understand this error is as a result of a zero being divided by a non-zero value or when being raised to the power of non-zero. But I don't quite get why this model is flagging an error from timm even before loading any data.</p>
","transformer-model"
"71906629","MultiHeadAttention giving very different values between versions (Pytorch/Tensorflow","2022-04-18 00:59:22","71912728","1","2324","<python><tensorflow><pytorch><transformer-model><attention-model>","<p>I'm trying to recreate a transformer that was written in Pytorch and make it Tensorflow. Everything was going pretty well until each version of MultiHeadAttention started giving extremely different outputs. Both methods are an implementation of multi-headed attention as described in the paper &quot;Attention is all you Need&quot;, so they should be able to achieve the same output.</p>
<p>I'm converting</p>
<pre><code>self_attn = nn.MultiheadAttention(dModel, nheads, dropout=dropout)
</code></pre>
<p>to</p>
<pre><code>self_attn = MultiHeadAttention(num_heads=nheads, key_dim=dModel, dropout=dropout)
</code></pre>
<p>For my tests, dropout is 0.</p>
<p>I'm calling them with:</p>
<pre><code>self_attn(x,x,x)
</code></pre>
<p>where x is a tensor with shape=(10, 128, 50)</p>
<p>As expected from the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"" rel=""nofollow noreferrer"">documentation</a>, the Pytorch version returns a tuple, (the target sequence length, embedding dimension), both with dimensions [10, 128, 50].</p>
<p>I'm having trouble getting the TensorFlow version to do the same thing. With Tensorflow I only get one tensor back, (size [10, 128, 50]) and it looks like neither the target sequence length or embedding dimension tensor from pytorch.
Based on the <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention"" rel=""nofollow noreferrer"">Tensorflow documentation</a> I should be getting something comparable.</p>
<p>How can I get them to operate the same way? I'm guessing I'm doing something wrong with Tensorflow but I can't figure out what.</p>
","transformer-model"
"71901531","Tensorflow: ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [16,101,13] vs. shape[1] = [16,52,13] [Op:ConcatV2] name: concat","2022-04-17 11:34:28","","1","729","<python><tensorflow><huggingface-transformers><transformer-model><huggingface-datasets>","<p>I'm doing Named Entity Recognition (NER) using transformers from <a href=""https://huggingface.co/docs/transformers/tasks/token_classification"" rel=""nofollow noreferrer"">Hugging Face - Transformers</a></p>
<p>The model is implemented exactly as stated in the guide but when I try to predict the tags using test dataset I'm getting an error</p>
<blockquote>
<p>InvalidArgumentError: ConcatOp : Dimension 1 in both shapes must be equal: shape[0] = [16,101,13] vs. shape<a href=""https://huggingface.co/docs/transformers/tasks/token_classification"" rel=""nofollow noreferrer"">1</a> = [16,52,13] [Op:ConcatV2] name: concat</p>
</blockquote>
<p>With following stack trace:</p>
<pre><code>---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/Users/raffaysajjad/Library/CloudStorage/OneDrive-Personal/MS Computer Science/Semester 4 (Spring 2022)/CS 5316 - Natural Language Processing/Assignments/Assignment 4/Assignment4_Part4_20030001.ipynb Cell 32' in &lt;module&gt;
----&gt; 1 predictions = model.predict(tf_test_set)

File /usr/local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs)
     65 except Exception as e:  # pylint: disable=broad-except
     66   filtered_tb = _process_traceback_frames(e.__traceback__)
---&gt; 67   raise e.with_traceback(filtered_tb) from None
     68 finally:
     69   del filtered_tb

File /usr/local/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7186, in raise_from_not_ok_status(e, name)
   7184 def raise_from_not_ok_status(e, name):
   7185   e.message += (&quot; name: &quot; + name if name is not None else &quot;&quot;)
-&gt; 7186   raise core._status_to_exception(e) from None
</code></pre>
<p>This is how I implemented the model:</p>
<p>from transformers import TFAutoModelForTokenClassification
model = TFAutoModelForTokenClassification.from_pretrained(&quot;distilbert-base-uncased&quot;, num_labels=len(label_list))</p>
<pre><code>from transformers import create_optimizer
batch_size = 16
num_train_epochs = 3
num_train_steps = (len(tokenized_wnut[&quot;train&quot;]) // batch_size) * num_train_epochs
optimizer, lr_schedule = create_optimizer(
    init_lr=2e-5,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
    num_warmup_steps=0,
)
model.compile(optimizer=optimizer)
model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)
</code></pre>
<p>Attempt at prediction that resulted in aforementioned error:</p>
<pre><code>tf_test_set = tokenized_wnut[&quot;test&quot;].to_tf_dataset(
    columns=[&quot;attention_mask&quot;, &quot;input_ids&quot;, &quot;labels&quot;],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)
predictions = model.predict(tf_test_set)
</code></pre>
<p>Can somebody help me figure out the issue here?</p>
","transformer-model"
"71873822","Spring Integration - Two Transformers For Same Method Which Annotated With @Transformer Notation","2022-04-14 15:09:58","","1","149","<spring-integration><transformer-model>","<p>I have a method which annotated with @Transformer notation. But, in other case needs to use this this transformer for different input and output channels. Let's think that I have a code like this:</p>
<pre><code>@Transformer(inputChannel = &quot;myInputChannel&quot;, outputChannel = &quot;myOutputChannel&quot;)
public Message&lt;List&lt;DataElement&gt;&gt; transformMyBusiness(Message&lt;byte[]&gt; message) throws Throwable {
    Message&lt;List&lt;CustomDataType&gt;&gt; incomingMessage = doAJob(message);
    return incomingMessage;
}
</code></pre>
<p>I have another input channel which is myInputChannel2 and output channel myOutputChannel2. These input and output channels will use the above transformer but I don't want to duplicate that code.</p>
<p>How can I use same method for two transformers with @Transformer annotation?</p>
","transformer-model"
"71791311","Don't understand the formula of absolute position encoder","2022-04-08 03:18:42","","1","15","<nlp><transformer-model>","<p><a href=""https://i.sstatic.net/6AD1e.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I can't understand this simple formula in NLP because I specialize in CV. I know what dose Xi<em>Wq</em>(Xj*Wk)T mean. But why do E and W exchange and why do Exi and Wq transpose rather than Exj and Wk in the formula.</p>
","transformer-model"
"71788825","Using the encoder part only from T5 model","2022-04-07 20:56:51","","5","4600","<deep-learning><pytorch><huggingface-transformers><transformer-model><encoder-decoder>","<p>I want to build a classification model that needs only the encoder part of language models. I have tried Bert, Roberta, xlnet, and so far I have been successful.</p>
<p>I now want to test the encoder part only from T5, so far, I found encT5 <a href=""https://github.com/monologg/EncT5"" rel=""noreferrer"">https://github.com/monologg/EncT5</a></p>
<p>And T5EncoderModel from HuggingFace.</p>
<p>Can anyone help me understand if T5EncoderModel is what I am looking for or not?</p>
<p>It says in the description: The bare T5 Model transformer outputting encoder’s raw hidden-states without any specific head on top.</p>
<p>This is slightly confusing to me, especially that encT5 mentioned that they implemented the encoder part only because it didn't exist in HuggingFace which is what makes me more doubtful here.</p>
<p>Please note that I am a beginner in deep learning, so please go easy on me I understand that ny questions can be naive to most of you.</p>
<p>Thank you</p>
","transformer-model"
"71773100","Vision transformer binary classifier is only predicting one class","2022-04-06 20:14:46","","3","497","<python><keras><deep-learning><classification><transformer-model>","<p>I wrote a code for a vision transformer to classify mammograms into benign and malignant. After training for 30 epochs, the model is, however, predicting only one class(benign). All the final predictions for test images are in the range: 0.47 - 0.49.</p>
<p>The code:</p>
<pre><code>datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,
                                                          samplewise_center = True,
                                                          samplewise_std_normalization = True,
                                                          validation_split = 0.1,
                                                          rotation_range=180,
                                                          shear_range=15,
                                                          zoom_range=0.2,
                                                          width_shift_range=0.2,
                                                          height_shift_range=0.2,
                                                          horizontal_flip=True,
                                                          vertical_flip=True,
                                                          fill_mode='reflect')


train_gen = datagen.flow_from_dataframe(dataframe = DF_TRAIN,
                                        directory = TRAIN_PATH,
                                        x_col = 'image_file_path',
                                        y_col = 'pathology',
                                        subset = 'training',
                                        batch_size = BATCH_SIZE,
                                        # seed = 1,
                                        color_mode = 'rgb',
                                        shuffle = True,
                                        class_mode = 'binary',
                                        target_size = (IMAGE_SIZE, IMAGE_SIZE))


valid_gen = datagen.flow_from_dataframe(dataframe = DF_TRAIN,
                                        directory = TRAIN_PATH,
                                        x_col = 'image_file_path',
                                        y_col = 'pathology',
                                        subset = 'validation',
                                        batch_size = BATCH_SIZE,
                                        # seed = 1,
                                        color_mode = 'rgb',
                                        shuffle = False,
                                        class_mode = 'binary',
                                        target_size = (IMAGE_SIZE, IMAGE_SIZE))


test_gen = datagen.flow_from_dataframe(dataframe = DF_TEST,
                                        directory = TEST_PATH,
                                        x_col = 'image_file_path',
                                        y_col = 'pathology',
                                        # subset = 'validation',
                                        batch_size = BATCH_SIZE,
                                        # seed = 1,
                                        color_mode = 'rgb',
                                        shuffle = False,
                                        class_mode = 'binary',
                                        target_size = (IMAGE_SIZE, IMAGE_SIZE))

def mlp(x, hidden_units, dropout_rate):
    for units in hidden_units:
        x = layers.Dense(units, activation=tf.nn.gelu)(x)
        x = layers.Dropout(dropout_rate)(x)
    return x


class Patches(layers.Layer):
    def __init__(self, patch_size):
        super(Patches, self).__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=images,
            sizes=[1, self.patch_size, self.patch_size, 1],
            strides=[1, self.patch_size, self.patch_size, 1],
            rates=[1, 1, 1, 1],
            padding=&quot;VALID&quot;,
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches

    def get_config(self):
        config = super().get_config().copy()
        config.update({
            'patch_size': self.patch_size,
        })
        return config


class PatchEncoder(layers.Layer):
    def __init__(self, num_patches, projection_dim):
        super(PatchEncoder, self).__init__()
        self.num_patches = num_patches
        self.projection = layers.Dense(units=projection_dim)
        self.position_embedding = layers.Embedding(
            input_dim=num_patches, output_dim=projection_dim
        )

    def call(self, patch):
        positions = tf.range(start=0, limit=self.num_patches, delta=1)
        encoded = self.projection(patch) + self.position_embedding(positions)
        return encoded

    def get_config(self):
        config = super().get_config().copy()
        config.update({
            'num_patches': self.num_patches,
            'projection': self.projection,
            'position_embedding': self.position_embedding,
        })
        return config


def create_vit_classifier():
    inputs = layers.Input(shape=input_shape)
    # Create patches.
    patches = Patches(patch_size)(inputs)
    # Encode patches.
    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

    # Create multiple layers of the Transformer block.
    for _ in range(transformer_layers):
        # Layer normalization 1.
        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
        # Create a multi-head attention layer.
        attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)
        # Skip connection 1.
        x2 = layers.Add()([attention_output, encoded_patches])
        # Layer normalization 2.
        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)
        # MLP.
        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)
        # Skip connection 2.
        encoded_patches = layers.Add()([x3, x2])

    # Create a [batch_size, projection_dim] tensor.
    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    representation = layers.Flatten()(representation)
    representation = layers.Dropout(0.5)(representation)
    # Add MLP.
    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)
    # Classify outputs.
    logits = layers.Dense(num_classes, activation=&quot;sigmoid&quot;)(features)
    # Create the Keras model.
    model = tf.keras.Model(inputs=inputs, outputs=logits)
    return model

def run_experiment(model):
    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)

    model.compile(
        optimizer=optimizer,
        loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
        metrics=['accuracy'])

    STEP_SIZE_TRAIN = train_gen.n // train_gen.batch_size
    STEP_SIZE_VALID = valid_gen.n // valid_gen.batch_size
    print(STEP_SIZE_TRAIN, STEP_SIZE_VALID)

    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',
                                                     factor=0.2,
                                                     patience=2,
                                                     verbose=1,
                                                     min_delta=1e-4,
                                                     min_lr=1e-6,
                                                     mode='max')


    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='./model_3.hdf5',
                                                      monitor='val_accuracy',
                                                      verbose=1,
                                                      save_best_only=True,
                                                      save_weights_only=True,
                                                      mode='max')

    callbacks = [reduce_lr, checkpointer]

    history = model.fit(x=train_gen,
                        steps_per_epoch=STEP_SIZE_TRAIN,
                        validation_data=valid_gen,
                        validation_steps=STEP_SIZE_VALID,
                        epochs=EPOCHS,
                        callbacks=callbacks,
                        verbose=1)

    model.save(f'{save_path}/model_3.h5')

    return history


vit_classifier = create_vit_classifier()
history = run_experiment(vit_classifier)

vit_classifier.load_weights(f'{save_path}/model_3.h5')

A = vit_classifier.predict(test_gen, steps = test_gen.n // test_gen.batch_size + 1)
predicted_classes = np.where(A &gt; 0.5, 1, 0)
true_classes = test_gen.classes
class_labels = list(test_gen.class_indices.keys())

results = pd.DataFrame(list(zip(test_gen.filenames, true_classes, predicted_classes)),
               columns =['Image name', 'True class', 'Predicted class'])
results = results.replace({&quot;True class&quot;: classes_dict})
results = results.replace({&quot;Predicted class&quot;: classes_dict})
prob = pd.DataFrame(A,  columns =['Predicted probability'])

result_df = pd.concat([results, prob], axis=1)
result_df['Predicted probability'] = pd.Series([&quot;{0:.1f}&quot;.format(val * 100) for val in result_df['Predicted probability']], index=result_df.index)

results_csv = f'{save_path}/results_3.csv'
with open(results_csv, mode='w') as f:
    result_df.to_csv(f)
</code></pre>
<p>Confusion matrix:</p>
<pre><code>[[428   0]
 [276   0]]
</code></pre>
<p>Performance metrics:</p>
<p><a href=""https://i.sstatic.net/Ms332.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Ms332.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/htx7O.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/htx7O.png"" alt=""enter image description here"" /></a></p>
<p>Please help me figure out how to rectify this problem</p>
","transformer-model"
"71714052","Why DETR need to set a empty class?","2022-04-02 00:51:51","","1","571","<pytorch><transformer-model>","<p>Why DETR need to set a empty class?
It has set a &quot;Background&quot; class, which means non-object, why?</p>
","transformer-model"
"71708136","Is it possible to access hugging face transformer embedding layer?","2022-04-01 14:01:20","71728688","1","1562","<python><machine-learning><nlp><huggingface-transformers><transformer-model>","<p>I want to use a pretrained hugging face transformer language model as an encoder in a sequence to sequence model.</p>
<p>The task is grammatical error correction, so both input and output come from the same language.</p>
<p>Therefore I was wondering if it is possible to access the embedding layer from the hugging face transformer encoder, and use it as the embedding layer for the decoder?</p>
<p>Or maybe there is some other approach that you'd recommend?</p>
","transformer-model"
"71705218","Bert prediction shape not equal to num_samples","2022-04-01 10:06:33","71705603","1","188","<python><tensorflow><tensorflow-datasets><bert-language-model><transformer-model>","<p>I have a text classification that I am trying to do using BERT. Below is the code I am using. The model training code(below) works fine but I am facing issue with the prediction part</p>
<pre><code>from transformers import TFBertForSequenceClassification
import tensorflow as tf

# recommended learning rate for Adam 5e-5, 3e-5, 2e-5
learning_rate = 5e-5
nlabels = 26

# we will do just 1 epoch for illustration, though multiple epochs might be better as long as we will not overfit the model
number_of_epochs = 1


# model initialization
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=nlabels,
                                                      output_attentions=False,
                                                      output_hidden_states=False)

# optimizer Adam
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)

# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

bert_history = model.fit(ds_tr_encoded, epochs=number_of_epochs)
</code></pre>
<p>I am getting the output using the following</p>
<pre><code>preds = model.predict(ds_te_encoded)
pred_labels_idx = np.argmax(preds['logits'], axis=1)
</code></pre>
<p>The issue I am facing is that the shape of <code>pred_labels_idx</code> is not the same as <code>ds_te_encoded</code></p>
<pre><code>len(pred_labels_idx) #426820
tf.data.experimental.cardinality(ds_te_encoded) #&lt;tf.Tensor: shape=(), dtype=int64, numpy=21341&gt;
</code></pre>
<p>Not sure why this is happening.</p>
","transformer-model"
"71675036","Transformer tutorial with tensorflow: GradientTape outside the with statment but still working","2022-03-30 09:48:47","77877305","3","279","<python><tensorflow><with-statement><transformer-model><gradienttape>","<p>Applying the tensorflow <a href=""https://www.tensorflow.org/text/tutorials/transformer#training_and_checkpointing"" rel=""nofollow noreferrer"">tutorial</a> on how to implement a transformer model I had some doubts on the training process.</p>
<p>The train_step function is implemented as:</p>
<pre><code>@tf.function(input_signature=train_step_signature)
def train_step(inp, tar):
  tar_inp = tar[:, :-1]
  tar_real = tar[:, 1:]

  with tf.GradientTape() as tape:
    predictions, _ = transformer([inp, tar_inp],
                                 training = True)
    loss = loss_function(tar_real, predictions)

  gradients = tape.gradient(loss, transformer.trainable_variables)
  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))

  train_loss(loss)
  train_accuracy(accuracy_function(tar_real, predictions))
</code></pre>
<p>We can see that <code>tf.GradientTape()</code> is defined as tape in a with statment. That works, but I don't understand how tape can be called outside the statement with:</p>
<pre><code>gradients = tape.gradient(loss, transformer.trainable_variables)
</code></pre>
<p>Shouldn't tape be closed at the end of the with statement?</p>
<p>I implemented the code from the tutorial and it works as is.</p>
","transformer-model"
"71671723","Why do I get unexpected tokenization while downloading the code-bert model?","2022-03-30 04:57:11","","2","4722","<deep-learning><huggingface-transformers><transformer-model><huggingface-tokenizers>","<p>I get the following error while loading the <code>BertEmbedding</code>:</p>
<p>Code:</p>
<pre><code>name = &quot;microsoft/codebert-base&quot;

from transformers import BertModel
from transformers import BertTokenizer

print(&quot;[ Using pretrained BERT embeddings ]&quot;)
self.bert_tokenizer = BertTokenizer.from_pretrained(name, do_lower_case=lower_case)
self.bert_model = BertModel.from_pretrained(name)
if fix_emb:
    print(&quot;[ Fix BERT layers ]&quot;)
    self.bert_model.eval()
    for param in self.bert_model.parameters():
        param.requires_grad = False
else:
    print(&quot;[ Finetune BERT layers ]&quot;)
    self.bert_model.train()
</code></pre>
<pre><code>[ Using pretrained BERT embeddings ]
The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. 
The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. 
The class this function is called from is 'BertTokenizer'.
</code></pre>
","transformer-model"
"71660733","PyTorch Temporal Fusion Transformer prediction output length","2022-03-29 10:50:26","","0","1326","<python><transformer-model><pytorch-dataloader><pytorch-forecasting>","<p>I have trained a temporal fusion transformer on some training data and would like to predict on some unseen data. To do so, I'm using the <code>pytorch_forecasting</code> <code>TimeSeriesDataSet</code> data structures</p>
<p><code>testing = TimeSeriesDataSet.from_dataset(training, df[lambda x: x.year &gt; validation_cutoff], predict=True, stop_randomization=True)</code></p>
<p>with</p>
<pre><code>df[lambda x: x.year &gt; validation_cutoff].shape
(97036, 13)
</code></pre>
<p>Given that</p>
<pre><code>testing.data['reals'].shape
torch.Size([97036, 9])
</code></pre>
<p>I would expect to receive a prediction output vector containing 97036 rows. So I proceed to generate my predictions like so</p>
<pre><code>test_dataloader = testing.to_dataloader(train=False, batch_size=128 * 10, num_workers=0)
raw_predictions, x = best_tft.predict(testing, mode=&quot;raw&quot;, return_x=True)
</code></pre>
<p>However, I receive an output of the size</p>
<pre><code>raw_predictions['prediction'].shape
torch.Size([25476, 1, 7])
</code></pre>
<p>Why are some of these 97036 observations being removed?</p>
<p>Or else, how can I find out which if these 97036 observations are being dropped and why the are being removed?</p>
","transformer-model"
"71650016","How to know if a word belong to a Transformer model?","2022-03-28 15:36:33","","2","1163","<python><nlp><bert-language-model><transformer-model><sentence-transformers>","<p>I use the python library <strong>sentence_transformers</strong> with the models <strong>RoBERTa</strong> and <strong>FlauBERT</strong>.
I use cosine scores to compute similarity but for some words it doesn't work well.
Those words seems to be the one that are not part of the &quot;known&quot; words from the model (words that weren't on the training set I guess) like :  &quot;WCFs&quot;, &quot;SARs&quot;, &quot;OSGi&quot;</p>
<p><strong>Is there a way to check if a string is &quot;known&quot; by a model ?</strong> (with this library or any other one able to load those Transformers model)</p>
<p>Thanks a lot.</p>
","transformer-model"
"71607360","How to properly finetune t5 model","2022-03-24 18:00:33","","1","5677","<huggingface-transformers><transformer-model>","<p>I'm finetuning a t5-base model following <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/translation.ipynb#scrollTo=ZwZDgY-0DbrD"" rel=""nofollow noreferrer"">this notebook</a>.
However, the loss of both validation set and training set decreases very slowly. I changed the learning_rate to a larger number, but it did not help. Eventually, the bleu score on the validation set was low (around 13.7), and the translation quality was low as well.</p>
<pre><code>***** Running Evaluation *****
  Num examples = 1000
  Batch size = 32
{'eval_loss': 1.06500244140625, 'eval_bleu': 13.7229, 'eval_gen_len': 17.564, 'eval_runtime': 16.7915, 'eval_samples_per_second': 59.554, 'eval_steps_per_second': 1.906, 'epoch': 5.0}
</code></pre>
<p>If I use the &quot;Helsinki-NLP/opus-mt-en-ro&quot; model, the loss decreases properly, and at the end, the finetuned model works pretty well.</p>
<p>How to fine-tune t5-base properly? Did I miss something?</p>
","transformer-model"
"71582280","RuntimeError: shape '[-1, 784]' is invalid for input of size 614400","2022-03-23 05:35:44","","1","10691","<python><pytorch><conv-neural-network><runtime-error><transformer-model>","<p>I'm practicing code that implements the &quot;Auto-Encoding Variable Bayes (VAE)&quot; paper.
However, the error &quot;RuntimeError:shape [16, 1, 28, 28] is invalid for input of size 37632&quot; has not been resolved. I don't know how to solve it. Please help me.</p>
<pre><code>EPOCHS = 50
BATCH_SIZE = 16
# Transformer code
transformer = transforms.Compose([
            transforms.Resize((28, 28)),
            transforms.ToTensor()
])

# Transform data
train_set = torchvision.datasets.ImageFolder(root = &quot;/home/seclab_dahae/dahye/VAE_data&quot;, transform = transformer)
train_set, test_set = train_test_split(train_set, test_size=0.2)
print(&quot;Train size is {}, test size is {} &quot;.format(len(train_set), len(test_set)))

#test_set = torchvision.datasets.ImageFolder(root = &quot;/home/seclab_dahae/dahye/VAE_data&quot;, transform = transformer)

# Loading trainloader, testloader
trainloader = torch.utils.data.DataLoader(train_set, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)
testloader = torch.utils.data.DataLoader(test_set, batch_size = BATCH_SIZE, shuffle = True, num_workers = 2)
</code></pre>
<p>This is the code that brings my data to pytorch.</p>
<pre><code># VAE model
class VAE(nn.Module):
    def __init__(self, image_size, hidden_size_1, hidden_size_2, latent_size): #28*28, 512, 256, 2
        super(VAE, self).__init__()

        self.fc1 = nn.Linear(image_size, hidden_size_1)
        self.fc2 = nn.Linear(hidden_size_1, hidden_size_2)
        self.fc31 = nn.Linear(hidden_size_2, latent_size)
        self.fc32 = nn.Linear(hidden_size_2, latent_size)

        self.fc4 = nn.Linear(latent_size, hidden_size_2)
        self.fc5 = nn.Linear(hidden_size_2, hidden_size_1)
        self.fc6 = nn.Linear(hidden_size_1, image_size)

    def encode(self, x):
        h1 = F.relu(self.fc1(x))
        h2 = F.relu(self.fc2(h1))
        return self.fc31(h2), self.fc32(h2)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + std * eps

    def decode(self, z):
        h3 = F.relu(self.fc4(z))
        h4 = F.relu(self.fc5(h3))
        return torch.sigmoid(self.fc6(h4))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

VAE_model = VAE(28*28, 512, 256, 2).to(DEVICE)
optimizer = optim.Adam(VAE_model.parameters(), lr = 1e-3)
</code></pre>
<p>This is the part that implements the VAE model.</p>
<pre><code>def loss_function(recon_x, x, mu, logvar):
    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction = 'sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE, KLD
</code></pre>
<p>Here is a code that implements a loss function.</p>
<pre><code>def train(epoch, model, train_loader, optimizer):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(DEVICE)
        optimizer.zero_grad()

        recon_batch, mu, logvar = model(data)

        BCE, KLD = loss_function(recon_batch, data, mu, logvar)

        loss = BCE + KLD

        loss.backward()

        train_loss += loss.item()

        optimizer.step()

        if batch_idx % 100 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\t Loss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader),
                loss.item() / len(data)))
            
    print(&quot;======&gt; Epoch: {} Average loss: {:.4f}&quot;.format(
        epoch, train_loss / len(train_loader.dataset)
    ))  

def test(epoch, model, test_loader):
    model.eval()
    test_loss = 0
    with torch.no_grad():
        for batch_idx, (data, _) in enumerate(test_loader):
            data = data.to(DEVICE)
            
            recon_batch, mu, logvar = model(data)
            BCE, KLD = loss_function(recon_batch, data, mu, logvar)

            loss = BCE + KLD
            test_loss += loss.item()

            if batch_idx == 0:
                n = min(data.size(0), 8)
                comparison = torch.cat([data[:n], recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]]) # (16, 1, 28, 28)
                grid = torchvision.utils.make_grid(comparison.cpu()) # (3, 62, 242)
</code></pre>
<p>This is the train code and test code.
Jupiter's error occurred in the &quot;recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]&quot; section of the test code.</p>
<pre><code>for epoch in tqdm(range(0, EPOCHS)):
    train(epoch, VAE_model, trainloader, optimizer)
    test(epoch, VAE_model, testloader)
    print(&quot;\n&quot;)
    latent_to_image(epoch, VAE_model)
</code></pre>
<p>Finally, This code is a trigger to start learning.
How can I solve this error?</p>
","transformer-model"
"71561824","Can transformer replace graph neural networks in node-level tasks?","2022-03-21 17:38:47","","1","38","<deep-learning><transformer-model>","<p>Is GNN a special transformer?</p>
<p>Just a thoughtful question.</p>
","transformer-model"
"71552716","How to get tokens to words in BERT tokenizer","2022-03-21 04:08:34","","3","3677","<nlp><huggingface-transformers><bert-language-model><transformer-model><huggingface-tokenizers>","<p>I have a list, using higgingface bert tokenizer I can get the mapping numerical representation.</p>
<pre><code>X = ['[CLS]', '[MASK]', 'love', 'this', '[SEP]']
tokens = tokenizer.convert_tokens_to_ids(X)
toekns: [101, 103, 2293, 2023, 102]
</code></pre>
<p>Is there any function so that I can get tokens=[101, 103, 2293, 2023, 102] to words ['[CLS]', '[MASK]', 'love', 'this', '[SEP]']?</p>
<p>One possible way is to mapping, but is there any defined function to do it easily ?</p>
","transformer-model"
"71521771","How to remove a prediction head from pytorch model based on the output tensor?","2022-03-18 02:15:53","71525143","0","991","<python><deep-learning><pytorch><distributed><transformer-model>","<p>I am working on a ViT (Vision Transformer) related project and some low level definition is deep inside timm library, which I can not change. The low level library definition involves a linear classification prediction head, which is not a part of my network.</p>
<p>Every thing was fine until I switched to DDP parallel implementation. Pytorch complained about some parameters which didn’t contribute to the loss, and it instructed me to use “find_unused_parameters=True”. In fact, it is a common scenario and it worked again if I added this “find_unused_parameters=True” to the training routine. However, I am only allowed to change the model definition in our code base, but I cannot modify anything related to training …</p>
<p>So I guess the only thing I can do right now, is to “remove” the linear head from the model.
Although I cannot dig into the low level definition of ViT, but I can output this tensor like this:</p>
<pre><code>encoder_output,   linear_head_output =  ViT(input)
</code></pre>
<p>Is it possible to remove this linear prediction head based on this linear_head_output tensor?</p>
","transformer-model"
"71515500","What is the correct return of BertForMaskedLM?","2022-03-17 16:01:33","","0","609","<nlp><huggingface-transformers><bert-language-model><transformer-model><language-model>","<p>I'm using huggingface BertForMaskedLM.
For a sentence, I'm getting a 3-dimensional return from BertForMaskedLM.
For example (P,N,V), Here I understand the N is the length of the sentence and V is the vocab size in Bert. But I'm confused about the P. What is exactly the first return of BertForMaskedLM?</p>
","transformer-model"
"71503858","Input 0 of layer ""model"" is incompatible with the layer: expected shape=(None, 250, 3), found shape=(None, 3) in trained transformer model","2022-03-16 20:27:00","71509057","0","1494","<numpy><tensorflow><tf.keras><transformer-model><attention-model>","<p>I have a keras <code>transformer</code> model trained with tensorflow <code>2.7.0</code> and python <code>3.7</code> with input shape: <code>(None, 250, 3)</code> and a 2D array input with shape: <code>(250, 3)</code>(not an image)</p>
<p>When making a prediction with:</p>
<p><code>prediction = model.predict(state)</code></p>
<p>I get <code>ValueError: Input 0 of layer &quot;model&quot; is incompatible with the layer: expected shape=(None, 250, 3), found shape=(None, 3)</code></p>
<p>project code: <a href=""https://github.com/MikeSifanele/TT"" rel=""nofollow noreferrer"">https://github.com/MikeSifanele/TT</a></p>
<p>This is how <code>state</code> looks like:</p>
<p><code>state = np.array([[-0.07714844,-0.06640625,-0.140625],[-0.140625,-0.1650391,-0.2265625]...[0.6376953,0.6005859,0.6083984],[0.7714844,0.7441406,0.7578125]], np.float32)</code></p>
","transformer-model"
"71494220","Transformer(Multi-Head-Attention) with Audio features, Val accuracy always the same","2022-03-16 09:01:35","","0","228","<tensorflow><keras><deep-learning><transformer-model><attention-model>","<p>I have some problems with the creation of a transformer model. Whatever I change in the parameters, I always get 11.86% validation accuracy which is not even changed If I train the model with only 1 input. The accuracy changes only if I change the size of the validation data. I tried to follow <a href=""https://keras.io/examples/nlp/text_classification_with_transformer/"" rel=""nofollow noreferrer"">this guide</a>. I have 500 audio and I extracted their 20 MFCC features. So let's say I have a (500, 20, 1) size of data. The labels are the emotions that belongs to these audios. Since I already have the data, I did not use any embedding or tokenizing. Here is the code right now:</p>
<pre><code>class TransformerBlock(layers.Layer):
    def __init__(self, key_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation=&quot;relu&quot;), layers.Dense(key_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

key_dim = 2  # Embedding size for each token
num_heads = 2  # Number of attention heads
ff_dim = 32  # Hidden layer size in feed forward network inside transformer

inputs = layers.Input(shape=(20,1))
transformer_block = TransformerBlock(key_dim, num_heads, ff_dim)
x = transformer_block(inputs)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.1)(x)
x = layers.Dense(20, activation=&quot;relu&quot;)(x)
x = layers.Dropout(0.1)(x)
outputs = layers.Dense(7, activation=&quot;softmax&quot;)(x)

model = keras.Model(inputs=inputs, outputs=outputs)
optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001, beta_1=0.9, beta_2=0.999,epsilon=1e-7,amsgrad=False,name='Adam')
model.compile(optimizer=optimizer, loss=&quot;categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])

history = model.fit(
    x_train, y_train, batch_size=16, epochs=15, validation_data=(x_val, y_val)
)
</code></pre>
<p>This starts training but the results are like this:</p>
<pre><code>30/30 [==============================] - 1s 17ms/step - loss: 1.9461 - accuracy: 0.1213 - val_loss: 1.9471 - val_accuracy: 0.1186
Epoch 2/15
30/30 [==============================] - 0s 11ms/step - loss: 1.9457 - accuracy: 0.1489 - val_loss: 1.9479 - val_accuracy: 0.1186
Epoch 3/15
30/30 [==============================] - 0s 11ms/step - loss: 1.9456 - accuracy: 0.1489 - val_loss: 1.9489 - val_accuracy: 0.1186
Epoch 4/15
30/30 [==============================] - 0s 11ms/step - loss: 1.9452 - accuracy: 0.1489 - val_loss: 1.9501 - val_accuracy: 0.1186
Epoch 5/15
30/30 [==============================] - 0s 11ms/step - loss: 1.9451 - accuracy: 0.1489 - val_loss: 1.9510 - val_accuracy: 0.1186
Epoch 6/15
30/30 [==============================] - 0s 11ms/step - loss: 1.9450 - accuracy: 0.1277 - val_loss: 1.9523 - val_accuracy: 0.1186
</code></pre>
<p>Val accuracy is 11.86% even I change any of the parameters and even I change the size of the train data. I think I made a mistake while creating the model but I could not find the problem. At first I tried normalization and one hot encoding but after I saw that even I use 1 data for the train, it gives 11.86% accuracy, I thought that this problem is completely independent from the data. There should be a problem in the creation of the model but I can not see it.</p>
<p>This is the model summary:</p>
<p><a href=""https://i.sstatic.net/YFv97.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YFv97.jpg"" alt=""model summary"" /></a></p>
<p>Edit 1: I tried increasing the unit of dense layers but the result did not change.</p>
<p>Edit 2: I tried to train a simple CNN model with the same data. I got exactly 11.86% accuracy with it too. So I am not sure what is the problem. Maybe the data has a problem?</p>
","transformer-model"
"71471406","How to compute the Hessian of a large neural network in PyTorch?","2022-03-14 16:50:50","","2","1949","<deep-learning><neural-network><pytorch><bert-language-model><transformer-model>","<p>How to compute the Hessian matrix of a large neural network or transformer model like BERT in PyTorch? I know <code>torch.autograd.functional.hessian</code>, but it seems like it only calculates the Hessian of a function, but not a neural network. I also saw the answer in <a href=""https://stackoverflow.com/questions/64024312/how-to-compute-hessian-matrix-for-all-parameters-in-a-network-in-pytorch"">How to compute hessian matrix for all parameters in a network in pytorch?</a>. The problem is, I want to compute the Hessian with respect to the weights, but for large neural networks, it is very inefficient to write it as a function of the weights. Is there a better way to do this? Any suggestion is appreciated. Thanks.</p>
","transformer-model"
"71459009","ValueError: Exception encountered when calling layer ""transformer"" (type Transformer)","2022-03-13 17:09:59","","5","1284","<python><tensorflow><transformer-model>","<p>So I code a Transformers neural network that works as an ASR, it works, it trains good and saved the model as...</p>
<pre><code>model.save(&quot;savedmodel.model&quot;)
</code></pre>
<p>The problem is that when I want to predict, I do this..</p>
<pre><code>speech_model = load_model('D:\DOT\Speechrecognition\speechrecognitionE.model')

path = &quot;D:\DOT\Speechrecognition\Data\LJSpeech-1.1\wavs\LJ001-0001.wav&quot;

def path_to_audio(path):
    # spectrogram using stft
    audio = tf.io.read_file(path)
    audio, _ = tf.audio.decode_wav(audio, 1)
    audio = tf.squeeze(audio, axis=-1)
    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)
    x = tf.math.pow(tf.abs(stfts), 0.5)
    # normalisation
    means = tf.math.reduce_mean(x, 1, keepdims=True)
    stddevs = tf.math.reduce_std(x, 1, keepdims=True)
    x = (x - means) / stddevs
    audio_len = tf.shape(x)[0]
    # padding to 10 seconds
    pad_len = 2754
    paddings = tf.constant([[0, pad_len], [0, 0]])
    x = tf.pad(x, paddings, &quot;CONSTANT&quot;)[:pad_len, :]
    return x

x = path_to_audio(path)
#print(x)
speech_model.predict(x)
</code></pre>
<p>The path to audio function, converts the audio path to an spectrogram, in the training model it receive audio spectrograms as inputs, but it show this error..</p>
<pre><code>    Traceback (most recent call last):
File &quot;C:\Users\berna\Desktop\Programming\AI_ML_DL\Projects\DOT\DOT-alpha.py&quot;, line 72, in &lt;module&gt;
    speech_model.predict(x)
File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\func_graph.py&quot;, line 1129, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1621, in predict_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1611, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1604, in run_step  **
        outputs = model.predict_step(data)
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1572, in predict_step
        return self(x, training=False)
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer &quot;transformer&quot; (type Transformer).

    Could not find matching concrete function to call loaded from the SavedModel. Got:
    Positional arguments (2 total):
        * Tensor(&quot;inputs:0&quot;, shape=(None, 129), dtype=float32)
        * False
    Keyword arguments: {}

    Expected these arguments to match one of the following 4 option(s):

    Option 1:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='inputs/1')]
        * False
    Keyword arguments: {}

    Option 2:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='inputs/1')]
        * True
    Keyword arguments: {}

    Option 3:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='input_2')]
        * False
    Keyword arguments: {}

    Option 4:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='input_2')]
        * True
    Keyword arguments: {}

    Call arguments received:
    • args=('tf.Tensor(shape=(None, 129), dtype=float32)',)
    • kwargs={'training': 'False'}
</code></pre>
<p>What does that means? what is wrong with the prediction?</p>
","transformer-model"
"71454623","Facing error while using keras subclassing","2022-03-13 07:16:26","","0","3079","<python-3.x><deep-learning><tensorflow2.0><transformer-model>","<p>I was training an image caption generator using tensorflow. I was using a transformer for captions. But getting error while calling train_step().</p>
<pre><code>class Transformer(tf.keras.Model):
  def __init__(self, num_layers, d_model, num_heads, dff, row_size, col_size,
               target_vocab_size, max_pos_encoding, rate=0.1):
    super(Transformer, self).__init__()
    self.encoder = Encoder(num_layers, d_model, num_heads, dff, row_size, col_size, rate)
    self.decoder = Decoder(num_layers, d_model, num_heads, dff, 
                           target_vocab_size, max_pos_encoding, rate)
    self.final_layer = tf.keras.layers.Dense(target_vocab_size)

    def call(self, inp, tar, training):
      # inp, tar = inputs
      look_ahead_mask=None
      enc_padding_mask=None
      dec_padding_mask= create_masks_decoder(tar)
      enc_output = self.encoder(inp, training, enc_padding_mask)
      dec_output, attention_weights = self.decoder(
          tar, enc_output, training, look_ahead_mask, dec_padding_mask)
      
      final_output = self.final_layer(dec_output) #(batch_size, tar_seq_len, targe_vocab_size)

      return final_output, attention_weights


@tf.function
def train_step(img_tensor, tar):
  tar_inp = tar[:, :-1]
  tar_real = tar[:, 1:]
  dec_mask = create_masks_decoder(tar_inp)
  with tf.GradientTape() as tape:
    predictions, _  = transformer(img_tensor, tar_inp, True)
    loss = loss_function(tar_real, predictions)

  gradients = tape.gradient(loss, transformer.trainable_variables)
  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
  train_loss(loss)
  train_accuracy(tar_real, predictions)
</code></pre>
<p>I was getting Notimplemented Error</p>
<pre><code>&gt; --------------------------------------------------------------------------- NotImplementedError                       Traceback (most recent call
&gt; last) &lt;ipython-input-91-d60ab8d0c9a6&gt; in &lt;module&gt;()
&gt;       4   train_accuracy.reset_states()
&gt;       5   for (batch, (img_tensor, tar)) in enumerate(dataset):
&gt; ----&gt; 6     train_step(img_tensor, tar)
&gt;       7     if batch % 50 == 0:
&gt;       8       print(&quot;Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}&quot;.format(
&gt; 
&gt; 1 frames
&gt; /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py
&gt; in autograph_handler(*args, **kwargs)    1145           except
&gt; Exception as e:  # pylint:disable=broad-except    1146             if
&gt; hasattr(e, &quot;ag_error_metadata&quot;):
&gt; -&gt; 1147               raise e.ag_error_metadata.to_exception(e)    1148             else:    1149               raise
&gt; 
&gt; NotImplementedError: in user code:
&gt; 
&gt;     File &quot;&lt;ipython-input-88-5348883c569f&gt;&quot;, line 7, in train_step  *
&gt;         predictions, _  = transformer(img_tensor, tar_inp, True)
&gt;     File &quot;/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py&quot;,
&gt; line 67, in error_handler  **
&gt;         raise e.with_traceback(filtered_tb) from None
&gt;     File &quot;/usr/local/lib/python3.7/dist-packages/keras/engine/training.py&quot;,
&gt; line 475, in call
&gt;         raise NotImplementedError('Unimplemented `tf.keras.Model.call()`: if you '
&gt; 
&gt;     NotImplementedError: Exception encountered when calling layer &quot;transformer_1&quot; (type Transformer).
&gt;     
&gt;     Unimplemented `tf.keras.Model.call()`: if you intend to create a `Model` with the Functional API, please provide `inputs` and `outputs`
&gt; arguments. Otherwise, subclass `Model` with an overridden `call()`
&gt; method.
&gt;     
&gt;     Call arguments received:
&gt;       • inputs=tf.Tensor(shape=(64, 64, 2048), dtype=float32)
&gt;       • training=tf.Tensor(shape=(64, 30), dtype=int32)
&gt;       • mask=True
</code></pre>
<p>I have searched in many places and found that most people were missing call functions inside Class, but I have implemented call() inside my transformer class, so I don't know what is going wrong..</p>
<p>tensorflow-version:2.8
python:3.7
google-colab</p>
","transformer-model"
"71413279","How to use TensorFlow RelativePositionEmbedding layers with batches?","2022-03-09 17:06:29","71422167","0","102","<python><tensorflow><keras><transformer-model>","<p>I'm trying to incorporate a <code>RelativePositionEmbedding</code> layer into a transformer example. The embedding layer can be found in the <code>build_model</code> method below:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

from official.nlp.modeling.layers import position_embedding


def readucr(filename):
    data = np.loadtxt(filename, delimiter=&quot;\t&quot;)
    y = data[:, 0]
    x = data[:, 1:]
    return x, y.astype(int)

root_url = &quot;https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/&quot;

x_train, y_train = readucr(root_url + &quot;FordA_TRAIN.tsv&quot;)
x_test, y_test = readucr(root_url + &quot;FordA_TEST.tsv&quot;)


x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))
x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))

n_classes = len(np.unique(y_train))

idx = np.random.permutation(len(x_train))

x_train = x_train[idx]
y_train = y_train[idx]

y_train[y_train == -1] = 0
y_test[y_test == -1] = 0

# Build model

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Attention and Normalization
    x = layers.MultiHeadAttention(
            key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(inputs, inputs)
    x = layers.Dropout(dropout)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;relu&quot;)(res)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    x = layers.LayerNormalization(epsilon=1e-6)(x)
    return x + res

def build_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0
):
    inputs = keras.Input(shape=input_shape)
    x = inputs # =&gt; shape is (None, 500, 1)

    x = position_embedding.RelativePositionEmbedding(hidden_size=500)(x) # Now (500, 500)

    # Add batch dimension back. But how to accept batch size greater than 1?
    x = layers.Lambda(lambda x: tf.expand_dims(x, axis=0))(x) # Now (1, 500, 500)
    
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format=&quot;channels_first&quot;)(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(n_classes, activation=&quot;softmax&quot;)(x)
    return keras.Model(inputs, outputs)

input_shape = x_train.shape[1:]

model = build_model(
    input_shape,
    head_size=256,
    num_heads=4,
    ff_dim=4,
    num_transformer_blocks=4,
    mlp_units=[128],
    mlp_dropout=0.4,
    dropout=0.25
)


model.compile(
    loss=&quot;sparse_categorical_crossentropy&quot;,
    optimizer=keras.optimizers.Adam(learning_rate=1e-4),
    metrics=[&quot;sparse_categorical_accuracy&quot;]
)

callbacks = [
    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
    keras.callbacks.TensorBoard(log_dir=&quot;./logs&quot;)
]

model.fit(
    x_train,
    y_train,
    validation_split=0.2,
    epochs=5,
    batch_size=64,
    callbacks=callbacks
)

model.evaluate(x_test, y_test, verbose=1)
</code></pre>
<p>The following blows up because I've specified <code>batch_size</code> of <code>64</code>. However everything works fine when setting <code>batch_size</code> to <code>1</code> because the <code>expand_dims</code> operation only adds a size <code>1</code> batch dimension, as opposed to an <code>Input</code> layer that adds <code>None</code> for arbitrary batch sizes.</p>
<p>So how can I add &quot;back in&quot; a batch dimension greater than 1? Is there another way I should be using the <code>RelativePositionEncoding</code> layer to not interfere with batch sizes?</p>
<p>I've tried looking into the <code>Reshape</code> method as well without success.
I thought <a href=""https://stackoverflow.com/questions/60486437/add-none-dimension-in-tensorflow-2-0"">this question</a> would solve my issue, but this only adds a leading <code>1</code> dimension like the Lambda layer I incorporated, rather than <code>None</code>, which I think would resolve the issue.</p>
","transformer-model"
"71339595","RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd when importing sentence-transformers","2022-03-03 15:22:47","73287807","3","2651","<python-3.x><importerror><transformer-model><sentence-transformers>","<p>I want to use sentence-transformers. To do this, I installed sentence-transformers as follows:</p>
<p><code>pip install sentence-transformers</code></p>
<p>Then, I did my import as follows:</p>
<p><code>from sentence_transformers import SentenceTransformer</code></p>
<p>Which resulted in the following error:</p>
<p><code>RuntimeError: module compiled against API version 0xe but this version of numpy is 0xd</code></p>
<p>The entire Traceback is:</p>
<pre><code>Traceback (most recent call last):

  File &quot;&lt;ipython-input-112-dbcd88385343&gt;&quot;, line 1, in &lt;module&gt;
    from sentence_transformers import SentenceTransformer

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\sentence_transformers\__init__.py&quot;, line 3, in &lt;module&gt;
    from .datasets import SentencesDataset, ParallelSentencesDataset

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\sentence_transformers\datasets\__init__.py&quot;, line 3, in &lt;module&gt;
    from .ParallelSentencesDataset import ParallelSentencesDataset

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\sentence_transformers\datasets\ParallelSentencesDataset.py&quot;, line 4, in &lt;module&gt;
    from .. import SentenceTransformer

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\sentence_transformers\SentenceTransformer.py&quot;, line 27, in &lt;module&gt;
    from .models import Transformer, Pooling, Dense

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\sentence_transformers\models\__init__.py&quot;, line 1, in &lt;module&gt;
    from .Transformer import Transformer

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\sentence_transformers\models\Transformer.py&quot;, line 2, in &lt;module&gt;
    from transformers import AutoModel, AutoTokenizer, AutoConfig

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\file_utils.py&quot;, line 1985, in __getattr__
    value = getattr(module, name)

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\file_utils.py&quot;, line 1984, in __getattr__
    module = self._get_module(self._class_to_module[name])

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\file_utils.py&quot;, line 1993, in _get_module
    return importlib.import_module(&quot;.&quot; + module_name, self.__name__)

  File &quot;C:\Users\20200016\Anaconda3\lib\importlib\__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\models\auto\modeling_auto.py&quot;, line 24, in &lt;module&gt;
    from ..albert.modeling_albert import (

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\models\albert\modeling_albert.py&quot;, line 51, in &lt;module&gt;
    from .configuration_albert import AlbertConfig

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\models\albert\configuration_albert.py&quot;, line 21, in &lt;module&gt;
    from ...onnx import OnnxConfig

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\onnx\__init__.py&quot;, line 17, in &lt;module&gt;
    from .convert import export, validate_model_outputs

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\onnx\convert.py&quot;, line 23, in &lt;module&gt;
    from .. import PreTrainedModel, PreTrainedTokenizer, TensorType, TFPreTrainedModel, is_torch_available

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\file_utils.py&quot;, line 1984, in __getattr__
    module = self._get_module(self._class_to_module[name])

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\file_utils.py&quot;, line 1993, in _get_module
    return importlib.import_module(&quot;.&quot; + module_name, self.__name__)

  File &quot;C:\Users\20200016\Anaconda3\lib\importlib\__init__.py&quot;, line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File &quot;C:\Users\20200016\AppData\Roaming\Python\Python38\site-packages\transformers\modeling_tf_utils.py&quot;, line 27, in &lt;module&gt;
    import tensorflow as tf

  File &quot;C:\Users\20200016\Anaconda3\lib\site-packages\tensorflow\__init__.py&quot;, line 37, in &lt;module&gt;
    from tensorflow.python.tools import module_util as _module_util

  File &quot;C:\Users\20200016\Anaconda3\lib\site-packages\tensorflow\python\__init__.py&quot;, line 37, in &lt;module&gt;
    from tensorflow.python.eager import context

  File &quot;C:\Users\20200016\Anaconda3\lib\site-packages\tensorflow\python\eager\context.py&quot;, line 35, in &lt;module&gt;
    from tensorflow.python.client import pywrap_tf_session

  File &quot;C:\Users\20200016\Anaconda3\lib\site-packages\tensorflow\python\client\pywrap_tf_session.py&quot;, line 19, in &lt;module&gt;
    from tensorflow.python.client._pywrap_tf_session import *

ImportError: SystemError: &lt;built-in method __contains__ of dict object at 0x0000021655B72740&gt; returned a result with an error set
</code></pre>
<p>I have tried upgrading Numpy:</p>
<p><code>pip install numpy --upgrade</code></p>
<p>But this returns:</p>
<p><code>Requirement already up-to-date: numpy in c:\...\site-packages (1.22.2)</code></p>
<p>What goes wrong here? And how can I overcome this error?</p>
","transformer-model"
"71306070","Do you need to put EOS and BOS tokens in autoencoder transformers?","2022-03-01 09:11:21","74708120","4","6696","<python><pytorch><transformer-model>","<p>I'm starting to wrap my head around the transformer architecture, but there are some things that I am not yet able to grasp.</p>
<p>In decoder-free transformers, such as BERT, the tokenizer includes always the tokens CLS and SEP before and after a sentence. I understand that CLS acts both as BOS and as a single hidden output that gives the classification information, but I am a bit lost about why does it need SEP for the masked language modeling part.</p>
<p>I'll explain a bit more about the utility I expect to get. In my case, I want to train a transformer to act as an autoencoder, so target = input. There would be no decoder, since my idea is to reduce the dimensionality of the original vocabulary into less embedding dimensions, and then study (not sure how yet, but will get there) the reduced space in order to extract useful information.</p>
<p>Therefore, an example would be:</p>
<pre class=""lang-py prettyprint-override""><code>string_input = &quot;The cat is black&quot; 
tokens_input =  [1,2,3,4]

string_target = &quot;The cat is black&quot;
tokens_output = [1,2,3,4]
</code></pre>
<p>Now when tokenizing, assuming that we tokenize in the basis of word by word, what would be the advantage of adding BOS and EOS?</p>
<p>I think these are only useful when you are using the self-attention decoder, right? so, since in that case, for the decoder the outputs would have to enter right-shifted, the vectors would be:</p>
<pre class=""lang-py prettyprint-override""><code>input_string = &quot;The cat is black EOS&quot;
input_tokens = [1,2,3,4,5]

shifted_output_string = &quot;BOS The cat is black&quot;
shifted_output_tokens = [6,1,2,3,4]

output_string = &quot;The cat is black EOS&quot;
output_token = [1,2,3,4,5]
</code></pre>
<p>However, BERT does not have a self-attention decoder, but a simple feedforward layer. That is why I'm not sure of understanding the purpose of these special tokens.</p>
<p>In summary, the questions would be:</p>
<ul>
<li>Do you always need BOS and EOS tokens, even if you don't have a transformer decoder?</li>
<li>Why does BERT, that does not have a transformer decoder, require the SEP token for the masked language model part?</li>
</ul>
","transformer-model"
"71289431","DistilBERT, less than 512 tokens, Colab crash","2022-02-28 00:06:28","","1","1166","<deep-learning><nlp><google-colaboratory><bert-language-model><transformer-model>","<p>I'm following this guide <a href=""https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"" rel=""nofollow noreferrer"">https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/</a> and my text observations contain less than 250-300 words on average, so I don't  have 512 tokens in any text row.</p>
<p>But similar to this question: <a href=""https://stackoverflow.com/questions/64520322/fluctuating-ram-in-google-colab-while-running-a-bert-model"">Fluctuating RAM in google colab while running a BERT model</a> I have to limit <code>max_length</code> to less than 100, because otherwise Google Colab crashes.</p>
<p>I see other examples of applying BERT-based transformers and using Pytorch <code>DataLoader</code> to load data in batches but can't figure out how to implement it in this example.</p>
","transformer-model"
"71274561","ValueError: No gradients provided for any variable Huggingface","2022-02-26 06:28:11","","1","101","<tensorflow><huggingface-transformers><transformer-model><huggingface-tokenizers>","<p>Hi I am following the Huggingface course for <a href=""https://huggingface.co/course/chapter7/7?fw=tf"" rel=""nofollow noreferrer"">Question Answering</a>.</p>
<p>I built my own Dataset and all the features are present and I get the exact same results up until fitting the model.
There I get the above error.
After some research it seems this is caused by not having the columns in the correct order.</p>
<p>The tokenizer does output it in a different order and I changed it, but neither the order in the course nor the order of the tokenizer seem to work.</p>
<p>Can someone think of another issue?
I don't have the Data Collator as it's deprecated now.
Token Type Ids are commented out because the tokenizer does not return them.
I'm using <code>&quot;distilbert-base-cased-distilled-squad&quot;</code> because I just want to try and that seems like the fastest (smallest) model.</p>
<pre><code>tf_train_dataset = train_dataset.to_tf_dataset(
    columns=[
        &quot;attention_mask&quot;,
        &quot;end_positions&quot;,
        &quot;input_ids&quot;,
        &quot;start_positions&quot;,
        #&quot;token_type_ids&quot;,
    ],
    shuffle=True,
    batch_size=4,
)
</code></pre>
<p>Thank you very much!</p>
<p>edit: I get the same error with the model from the tutorial.</p>
","transformer-model"
"71205193","TypeError: forward() got an unexpected keyword argument 'labels'","2022-02-21 11:03:42","","0","1474","<pytorch><named-entity-recognition><transformer-model>","<p>I am trying to train a BERT model using PyTorch transformers and CRF in colab and I got error:
TypeError: forward() got an unexpected keyword argument 'labels'</p>
<p>[PHOTO] <a href=""https://i.sstatic.net/jpcVD.png"" rel=""nofollow noreferrer"">1</a></p>
<p>Could you please help in figuring out the problem</p>
<p>this is my code:</p>
<pre><code>class Bert_CRF(BertPreTrainedModel):
def __init__(self, config):
    super(Bert_CRF, self).__init__(config)
    self.num_labels = config.num_labels
    self.bert = BertModel(config)
    self.dropout = nn.Dropout(config.hidden_dropout_prob)
    self.classifier = nn.Linear(config.hidden_size, self.num_labels)
    self.init_weights()
    self.crf = CRF(self.num_labels, batch_first=True)    

 def forward(self, input_ids, attention_mask, labels=None):  # dont confuse this with _forward_alg above.
    outputs = self.bert(input_ids, attention_mask)
    sequence_output = outputs[0]
    sequence_output = self.dropout(sequence_output)
    emission = self.classifier(sequence_output)        
    attention_mask = attention_mask.type(torch.uint8)
    if labels is not None:
        loss = -self.crf(log_soft(emission, 2), labels, mask=attention_mask, reduction='mean')
        return loss
    else:
        prediction = self.crf.decode(emission, mask=attention_mask)
        return prediction
</code></pre>
","transformer-model"
"71182119","Finetuning Transformers in PyTorch (BERT, RoBERTa, etc.)","2022-02-19 04:16:09","","2","1089","<deep-learning><pytorch><bert-language-model><transformer-model><fine-tuning>","<p>Alright. So there are multiple methods to fine tune a transformer:</p>
<ol>
<li>freeze transformer's parameters and only its final outputs are fed into another model (user trains this &quot;another&quot; model),</li>
<li>the whole transformer, with a user-added custom layer, is fine tuned.
Multiple papers in top conferences use the second method. The same goes for those &quot;how to fine-tune BERT&quot; blog posts, which usually define a PyTorch custom layer as a nn.Module object. A common implementation might be such:]</li>
</ol>
<p>#Example 1 Start)</p>
<pre><code>from transformers import RobertaModel
import pytorch

class ClassificationHead(nn.Module):
    def __init__(self):
        super().__init__()
        self.dense = nn.Linear(args.hidden_dim, args.hidden_dim)
        classifier_dropout = (args.drop_out if args.drop_out is not None else 0.1)
        self.dropout = nn.Dropout(classifier_dropout)
        self.out_proj = nn.Linear(args.hidden_dim, args.num_labels)

    def forward(self, features):
        x = features[:, 0, :]  # take &lt;s&gt; token (equiv. to [CLS])
        x = self.dropout(x)
        x = self.dense(x)
        x = torch.tanh(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x

class SequenceClassification(nn.Module):
    def __init__(self):
        super().__init__()
        self.num_labels = args.num_labels
        self.model = RobertaModel.from_pretrained(roberta-base)
        self.classifier = ClassificationHead()

    def forward(self, input_ids, attention_mask):
        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs[0] #last hidden state
        logits = self.classifier(sequence_output)
        return logits
</code></pre>
<p>#Example 1 End)</p>
<p>#Example 2 Start)</p>
<pre><code>class BertBinaryClassifier(nn.Module):
    def __init__(self, dropout=0.1):
        super(BertBinaryClassifier, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.linear = nn.Linear(768, 1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, tokens):
        _, pooled_output = self.bert(tokens, utput_all=False)
        linear_output = self.linear(dropout_output)
        proba = self.sigmoid(linear_output)
        return proba
</code></pre>
<p>#Example 2 End)</p>
<p>So, I was wondering if model classes like above can be used to train the transformer model itself. From what it seems, it's like as if transformer models are only used to give output (feature extraction) and the user is training only the new custom layers (like method 1 above).</p>
<p>But many blog posts say above codes correspond to method 2, where the whole transformer is fine-tuned along with the new custom layer. How is this so?</p>
<p>If the above codes are method 2, how does one freeze BERT model and train only the new custom layer? Does anyone know a good code implementation to refer to?</p>
","transformer-model"
"71160992","How to multiply two 4-D tensors in Pytorch (implementing Self Attention from paper Attention is all you need)","2022-02-17 15:33:53","","0","819","<python><deep-learning><nlp><pytorch><transformer-model>","<p>I am trying to learn how to create a <a href=""https://www.geeksforgeeks.org/self-attention-in-nlp/"" rel=""nofollow noreferrer"">SelfAttention with Heads</a> layer in <code>Pytorch</code>. Below is the code which is done using <code>torch.einsum()</code>. I am curious how it would look without the function. I have found out that it'll need <code>torch.bmm</code> but I'm not sure how.</p>
<pre><code>class SelfAttention(nn.module):
    def __init__(self, embed_size:int, num_heads:int):
        '''
        https://lilianweng.github.io/lil-log/assets/images/transformer.png
        https://www.youtube.com/watch?v=U0s0f995w14
        '''
        super().__init__()
        assert embed_size // num_heads == 0 , &quot;'embed_size' or Embedding Dimension should be perfectly divisible my 'num_heads_parts'&quot;
        self.embed_size = embed_size
        self.num_heads = num_heads
        self.head_size = self.embed_size // self.num_heads # Embedding is divided into num_heads parts so that each head has same dimension. Dimension of Each Head

        self.key_linear = nn.Linear(self.head_size, self.head_size) # Keys will be multiplied to this weight matrix
        self.query_linear = nn.Linear(self.head_size, self.head_size) # Queries will be multiplied to this weight matrix
        self.value_linear = nn.Linear(self.head_size, self.head_size) # Values will be multiplied to this weight matrix

        self.final_fully_connected = nn.Linear(self.embed_size, self.embed_size) # OR in other terms, nn.Linear(self.num_heads * self.head_size, self.embed_size)
        # Each head's final output will come into this Layer giving a final vector of the size of embedding

    
    def forward(self, key,query,value, mask):
        '''
        For Encoder - Decoder :: Query's next word is dependent on the the words already generated + the attention given to each word in the input sentence
        key: Batch of the sentence we have given as INPUT
        query: Batch of Target Sentences

        key, query and values are batches of sentences of shape [No of samples, Words in each sentence, Embedding Dimension of Each Word]
        '''
        N = key.shape[0] # Number of samples which should be equal in all the key, queries and values
        key_len, query_len, value_len = key.shape[1], query.shape[1], value.shape[1] # number of tokens OR words == length of each sentence 

        assert key_len == value_len, &quot;Key and Value lrngth must be same&quot; # Look at the second einsum where key_len dimension by common &quot;l&quot;

        # Split the Embeddings of key, query and values in [No of samples, Length of each sentence, No of Heads, Dimension of each head]
        key = key.reshape(N, key_len, self.num_heads, self.head_size)
        query = query.reshape(N, query_len, self.num_heads, self.head_size)
        value = value.reshape(N, value_len, self.num_heads, self.head_size)

        # Look for each block of &quot;Scaled Dot Product Attention&quot; in the image https://lilianweng.github.io/lil-log/assets/images/transformer.png

        # finds out the MATMUL as -&gt; Go each word given in our target, how much ATTENTION do we have to pay for each word in our source
        MATMUL = torch.einsum('nqhd,nkhd-&gt;nhqk', [query, key])  # Output is (No of samples, No of heads, query Length, Key Length)

        # --------- n: Axis for no of samples , q: axis for sentence length in QUERY, k: axis for sentence length in KEY, h: Axis for number of heads, d: Dimension of each head ---------
       # MATMUL has a shape [No of samples, Number of heads, Query length, Key Length]

        SCALE = MATMUL / torch.sqrt(self.embed_size)

        if mask is not None: # If mask is given, means we want to cover the value at a particular place inside MATMUL, then keep its value as negative infinity or close to it
            SCALE = SCALE.masked_fill(mask == 0, float('-1e-30')) # When a place inside MASK is 0, means we want to cover it. So fill the particular place inside ENERGY as -infinity
        
        SOFTMAX = torch.softmax(SCALE, dim = 3) # Softmax the scores according to the last axis

        # Now Multiply the Normalized  SOFTMAX to the Value -&gt; Long arrow coming from the beginning in the image given

        MATMUL = torch.einsum('nhql,nlhd-&gt;nqhd',[SOFTMAX, value]) # original 'nhqk' is replaced by 'nhql' because k == v == l
        MATMUL = MATMUL.reshape(N,query_len,self.embed_size) # embed_size = No of heads * Dimension of each head we need to reshape because we have our Original Embedding fixed

        return self.final_fully_connected(MATMUL)
</code></pre>
<p>This is what I'm trying to build as exactly given in the figure
<a href=""https://i.sstatic.net/FPRAA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/FPRAA.png"" alt=""enter image description here"" /></a></p>
<p>This is the formula for <code>Scaled Dot Product</code></p>
<p><a href=""https://i.sstatic.net/YOvXb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YOvXb.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"71159073","Transformer TFBertForSequenceClassification Error(compute_loss() got an unexpected keyword argument 'labels')","2022-02-17 13:30:29","","0","691","<nlp><transformer-model>","<p>2days ago, this code worked. but it don't work now.</p>
<p>I don't know why this code is not working</p>
<p>Please write in details how to work normally this code. (Please)</p>
<p>here is my code and Error.
(I used huggingface Transformer API)</p>
<p>Data =&gt; Tensor
(I used tf.data.Dataset.from_tensor_slices)
import tensorflow as tf</p>
<pre><code>with tf.device('/device:GPU:0'):
  train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_tokenizer),
    train_target
    ))
  
  val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_tokenizer),
    val_target
    ))
  
  test_dataset = tf.data.Dataset.from_tensor_slices((
      dict(test_tokenizer),
      test_target
    ))
</code></pre>
<p>Trainer Code</p>
<pre><code>    for epoch in range(5,11):
  training_args = TFTrainingArguments( 
      ...
  )


  def get_model():
    with training_args.strategy.scope():
      config = ...
      model = TFBertForSequenceClassification.from_pretrained(BERT_MODEL, config=config, from_pt=True)
    return model

  model = get_model()

  def compute_metrics(eval_preds):
    metric = load_metric(&quot;glue&quot;, &quot;mrpc&quot;)
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

  trainer = TFTrainer(                    
      model= model,               
      args=training_args,                  
      train_dataset=train_dataset,         
      eval_dataset=val_dataset,            
      compute_metrics = compute_metrics    

  trainer.train()
  trainer.evaluate()
</code></pre>
<p>Error Code</p>
<pre><code>in user code:

    File &quot;/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_tf_bert.py&quot;, line 1455, in call  *
        loss = None if inputs[&quot;labels&quot;] is None else self.compute_loss(labels=inputs[&quot;labels&quot;], logits=logits)

    TypeError: compute_loss() got an unexpected keyword argument 'labels'

Call arguments received:
  • input_ids={'input_ids': 'tf.Tensor(shape=(16, 128), dtype=int32)', 'token_type_ids': 'tf.Tensor(shape=(16, 128), dtype=int32)', 'attention_mask': 'tf.Tensor(shape=(16, 128), dtype=int32)'}
  • attention_mask=None
  • token_type_ids=None
  • position_ids=None
  • head_mask=None
  • inputs_embeds=None
  • output_attentions=None
  • output_hidden_states=None
  • return_dict=None
  • labels=tf.Tensor(shape=(16,), dtype=int32)
  • training=True
  • kwargs=&lt;class 'inspect._empty'&gt;
</code></pre>
","transformer-model"
"71148270","SBERT's Interpretability: can we better understand the final results?","2022-02-16 19:37:23","","2","266","<nlp><transformer-model><sentence-transformers>","<p>I'm familiar with SBERT and its pre-trained models and they are amazing! But at the same time, I want to understand how the results are calculated, and I can't find anything more specific in their <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">website</a>.
For example, I have a document and I want to find other documents that are similar to it. I used 2 documents containing 200-250 words each (I changed the model.max_seq_length to 350 so the model can handle bigger texts), and in the end we can see that the cosine-similarity is 0.79. Is that all we can see? Is there a way to extract the main phrases/keywords that made the model return this high value of similarity?</p>
<p>Thanks in advance!</p>
","transformer-model"
"71145832","Properly evaluate a test dataset","2022-02-16 16:43:11","71913198","1","1404","<huggingface-transformers><transformer-model><huggingface-datasets>","<p>I trained a machine translation model using huggingface library:</p>
<pre><code>def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {&quot;bleu&quot;: result[&quot;score&quot;]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result[&quot;gen_len&quot;] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result


trainer = Seq2SeqTrainer(
    model,
    args,
    train_dataset=tokenized_datasets['train'],
    eval_dataset=tokenized_datasets['test'],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)
trainer.train()

model_dir = './models/'
trainer.save_model(model_dir)
</code></pre>
<p>The code above is taken from this <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/translation.ipynb#scrollTo=IreSlFmlIrIm"" rel=""nofollow noreferrer"">Google Colab notebook</a>. After the training, I can see the trained model is saved to the folder <code>models</code> and the metric is calculated. Now I want to load the trained model and do the prediction on a new dataset, here is what I tried:</p>
<pre><code>dataset = load_dataset('csv', data_files='data/training_data.csv')
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

# Tokenize the test dataset
tokenized_datasets = train_test.map(preprocess_function_v2, batched=True)
test_dataset = tokenized_datasets['test']
model = AutoModelForSeq2SeqLM.from_pretrained('models')
model(test_dataset)
</code></pre>
<p>It threw the following error:</p>
<pre><code>*** AttributeError: 'Dataset' object has no attribute 'size'
</code></pre>
<p>I tried the <code>evaluate()</code> function as well, but it said:</p>
<pre><code>*** torch.nn.modules.module.ModuleAttributeError: 'MarianMTModel' object has no attribute 'evaluate'
</code></pre>
<p>And the function <code>eval</code> only prints the configuration of the model.
What is the proper way to evaluate the performance of the trained model on a new dataset?</p>
","transformer-model"
"71137232","Unable to generate chunks (If length is greater than 512 in bert), we can use to split into chunks","2022-02-16 06:24:05","","1","502","<deep-learning><huggingface-transformers><bert-language-model><transformer-model><nlp-question-answering>","<p>I'm working Question &amp; Answering hugging face pipeline, my sentence length is 3535, bert only takes 512 length, so i'm trying to divide into chunks and work on it.</p>
<p>In the code, i'm working on question and answering model from hugging face, if the length of the sentence is greater than 512, bert won't take it and we've to add extra argument Truncation=True, which doesn't consider some content from the sentence, which is a drawback. That's why i'm splitting the sentence into chunks and adding back.</p>
<p>Below is the code</p>
<pre><code>from transformers import pipeline

def load_qa_model():
    model = pipeline(task='question-answering', model=model, tokenizer=tokenizer)
    return model

def generate_chunks(inp_str):
    max_chunk = 500
    inp_str = inp_str.replace('.', '.&lt;eos&gt;')
    inp_str = inp_str.replace('?', '?&lt;eos&gt;')
    inp_str = inp_str.replace('!', '!&lt;eos&gt;')

    sentences = inp_str.split('&lt;eos&gt;')
    current_chunk = 0
    chunks = []
    for sentence in sentences:
        if len(chunks) == current_chunk + 1:
            if len(chunks[current_chunk]) + len(sentence.split(' ')) &lt;= max_chunk:
                chunks[current_chunk].extend(sentence.split(' '))
            else:
                current_chunk += 1
                chunks.append(sentence.split(' '))
        else:
            chunks.append(sentence.split(' '))

    for chunk_id in range(len(chunks)):
        chunks[chunk_id] = ' '.join(chunks[chunk_id])
    return chunks

sentence = &quot;&quot;  # Consider random sentence where the length is greater than 512
vect = generate_chunks(sentence)

qa = load_qa_model()
question = &quot;Who released this article?&quot;
answers = qa(question=question, context=vect)
print(answers['answer'])
</code></pre>
<p>Below is the link for the sentence (Article)</p>
<p><a href=""https://drive.google.com/file/d/1m8rYuOaFSW7bxqm_nYo_8Ryi9RUCY3Tq/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1m8rYuOaFSW7bxqm_nYo_8Ryi9RUCY3Tq/view?usp=sharing</a></p>
<p>The output is below</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_11012\2006680085.py in &lt;module&gt;
      1 qa = load_qa_model()
      2 question = &quot;Who released this article?&quot;
----&gt; 3 answers = qa(question=question, context=vect)
      4 print(answers['answer'])

c:\users\nithi\miniconda3\lib\site-packages\transformers\pipelines\question_answering.py in __call__(self, *args, **kwargs)
    248 
    249         # Convert inputs to features
--&gt; 250         examples = self._args_parser(*args, **kwargs)
    251         if len(examples) == 1:
    252             return super().__call__(examples[0], **kwargs)

c:\users\nithi\miniconda3\lib\site-packages\transformers\pipelines\question_answering.py in __call__(self, *args, **kwargs)
     80                 inputs = [{&quot;question&quot;: kwargs[&quot;question&quot;], &quot;context&quot;: kwargs[&quot;context&quot;]}]
     81             else:
---&gt; 82                 raise ValueError(&quot;Arguments can't be understood&quot;)
     83         else:
     84             raise ValueError(f&quot;Unknown arguments {kwargs}&quot;)

ValueError: Arguments can't be understood
</code></pre>
<p><strong>How to overcome this issue?</strong></p>
","transformer-model"
"71126974","Java Transformer not writing unicode symbol in CDATA section","2022-02-15 13:06:30","","0","309","<java><unicode><emoji><cdata><transformer-model>","<p>My goal is to write a String containing the xmldata into a XML file.</p>
<p>But for some reason unicodes are not written into the cdata sections properly but rather end up outside of them.</p>
<p>For example:</p>
<p>With the string received containing <code>...&lt;TAG&gt;&lt;![CDATA[Save ߒ¾ Test]]&gt;&lt;/TAG&gt;...</code> the wanted content of the file is:</p>
<p><code>&lt;TAG&gt;&lt;![CDATA[Save 💾 Test]]&gt;&lt;/TAG&gt;</code></p>
<p>But after transforming it turns out as:</p>
<p><code>&lt;TAG&gt;&lt;![CDATA[Save ]]&gt;💾&lt;![CDATA[ Test]]&gt;&lt;/TAG&gt;</code></p>
<p>Which results in problems when wanting to read the file.</p>
<p>Here the code for the function:</p>
<pre><code>public static void saveFile(String fileName, String xmlData) throws Exception {
OutputStream out = null;
Writer writer = null;
try {
  DocumentBuilderFactory dbFactory = DocumentBuilderFactory.newInstance();
  DocumentBuilder dBuilder = dbFactory.newDocumentBuilder();
  InputStream iStream= new ByteArrayInputStream(xmlData.getBytes(Charset.forName(&quot;UTF-8&quot;)));
  Document document = dBuilder.parse(iStream);

  TransformerFactory tFactory = TransformerFactory.newInstance();
  Transformer transformer = tFactory.newTransformer();
  transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, &quot;no&quot;);
  transformer.setOutputProperty(OutputKeys.METHOD, &quot;xml&quot;);
  transformer.setOutputProperty(OutputKeys.INDENT, &quot;yes&quot;);
  transformer.setOutputProperty(OutputKeys.ENCODING, document.getXmlEncoding());

  out = new FileOutputStream(new File(fileName));
  writer = new OutputStreamWriter(out, document.getXmlEncoding());
  transformer.transform(new DOMSource(document), new StreamResult(writer));

} catch (Exception e) {
  e.printStackTrace();
  throw new Exception(&quot;Could not export XML-File!&quot;, e);
} finally {
  CommonUtil.close(writer, out);
}
</code></pre>
<p>}</p>
<p>For reference the corresponding node in the document object also contains the whole text:</p>
<p><code>[[#cdata-section: Speichern ߒ¾ Test]]</code></p>
<p>Am I missing something or has anyone run into a similar problem before?</p>
","transformer-model"
"71098518","Unknown category '2' encountered. Set `add_nan=True` to allow unknown categories pytorch_forecasting","2022-02-13 06:47:39","","6","3083","<time-series><transformer-model><pytorch-forecasting>","<p>error: &quot;Unknown category '2' encountered. Set <code>add_nan=True</code> to allow unknown categories&quot; while creating time series dataset in pytorch forecasting.</p>
<pre><code>training = TimeSeriesDataSet(
train,
time_idx=&quot;index&quot;,
target=dni,
group_ids=[&quot;Solar Zenith Angle&quot;, &quot;Relative Humidity&quot;,&quot;Dew 
Point&quot;,&quot;Temperature&quot;,&quot;Precipitable Water&quot;, &quot;Wind Speed&quot;],
min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the 
validation set)
max_encoder_length=max_encoder_length,
min_prediction_length=1,
max_prediction_length=max_prediction_length,
static_reals=[&quot;Wind Direction&quot;],
time_varying_known_reals=[&quot;index&quot;, &quot;Solar Zenith Angle&quot;, &quot;Relative Humidity&quot;,&quot;Dew 
Point&quot;,&quot;Temperature&quot;,&quot;Precipitable Water&quot;],
#     time_varying_unknown_categoricals=[],
time_varying_unknown_reals=[dhi,dni,ghi],
categorical_encoders={data.columns[2]: NaNLabelEncoder(add_nan=True)},
target_normalizer=GroupNormalizer(
    groups=[&quot;Solar Zenith Angle&quot;, &quot;Relative Humidity&quot;,&quot;Dew 
Point&quot;,&quot;Temperature&quot;,&quot;Precipitable Water&quot;, &quot;Wind Speed&quot;], transformation=&quot;softplus&quot;
),  # use softplus and normalize by group
add_relative_time_idx=True,
add_target_scales=True,
add_encoder_length=True,
</code></pre>
<p>)</p>
","transformer-model"
"71082068","mat1 and mat2 shapes cannot be multiplied (1323x9248 and 1568x32)","2022-02-11 14:51:27","","0","104","<machine-learning><pytorch><speech-recognition><transformer-model>","<p>I am using a Conformer package from Github:
<a href=""https://github.com/sooftware/conformer"" rel=""nofollow noreferrer"">Conformer</a></p>
<p>Raw data is converted to a Dataloader containing spectograms for audio and the translation converted to numbers using a Dictionary.</p>
<p>These are the shapes of my input, input_len, target, target_len where batch size=27</p>
<pre><code>torch.Size([27])
torch.Size([27, 20])
torch.Size([27])
</code></pre>
<p>This is the setup I am running (only using first batch to check that is working before training with all the batches)</p>
<pre><code>import torch
import torch.nn as nn
from conformer import Conformer

cuda = torch.cuda.is_available()  
device = torch.device('cuda' if cuda else 'cpu')

################################################################################

# batch_size, dim, sequence_length= 27, 201, 1162

for idx, (audio, audio_len, translations, translation_len) in enumerate(loader):
    inputs=audio.to(device)
    input_lengths=audio_len
    targets=translations.to(device)
    target_lengths=translation_len
    break

model = nn.DataParallel(Conformer(num_classes=6030, input_dim=201, 
                                  encoder_dim=32, num_encoder_layers=3, 
                                  decoder_dim=32)).to(device)

outputs = model(inputs, input_lengths, targets, target_lengths)
</code></pre>
<p>This is the error I am getting</p>
<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-5-50bacd198d81&gt; in &lt;module&gt;()
     43 
     44 # Forward propagate
---&gt; 45 outputs = model(inputs, input_lengths, targets, target_lengths)

12 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in linear(input, weight, bias)
   1846     if has_torch_function_variadic(input, weight, bias):
   1847         return handle_torch_function(linear, (input, weight, bias), input, weight, bias=bias)
-&gt; 1848     return torch._C._nn.linear(input, weight, bias)
   1849 
   1850 

RuntimeError: mat1 and mat2 shapes cannot be multiplied (1323x9248 and 1568x32)
</code></pre>
","transformer-model"
"71062002","How can I use custom tokenizer in opennmt transformer","2022-02-10 08:36:12","71077516","0","622","<pytorch><transformer-model><opennmt>","<p>I'm tring to transformer for translation with opennmt-py.<br />
And I already have the tokenizer trained by sentencepiece(unigram).<br />
But I don't know how to use my custom tokenizer in training config yaml.<br />
I'm refering the site of opennmt-docs (<a href=""https://opennmt.net/OpenNMT-py/examples/Translation.html"" rel=""nofollow noreferrer"">https://opennmt.net/OpenNMT-py/examples/Translation.html</a>).<br />
Here are my code and the error .</p>
<pre><code># original_ko_en.yaml
## Where is the vocab(s)
src_vocab: /workspace/tokenizer/t_50k.vocab
tgt_vocab: /workspace/tokenizer/t_50k.vocab

# Corpus opts:
data:
   corpus_1:
       path_src: /storage/genericdata_basemodel/train.ko
       path_tgt: /storage/genericdata_basemodel/train.en
       transforms: [sentencepiece]
       weight: 1
   valid:
       path_src: /storage/genericdata_basemodel/valid.ko
       path_tgt: /storage/genericdata_basemodel/valid.en
       transforms: [sentencepiece]

#### Subword
src_subword_model: /workspace/tokenizer/t_50k.model
tgt_subword_model: /workspace/tokenizer/t_50k.model
src_subword_nbest: 1
src_subword_alpha: 0.0
tgt_subword_nbest: 1
tgt_subword_alpha: 0.0

# filter
# src_seq_length: 200
# tgt_seq_length: 200

# silently ignore empty lines in the data
skip_empty_level: silent

# Train on a single GPU
world_size: 1
gpu_ranks: [0]

# General opts
save_model: /storage/models/opennmt_v1/opennmt
keep_checkpoint: 100
save_checkpoint_steps: 10000
average_decay: 0.0005
seed: 1234
train_steps: 500000
valid_steps: 20000
warmup_steps: 8000
report_every: 1000

# Model
decoder_type: transformer
encoder_type: transformer
layers: 6
heads: 8
word_vec_size: 512
rnn_size: 512
transformer_ff: 2048
dropout: 0.1
label_smoothing: 0.1

# Optimization
optim: adam
adam_beta1: 0.9
adam_beta2: 0.998
decay_method: noam
learning_rate: 2.0
max_grad_norm: 0.0
normalization: tokens
param_init: 0.0
param_init_glorot: 'true'
position_encoding: 'true'

# Batching
batch_size: 4096
batch_type: tokens
accum_count: 8
max_generator_batches: 2

# Visualization
tensorboard: True
tensorboard_log_dir: /workspace/runs/onmt1
</code></pre>
<p>and When I typing &lt; onmt_train -config xxx.yaml &gt;
</p>
<p>So, the questions are two.</p>
<ol>
<li>my sentencepiece tokenizer embedding is float. How can i resolve the int error?</li>
<li>When training stopped by accident or I want to train more some model.pt
what is the command to start training from the some model.pt ?</li>
</ol>
<p>I'll look forward to any opinion.
Thanks.</p>
","transformer-model"
"70994503","SimpleTransformers Can't Find GPT_PERSONA_CHAT","2022-02-05 01:02:18","","0","142","<python><huggingface-transformers><transformer-model><large-language-model>","<p>So I'm using simple transformers and instantiating a pretty basic ConvAi model.</p>
<pre><code>from simpletransformers.conv_ai import ConvAIModel
train_args = {
    &quot;num_train_epochs&quot;: 50,
    &quot;save_model_every_epoch&quot;: False,
}
model = ConvAIModel('gpt','gpt_personachat_cache',use_cuda=True, args=train_args)
model.train_model('data.json')


</code></pre>
<p>However when I run this I get this error.</p>
<pre><code>404 Client Error: Repository Not Found for url: https://huggingface.co/gpt_personachat_cache/resolve/main/config.json

</code></pre>
<p>I might be really bad at searching (xD) but I can't find any solution to this. How would I remedy this?</p>
","transformer-model"
"70991992","How to filter keys by the value of another elements with JOLT?","2022-02-04 19:36:57","","1","174","<json><jolt><transformer-model>","<p>I have the following input JSON:</p>
<pre><code>{
  &quot;2021&quot; : [ &quot;a&quot;, &quot;b&quot;, &quot;c&quot; ],
  &quot;2022&quot; : [ &quot;d&quot;, &quot;e&quot;, &quot;f&quot; ],
  &quot;2023&quot; : [ &quot;g&quot;, &quot;h&quot;, &quot;i&quot; ],
  &quot;2024&quot; : [ &quot;j&quot;, &quot;k&quot;, &quot;l&quot; ],
  &quot;2025&quot; : [ &quot;m&quot;, &quot;n&quot;, &quot;o&quot; ],
  &quot;year&quot; : &quot;2022&quot;
}
</code></pre>
<p>And I need to filter the keys that are equal to the value of 'year'. In this case, I just need the key '2022'. Something like that:</p>
<pre><code>{
  &quot;results_this_year&quot; : [ &quot;d&quot;, &quot;e&quot;, &quot;f&quot; ],
  &quot;year&quot; : &quot;2022&quot;
}
</code></pre>
","transformer-model"
"70922216","Saving bert model at every epoch for further training","2022-01-31 06:59:32","","1","1039","<huggingface-transformers><bert-language-model><transformer-model>","<p>I am using <code>bert_model.save_pretrained</code> for saving the model at end as this is the command that helps in saving the model with all configurations and weights but this cannot be used in model.fit command as in callbacks saving model at each epoch does not save with save_pretrained. Can anybody help me in saving bert model at each epoch since i cannot train whole bert model in one go?</p>
<p><strong>Edit</strong></p>
<p>Code for loading pre trained bert model</p>
<pre><code>bert_model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)
</code></pre>
<p>Code for compiling the bert model</p>
<pre><code>from tensorflow.keras import optimizers
bert_model.compile(loss='categorical_crossentropy',
               optimizer=optimizers.Adam(learning_rate=0.00005),
               metrics=['accuracy'])
bert_model.summary()
</code></pre>
<p>Code for training and saving the bert model</p>
<pre><code>checkpoint_filepath_1 = 'callbacks_models/BERT1.{epoch:02d}- 
{val_loss:.2f}.h5'
checkpoint_filepath_2 = 'callbacks_models/complete_best_BERT_model_1.h5'
callbacks_1 = ModelCheckpoint(
filepath=checkpoint_filepath_1,
monitor='val_loss',
mode='min',
save_best_only=False,
save_weights_only=False,
save_freq='epoch')
callbacks_2 = ModelCheckpoint(
filepath=checkpoint_filepath_2,
monitor='val_loss',
mode='min',
save_best_only=True)


es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, 
patience=5)
hist = bert_model.fit([train1_input_ids, train1_attention_masks],
                   y_train1, batch_size=16, epochs=1,validation_data= 
([val_input_ids, val_attention_masks], y_val), 
                   callbacks 
 [es,callbacks_1,callbacks_2,history_logger])


min_val_score = min(hist.history['val_loss'])
print (&quot;\nMinimum validation loss = &quot;, min_val_score)

bert_model.save_pretrained(&quot;callbacks_models/Complete_BERT_model_1.h5&quot;)
</code></pre>
","transformer-model"
"71214382","str' object has no attribute 'squeeze'","2022-01-28 11:28:01","","0","1477","<bert-language-model><transformer-model><tokenize>","<p>i got  <code>AttributeError: 'BertTokenizer' object has no attribute 'encode</code> as i import <code>BertTokenizer</code> as</p>
<pre><code>from pytorch_pretrained_bert import BertTokenizer, BertModel
</code></pre>
<p>when I changed to</p>
<pre><code>from transformers import BertTokenizer, BertModel
</code></pre>
<p>the error was solved but another appeared with using <code>squeeze(0)</code> as i got</p>
<pre><code>bert_embedding = encoded_layers[11].squeeze(0)
AttributeError: 'str' object has no attribute 'squeeze'
</code></pre>
","transformer-model"
"70867267","How to handle target decoder inputs for self attention transformer model during predict()","2022-01-26 16:54:00","72877472","1","370","<tensorflow><keras><transformer-model><attention-model>","<p>My question is essentially a duplicate of <a href=""https://stackoverflow.com/questions/68306221/tensorflow-transformer-model-decoder-target-input"">this one</a>, where I'm confused as to what to pass into the decoder during the predict() (i.e., call()) phase. I've modified tutorials found <a href=""https://trungtran.io/2019/04/29/create-the-transformer-with-tensorflow-2-0/"" rel=""nofollow noreferrer"">here</a> and <a href=""https://deepnote.com/project/TensorFlow-Transformer-Model-cuvRiHurQVqznajODH76xw/%2Ftransformer-2.ipynb/#00017-2e12c62f-5a45-4ed8-8189-a2f4155d5971"" rel=""nofollow noreferrer"">here</a> in order to create this script. This is being used for the purposes of self-attention on a time series dataset for regression (not NLP).</p>
<p>There's too much boilerplate to provide the full model so I'll write in the pertinent script:</p>
<p><strong>Transformer.py</strong></p>
<pre><code>import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense
# The following imports are my custom Layers/Functions
from Utilities.MachineLearning.Keras.Layers.Encoder import Encoder
from Utilities.MachineLearning.Keras.Layers.Decoder import Decoder
from Utilities.MachineLearning.Keras.Functions.etc import create_padding_mask, create_look_ahead_mask


def create_masks(input, target):
    # Encoder padding mask
    encoder_mask = create_padding_mask(input)

    # Used in the 2nd attention block in the decoder.
    # This padding mask is used to mask the encoder outputs.
    decoder_mask = create_padding_mask(input)

    # Used in the 1st attention block in the decoder.
    # It is used to pad and mask future tokens in the input received by
    # the decoder.
    look_ahead_mask = create_look_ahead_mask(tf.shape(target)[1])
    target_mask = create_padding_mask(target)
    encoder_decoder_mask = tf.maximum(target_mask, look_ahead_mask)

    return encoder_mask, encoder_decoder_mask, decoder_mask


class Transformer(Model):
    def __init__(
        self,
        num_inputs,
        num_outputs=1,
        num_heads=1,
        num_layers=1,
        num_embedding_inputs=None,
        num_ff_inputs=None,
        dropout=0,
    ):
        super().__init__()

        self.encoder = Encoder(
            num_inputs,
            num_heads,
            num_layers,
            num_embedding_inputs,
            num_ff_inputs,
            dropout,
        )

        self.decoder = Decoder(
            num_inputs,
            num_heads,
            num_layers,
            num_embedding_inputs,
            num_ff_inputs,
            dropout,
        )

        self.output_layer = Dense(num_outputs, name=&quot;Output&quot;)

    def call(
        self,
        inputs,
        targets,
        training=None,
    ):
        encoder_mask, encoder_decoder_mask, decoder_mask = create_masks(inputs, targets)

        encoder_output = self.encoder(inputs, encoder_mask, training)

        decoder_output, attention_weights = self.decoder(
            targets, encoder_output, encoder_decoder_mask, decoder_mask, training
        )

        output = self.output_layer(decoder_output)

        return output, attention_weights

    train_step_signature = [
        tf.TensorSpec(shape=(None, None), dtype=tf.int64),
        tf.TensorSpec(shape=(None, None), dtype=tf.int64),
    ]

    @tf.function(input_signature=train_step_signature)
    def train_step(self, data):
        x, y = data

        with tf.GradientTape() as tape:
            y_pred = self(x, y, training=True)
            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        # Update metrics (includes the metric that tracks the loss)
        self.compiled_metrics.update_state(y, y_pred)
        # Return a dict mapping metric names to current value
        return {m.name: m.result() for m in self.metrics}
</code></pre>
<p><strong>SelfAttention.py</strong></p>
<pre><code># Don't worry about what Custom is, it's basically a modified Keras Model
from Utilities.MachineLearning.Keras.Models.Custom import Custom
from Utilities.MachineLearning.Keras.Models.Transformer import Transformer


class SelfAttention(Custom):
    def initialize(self):
        self.transformer = Transformer(
            self.batch_input_shape[-1],
            num_heads=self.attention_units,
            dropout=self.attention_dropout,
            name=&quot;Transformer&quot;,

        )

    def call(self, inputs, training=False):
        # TODO: What about `targets`?
        return self.transformer(inputs, training=training)
</code></pre>
","transformer-model"
"70838353","Mirth Javascript How to set value if received value is in an array","2022-01-24 17:59:13","","0","689","<javascript><transformer-model><mirth>","<p>What I need to do is very simple.  I need to set the PV1-2.1 value if the received PV1-18.1 value is in a list of values.  I set my variable to an array of those values but the check doesn't seem to be working</p>
<pre><code>var type = ['YB','ES','EO','SO','CX']
if msg['PV1']['PV1.18']['PV1.18.1'] = type {
msg['PV1']['PV1.2']['PV1.2.1'] = 'V';
}
</code></pre>
","transformer-model"
"70828240","Why does upload a model to HuggingFace repository so slow?","2022-01-24 02:00:52","","3","848","<huggingface-transformers><transformer-model>","<p>I have a problem, I'm trying to push a model to HuggingFace repository. The problem is that it says uploading for the past 16 hours and that's just the pytorch_model bin file which is about 850MB. I am using the LFS. I have tried to manually add the files to the repository which takes an eternity that I am not willing to wait, there's no completion percentage so you are not aware if it's progressing or hanging.
I have tried using the git commands, same long wait.
However, if I try to upload to Github rather than HuggingFace, it doesn't take an eternity 30 mins at most. I felt as if I wasted my time doing all this preprocessing and training for nothing.
Any suggestions or similar problems ?</p>
","transformer-model"
"70825604","BERT error - module 'tensorflow_core.keras.activations' has no attribute 'swish'","2022-01-23 19:05:29","","1","1628","<python><nlp><tensorflow2.0><bert-language-model><transformer-model>","<p>I am trying to execute the transformer model but ended up with error.</p>
<ul>
<li>Python version == 3.7</li>
<li>Tensorflow     == 2.0</li>
<li>Transformers   == 4.15.0</li>
</ul>
<p>Source : <a href=""https://huggingface.co/cross-encoder/nli-deberta-base?candidateLabels=supply+chain%2C+scientific+discovery%2C+microbiology%2C+robots%2C+archeology&amp;multiClass=false&amp;text=shipment+will+arrive+on+next+week.+our+company+will+transport"" rel=""nofollow noreferrer"">https://huggingface.co/cross-encoder/nli-deberta-base?candidateLabels=supply+chain%2C+scientific+discovery%2C+microbiology%2C+robots%2C+archeology&amp;multiClass=false&amp;text=shipment+will+arrive+on+next+week.+our+company+will+transport</a></p>
<p>my code:</p>
<pre><code>import tensorflow
from transformers import pipeline, AutoModelForTokenClassification,BertTokenizer
pipeline(&quot;zero-shot-classification&quot;,model=&quot;cross-encoder/nli-deberta-v3-small&quot;)
</code></pre>
<p>Error : module 'tensorflow_core.keras.activations' has no attribute 'swish'</p>
","transformer-model"
"70815489","Vectorize only text column and standardize numeric column using pipeline","2022-01-22 17:43:48","","1","714","<python><pandas><scikit-learn><pipeline><transformer-model>","<p>I am trying to vectorize the text column and then standardize the numeric column. Following are the python scripts:</p>
<pre><code>from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_transformer


numeric_features = [&quot;sklearn_cosine_similarity&quot;] # numeric feature

categorical_features = [&quot;clean_text&quot;] # text feature
categorical_transformer = CountVectorizer()

preprocess = make_column_transformer((CountVectorizer(), categorical_features),(StandardScaler(), numeric_features))

pipeline = Pipeline(
    [
        (&quot;vect&quot;, preprocess),
        (&quot;knnr&quot;, KNeighborsRegressor())
    ]
)

parameters = {
    &quot;vect__countvectorizer__max_features&quot; : [None, 50,100],
    &quot;vect__countvectorizer__ngram_range&quot; : [(1, 1)],  # unigrams
    &quot;knnr__n_neighbors&quot; : [3,4,5],
    &quot;knnr__weights&quot; :['uniform', 'distance'],
    &quot;knnr__leaf_size&quot; :[20],
    'knnr__metric' : ['euclidean']
}

grid_search = GridSearchCV(pipeline, parameters, verbose=4,scoring='neg_root_mean_squared_error',cv=3)

grid_search.fit(X_train[['clean_text','sklearn_cosine_similarity']].values, y_train.values)
</code></pre>
<p><code>type(X_train[['clean_text','sklearn_cosine_similarity']]) is pandas.core.frame.DataFrame</code></p>
<p><code>type(y_train) is pandas.core.frame.DataFrame</code></p>
<p>Getting following errors while running the fit,</p>
<pre><code>Fitting 3 folds for each of 18 candidates, totalling 54 fits
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=uniform, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=uniform, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=uniform, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=uniform, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=uniform, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=uniform, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=uniform, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=uniform, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=uniform, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=distance, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=distance, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=distance, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=distance, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=distance, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=distance, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=distance, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=distance, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=3, knnr__weights=distance, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=uniform, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=uniform, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=uniform, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=uniform, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=uniform, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=uniform, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=uniform, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=uniform, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=uniform, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=distance, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=distance, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=distance, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=distance, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=distance, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=distance, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=distance, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=distance, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=4, knnr__weights=distance, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=uniform, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=uniform, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=uniform, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=uniform, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=uniform, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=uniform, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=uniform, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=uniform, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=uniform, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=distance, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=distance, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=distance, vect__countvectorizer__max_features=None, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=distance, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=distance, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=distance, vect__countvectorizer__max_features=50, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 1/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=distance, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 2/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=distance, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
[CV 3/3] END knnr__leaf_size=20, knnr__metric=euclidean, knnr__n_neighbors=5, knnr__weights=distance, vect__countvectorizer__max_features=100, vect__countvectorizer__ngram_range=(1, 1); total time=   0.0s
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
~\anaconda3\envs\deepai\lib\site-packages\sklearn\utils\__init__.py in _get_column_indices(X, key)
    373         try:
--&gt; 374             all_columns = X.columns
    375         except AttributeError:

AttributeError: 'numpy.ndarray' object has no attribute 'columns'

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp/ipykernel_9996/3908756850.py in &lt;module&gt;
      1 from time import time
      2 t0 = time()
----&gt; 3 grid_search.fit(X_train[['clean_text','sklearn_cosine_similarity']].values, y_train.values)
      4 print(&quot;done in %0.3fs&quot; % (time() - t0))

~\anaconda3\envs\deepai\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
     61             extra_args = len(args) - len(all_args)
     62             if extra_args &lt;= 0:
---&gt; 63                 return f(*args, **kwargs)
     64 
     65             # extra_args &gt; 0

~\anaconda3\envs\deepai\lib\site-packages\sklearn\model_selection\_search.py in fit(self, X, y, groups, **fit_params)
    878             refit_start_time = time.time()
    879             if y is not None:
--&gt; 880                 self.best_estimator_.fit(X, y, **fit_params)
    881             else:
    882                 self.best_estimator_.fit(X, **fit_params)

~\anaconda3\envs\deepai\lib\site-packages\sklearn\pipeline.py in fit(self, X, y, **fit_params)
    339         &quot;&quot;&quot;
    340         fit_params_steps = self._check_fit_params(**fit_params)
--&gt; 341         Xt = self._fit(X, y, **fit_params_steps)
    342         with _print_elapsed_time('Pipeline',
    343                                  self._log_message(len(self.steps) - 1)):

~\anaconda3\envs\deepai\lib\site-packages\sklearn\pipeline.py in _fit(self, X, y, **fit_params_steps)
    301                 cloned_transformer = clone(transformer)
    302             # Fit or load from cache the current transformer
--&gt; 303             X, fitted_transformer = fit_transform_one_cached(
    304                 cloned_transformer, X, y, None,
    305                 message_clsname='Pipeline',

~\anaconda3\envs\deepai\lib\site-packages\joblib\memory.py in __call__(self, *args, **kwargs)
    347 
    348     def __call__(self, *args, **kwargs):
--&gt; 349         return self.func(*args, **kwargs)
    350 
    351     def call_and_shelve(self, *args, **kwargs):

~\anaconda3\envs\deepai\lib\site-packages\sklearn\pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    752     with _print_elapsed_time(message_clsname, message):
    753         if hasattr(transformer, 'fit_transform'):
--&gt; 754             res = transformer.fit_transform(X, y, **fit_params)
    755         else:
    756             res = transformer.fit(X, y, **fit_params).transform(X)

~\anaconda3\envs\deepai\lib\site-packages\sklearn\compose\_column_transformer.py in fit_transform(self, X, y)
    504         self._validate_transformers()
    505         self._validate_column_callables(X)
--&gt; 506         self._validate_remainder(X)
    507 
    508         result = self._fit_transform(X, y, _fit_transform_one)

~\anaconda3\envs\deepai\lib\site-packages\sklearn\compose\_column_transformer.py in _validate_remainder(self, X)
    330         cols = []
    331         for columns in self._columns:
--&gt; 332             cols.extend(_get_column_indices(X, columns))
    333 
    334         remaining_idx = sorted(set(range(self._n_features)) - set(cols))

~\anaconda3\envs\deepai\lib\site-packages\sklearn\utils\__init__.py in _get_column_indices(X, key)
    374             all_columns = X.columns
    375         except AttributeError:
--&gt; 376             raise ValueError(&quot;Specifying the columns using strings is only &quot;
    377                              &quot;supported for pandas DataFrames&quot;)
    378         if isinstance(key, str):

ValueError: Specifying the columns using strings is only supported for pandas DataFrames
</code></pre>
<p>if more details required then please let me know.</p>
<p><strong>Please do not delete this question as I have not found any similar question.</strong>
<strong>if duplicate question is there then please refer the link.</strong></p>
","transformer-model"
"70811538","Why does a finetuned Wav2Vec2 model Inference return an empty string of transcriptions?","2022-01-22 09:29:14","","0","182","<facebook><nlp><pytorch><tensor><transformer-model>","<p>Finetuned a Facebook wav2vec2 base model for transcriptions. Why does my model return an empty string when I try to get the transcriptions?</p>
","transformer-model"
"70791672","huggingface transformers RuntimeError: No module named 'tensorflow.python.keras.engine.keras_tensor'","2022-01-20 18:37:14","","1","6206","<python><huggingface-transformers><transformer-model>","<p>I am looking at this <a href=""https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section4_pt.ipynb"" rel=""nofollow noreferrer"">workbook</a> which comes from <a href=""https://huggingface.co/course/chapter2/2?fw=pt"" rel=""nofollow noreferrer"">huggingface</a> course. I dont have internet access from my python environment but I could download files and save them in python environment. I copied all file from this <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""nofollow noreferrer"">folder</a> and saved it in the folder <code>bert-base-uncased/</code>. I renamed some of the files to match what is in the above folder</p>
<p>I have below packages</p>
<pre><code>tensorflow.__version__
'2.2.0'

keras.__version__
'2.4.3'
</code></pre>
<p>Then I installed transformers</p>
<pre><code>!pip install datasets transformers[sentencepiece]
checkpoint = &quot;bert-base-uncased/&quot;
from transformers import TFAutoModelForSequenceClassification



Successfully installed datasets-1.17.0 dill-0.3.4 fsspec-2022.1.0 huggingface-hub-0.4.0 multiprocess-0.70.12.2 pyarrow-6.0.1 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.15.0 xxhash-2.0.2
</code></pre>
<p>all the files are available</p>
<pre><code>#files from https://huggingface.co/bert-base-uncased/tree/main
import os
cwd = os.getcwd()
print (cwd)
os.listdir('bert-base-uncased/')

['gitattributes',
 'vocab.txt',
 'tokenizer_config.json',
 'tokenizer.json',
 'README.md',
 '.dominokeep',
 'config.json',
 'tf_model.h5',
 'flax_model.msgpack',
 'rust_model.ot',
 'pytorch_model.bin']
</code></pre>
<p>But I still get the below error. I don't get this error when I run the same code using google colab</p>
<pre><code>model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-uncased/', num_labels=2)

print (&quot;----------------&quot;)
print (type(model))
print (&quot;----------------&quot;)

RuntimeError: Failed to import transformers.models.bert.modeling_tf_bert because of the following error (look up to see its traceback):
No module named 'tensorflow.python.keras.engine.keras_tensor'
</code></pre>
","transformer-model"
"70776463","Swin Transformer attention maps visualization","2022-01-19 19:11:26","","2","2411","<maps><visualization><transformer-model><explain><self-attention>","<p>I am using a Swin Transformer for a hierarchical problem of multi calss multi label classification. I would like to visualize the self attention maps on my input image trying to extract them from the model, unfortunately I am not succeeding in this task. Could you give me a hint on how to do it?
I share you the part of the code in which I am trying to do this task.</p>
<pre><code>attention_maps = []
for module in model.modules():
    #print(module)
    if hasattr(module,'attention_patches'):  #controlla se la variabile ha l' attributo
        print(module.attention_patches.shape)
        if module.attention_patches.numel() == 224*224:
            attention_maps.append(module.attention_patches)
for attention_map in attention_maps:
    attention_map = attention_map.reshape(224, 224, 1)
    plt.imshow(sample['image'].permute(1, 2, 0), interpolation='nearest')
    plt.imshow(attention_map, alpha=0.7, cmap=plt.cm.Greys)
    plt.show()
``

In addition if you know about some explainability techniques, like Grad-CAM, which could be used with a hierarchical Swin Transformer, feel free to attach a link, it would be very helpful for me.  
</code></pre>
","transformer-model"
"70772641","How to resume training in spacy transformers for NER","2022-01-19 14:44:33","70782149","3","2412","<deep-learning><spacy><named-entity-recognition><transformer-model>","<p>I have created a spacy transformer model for named entity recognition. Last time I trained till it reached 90% accuracy and I also have a <code>model-best</code> directory from where I can load my trained model for predictions. But now I have some more data samples and I wish to resume training this spacy transformer. I saw that we can do it by changing the <code>config.cfg</code> but clueless about 'what to change?'</p>
<p>This is my <code>config.cfg</code> after running <code>python -m spacy init fill-config ./base_config.cfg ./config.cfg</code>:</p>
<pre><code>[paths]
train = null
dev = null
vectors = null
init_tok2vec = null

[system]
gpu_allocator = &quot;pytorch&quot;
seed = 0

[nlp]
lang = &quot;en&quot;
pipeline = [&quot;transformer&quot;,&quot;ner&quot;]
batch_size = 128
disabled = []
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {&quot;@tokenizers&quot;:&quot;spacy.Tokenizer.v1&quot;}

[components]

[components.ner]
factory = &quot;ner&quot;
incorrect_spans_key = null
moves = null
scorer = {&quot;@scorers&quot;:&quot;spacy.ner_scorer.v1&quot;}
update_with_oracle_cut_size = 100

[components.ner.model]
@architectures = &quot;spacy.TransitionBasedParser.v2&quot;
state_type = &quot;ner&quot;
extra_state_tokens = false
hidden_width = 64
maxout_pieces = 2
use_upper = false
nO = null

[components.ner.model.tok2vec]
@architectures = &quot;spacy-transformers.TransformerListener.v1&quot;
grad_factor = 1.0
pooling = {&quot;@layers&quot;:&quot;reduce_mean.v1&quot;}
upstream = &quot;*&quot;

[components.transformer]
factory = &quot;transformer&quot;
max_batch_items = 4096
set_extra_annotations = {&quot;@annotation_setters&quot;:&quot;spacy-transformers.null_annotation_setter.v1&quot;}

[components.transformer.model]
@architectures = &quot;spacy-transformers.TransformerModel.v3&quot;
name = &quot;roberta-base&quot;
mixed_precision = false

[components.transformer.model.get_spans]
@span_getters = &quot;spacy-transformers.strided_spans.v1&quot;
window = 128
stride = 96

[components.transformer.model.grad_scaler_config]

[components.transformer.model.tokenizer_config]
use_fast = true

[components.transformer.model.transformer_config]

[corpora]

[corpora.dev]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.dev}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[corpora.train]
@readers = &quot;spacy.Corpus.v1&quot;
path = ${paths.train}
max_length = 0
gold_preproc = false
limit = 0
augmenter = null

[training]
accumulate_gradient = 3
dev_corpus = &quot;corpora.dev&quot;
train_corpus = &quot;corpora.train&quot;
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
dropout = 0.1
patience = 1600
max_epochs = 0
max_steps = 20000
eval_frequency = 200
frozen_components = []
annotating_components = []
before_to_disk = null

[training.batcher]
@batchers = &quot;spacy.batch_by_padded.v1&quot;
discard_oversize = true
size = 2000
buffer = 256
get_length = null

[training.logger]
@loggers = &quot;spacy.ConsoleLogger.v1&quot;
progress_bar = false

[training.optimizer]
@optimizers = &quot;Adam.v1&quot;
beta1 = 0.9
beta2 = 0.999
L2_is_weight_decay = true
L2 = 0.01
grad_clip = 1.0
use_averages = false
eps = 0.00000001

[training.optimizer.learn_rate]
@schedules = &quot;warmup_linear.v1&quot;
warmup_steps = 250
total_steps = 20000
initial_rate = 0.00005

[training.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
ents_per_type = null

[pretraining]

[initialize]
vectors = ${paths.vectors}
init_tok2vec = ${paths.init_tok2vec}
vocab_data = null
lookups = null
before_init = null
after_init = null

[initialize.components]

[initialize.tokenizer]
</code></pre>
<p>As you can see there is a 'vectors' parameter under <code>[initialize]</code> so I tried giving vectors from 'model-best' like this:</p>
<p><a href=""https://i.sstatic.net/efucM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/efucM.png"" alt=""enter image description here"" /></a></p>
<p>But it gave me this error</p>
<pre><code>OSError: [E884] The pipeline could not be initialized because the vectors could not be found at './model-best/ner'. If your pipeline was already initialized/trained before, call 'resume_training' instead of 'initialize', or initialize only the components that are new.
</code></pre>
<p>For those who are wondering that I have been given the wrong path. No, that directory exists. You can see directory structure,</p>
<p><a href=""https://i.sstatic.net/BoOiX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BoOiX.png"" alt=""enter image description here"" /></a></p>
<p>So, please guide me on how I can successfully resume the training from previous weights.</p>
<p>Thank you!</p>
","transformer-model"
"70749895","I want to ask you about the structure of ""query, key, value"" of ""transformer""","2022-01-18 03:10:47","","0","305","<nlp><pytorch><translation><transformer-model><self-attention>","<p>I'm a beginner at NLP.
So I'm trying to reproduce the most basic transformer all you need code.</p>
<p>But I got a question while doing it.</p>
<p>In the MultiHeadAttention layer,
I printed out the shape of &quot;query, key, value&quot;.
However, the different shapes of &quot;query&quot; and &quot;key, value&quot; were printed.
&quot;self-attention&quot; eventually finds a correlation with oneself, which is different&quot;.I don't understand the shape of &quot;query, key, value&quot;.</p>
<p><a href=""https://i.sstatic.net/S97ln.png"" rel=""nofollow noreferrer"">enter image description here</a>
The value of &quot;query, key, value&quot; comes from src, but why are the values different?
<a href=""https://i.sstatic.net/sQaTR.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p><a href=""https://i.sstatic.net/b38AG.png"" rel=""nofollow noreferrer"">enter image description here</a></p>
<p>I brought the code from here.</p>
<p><a href=""https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb"" rel=""nofollow noreferrer"">https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/Attention_is_All_You_Need_Tutorial_(German_English).ipynb</a></p>
","transformer-model"
"70735542","Spacy - erroneous config.file","2022-01-17 01:13:29","","0","270","<config><spacy><named-entity-recognition><transformer-model>","<p>While training <em>ner</em> with custom labels, I created a .json file the exact similar way, but with my own data as stated in <a href=""https://github.com/explosion/projects/tree/v3/pipelines/ner_demo/assets"" rel=""nofollow noreferrer"">the example</a>.</p>
<p>Then I tried to convert it (both train/dev) to the binary format needed for training using the command:</p>
<pre class=""lang-none prettyprint-override""><code>python -m spacy convert train.json ./ -t spacy
</code></pre>
<p>which <strong>did</strong> result in creating two files.</p>
<p>The error I got while launching the training process:</p>
<blockquote>
<p>[E923] It looks like there is no proper sample data to initialize the Model of component 'ner'. To check your input data paths and annotation, run: python -m spacy debug data config.cfg</p>
</blockquote>
<p>The debug command output is the same.</p>
<p>How can I fix it?</p>
","transformer-model"
"70685858","Can I use pad_sequence with transformer in Pytorch？","2022-01-12 17:26:53","","0","2413","<deep-learning><pytorch><transformer-model>","<p>I'm trying to use transformer to process some image data (not NLP data), e.g. 480 x 640 images with different sequence length, an example would be [6, 480, 640], [7, 480, 640], [8, 480, 640]. And I would like to put these three sequences into one batch.</p>
<p>However, most tutorials I saw use torchtext to deal with the non-fixed length problem. But since I run the transformer with my own dataset, torchtext is not applicable(is it?). After searching I find pad_sequence can be used to deal with this problem.</p>
<p>However I didn't find any tutorials about using pad_sequence with transformer. Is it applicable？Has anyone try it before?</p>
","transformer-model"
"70676582","How can I find optimal values for training arguments?","2022-01-12 05:08:45","","0","2557","<python><pytorch><huggingface-transformers><transformer-model>","<p>These are the training arguments for a text classification bert model. (I'm using huggingface trainer)
I need to find the optimal values of <code>training epochs</code>, <code>batch size</code>, <code>learning rate</code>, <code>warmup steps</code>, <code>weight decay</code> for my dataset. Is there any way to check them before training?</p>
<p>Are there other arguments that I should consider?</p>
<pre><code>training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate= 5e-05
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    load_best_model_at_end=True,
    logging_steps=400,         
    save_steps=400,            
    evaluation_strategy=&quot;steps&quot;,     
)
</code></pre>
","transformer-model"
"70676122","When training in Transformer with multi-GPU, the shape of mask would get divided by the number of GPUs. Why?","2022-01-12 03:53:33","","1","878","<nlp><pytorch><transformer-model>","<p>there
I am training Transformer with multi-GPU, but I got a problem.
I am using Pytorch and use</p>
<pre><code>model = Transformer(
src_tokens=src_tokens, tgt_tokens=tgt_tokens, dim_model=dim_model, num_heads=num_heads,
num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dropout_p=0.1)
model = nn.DataParallel(model, device_ids=device_ids)
model.to(device)
</code></pre>
<p>The training process is like:</p>
<pre><code>def train_loop(model, opt, loss_fn, dataloader):
    model.train()
    total_loss = 0

    for X, y in dataloader:
        X, y = X.t().to(device), y.t().to(device)

        y_input = y[:, :-1]
        y_expected = y[:, 1:]

        sequence_length = y_input.size(1)
        src_pad_mask = create_pad_mask(X, 1)
        tgt_pad_mask = create_pad_mask(y_input, 1)
        tgt_mask = get_tgt_mask(sequence_length)
        pred = model(X, y_input, tgt_mask=tgt_mask, src_pad_mask=src_pad_mask, tgt_pad_mask=tgt_pad_mask)
        # Permute pred to have batch size first again

        pred = pred.permute(1, 2, 0)
        loss = loss_fn(pred, y_expected)
        opt.zero_grad()
        loss.backward()
        opt.step()
        total_loss += loss.detach().item()
    return total_loss / len(dataloader)
</code></pre>
<p>my model.py is like that:</p>
<pre><code>class Transformer(nn.Module):
    &quot;&quot;&quot;
    Model from &quot;A detailed guide to Pytorch's nn.Transformer() module.&quot;, by
    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1
    &quot;&quot;&quot;

    # Constructor
    def __init__(
            self,
            src_tokens,
            tgt_tokens,
            dim_model,
            num_heads,
            num_encoder_layers,
            num_decoder_layers,
            dropout_p,
    ):
        super().__init__()

        # INFO
        self.model_type = &quot;Transformer&quot;
        self.dim_model = dim_model

        # LAYERS
        self.positional_encoder = PositionalEncoding(
            dim_model=dim_model, dropout_p=dropout_p, max_len=5000
        )
        self.src_embedding = nn.Embedding(src_tokens, dim_model)
        self.tgt_embedding = nn.Embedding(tgt_tokens, dim_model)
        self.transformer = nn.Transformer(
            d_model=dim_model,
            nhead=num_heads,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dropout=dropout_p,
        )
        self.out = nn.Linear(dim_model, tgt_tokens)

    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):
        # Src size must be (batch_size, src sequence length)
        # Tgt size must be (batch_size, tgt sequence length)

        # Embedding + positional encoding - Out size = (batch_size, sequence length, dim_model)

        src = self.src_embedding(src) * math.sqrt(self.dim_model)
        tgt = self.tgt_embedding(tgt) * math.sqrt(self.dim_model)
        src = self.positional_encoder(src)
        tgt = self.positional_encoder(tgt)

        # We could use the parameter batch_first=True, but our KDL version doesn't support it yet, so we permute
        # to obtain size (sequence length, batch_size, dim_model),
        src = src.permute(1, 0, 2)
        tgt = tgt.permute(1, 0, 2)
        print('src_pad_mask: '+str(src_pad_mask.shape)+'  tgt_pad_mask: '+str(tgt_pad_mask.shape)+'  tgt_mask: '+str(tgt_mask.shape))
        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++')

        # Transformer blocks - Out size = (sequence length, batch_size, num_tokens)
        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask,
                                           tgt_key_padding_mask=tgt_pad_mask)
        out = self.out(transformer_out)

        return out
</code></pre>
<p>I get this error:</p>
<pre><code>root@3b:/koi/transformer-multi# python train.py
src_pad_mask: torch.Size([1, 4784])  tgt_pad_mask: torch.Size([1, 3225])  tgt_mask: torch.Size([538, 3225])
++++++++++++++++++++++++++++++++++++++++++++++++++++++++
src_pad_mask: torch.Size([1, 4784])  tgt_pad_mask: torch.Size([1, 3225])  tgt_mask: torch.Size([538, 3225])
++++++++++++++++++++++++++++++++++++++++++++++++++++++++
src_pad_mask: torch.Size([1, 4784])  tgt_pad_mask: torch.Size([1, 3225])  tgt_mask: torch.Size([538, 3225])
++++++++++++++++++++++++++++++++++++++++++++++++++++++++
src_pad_mask: torch.Size([1, 4784])  tgt_pad_mask: torch.Size([1, 3225])  tgt_mask: torch.Size([538, 3225])
++++++++++++++++++++++++++++++++++++++++++++++++++++++++
src_pad_mask: torch.Size([1, 4784])  tgt_pad_mask: torch.Size([1, 3225])  tgt_mask: torch.Size([538, 3225])
++++++++++++++++++++++++++++++++++++++++++++++++++++++++
src_pad_mask: torch.Size([1, 4784])  tgt_pad_mask: torch.Size([1, 3225])  tgt_mask: torch.Size([535, 3225])
++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Traceback (most recent call last):
  File &quot;/koi/transformer-multi/train.py&quot;, line 160, in &lt;module&gt;
    train_loss = train_loop(model, opt, loss_fn, trn_loader)
  File &quot;/koi/transformer-multi/train.py&quot;, line 121, in train_loop
    pred = model(X, y_input, tgt_mask=tgt_mask, src_pad_mask=src_pad_mask, tgt_pad_mask=tgt_pad_mask)
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 86, in parallel_apply
    output.reraise()
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/_utils.py&quot;, line 434, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 61, in _worker
    output = module(*input, **kwargs)
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/koi/transformer-multi/model.py&quot;, line 93, in forward
    transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask,
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/transformer.py&quot;, line 142, in forward
    output = self.decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/transformer.py&quot;, line 248, in forward
    output = mod(output, memory, tgt_mask=tgt_mask,
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/transformer.py&quot;, line 451, in forward
    x = self.norm1(x + self._sa_block(x, tgt_mask, tgt_key_padding_mask))
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/transformer.py&quot;, line 460, in _sa_block
    x = self.self_attn(x, x, x,
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/modules/activation.py&quot;, line 1003, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File &quot;/root/anaconda3/envs/koi/lib/python3.9/site-packages/torch/nn/functional.py&quot;, line 5011, in multi_head_attention_forward
    raise RuntimeError(f&quot;The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.&quot;)
RuntimeError: The shape of the 2D attn_mask is torch.Size([538, 3225]), but should be (3225, 3225).
</code></pre>
<p>And I test for several times, every time I changed the number of GPUs. The shape of mask would get divided by the number of GPUs.
I dont know how to solve this.</p>
","transformer-model"
"70663238","Can the increase in training loss lead to better accuracy?","2022-01-11 07:48:09","70663937","0","325","<nlp><pytorch><training-data><transformer-model>","<p>I'm working on a competition on Kaggle. First, I trained a Longformer base with the competition dataset and achieved a quite good result on the leaderboard. Due to the CUDA memory limit and time limit, I could only train 2 epochs with a batch size of 1. The loss started at about 2.5 and gradually decreased to 0.6 at the end of my training.</p>
<p>I then continued training 2 more epochs using that saved weights. This time I used a little bit larger learning rate (the one on the Longformer paper) and added the validation data to the training data (meaning I no longer split the dataset 90/10). I did this to try to achieve a better result.</p>
<p>However, this time the loss started at about 0.4 and constantly increased to 1.6 at about half of the first epoch. I stopped because I didn't want to waste computational resources.</p>
<p>Should I have waited more? Could it eventually lead to a better test result? I think the model could have been slightly overfitting at first.</p>
","transformer-model"
"70662675","Failed API Call - How to handle ""Error 400"" for Jurassic API?","2022-01-11 06:51:44","","0","131","<python><transformer-model><jurassic>","<p>When setting up the API call for Jurassic (requested from <a href=""https://studio.ai21.com/docs/api/"" rel=""nofollow noreferrer"">https://studio.ai21.com/docs/api/</a>), I always get the 400 error response.</p>
<pre class=""lang-py prettyprint-override""><code>import json
import requests

   requests.post(
    &quot;https://api.ai21.com/studio/v1/j1-large/complete&quot;,
    headers={&quot;Authorization&quot;: &quot;PERSONAL API KEY&quot;}, 
    json={
        &quot;prompt&quot;: &quot;Life is like&quot;, 
        &quot;numResults&quot;: 1, 
        &quot;maxTokens&quot;: 8, 
        &quot;stopSequences&quot;: [&quot;.&quot;],
        &quot;topKReturn&quot;: 0,
        &quot;temperature&quot;: 0.0
    }
)
</code></pre>
<p>Output: &lt;Response [400]&gt;</p>
<p>Could anyone give me some advice please?</p>
","transformer-model"
"70654644","how to pass string y label validation for TransformedTargetRegressor?","2022-01-10 15:14:24","","1","68","<python><numpy><scikit-learn><pipeline><transformer-model>","<p>I need help in transforming target y label.</p>
<p>Currently, I'm working on an NLP prediction labeling. I'm using sklearn pipeline to apply list of transformers (*using tfidf vectorizer) and a final estimator (*using linearSVC classification) in the pipeline, then using TransformedTargetRegressor to transform the target y (*using MultiLabelBinarizer).</p>
<p>It works well when i just use the pipeline without TransformedTargetRegressor to fit and predict the label (i transform and inverse_transform the target y manually). But since I need to bundle all them up end to end, I use TransformedTargetRegressor to do the target y transformation, which returning some error:</p>
<pre><code>ValueError                                Traceback (most recent call last)
~/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    778             try:
--&gt; 779                 array = array.astype(np.float64)
    780             except ValueError as e:

ValueError: could not convert string to float: 'account'

The above exception was the direct cause of the following exception:

ValueError                                Traceback (most recent call last)
/var/folders/2b/cyjjz4sn4fq5vz72qz2qcnvh9dlt67/T/ipykernel_74402/135390795.py in &lt;module&gt;
     18 
     19 with mlflow.start_run(run_name=&quot;TFIDF + LinearSVC + with Pipeline and column&quot;):
---&gt; 20     regr.fit(X_train[:1], tesst_y)
     21     mlflow.set_tag('model','NLP')
     22     #mlflow.log_metric('j_score', j_score(y_test, y_pred))

~/miniconda3/lib/python3.9/site-packages/sklearn/compose/_target.py in fit(self, X, y, **fit_params)
    208             Fitted estimator.
    209         &quot;&quot;&quot;
--&gt; 210         y = check_array(
    211             y,
    212             accept_sparse=False,

~/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    779                 array = array.astype(np.float64)
    780             except ValueError as e:
--&gt; 781                 raise ValueError(
    782                     &quot;Unable to convert array of bytes/strings &quot;
    783                     &quot;into decimal numbers with dtype='numeric'&quot;

ValueError: Unable to convert array of bytes/strings into decimal numbers with dtype='numeric'
</code></pre>
<p>From the error message, says that the error comes from the target y. Here's a glimpse of the data:</p>
<pre><code>array([['account', 'change_number'],
       ['apple', 'banana']], dtype='&lt;U13')
</code></pre>
<p>So, I assume that the error comes from the y label is not in numeric, then I tried using numeric y label (I'm using dummy data here), like this and it works:</p>
<pre><code>array([[1, 5],
       [2, 6]])
</code></pre>
<p>Looks like TransformedTargetRegressor need the target y label in numeric (i've observe the validation.py in the source code as well). Is there any posibilities for the target y label to be in string? or did I miss anything here to make it happen?</p>
","transformer-model"
"70640915","Unable to load ted_hrlr_translate/pt_to_en dataset","2022-01-09 11:41:08","","2","861","<python><tensorflow><jupyter-notebook><tensorflow-datasets><transformer-model>","<p>I was running transformer<a href=""https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/tensorflow/transformer.ipynb#scrollTo=8q9t4FmN96eN"" rel=""nofollow noreferrer"">[link]</a> code in my jupyter notebook by looking at the google collab code. The tfds.load was able to load ted_hrlr_translate/pt_to_en dataset in collab but was unable in jupyter notebook. The error that hit the screen was :</p>
<blockquote>
<p>Failed to construct dataset ted_hrlr_translate: Message type
&quot;tensorflow_datasets.DatasetInfo&quot; has no field named &quot;releaseNotes&quot;.
Available Fields(except extensions): ['name', 'description',
'version', 'configName', 'configDescription', 'citation',
'sizeInBytes', 'downloadSize', 'location', 'downloadChecksums',
'schema', 'splits', 'supervisedKeys', 'redistributionInfo',
'moduleName', 'disableShuffling', 'fileFormat']</p>
</blockquote>
<p>Is this a bug or I am too dumb to understand the error?
Do I need to download the dataset? If so then i was able to load mnist data in jupyter notebook without downloading it.</p>
","transformer-model"
"70632825","IndexError: index out of range in self - Transformers Module, using Pegasus Model for Paraphrasing","2022-01-08 13:36:49","","1","394","<python><python-3.x><nlp><huggingface-transformers><transformer-model>","<p>So I'm using Transformers library and using the model &quot;Pegasus&quot; for Paraphrasing news articles, etc that I scrape online. I split the articles into sentences using NLTK library &quot;sent_tokenize&quot; and then run each sentence through Pegasus, however, I keep getting the error &quot;IndexError: index out of range in self&quot; and I can't seem to figure out what I'm doing wrong or how to fix.</p>
<p>Text: &quot;Soon thereafter, the whites (conjunctiva) and corneas of the eyes may become dry and thick—a condition called xerophthalmia Keratomalacia Keratomalacia is an eye disorder that involves drying and clouding of the cornea (the clear layer in front of the iris and pupil) due to vitamin A deficiency in people with undernutrition.&quot;</p>
<p>Code:</p>
<pre><code>def get_response(input_text,num_return_sequences):
  batch = tokenizer.prepare_seq2seq_batch([input_text],padding='longest',max_length=1024, return_tensors=&quot;pt&quot;).to(torch_device)
  translated = model.generate(**batch,max_length=1024,num_beams=10, num_return_sequences=num_return_sequences, temperature=1.5)
  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)
  return tgt_text

introduction_list = nltk.tokenize.sent_tokenize(introtext)
for phrase in introduction_list:
   if len(phrase) &gt; 15 and len(phrase) &lt; 1024:
        para_phrases = get_response(phrase, 1)
</code></pre>
<p>The error is triggering in:</p>
<pre><code>translated = model.generate(**batch,max_length=1024,num_beams=10, num_return_sequences=num_return_sequences, temperature=1.5)
</code></pre>
<p>Error:</p>
<pre><code>File &quot;C:\Python\lib\site-packages\torch\nn\functional.py&quot;, line 2044, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
IndexError: index out of range in self
</code></pre>
","transformer-model"
"70606412","Does torch.nn.MultiheadAttention contain normalisation layer and feed forward layer?","2022-01-06 11:26:08","70608623","0","574","<python><pytorch><bert-language-model><transformer-model><attention-model>","<p>Tried to find the source code of multihead attention but could not find any implementation details. I wonder if this module only contains the attention part rather than the whole transformer block (i.e. It does not contain the normalisation layer, residual connection and an additional feedforward neural network)?</p>
","transformer-model"
"70589452","For an image or sequence, what is the properties transformers use?","2022-01-05 07:54:25","70590040","2","186","<conv-neural-network><transformer-model><self-attention>","<p>Today my teacher ask me a question: he said the CNN is use the translation invariance of the images or matrixs. So what is the properties Transformer uses ???</p>
","transformer-model"
"70589022","Transformer didn't work well with tensorflow gradient tape","2022-01-05 07:11:44","","0","105","<tensorflow><transformer-model>","<p>I implemented transformer with tensorflow 2.0. The model works well when I train the model with model.fit(dataset)</p>
<p>However, when I train the model with tensorflow.GradientTape and evaluate it, the model yields blank space token for all inputs. Here is my code, and tensorflow version is 2.7.0</p>
<pre><code>def loss_function(y_true, y_pred):
  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))

  loss = tf.keras.losses.SparseCategoricalCrossentropy(
      from_logits=True, reduction='none')(y_true, y_pred)


  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)
  loss = tf.multiply(loss, mask)
  return tf.reduce_mean(loss)

for epoch in range(num_epochs):
  for step, data in enumerate(dataset):
    enc_inputs, dec_inputs, outputs = data[0]['inputs'], data[0]['dec_inputs'], data[1]['outputs']
    
    with tf.GradientTape() as tape:
      logits = model([enc_inputs, dec_inputs], training = True)
      loss   = loss_function(outputs, logits)

    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
</code></pre>
<p>I think there is no problem with my transformer model code, because it works well with model.fit(dataset). What's wrong with my code?</p>
","transformer-model"
"70556326","SimpleTransformers ""max_seq_length"" argument results in CUDA out of memory error in Kaggle and Google Colab","2022-01-02 13:23:04","70556530","0","1120","<pytorch><kaggle><transformer-model><simpletransformers>","<p>When fine-tuning the sloBERTa Transformer model, based on CamemBERT, for a multiclass classification task with SimpleTransformers, I want to use the model argument &quot;max_seq_length&quot;: 512, as previous work states that it gives better results than 128, but the inclusion of this argument triggers the error below. The error is the same in Kaggle and Google Colab environment, and terminating the execution and reruning it does not help. The error is triggered not matter how small the number of training epochs is, and the dataset contains only 600 instances (with text as strings, and labels as integers). I've tried lowering the max_seq_length to 509, 500 and 128, but the error persists.</p>
<p>The setup without this argument works normally and allows training with 90 epochs, so I otherwise have enough memory.</p>
<pre><code>from simpletransformers.classification import ClassificationModel

# define hyperparameter
model_args ={&quot;overwrite_output_dir&quot;: True,
             &quot;num_train_epochs&quot;: 90,
             &quot;labels_list&quot;: LABELS_NUM,
             &quot;learning_rate&quot;: 1e-5,
             &quot;train_batch_size&quot;: 32,
             &quot;no_cache&quot;: True,
             &quot;no_save&quot;: True,
             #&quot;max_seq_length&quot;: 512,
             &quot;save_steps&quot;: -1,
             }

model = ClassificationModel(
    &quot;camembert&quot;, &quot;EMBEDDIA/sloberta&quot;,
    use_cuda = device,
    num_labels = NUM_LABELS,
    args = model_args)

model.train_model(train_df)
</code></pre>
<p>This is the error:</p>
<pre><code>RuntimeError                              Traceback (most recent call last)
/tmp/ipykernel_34/2529369927.py in &lt;module&gt;
    19     args = model_args)
    20 
---&gt; 21 model.train_model(train_df)

/opt/conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py in train_model(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)
   610             eval_df=eval_df,
   611             verbose=verbose,
--&gt; 612             **kwargs,
   613         )
   614 

/opt/conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py in train(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, test_df, verbose, **kwargs)
   883                             loss_fct=self.loss_fct,
   884                             num_labels=self.num_labels,
--&gt; 885                             args=self.args,
   886                         )
   887                 else:

/opt/conda/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py in _calculate_loss(self, model, inputs, loss_fct, num_labels, args)
  2256 
  2257     def _calculate_loss(self, model, inputs, loss_fct, num_labels, args):
-&gt; 2258         outputs = model(**inputs)
  2259         # model outputs are always tuple in pytorch-transformers (see doc)
  2260         loss = outputs[0]

/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   720             result = self._slow_forward(*input, **kwargs)
   721         else:
--&gt; 722             result = self.forward(*input, **kwargs)
   723         for hook in itertools.chain(
   724                 _global_forward_hooks.values(),

/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
  1210             output_attentions=output_attentions,
  1211             output_hidden_states=output_hidden_states,
-&gt; 1212             return_dict=return_dict,
  1213         )
  1214         sequence_output = outputs[0]

/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   720             result = self._slow_forward(*input, **kwargs)
   721         else:
--&gt; 722             result = self.forward(*input, **kwargs)
   723         for hook in itertools.chain(
   724                 _global_forward_hooks.values(),

/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
   859             output_attentions=output_attentions,
   860             output_hidden_states=output_hidden_states,
--&gt; 861             return_dict=return_dict,
   862         )
   863         sequence_output = encoder_outputs[0]

/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   720             result = self._slow_forward(*input, **kwargs)
   721         else:
--&gt; 722             result = self.forward(*input, **kwargs)
   723         for hook in itertools.chain(
   724                 _global_forward_hooks.values(),

/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py in forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)
   531                     encoder_attention_mask,
   532                     past_key_value,
--&gt; 533                     output_attentions,
   534                 )
   535 

/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   720             result = self._slow_forward(*input, **kwargs)
   721         else:
--&gt; 722             result = self.forward(*input, **kwargs)
   723         for hook in itertools.chain(
   724                 _global_forward_hooks.values(),

/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py in forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
   415             head_mask,
   416             output_attentions=output_attentions,
--&gt; 417             past_key_value=self_attn_past_key_value,
   418         )
   419         attention_output = self_attention_outputs[0]

/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   720             result = self._slow_forward(*input, **kwargs)
   721         else:
--&gt; 722             result = self.forward(*input, **kwargs)
   723         for hook in itertools.chain(
   724                 _global_forward_hooks.values(),

/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py in forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
   344             encoder_attention_mask,
   345             past_key_value,
--&gt; 346             output_attentions,
   347         )
   348         attention_output = self.output(self_outputs[0], hidden_states)

/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   720             result = self._slow_forward(*input, **kwargs)
   721         else:
--&gt; 722             result = self.forward(*input, **kwargs)
   723         for hook in itertools.chain(
   724                 _global_forward_hooks.values(),

/opt/conda/lib/python3.7/site-packages/transformers/models/roberta/modeling_roberta.py in forward(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)
   273             attention_probs = attention_probs * head_mask
   274 
--&gt; 275         context_layer = torch.matmul(attention_probs, value_layer)
   276 
   277         context_layer = context_layer.permute(0, 2, 1, 3).contiguous()

RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 15.90 GiB total capacity; 15.04 GiB already allocated; 15.75 MiB free; 15.12 GiB reserved in total by PyTorch)
</code></pre>
<p>Additional code (if it helps - I've tried everything regarding the pytorch I found on the web - the full code can be accessed at <a href=""https://www.kaggle.com/tajakuz/0-sloberta-example-max-seq-length-error"" rel=""nofollow noreferrer"">https://www.kaggle.com/tajakuz/0-sloberta-example-max-seq-length-error</a>):</p>
<pre><code>!conda install --yes pytorch&gt;=1.6 cudatoolkit=11.0 -c pytorch

# install simpletransformers
!pip install -q transformers
!pip install --upgrade transformers
!pip install -q simpletransformers

# check installed version
!pip freeze | grep simpletransformers

!pip uninstall -q torch -y
!pip install -q torch==1.6.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html

# pytorch libraries
import torch # the main pytorch library
import torch.nn as nn # the sub-library containing Softmax, Module and other useful functions
import torch.optim as optim # the sub-library containing the common optimizers (SGD, Adam, etc.)
from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler

from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'

#importing other necessary packages and ClassificationModel for bert
from tqdm import tqdm
import warnings
warnings.simplefilter('ignore')

from scipy.special import softmax
</code></pre>
<p>Thank you so much for your help, it is really appreciated!</p>
","transformer-model"
"70507101","Transformers pretraining with MLM problem - sentence embeddings","2021-12-28 12:30:12","","0","812","<pytorch><torch><huggingface-transformers><transformer-model><sentence-transformers>","<p>Im pretraining trasformer with my own unlabeled data like this:</p>
<p><code>python train_mlm.py sentence-transformers/LaBSE train.txt</code>
Based on <a href=""https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/MLM"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers/tree/master/examples/unsupervised_learning/MLM</a></p>
<p>Then I want to get embeddings for setnences. Code:</p>
<pre><code>model = AutoModelForMaskedLM.from_pretrained('output/sentence-transformers_LaBSE-2021-12-28_13-03-20')
tokenizer = AutoTokenizer.from_pretrained('output/sentence-transformers_LaBSE-2021-12-28_13-03-20')

model = model.eval()

english_sentences = [
    &quot;dog&quot;,
    &quot;Puppies are nice.&quot;,
    &quot;I enjoy taking long walks along the beach with my dog.&quot;,
]
encoded_input = tokenizer(english_sentences, padding=True, truncation=True, max_length=64, return_tensors='pt')
with torch.no_grad():
    model_output = model(**encoded_input)

print(model_output[0].shape)
</code></pre>
<p>Problem is that shape of my output is someting like (3, 14, 500 000).
When without training on my data shape is (3, 14, 768). What I have done wrong? How can I get final embeddings after my training?</p>
","transformer-model"
"70482540","What happens if optimal training loss is too high","2021-12-25 20:22:20","","0","1985","<pytorch><huggingface-transformers><transformer-model><gpt-2><trainingloss>","<p>I am training a Transformer. In many of my setups I obtain validation and training loss that look like this:</p>
<p><a href=""https://i.sstatic.net/VVtNm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VVtNm.png"" alt=""Training and validation loss for my dataset"" /></a></p>
<p>Then, I understand that I should stop training at around epoch 1. But then the training loss is very high. Is this a problem? Does the value of training loss actually mean anything?</p>
<p>Thanks</p>
","transformer-model"
"70464428","How to calculate perplexity of a sentence using huggingface masked language models?","2021-12-23 15:50:06","70482924","8","10147","<nlp><pytorch><huggingface-transformers><bert-language-model><transformer-model>","<p>I have several masked language models (mainly Bert, Roberta, Albert, Electra). I also have a dataset of sentences. How can I get the perplexity of each sentence?</p>
<p>From the huggingface documentation <a href=""https://huggingface.co/docs/transformers/perplexity"" rel=""noreferrer"">here</a> they mentioned that perplexity &quot;is not well defined for masked language models like BERT&quot;, though I still see people somehow calculate it.</p>
<p>For example in this <a href=""https://stackoverflow.com/questions/63030692/how-do-i-use-bertformaskedlm-or-bertmodel-to-calculate-perplexity-of-a-sentence"">SO</a> question they calculated it using the function</p>
<pre><code>def score(model, tokenizer, sentence,  mask_token_id=103):
  tensor_input = tokenizer.encode(sentence, return_tensors='pt')
  repeat_input = tensor_input.repeat(tensor_input.size(-1)-2, 1)
  mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]
  masked_input = repeat_input.masked_fill(mask == 1, 103)
  labels = repeat_input.masked_fill( masked_input != 103, -100)
  loss,_ = model(masked_input, masked_lm_labels=labels)
  result = np.exp(loss.item())
  return result

score(model, tokenizer, '我爱你') # returns 45.63794545581973
</code></pre>
<p>However, when I try to use the code I get <code>TypeError: forward() got an unexpected keyword argument 'masked_lm_labels'</code>.</p>
<p>I tried it with a couple of my models:</p>
<pre><code>from transformers import pipeline, BertForMaskedLM, BertForMaskedLM, AutoTokenizer, RobertaForMaskedLM, AlbertForMaskedLM, ElectraForMaskedLM
import torch

1)
tokenizer = AutoTokenizer.from_pretrained(&quot;bioformers/bioformer-cased-v1.0&quot;)
model = BertForMaskedLM.from_pretrained(&quot;bioformers/bioformer-cased-v1.0&quot;)
2)
tokenizer = AutoTokenizer.from_pretrained(&quot;sultan/BioM-ELECTRA-Large-Generator&quot;)
model = ElectraForMaskedLM.from_pretrained(&quot;sultan/BioM-ELECTRA-Large-Generator&quot;)
</code></pre>
<p><a href=""https://stackoverflow.com/questions/61470768/how-does-masked-lm-labels-argument-work-in-bertformaskedlm"">This</a> SO question also used the <code>masked_lm_labels</code> as an input and it seemed to work somehow.</p>
","transformer-model"
"70457030","Does Keras official sample code about Transformer applied in time-series contain Position Embedding part?","2021-12-23 02:13:42","","1","348","<keras><time-series><transformer-model>","<p>The sample code for referring from url:<a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">https://keras.io/examples/timeseries/timeseries_transformer_classification/</a></p>
<p>I could not find out any description about &quot;Position Embedding&quot; content in full page of above url. When I looked through Transformer applied in NLP, I can clearly see the class named &quot;TokenAndPositionEmbedding&quot;.</p>
<p>If it does not contain &quot;Position Embedding&quot;, how can I apply Position Embedding in time series in sample code?</p>
","transformer-model"
"70433105","Position wise FeedForward of the transformer","2021-12-21 09:21:46","","1","255","<nlp><transformer-model>","<p>How is dimensionality the inner-layer (dff) ,in Position wise FeedForward of the transformer determined?</p>
","transformer-model"
"70429738","Set Dropout to be non-zero in Vision Transformer model","2021-12-21 01:28:31","","0","1013","<pytorch><computer-vision><transformer-model><dropout><overfitting-underfitting>","<p>I am using a Vision Transformer model to do image classification. I am importing
<code>model_ft = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)</code></p>
<p>Once the model is loaded I print the model to see the different layers and I get :</p>
<pre><code>(patch_embed): PatchEmbed(
(proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
(norm): Identity()
)
(pos_drop): Dropout(p=0.5, inplace=True)
(blocks): Sequential(
(0): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(1): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(2): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(3): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(4): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(5): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(6): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(7): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(8): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(9): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(10): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
(11): Block(
(norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(attn): Attention(
(qkv): Linear(in_features=768, out_features=2304, bias=True)
(attn_drop): Dropout(p=0.0, inplace=False)
(proj): Linear(in_features=768, out_features=768, bias=True)
(proj_drop): Dropout(p=0.0, inplace=False)
)
(drop_path): Identity()
(norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(mlp): Mlp(
(fc1): Linear(in_features=768, out_features=3072, bias=True)
(act): GELU()
(fc2): Linear(in_features=3072, out_features=768, bias=True)
(drop): Dropout(p=0.0, inplace=False)
)
)
)
(norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
(pre_logits): Identity()
(head): Linear(in_features=768, out_features=2, bias=True)
</code></pre>
<p>**I want to set dropout to be 0.5 in all the different layers. Starting from the first layer when I do : <code>model_ft._modules[&quot;pos_drop&quot;] = nn.Dropout(0.5, inplace=True)</code>, it works for the first instance of the dropout but when I want to do the same thing for the second dropout and I try <code>model_ft._modules[&quot;blocks&quot;].attn.proj_drop = nn.Dropout(0.5, inplace=True)</code>, it throws an error.</p>
<p>The real problem is that I don’t know how to access the dropout layers in the network and set them all to non-zero values. I need to know how to index the different layers which have Dropout to option and set them to non-zero values.</p>
<p>I would be very thankful to you if you could help me with how to access the different layers of the model and set dropout to be true in all of them.**</p>
","transformer-model"
"70387846","Difference in Set Transformer and Standard Transformer Model?","2021-12-17 02:43:03","70887375","1","399","<machine-learning><artificial-intelligence><transformer-model>","<p>The advantage of a set transformer is the ability to handle variable-size inputs. However, I thought a regular transformer would be able to do the same thing. What is the difference between these two models and why should you use one over the other?</p>
<p>Does the set transformer not require positional encoding? Is it just more modular and easier to pick what piece you want to use?</p>
<p>For reference here is the set transformer paper and code</p>
<p><a href=""https://arxiv.org/pdf/1810.00825v3.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1810.00825v3.pdf</a></p>
<p><a href=""https://github.com/arrigonialberto86/set_transformer"" rel=""nofollow noreferrer"">https://github.com/arrigonialberto86/set_transformer</a></p>
","transformer-model"
"70374023","How to imply swin transformer to arbitrary image size? ( tensorflow, keras )","2021-12-16 04:59:21","","1","741","<tensorflow><keras><transformer-model>","<p>I am studying about image swin transformer(<a href=""https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf"" rel=""nofollow noreferrer"">https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf</a>) ,</p>
<p>and mainly try to follow swinIR(<a href=""https://arxiv.org/pdf/2108.10257.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2108.10257.pdf</a>) that make super-resolution-ed image using swin transformer.</p>
<p>However, The examples and code I found only describes in the case that the image has fixed size(224 x 224 x 3 e.g.) And I cannot understand how the transformer block is adjusted for arbitrary sized image inputs. I am mainly referring the author's github <a href=""https://github.com/JingyunLiang/SwinIR/blob/main/models/network_swinir.py"" rel=""nofollow noreferrer"">https://github.com/JingyunLiang/SwinIR/blob/main/models/network_swinir.py</a></p>
<p>Please somebody tell me how and thank you very much .</p>
","transformer-model"
"70347968","Explanation of dimension of input in a transformer model","2021-12-14 11:15:42","","0","1547","<python><machine-learning><artificial-intelligence><transformer-model><attention-model>","<p><a href=""https://i.sstatic.net/phGpE.png"" rel=""nofollow noreferrer"">Model Architechture</a></p>
<p>Hey,
I was trying to code the Transformer architecture from scratch as a part of my project. I wanted to ask that, what does the batch-size refer to in the dimension of the of the input matrix of the encoder.</p>
<p>I have us the attached model architecture as my reference. Let me consider the example of an essay document. The <code>seq-len</code> refers to the maximum length of the sentence inside the document and the <code>embedding-size</code> refers to the size of the word embedding of each word of the sentence. In that case does <code>batch-size</code> refer to the number of sentences in the document ?</p>
","transformer-model"
"70319286","Question about Google Colab Transformer Tutorial","2021-12-11 21:43:23","","0","285","<tensorflow><google-colaboratory><transformer-model>","<p>I'm trying to follow the Tensorflow Transformer tutorial here:</p>
<p><a href=""https://github.com/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb</a></p>
<p>In the tutorial, they reproduce the image of the Transformer model from the original &quot;Attention is All You Need&quot; paper. In the image the final layers of the Transformer model are a Dense layer followed by Softmax Activation. However in the code I only see something like this:</p>
<p><code>self.final_layer = tf.keras.layers.Dense(target_vocab_size)</code></p>
<p>where the Dense layer is defined. But I cannot find the Softmax Activation applied anywhere in the tutorial.</p>
<p>What am I missing? Thanks in advance for your assistance.</p>
","transformer-model"
"70308466","Why are weight matrices shared between embedding layers in 'Attention is All You Need' paper?","2021-12-10 17:42:51","","3","964","<nlp><pytorch><word-embedding><transformer-model>","<p>I am using the Transformer module in pytorch from the paper &quot;Attention is All You Need&quot;. On page 5, the authors state that</p>
<blockquote>
<p>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. (page 5)</p>
</blockquote>
<p>The embedding layer, at least in pytorch, is a learnable tensor whose columns are the embedding vectors corresponding to each word. My confusion stems from the fact that in the paper, the Transformer learns a translation task between languages (i.e. English to German). <strong>Thus, how could the embedding weights be shared for the English and German embedding vectors?</strong></p>
<p><strong>In addition, how could the weights be shared between the output embedding (which goes from word index to embedding vector) and the linear layer (which goes from embedding vector to word probabilities)?</strong> As far as I can tell there is no constraint requiring the embedding tensor must be orthogonal (so that its inverse is its transpose).</p>
","transformer-model"
"70298747","why are new dim added when coding ""padding mask""","2021-12-10 01:35:19","","0","114","<tensorflow2.0><transformer-model>","<p><a href=""https://www.tensorflow.org/text/tutorials/transformer?hl=en"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer?hl=en</a>
In the Mask part of tf's official document &quot;Transformer model for language understanding&quot;,why newaix should be added and why must be added here?</p>
<pre><code>def create_padding_mask(seq):
  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)

  return seq[:, tf.newaxis, tf.newaxis, :] 
</code></pre>
","transformer-model"
"70275016","Split torch dataset without shuffling","2021-12-08 12:27:55","70278974","6","4444","<python><pytorch><torch><transformer-model><pytorch-dataloader>","<p>I'm using <code>Pytorch</code> to run Transformer model. when I want to split data (tokenized data) i'm using this code:</p>
<pre><code>train_dataset, test_dataset = torch.utils.data.random_split(
                                                            tokenized_datasets,
                                                            [train_size, test_size])
</code></pre>
<p><code>torch.utils.data.random_split</code> using shuffling method, but I don't want to shuffle. I want to split it sequentially.</p>
<p>Any advice? thanks</p>
","transformer-model"
"70271797","How to implement hierarchical Transformer for document classification in Keras?","2021-12-08 08:13:49","","2","807","<keras><deep-learning><nlp><transformer-model><self-attention>","<p>Hierarchical attention mechanism for document classification has been presented by Yang et al.
<a href=""https://www.cs.cmu.edu/%7E./hovy/papers/16HLT-hierarchical-attention-networks.pdf"" rel=""nofollow noreferrer"">https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf</a></p>
<p>Its implementation is available on <a href=""https://github.com/ShawnyXiao/TextClassification-Keras"" rel=""nofollow noreferrer"">https://github.com/ShawnyXiao/TextClassification-Keras</a></p>
<p>Also, the implementation of the document classification with Transformer is available on <a href=""https://keras.io/examples/nlp/text_classification_with_transformer"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/text_classification_with_transformer</a></p>
<p>But, it's not hierarchical.</p>
<p>I have googled a lot but didn't find any implementation of a hierarchical Transformer. Does anyone know how to implement a hierarchical transformer for document classification in Keras?</p>
<p>My implementation is as follows. Note that the implementation extended from Nandan implementation for document classification. <a href=""https://keras.io/examples/nlp/text_classification_with_transformer"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/text_classification_with_transformer</a>.</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras.utils.np_utils import to_categorical


class MultiHeadSelfAttention(layers.Layer):
    def __init__(self, embed_dim, num_heads=8):
        super(MultiHeadSelfAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        if embed_dim % num_heads != 0:
            raise ValueError(
                f&quot;embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}&quot;
            )
        self.projection_dim = embed_dim // num_heads
        self.query_dense = layers.Dense(embed_dim)
        self.key_dense = layers.Dense(embed_dim)
        self.value_dense = layers.Dense(embed_dim)
        self.combine_heads = layers.Dense(embed_dim)

    def attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output, weights

    def separate_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        # x.shape = [batch_size, seq_len, embedding_dim]
        batch_size = tf.shape(inputs)[0]
        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)
        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)
        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)
        query = self.separate_heads(
            query, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        key = self.separate_heads(
            key, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        value = self.separate_heads(
            value, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        attention, weights = self.attention(query, key, value)
        attention = tf.transpose(
            attention, perm=[0, 2, 1, 3]
        )  # (batch_size, seq_len, num_heads, projection_dim)
        concat_attention = tf.reshape(
            attention, (batch_size, -1, self.embed_dim)
        )  # (batch_size, seq_len, embed_dim)
        output = self.combine_heads(
            concat_attention
        )  # (batch_size, seq_len, embed_dim)
        return output

    def compute_output_shape(self, input_shape):
        # it does not change the shape of its input
        return input_shape


class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate, name=None):
        super(TransformerBlock, self).__init__(name=name)
        self.att = MultiHeadSelfAttention(embed_dim, num_heads)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation=&quot;relu&quot;), layers.Dense(embed_dim), ]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(dropout_rate)
        self.dropout2 = layers.Dropout(dropout_rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

    def compute_output_shape(self, input_shape):
        # it does not change the shape of its input
        return input_shape


class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim, name=None):
        super(TokenAndPositionEmbedding, self).__init__(name=name)
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

    def compute_output_shape(self, input_shape):
        # it changes the shape from (batch_size, maxlen) to (batch_size, maxlen, embed_dim)
        return input_shape + (self.pos_emb.output_dim,)



# Lower level (produce a representation of each sentence):

embed_dim = 100  # Embedding size for each token
num_heads = 2  # Number of attention heads
ff_dim = 64  # Hidden layer size in feed forward network inside transformer
L1_dense_units = 100  # Size of the sentence-level representations output by the word-level model
dropout_rate = 0.1
vocab_size = 1000
class_number = 5
max_docs = 10000
max_sentences = 15
max_words = 60

word_input = layers.Input(shape=(max_words,), name='word_input')
word_embedding = TokenAndPositionEmbedding(maxlen=max_words, vocab_size=vocab_size,
                                           embed_dim=embed_dim, name='word_embedding')(word_input)
word_transformer = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim,
                                    dropout_rate=dropout_rate, name='word_transformer')(word_embedding)
word_pool = layers.GlobalAveragePooling1D(name='word_pooling')(word_transformer)
word_drop = layers.Dropout(dropout_rate, name='word_drop')(word_pool)
word_dense = layers.Dense(L1_dense_units, activation=&quot;relu&quot;, name='word_dense')(word_drop)
word_encoder = keras.Model(word_input, word_dense)

word_encoder.summary()

# =========================================================================
# Upper level (produce a representation of each document):

L2_dense_units = 100

sentence_input = layers.Input(shape=(max_sentences, max_words), name='sentence_input')

sentence_encoder = tf.keras.layers.TimeDistributed(word_encoder, name='sentence_encoder')(sentence_input)

sentence_transformer = TransformerBlock(embed_dim=L1_dense_units, num_heads=num_heads, ff_dim=ff_dim,
                               dropout_rate=dropout_rate, name='sentence_transformer')(sentence_encoder)
sentence_pool = layers.GlobalAveragePooling1D(name='sentence_pooling')(sentence_transformer)
sentence_out = layers.Dropout(dropout_rate)(sentence_pool)
preds = layers.Dense(class_number , activation='softmax', name='sentence_output')(sentence_out)

model = keras.Model(sentence_input, preds)
model.summary()
</code></pre>
<p>The summary of the model is as follows:</p>
<pre><code>Model: &quot;model_1&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 word_input (InputLayer)     [(None, 60)]              0         
                                                                 
 word_embedding (TokenAndPos  (None, 60, 100)          106000    
 itionEmbedding)                                                 
                                                                 
 word_transformer (Transform  (None, 60, 100)          53764     
 erBlock)                                                        
                                                                 
 word_pooling (GlobalAverage  (None, 100)              0         
 Pooling1D)                                                      
                                                                 
 word_drop (Dropout)         (None, 100)               0         
                                                                 
 word_dense (Dense)          (None, 100)               10100     
                                                                 
=================================================================
Total params: 169,864
Trainable params: 169,864
Non-trainable params: 0
_________________________________________________________________
Model: &quot;model_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 sentence_input (InputLayer)  [(None, 15, 60)]         0         
                                                                 
 sentence_encoder (TimeDistr  (None, 15, 100)          169864    
 ibuted)                                                         
                                                                 
 sentence_transformer (Trans  (None, 15, 100)          53764     
 formerBlock)                                                    
                                                                 
 sentence_pooling (GlobalAve  (None, 100)              0         
 ragePooling1D)                                                  
                                                                 
 dropout_9 (Dropout)         (None, 100)               0         
                                                                 
 sentence_output (Dense)     (None, 5)                 505       
                                                                 
=================================================================
Total params: 224,133
Trainable params: 224,133
Non-trainable params: 0
</code></pre>
<p>Everything is ok and you can copy and paste these codes in colab to see the summary of the model.
But, my problem is for positional encoding at the sentence level.
How to apply positional encoding at the sentence level?</p>
","transformer-model"
"70241625","NameError: name 'util' is not defined in sentence bert","2021-12-06 06:32:13","","0","53","<python><bert-language-model><transformer-model>","<p>after installing sentence bert with following command</p>
<pre><code>!pip install -U sentence-transformers
</code></pre>
<p>I used this code</p>
<pre><code>util.http_get('http://sbert.net/datasets/simplewiki-2020-11-01.jsonl.gz', wikipedia_filepath)
</code></pre>
<p>getting following error</p>
<p>NameError: name 'util' is not defined</p>
","transformer-model"
"70157962","SHAP Error - OM when allocating tensor with shape[23020,128,768]","2021-11-29 16:27:47","","0","170","<nlp><bert-language-model><transformer-model><shap>","<p>I am trying to get shap values using  shap.KernelExplainer for a bert classifier implemented using keras layers. The error that I get comes because there is not enough memory. But I am not sure what it is causing it because I have reduced all the parameters to as much as I can.</p>
<p><strong>Error:</strong></p>
<blockquote>
<p>ResourceExhaustedError: OOM when allocating tensor with shape[23020,128,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Tile]</p>
</blockquote>
<p><strong>Transformer Model:</strong></p>
<pre><code>def process_sentences(sentence: List[str],
                      tokenizer: PreTrainedTokenizer,
                      max_len: int) -&gt; Dict[str, np.ndarray]:
    &quot;&quot;&quot;
    Tokenize the text sentences.

    Parameters
    ----------
    sentence:
        Sentence to be processed.
    tokenizer:
        Tokenizer to be used.

    Returns
    -------
        Tokenized representation containing:
         - input_ids
         - attention_mask
    &quot;&quot;&quot;
    # since we are using the model for classification, we need to include special char (i.e, '[CLS]', ''[SEP]')
    # check the example here: https://huggingface.co/transformers/v4.4.2/quicktour.html
    z = tokenizer(sentence,
                  add_special_tokens=True,
                  padding='max_length',
                  max_length=max_len,
                  truncation=True,
                  return_attention_mask = True,
                  return_tensors='np')
    return z;


use_bert = True

if use_bert:
    from transformers import BertTokenizerFast
    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')
else:
    from transformers import DistilBertTokenizerFast
    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')


if use_bert:
    from transformers import TFBertModel, BertConfig
    config = BertConfig(output_hidden_states=True)
    transformer = TFBertModel.from_pretrained('bert-base-uncased', config=config)
else:
    from transformers import TFDistilBertModel, DistilBertConfig
    config = DistilBertConfig(output_hidden_states=True)
    transformer = TFDistilBertModel.from_pretrained('distilbert-base-uncased', config=config)

X_train = train.clean_review.values.tolist()
X_test=test.clean_review.values.tolist()


# tokenize datasets
X_train = process_sentences(X_train, tokenizer, max_len)
X_test = process_sentences(X_test, tokenizer, max_len)

y_train = y_train.rate
y_test = y_test.rate


class Classifier(tf.keras.Model):
    def __init__(self,
                 transformer,
                 hidden_dims: int = 128,
                 output_dims: int = 2,
                 dropout_rate: float = 0.2):
        &quot;&quot;&quot;
        Constructor

        Parameters
        ----------
        transformer:
            Transformer model to be leveraged.
        hidden_dims:
            hidden layer's dimension.
        output_dims:
            Output layer's dimension.
        dropout_rate:
            Dropout layer's dropout rate.
        &quot;&quot;&quot;
        super().__init__()
        self.hidden_dims = hidden_dims
        self.output_dims = output_dims
        self.dropout_rate = dropout_rate

        self.transformer = transformer
        self.dense_1 = tf.keras.layers.Dense(self.hidden_dims, activation='relu')
        self.dropout_1 = tf.keras.layers.Dropout(self.dropout_rate)
        self.dense_2 = tf.keras.layers.Dense(self.output_dims, activation='softmax')

    def call(self,
             input_ids: Union[np.ndarray, tf.Tensor],
             attention_mask: Optional[Union[np.ndarray, tf.Tensor]]=None,
             training=False):
        &quot;&quot;&quot;
        Performs forward pass throguh the model.

        Parameters
        ----------
        input_ids:
            Indices of input sequence tokens in the vocabulary.
        attention_mask:
            Mask to avoid performing attention on padding token indices.

        Returns
        -------
            Classification probabilities.
        &quot;&quot;&quot;
        out = self.transformer(input_ids=input_ids, attention_mask=attention_mask, training=training)
        out = out.last_hidden_state[:, 0, :]  # extract the embedding corresponding to [CLS] token
        out = self.dense_1(out)
        out = self.dropout_1(out, training=training)
        out = self.dense_2(out)
        return out
# define the classification model
model = Classifier(transformer)
</code></pre>
<p><strong>ShAP:</strong></p>
<pre><code>
##background

X_train=train.clean_review.values.tolist()[:10]
X_train = process_sentences(X_train, tokenizer, max_len)

# tokenize text
tokenized_samples = X_train
X_train = tokenized_samples['input_ids']

# the values of the kwargs have to be `tf.Tensor`.
# see transformers issue #14404: https://github.com/huggingface/transformers/issues/14404
kwargs_train = {k:tf.constant(v) for k, v in tokenized_samples.items() if k == 'attention_mask'}

##SAMPLE
X_test=test.clean_review.values.tolist()[:3]
X_test = process_sentences(X_test, tokenizer, max_len)

# tokenize text
tokenized_samples = X_test
X_test = tokenized_samples['input_ids']

# the values of the kwargs have to be `tf.Tensor`.
# see transformers issue #14404: https://github.com/huggingface/transformers/issues/14404
kwargs_test = {k:tf.constant(v) for k, v in tokenized_samples.items() if k == 'attention_mask'}
kernel_explainer = shap.KernelExplainer(model, X_train)
kernel_shap_values = kernel_explainer.shap_values(X_test)
</code></pre>
","transformer-model"
"70146811","Feed decoder input in transformers","2021-11-28 19:35:51","70151330","0","998","<python><tensorflow><machine-learning><nlp><transformer-model>","<p>Reading <a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">this tutorial</a> on how to implement an Encoder/Decoder transformer I had some doubts on the training process. Specifically as reported by the original <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">paper</a> the decoder should iteratively use the last iteration output as input of the decoder. However the training step is implemented as</p>
<pre><code>@tf.function(input_signature=train_step_signature)
def train_step(inp, tar):
  tar_inp = tar[:, :-1]
  tar_real = tar[:, 1:]

  with tf.GradientTape() as tape:
    predictions, _ = transformer([inp, tar_inp],
                                 training = True)
    loss = loss_function(tar_real, predictions)

  gradients = tape.gradient(loss, transformer.trainable_variables)
  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
</code></pre>
<p>Where <code>tar_inp</code> is simply a tokenized sentence without the EOS token and <code>tar_real</code> is the same sentence shifted by one position.</p>
<p>However I would have expected the target input (the decoder input) to be iteratively concatenated by previous prediction (or in teacher-forced by incrementing by one ground truth token at a time).</p>
<p>Why is it not the case?</p>
","transformer-model"
"70146075","Error message when trying to use huggingface pretrained Tokenizer (roberta-base)","2021-11-28 17:58:35","","0","626","<tokenize><huggingface-transformers><transformer-model><huggingface-tokenizers><roberta>","<p>I am pretty new at this, so there might be something I am missing completely, but here is my problem: I am trying to create a Tokenizer class that uses the pretrained tokenizer models from Huggingface. I would then like to use this class in a larger transformer model to tokenize my input data. Here is the class code
class Roberta(MyTokenizer):</p>
<pre><code>from transformers import AutoTokenizer
from transformers import RobertaTokenizer


class Roberta(MyTokenizer):

def build(self, *args, **kwargs):
    self.max_length = self.phd.max_length
    self.untokenized_data = self.questions + self.answers

def tokenize_and_filter(self):
    # Initialize the tokenizer with a pretrained model
    Tokenizer = AutoTokenizer.from_pretrained('roberta')

    tokenized_inputs, tokenized_outputs = [], []

    inputs = Tokenizer(self.questions, padding=True)
    outputs = Tokenizer(self.answers, padding=True)

    tokenized_inputs = inputs['input_ids']
    tokenized_outputs = outputs['input_ids']

    return tokenized_inputs, tokenized_outputs
</code></pre>
<p>When I call the function tokenize_and_filter in my Transformer model as below</p>
<pre><code>    questions = self.get_tokenizer().tokenize_and_filter
    answers   = self.get_tokenizer().tokenize_and_filter

    print(questions)
</code></pre>
<p>and I try to print the tokenized data, I get this message:</p>
<pre><code>&lt;bound method Roberta.tokenize_and_filter of &lt;MyTokenizer.Roberta.Roberta object at 
      0x000002779A9E4D30&gt;&gt;
</code></pre>
<p>It appears that the function returns a method instead of a list or a tensor - I've tried passing the parameter 'return_tensors='tf'', I have tried using the tokenizer.encode() method, I have tried both with AutoTokenizer and with RobertaTokenizer, I have tried the batch_encode_plus() method, nothing seems to work.</p>
<p>Please help!</p>
","transformer-model"
"70139384","Train transformer model with low performance","2021-11-27 23:00:57","","0","255","<python><gpu><translation><huggingface-transformers><transformer-model>","<p>I am following this <a href=""https://huggingface.co/course/chapter7/4?"" rel=""nofollow noreferrer"">tutorial</a> to try to train data to translate a language, however using only my cpu, I have to wait 5 hours to train the data (and after these 5 hours the process is killed because it uses too many resources). Here is my configuration:</p>
<ul>
<li>Operating System: Ubuntu 20.04</li>
<li>OS Type: 64-bit</li>
<li>Processors: 4 × Intel® Core™ i5-7500T CPU @ 2.70GHz</li>
<li>Memory: 7,5 Gio</li>
<li>GPU:Intel HD Graphics 630</li>
</ul>
<p>so i tried to see how i can use my gpu to train my model, but unfortunately pytorch only supports cuda, but i saw this <a href=""https://stackoverflow.com/questions/64593792/how-to-make-intel-gpu-available-for-processing-through-pytorch"">post</a> that referred to this <a href=""https://www.intel.com/content/www/us/en/developer/articles/guide/getting-started-with-intel-optimization-of-pytorch.html"" rel=""nofollow noreferrer"">tool</a>, however after installing (I have do <code>python -m pip install torch_ipex==1.9.0 -f https://software.intel.com/ipex-whl-stable</code>) it, i get this error:</p>
<pre><code>ImportError: /home/USER/.local/lib/python3.8/site-packages/torch_ipex/lib/libtorch_ipex.so: undefined symbol: _ZN2at20hinge_embedding_lossERKNS_6TensorES2_dl
</code></pre>
<p>You can help me to solve this error or give me the link of a pre-entrainer dataset containing several languages that makes accurate translations (using t5, the quality is not incredible, for example it translates &quot;hello world&quot; to &quot;bonjour monde&quot; in French (the good translation is &quot;bonjour le monde&quot;) :D.</p>
","transformer-model"
"70067608","How padding in huggingface tokenizer works?","2021-11-22 14:43:54","70071376","7","16224","<nlp><huggingface-transformers><bert-language-model><transformer-model><huggingface-tokenizers>","<p>I tried following tokenization example:</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True)
sent = &quot;I hate this. Not that.&quot;,        
_tokenized = tokenizer(sent, padding=True, max_length=20, truncation=True)
print(_tknzr.decode(_tokenized['input_ids'][0]))
print(len(_tokenized['input_ids'][0]))
</code></pre>
<p>The output was:</p>
<pre><code>[CLS] i hate this. not that. [SEP]
9
</code></pre>
<p>Notice the parameter to <code>tokenizer</code>: <code>max_length=20</code>. How can I make Bert tokenizer to append 11 <code>[PAD]</code> tokens to this sentence to make it total <code>20</code>?</p>
","transformer-model"
"70057975","How to get cosine similarity of word embedding from BERT model","2021-11-21 19:39:24","70058237","5","13471","<python><bert-language-model><word-embedding><transformer-model>","<p>I was interesting in how to get the similarity of word embedding in different sentences from BERT model (actually, that means words have different meanings in different scenarios).</p>
<p>For example:</p>
<pre><code>sent1 = 'I like living in New York.'
sent2 = 'New York is a prosperous city.'
</code></pre>
<p>I want to get the cos(New York, New York)'s value from sent1 and sent2, even if the phrase 'New York' is same, but it appears in different sentence. I got some intuition from <a href=""https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958/2"" rel=""noreferrer"">https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958/2</a></p>
<p>But I still do not know which layer's embedding I need to extract and how to caculate the cos similarity for my above example.</p>
<p>Thanks in advance for any suggestions!</p>
","transformer-model"
"69910101","Getting Model from DB query instead of StdClass","2021-11-10 08:42:19","69910158","1","948","<laravel><eloquent><model><transformer-model>","<p>Using latest Laravel, I have written next query:</p>
<pre><code>        $news = DB::table('news_content')
            -&gt;join('news_permissions', 'news_content.id', '=', 'news_permissions.news_id')
            -&gt;select('news_content.*')
            -&gt;whereIn('news_permissions.group_id', $allPGs)
            -&gt;get();
</code></pre>
<p>Using Eloquent with join seems alot of complication.
This query returns array of StdClasses, but I would need array of Models, namely NewsContent Models for using it further in Transformer.</p>
<p>Casting like:</p>
<pre><code>    foreach($news as $item){
            $result[] = (NewsContent)$item;
    }
</code></pre>
<p>doesnt work, other solutions with custom casting from StdClass to Model seem not optimal.</p>
<p>Is there a better way to get Model out of Laravels DB query? Or shorter casting procedure from StdClass to Model than this suggestions: <a href=""https://stackoverflow.com/questions/3243900/convert-cast-an-stdclass-object-to-another-class"">Convert/cast an stdClass object to another class</a>?</p>
","transformer-model"
"69894941","LTspice Transformer issues","2021-11-09 08:29:49","","0","318","<transformer-model><pspice>","<p>I'm struggling with this simple schematic in LTspice.
I just want to made a 1:1 insulation transformer but probably I'm missing something.
In theory it should works, the inductor are identical and the voltage is imposed from the primary V1. The power should be converted so V1 * I1 should be equal V2 * I2, but it doesn't happen.
What am I missing?</p>
<p>Thanks!</p>
<p><a href=""https://i.sstatic.net/4H414.jpg"" rel=""nofollow noreferrer"">LTspice simulation</a></p>
","transformer-model"
"69887535","Question about tokens used in Transformer decoder attention layers during Inference","2021-11-08 17:25:45","","1","1392","<matrix-multiplication><huggingface-transformers><transformer-model><attention-model><self-attention>","<p>I was looking at the shapes used during decoder (both self-attention and enc-dec-attention blocks) and understand there is a difference in the way decoder runs during training versus during inference based <a href=""https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452"" rel=""nofollow noreferrer"">on this link</a> and the original <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention paper</a></p>
<p>In Inference, it uses all previous tokens generated until that time step (say <code>k</code>th time-step), as shown in the diagram below and explained at <a href=""https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452"" rel=""nofollow noreferrer"">this link.</a></p>
<p><a href=""https://i.sstatic.net/0Lt8b.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0Lt8b.png"" alt="""" /></a></p>
<p><strong>Issue:</strong></p>
<p>However when I look at actual shapes of the QKV projection in the decoder self-attention, and feeding of the decoder self-attention output to the &quot;enc-dec-attention&quot;'s Q matrix, I see only 1 token from the output being used.</p>
<p>I'm very confused how the shapes for all matrices in the <strong>Decoder's</strong> self-attention and enc-dec-attention can match up with variable length of input to the decoder during inference. I looked at several online material but couldn't find answer.
I see only the BGemms in the decoder's self-attention (not enc-dec-attention) using the variable shapes until all previous <code>k</code> steps, but all other Gemms are fixed size.</p>
<ul>
<li>How is that possible? Is only 1 token (last one from decoder output) is being used for qkv matmuls in self-attention and Q-matmul in enc-dec-attention (which is what I see when running the model)?</li>
<li>Could someone elaborate how all these shapes for QKV in self-attention and Q in enc-dec-attention match up with decoder input length being different at each time-step?**</li>
</ul>
<p>Another diagram that shows self-attention and enc-dec-attention within decoder:</p>
<p><a href=""https://i.sstatic.net/BbCxU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BbCxU.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"69820065","how to extend a pretrained transformer model configured with small max_position_embeddings to a longer one","2021-11-03 05:31:46","","1","914","<pytorch><huggingface-transformers><bert-language-model><transformer-model><huggingface-tokenizers>","<p>suppose I want to use the existing pre-trained model.
<a href=""https://huggingface.co/Salesforce/grappa_large_jnt/"" rel=""nofollow noreferrer"">https://huggingface.co/Salesforce/grappa_large_jnt/</a>
as the initial checkpoint for finetuning.</p>
<p>This grappa model has max position embedding as 514 in config.json</p>
<blockquote>
<p>&quot;max_position_embeddings&quot;: 514,</p>
</blockquote>
<p>Now I want to extend this model from 514 to 1024 tokens. The first 0-513 embeddings are initialized with the pre-trained model, the rest (514-1023) are randomly initialized.</p>
<p>How to archieve this?</p>
","transformer-model"
"69778483","Converting from PyTorch to Tensorflow for Self-Attention Pooling Layer","2021-10-30 09:49:38","","1","1117","<tensorflow><deep-learning><pytorch><transformer-model><code-translation>","<p>I have found an implementation of the said layer from this paper, &quot;Self-Attention Encoding and Pooling for Speaker Recognition&quot;, available at <a href=""https://arxiv.org/abs/2008.01077"" rel=""nofollow noreferrer"">here</a> via Pytorch. However, due to CUDA compatibility issues, I can't want to use the said code. Also, thus far, all my codes have been implemented in Tensorflow. So, I want to do a one-to-one translation/conversion or whatever, from PyTorch to Tensorflow.</p>
<p>First of all, this is the code in PyTorch:</p>
<pre><code>class SelfAttentionPooling(nn.Module):
    def __init__(self, input_dim):
        super(SelfAttentionPooling, self).__init__()
        self.W = nn.Linear(input_dim, 1)
    
    def forward(self, batch_rep):
        &quot;&quot;&quot;
        input:
            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension
      
        attention_weight:
            att_w : size (N, T, 1)
    
        return:
            utter_rep: size (N, H)
        &quot;&quot;&quot;
        softmax = nn.functional.softmax
        att_w = softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)
        utter_rep = torch.sum(batch_rep * att_w, dim=1)

        return utter_rep
</code></pre>
<p>And this is my translation of the snippet code to Tensorflow:</p>
<pre><code>class Self_Attention_Pooling(keras.layers.Layer): ?
    def __init__(self, input_dim):
        super(Self_Attention_Pooling, self).__init__()

        self.W = Dense(input_dim)

    def forward(self, batch_rep):
        softmax = Softmax()
        att_w = self.W(batch_rep)
        att_w = softmax(att_w)
        
        # Not so sure about these two lines though.
        #x = np.expand(batch_rep)
        #att_w = softmax(self.W(x))

        utter_rep = np.sum(batch_rep * att_w, axis=1)

        return utter_rep
</code></pre>
<p>Is my implementation/translation/conversion from PyTorch to Tensorflow correct? If not, please edit and help me.</p>
<p>Thank you very much.</p>
","transformer-model"
"69724009","Converting a pandas dataframe into a torch Dataset","2021-10-26 13:28:50","","2","7008","<python><pandas><pytorch><transformer-model><torchaudio>","<p>I have a pandas dataframe with the following structure:</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>path</th>
<th>sentence</th>
<th>speech</th>
<th>input_values</th>
<th>labels</th>
</tr>
</thead>
<tbody>
<tr>
<td>audio1.mp3</td>
<td>This is the first audio</td>
<td>[[0.0, 0.0, 0.0, ..., 0.0, 0.0]]</td>
<td>[[0.00005, ..., 0.0003]]</td>
<td>[23, 4, 6, 11, ..., 12</td>
</tr>
<tr>
<td>audio2.mp3</td>
<td>This is the second audio</td>
<td>[[0.0, 0.0, 0.0, ..., 0.0, 0.0]]</td>
<td>[[0.000044, ..., 0.00033]]</td>
<td>[23, 4, 6, 11, ..., 12</td>
</tr>
</tbody>
</table>
</div>
<p>The sentence is the transcription of the audio, the speech column is the array representation of the audio, and labels is the number representation of the each letter of the sentence based on a defined vocab list.</p>
<p>I'm fine-tuning a pre-trained ASR model, but when I try to pass the pandas df to the Trainer class and call <code>.train()</code> on it, it errors out (KeyError: 0). From the documentation, it only accepts <code>torch.utils.data.Dataset</code> or <code>torch.utils.data.IterableDataset</code> as train_/eval_dataset arguments. This is how my Trainer definition looks like:</p>
<pre><code>trainer = Trainer(
    model=model,
    data_collator=data_collator,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=ds_train, 
    eval_dataset=ds_test,
    tokenizer=processor.feature_extractor
)
</code></pre>
<p>ds_train and ds_test are my training and validation dataframes respectively. I just split my main dataframe (80/20). How can I convert my pandas dataframes into the required Dataset type? I tried tailoring the <code>data_collator</code> class definition to a pandas df but that predictably didn't work either. I'm assuming the train and eval datasets both call the <code>data_collator</code> class when you call <code>.train()</code> on the trainer?</p>
<p><strong>EDIT</strong>: I tried using <code>Dataset.from_pandas(ds_train)</code> but it couldn't convert it because I had columns with two-dimensional arrays and it can apparently only convert one-dimensional array values.</p>
","transformer-model"
"69720454","Questions when training language models from scratch with Huggingface","2021-10-26 09:17:46","69721327","1","1829","<python><nlp><huggingface-transformers><transformer-model><roberta>","<p>I'm following the guide here (<a href=""https://github.com/huggingface/blog/blob/master/how-to-train.md"" rel=""nofollow noreferrer"">https://github.com/huggingface/blog/blob/master/how-to-train.md</a>, <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">https://huggingface.co/blog/how-to-train</a>) to train a RoBERTa-like model from scratch. (With my own tokenizer and dataset)</p>
<p>However, when I run <strong>run_mlm.py</strong> (<a href=""https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py</a>) to train my model with masking task, the following messages appear:</p>
<pre><code>All model checkpoint weights were used when initializing RobertaForMaskedLM.

All the weights of RobertaForMaskedLM were initialized from the model checkpoint at roberta-base.

If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.
</code></pre>
<p>I'm wondering does it mean that I'm training from scratch with <strong>&quot;the pretrained weight&quot;</strong> of RoBERTa? And if it's training from the pretrained weights, is there a way to use randomly initiated weights rather than the pretrained ones?</p>
<p>==== 2021/10/26 Updated ===</p>
<p>I  am training the model with Masked Language Modeling task by following commands:</p>
<pre><code>python transformer_run_mlm.py \
--model_name_or_path roberta-base  \
--config_name ./my_dir/ \
--tokenizer_name ./my_dir/ \
--no_use_fast_tokenizer \
--train_file ./my_own_training_file.txt \
--validation_split_percentage 10 \
--line_by_line \
--output_dir /my_output_dir/ \
--do_train \
--do_eval \
--per_device_train_batch_size 64 \
--per_device_eval_batch_size 16 \
--learning_rate 1e-4 \
--max_seq_length 1024 \
--seed 42 \
--num_train_epochs 100 
</code></pre>
<p>The  <strong>./my_dir/</strong> consists of three files:</p>
<p><strong>config.json</strong> produced by the following codes:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import RobertaModel

model = RobertaModel.from_pretrained('roberta-base')
model.config.save_pretrained(MODEL_CONFIG_PATH)
</code></pre>
<p>And here's the content:</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;roberta-base&quot;,
  &quot;architectures&quot;: [
    &quot;RobertaForMaskedLM&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.1,
  &quot;bos_token_id&quot;: 0,
  &quot;classifier_dropout&quot;: null,
  &quot;eos_token_id&quot;: 2,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.1,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-05,
  &quot;max_position_embeddings&quot;: 514,
  &quot;model_type&quot;: &quot;roberta&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 1,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;transformers_version&quot;: &quot;4.12.0.dev0&quot;,
  &quot;type_vocab_size&quot;: 1,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 50265
}

</code></pre>
<p><strong>vocab.json, merges.tx</strong>t produced by the following codes:</p>
<pre class=""lang-py prettyprint-override""><code>from tokenizers.implementations import ByteLevelBPETokenizer

tokenizer = ByteLevelBPETokenizer()

tokenizer.train(files=OUTPUT_DIR + &quot;seed.txt&quot;, vocab_size=52_000, min_frequency=2, special_tokens=[
    &quot;&lt;s&gt;&quot;,
    &quot;&lt;pad&gt;&quot;,
    &quot;&lt;/s&gt;&quot;,
    &quot;&lt;unk&gt;&quot;,
    &quot;&lt;mask&gt;&quot;,
])

# Save files to disk
tokenizer.save_model(MODEL_CONFIG_PATH)
</code></pre>
<p>And here's the content of <strong>vocab.json</strong> (A proportion of)</p>
<pre><code>{&quot;&lt;s&gt;&quot;:0,&quot;&lt;pad&gt;&quot;:1,&quot;&lt;/s&gt;&quot;:2,&quot;&lt;unk&gt;&quot;:3,&quot;&lt;mask&gt;&quot;:4,&quot;!&quot;:5,&quot;\&quot;&quot;:6,&quot;#&quot;:7,&quot;$&quot;:8,&quot;%&quot;:9,&quot;&amp;&quot;:10,&quot;'&quot;:11,&quot;(&quot;:12,&quot;)&quot;:13,&quot;*&quot;:14,&quot;+&quot;:15,&quot;,&quot;:16,&quot;-&quot;:17,&quot;.&quot;:18,&quot;/&quot;:19,&quot;0&quot;:20,&quot;1&quot;:21,&quot;2&quot;:22,&quot;3&quot;:23,&quot;4&quot;:24,&quot;5&quot;:25,&quot;6&quot;:26,&quot;7&quot;:27,&quot;8&quot;:28,&quot;9&quot;:29,&quot;:&quot;:30,&quot;;&quot;:31,&quot;&lt;&quot;:32,&quot;=&quot;:33,&quot;&gt;&quot;:34,&quot;?&quot;:35,&quot;@&quot;:36,&quot;A&quot;:37,&quot;B&quot;:38,&quot;C&quot;:39,&quot;D&quot;:40,&quot;E&quot;:41,&quot;F&quot;:42,&quot;G&quot;:43,&quot;H&quot;:44,&quot;I&quot;:45,&quot;J&quot;:46,&quot;K&quot;:47,&quot;L&quot;:48,&quot;M&quot;:49,&quot;N&quot;:50,&quot;O&quot;:51,&quot;P&quot;:52,&quot;Q&quot;:53,&quot;R&quot;:54,&quot;S&quot;:55,&quot;T&quot;:56,&quot;U&quot;:57,&quot;V&quot;:58,&quot;W&quot;:59,&quot;X&quot;:60,&quot;Y&quot;:61,&quot;Z&quot;:62,&quot;[&quot;:63,&quot;\\&quot;:64,&quot;]&quot;:65,&quot;^&quot;:66,&quot;_&quot;:67,&quot;`&quot;:68,&quot;a&quot;:69,&quot;b&quot;:70,&quot;c&quot;:71,&quot;d&quot;:72,&quot;e&quot;:73,&quot;f&quot;:74,&quot;g&quot;:75,&quot;h&quot;:76,&quot;i&quot;:77,&quot;j&quot;:78,&quot;k&quot;:79,&quot;l&quot;:80,&quot;m&quot;:81,&quot;n&quot;:82,&quot;o&quot;:83,&quot;p&quot;:84,&quot;q&quot;:85,&quot;r&quot;:86,&quot;s&quot;:87,&quot;t&quot;:88,&quot;u&quot;:89,&quot;v&quot;:90,&quot;w&quot;:91,&quot;x&quot;:92,&quot;y&quot;:93,&quot;z&quot;:94,&quot;{&quot;:95,&quot;|&quot;:96,&quot;}&quot;:97,&quot;~&quot;:98,&quot;¡&quot;:99,&quot;¢&quot;:100,&quot;£&quot;:101,&quot;¤&quot;:102,&quot;¥&quot;:103,&quot;¦&quot;:104,&quot;§&quot;:105,&quot;¨&quot;:106,&quot;©&quot;:107,&quot;ª&quot;:108,&quot;«&quot;:109,&quot;¬&quot;:110,&quot;®&quot;:111,&quot;¯&quot;:112,&quot;°&quot;:113,&quot;±&quot;:114,&quot;²&quot;:115,&quot;³&quot;:116,&quot;´&quot;:117,&quot;µ&quot;:118,&quot;¶&quot;:119,&quot;·&quot;:120,&quot;¸&quot;:121,&quot;¹&quot;:122,&quot;º&quot;:123,&quot;»&quot;:124,&quot;¼&quot;:125,&quot;½&quot;:126,&quot;¾&quot;:12
</code></pre>
<p>And here's the content of <strong>merges.txt</strong> (A proportion of)</p>
<pre><code>#version: 0.2 - Trained by `huggingface/tokenizers`
e n
T o
k en
Ġ To
ĠTo ken
E R
V ER
VER B
a t
P R
PR O
P N
PRO PN
Ġ n
U N
N O
NO UN
E n
i t
t it
En tit
Entit y
b j
c o
Ġ a

</code></pre>
","transformer-model"
"69667729","how to use sentence bert with transformers and torch","2021-10-21 19:24:51","","3","1588","<nlp><huggingface-transformers><transformer-model><sentence-similarity><sentence-transformers>","<p>I would like to use <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">sentence_transformers</a><br />
But due to policy restrictions I cannot install the package sentence-transformers</p>
<p>I have transformers and torch package though.</p>
<p>I went to this <a href=""https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1"" rel=""nofollow noreferrer"">page</a> and tried to run the below code</p>
<p>Before doing that I went to the <a href=""https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1/tree/main"" rel=""nofollow noreferrer"">page</a> and downloaded all the files</p>
<pre><code>import os
path=&quot;/yz/sentence-transformers/multi-qa-mpnet-base-dot-v1/&quot; #local path where I have stored files
os.listdir(path)

['.dominokeep',
 'config.json',
 'data_config.json',
 'modules.json',
 'sentence_bert_config.json',
 'special_tokens_map.json',
 'tokenizer_config.json',
 'train_script.py',
 'vocab.txt',
 'tokenizer.json',
 'config_sentence_transformers.json',
 'README.md',
 'gitattributes',
 '9e1e76b7a067f72e49c7f571cd8e811f7a1567bec49f17e5eaaea899e7bc2c9e']
</code></pre>
<p>The code that I ran is</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
import torch

# Load model from HuggingFace Hub

path=&quot;/yz/sentence-transformers/multi-qa-mpnet-base-dot-v1/&quot;

&quot;&quot;&quot;tokenizer = AutoTokenizer.from_pretrained(&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;)
model = AutoModel.from_pretrained(&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;)&quot;&quot;&quot;

tokenizer = AutoTokenizer.from_pretrained(path)
model = AutoModel.from_pretrained(path)
</code></pre>
<p>The error that I get is as below</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-18-bb33f7c519e0&gt; in &lt;module&gt;
     32 model = AutoModel.from_pretrained(&quot;sentence-transformers/multi-qa-mpnet-base-dot-v1&quot;)&quot;&quot;&quot;
     33 
---&gt; 34 tokenizer = AutoTokenizer.from_pretrained(path)
     35 model = AutoModel.from_pretrained(path)
     36 

/usr/local/anaconda/lib/python3.6/site-packages/transformers/models/auto/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    308         config = kwargs.pop(&quot;config&quot;, None)
    309         if not isinstance(config, PretrainedConfig):
--&gt; 310             config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
    311 
    312         if &quot;bert-base-japanese&quot; in str(pretrained_model_name_or_path):

/usr/local/anaconda/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    342 
    343         if &quot;model_type&quot; in config_dict:
--&gt; 344             config_class = CONFIG_MAPPING[config_dict[&quot;model_type&quot;]]
    345             return config_class.from_dict(config_dict, **kwargs)
    346         else:

KeyError: 'mpnet'
</code></pre>
<p>my questions:</p>
<ol>
<li>How should I fix this error?</li>
<li>is there a way to use the same method for <a href=""https://huggingface.co/nreimers/MiniLM-L6-H384-uncased/tree/main"" rel=""nofollow noreferrer"">MiniLM-L6-H384-uncased</a>-
. I would like to use it as seems to be faster</li>
</ol>
<p>==============================
package versions as below -</p>
<pre><code>transformers - 4.0.0
torch - 1.4.0
</code></pre>
","transformer-model"
"69648338","Exporting PyTorch Lightning model to ONNX format not working","2021-10-20 14:54:19","","0","1990","<python-3.x><huggingface-transformers><transformer-model><pytorch-lightning>","<p>am using Jupyter Lab to run. It has pre-installed tf2.3_py3.6 kernel installed in it. It has 2 GPUS in it.</p>
<pre><code>PyTorch Lightning Version (e.g., 1.3.0): '1.4.6'
PyTorch Version (e.g., 1.8): '1.6.0+cu101'
Python version: 3.6
OS (e.g., Linux): system='Linux'
CUDA/cuDNN version: 11.2
How you installed PyTorch (conda, pip, source): pip
</code></pre>
<p>I am saving the best model in checkpoint.</p>
<p>I am doing multi-label classification using <code>Hugging face model</code>. After training the model I want to export the model using ONNX format. The input is <code>attention mask</code>, <code>input ids</code>.</p>
<p><strong>Here is the DataModule Class</strong></p>
<pre><code>N_EPOCHS = 30
BATCH_SIZE = 10

class  SRDataModule(pl.LightningDataModule):
    
    def __init__(self, X_train,y_train, X_test,y_test, tokenizer, batch_size=8, max_token_len=512):
        super().__init__()
        self.batch_size = batch_size
        self.train_df = X_train
        self.test_df = X_test
        self.train_lab = y_train
        self.test_lab = y_test
        self.tokenizer = tokenizer
        self.max_token_len = max_token_len

    def setup(self, stage=None):
        self.train_dataset = SRDataset(
          self.train_df,
          self.train_lab,
          self.tokenizer,
          self.max_token_len
        )

        self.test_dataset = SRDataset(
          self.test_df,
          self.test_lab,
          self.tokenizer,
          self.max_token_len
    )

    def train_dataloader(self):
        return DataLoader(
          self.train_dataset,
          batch_size=self.batch_size,
          shuffle=True,
          num_workers=10
        )

    def val_dataloader(self):
        return DataLoader(
          self.test_dataset,
          batch_size=self.batch_size,
          num_workers=10
        )

    def test_dataloader(self):
        return DataLoader(
          self.test_dataset,
          batch_size=self.batch_size,
          num_workers=10
        )
</code></pre>
<p><strong>Here is the model class:</strong></p>
<pre><code>class SRTagger(pl.LightningModule):

  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):
    super().__init__()
    self.save_hyperparameters()
    self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)
    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)
    self.n_training_steps = n_training_steps
    self.n_warmup_steps = n_warmup_steps
    self.criterion = nn.BCELoss()

  def forward(self, input_ids, attention_mask, labels=None):
    output = self.bert(input_ids, attention_mask=attention_mask)
    output = self.classifier(output.pooler_output)
    output = torch.sigmoid(output)    
    loss = 0
    if labels is not None:
        loss = self.criterion(output, labels)
    return loss, output

  def training_step(self, batch, batch_idx):
    input_ids = batch[&quot;input_ids&quot;]
    attention_mask = batch[&quot;attention_mask&quot;]
    labels = batch[&quot;labels&quot;]
    loss, outputs = self(input_ids, attention_mask, labels)
    
    self.log(&quot;train_loss&quot;, loss, prog_bar=True, logger=True)
    return {&quot;loss&quot;: loss, &quot;predictions&quot;: outputs, &quot;labels&quot;: labels}

  def validation_step(self, batch, batch_idx):
    input_ids = batch[&quot;input_ids&quot;]
    attention_mask = batch[&quot;attention_mask&quot;]
    labels = batch[&quot;labels&quot;]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log(&quot;val_loss&quot;, loss, prog_bar=True, logger=True)
    return loss

  def test_step(self, batch, batch_idx):
    input_ids = batch[&quot;input_ids&quot;]
    attention_mask = batch[&quot;attention_mask&quot;]
    labels = batch[&quot;labels&quot;]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log(&quot;test_loss&quot;, loss, prog_bar=True, logger=True)
    return loss

  def training_epoch_end(self, outputs):
    
    labels = []
    predictions = []
    for output in outputs:
      for out_labels in output[&quot;labels&quot;].detach().cpu():
        labels.append(out_labels)
      for out_predictions in output[&quot;predictions&quot;].detach().cpu():
        predictions.append(out_predictions)

    labels = torch.stack(labels).int()
    predictions = torch.stack(predictions)

    for i, name in enumerate(LABEL_COLUMNS):
      class_roc_auc = auroc(predictions[:, i], labels[:, i])
      self.logger.experiment.add_scalar(f&quot;{name}_roc_auc/Train&quot;, class_roc_auc, self.current_epoch)


  def configure_optimizers(self):

    optimizer = optim.RAdam(self.parameters(), lr=2e-4)

    scheduler = get_linear_schedule_with_warmup(
      optimizer,
      num_warmup_steps=self.n_warmup_steps,
      num_training_steps=self.n_training_steps
    )

    return dict(
      optimizer=optimizer,
      lr_scheduler=dict(
        scheduler=scheduler,
        interval='step'
      )
    )
</code></pre>
<p><strong>Sample Data</strong></p>
<pre><code>sample_batch = next(iter(DataLoader(train_dataset, batch_size=10, num_workers=2)))
sample_batch[&quot;input_ids&quot;].shape, sample_batch[&quot;attention_mask&quot;].shape

(torch.Size([10, 512]), torch.Size([10, 512]))
sample_batch.keys()
dict_keys(['text_data', 'input_ids', 'attention_mask', 'labels'])
</code></pre>
<p><strong>Model</strong></p>
<pre><code>model = SRTagger(
  n_classes=100,
  n_warmup_steps=warmup_steps,
  n_training_steps=total_training_steps 
)
</code></pre>
<p><strong>ONNX code</strong></p>
<pre><code># # Export the model
torch.onnx.export(model,                     # model being run
                  ##since model is in the cuda mode, input also need to be
                  (sample_batch[&quot;input_ids&quot;],sample_batch[&quot;attention_mask&quot;]),              # model input (or a tuple for multiple inputs)
                  &quot;model_torch_export.onnx&quot;, # where to save the model (can be a file or file-like object)
                  export_params=True,        # store the trained parameter weights inside the model file
                  opset_version=10,          # the ONNX version to export the model to
                  do_constant_folding=True,  # whether to execute constant folding for optimization
                  input_names = ['input'],   # the model's input names
                  output_names = ['output'], # the model's output names
                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable lenght axes
                                'output' : {0 : 'batch_size'}})
</code></pre>
<p><strong>Error</strong></p>
<p>RuntimeError: output 1 (0
[ CPULongType{} ]) of traced region did not have observable data dependence with trace inputs; this probably indicates your program cannot be understood by the tracer.</p>
","transformer-model"
"69628487","How to get SHAP values for Huggingface Transformer Model Prediction [Zero-Shot Classification]?","2021-10-19 09:39:46","69683069","9","4762","<pytorch><huggingface-transformers><transformer-model><shap>","<p>Given a Zero-Shot Classification Task via Huggingface as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
classifier = pipeline(&quot;zero-shot-classification&quot;, model=&quot;facebook/bart-large-mnli&quot;)

example_text = &quot;This is an example text about snowflakes in the summer&quot;
labels = [&quot;weather&quot;, &quot;sports&quot;, &quot;computer industry&quot;]
        
output = classifier(example_text, labels, multi_label=True)
output 
{'sequence': 'This is an example text about snowflakes in the summer',
'labels': ['weather', 'sports'],
'scores': [0.9780895709991455, 0.021910419687628746]}
</code></pre>
<p>I am trying to extract the SHAP values to generate a text-based explanation for the prediction result like shown here: <a href=""https://colab.research.google.com/github/ml6team/quick-tips/blob/main/nlp/2021_04_22_shap_for_huggingface_transformers/explainable_transformers_using_shap.ipynb#scrollTo=Ocd9majYrupz"" rel=""noreferrer"">SHAP for Transformers</a></p>
<p>I already tried the following based on the above url:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer, ZeroShotClassificationPipeline

model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')

pipe = ZeroShotClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)

def score_and_visualize(text):
    prediction = pipe([text])
    print(prediction[0])

    explainer = shap.Explainer(pipe)
    shap_values = explainer([text])

    shap.plots.text(shap_values)

score_and_visualize(example_text)
</code></pre>
<p><strong>Any suggestions? Thanks for your help in advance!</strong></p>
<p>Alternatively to the above pipeline the following also works:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForSequenceClassification, AutoTokenizer, ZeroShotClassificationPipeline

model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')
tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')

classifier = ZeroShotClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)

example_text = &quot;This is an example text about snowflakes in the summer&quot;
labels = [&quot;weather&quot;, &quot;sports&quot;]

output = classifier(example_text, labels)
output 
{'sequence': 'This is an example text about snowflakes in the summer',
'labels': ['weather', 'sports'],
'scores': [0.9780895709991455, 0.021910419687628746]}
</code></pre>
","transformer-model"
"69595863","Machine translation transformer output - ""unknown"" tokens?","2021-10-16 13:06:02","69627137","1","1157","<python><transformer-model><machine-translation><opennmt>","<p>When decoding / translating a test dataset after training on the base Transformer model (Vaswani et. al.), I sometimes see this token &quot;unk&quot; in the ouput.</p>
<p>&quot;unk&quot; here refers to an unknown token, but my question is what is the reasoning behind that? Based on <a href=""https://nlp.stanford.edu/pubs/acl15_nmt.pdf"" rel=""nofollow noreferrer"">https://nlp.stanford.edu/pubs/acl15_nmt.pdf</a>, does it mean that the vocab I built for the training set does not contain the word present in the test set?</p>
<p>For reference, I built the <code>Vocab</code> using <code>Spacy</code> <code>en_core_web_sm</code> and <code>de_core_news_sm</code> for a German to English translation task.</p>
<p>Example output:</p>
<pre><code>ground truth = ['a', 'girl', 'in', 'a', 'jean', 'dress', 'is', 'walking', 'along', 'a', 'raised', 'balance', 'beam', '.']

predicted = ['a', 'girl', 'in', 'a', '&lt;unk&gt;', 'costume', 'is', 'jumping', 'on', 'a', 'clothesline', '.', '&lt;eos&gt;']
</code></pre>
<p>As you can see, the <em>jean</em> is &quot;unk&quot; here.</p>
","transformer-model"
"69576720","Implementing custom learning rate scheduler in Pytorch?","2021-10-14 20:01:52","69577764","3","7562","<tensorflow><pytorch><transformer-model><attention-model>","<p>I would like to implement this learning rate method as in the paper Attention is all you need. I have this code in Tensorflow, but I would like to implement it in Pytorch too. I know that Pytorch has modules for this (<a href=""https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html</a>), but how could I go about making a custom scheduler? Or perhaps one of the above lr_scheduler already fulfils the same function?</p>
<p>Tensorflow code:</p>
<pre><code>class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, d_model, warmup_steps=4000):
    super(CustomSchedule, self).__init__()

    self.d_model = d_model
    self.d_model = tf.cast(self.d_model, tf.float32)

    self.warmup_steps = warmup_steps

  def __call__(self, step):
    arg1 = tf.math.rsqrt(step)
    arg2 = step * (self.warmup_steps ** -1.5)

    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

learning_rate = CustomSchedule(d_model)

optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,
                                     epsilon=1e-9)
</code></pre>
<p>Pytorch?</p>
<pre><code>import torch 

optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)

scheduler =
</code></pre>
","transformer-model"
"69540267","how do vision transformers deal with input images with different sizes?","2021-10-12 12:07:42","","1","361","<computer-vision><transformer-model><vision-transformer>","<p>I want to train a vision transformer with progressive learning which is used in EffientNetV2. Is there any way to do this in a transformer model?</p>
","transformer-model"
"69534536","How does ""Add and Norm"" work across a Position-wise FeedForward Layer in a Transformer?","2021-10-12 02:49:04","","0","408","<huggingface-transformers><transformer-model>","<p>In Transformer architecture, inputs to the Position-wise FFNs have shape <code>(num_samples, sequence_length, input_dim)</code> and outputs have a shape <code>(num_samples, sequence_length, output_dim)</code>.<br />
Now, I am curious as to how do the residual connections work over here when the input and output dimensions are not the same? i.e how is <code>x + F(x)</code> valid when <code>x</code> has shape <code>(num_samples, sequence_length, input_dim)</code> and <code>F(x)</code> has shape <code>(num_samples, sequence_length, output_dim)</code>. Is there any modification done to <code>x</code> to make the shape compatible for residual connection?</p>
","transformer-model"
"69530032","How to use Huggingface pretrained models to get the output of the dataset that was used to train the model?","2021-10-11 17:06:55","","0","1060","<huggingface-transformers><transformer-model><summarization><huggingface-datasets>","<p>I am working on getting the abstractive summaries of the XSUM and the CNN DailyMail datasets using Huggingface's pre-trained BART, Pegasus, and T5 models.</p>
<p>I am confused because there already exist checkpoints of models pre-trained on the same dataset.</p>
<p>So even if I do:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
  
tokenizer = AutoTokenizer.from_pretrained(&quot;mwesner/pretrained-bart-CNN-Dailymail-summ&quot;)

model = AutoModelForSeq2SeqLM.from_pretrained(&quot;mwesner/pretrained-bart-CNN-Dailymail-summ&quot;)
</code></pre>
<p>I can't understand how to get the summaries of either dataset since I don't have any new sentences that I can feed in.</p>
<p>This is how a pretrained model is normally used:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')

ARTICLE_TO_SUMMARIZE = &quot;My friends are cool but they eat too many carbs.&quot;

inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')

# Generate Summary
summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=5, early_stopping=True)
print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])
</code></pre>
<p>But I need the summaries generated by the pre-trained model on the dataset that was used to train them (XSUM and CNN DailyNews).</p>
","transformer-model"
"69436845","BERT Heads Count","2021-10-04 13:29:17","69450719","3","1620","<bert-language-model><transformer-model>","<p>From the literature I read,</p>
<p>Bert Base has 12 encoder layers and 12 attention heads.  Bert Large has 24 encoder layers and 16 attention heads.</p>
<p>Why is Bert large having 16 attentions heads ?</p>
","transformer-model"
"69428811","How does BERT word embedding preprocess work","2021-10-03 20:54:45","69434873","1","1887","<nlp><huggingface-transformers><bert-language-model><transformer-model>","<p>I'm trying to figure out what <code>BERT</code> preprocess does. I mean, how it is done. But I can't find a good explanation. I would appreciate, if somebody know, a link to a better and deeply explained solution.
If someone, by the other hand, wants to solve it here, I would be also extremly thankful!</p>
<p>My question is, how does <code>BERT</code> mathematically convert a string input into a vector of numbers with fixed size? Which are the logical steps that follows?</p>
","transformer-model"
"69426006","Tensorflow ""Transformer model for language understanding"" with another Dataset?","2021-10-03 14:54:48","69532366","4","677","<python><tensorflow><translation><transformer-model><opennmt>","<p>I have been reading the official guide here (<a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a>) to try and recreate the Vanilla Transformer in Tensorflow. I notice the dataset used is quite specific, and at the end of the guide, it says to try with a different dataset.</p>
<p>But that is where I have been stuck for a long time! I am trying to use the WMT14 dataset (as used in the original paper, Vaswani et. al.) here: <a href=""https://www.tensorflow.org/datasets/catalog/wmt14_translate#wmt14_translatede-en"" rel=""nofollow noreferrer"">https://www.tensorflow.org/datasets/catalog/wmt14_translate#wmt14_translatede-en</a> .</p>
<p>I have also tried Multi30k and IWSLT dataset from Spacy, but are there any guides on how I can fit the dataset to what the model requires? Specifically, to tokenize it. The official TF guide uses a pretrained tokenizer, which is specific to the PR-EN dataset given.</p>
<pre><code>model_name = &quot;ted_hrlr_translate_pt_en_converter&quot;
</code></pre>
<p>I am wondering, how I can use the TF (bert) tokenizer to tokenize the Spacy dataset? I have the code for PyTorch, unfortunately I do not know how to adapt it for Tensorflow. Any help would be greatly appreciated!</p>
<pre><code>import spacy

spacy_de = spacy.load('de')
spacy_en = spacy.load('en')

def tokenize_de(text):
    return [tok.text for tok in spacy_de.tokenizer(text)]

def tokenize_en(text):
    return [tok.text for tok in spacy_en.tokenizer(text)]

BOS_WORD = '&lt;s&gt;'
EOS_WORD = '&lt;/s&gt;'
BLANK_WORD = &quot;&lt;blank&gt;&quot;
SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)
TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, 
                 eos_token = EOS_WORD, pad_token=BLANK_WORD)

MAX_LEN = 100
train, val, test = datasets.IWSLT.splits(
    exts=('.de', '.en'), fields=(SRC, TGT), 
    filter_pred=lambda x: len(vars(x)['src']) &lt;= MAX_LEN and 
        len(vars(x)['trg']) &lt;= MAX_LEN)
MIN_FREQ = 2
SRC.build_vocab(train.src, min_freq=MIN_FREQ)
TGT.build_vocab(train.trg, min_freq=MIN_FREQ)
</code></pre>
","transformer-model"
"69380237","Why is my Transformer implementation losing to a BiLSTM?","2021-09-29 16:28:44","69386710","1","591","<deep-learning><pytorch><lstm><transformer-model><language-model>","<p>I am dealing with a sequence tagging problem and I am using a single Transformer Encoder to obtain logits from each element of the sequence. Having experimented both with Transformer and BiLSTM it looks like in my case BiLSTM is working better, so I was wondering if maybe it is because my Transformer implementation has some problem... Below is my implementation of the Transformer Encoder and related functions for creating padding mask and positional embeddings:</p>
<pre><code>def create_mask(src, lengths):
    &quot;&quot;&quot;Create a mask hiding future tokens
    Parameters:
        src (tensor): the source tensor having shape [batch_size, number_of_steps, features_dimensions]
        length (list): a list of integers representing the length (i.e. number_of_steps) of each sample in the batch.&quot;&quot;&quot;
    mask = []
    max_len = src.shape[1]
    for index, i in enumerate(src):
        # The mask consists in tensors having false at the step number that doesn't need to be hidden and true otherwise
        mask.append([False if (i+1)&gt;lengths[index] else True for i in range(max_len)])
    return torch.tensor(mask)

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=5000, device = 'cpu'):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.device = device
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(1, max_len, d_model)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :].to(self.device)
        return self.dropout(x)

class Transformer(nn.Module):
    &quot;&quot;&quot;Class implementing transformer ecnoder, partially based on
    https://pytorch.org/tutorials/beginner/transformer_tutorial.html&quot;&quot;&quot;
    def __init__(self, in_dim, h_dim, n_heads, n_layers, dropout=0.2, drop_out = 0.0, batch_first = True, device = 'cpu', positional_encoding = True):
        super(Transformer, self).__init__()
        self.model_type = 'Transformer'
        self.pos_encoder = PositionalEncoding(in_dim, dropout, device = device)
        encoder_layers = nn.TransformerEncoderLayer(in_dim, n_heads, h_dim, dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers, norm=nn.LayerNorm(in_dim))
        self.in_dim = in_dim
        self.drop_out = drop_out
        self.positional_encoding = positional_encoding
    
        
    def forward(self, src, mask = None, line_len=None):
        src = src * math.sqrt(self.in_dim)
        if self.positional_encoding:
            src = self.pos_encoder(src)
        if line_len is not None and mask is None:
            mask = create_mask(src, line_len)
        else:
            mask = None
        output = self.transformer_encoder(src, src_key_padding_mask = mask)
        if self.drop_out:
            output = F.dropout(output, p = self.drop_out)
        return src, output
</code></pre>
<p>As it can be seen, the above network outputs the hidden states and then I pass them into an additional linear layer and train with a CrossEntropy loss over two classes and Adam optimizer. I have tried multiple combinations of hyperparameters but the BiLSTM still performs better. Can anyone spot anything off in my Transformer or suggest why I experience such a counterintuitive result?</p>
","transformer-model"
"69341381","Conjunction issue in OPENIE 6","2021-09-27 04:53:59","","1","118","<stanford-nlp><transformer-model><allennlp><triples><knowledge-graph>","<p>I am using OPENIE6 (<a href=""https://github.com/dair-iitd/openie6"" rel=""nofollow noreferrer"">https://github.com/dair-iitd/openie6</a>) with the following input:-</p>
<p><strong>President Trump met the leaders of India and China.</strong></p>
<p>But I am getting only one triplet:-</p>
<pre><code>ARG1 = President trump
V = met
ARG2 = the leaders of India and China.
</code></pre>
<p>Instead, as mentioned in the documentation and demos, there should be two triplets:-</p>
<pre><code>ARG1 = President trump
V = met,
ARG2 = the leaders of India.

ARG1 = President trump
V = met
ARG2 = the leaders of China.
</code></pre>
<p>Can anyone help, what is the exact issue?</p>
","transformer-model"
"69340856","How to calculate word similarity based on transformer?","2021-09-27 03:23:53","","1","975","<huggingface-transformers><bert-language-model><word-embedding><transformer-model>","<p>I know I can train word embedding in Tensorflow or Gensim, then I can retrieve top N most similar words for a target word. Given that transformer is now the main stream model for text representation, I want to know whether there is a better way to compute word similarity than Word Embedding. In genesim, I can do:</p>
<pre><code>sims = model.wv.most_similar('computer', topn=10)
</code></pre>
<p>For example, if I use sentence transformer to compute:</p>
<p><a href=""https://huggingface.co/sentence-transformers/LaBSE"" rel=""nofollow noreferrer"">https://huggingface.co/sentence-transformers/LaBSE</a></p>
<pre><code>from sentence_transformers import SentenceTransformer
sentences = [&quot;This is an example sentence&quot;, &quot;Each sentence is converted&quot;]

model = SentenceTransformer('sentence-transformers/LaBSE')
embeddings = model.encode(sentences)
print(embeddings)
</code></pre>
<p>Then use this embedding to compute similarity, would that work for word similarity, if I treat any word as a 'sentence'? Or I use Bert embedding model:</p>
<p><a href=""https://huggingface.co/transformers/model_doc/bert.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html</a></p>
<p>I feed a word like 'computer' as input to get an embedding, then compute its topN similarity. Does this make sense? or it won't work better than embedding trained without involving transformer?</p>
","transformer-model"
"69334124","How to train AutoModelForQuestionAnswering model with Distribute Data Parallel?","2021-09-26 10:44:57","","2","834","<pytorch><multiprocessing><huggingface-transformers><transformer-model>","<p>I reproduce the training code from DataParallel to DistributedDataParallel, It does not release bugs in training, but it does not print any log or running.
Could show me what is wrong with my code?</p>
<p>This is my code and the distribute data parallel reference from <a href=""https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#basic-use-case"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#basic-use-case</a></p>
<pre><code>&quot;&quot;&quot;
    Usages:
        CUDA_VISIBLE_DEVICES=0 python train.py --train_config default_config --work_dir runs/train/layoutlmv2-base-uncased_50e/

&quot;&quot;&quot;

import argparse
from torch import distributed as dist
from transformers import AutoModelForQuestionAnswering
import torch.nn as nn
from utils import create_logger, get_gpu_memory_map, load_feature_from_file, setup, cleanup
from config import TRAIN_FEATURE_PATH, VAL_FEATURE_PATH, MODEL_CHECKPOINT, TRAINING_CONFIGs
import numpy as np
import torch
import os
os.environ[&quot;TOKENIZERS_PARALLELISM&quot;] = &quot;false&quot;
from torch.nn.parallel import DistributedDataParallel as DDP
import torch.multiprocessing as mp


def train(rank, model, train_data, val_data, world_size,
        epochs, optimizer, lr, save_freq,
        eval_freq, work_dir):


    device = rank
    print(&quot;Running DDP with model parallel example on cuda:{} device&quot;.format(rank))
    # logger.info(&quot;Running DDP with model parallel example on cuda:{} device&quot;.format(rank))

    setup(rank, world_size)
    GPU_usage_before = get_gpu_memory_map()
    model = model.to(rank)
    model = DDP(model, device_ids=[rank], find_unused_parameters=True)
    gpus_usage = np.sum(get_gpu_memory_map() - GPU_usage_before)
    print(&quot;GPUs usages for model: {} Mb&quot;.format(gpus_usage))
    # logger.info(&quot;GPUs usages for model: {} Mb&quot;.format(gpus_usage))
    
    optimizer = optimizer(model.parameters(), lr=lr)

    model.train()

    min_valid_loss = np.inf
    idx = 1

    for epoch in range(1, epochs):

        print(&quot;Epoch {}/{}&quot;.format(epoch, epochs))
        
        # logger.info(&quot;Epoch {}/{}&quot;.format(epoch, epochs))
        
        train_loss = 0.0
        for _, train_batch in enumerate(train_data):

            input_ids         = train_batch[&quot;input_ids&quot;].to(device)
            attention_mask    = train_batch[&quot;attention_mask&quot;].to(device)
            token_type_ids    = train_batch[&quot;token_type_ids&quot;].to(device)
            bbox              = train_batch[&quot;bbox&quot;].to(device)
            image             = train_batch[&quot;image&quot;].to(device)
            start_positions   = train_batch[&quot;start_positions&quot;].to(device)
            end_positions     = train_batch[&quot;end_positions&quot;].to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # forward + backward + optimize
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,
                            bbox=bbox, image=image, start_positions=start_positions, end_positions=end_positions)
    
            loss = outputs.loss
            
            loss.backward()
            
            optimizer.step()

            train_loss += loss.item()


            # Evaluate current model on entire validation dataset after each `eval_freq` iterations
            if idx % eval_freq == 1:
                val_loss = 0.0
                model.eval()
                for _, val_batch in enumerate(val_data):
                    
                    # val_batch               = val_batch.to(device)
                    input_ids               = val_batch[&quot;input_ids&quot;].to(device)
                    attention_mask          = val_batch[&quot;attention_mask&quot;].to(device)
                    token_type_ids          = val_batch[&quot;token_type_ids&quot;].to(device)
                    bbox                    = val_batch[&quot;bbox&quot;].to(device)
                    image                   = val_batch[&quot;image&quot;].to(device)
                    start_positions         = val_batch[&quot;start_positions&quot;].to(device)
                    end_positions           = val_batch[&quot;end_positions&quot;].to(device)
                    
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,
                                bbox=bbox, image=image, start_positions=start_positions, end_positions=end_positions)
                    loss = outputs.loss
                    # Calculate Loss
                    val_loss += loss.item()
                
                print(&quot;Iterations: {:&lt;6} - epoch: {:&lt;3} - train_loss: {:&lt;6} - val_loss: {:&lt;6}&quot;.format(idx, epoch, train_loss/eval_freq, val_loss/len(val_data)))
                print(&quot;Iterations: {:&lt;6} - epoch: {:&lt;3} - train_loss: {:&lt;6} - val_loss: {:&lt;6}&quot;.format(idx, epoch, train_loss/eval_freq, val_loss/len(val_data)))

                # logger.info(&quot;Iterations: {:&lt;6} - epoch: {:&lt;3} - train_loss: {:&lt;6} - val_loss: {:&lt;6}&quot;.format(idx, epoch, train_loss/eval_freq, val_loss/len(val_data)))
                # loss_log.info(&quot;Iterations: {:&lt;6} - epoch: {:&lt;3} - train_loss: {:&lt;6} - val_loss: {:&lt;6}&quot;.format(idx, epoch, train_loss/eval_freq, val_loss/len(val_data)))

                    
                if min_valid_loss &gt; val_loss/len(val_data):
                    print(&quot;Found best model !! Validation loss descreased from {} to {}&quot;.format(min_valid_loss, val_loss/len(val_data)))
                    # logger.info(&quot;Found best model !! Validation loss descreased from {} to {}&quot;.format(min_valid_loss, val_loss/len(val_data)))
                    torch.save(model.state_dict(), os.path.join(work_dir, 'best'+'.pth'))
                    min_valid_loss = val_loss/len(val_data)

                # Save model each save_freq iteration
                if idx % save_freq == 1:
                    print(&quot;Saving model to {}&quot;.format(os.path.join(work_dir, str(idx).zfill(5)+'.pth')))
                    # logger.info(&quot;Saving model to {}&quot;.format(os.path.join(work_dir, str(idx).zfill(5)+'.pth')))
                    torch.save(model.state_dict(), os.path.join(work_dir, str(idx).zfill(5)+'.pth'))
                    dist.barrier()

                # Reset training loss
                train_loss = 0.0
                
            idx += 1


    # logger.info(&quot;Done !&quot;)
    # logger.info(&quot;The minimum on validation {}&quot;.format(min_valid_loss))
    print(&quot;DONE !&quot;)
    print(&quot;The minimum on validation {}&quot;.format(min_valid_loss))

    cleanup()

    return model


def main(args):

    gpu_ids = [i for i in range(torch.cuda.device_count())]
    torch.cuda.set_device(gpu_ids[0])
    
    if not os.path.exists(args['work_dir']):
        os.mkdir(args['work_dir'])
    # Create logger
    loss_log = create_logger(os.path.join(args[&quot;work_dir&quot;], 'loss.log'))
    logger = create_logger(os.path.join(args['work_dir'], 'log.log'))

    logger.info('Loading training configuration ...')
    config = TRAINING_CONFIGs[args['train_config']]
    optimizer, momentum, lr, epochs, batch_size,\
         eval_freq, save_freq, num_workers = config['optimizer'], config['momentum'], \
        config['lr'], config['epochs'], \
        config['batch_size'], config['eval_freq'], config['save_freq'], config['num_workers']
    logger.info(&quot;Configuration: {}&quot;.format(config))

    # Check whether feature path file existing or not
    if not os.path.exists(TRAIN_FEATURE_PATH):
        logger.error(&quot;Invalid training feature path&quot;)
        exit(0)
    if not os.path.exists(VAL_FEATURE_PATH):
        logger.error(&quot;Invalid validation feature path&quot;)
        exit(0)

    # Load data into program 
    logger.info(&quot;Loading training dataset from {} ...&quot;.format(TRAIN_FEATURE_PATH))
    train_dataloader = load_feature_from_file(path=TRAIN_FEATURE_PATH, 
                                            batch_size=batch_size, num_workers=num_workers)

    logger.info(&quot;Loading validation dataset from {} ...&quot;.format(VAL_FEATURE_PATH))
    val_dataloader = load_feature_from_file(path=VAL_FEATURE_PATH, 
                                            batch_size=batch_size, num_workers=num_workers)

    logger.info(&quot;Training size: {} - Validation size: {}&quot;.format(
        len(train_dataloader.dataset), len(val_dataloader.dataset)))

    logger.info(&quot;Loading pre-training model from {} checkpoint&quot;.format(MODEL_CHECKPOINT))
    model = AutoModelForQuestionAnswering.from_pretrained(MODEL_CHECKPOINT)
    
    # Fine-tuning model
    # trained_model = train(model=model, train_data=train_dataloader, val_data=val_dataloader,
    #                   epochs=epochs, optimizer=optimizer, lr=lr, loss_log=loss_log, save_freq=save_freq,
    #                     work_dir=args['work_dir'], logger=logger, eval_freq=eval_freq, gpu_ids=gpu_ids)

    mp.spawn(train,
             args=(model, train_dataloader, val_dataloader, len(gpu_ids), 
                   epochs, optimizer, lr, save_freq,
                   eval_freq, args['work_dir']),
             nprocs=len(gpu_ids),
             join=True)


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Fine tuning pre-training model on DocVQA data')

    parser.add_argument('--work_dir', default='runs/train/train_1/',
        help='The directory store model checkpoint and log file',
    )

    parser.add_argument('--train_config', default='default_config', 
        help='The training configurations: learning rate, batch size, epochs, optimizer, ...'
    )

    args = vars(parser.parse_args())

    main(args)
</code></pre>
<p><strong>The output in terminal like below.</strong></p>
<pre class=""lang-sh prettyprint-override""><code>(transformer_env)root@ae94a4e6c92d:/mlcv/WorkingSpace/NCKH/tiennv/vqa_thesis/docvqa/libs/layoutlmv2# CUDA_VISIBLE_DEVICES=1,2 python train.py --work_dir ./runs/train/test_multi-gpus --train_config default_config
2021-09-26 10:11:49,801 - INFO - Loading training configuration ...
2021-09-26 10:11:49,802 - INFO - Configuration: {'optimizer': &lt;class 'torch.optim.adam.Adam'&gt;, 'lr': 0.0001, 'epochs': 2, 'batch_size': 2, 'momentum': 0.9, 'eval_freq': 1, 'save_freq': 1, 'num_workers': 4}
2021-09-26 10:11:49,803 - INFO - Loading training dataset from /mlcv/Databases/DocVQA_2020-21/task_1/extracted_features/layoutlmv2/train ...
2021-09-26 10:11:49,953 - INFO - Loading validation dataset from /mlcv/Databases/DocVQA_2020-21/task_1/extracted_features/layoutlmv2/val ...
2021-09-26 10:11:49,977 - INFO - Training size: 39456 - Validation size: 5344
2021-09-26 10:11:49,978 - INFO - Loading pre-training model from microsoft/layoutlmv2-base-uncased checkpoint
Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2ForQuestionAnswering: ['layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked']
- This IS expected if you are initializing LayoutLMv2ForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LayoutLMv2ForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LayoutLMv2ForQuestionAnswering were not initialized from the model checkpoint at microsoft/layoutlmv2-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias', 'layoutlmv2.visual_segment_embedding']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running DDP with model parallel example on cuda:0 device
Running DDP with model parallel example on cuda:1 device
GPUs usages for model: 6721 Mb
Epoch 1/2
GPUs usages for model: 6721 Mb
Epoch 1/2
</code></pre>
<p>It still prints anything ...
Please help me ...</p>
","transformer-model"
"69323932","Large, exploding loss in Pytorch transformer model","2021-09-25 07:14:19","","0","2170","<machine-learning><pytorch><transformer-model>","<p>I am trying to solve a sequence to sequence problem with a transformer model. The data is derived from a set of crossword puzzles.</p>
<p>The positional encoding and transformer classes are as follows:</p>
<pre><code>class PositionalEncoding(nn.Module):
    
    
    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 3000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
    
        position = torch.arange(max_len).unsqueeze(1) 
        
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        
        pe = torch.zeros(1, max_len, d_model)
     
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)
    
        self.register_buffer('pe', pe)
        
    def debug(self, x):
        return x.shape, x.size()

    def forward(self, x: Tensor) -&gt; Tensor:

        x = x + self.pe[:, :x.size(1), :]
        
        return self.dropout(x)
   
        

class Transformer(nn.Module):

    def __init__(
        self,
        num_tokens,
        dim_model,
        num_heads,
        num_encoder_layers,
        num_decoder_layers,
        batch_first,
        dropout_p,
    ):
        super().__init__()

        self.model_type = &quot;Transformer&quot;
        self.dim_model = dim_model

        self.positional_encoder = PositionalEncoding(
            d_model=dim_model, dropout=dropout_p, max_len=3000
        )
        self.embedding = nn.Embedding.from_pretrained(vec_weights, freeze=False)#nn.Embedding(num_tokens, dim_model)
        self.transformer = nn.Transformer(
            d_model=dim_model,
            nhead=num_heads,
            num_encoder_layers=num_encoder_layers,
            num_decoder_layers=num_decoder_layers,
            dropout=dropout_p,
            batch_first = batch_first
        )
        
        self.out = nn.Linear(dim_model, num_tokens)
        
    def forward(self, src, tgt, tgt_mask=None, src_pad_mask=None, tgt_pad_mask=None):
       
        src = self.embedding(src)*math.sqrt(self.dim_model)
        tgt = self.embedding(tgt)*math.sqrt(self.dim_model)
        src = self.positional_encoder(src)
        tgt = self.positional_encoder(tgt)
        
        transformer_out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=tgt_pad_mask)
        out = self.out(transformer_out)
        
        return out
      
    def get_tgt_mask(self, size) -&gt; torch.tensor:
        mask = torch.tril(torch.ones(size, size) == 1) 
        mask = mask.float()
        mask = mask.masked_fill(mask == 0, float('-inf'))
        mask = mask.masked_fill(mask == 1, float(0.0))
        
        return mask
    
    def create_pad_mask(self, matrix: torch.tensor, pad_token: int) -&gt; torch.tensor:
        return (matrix == pad_token) 
</code></pre>
<p>The input tensors are a source tensor of size N by S, where N is the batch size and S is the source sequence length, and a target tensor of size N by T, where T is the target sequence length. S is about 10 and T is about 5, while the total number of items is about 160,000-200,000, divided into batch sizes of 512.  They are torch.IntTensors, with elements in the range from 0 to V, where V is the vocabulary length.</p>
<p>The first layer is an embedding layer that takes the input from N by S to N by S by E, where E is the embedding dimension (300), or to N by T by E in the case of the target. The second layer adds position encoding without changing the shape. Then both tensors are passed through the transformer layer, which outputs an N by T by E tensor. Finally, we pass this output through a linear layer, which produces an N by T by V output, where V is the size of the vocabulary used in the problem. Here V is about 56,697. The most frequent tokens (words) appear about 50-60 times in the target tensor.</p>
<p>The transformer class also contains the functions for implementing the masking matrices.</p>
<p>Then we create the model and run it (this process is wrapped in a function).</p>
<pre><code>device = &quot;cuda&quot;

src_train, src_test = torch.utils.data.random_split(src_t, [int(0.9*len(src_t)), len(src_t)-int(0.9*len(src_t))])
src_train, src_test = src_train[:512], src_test[:512]
tgt_train, tgt_test = torch.utils.data.random_split(tgt_t, [int(0.9*len(tgt_t)), len(tgt_t)-int(0.9*len(tgt_t))])
tgt_train, tgt_test = tgt_train[:512], tgt_test[:512]
train_data, test_data = list(zip(src_train, tgt_train)), list(zip(src_test, tgt_test))
train, test = torch.utils.data.DataLoader(dataset=train_data), torch.utils.data.DataLoader(dataset=test_data)


model = Transformer(num_tokens=ntokens, dim_model=300, num_heads=2, num_encoder_layers=3, num_decoder_layers=3, batch_first = True, dropout_p=0.1).to(device)
loss_function = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.0000001)

n_epochs = 50


def train_model(model, optimizer, loss_function, n_epochs):
    loss_value=0
    for epoch in range(n_epochs):
        print(f&quot;Starting epoch {epoch}&quot;)
        for batch, data in enumerate(train):
            
            x, y = data
            if batch%100 == 0:
                print(f&quot;Batch is {batch}&quot;)
                
            batch += 1
            optimizer.zero_grad()
            

            x, y = torch.tensor(x).to(device), torch.tensor(y).to(device)
            y_input, y_base = y[:, :-1], y[:, 1:]
            y_input, y_base = y_input.to(device), y_base.to(device)

            
            tgt_mask = model.get_tgt_mask(y_input.shape[1]).to(device)
            pad_token = vocabulary_table[embeddings.key_to_index[&quot;/&quot;]]
            src_pad_mask = model.create_pad_mask(x, pad_token).to(device)
            tgt_pad_mask = model.create_pad_mask(y_input, pad_token).to(device)
            
            
            z = model(x, y_input, tgt_mask, src_pad_mask, tgt_pad_mask)
            z = z.permute(0, 2, 1).to(device)
            
            
            y_base = y_base.long().to(device)
        
            loss = loss_function(z, y_base).to(device)
                
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2)
            optimizer.step()
            
            loss_value += float(loss)
            
            if batch%100 == 0:
                print(f&quot;For epoch {epoch}, batch {batch} the cross-entropy loss is {loss_value}&quot;)
            
            
            #Free GPU memory.
            del z
            del x
            del y
            del y_input
            del y_base
            del loss
            torch.cuda.empty_cache() 
            
            
    return model.parameters(), loss_value
</code></pre>
<p>Basically, we split the data into test and training sets and use an SGD optimizer and cross-entropy loss. We create a masking matrix for the padding for both the target and source tensors, and a masking matrix for unseen elements for the target tensor. We then do the usual gradient update steps. Right now, there is no validation loop, because I cannot even get the training loss to decrease.</p>
<p>The loss is very high, reaching more than 1000 after 100 batches. More concerningly, the loss also increases rapidly during training, rather than decreasing. In the code that I included, I tried clipping the gradients, lowering the learning rate, and using a much smaller sample to debug the code.</p>
<p>What could be causing this behavior?</p>
","transformer-model"
"69314813","Unable to transform stringfy json array of objects to json object using jolt transformer","2021-09-24 12:11:43","","-1","235","<json><jolt><transformer-model>","<p>Input json</p>
<pre class=""lang-json prettyprint-override""><code>&quot;sales&quot; : &quot;[{\&quot;Option\&quot;:\&quot;Britania\&quot;,\&quot;value\&quot;:\&quot;200\&quot;}{\&quot;Option\&quot;:\&quot;Parle\&quot;,\&quot;value\&quot;:\&quot;100\&quot;}{\&quot;Option\&quot;:\&quot;mariegold\&quot;,\&quot;value\&quot;:\&quot;500\&quot;}{\&quot;Option\&quot;:\&quot;snacks\&quot;,\&quot;value\&quot;:\&quot;200\&quot;}]&quot;,
</code></pre>
<p>jolt transformer used</p>
<pre class=""lang-json prettyprint-override""><code>[
  {
    &quot;operation&quot;: &quot;modify-overwrite-beta&quot;,
    &quot;spec&quot;: {
      &quot;CREATIONDATETIME&quot;: &quot;=substring(@(1,CREATIONDATETIME),0,19)&quot;
    }
  },
  {
    &quot;operation&quot;: &quot;shift&quot;,
    &quot;spec&quot;: {
      &quot;sales&quot;: &quot;extendedAttributes.salesValueOptions&quot;,
      &quot;status&quot;: {
        &quot;SUBMITTED&quot;: {
          &quot;#submitted&quot;: &quot;key6&quot;
        }
      }
    }
  },
  {
    &quot;operation&quot;: &quot;modify-default-beta&quot;,
    &quot;spec&quot;: {
      &quot;key6&quot;: &quot;pending&quot;
    }
  }
]
</code></pre>
<p>Output needed</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;sales&quot;: {
    &quot;option1&quot;: &quot;Britannia&quot;,
    &quot;value1&quot;: &quot;0&quot;,
    &quot;option2&quot;: &quot;cadbury&quot;,
    &quot;value2&quot;: &quot;0&quot;,
    &quot;option3&quot;: &quot;Parle&quot;,
    &quot;value3&quot;: &quot;0&quot;
  }
}
</code></pre>
<p>I have tried the different JsonSpecs provided at the JOLT github help page. But I am not able to solve this. Any help or pointers will be appreciated.</p>
","transformer-model"
"69286889","transformers and BERT downloading to your local machine","2021-09-22 15:07:44","69287116","1","15842","<python><torch><bert-language-model><transformer-model><doc2vec>","<p>I am trying to replicates the code from <a href=""https://colab.research.google.com/drive/1yFphU6PW9Uo6lmDly_ud9a6c4RCYlwdX"" rel=""nofollow noreferrer"">this page</a>.</p>
<p>At my workplace we have access to transformers and pytorch library but cannot connect to internet from our python environment. Could anyone help with how we could get the script working after manually downloading files to my machine?</p>
<p>my specific questions are -</p>
<ol>
<li><p>should I go to the location <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""nofollow noreferrer"">bert-base-uncased at main</a> and download all the files? Do I have put them in a folder with a specific name?</p>
</li>
<li></li>
</ol>
<p>How should I change the below code</p>
<pre><code># Load pre-trained model tokenizer (vocabulary)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# Tokenize our sentence with the BERT tokenizer.
tokenized_text = tokenizer.tokenize(marked_text)
</code></pre>
<ol start=""3"">
<li></li>
</ol>
<p>How should I change the below code</p>
<pre><code># Load pre-trained model (weights)
model = BertModel.from_pretrained('bert-base-uncased',

                                  output_hidden_states = True, # Whether the model returns all hidden-states.

                              )
</code></pre>
<p>Please let me know if anyone has done this…thanks</p>
<p>###update1</p>
<p>I went to the <a href=""https://huggingface.co/bert-base-uncased/tree/main"" rel=""nofollow noreferrer"">link</a> and manually downloaded all files to a folder and specified path of that folder in my code. Tokenizer works but this line <code>model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True, # Whether the model returns all hidden-states. )</code> fails. Any idea what should i do? I noticed that 4 big files when downloaded have very strange name...should I rename them to same names as shown on the above page? Do I need to download any other files?</p>
<p>the error message is <code>OSErrr: unable to load weights from pytorch checkpoint file for bert-base-uncased2/ at bert-base-uncased/pytorch_model.bin If you tried to load a pytroch model from a TF 2 checkpoint, please set from_tf=True</code></p>
","transformer-model"
"69276018","How to adapt transfomer pretrained tokenizers to work with this translation tutorial?","2021-09-21 22:03:32","","1","295","<tensorflow><deep-learning><nlp><huggingface-transformers><transformer-model>","<p>The tutorial url:
<a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<p>The pt-en tokenizer model code:</p>
<pre><code>examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)
train_examples, val_examples = examples['train'], examples['validation']

# 2. Get BertTokenizer
model_name = &quot;ted_hrlr_translate_pt_en_converter&quot;
tf.keras.utils.get_file(
    f&quot;{model_name}.zip&quot;,
    f&quot;https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip&quot;,
    cache_dir='.', cache_subdir='', extract=True
)

tokenizers = tf.saved_model.load(model_name)
en_tokenizer_items = [item for item in dir(tokenizers.en) if not item.startswith('_')]
print('En tokenizer methods: ', en_tokenizer_items)

# 3. Tokenizer examples
def tokenize_pairs(pt, en):
    pt = tokenizers.pt.tokenize(pt)
    
    # Convert from ragged to dense, padding with zeros.
    pt = pt.to_tensor()

    en = tokenizers.en.tokenize(en)
    # Convert from ragged to dense, padding with zeros.
    en = en.to_tensor()
    return pt, en


# 4. Make batches
BUFFER_SIZE = 20000
BATCH_SIZE = 64
def make_batches(ds):
  return (
      ds
      .cache()
      .shuffle(BUFFER_SIZE)
      .batch(BATCH_SIZE)
      .map(tokenize_pairs, num_parallel_calls=tf.data.experimental.AUTOTUNE)
      .prefetch(tf.data.experimental.AUTOTUNE))

train_batches = make_batches(train_examples)
val_batches = make_batches(val_examples)
</code></pre>
<p>In this line of code below, the tokenize takes a tensor of string as input:</p>
<pre><code>pt = tokenizers.pt.tokenize(pt)
</code></pre>
<p>A transorformer pretrained tokenizer usually takes a string as input rather than a tensor here. If I want to switch the tokenizers from portugues to Chinese, how can I adapt the transformers tokenizer to work with the 'make_batches' and 'tokenize_pairs' functions?</p>
<p>I simply import the transformer tokenizers but it didn't work:</p>
<pre><code>from transformers import BertTokenizer
tokenizer_en = BertTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
tokenizer_zh = BertTokenizer.from_pretrained(&quot;bert-base-chinese&quot;)

def tokenize_pairs(zh, en):
    zh = tokenizer_zh.tokenize(zh)
    # Convert from ragged to dense, padding with zeros.
    zh = zh.to_tensor()
    en = tokenizer_en.tokenize(en)
    # Convert from ragged to dense, padding with zeros.
    en = en.to_tensor()
    return zh, en
</code></pre>
<p>This line below reports an error:</p>
<pre><code>zh = tokenizer_zh.tokenize(zh)



/Users/cong/transformer/data_zh.py:44 tokenize_pairs  *
        zh = tokenizer_zh.tokenize(zh)
    /Users/cong/.venv/tf2/lib/python3.8/site-packages/transformers/tokenization_utils.py:336 split_on_tokens  *
        if not text.strip():
    /Users/cong/.venv/tf2/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:401 __getattr__
        self.__getattribute__(name)

    AttributeError: 'Tensor' object has no attribute 'strip'
</code></pre>
","transformer-model"
"69233467","How to use tflite model coverted from TFMarianMTModel on huggingface","2021-09-18 09:43:58","","0","309","<nlp><tensorflow-lite><transformer-model>","<p>I am looking for a en-zh translate model that can be used in TFLite, and i found one on huggingface: <a href=""https://huggingface.co/Helsinki-NLP/opus-mt-en-zh"" rel=""nofollow noreferrer"">https://huggingface.co/Helsinki-NLP/opus-mt-en-zh</a></p>
<p>I have coverted the model to .tflite by follow code:</p>
<pre><code>import tensorflow as tf
from transformers import TFMarianMTModel, AutoTokenizer

print(&quot;loading model...&quot;)
model_name = 'Helsinki-NLP/opus-mt-en-zh'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFMarianMTModel.from_pretrained(model_name, from_pt=True)

converter = tf.lite.TFLiteConverter.from_keras_model(model);
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert();

with open(&quot;./out/tf_model.tflite&quot;, 'wb') as o_:
    o_.write(tflite_model)
</code></pre>
<p>but when i try to use it for infrence, i meet some problem:</p>
<pre><code>from transformers import AutoTokenizer
import tensorflow as tf
import numpy as np

model_name = 'Helsinki-NLP/opus-mt-en-zh'
tokenizer = AutoTokenizer.from_pretrained(model_name)
result = tokenizer(&quot;&gt;&gt;cmn_Hans&lt;&lt; hello world&quot;, return_tensors=&quot;tf&quot;, padding=True)

print(&quot;tokenize result: &quot;, result, '\n')

interpreter = tf.lite.Interpreter(model_path=&quot;./out/tf_model.tflite&quot;)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_ids = result[&quot;input_ids&quot;]

interpreter.set_tensor(input_details[2]['index'], input_ids)
interpreter.invoke()

# The function `get_tensor()` returns a copy of the tensor data.
# Use `tensor()` in order to get a pointer to the tensor.
output_data = interpreter.get_tensor(output_details[0]['index'])
print(&quot;output_data:&quot;, output_data)
</code></pre>
<p>the error is:</p>
<pre><code>Traceback (most recent call last):
  File &quot;lite.py&quot;, line 48, in &lt;module&gt;
    interpreter.set_tensor(input_details[2]['index'], input_ids)
  File &quot;/Users/xuanyue/venv/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py&quot;, line 607, in set_tensor
    self._interpreter.SetTensor(tensor_index, value)
ValueError: Cannot set tensor: Dimension mismatch. Got 1 but expected 3 for dimension 0 of input 2.
</code></pre>
<p>I try to resize the shape, but i don't know how to do that, any suggestions?</p>
","transformer-model"
"69196995","Using Hugging-face transformer with arguments in pipeline","2021-09-15 16:47:04","69215166","4","2199","<pytorch><huggingface-transformers><bert-language-model><transformer-model><huggingface-tokenizers>","<p>I am working on using a transformer. Pipeline to get BERT embeddings to my input. using this without a pipeline i am able to get constant outputs but not with pipeline since I was not able to pass arguments to it.</p>
<p>How can I pass transformer-related arguments for my Pipeline?</p>
<pre class=""lang-py prettyprint-override""><code># These are BERT and tokenizer definitions
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

inputs = ['hello world']

# Normally I would do something like this to initialize the tokenizer and get the result with constant output
tokens = tokenizer(inputs,padding='max_length', truncation=True, max_length = 500, return_tensors=&quot;pt&quot;)
model(**tokens)[0].detach().numpy().shape


# using the pipeline 
pipeline(&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer, device=0)

# or other option
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;,padding='max_length', truncation=True, max_length = 500, return_tensors=&quot;pt&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

nlp=pipeline(&quot;feature-extraction&quot;, model=model, tokenizer=tokenizer, device=0)

# to call the pipeline
nlp(&quot;hello world&quot;)
</code></pre>
<p>I have tried several ways like the options listed above but was not able to get results with constant output size. one can achieve constant output size by setting the tokenizer arguments but have no idea how to give arguments for the pipeline.</p>
<p>any idea?</p>
","transformer-model"
"69159507","Load a model as DPRQuestionEncoder in HuggingFace","2021-09-13 08:26:35","69200389","1","2306","<python><nlp><huggingface-transformers><bert-language-model><transformer-model>","<p>I would like to load the BERT's weights (or whatever transformer) into a <a href=""https://huggingface.co/transformers/model_doc/dpr.html#transformers.DPRQuestionEncoder"" rel=""nofollow noreferrer"">DPRQuestionEncoder</a> architecture, such that I can use the HuggingFace <em>save_pretrained</em> method and plug the saved model into the <a href=""https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag"" rel=""nofollow noreferrer"">RAG architecture to do end-to-end fine-tuning</a>.</p>
<pre><code>from transformers import DPRQuestionEncoder
model = DPRQuestionEncoder.from_pretrained('bert-base-uncased')
</code></pre>
<p>But I got the following error</p>
<pre><code>You are using a model of type bert to instantiate a model of type dpr. This is not supported for all configurations of models and can yield errors.

NotImplementedErrorTraceback (most recent call last)
&lt;ipython-input-27-1f1b990b906b&gt; in &lt;module&gt;
----&gt; 1 model = DPRQuestionEncoder.from_pretrained(model_name)
      2 # https://github.com/huggingface/transformers/blob/41cd52a768a222a13da0c6aaae877a92fc6c783c/src/transformers/models/dpr/modeling_dpr.py#L520

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1211                     )
   1212 
-&gt; 1213             model, missing_keys, unexpected_keys, error_msgs = cls._load_state_dict_into_model(
   1214                 model, state_dict, pretrained_model_name_or_path, _fast_init=_fast_init
   1215             )

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in _load_state_dict_into_model(cls, model, state_dict, pretrained_model_name_or_path, _fast_init)
   1286             )
   1287             for module in unintialized_modules:
-&gt; 1288                 model._init_weights(module)
   1289 
   1290         # copy state_dict so _load_from_state_dict can modify it

/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py in _init_weights(self, module)
    515         Initialize the weights. This method should be overridden by derived class.
    516         &quot;&quot;&quot;
--&gt; 517         raise NotImplementedError(f&quot;Make sure `_init_weigths` is implemented for {self.__class__}&quot;)
    518 
    519     def tie_weights(self):

NotImplementedError: Make sure `_init_weigths` is implemented for &lt;class 'transformers.models.dpr.modeling_dpr.DPRQuestionEncoder'&gt;
</code></pre>
<p>I am using the last version of Transformers.</p>
","transformer-model"
"69126923","how to train a bert model from scratch with huggingface?","2021-09-10 03:30:56","","4","3616","<huggingface-transformers><bert-language-model><transformer-model><fine-tuning>","<p>i find a answer of training model from scratch in this question:
<a href=""https://stackoverflow.com/questions/65646925/how-to-train-bert-from-scratch-on-a-new-domain-for-both-mlm-and-nsp"">How to train BERT from scratch on a new domain for both MLM and NSP?</a></p>
<p>one answer use Trainer and TrainingArguments like this:</p>
<pre><code>from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir= &quot;/path/to/output/dir/for/training/arguments&quot;
    overwrite_output_dir=True,
    num_train_epochs=2,
    per_gpu_train_batch_size= 16,
    save_steps=10_000,
    save_total_limit=2,
    prediction_loss_only=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=dataset,
)

trainer.train()
trainer.save_model(&quot;path/to/your/model&quot;)
</code></pre>
<p>but huggingface official doc <a href=""https://huggingface.co/transformers/training.html"" rel=""nofollow noreferrer"">Fine-tuning a pretrained model
</a> also use Trainer and TrainingArguments in the same way to finetune .
so when I use Trainer and TrainingArguments to train model, Do I train model from scratch or just finetune?</p>
","transformer-model"
"69118249","Is there a maximum sequence length for the output of a transformer?","2021-09-09 12:33:04","","4","4492","<nlp><artificial-intelligence><transformer-model>","<p>There's just one thing that I can't find an answer to :
When putting the ouput back in the transformer, we compute it similarly to the inputs (with added masks), so is there also a sequence size limit ?</p>
<p>Even BERT has an input size limit of 512 tokens, so transformers are limited in how much they can take in.
So is there something to make the output length as big as wanted or is there a fixed max length ?</p>
<p>If I wasn't clear enough, does the network generate words infinitely until the &lt; end &gt; token or is there a token limit for the outputs?</p>
","transformer-model"
"69110073","What is the dff parameter in transformer model?","2021-09-08 21:55:29","","1","1452","<transformer-model>","<p>In the paper, it describes the base model's network configuration are below:</p>
<p><a href=""https://i.sstatic.net/0CfRQ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0CfRQ.png"" alt=""enter image description here"" /></a></p>
<pre><code>d_model:  embedding size
h: attention head count
d_k: key matrix dimension
d_v: value matrix dimension
dff: 2048?
</code></pre>
<p>What's the dff?</p>
","transformer-model"
"69091576","String comparison with BERT seems to ignore ""not"" in sentence","2021-09-07 16:18:05","69260955","3","1200","<nlp><bert-language-model><transformer-model><sentence-similarity><sentence-transformers>","<p>I implemented a string comparison method using SentenceTransformers and BERT like following</p>
<pre><code>from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')

sentences = [
    &quot;I'm a good person&quot;,
    &quot;I'm not a good person&quot;
]

sentence_embeddings = model.encode(sentences)

cosine_similarity(
    [sentence_embeddings[0]],
    sentence_embeddings[1:]
)
</code></pre>
<p>Notice how my sentence examples are very similar but with the opposite meaning. The problem is the cosine similarity returns 0.9, indicating that these two strings are very similar in context when I expected it to return something closer to zero, as they have the opposite meanings.</p>
<p>How can I adapt my code to return a more accurate result?</p>
","transformer-model"
"69084798","In transformers of ViT model, last_hidden_state is not equal to hidden_states[-1]","2021-09-07 08:29:13","","1","910","<python><pytorch><huggingface-transformers><transformer-model>","<p>When input the same image, in Google ViT model output.last_hidden_state is not equal to output.hidden_states[-1] ?
I tried in Bert， the outputs are the same.</p>
<p>feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')</p>
<pre><code>model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')
inputs = feature_extractor(images=[image], return_tensors=&quot;pt&quot;)
outputs = model(pixel_values=inputs['pixel_values'], output_hidden_states=True)

vec1 = outputs.hidden_states[-1][0, 0, :]
vec2 = outputs.last_hidden_state[0, 0, :]
</code></pre>
<p>in my mind, vec1 should be the same as vec2. But the fact is they are not the same at all.</p>
","transformer-model"
"69082230","Python: Text Classification with BERT stuck at 0%","2021-09-07 03:53:32","","0","222","<python><nlp><text-classification><bert-language-model><transformer-model>","<p>Being new to text classification, I was playing with <a href=""https://www.kaggle.com/abhishek/aaamlp/version/1?select=imdb_folds.csv"" rel=""nofollow noreferrer"">kaggle data</a> named <code>imdb_folds.csv</code>, and then the running of my code got stuck while showing this message:</p>
<pre><code>0%|                                      | 0/1250 [00:00&lt;?, ?it/s]
</code></pre>
<p>My is code is:</p>
<pre><code>## Import Packages
import tez
import torch
import torch.nn as nn
import transformers
from transformers import AdamW, get_linear_schedule_with_warmup
from sklearn import metrics
import pandas as pd

## Create a data loader using a class named BERTDataset 
class BERTDataset:
    def __init__(self, texts, targets, max_len = 64):
        self.texts = texts
        self.targets = targets
        self.tokenizer = transformers.BertTokenizer.from_pretrained(
            &quot;bert-base-uncased&quot;, #model name
            do_lower_case = False
        )
        self.max_len = max_len
        
    # length function
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        inputs = self.tokenizer.encode_plus(
        texts,
        None,
        add_special_tokens = True,
        max_length = self.max_len,
        padding = &quot;max_length&quot;,
        truncation = True
        )
        resp = {
            &quot;ids&quot;: torch.tensor(inputs[&quot;input_ids&quot;], dtype = torch.long),
            &quot;mask&quot;: torch.tensor(inputs[&quot;attention_mask&quot;], dtype = torch.long),
            &quot;token_type_ids&quot;: torch.tensor(inputs[&quot;token_type_ids&quot;], dtype = torch.long),
            &quot;targets&quot;: torch.tensor(self.targets[idx], dtype = torch.float),
            
            ## for multiclass classification, convert change dtype from torch.float to torch.long
            #&quot;targets&quot;: torch.tensor(self.targets[idx], dtype = torch.long),
        }
        return resp

## Build the model
class TextModel(tez.Model):
    def __init__(self, num_classes, num_train_steps):
        
        super().__init__()
        self.bert = transformers.BertModel.from_pretrained(
            &quot;bert-base-uncased&quot;, return_dict = False
        )
        self.bert_drop = nn.Dropout(0.3)
        self.out = nn.Linear(768, num_classes) # num_classes is 1 or &gt; 1 if it is binary multiclass classification respectively
        self.num_train_steps = num_train_steps
        
        
    # optimizer
    def fetch_optimizer(self):
        opt = AdamW(self.parameters(), lr = 1e-4)
        return opt
    
    # scheduler
    def fetch_scheduler(self):
        sch = get_linear_schedule_with_warmup(
            self.optimizer, num_warmup_steps = 0, num_training_steps = self.num_train_steps        
        )
        return sch
    
    # loss
    def loss(self, outputs, targets):
        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))
    
        ## include the next line if you have multiclass classification 
        # return nn.CrossEntropyLoss()(outputs, targets)
    
    # calculate accuracy
    def monitor_metrics(self, outputs, targets):
        outputs = torch.sigmoid(outputs).cpu().detach().numpy() &gt;= 0.5
        
       
        targets = targets.cpu().detach().numpy()
        return {&quot;accuracy&quot;: metrics.accuracy_score(targets, outputs)}
    
    # forward function
    def forward(self, ids, mask, token_type_ids, targets = None):
        _, x = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids)
        x = self.bert_drop(x)
        x = self.out(x)
        if targets is not None:
            loss = self.loss(outputs, targets)
            met = self.monitor_metrics(outputs, targets)
            return x, loss, met
        return x, 0, {} # if there is no target, return 0        


## Read Dataset
def train_model(fold):
    df = pd.read_csv(&quot;imdb_folds.csv&quot;) # read file
    df_train = df[df.kfold != fold].reset_index(drop=True)
    df_valid = df[df.kfold == fold].reset_index(drop=True)
    
    train_dataset = BERTDataset(df_train.review.values, df_train.sentiment.values)
    valid_dataset = BERTDataset(df_valid.review.values, df_valid.sentiment.values)
    
    # n_train_steps = int(len(df_train) / TRAIN_BS * EPOCHS)
    n_train_steps = int(len(df_train) / 32 * 10)
    model = TextModel(num_classes = 1, num_train_steps = n_train_steps)
    
    es = tez.callbacks.EarlyStopping(monitor = &quot;valid_loss&quot;, patience = 3, model_path = &quot;model.bin&quot;)
    model.fit(
        train_dataset, 
        valid_dataset = valid_dataset, 
        device = &quot;cuda&quot;, 
        epochs = 10, 
        train_bs = 32,
        callbacks = [es],
    )
    
    
if __name__==&quot;__main__&quot;:
    train_model(fold = 0)
</code></pre>
<p>I found similar problem on <a href=""https://stackoverflow.com/questions/67506630/python-tqdm-progress-bar-stuck-at-0"">stackoverflow</a> but could not adapt the solution. Any help to fix this ll be appreciated.</p>
","transformer-model"
"69036265","Why does the 'i' need to be divided by 2 in caculating positional encoding?","2021-09-02 19:48:26","","1","207","<tensorflow><transformer-model>","<p>In this transformer tutorial:</p>
<p><a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<pre><code>def get_angles(pos, i, d_model):
  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))
  return pos * angle_rates
</code></pre>
<p>I don't understand why 'i//2' is used, since in the original formula there is no specification of the integer division.</p>
<p>So what's the purpose of i//2?</p>
","transformer-model"
"68991310","How to use multiple heads option in selfAttention class?","2021-08-30 23:30:13","","1","50","<nlp><transformer-model><attention-model><trax><self-attention>","<p>I am playing around with Self-attention model from <a href=""https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.research.efficient_attention.SelfAttention"" rel=""nofollow noreferrer"">trax</a> library.</p>
<p>when I set <code>n_heads=1</code>, everything works fine. But when I set <code>n_heads=2</code>, my code breaks.</p>
<p>I use only input activations and one SelfAttention layer.</p>
<p>Here is a minimal code:</p>
<pre><code>import trax
import numpy as np

attention = trax.layers.SelfAttention(n_heads=2)

activations = np.random.randint(0, 10, (1, 100, 1)).astype(np.float32)
input = (activations, )

init = attention.init(input)

output = attention(input)

</code></pre>
<p>But I have en error:</p>
<pre><code> File [...]/site-packages/jax/linear_util.py, line 166, in call_wrapped
    ans = self.f(*args, **dict(self.params, **kwargs))

  File [...]/layers/research/efficient_attention.py, line 1637, in forward_unbatched_h
    return forward_unbatched(*i_h, weights=w_h, state=s_h)

  File [...]/layers/research/efficient_attention.py, line 1175, in forward_unbatched
    q_info = kv_info = np.arange(q.shape[-2], dtype=np.int32)

IndexError: tuple index out of range
</code></pre>
<p>What I do wrong?</p>
","transformer-model"
"68962090","Transformer model using functional API","2021-08-28 07:04:15","","1","197","<python-3.x><nlp><tensorflow2.0><tf.keras><transformer-model>","<p>I started learning NLP a couple of months ago. So now i am trying to implement transformer model using functional API and i want to train this transformer model using model.fit. Encoder and Decoder part works just fine when i call them like this ' dec=decoder(8000, 2, 512, 256, 8, 0.1) '
and plot model using tf.keras.utils.plot_model</p>
<pre><code>def transformer(vocab_size,
            num_layers,
            units,
            d_model,
            num_heads,
            dropout,
            name=&quot;transformer&quot;):
inputs = tf.keras.Input(shape=(None,), name=&quot;inputs&quot;)
dec_inputs = tf.keras.Input(shape=(None,), name=&quot;dec_inputs&quot;)

enc_padding_mask = tf.keras.layers.Lambda(
  create_padding_mask, output_shape=(1, 1, None),
  name='enc_padding_mask')(inputs)

look_ahead_mask = tf.keras.layers.Lambda(
  create_look_ahead_mask,
  output_shape=(1, None, None),
  name='look_ahead_mask')(dec_inputs)

dec_padding_mask = tf.keras.layers.Lambda(
  create_padding_mask, output_shape=(1, 1, None),
  name='dec_padding_mask')(inputs)

enc_outputs = encoder(
  vocab_size=vocab_size,
  num_layers=num_layers,
  units=units,
  d_model=d_model,
  num_heads=num_heads,
  dropout=dropout,
)(inputs=[inputs, enc_padding_mask])

dec_outputs = decoder(
  vocab_size=vocab_size,
  num_layers=num_layers,
  units=units,
  d_model=d_model,
  num_heads=num_heads,
  dropout=dropout,
)(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])

outputs = tf.keras.layers.Dense(units=vocab_size, name=&quot;outputs&quot;)(dec_outputs)

return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)
</code></pre>
<p>But each time i try to call this transformer model like this, the Error shown below occur.</p>
<pre><code>NUM_LAYERS = 2
D_MODEL = 256
NUM_HEADS = 8
UNITS = 512
DROPOUT = 0.1


model = transformer(
vocab_size=8000,
num_layers=NUM_LAYERS,
units=UNITS,
d_model=D_MODEL,
num_heads=NUM_HEADS,
dropout=0.1)
</code></pre>
<p>Error shown below</p>
<pre><code>---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in 
_create_c_op(graph, node_def, inputs, control_inputs, op_def)
1879   try:
-&gt; 1880     c_op = pywrap_tf_session.TF_FinishOperation(op_desc)
1881   except errors.InvalidArgumentError as e:

InvalidArgumentError: Shape must be rank 1 but is rank 3 for '{{node look_ahead_mask/ones}} = 
Fill[T=DT_FLOAT, index_type=DT_INT32](look_ahead_mask/ones/packed, 
look_ahead_mask/ones/Const)' with input shapes: [2,?,?], [].

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
17 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in 
_create_c_op(graph, node_def, inputs, control_inputs, op_def)
1881   except errors.InvalidArgumentError as e:
1882     # Convert to ValueError for backwards compatibility.
-&gt; 1883     raise ValueError(str(e))
1884 
1885   return c_op

ValueError: Shape must be rank 1 but is rank 3 for '{{node look_ahead_mask/ones}} = 
Fill[T=DT_FLOAT, index_type=DT_INT32](look_ahead_mask/ones/packed, 
look_ahead_mask/ones/Const)' with input shapes: [2,?,?], [].
</code></pre>
","transformer-model"
"68936519","Transformer model is very slow and doesn't predict well","2021-08-26 10:06:24","","1","445","<python><tensorflow><machine-learning><keras><transformer-model>","<p>I created my first transformer model, after having worked so far with LSTMs. I created it for multivariate time series predictions - I have 10 different meteorological features (temperature, humidity, windspeed, pollution concentration a.o.) and with them I am trying to predict time sequences (24 consecutive values/hours) of air pollution. So my input has the shape <code>X.shape = (75575, 168, 10)</code> - 75575 time sequences, each sequence contains 168 hourly entries/vectors and each vector contains 10 meteo features. My output has the shape <code>y.shape = (75575, 24)</code> - 75575 sequences each containing 24 consecutive hourly values of the air pollution concentration.</p>
<p>I took as a model an <a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">example</a> from the official keras site. It is created for classification problems, I only took out the <code>softmax</code> activation and in the last dense layer I set the number of neurons to 24 and I hoped it would work. I runs and trains, but it doesn't do a better job than the LSTMs I have used on the same problem and more importantly - it is very slow - 4 min/epoch. Below I attach the model and I would like to know:</p>
<p>I) Have I done something wrong in the model? can the accuracy or speed be improved? Are there maybe some other parts of the code I need to change for it to work on regression, not classification problems?</p>
<p>II) Also, can a transformer at all work on multivariate problems of my kind (10 features input, 1 feature output) or do transformers only work on univariate problems? Tnx</p>
<pre><code>def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):

    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):

        # Normalization and Attention
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        x = layers.MultiHeadAttention(
            key_dim=head_size, num_heads=num_heads, dropout=dropout
        )(x, x)
        x = layers.Dropout(dropout)(x)
        res = x + inputs

        # Feed Forward Part
        x = layers.LayerNormalization(epsilon=1e-6)(res)
        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(dropout)(x)
        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
        x = x + res

    x = layers.GlobalAveragePooling1D(data_format=&quot;channels_first&quot;)(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(mlp_dropout)(x)
    x = layers.Dense(24)(x)
    return keras.Model(inputs, x)

model_tr = build_transformer_model(input_shape=(window_size, X_train.shape[2]), head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], mlp_dropout=0.4, dropout=0.25)
model_tr.compile(loss=&quot;mse&quot;,optimizer='adam') 
m_tr_history = model_tr.fit(x=X_train, y=y_train, validation_split=0.25, batch_size=64, epochs=10, callbacks=[modelsave_cb])
</code></pre>
","transformer-model"
"68874549","PyTorch Transformer: ValueError: Expected target size (2, 256), got torch.Size([2, 8, 256])","2021-08-21 15:50:44","","1","378","<python><machine-learning><pytorch><transformer-model>","<p>I am relatively new to transformers and thought of programming one from scratch with pytorch as a good exercise. I already tested the model and it worked. However, when implementing training for english-french translation tasks, I get the said error when computing the loss.</p>
<p>The code of the training function is as follows:</p>
<pre><code>    def train_(self, x, y, lr, steps, path=None):

        self.train()
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.parameters(), lr=lr)

        for epoch in range(steps):

            for batch_id, (batch_x, batch_y) in enumerate(zip(x, y)):

                if torch.cuda.is_available() and self.is_cuda:
                    batch_x = batch_x.cuda()
                    batch_y = batch_y.cuda()

                out = self(batch_x, batch_y)
                print(out.shape, batch_y.shape)

                # Embed batch_y so result is comparable
                batch_y = self.decoder.word_embedding(batch_y)
                print(batch_y.shape)

                loss = criterion(out, batch_y)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                print(f&quot;Training: epoch {epoch} batch {batch_id} loss {loss}&quot;)
</code></pre>
<p>The prints of the shapes give the following output:</p>
<pre><code>torch.Size([2, 8, 256]) torch.Size([2, 8])
torch.Size([2, 8, 256])
</code></pre>
<p>In terms of dimensionality, I'm using an embedding size of 256.</p>
<p>If needed, I can also provide the entire code.</p>
<p>Thank you.</p>
<p>Edit:</p>
<p>Here's the whole code</p>
<pre><code># Imports
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


# Hyper-parameters
lr = 0.001
steps = 1000


# Attention head
class AttentionHead(nn.Module):

    def __init__(self, embed_dim, head_dim):

        super(AttentionHead, self).__init__()

        self.embed_dim = embed_dim

        self.values_layer = nn.Linear(head_dim, head_dim, bias=False)
        self.keys_layer = nn.Linear(head_dim, head_dim, bias=False)
        self.queries_layer = nn.Linear(head_dim, head_dim, bias=False)

    def forward(self, values, keys, queries, mask=None):

        # Send them through the linear layers
        values = self.values_layer(values)
        keys = self.keys_layer(keys)
        queries = self.queries_layer(queries)

        # Multiply queries and keys to score matrix
        scores = torch.einsum(&quot;nah,nbh-&gt;nab&quot;, queries, keys)
        # Keys shape: (n, m, head_dim)

        # Queries shape: (n, m, heads_dim)
        # Score shape: (n, m, m)

        # If needed, then mask the score matrix
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float(&quot;-1e20&quot;))

        # Scale the (masked) score matrix
        scaled = scores / (self.embed_dim ** (1 / 2))

        # Normalize the scaled score matrix
        attention = torch.softmax(scaled, dim=1)

        # Multiply scores and values to output
        out = torch.einsum(&quot;nab,nbh-&gt;nah&quot;, attention, values)
        # attention shape: (n, m, m)
        # values shape: (n, m, head_dim)
        # out shape: (n, m, head_dim)

        return out


# Multi head attention mechanism
class MultiHeadAttentionBlock(nn.Module):

    def __init__(self, embed_dim, head_num):

        super(MultiHeadAttentionBlock, self).__init__()

        self.embed_dim = embed_dim
        self.head_num = head_num
        self.head_dim = embed_dim // head_num
        assert (head_num * self.head_dim == embed_dim), \
            &quot;Embed size is required to be dividable by heads.&quot;

        self.heads = nn.ModuleList(
            [AttentionHead(embed_dim, self.head_dim)
            for _ in range(head_num)]
        )

        self.out_layer = nn.Linear(embed_dim, embed_dim)

    def forward(self, values, keys, queries, mask=None):

        n = values.shape[0]  # Number of examples / batch size
        v_dim = values.shape[1]  # Quantity of embeddings
        k_dim = keys.shape[1]
        q_dim = queries.shape[1]

        # Split up the values, keys and queries
        values = values.reshape(n, v_dim, self.head_num, self.head_dim)
        keys = keys.reshape(n, k_dim, self.head_num, self.head_dim)
        queries = queries.reshape(n, q_dim, self.head_num, self.head_dim)

        # Iterate through heads
        for i, head in enumerate(self.heads):
            globals()[f&quot;out{i}&quot;] = head(values[:, :, i, :], keys[:, :, i, :], queries[:, :, i, :], mask)
            # out shape: (n, m, head_dim)

        # Concatenate the output of each head
        out = globals()[f&quot;out{0}&quot;]
        for i in range(self.head_num - 1):
            out = torch.cat((out, globals()[f&quot;out{i + 1}&quot;]), dim=2)
            # Out shape: (n, m, head_num * head_dim / embed_dim)

        # Send output through a last linear layer and return the outcome
        out = self.out_layer(out)
        return out


# Transformer block
class TransformerBlock(nn.Module):

    def __init__(self, embed_dim, head_num, dropout, forward_expansion):

        super(TransformerBlock, self).__init__()

        self.attention = MultiHeadAttentionBlock(embed_dim, head_num)

        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)

        self.feed_forward = nn.Sequential(
            nn.Linear(embed_dim, forward_expansion * embed_dim),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_dim, embed_dim)
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, values, keys, queries, mask=None):

        attention = self.attention(values, keys, queries, mask)
        x = self.dropout(self.norm1(attention + queries))

        forward = self.feed_forward(x)
        x = self.dropout(self.norm2(forward + x))

        return x


# Encoder
class Encoder(nn.Module):

    def __init__(self, src_vocab_dim, embed_dim, head_num, block_num, dropout, forward_expansion, max_length, device):

        super(Encoder, self).__init__()

        self.device = device
        self.embed_dim = embed_dim
        self.word_embedding = nn.Embedding(src_vocab_dim, embed_dim)
        self.position_embedding = nn.Embedding(max_length, embed_dim)  # max_length: max word length of all data

        self.blocks = nn.ModuleList(
            [TransformerBlock(embed_dim, head_num, dropout, forward_expansion)
            for _ in range(block_num)]
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, x):

        n, seq_length = x.shape  # (batch size, max word length of that batch)

        positions = torch.arange(0, seq_length).expand(n, seq_length).to(self.device)  # 0 - seq_length along dim 1
        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))

        for block in self.blocks:
            x = block(x, x, x)

        return x


# Decoder block
class DecoderBlock(nn.Module):

    def __init__(self, embed_dim, head_num, dropout, forward_expansion):

        super(DecoderBlock, self).__init__()

        self.attention = MultiHeadAttentionBlock(embed_dim, head_num)
        self.norm = nn.LayerNorm(embed_dim)
        self.transformer_block = TransformerBlock(embed_dim, head_num, dropout, forward_expansion)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, values, keys, mask):

        attention = self.attention(x, x, x, mask)
        # As the outputs of the decoder's first self attention block are the queries, the encoder's
        # output can be of different size. Only keys and values have to be indentical in size.
        queries = self.dropout(self.norm(attention + x))

        x = self.transformer_block(values, keys, queries)

        return x


# Decoder
class Decoder(nn.Module):

    def __init__(self, trg_vocab_dim, embed_dim, head_num, block_num, dropout, forward_expansion, max_length, device):

        super(Decoder, self).__init__()

        self.device = device
        self.embed_dim = embed_dim
        self.word_embedding = nn.Embedding(trg_vocab_dim, embed_dim)
        self.position_embedding = nn.Embedding(max_length, embed_dim)

        self.blocks = nn.ModuleList(
            [DecoderBlock(embed_dim, head_num, dropout, forward_expansion)
             for _ in range(block_num)]
        )

        self.dropout = nn.Dropout(dropout)
        self.out_layer = nn.Linear(embed_dim, embed_dim)  # changed embed_dim (second time in bracket) from trg_vocab_dim

    def forward(self, x, enc_out, mask):

        n, seq_length = x.shape

        positions = torch.arange(0, seq_length).expand(n, seq_length).to(self.device)  # 0 - seq_length along dim 1
        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))

        for block in self.blocks:
            x = block(x, enc_out, enc_out, mask)

        x = self.out_layer(x)

        return x


# Transformer
class Transformer(nn.Module):

    def __init__(self, src_vocab_dim, trg_vocab_dim, embed_dim, head_num, block_num_enc, block_num_dec,
                 dropout, forward_expansion, max_length, device):

        super(Transformer, self).__init__()

        self.device = device

        self.encoder = Encoder(src_vocab_dim, embed_dim, head_num, block_num_enc, dropout, forward_expansion, max_length, device)
        self.decoder = Decoder(trg_vocab_dim, embed_dim, head_num, block_num_dec, dropout, forward_expansion, max_length, device)

    def make_mask(self, y):

        n, m = y.shape
        mask = torch.tril(torch.ones((m, m))).expand(n, m, m)

        return mask.to(self.device)

    def forward(self, x, y):

        mask = self.make_mask(y)

        out_enc = self.encoder(x)
        out_dec = self.decoder(y, out_enc, mask)

        return out_dec

    def train_(self, x, y, lr, steps, path=None):

        self.train()
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.parameters(), lr=lr)

        for epoch in range(steps):

            for batch_id, (batch_x, batch_y) in enumerate(zip(x, y)):

                if torch.cuda.is_available() and self.is_cuda:
                    batch_x = batch_x.cuda()
                    batch_y = batch_y.cuda()

                out = self(batch_x, batch_y)
                print(out.shape, batch_y.shape)

                # Embed batch_y so result is comparable
                batch_y = self.decoder.word_embedding(batch_y)
                print(batch_y.shape)

                loss = criterion(out, batch_y)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                print(f&quot;Training: epoch {epoch} batch {batch_id} loss {loss}&quot;)

            if path is not None:
                torch.save(self, path)


# Run
if __name__ == &quot;__main__&quot;:

    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

    # use for normal run
    # x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)  # input
    # y = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(device)  # target

    # added one bracket for training so this is one batch
    x = torch.tensor([[[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]]).to(device)  # input
    y = torch.tensor([[[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]]).to(device)  # target

    src_vocab_dim = 10
    trg_vocab_dim = 10

    model = Transformer(src_vocab_dim, trg_vocab_dim, embed_dim=256, head_num=8, block_num_enc=6, block_num_dec=6,
                 dropout=0, forward_expansion=4, max_length=100, device=device)

    model.train_(x, y, lr, steps)
</code></pre>
<p>Here's the whole error trace:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:/Users/user/PycharmProjects/Transformer/Code.py&quot;, line 310, in &lt;module&gt;
    model.train_(x, y, lr, steps)
  File &quot;C:/Users/user/PycharmProjects/Transformer/Code.py&quot;, line 279, in train_
    loss = criterion(out, batch_y)
  File &quot;C:\Users\user\Anaconda3\envs\Transformer\lib\site-packages\torch\nn\modules\module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;C:\Users\user\Anaconda3\envs\Transformer\lib\site-packages\torch\nn\modules\loss.py&quot;, line 948, in forward
    ignore_index=self.ignore_index, reduction=self.reduction)
  File &quot;C:\Users\user\Anaconda3\envs\Transformer\lib\site-packages\torch\nn\functional.py&quot;, line 2422, in cross_entropy
    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
  File &quot;C:\Users\user\Anaconda3\envs\Transformer\lib\site-packages\torch\nn\functional.py&quot;, line 2228, in nll_loss
    out_size, target.size()))
ValueError: Expected target size (2, 256), got torch.Size([2, 8, 256])
</code></pre>
<p>Concerning your question:</p>
<p>My thoughts were that the loss function cannot compare the output with the target if the target is not embedded as well. With the embedding missing, this gives the following shapes:</p>
<pre><code>torch.Size([2, 8])  # target
torch.Size([2, 8, 256])  # output
</code></pre>
","transformer-model"
"68852988","How to get output from intermediate encoder layers in PyTorch Transformer?","2021-08-19 18:32:45","68854535","1","3626","<pytorch><transformer-model>","<p>I have trained a fairly simple Transformer model with 6 TransformerEncoder layers:</p>
<pre><code>class LitModel(pl.LightningModule):
    def __init__(self,
                 num_tokens: int,
                 dim_model: int = 96,
                 dim_h: int = 128,
                 n_head: int = 1,
                 dropout: float = 0.1,
                 activation: str = 'relu',
                 num_layers: int = 2,
                 lr: float=1e-3):
        &quot;&quot;&quot;

        :param num_tokens:
        :param dim_model:
        :param dim_h:
        :param n_head:
        :param dropout:
        :param activation:
        :param num_layers:
        &quot;&quot;&quot;
        super().__init__()
        self.lr = lr
        self.embed = torch.nn.Embedding(num_embeddings=num_tokens,
                                        embedding_dim=dim_model)
        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=dim_model,
                                                         nhead=n_head,
                                                         dim_feedforward=dim_h,
                                                         dropout=dropout,
                                                         activation=activation,
                                                         batch_first=True)
        self.encoder = torch.nn.TransformerEncoder(encoder_layer=encoder_layer,
                                                   num_layers=num_layers)
        self.linear = torch.nn.Linear(in_features=dim_model, out_features=num_tokens)

    def forward(self, indices, mask):
        x = self.embed(indices)
        x = self.encoder(x, src_key_padding_mask=mask)
        return x

    def training_step(self, batch, batch_idx):
        x = batch['src']
        y = batch['label']
        mask = batch['mask']

        x = self.embed(x)
        x = self.encoder(x, src_key_padding_mask=mask)
        x = self.linear(x)

        loss = F.cross_entropy(input=x.transpose(1, 2),
                               target=y,
                               ignore_index=0)
        self.log('train_loss', loss)
        return loss
</code></pre>
<p>After training the model to predict [MASK] tokens (exactly like BERT), I would like to be able to extract the outputs from the lower layers, specifically, the second to last <code>TransformerEncoderLayer</code>, which may give a better vector encoding than the final layer (according to the original BERT paper). I'm not sure how to go about doing this.</p>
","transformer-model"
"68800382","How to pass possible class names to distilbert","2021-08-16 09:30:42","","0","444","<python><machine-learning><pytorch><huggingface-transformers><transformer-model>","<p>I've been trying to get distilbert to work and I've downloaded the model and used AutoTokenizer.from_pretrained() and AutoModelForSequenceClassification.from_pretrained(). I tried for a couple days now to pass the parameters from the &quot;Possible class names&quot; on the huggingface model card page: <a href=""https://huggingface.co/typeform/distilbert-base-uncased-mnli?candidateLabels=positive%2C+negative%2C+neutral&amp;multiClass=true&amp;text=which+stocks+will+go+down+during+new+years"" rel=""nofollow noreferrer"">https://huggingface.co/typeform/distilbert-base-uncased-mnli?candidateLabels=positive%2C+negative%2C+neutral&amp;multiClass=true&amp;text=which+stocks+will+go+down+during+new+years</a></p>
<p>I tried:</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained('.')
model = AutoModelForSequenceClassification.from_pretrained('.')

text = &quot;Dummy text&quot;
text += &quot;[SEP]Positive[SEP]Neutral[SEP]Negative&quot;
encodedInput = tokenizer(text, return_tensors=&quot;pt&quot;)
output = model(**encodedInput)
print(output)
</code></pre>
<p>It's supposed to output entailment values for &quot;Positive&quot;, &quot;Neutral&quot;, and &quot;Negative&quot;.
Anyone know how to do it? I'm using pytorch.</p>
","transformer-model"
"68797901","Training an Transformer Encoder layer directly and the proper way to pad sequences","2021-08-16 05:47:41","68798352","2","2342","<deep-learning><pytorch><transformer-model>","<p>I am working on a problem in which I want to train a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html"" rel=""nofollow noreferrer"">Transformer Encoder Layer</a> directly (i.e. with no embedding layer). I already have the sequences of embeddings that I will treat as my dataset. I am confused about how I should handle the padding and the attention mask and would simply like to make sure that my understanding is correct.</p>
<p>My sequences have lengths varying between as little as 3 to as many as 130. Does this mean that I should pad all my sequences to have 130 parts? If so does it matter which value I pad with?</p>
<p>For the attention mask, I believe that I want each part to attend to all other parts in the sequence. In the <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">docs</a> I see that they have it set up such that each part is only allowed to attend to earlier parts in the sequence. Is this the most natural approach or is it just for the language modeling task? Also why (in the same link) do they use -Inf and 0 for the values of the attention mask as opposed to simply 1s and 0s?</p>
<p>As a little toy example, say that I have two samples in my dataset with sequence lengths of 2 and 3 respectively (AKA 3 is the max):</p>
<pre><code>s_1 = torch.Tensor([0.001, 0.002, ..., 0.768], [0.001, 0.002, ..., 0.768]) # length 2
s_2 = torch.Tensor([0.001, 0.002, ..., 0.768], [0.001, 0.002, ..., 0.768], [0.001, 0.002, ..., 0.768]) # length 3
</code></pre>
<p>Does this mean that I should then pad <code>s_1</code> to have length 3? And do something like:</p>
<pre><code>s_1 = torch.Tensor([0.001, 0.002, ..., 0.768], [0.001, 0.002, ..., 0.768], [0, 0, ..., 0])
</code></pre>
<p>And then my attention masks would then look like:</p>
<pre><code>attn_mask_s1 = [[0    -Inf    0],
                [-Inf   0     0],
                [0      0     0]]


attn_mask_s2 = [[0     -Inf   -Inf],
                [-Inf   0     -Inf],
                [-Inf  -Inf    0  ]]
</code></pre>
<p>Sorry to package so many questions into one but they all break down my doubts of how data should be passed to the TransformerEncoder block.</p>
","transformer-model"
"68790740","Questions about Transformer evaluation part","2021-08-15 10:35:36","","0","60","<python><deep-learning><transformer-model>","<p>And this is the time series prediction of Transformer</p>
<p>I'm trying to train the model, but I got <code>Validation loss: nan</code> , <code>R_p=nan</code></p>
<p>when I check the train_loss, I found the loss function is nothing wrong, but the <code>test_loss and val_loss are nan</code></p>
<p>I could not figure out which part is wrong.</p>
<pre><code>t0: previous t0 data points to predict from

N: number of data points
</code></pre>
<pre><code>t0 = 2073
future = 231

def Dp(y_pred,y_true,q):
    return max([q*(y_pred-y_true),(q-1)*(y_pred-y_true)])

def Rp_num_den(y_preds,y_trues,q):
    numerator = np.sum([Dp(y_pred,y_true,q) for y_pred,y_true in zip(y_preds,y_trues)])
    denominator = np.sum([np.abs(y_true) for y_true in y_trues])
    return numerator,denominator


def train_epoch(model, train_dl, t0):
    model.train()
    train_loss = 0
    n = 0
    for step, (x, y, attention_masks) in enumerate(train_dl):
        optimizer.zero_grad()
        output = model(x.to(device), y.to(device), attention_masks[0].to(device))
        loss = criterion(output.squeeze()[:, (t0 - 1):(t0 + future - 1)], y.to(device)[:, t0:])  # not missing data
        # loss = criterion(output.squeeze()[:,(t0-1-10):(t0+24-1-10)],y.cuda()[:,(t0-10):]) # missing data
        loss.backward()
        optimizer.step()

        train_loss += (loss.detach().cpu().item() * x.shape[0])
        n += x.shape[0]
    return train_loss / n


def eval_epoch(model, val_dl, t0):
    model.eval()
    eval_loss = 0
    n = 0
    with torch.no_grad():
        for step, (x, y, attention_masks) in enumerate(val_dl):
            output = model(x.to(device), y.to(device), attention_masks[0].to(device))
            loss = criterion(output.squeeze()[:, (t0 - 1):(t0 + future - 1)], y.to(device)[:, t0:])  # not missing dataloss = {Tensor} tensor(nan, device='cuda:0')
            # loss = criterion(output.squeeze()[:,(t0-1-10):(t0+24-1-10)],y.cuda()[:,(t0-10):]) # missing data

            eval_loss += (loss.detach().cpu().item() * x.shape[0])
            n += x.shape[0]

    return eval_loss / n


def test_epoch(model, test_dl, t0):
    with torch.no_grad():
        predictions = []
        observations = []

        model.eval()
        for step, (x, y, attention_masks) in enumerate(test_dl):
            output = model(x.to(device), y.to(device), attention_masks[0].to(device))

            for p, o in zip(output.squeeze()[:, (t0 - 1):(t0 + future - 1)].cpu().numpy().tolist(),
                            y.to(device)[:, t0:].cpu().numpy().tolist()):  # not missing data
                # for p,o in zip(output.squeeze()[:,(t0-1-10):(t0+24-1-10)].cpu().numpy().tolist(),y.cuda()[:,(t0-10):].cpu().numpy().tolist()): # missing data

                predictions.append(p)
                observations.append(o)

        num = 0
        den = 0
        for y_preds, y_trues in zip(predictions, observations):
            num_i, den_i = Rp_num_den(y_preds, y_trues, .5)
            num += num_i
            den += den_i
        Rp = (2 * num) / den

        print(Rp)

    return Rp


train_epoch_loss = []
eval_epoch_loss = []
Rp_best = 10
model_save_path = 'ConvTransformer_nologsparse.pth'
for e, epoch in enumerate(range(epochs)):
    train_loss = []
    eval_loss = []

    l_t = train_epoch(model, train_dl, t0)
    train_loss.append(l_t)

    l_e = eval_epoch(model, val_dl, t0)
    eval_loss.append(l_e)

    Rp = test_epoch(model, test_dl, t0)

    if Rp_best &gt; Rp:
        Rp_best = Rp

    train_epoch_loss.append(np.mean(train_loss))
    eval_epoch_loss.append(np.mean(eval_loss))

    print(&quot;Epoch {}: Train loss: {} \t Validation loss: {} \t R_p={}&quot;.format(e,
                                                                             np.mean(train_loss),
                                                                             np.mean(eval_loss), Rp))
print(&quot;Rp best={}&quot;.format(Rp_best))
</code></pre>
<p>output:</p>
<pre><code>Epoch 94: Train loss: 0.5179461240768433     Validation loss: nan    R_p=nan
nan
Epoch 95: Train loss: 0.5195054411888123     Validation loss: nan    R_p=nan
nan
Epoch 96: Train loss: 0.5231510996818542     Validation loss: nan    R_p=nan
nan
Epoch 97: Train loss: 0.520694375038147      Validation loss: nan    R_p=nan
nan
Epoch 98: Train loss: 0.525615394115448      Validation loss: nan    R_p=nan
nan
Epoch 99: Train loss: 0.5247501134872437     Validation loss: nan    R_p=nan
Rp best=10

</code></pre>
","transformer-model"
"68707747","ViVIT PyTorch: RuntimeError: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15","2021-08-09 06:35:47","","0","380","<nlp><pytorch><computer-vision><loss-function><transformer-model>","<p>I am trying to run Video Vision Transformer (ViViT) code with my dataset but getting an error using <strong>CrossEntropyLoss</strong> from Pytorch as the Loss function.</p>
<p>There are 6 classes I have:</p>
<pre><code>['Run', 'Sit', 'Walk', 'Wave', 'Sit', 'Stand']
</code></pre>
<p><strong>Optimizer</strong></p>
<pre><code>optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=1e-9, momentum=0.9)
</code></pre>
<p><strong>Class Weights</strong></p>
<pre><code>tensor([0.0045, 0.0042, 0.0048, 0.0038, 0.0070, 0.0065])
</code></pre>
<p><strong>Loss Function</strong></p>
<pre><code>loss_func = nn.CrossEntropyLoss(weight=class_weights.to(device))
</code></pre>
<p><strong>Code Throwning Error</strong></p>
<pre><code>train_epoch(model, optimizer, train_loader, train_loss_history, loss_func)
</code></pre>
<p><strong>Error</strong></p>
<p><a href=""https://i.sstatic.net/CIJaD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CIJaD.png"" alt=""Error Stack"" /></a></p>
<pre><code>RuntimeError: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15
</code></pre>
<p><strong>Code Calling the transformer</strong></p>
<pre><code>model = ViViT(224, 16, 100, 16).cuda()
</code></pre>
<p><strong>Getting Video Frames</strong></p>
<pre><code>def get_frames(filename, n_frames=1):
    frames = []
    v_cap = cv2.VideoCapture(filename)
    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_list = np.linspace(0, v_len - 1, n_frames + 1, dtype=np.int16)
    frame_dims = np.array([224, 224, 3])
    for fn in range(v_len):
        success, frame = v_cap.read()
        if success is False:
            continue
        if (fn in frame_list):
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frame = cv2.resize(frame, (frame_dims[0], frame_dims[1]))
            frames.append(frame)
    v_cap.release()
    return frames, v_len
</code></pre>
<p><strong>Dataset Preprocessing</strong></p>
<pre><code>class DatasetProcessing(data.Dataset):
    def __init__(self, df, root_dir):
        super(DatasetProcessing, self).__init__()
        # List of all videos path
        video_list = df[&quot;Video&quot;].apply(lambda x: root_dir + '/' + x)
        self.video_list = np.asarray(video_list)
        self.df = df    
    def __getitem__(self, index):
        # Ensure that the raw videos are in respective folders and folder name matches the output class label
        video_label = self.video_list[index].split('/')[-2]
        video_name = self.video_list[index].split('/')[-1]
        
        video_frames, len_ = get_frames(self.video_list[index], n_frames = 15)
        video_frames = np.asarray(video_frames)
        video_frames = video_frames/255
        class_list = ['Run', 'Walk', 'Wave', 'Sit', 'Turn', 'Stand']
        class_id_loc = np.where(class_list == video_label)
        label = class_id_loc
        d = torch.as_tensor(np.array(video_frames).astype('float'))
        l = torch.as_tensor(np.array(label).astype('float'))
        return (d, l)

    def __len__(self):
        return self.video_list.shape[0]
</code></pre>
<p><strong>Training Epochs</strong></p>
<pre><code>def train_epoch(model, optimizer, data_loader, loss_history, loss_func):
    total_samples = len(data_loader.dataset)
    model.train()

    for i, (data, target) in enumerate(data_loader):
        optimizer.zero_grad()
        x = data.cuda()
        data = rearrange(x, 'b p h w c -&gt; b p c h w').cuda()
        target = target.type(torch.LongTensor).cuda()
        pred = model(data.float())
        output = F.log_softmax(pred, dim=1)
        loss = loss_func(output, target.squeeze(1))
        loss.backward()
        optimizer.step()

        if i % 100 == 0:
            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +
                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +
                  '{:6.4f}'.format(loss.item()))
            loss_history.append(loss.item())
</code></pre>
<p><strong>Evaluate Model</strong></p>
<pre><code>def evaluate(model, data_loader, loss_history, loss_func):
    model.eval()

    total_samples = len(data_loader.dataset)
    correct_samples = 0
    total_loss = 0

    with torch.no_grad():
        for data, target in data_loader:
            x = data.cuda()
            data = rearrange(x, 'b p h w c -&gt; b p c h w').cuda()
            target = target.type(torch.LongTensor).cuda()
            output = F.log_softmax(model(data.float()), dim=1)
            loss = loss_func(output, target)
            _, pred = torch.max(output, dim=1)
            
            total_loss += loss.item()
            correct_samples += pred.eq(target).sum()

    avg_loss = total_loss / total_samples
    loss_history.append(avg_loss)
    print('\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +
          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +
          '{:5}'.format(total_samples) + ' (' +
          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\n')
</code></pre>
<p><strong>Transformer</strong></p>
<pre><code>class Transformer(nn.Module):
    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):
        super().__init__()
        self.layers = nn.ModuleList([])
        self.norm = nn.LayerNorm(dim)
        for _ in range(depth):
            self.layers.append(nn.ModuleList([
                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),
                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))
            ]))

    def forward(self, x):
        for attn, ff in self.layers:
            x = attn(x) + x
            x = ff(x) + x
        return self.norm(x)
</code></pre>
<p><strong>ViViT Code</strong></p>
<pre><code>class ViViT(nn.Module):
    def __init__(self, image_size, patch_size, num_classes, num_frames, dim = 192, depth = 4, heads = 3, pool = 'cls', in_channels = 3, dim_head = 64, dropout = 0.,
                 emb_dropout = 0., scale_dim = 4, ):
        super().__init__()
        
        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'

        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'
        num_patches = (image_size // patch_size) ** 2
        patch_dim = in_channels * patch_size ** 2
        self.to_patch_embedding = nn.Sequential(
            Rearrange('b t c (h p1) (w p2) -&gt; b t (h w) (p1 p2 c)', p1 = patch_size, p2 = patch_size),
            nn.Linear(patch_dim, dim),
        )

        self.pos_embedding = nn.Parameter(torch.randn(1, num_frames, num_patches + 1, dim))
        self.space_token = nn.Parameter(torch.randn(1, 1, dim))
        self.space_transformer = Transformer(dim, depth, heads, dim_head, dim*scale_dim, dropout)

        self.temporal_token = nn.Parameter(torch.randn(1, 1, dim))
        self.temporal_transformer = Transformer(dim, depth, heads, dim_head, dim*scale_dim, dropout)

        self.dropout = nn.Dropout(emb_dropout)
        self.pool = pool

        self.mlp_head = nn.Sequential(
            nn.LayerNorm(dim),
            nn.Linear(dim, num_classes)
        )

    def forward(self, x):
        x = self.to_patch_embedding(x)
        b, t, n, _ = x.shape

        cls_space_tokens = repeat(self.space_token, '() n d -&gt; b t n d', b = b, t=t)
        x = torch.cat((cls_space_tokens, x), dim=2)
        x += self.pos_embedding[:, :, :(n + 1)]
        x = self.dropout(x)

        x = rearrange(x, 'b t n d -&gt; (b t) n d')
        x = self.space_transformer(x)
        x = rearrange(x[:, 0], '(b t) ... -&gt; b t ...', b=b)

        cls_temporal_tokens = repeat(self.temporal_token, '() n d -&gt; b n d', b=b)
        x = torch.cat((cls_temporal_tokens, x), dim=1)
        x = self.temporal_transformer(x)
        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]

        return self.mlp_head(x)
</code></pre>
","transformer-model"
"68684096","Spacy 3.1 example code for Transformers in the online documentation seems to be wrong","2021-08-06 15:26:42","","1","2029","<transformer-model><code-documentation><spacy-3><spacy-transformers>","<p>In the latest documentation for Spacy, the following example is given at the following link:</p>
<p><a href=""https://spacy.io/usage/embeddings-transformers"" rel=""nofollow noreferrer"">https://spacy.io/usage/embeddings-transformers</a></p>
<pre><code>import spacy
from spacy.tokens import Doc
from spacy_transformers import TransformerData
from thinc.api import set_gpu_allocator, require_gpu


def custom_annotation_setter(docs, trf_data):
    doc_data = list(trf_data.doc_data)
    for doc, data in zip(docs, doc_data):
        doc._.custom_attr = data


nlp = spacy.load(&quot;en_core_web_trf&quot;)
nlp.get_pipe(&quot;transformer&quot;).set_extra_annotations = custom_annotation_setter
doc = nlp(&quot;This is a text&quot;)
assert isinstance(doc._.custom_attr, TransformerData)
print(doc._.custom_attr.tensors)
</code></pre>
<p>This code throws an exception when it try's to process the test data:</p>
<p>AttributeError: [E047] Can't assign a value to unregistered extension attribute 'custom_attr'. Did you forget to call the <code>set_extension</code> method?</p>
<p>I set the extension using:</p>
<p>Doc.set_extension('custom_attr', default=True)</p>
<p>My question is, should the Transform class handle adding this special extension itself (as is implied in the example code), or is this just a bug in the example?</p>
","transformer-model"
"68674996","How to save BERT Huggingface Question Answer transformer pipeline as a reusable model","2021-08-06 01:06:17","","1","555","<raspberry-pi><bert-language-model><transformer-model>","<p>I have written a Question/Answer BERT application that uses the transformer pipeline protocol.  I would like to port this to the Raspberry PI 4.  Is there a way to capture the complete cached  inference transformers pipeline model, quantize it, save it, and convert it to Tensorflow Lite model?  I want the user to be able to ask multiple ad hoc questions against the fine-tuned model.</p>
","transformer-model"
"68637964","ValueError: Input 0 is incompatible with layer model error for 1-D time series classification model","2021-08-03 14:30:38","","0","54","<python><tensorflow><keras><transformer-model>","<p>I'm attempting to follow this <a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">time series classification with transformers</a> with Keras tutorial. This is the relevant part of my code:</p>
<pre><code>x_trainScaledNPArray = np.array(x_trainScaled)
x_testScaledNPArray = np.array(x_testScaled)
y_trainNPArray = np.array(y_train)
y_testNPArray = np.array(y_test)

print(x_trainScaledNPArray.shape)
print(x_testScaledNPArray.shape)
print(y_trainNPArray.shape)
print(y_testNPArray.shape)

n_classes = len(np.unique(y_train))
input_shapeIndex0 = x_trainScaledNPArray.shape[0:]
print(input_shapeIndex0)

model = build_model(n_classes,input_shapeIndex0,head_size=256,num_heads=4,ff_dim=4,num_transformer_blocks=4,mlp_units=[128],mlp_dropout=0.4,dropout=0.25)
model.compile(loss=&quot;sparse_categorical_crossentropy&quot;,optimizer=keras.optimizers.Adam(learning_rate=1e-4),metrics=[&quot;sparse_categorical_accuracy&quot;])

callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]
model.fit(x_trainScaledNPArray,y_trainNPArray,validation_split=0.2,epochs=200,batch_size=64,callbacks=callbacks)
</code></pre>
<p>This is the output I get:</p>
<pre><code>(18287, 2048)
(347, 2048)
(18287,)
(347,)
(18287, 2048)
[...]

ValueError: Input 0 is incompatible with layer model: expected shape=(None, 18287, 2048), found shape=(None, 2048)
</code></pre>
<p>I already tried to solve this by following the hints given <a href=""https://stackoverflow.com/questions/47665391/keras-valueerror-input-0-is-incompatible-with-layer-conv2d-1-expected-ndim-4"">here</a> and <a href=""https://stackoverflow.com/questions/53249386/valueerror-input-0-is-incompatible-with-layer-conv2d-1-expected-ndim-4-found"">here</a> but without any success. Any help would be highly appreciated.</p>
","transformer-model"
"68603462","How to extract Sentence Embedding Using BERT model from [CLS] token","2021-07-31 15:28:41","","0","3646","<python-3.x><embedding><bert-language-model><transformer-model>","<p>I am following this link:</p>
<p><a href=""https://stackoverflow.com/questions/63209960/bert-document-embedding"">BERT document embedding</a></p>
<p>I want to extract sentence-embedding using <code>BERT</code> model using <code>CLS</code> token. Here is the code:</p>
<pre><code>import torch
from keras.preprocessing.sequence import pad_sequences
import tensorflow as tf

def text_to_embedding(tokenizer, model, in_text):
    '''
    Uses the provided BERT 'model' and 'tokenizer' to generate a vector
    representation of the input string, 'in_text'.

    Returns the vector stored as a numpy ndarray.
    '''

    # ===========================
    #   STEP 1: Tokenization
    # ===========================

    MAX_LEN = 510

    # 'encode' will:
    #  (1) Tokenize the sentence
    #  (2) Prepend the '[CLS]' token to the start.
    #  (3) Append the '[SEP]' token to the end.
    #  (4) Map tokens to their IDs.
    input_ids = tokenizer.encode(
        in_text,                         # sentence to encode.
        add_special_tokens = True,       # Add '[CLS]' and '[SEP]'
        max_length = MAX_LEN,            # Truncate all sentences.
        #return_tensors = 'pt'           # Return pytorch tensors.
    )

    print(input_ids)
    print(tokenizer.decode(input_ids))

    # Pad our input tokens. Truncation was handled above by the 'encode'
    # function, which also makes sure that the '[SEP]' token is placed at the
    # end *after* truncating.
    # Note: 'pad_sequences' expects a list of lists, but we only have one
    # piece of text, so we surround 'input_ids' with an extra set of brackets.

    results = tokenizer(in_text, max_length=MAX_LEN, truncation=True)
    input_ids = results.input_ids
    attn_mask = results.attention_mask
    
    print(results)

    # Cast to tensors.
    input_ids = torch.tensor(input_ids)
    attn_mask = torch.tensor(attn_mask)

    # Add an extra dimension for the &quot;batch&quot; (even though there is only one
    # input in this batch)
    input_ids = input_ids.unsqueeze(0)
    attn_mask = attn_mask.unsqueeze(0)


    # ===========================
    #   STEP 1: Tokenization
    # ===========================

    # Put the model in evaluation mode--the dropout layers behave differently
    # during evaluation.
    
    #model.eval()

    # Copy the inputs to the GPU
    #input_ids = input_ids.to(device)
    #attn_mask = attn_mask.to(device)

    # telling the model not to build the backward graph will make this
    # a little quicker.
    with torch.no_grad():

        # Forward pass, returns hidden states and predictions
        # This will return the logits rather than the loss because we have
        # not provided labels.
        outputs = model(input_ids = input_ids,token_type_ids = None,attention_mask = attn_mask)
        

        hidden_states = outputs[2]

        #Sentence Vectors
        #To get a single vector for our entire sentence we have multiple 
        #application-dependent strategies, but a simple approach is to 
        #average the second to last hiden layer of each token producing 
        #a single 768 length vector.
        # `hidden_states` has shape [13 x 1 x ? x 768]

        # `token_vecs` is a tensor with shape [? x 768]
        token_vecs = hidden_states[-2][0]

        # Calculate the average of all ? token vectors.
        sentence_embedding = torch.mean(token_vecs, dim=0)
        # Move to the CPU and convert to numpy ndarray.
        sentence_embedding = sentence_embedding.detach().cpu().numpy()

        return(sentence_embedding)


from transformers import BertTokenizer, BertModel
# Load pre-trained model (weights)
model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states = True), # Whether the model returns all hidden-states.
#model.cuda()

from transformers import BertTokenizer

# Load the BERT tokenizer.
print('Loadin BERT tokenizer...')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

k=text_to_embedding(tokenizer, model, &quot;I like to play cricket&quot;)
</code></pre>
<p>Output:</p>
<pre><code>&lt;ipython-input-14-f03410b60544&gt; in text_to_embedding(tokenizer, model, in_text)
     77         # This will return the logits rather than the loss because we have
     78         # not provided labels.
---&gt; 79         outputs = model(input_ids = input_ids,token_type_ids = None,attention_mask = attn_mask)
     80 
     81 

TypeError: 'tuple' object is not callable
</code></pre>
<p>I get an error in this line <code>outputs = model(input_ids = input_ids,token_type_ids = None,attention_mask = attn_mask)</code></p>
<p>Instead of using average of hidden layer, I want to modify code to get embedding for input sentence using <code>CLS </code> token.</p>
","transformer-model"
"68586645","AttributeError: 'PositionalEncoding' object has no attribute 'position'","2021-07-30 06:55:15","","1","1119","<python><tensorflow><machine-learning><chatbot><transformer-model>","<p>I am unable to understand that why i am getting this error.Code is working fine without saving the model but when i run <code>model.save()</code> it throws exception.
<strong>Error is</strong></p>
<blockquote>
<p>AttributeError:Traceback (most recent call last)
 in ()
1 model.summary()
2 model.save('/content/Data/chatbot_model',include_optimizer=False)</p>
</blockquote>
<blockquote>
<p>in get_config(self)</p>
</blockquote>
<pre><code>  9         config = super(PositionalEncoding, self).get_config()
 10         config.update({'position': self.position,
 12             'd_model': self.d_model,
 13 
</code></pre>
<blockquote>
<p>AttributeError: 'PositionalEncoding' object has no attribute 'position'</p>
</blockquote>
<p><strong>Here is the class code of Positional Encoding Class</strong></p>
<pre><code>
 def __init__(self, position, d_model):
   super(PositionalEncoding, self).__init__()
   self.pos_encoding = self.positional_encoding(position, d_model)
 
 def get_config(self):

       config = super(PositionalEncoding, self).get_config()
       config.update({
           'position': self.position,
           'd_model': self.d_model,
           
       })
       return config

 def get_angles(self, position, i, d_model):
   angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))
   return position * angles

 def positional_encoding(self, position, d_model):
   angle_rads = self.get_angles(
       position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],
       i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],
       d_model=d_model)
   # apply sin to even index in the array
   sines = tf.math.sin(angle_rads[:, 0::2])
   # apply cos to odd index in the array
   cosines = tf.math.cos(angle_rads[:, 1::2])

   pos_encoding = tf.concat([sines, cosines], axis=-1)
   pos_encoding = pos_encoding[tf.newaxis, ...]
   return tf.cast(pos_encoding, tf.float32)

 def call(self, inputs):
   return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]```
</code></pre>
","transformer-model"
"68573795","using spacy to extract tensor by token id","2021-07-29 09:54:23","68576309","1","394","<python><nlp><spacy><transformer-model><spacy-3>","<p>I'm using spacy 3.0 to vectorize a text with a transformer model. Due to data privacy reason the vectorization has to be on a different machine than the one that trains the model. To reduce the amount of data I generate and would have to transfer between machines, I extract the token ids of the text like this:</p>
<pre><code>import spacy
nlp = spacy.load(&quot;de_dep_news_trf&quot;)
doc = nlp(&quot;Eine Bank steht im Park.&quot;)
print(doc._.trf_data.tokens[&quot;input_ids&quot;])
</code></pre>
<p>which returns</p>
<pre><code>tensor([[    3,   917,  2565,  1302,   106,  3087, 26914,     4]])
</code></pre>
<p>Having the ids now, is it possible to extract the correct tensors from the language model (<code>de_dep_news_trf</code>) using spacy?</p>
","transformer-model"
"68493707","How to reshape in PyTorch. [1, 257, 512] ->[1, 512, 16,16]","2021-07-23 03:31:22","","1","118","<python><pytorch><transformer-model>","<p>I am currently working on image generation using transformers.</p>
<p>I used the Vit part as it is for the encoder part.</p>
<p>In addition, I want to attach a transformer decoder and pass the encoder output, and put the output of the transformer decoder into the CNN decoder to create an image.</p>
<p>With:</p>
<pre><code>image size = 128 * 128
patch_size = 8
d_model = 512
</code></pre>
<p>The output of the transformer decoder is <code>[1, 257, 512]</code>.</p>
<p><code>[1,257,512]</code> =&gt; <code>hw/64 * 512</code> i want reshape <code>h/8 * w/8 * 512</code>, but I'm not sure how to reshape it.</p>
<p>How can I transform <code>257</code> into <code>256</code>?</p>
<p>i use</p>
<pre><code>decoder_out = decoder_out.permute(0, 2, 1).view(1, self.d_model, 16, 16)
</code></pre>
","transformer-model"
"68477306","Positional Encoding for time series based data for Transformer DNN models","2021-07-21 22:50:03","68478924","3","6947","<python><tensorflow><deep-learning><pytorch><transformer-model>","<p>In several academic papers, researchers use the following positional encoding to denote the positioning of elements in a sequence, whether it be a time series-based sequence or words in a sentence for NLP purposes.</p>
<p>My question is how the positioning is actually applied to the data before it is fed to the deep neural network (in my case a transformer network):</p>
<ul>
<li>Are the positional values added directly to the actual values of the elements in the sequence (or to the word representation values)? Or are they concatinated? Is the positional embedding part of the data preprocessing stage?</li>
<li>Does the Tensorflow/Keras <code>MultiHeadAttention</code> layer actually already contain an <code>Embeeding</code> layer that takes care of the positional encoding? Or not?</li>
<li>What about the normalization of data? Are only the actual element values normalized and then the positional encoding is added to that normalized value? Or is the positional encoding value added to the raw value of the element and the resulting values are normalized?</li>
</ul>
<p>I am interested in actual implementation details not the conceptual part of positional encoding as I read most of the academic papers on positional encoding already. Unfortunately, most academic papers fall short of describing in detail at what stage and how precisely the positional encoding is applied to the data structure.</p>
<p>Thanks!!!</p>
<p><a href=""https://i.sstatic.net/PxeeE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PxeeE.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"68442973","Fully delegate BERT models on Mali GPU using","2021-07-19 15:16:41","","0","685","<tensorflow><tensorflow-lite><bert-language-model><transformer-model><mali>","<p>I am trying to deploy BERT and Transformer models on Mali GPU, using TensorflowLite. But the problem is that TensorflowLite does not support some operations in these models, including {CAST, GATHER, MUL, RESHAPE, UNPACK}. Does anyone have any idea how I can delegate those operations on GPU? Are there any other TensorflowLite libraries that could support embedded GPU and specifically Mali GPU?
I just want to measure their latency on GPU.</p>
<pre><code>STARTING!
Log parameter values verbosely: [0]
Min num runs: [1]
**Graph**: [mobilebert_float_384_gpu.tflite]
**Use gpu**: [1]
Loaded model mobilebert_float_384_gpu.tflite
**INFO**: Created TensorFlow Lite delegate for GPU.
**ERROR**: Following operations are not supported by GPU delegate:
CAST: Not supported cast case
GATHER: Operation is not supported.
MUL: MUL requires one tensor that not less than second in all dimensions.
RESHAPE: OP is supported, but tensor type isn't matched!
UNPACK: Operation is not supported.
**2661 operations will run on the GPU, and the remaining 81 operations will run on the CPU.**
INFO: Initialized OpenCL-based API.
INFO: Created 1 GPU delegate kernels.
Explicitly applied GPU delegate, and the model graph will be partially executed by the delegate w/ 1 delegate kernels.
The input model file size (MB): 100.239
Initialized session in 2491.9ms.
Running benchmark for at least 1 iterations and at least 0.5 seconds but terminate if exceeding 150 seconds.
count=2 first=434022 curr=247839 min=247839 max=434022 avg=340930 std=93091
</code></pre>
","transformer-model"
"68430210","How to limit the size of the features vector in Wav2Vec?","2021-07-18 14:48:29","","1","989","<python><numpy><huggingface-transformers><transformer-model>","<p>I'm attempting to receive a features vector of short wav (audio) files using wav2vec by using <a href=""https://huggingface.co/transformers/model_doc/wav2vec2.html"" rel=""nofollow noreferrer"">Hugging Face Transformers</a>.</p>
<p>However, for unknown reasons, no matter which approach I use to control the output size, the results do not meet my requirements.</p>
<p>Ideally, I'd like to get all of the vectors to be the same length (e.g. 60K).
I try to get it with the following command:</p>
<pre class=""lang-py prettyprint-override""><code>feature_extractor(input_audio, sampling_rate=16000, return_tensors=&quot;np&quot;, padding=&quot;max_length&quot;,
                                    max_length=60000).input_values
</code></pre>
<p>That command helped me create a minimal boundary of the data size by padding all the vectors into a minimum of 60K length, but I was surprised to see vectors with 120K values created as well.</p>
<p>Then I remove the padding parameter in the hope of obtaining vectors with no padding but an upper boundary of 60K.
Based on the <code>max_length</code> documentation:</p>
<blockquote>
<p>Maximum length of the returned list and optionally padding length</p>
</blockquote>
<p>So I executed this line:</p>
<pre class=""lang-py prettyprint-override""><code>feature_extractor(input_audio, sampling_rate=16000, return_tensors=&quot;np&quot;,
                                    max_length=60000).input_values
</code></pre>
<p>Unexpectedly, I receive vectors ranging in length from 20K to 120K. Not limited at all.</p>
<hr />
<p>To reproduce my bug and results, I've included a snippet of code and a link to relevant audio data.</p>
<pre class=""lang-py prettyprint-override""><code>import librosa
import numpy as np
from transformers import Wav2Vec2FeatureExtractor
from pathlib import Path

    p = Path(dataset_path)
    audio_files = [i.parents[0] / i.name for i in p.glob('**/*.wav')]
    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-base-960h')
    for file in (audio_files):
        input_audio, _ = librosa.load(file,
                                      sr=16000)
        features_with_padding = feature_extractor(input_audio, sampling_rate=16000,
                                return_tensors=&quot;np&quot;, padding=&quot;max_length&quot;, max_length=60000).input_values                                
        features_without_padding = feature_extractor(input_audio, sampling_rate=16000,
                                  return_tensors=&quot;np&quot;, max_length=60000).input_values
        print(features_with_padding.shape, features_without_padding.shape)
</code></pre>
<p>In <a href=""https://drive.google.com/drive/folders/1j-BNp8D8yN16exgoacgDtDrnMF-jQP91?usp=sharing"" rel=""nofollow noreferrer"">this</a> drive folder, I attached 2 wav files that create about 80K length vector.</p>
<p>How could I create a one-size feature vector with a wav2vec transformer?</p>
","transformer-model"
"68417102","Custom Transformer to Filter out Outliers","2021-07-17 03:12:04","","1","423","<python><scikit-learn><transformer-model>","<p>I'm trying to build a transformer that will allow me to specify a feature and then filter out any outliers along this feature. An outlier is an observation that has a value for that feature which deviates from the median by more than 2 times the width of the distribution.</p>
<p>Below is the code I currently have. There are 3 lines of code I'm not sure if they are correct. Please let me know if I did it wrong and how to correct them. Thanks!</p>
<pre><code>import numpy as np

class FilterOutliersTransformer(base.BaseEstimator, base.TransformerMixin):
    
    def __init__(self, feature):
        
        self.feature = feature
        
    def fit(self, X, y=None):
        
        Q1 = np.percentile(X.loc[:, self.feature], 25)
        Q3 = np.percentile(X.loc[:, self.feature], 75)
        
        deviation_allowed = 1.5*(Q3 - Q1)
        lower_bound = Q1 - deviation_allowed
        upper_bound = Q3 + deviation_allowed
        
        # not sure here 1
        self.params_ = [lower_bound, upper_bound]

        # not sure here 2   
        return self    
    
    def transform(self, X, y=None):
        
        X_transformed = X[(X[self.feature] &gt; self.params_[0]) &amp; (X[self.feature] &lt; self.params_[1])]
        
        # not sure here 3 
        return X_transformed
</code></pre>
","transformer-model"
"68388413","How to generate sentence embedding using long-former model","2021-07-15 05:30:20","","2","2109","<python-3.x><deep-learning><embedding><huggingface-transformers><transformer-model>","<p>I am using Hugging Face <code>mrm8488/longformer-base-4096-finetuned-squadv2</code> pre-trained model
<a href=""https://huggingface.co/mrm8488/longformer-base-4096-finetuned-squadv2"" rel=""nofollow noreferrer"">https://huggingface.co/mrm8488/longformer-base-4096-finetuned-squadv2</a>.</p>
<p>I want to generate sentence level embedding. I have a data-frame which has a text column.</p>
<p>I am using this code:</p>
<pre><code>import torch
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
ckpt = &quot;mrm8488/longformer-base-4096-finetuned-squadv2&quot;
tokenizer = AutoTokenizer.from_pretrained(ckpt)
model = AutoModelForQuestionAnswering.from_pretrained(ckpt)

text = &quot;Huggingface has democratized NLP. Huge thanks to Huggingface for this.&quot; # I will pas text-column here from my data-frame
#question = &quot;What has Huggingface done ?&quot;
encoding = tokenizer(question, text, return_tensors=&quot;pt&quot;)
# I don't want to use it for Question-Answer use-case. I just need the sentence embeddings
input_ids = encoding[&quot;input_ids&quot;]

# default is local attention everywhere
# the forward method will automatically set global attention on question tokens
attention_mask = encoding[&quot;attention_mask&quot;] 
</code></pre>
<p>How can I do modification  in the above code to generate embedding for sentences. ?</p>
<p>I have the following examples:</p>
<pre><code>                           Text
i've added notes to the claim and it's been escalated for final review
after submitting the request you'll receive an email confirming the open request.
hello my name is person and i'll be assisting you
this is sam and i'll be assisting you for date.
I'll return the amount as asap.
ill return it to you.
</code></pre>
","transformer-model"
"68334657","This is a transformer code using pytorch, but an error occurs when using gpu","2021-07-11 08:51:51","","0","144","<python><deep-learning><pytorch><gpu><transformer-model>","<pre><code>File &quot;C:\Users\Administrator\anaconda3\envs\wonyong_tf1_2\lib\site- 
packages\torch\nn\functional.py&quot;, line 1371, in linear
output = input.matmul(weight.t())
RuntimeError: Expected object of backend CUDA but got backend CPU for argument #2 'mat2'
</code></pre>
<p><strong>The above error message pops up.</strong></p>
<p><strong>Here's the actual code.</strong></p>
<p>########################
'''</p>
<pre><code>USE_CUDA = torch.cuda.is_available()
device = torch.device('cuda:0' if USE_CUDA else 'cpu')


t = network.Transformer(dim_val, dim_attn, 51, dec_seq_len, output_sequence_length, 
n_decoder_layers, n_encoder_layers,
                        n_heads)
t = t.to(device)

optimizer = torch.optim.Adam(t.parameters(), lr=lr)
for e in range(epochs):

    for inputs in Data.get_batches(Data.train, batch_size,  True):
        X, Y = inputs[0], inputs[1]
        X = X.to(device)
        Y = Y.to(device)
        # print(X.shape, Y.shape)
        optimizer.zero_grad()
        t = t.to(device)
        net_out = t(X)
        net_out = net_out.to(device)
        # print(net_out.shape,Y.shape)
        loss = torch.mean((net_out - Y) ** 2)
        # loss(net_out, Y)

        loss.backward()
        optimizer.step()
</code></pre>
<p>'''</p>
<p><strong>This is the code for learning using the transformer module.</strong></p>
<p>What's the problem with this code?
I want to use GPU :(</p>
","transformer-model"
"68327701","How to resolve error with Styleformer package in python","2021-07-10 12:26:06","","1","233","<python-3.x><transformer-model>","<p>I am trying to install <code>styleformer</code> module in <code>Jupyter Notebook Server</code> having <code>Tensorflow 2.3</code> as kernel with <code>Python 3.6</code></p>
<p>Module: <a href=""https://github.com/PrithivirajDamodaran/Styleformer"" rel=""nofollow noreferrer"">https://github.com/PrithivirajDamodaran/Styleformer</a></p>
<p>Since there is no <code>internet</code> in server, I upload the <code>github file</code> in notebook from local and install this using <code>setup.py</code> file.</p>
<p>One problem is that I have to be always inside the <code>Styleformer</code> ( Note: <code>S</code> capital) folder to run this command</p>
<p><code>from styleformer import Styleformer</code></p>
<p>But as soon as I run this command:</p>
<p><code>sf = Styleformer(style = 0)</code></p>
<p>It gives the below error:</p>
<pre><code>---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
/opt/conda/lib/python3.6/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    241             if resolved_config_file is None:
--&gt; 242                 raise EnvironmentError
    243             config_dict = cls._dict_from_json_file(resolved_config_file)

OSError: 

During handling of the above exception, another exception occurred:

OSError                                   Traceback (most recent call last)
/usr/local/bin/kernel-launchers/python/scripts/launch_ipykernel.py in &lt;module&gt;
      1 from styleformer import Styleformer
----&gt; 2 sf = Styleformer(style = 0)

~/Styleformer/styleformer/styleformer.py in __init__(self, style)
      7 
      8     self.style = style
----&gt; 9     self.adequacy = Adequacy()
     10     self.model_loaded = False
     11 

~/Styleformer/styleformer/adequacy.py in __init__(self, model_tag)
      3   def __init__(self, model_tag='prithivida/parrot_adequacy_on_BART'):
      4     from transformers import AutoModelForSequenceClassification, AutoTokenizer
----&gt; 5     self.nli_model = AutoModelForSequenceClassification.from_pretrained(model_tag)
      6     self.tokenizer = AutoTokenizer.from_pretrained(model_tag)
      7 

/opt/conda/lib/python3.6/site-packages/transformers/modeling_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
   1357         config = kwargs.pop(&quot;config&quot;, None)
   1358         if not isinstance(config, PretrainedConfig):
-&gt; 1359             config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)
   1360 
   1361         for config_class, model_class in MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING.items():

/opt/conda/lib/python3.6/site-packages/transformers/configuration_auto.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)
    201 
    202         &quot;&quot;&quot;
--&gt; 203         config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
    204 
    205         if &quot;model_type&quot; in config_dict:

/opt/conda/lib/python3.6/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    249                 f&quot;- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\n\n&quot;
    250             )
--&gt; 251             raise EnvironmentError(msg)
    252 
    253         except json.JSONDecodeError:

OSError: Can't load config for 'prithivida/parrot_adequacy_on_BART'. Make sure that:

- 'prithivida/parrot_adequacy_on_BART' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'prithivida/parrot_adequacy_on_BART' is the correct path to a directory containing a config.json file
</code></pre>
<p><code>Is there a way to fix this error. May be by uploading the file manually? As it calls the below lines for style=0:</code></p>
<pre><code>class Styleformer():

  def __init__(self,  style=0):
    from transformers import AutoTokenizer
    from transformers import AutoModelForSeq2SeqLM
    from styleformer import Adequacy

    self.style = style
    self.adequacy = Adequacy()
    self.model_loaded = False

    if self.style == 0:
      self.ctf_tokenizer = AutoTokenizer.from_pretrained(&quot;prithivida/informal_to_formal_styletransfer&quot;)
      self.ctf_model = AutoModelForSeq2SeqLM.from_pretrained(&quot;prithivida/informal_to_formal_styletransfer&quot;)
      print(&quot;Casual to Formal model loaded...&quot;)
      self.model_loaded = True
</code></pre>
<p><code>Note: When I run same thing on Google Colab, It automatically downloads few files and works fine. I do not know which file or models it downloads. It's a bit confusing. Please help.</code></p>
","transformer-model"
"68322542","Problem connecting transformer output to CNN input in Keras","2021-07-09 20:46:22","68451347","1","1014","<tensorflow><keras><conv-neural-network><huggingface-transformers><transformer-model>","<p>I need to build a transformer-based architecture in Tensorflow following the encoder-decoder approach where the encoder is a preexisting Huggingface Distilbert model and the decoder is a CNN.</p>
<p>Inputs: a text containing texts with several phrases in a row. Outputs: codes according to taxonomic criteria. My data file has 7387 pairs text-label in TSV format:</p>
<pre><code>text \t code
This is example text number one. It might contain some other phrases. \t C21
This is example text number two. It might contain some other phrases. \t J45.1
This is example text number three. It might contain some other phrases. \t A27
</code></pre>
<p>The remainder of the code is this:</p>
<pre><code>        text_file = &quot;data/datafile.tsv&quot;
        with open(text_file) as f:
                lines = f.read().split(&quot;\n&quot;)[:-1]
                text_and_code_pairs = []
                for line in lines:
                        text, code = line.split(&quot;\t&quot;)
                        text_and_code_pairs.append((text, code))


        random.shuffle(text_and_code_pairs)
        num_val_samples = int(0.10 * len(text_and_code_pairs))
        num_train_samples = len(text_and_code_pairs) - 3 * num_val_samples
        train_pairs = text_and_code_pairs[:num_train_samples]
        val_pairs = text_and_code_pairs[num_train_samples : num_train_samples + num_val_samples]
        test_pairs = text_and_code_pairs[num_train_samples + num_val_samples :]

        train_texts = [fst for (fst,snd) in train_pairs]
        train_labels = [snd for (fst,snd) in train_pairs]
        val_texts = [fst for (fst,snd) in val_pairs]
        val_labels = [snd for (fst,snd) in val_pairs]
        test_texts = [fst for (fst,snd) in test_pairs]
        test_labels = [snd for (fst,snd) in test_pairs]

        distilbert_encoder = TFDistilBertModel.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;)
        tokenizer = DistilBertTokenizerFast.from_pretrained(&quot;distilbert-base-multilingual-cased&quot;)

        train_encodings = tokenizer(train_texts, truncation=True, padding=True)
        val_encodings = tokenizer(val_texts, truncation=True, padding=True)
        test_encodings = tokenizer(test_texts, truncation=True, padding=True)

        train_dataset = tf.data.Dataset.from_tensor_slices((
                dict(train_encodings),
                train_labels
        ))
        val_dataset = tf.data.Dataset.from_tensor_slices((
                dict(val_encodings),
                val_labels
        ))
        test_dataset = tf.data.Dataset.from_tensor_slices((
                dict(test_encodings),
                test_labels
        ))

        model = build_model(distilbert_encoder)
        model.fit(train_dataset.batch(64), validation_data=val_dataset, epochs=3, batch_size=64)
        model.predict(test_dataset, verbose=1)
</code></pre>
<p>Lastly, the <code>build_model</code> function:</p>
<pre><code>def build_model(transformer, max_len=512):
        model = tf.keras.models.Sequential()
        # Encoder
        inputs = layers.Input(shape=(max_len,), dtype=tf.int32)
        distilbert = transformer(inputs)
        # LAYER - something missing here?
        # Decoder
        conv1D = tf.keras.layers.Conv1D(filters=5, kernel_size=10)(distilbert)
        pooling = tf.keras.layers.MaxPooling1D(pool_size=2)(conv1D)
        flat = tf.keras.layers.Flatten()(pooling)
        fc = tf.keras.layers.Dense(1255, activation='relu')(flat)
        softmax = tf.keras.layers.Dense(1255, activation='softmax')(fc)
        model = tf.keras.models.Model(inputs = inputs, outputs = softmax)
        model.compile(tf.keras.optimizers.Adam(learning_rate=5e-5), loss=&quot;categorical_crossentropy&quot;, metrics=['accuracy'])
        print(model.summary())
        return model
</code></pre>
<p>I managed to narrow down the possible locations of my problem. After changing from sequential to functional Keras API, I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;keras_transformer.py&quot;, line 99, in &lt;module&gt;
    main()
  File &quot;keras_transformer.py&quot;, line 94, in main
    model = build_model(distilbert_encoder)
  File &quot;keras_transformer.py&quot;, line 23, in build_model
    conv1D = tf.keras.layers.Conv1D(filters=5, kernel_size=10)(distilbert)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 897, in __call__
    self._maybe_build(inputs)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py&quot;, line 2416, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py&quot;, line 152, in build
    input_shape = tensor_shape.TensorShape(input_shape)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 771, in __init__
    self._dims = [as_dimension(d) for d in dims_iter]
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 771, in &lt;listcomp&gt;
    self._dims = [as_dimension(d) for d in dims_iter]
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 716, in as_dimension
    return Dimension(value)
  File &quot;/home/users/user/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py&quot;, line 200, in __init__
    None)
  File &quot;&lt;string&gt;&quot;, line 3, in raise_from
TypeError: Dimension value must be integer or None or have an __index__ method, got 'last_hidden_state'
</code></pre>
<p>It seems that the error lies in the connection between the output of the transformer and the input of the convolutional layer. Am I supposed to include another layer between them so as to adapt the output of the transformer? If so, what would be the best option?I'm using tensorflow==2.2.0, transformers==4.5.1 and Python 3.6.9</p>
","transformer-model"
"68306221","Tensorflow : Transformer Model Decoder Target Input","2021-07-08 17:27:20","","1","353","<tensorflow><keras><nlp><transformer-model>","<p>I'm quite new to Tensorflow and machine learning. Sorry if I haven't asked the question accurately or not making sense somewhere. I have recently got to read about and try to understand the transformer model, after its reputation in NLP and thankfully TensorFlow website has in details code and explanation.</p>
<p><a href=""https://www.tensorflow.org/text/tutorials/transformer#training_and_checkpointing"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer#training_and_checkpointing</a></p>
<p>I have no problem understanding the code: the attention layer, positional encoding, encoder, decoder, masking etc.<br />
When training the model, the input is the <code>sentence to be translated</code> and the one <code>in the target language</code>. where the target language is shifted and masked.</p>
<p>My problem is when the trainned model is used for evaluation, the mission is to translate <code>an unseen sentence</code> to the target language, and so the <code>input for target</code> would be an empty token, how would this empty tensor react with the trained model within the attention layer. Its empty? and in the first place what would be the effect of neglecting it.</p>
<p>To be more precise, please look at the screenshot below:</p>
<p><code>tar_inp</code> in inputted in transformer, and loss is computed between <code>prediction</code> and <code>tar_real</code> but when evaluating the model, what is the function of an empty <code>tar_inp</code> do in the layer. Thank you very much sorry if it's a dumb question and may you please provide some intuition for understanding.</p>
<p><a href=""https://i.sstatic.net/mqxuS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mqxuS.png"" alt=""enter image description here"" /></a></p>
","transformer-model"
"68266490","Dimension of Query and Key Tensor in MultiHeadAttention","2021-07-06 07:43:52","68282055","2","1499","<keras><deep-learning><nlp><transformer-model><attention-model>","<p>I am confused about the dimensions that are mentioned for query and key tensors in the documentation of <code>MultiHeadAttention</code> Layer in Keras documentation <a href=""https://keras.io/api/layers/attention_layers/multi_head_attention/"" rel=""nofollow noreferrer"">https://keras.io/api/layers/attention_layers/multi_head_attention/</a></p>
<blockquote>
<p><strong>query</strong>: Query Tensor of shape (B, T, dim)</p>
</blockquote>
<blockquote>
<p><strong>value</strong>: Value Tensor of shape (B, S, dim).</p>
</blockquote>
<p>Here I am presuming that <code>T</code> and <code>S</code> corresponds to Sequence of words fed in the model which should be same then why they are unequal?</p>
","transformer-model"
"68259566","Transformer: Why doesn't the softmax in the classification layer (i.e. FFNN with a softmax) need scaling input which same operation as self-attention?","2021-07-05 16:47:16","","1","179","<deep-learning><nlp><transformer-model><softmax><attention-model>","<p>The reason why the scaled self-attention need to be scaled, said in the original paper, is that &quot;dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients&quot;.</p>
<p>So if the problem is a large value or some large value in the input will make the gradient of softmax sparse, why doesn’t the last layer of Transformer (classification layer, i.e. feed-forward network with a softmax) need scaling input?</p>
<p>In other words, we know that softmax is a very common function in machine learning, why is it only accept scaled input in self-attention scenarios?</p>
","transformer-model"
"68230149","Is the score from TransformedTargetRegressor of scikit-learn correct?","2021-07-02 18:58:00","68234596","1","249","<python><machine-learning><scikit-learn><transformer-model>","<p>I made a short <a href=""https://github.com/mGolos/test/blob/master/sklearn_TransformedTargetRegressor.ipynb"" rel=""nofollow noreferrer"">Jupyter notebook</a> to go with my question regarding the TransformedTargetRegressor.<br />
I wanted to put a transformer inside a pipeline to play with a parameter grid but the scores didn't match.</p>
<pre class=""lang-py prettyprint-override""><code>...
model = linear_model.LinearRegression()
lg_tr = preprocessing.FunctionTransformer(func=np.log, inverse_func=np.exp, check_inverse=True)
y_log = lg_tr.transform(y)
score_0 = model.fit(X, y_log).score(X, y_log)
</code></pre>
<pre class=""lang-py prettyprint-override""><code>...
model = compose.TransformedTargetRegressor(func=np.log, inverse_func=np.exp, check_inverse=True,
    regressor=linear_model.LinearRegression())
score_1 = model.fit(X, y).score(X, y)
</code></pre>
<p>The <code>score_0</code> value is correct. Why is the one from <code>score_1</code> not?<br />
I don't have problem with the prediction that works fine, only the score.<br />
Did I miss something?<br />
Thank you =)</p>
","transformer-model"
"68205894","How to prepare data for TpyTorch's 3d attn_mask argument in MultiHeadAttention","2021-07-01 07:19:57","","4","1794","<python><pytorch><transformer-model>","<p>I'm currently trying to implement an Encoder-Decoder architecture for text summarization based on Transformers. Thus I need ti apply MultiHeadAttention on the Decoder site of the model. Since I want to ensure that the model doesn't attend to unseen tokens of the target sequence, I need to use the 3D attention mask (<code>attn_mask</code>) argument.</p>
<p>According to the documentation (<a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a>), the shape of the mask must be <code>BATCH_SIZE * NUMBER_HEADS, SEQUENCE_LENGTH, SEQUENCE_LENGTH</code>. Which is fine, as it provides the possibility to use different attentions between the Heads, which I don't need in my case ...</p>
<p>But the documentation doesn't state how the tensor needs to be filled regarding it's first dimension and I can't see/ find it in the implementation how it is actually used...</p>
<p>Is it:</p>
<pre><code>[
  [2D Attention for Batch 1 for Head 1]
  [2D Attention for Batch 2 for Head 1]
  ...
  [2D Attention for Batch 1 for Head 2]
  [2D Attention for Batch 2 for Head 2]
  ...
  [2D Attention for Batch n for Head n]
]
</code></pre>
<p>or</p>
<pre><code>[
  [2D Attention for Batch 1 for Head 1]
  [2D Attention for Batch 1 for Head 2]
  ...
  [2D Attention for Batch 2 for Head 1]
  [2D Attention for Batch 2 for Head 2]
  ...
  [2D Attention for Batch n for Head n]
]
</code></pre>
<p>Would be great if someone knows :)</p>
","transformer-model"
"68193323","When I use transformer model to train a translator, why it cannot convert y to a tensor","2021-06-30 10:46:32","","0","29","<tensorflow><machine-learning><nlp><transformer-model><attention-model>","<pre><code>def encoder():
    input_layer = Input(batch_shape=(None, 13, 128))
    h= layer(input_layer)
    h= Masking(mask_value=0.0)(h)
    h, hidden_layer, cell_layer = LSTM(512, return_state=True)(h)
    model = Model(inputs = input_layer, outputs = [hidden_layer, cell_layer])
    return model
model=encoder()
model.summary()

class Decoder(Model):
    def __init__(self):
        super(Decoder, self).__init__()
        self.embedding_layer = Embedding(input_dim=max_tokens+1, output_dim=128, mask_zero=True)
        self.lstm_layer = LSTM(512,
        return_state=True, return_sequences=True)
        self.dense_layer = Dense(units=max_tokens+1)
    def call(self,inputer,hidden_layer=None,cell_layer=None):
        x=self.embedding_layer(inputer)
        if hidden_layer!=None and cell_layer!=None:
            x, h, c = self.lstm_layer(x, initial_state=[hidden_layer, cell_layer])
        else:
             x, h, c = self.lstm_layer(x)
        x=self.dense_layer(x)
        return x,h,c
        
decoder=Decoder()
for eng,germ in train.take(1):
    y,hidden,cell = decoder(germ)
@tf.function
def loss_fn(en_input, germ_input, germ_output, loss):
    with tf.GradientTape() as tape:
        enc_hidden_s, enc_cell_s = model(en_input)
        dec_output, dec_hidden_s, dec_cell_s = decoder(germ_input, enc_hidden_s,enc_cell_s)
        loss_value = loss(germ_output, dec_output)
        return loss_value, tape.gradient(loss_value, variables)
 
def fit_german_shape(german):
    input_data = german[:,:-1]
    output_data = german[:,1:]
    return input_data,output_data
def training(train_data, test_data,optimizer, loss,epochs=5):
    batch_num=0
    batch_num2=0
    epoch_loss=0
    epoch_loss2=0
    for english,germany in train:
        germany_in,germany_out=fit_german_shape(germany)
        loss2, grad= loss_fn(english, germany_in, germany_out, loss)
        optimizer.apply_gradients(zip(grad,model.trainable_variables + decoder.trainable_variables))
        epoch_loss=epoch_loss+loss2
        batch_num=batch_num+1
        avg_loss=epoch_loss/batch_num
        avg_loss3=String(avg_loss1)
        print(&quot;In this train epoch, the loss is&quot;+ave_loss3)
    for english2,germany2 in test:
        germany_in2,germany_out2=fit_german_shape(germany2)
        hidden_state,cell_state=model(en)
        pred,temp1,temp2=decoder(germany_in2,hidden_state,cell_state)
        loss, temp3 = loss_fn(english2, germany_in2, germany_out2)
        epoch_loss2=loss+epoch_loss2
        batch_num=batch_num+1
        avg_loss2=epoch_loss2/batch_num2
        avg_loss4=String(avg_loss2)
        print(&quot;In this test epoch, the loss is&quot;+ave_loss4)
    return avg_loss,avg_loss2
</code></pre>
<p>When I use this model to translate German to English, it report the error that &quot;Tried to convert 'y' to a tensor and failed. Error: None values not supported.&quot; Error may occur in the decoder to assign value to x,h,c, but I dont know why cannot convert y to a tensor.</p>
","transformer-model"
"68176844","Is tensorflow multi-head attention layer autoregressive? e.g. ""tfa.layers.MultiHeadAttention""","2021-06-29 10:20:59","68755710","1","524","<tensorflow><transformer-model><attention-model><autoregressive-models>","<p>I looked at the difference between an autoregressive vs non-autoregressive in transformer architecture. but I am wondering whether the attention layer in TensorFlow is actually autoregressive? or do I need to implement the autoregressive mechanism?</p>
<p>I don't see any option for causal (e.g. causal=true/false)</p>
<p>I do not see documentation that states if &quot;tfa.layers.MultiHeadAttention&quot; is autoregressive or not</p>
<p>Any thoughts on that would be appreciated.</p>
","transformer-model"
"68149998","fast filtering of sentences in spacy","2021-06-27 10:04:05","68153440","1","1625","<python><nlp><spacy><transformer-model><sentence>","<p>I'm using SpaCy to divide a text into sentences, match a regex pattern on each sentence, and use some logic based on the results of the match. I started with a naive approach such as:</p>
<pre><code>nlp = spacy.load(&quot;en_core_web_trf&quot;)
regex = re.compile(r'\b(foo|bar)\b')

for text in texts_list:
  doc = nlp(text)
  for sent in doc.sents:
    if re.search(regex, str(s)):
    [...]
    else:
    [...]
</code></pre>
<p>and it was very slow. Then I used a pipe:</p>
<pre><code>for doc in nlp.pipe(texts_list, disable=['tagger', 'ner', 'attribute_ruler', 'lemmatizer'], n_process=4):
  for sent in doc.sents:
    if re.search(regex, str(s)):
    [...]
    else:
    [...]
</code></pre>
<p>but it's still slow. Am I missing something?</p>
","transformer-model"
"68120730","HuggingFace Tranfsormers BERTForSequenceClassification with Trainer: How to do multi-output regression?","2021-06-24 18:04:39","","2","995","<python><pytorch><bert-language-model><huggingface-transformers><transformer-model>","<p>I am trying to fine-tune a BERT model on a dataset of sentences that has two different real-valued attributes for each sentence. For each one, there is a Valence score and an Arousal score, with real values between 0 and 1. I need to add a classification layer on top of BERTbase, and that layer must have two outputs, and take them both into account for calculating the loss and performing the backpropagation.</p>
<p>Since I am inexperienced, it was suggested to be that I use the Trainer classes / API for this task. I am using BertForSequenceClassification, and I was told that by setting the num_labels parameter to 2, giving the model an input with two feature columns, and overriding the compute_loss method of the Trainer class to force the loss to be computed with both the outputs.</p>
<p>The thing is, when I read the BERTForSequenceClassification documentation, it says that &quot;If config.num_labels == 1 a regression loss is computed (Mean-Square loss), If config.num_labels &gt; 1 a classification loss is computed (Cross-Entropy).&quot;. On the other hand, the Trainer documentation has the compute_loss function, which I override. From inspecting the code, I think that by overriding compute_loss and defining it as desired, the &quot;num_labels&quot; parameter on BertForSequenceClassification actually becomes irrelevant and the loss is entirely computed on the redefined method.</p>
<p>I am still afraid that something might be wrong in this adaptation for a 2-output regression model, either with the loss, with the gradient backpropagation or somewhere else. I will leave my code below, and I am incredibly grateful for any help or feedback in using HuggingFace's Trainer for a 2-output regression BERT model.</p>
<pre><code>model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', num_labels=2)
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case = True)

class MyDataset(Dataset):
    def __init__(self, filename,maxlen):
        nas = ['-1.#IND', '1.#QNAN', '1.#IND', '-1.#QNAN', '#N/A N/A', '#N/A', 'N/A', 'n/a', 'NA', '', '#NA', 'NaN', '-NaN', '']
        data = pd.read_csv(filename,na_values=nas,keep_default_na=False)
        self.maxlen = maxlen
        self.texts = data[&quot;Word/Sentence&quot;].tolist()
        self.labels1 = data['Valence'].tolist()
        self.labels2 = data['Arousal'].tolist()
    def __getitem__(self, idx):
        item = { }
        aux = tokenizer(self.texts[idx], max_length=self.maxlen, truncation=True, padding=False)
        item['input_ids'] = torch.tensor(aux['input_ids'])
        item['attention_mask'] = torch.tensor(aux['attention_mask'])
        item['labels'] = torch.tensor( [ self.labels1[idx], self.labels2[idx] ] )
        return item
    def __len__(self):
        return len(self.texts)

class MyTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.pop(&quot;labels&quot;)
        outputs = model(**inputs)
        predictions = outputs[0]
        predictions = torch.sigmoid(predictions)
        loss = torch.nn.MSELoss()
        loss = loss(predictions.view(-1), labels.view(-1))
        return (loss, outputs) if return_outputs else loss

train_dataset = MyDataset(&quot;fold1.csv&quot;,250)
val_dataset = MyDataset(&quot;fold2.csv&quot;,250)
data_collator = DataCollatorWithPadding(tokenizer, padding = &quot;longest&quot;, max_length = 250)

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=4,              # total number of training epochs
    per_device_train_batch_size=16,   # batch size per device during training
    per_device_eval_batch_size=16,    # batch size for evaluation
    warmup_steps=50,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
    save_steps=150,
    group_by_length = True,
    evaluation_strategy = &quot;epoch&quot;
)

trainer = MyTrainer(
    model=model,                         # the instantiated :hugging_face: Transformers model to be traine
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,            # evaluation dataset
    data_collator=data_collator,           
)
</code></pre>
","transformer-model"
"68118815","How to run Transformer and BERT models on Mali-GPU using Tensorflow Lite?","2021-06-24 15:40:07","","1","440","<tensorflow><tensorflow-lite><bert-language-model><transformer-model>","<p>I am trying to run Transformer and BERT models on Mali-GPU using Tensorflow Lite, but as long as I know, tflite only supports some operations on GPU, not the deep learning models themself.
Do you have any ideas and tips on how I can run these Transformer and BERT models on Mali-GPU? Can I convert Tensoflow GPU model to tflite GPU model? Or is there any other library that supports transformers on embedded GPU?</p>
","transformer-model"
"68105165","Input Embeddings to BERT","2021-06-23 18:13:04","","1","560","<word2vec><bert-language-model><huggingface-transformers><transformer-model>","<p>According to theory, BERT takes word embeddings and position embeddings as input. My goal is to feed the BERT model with word embeddings from a different model like word2vec or Glove.
Is there a way to feed static word embedding to BERT to get contextualized Word embedding from BERT finally?
Please let me know about any relevant links</p>
","transformer-model"
"68058905","How to get the output of the last but one layer of the Vision transformer using the hugging face implementation?","2021-06-20 18:27:50","","0","661","<huggingface-transformers><transformer-model>","<p>I am trying to use the huggingface implementation of the vision transformer to get the feature vector of the last but one dense layer</p>
","transformer-model"
"68022966","Saving Bert Fine tuned models","2021-06-17 16:10:58","","0","356","<python><bert-language-model><transformer-model>","<p>After tuning my BERT model I want to save it to use it later (once the task is learned). I use these 2 lines to do it.</p>
<pre><code>model_to_save.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
</code></pre>
<p>The problem is that it creates a +400Mb file that I cant upload to github because it is too big.</p>
<p>Is there a way to save the model creating a smaller file and loading it back later?</p>
<p>Regards</p>
","transformer-model"
"68007097","Getting a Mixed Precison Cuda Error while running a cell trying to fine tuning a Wav2Wav medical vocabulary module","2021-06-16 16:57:17","","2","3801","<python-3.x><speech-recognition><huggingface-transformers><transformer-model>","<p>Following is my Cell Code:</p>
<pre><code>from transformers import TrainingArguments

training_args = TrainingArguments(
  output_dir=&quot;wav2vec2-medical&quot;,
  group_by_length=True,
  per_device_train_batch_size=32,
  evaluation_strategy=&quot;steps&quot;,
  num_train_epochs=30,
  fp16=True,
  save_steps=500,
  eval_steps=500,
  logging_steps=500,
  learning_rate=1e-4,
  weight_decay=0.005,
  warmup_steps=1000,
  save_total_limit=2,
)
</code></pre>
<p>And here is the error that I am getting. I am not sure about what to make out of it.</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-26-f9014a6221db&gt; in &lt;module&gt;
      1 from transformers import TrainingArguments
      2 
----&gt; 3 training_args = TrainingArguments(
      4   # output_dir=&quot;/content/gdrive/MyDrive/wav2vec2-base-timit-demo&quot;,
      5   output_dir=&quot;./wav2vec2-medical&quot;,

~/Library/Python/3.8/lib/python/site-packages/transformers/training_args.py in __init__(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, logging_dir, logging_strategy, logging_first_step, logging_steps, save_strategy, save_steps, save_total_limit, no_cuda, seed, fp16, fp16_opt_level, fp16_backend, fp16_full_eval, local_rank, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, deepspeed, label_smoothing_factor, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, mp_parameters)

~/Library/Python/3.8/lib/python/site-packages/transformers/training_args.py in __post_init__(self)
    609 
    610         if is_torch_available() and self.device.type != &quot;cuda&quot; and (self.fp16 or self.fp16_full_eval):
--&gt; 611             raise ValueError(
    612                 &quot;Mixed precision training with AMP or APEX (`--fp16`) and FP16 evaluation can only be used on CUDA devices.&quot;
    613             )

ValueError: Mixed precision training with AMP or APEX (`--fp16`) and FP16 evaluation can only be used on CUDA devices.
</code></pre>
<p>I tried to run it on Jupyter notebook on local device and also on Google Colab but still I got the same error</p>
","transformer-model"
"67991110","Passage limit for Reading comprehension by using Transformer QA pretrained model in allennlp","2021-06-15 17:40:33","","0","68","<deep-learning><nlp><transformer-model><nlp-question-answering><allennlp>","<p>What is the max passage limit or hardware limit to use transformer-qa model for reading comprehension in allennlp:</p>
<p><strong>Predictor.from_path('https://storage.googleapis.com/allennlp-public-models/transformer-qa-2020-10-03.tar.gz').predict(passage=passage, question=question)</strong></p>
<p>I'm getting &quot;DefaultCPUAllocator: not enough memory: you tried to allocate 23437770752 bytes. Buy new RAM!&quot; error</p>
","transformer-model"
"67984033","Pytorch Transformer won't train due to tensor sizes","2021-06-15 09:55:27","","0","105","<nlp><pytorch><tensor><transformer-model>","<p>I tried following this tutorial for transformers:
<a href=""https://www.youtube.com/watch?v=U0s0f995w14"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=U0s0f995w14</a></p>
<p>However, when I try to train the code with my own vectors, I get the following error message:</p>
<blockquote>
<p>Traceback (most recent call last):</p>
<p>File &gt;&quot;C:\Users\rreichel\Desktop\Smaragd_local\Programming\Scripts\Transformer_se&gt;lfbuilt.py&quot;, line 279, in 
loss = loss_func(outputs, target)</p>
<p>File &quot;C:\Users\rreichel\Anaconda3\lib\site-&gt;packages\torch\nn\modules\module.py&quot;, line 889, in _call_impl
result = self.forward(*input, **kwargs)</p>
<p>File &quot;C:\Users\rreichel\Anaconda3\lib\site-&gt;packages\torch\nn\modules\loss.py&quot;, line 1047, in forward
return F.cross_entropy(input, target, weight=self.weight,</p>
<p>File &quot;C:\Users\rreichel\Anaconda3\lib\site-&gt;packages\torch\nn\functional.py&quot;, line 2693, in cross_entropy
return nll_loss(log_softmax(input, 1), target, weight, None, &gt;ignore_index, None, reduction)</p>
<p>File &quot;C:\Users\rreichel\Anaconda3\lib\site-&gt;packages\torch\nn\functional.py&quot;, line 2397, in nll_loss
raise ValueError(&quot;Expected target size {}, got {}&quot;.format(out_size, &gt;target.size()))</p>
<p>ValueError: Expected target size (3, 199), got torch.Size([3, 119])
when calculating the loss during training.</p>
</blockquote>
<p>The code:</p>
<pre><code># -*- coding: utf-8 -*-
&quot;&quot;&quot;
Created on Tue Apr  6 08:13:38 2021

@author: rreichel
&quot;&quot;&quot;

import torch
import torch.nn as nn
import pickle
import glob
import os
from SelfbuiltDataset_test import myDataset
import torch.optim as optim


class SelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(SelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert(self.head_dim * heads == embed_size
        ), &quot;Embedding size needs to be divisible by heads&quot;

        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)

    def forward(self, values, keys, query, mask):
        #Get number of training examples
        N = query.shape[0]

        value_len, key_len, query_len = values.shape[1], keys.shape[1], \
        query.shape[1]

        #Split the embedding into self.heads different pieces
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        query = query.reshape(N, query_len, self.heads, self.head_dim)

        #(N, value_len, heads, head_dim)
        values = self.values(values)
        #(N, key_len, heads, head_dim)
        keys = self.keys(keys)
        #(N, query_len, heads, heads_dim)
        queries = self.queries(query)

        energy = torch.einsum(&quot;nqhd, nkhd -&gt; nhqk&quot;, [queries, keys])
        #queries shape: (N, query_len, heads, heads_dim),
        #keys shape: (N, key_len, heads, heads_dim)
        #energy: (N, heads, query_len, key_len)

        #Mask padded indices so their weights become 0
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float(&quot;-1e20&quot;))

        #Normalize energy values
        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)
        #attention shape: (N, heads, query_len, key_len)

        out = torch.einsum(&quot;nhql, nlhd -&gt; nqhd&quot;, [attention, values]).reshape(
            N, query_len, self.heads * self.head_dim)
        #attention shape: (N, heads, query_len, key_len)
        #values shape: (N, value_len, heads, heads_dim)
        #out after matrix multiply: (N, query_len, heads, head_dim), then
        #we reshape and flatten the last two dimensions.

        out = self.fc_out(out)

        return out


class TransformerBlock(nn.Module):
    def __init__(self, embed_size, heads, dropout, forward_expansion):
        super(TransformerBlock, self).__init__()
        self.attention = SelfAttention(embed_size, heads)
        self.norm1 = nn.LayerNorm(embed_size)
        self.norm2 = nn.LayerNorm(embed_size)

        self.feed_forward = nn.Sequential(
            nn.Linear(embed_size, forward_expansion * embed_size),
            nn.ReLU(),
            nn.Linear(forward_expansion * embed_size, embed_size))

        self.dropout = nn.Dropout(dropout)

    def forward(self, value, key, query, mask):
        attention = self.attention(value, key, query, mask)

        # Add skip connection, run through normalization and finally dropout
        x = self.dropout(self.norm1(attention + query))
        forward = self.feed_forward(x)
        out = self.dropout(self.norm2(forward + x))
        return out


class Encoder(nn.Module):
    def __init__(self, src_vocab_size, embed_size, num_layers, heads, device,
        forward_expansion, dropout, max_length):

        super(Encoder, self).__init__()
        self.embed_size = embed_size
        self.device = device
        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)
        self.position_embedding = nn.Embedding(max_length, embed_size)

        self.layers = nn.ModuleList([TransformerBlock(embed_size, heads,
                    dropout=dropout, forward_expansion=forward_expansion)
                    for _ in range(num_layers)])

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        N, seq_length = x.shape
        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)
        out = self.dropout(
            (self.word_embedding(x) +
             self.position_embedding(positions))
        )

        #In the Encoder the query, key, value are all the same, it's in the
        #decoder this will change. This might look a bit odd in this case.
        for layer in self.layers:
            out = layer(out, out, out, mask)

        return out


class DecoderBlock(nn.Module):
    def __init__(self, embed_size, heads, forward_expansion, dropout, device):
        super(DecoderBlock, self).__init__()
        self.norm = nn.LayerNorm(embed_size)
        self.attention = SelfAttention(embed_size, heads=heads)
        self.transformer_block = TransformerBlock(embed_size, heads, dropout,
                                                  forward_expansion)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, value, key, src_mask, trg_mask):
        attention = self.attention(x, x, x, trg_mask)
        query = self.dropout(self.norm(attention + x))
        out = self.transformer_block(value, key, query, src_mask)
        return out


class Decoder(nn.Module):
    def __init__(self, trg_vocab_size, embed_size, num_layers, heads,
        forward_expansion, dropout, device, max_length):
        
        super(Decoder, self).__init__()
        self.device = device
        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)
        self.position_embedding = nn.Embedding(max_length, embed_size)

        self.layers = nn.ModuleList([DecoderBlock(embed_size, heads,
                                                  forward_expansion, dropout,
                                                  device)
                for _ in range(num_layers)])
        
        self.fc_out = nn.Linear(embed_size, trg_vocab_size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_out, src_mask, trg_mask):
        N, seq_length = x.shape
        positions = torch.arange(0, seq_length).expand(N,seq_length).to(self.device)
        x = self.dropout((self.word_embedding(x) +
                          self.position_embedding(positions)))

        for layer in self.layers:
            x = layer(x, enc_out, enc_out, src_mask, trg_mask)

        out = self.fc_out(x)

        return out


class Transformer(nn.Module):
    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx,
                 trg_pad_idx, embed_size=512, num_layers=6,
                 forward_expansion=4, heads=8, dropout=0, device=&quot;cpu&quot;,
                 max_length=100):

        super(Transformer, self).__init__()

        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads,
                               device, forward_expansion, dropout, max_length)

        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads,
                               forward_expansion, dropout, device, max_length)

        self.src_pad_idx = src_pad_idx
        self.trg_pad_idx = trg_pad_idx
        self.device = device

    def make_src_mask(self, src):
        #(N, 1, 1, src_len)
        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)
        return src_mask.to(self.device)

    def make_trg_mask(self, trg):
        N, trg_len = trg.shape
        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(N, 1,
                                                            trg_len, trg_len)

        return trg_mask.to(self.device)

    def forward(self, src, trg):
        src_mask = self.make_src_mask(src)
        trg_mask = self.make_trg_mask(trg)
        enc_src = self.encoder(src, src_mask)
        out = self.decoder(trg, enc_src, src_mask, trg_mask)
        return out

def nextMultiple(n, x):
    n = n + x / 2
    n = n - (n % x)
    return int(n)


if __name__ == &quot;__main__&quot;:
    device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    &quot;&quot;&quot;
    #This shit are the one-hot encoded sentences (word 1, word 4 etc. as sentence)
    train = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0, 1, 11],
                      [1, 8, 7, 3, 4, 5, 6, 11, 2, 1, 3]]).to(device)
    target = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0, 2, 2],
                        [1, 5, 6, 2, 4, 7, 6, 2, 9, 1]]).to(device)
    max_len = max([len(x) for x in train]) + 1
    
    &quot;&quot;&quot;
    #Loading in data
    data = pickle.load(open('Testdaten.pkl', 'rb'))
    tmp = myDataset(data, 'POS')
    #Calculating maximum sentence length (+ 1 because of start tag)
    max_len = max([len(x) for x in tmp.sent_encoded]) + 1
    pad_element = len(tmp.lookup_words)
    
    #Padding everything out to maximum sentence length
    train_tmp = []
    for sent in tmp.sent_encoded:
        train_tmp.append([pad_element] + sent + [pad_element] * (max_len - len(sent) - 1))
    target_tmp = []
    for sent in tmp.tags_encoded:
        target_tmp.append(sent + [pad_element] * (max_len - len(sent) - 1))
        
    #Creating tensors for model
    train = torch.squeeze(torch.tensor(train_tmp))
    target = torch.squeeze(torch.tensor(target_tmp))
    #&quot;&quot;&quot;
    src_pad_idx = 0
    trg_pad_idx = 0
    src_vocab_size = int(torch.max(train)) + 1
    trg_vocab_size = int(torch.max(target)) + 1
    heads = 8
    es = nextMultiple(max(src_vocab_size, trg_vocab_size), heads)
    
    model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx,
                        trg_pad_idx, es, 3, 2, heads, 0.1, device,
                        max_len).to(device)
    
    #Defining loss function and optimizer
    lr = 0.001
    num_epochs = 2
    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)
    loss_func = nn.CrossEntropyLoss()
    
    # optimization algorithm
    optimizer = optim.Adam(model.parameters(), lr=lr)
    # train and evaluation
    for cnt in range(num_epochs):
        optimizer.zero_grad()

        outputs = model(train, target)
        outputs = outputs
        #Outputs now are size[3, 119, 119]
        #CrossEntropyLoss mag one-hot-encoding nicht, how to deal with this?
        loss = loss_func(outputs, target)
        loss.backward()
        optimizer.step()
    #out = model(train, target)
    #print(out.shape)
</code></pre>
<p>I am confused since the code works with the vectors from the tutorial, but once I try to run the model with my own vocabulary, it produces this strange error. The data is just integer values encoding the corresponding words, e.g. &quot;Hello World&quot; would result in the training vector [1 2].</p>
<p>There is no differences between my data and the data from the tutorial as far as I can see. The tensor types are the same (Torch.LongTensor), they are both integer values and in a specified range. The difference is in dimensionality, the tutorial uses vectors with dimension (2, 10), while mine are (3, 199).</p>
<p>Also, I am sorry, but I can't reduce the code any more since otherwise, the error might not be reproduceable.</p>
<p>Did anyone encounter this error before?</p>
","transformer-model"
"67979876","Must the vocab size must math the vocab_size in bert_config.json exactly?","2021-06-15 04:07:50","67982206","3","1925","<bert-language-model><huggingface-transformers><transformer-model>","<p>I am seeing someone other's BERT model, in which the vocab.txt's size is 22110, but the <code>vocab_size</code> parameter's value is 21128 in bert_config.json.</p>
<p>I understand that these two numbers must be exactly the same. Is that right?</p>
","transformer-model"
"67952967","Import error for Facebook library 'faiss'. It's throwing DLL load failed error","2021-06-12 21:00:35","","0","363","<python><dll><pytorch><transformer-model><faiss>","<p>Import error for Facebook library 'faiss'. It's throwing DLL load failed error</p>
<pre><code>import faiss
</code></pre>
<p>Error is :</p>
<p><strong>ImportError: DLL load failed: The specified module could not be found.</strong></p>
<p>Can anyone help me with this? I've installed with pip install faiss-cpu and python version is 3.7.9</p>
<p>It's running good in colab, but not in local pc</p>
","transformer-model"
"67948945","Force BERT transformer to use CUDA","2021-06-12 12:39:11","67950754","4","8862","<python><pytorch><huggingface-transformers><transformer-model>","<p>I want to force the Huggingface transformer (BERT) to make use of CUDA.
nvidia-smi showed that all my CPU cores were maxed out during the code execution, but my GPU was at 0% utilization. Unfortunately, I'm new to the Hugginface library as well as PyTorch and don't know where to place the CUDA attributes <code>device = cuda:0</code> or <code>.to(cuda:0)</code>.</p>
<p>The code below is basically a customized part from <a href=""https://huggingface.co/oliverguhr/german-sentiment-bert"" rel=""nofollow noreferrer"">german sentiment BERT working example</a></p>
<pre><code>class SentimentModel_t(pt.nn.Module):
      def __init__(self, model_name: str = &quot;oliverguhr/german-sentiment-bert&quot;):
           DEVICE = &quot;cuda:0&quot; if pt.cuda.is_available() else &quot;cpu&quot;
           print(DEVICE)
           super(SentimentModel_t,self).__init__()

           self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(DEVICE)
           self.tokenizer = BertTokenizerFast.from_pretrained(model_name)
    
        def predict_sentiment(self, texts: List[str])-&gt; List[str]:
            texts = [self.clean_text(text) for text in texts]
            # Add special tokens takes care of adding [CLS], [SEP], &lt;s&gt;... tokens in the right way for each model.
            input_ids = self.tokenizer.batch_encode_plus(texts,padding=True, add_special_tokens=True, truncation=True, max_length=self.tokenizer.max_len_single_sentence)
            input_ids = pt.tensor(input_ids[&quot;input_ids&quot;])
    
            with pt.no_grad():
                logits = self.model(input_ids)
    
            label_ids = pt.argmax(logits[0], axis=1)
    
            labels = [self.model.config.id2label[label_id] for label_id in label_ids.tolist()]
            return labels
</code></pre>
<p>EDIT: After applying the suggestions of @KonstantinosKokos (see edited code above) I got a</p>
<pre><code>RuntimeError: Input, output and indices must be on the current device
</code></pre>
<p>pointing to</p>
<pre><code>        with pt.no_grad():
           logits = self.model(input_ids)
</code></pre>
<p>The full error code can be obtained down below:</p>
<pre><code>&lt;ipython-input-15-b843edd87a1a&gt; in predict_sentiment(self, texts)
     23 
     24         with pt.no_grad():
---&gt; 25             logits = self.model(input_ids)
     26 
     27         label_ids = pt.argmax(logits[0], axis=1)

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)
   1364         return_dict = return_dict if return_dict is not None else self.config.use_return_dict
   1365 
-&gt; 1366         outputs = self.bert(
   1367             input_ids,
   1368             attention_mask=attention_mask,

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, output_attentions, output_hidden_states, return_dict)
    859         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)
    860 
--&gt; 861         embedding_output = self.embeddings(
    862             input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
    863         )

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py in forward(self, input_ids, token_type_ids, position_ids, inputs_embeds)
    196 
    197         if inputs_embeds is None:
--&gt; 198             inputs_embeds = self.word_embeddings(input_ids)
    199         token_type_embeddings = self.token_type_embeddings(token_type_ids)
    200 

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/modules/sparse.py in forward(self, input)
    122 
    123     def forward(self, input: Tensor) -&gt; Tensor:
--&gt; 124         return F.embedding(
    125             input, self.weight, self.padding_idx, self.max_norm,
    126             self.norm_type, self.scale_grad_by_freq, self.sparse)

~/PycharmProjects/Test_project/venv/lib/python3.8/site-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   1850         # remove once script supports set_grad_enabled
   1851         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-&gt; 1852     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   1853 
   1854 
</code></pre>
","transformer-model"
"67924216","Why would a Torchscript trace return different looking encoded_inputs compared to the original Transformer model?","2021-06-10 15:29:36","68366022","1","1233","<pytorch><huggingface-transformers><transformer-model><machine-translation><torchscript>","<h2>Background</h2>
<p>I'm working with a finetuned <a href=""https://huggingface.co/transformers/model_doc/mbart.html?highlight=mbart"" rel=""nofollow noreferrer"">Mbart50</a> model that I need sped up for inferencing because using the HuggingFace model as-is is fairly slow with my current hardware. I wanted to use <a href=""https://pytorch.org/docs/stable/jit.html"" rel=""nofollow noreferrer"">TorchScript</a> because I couldn't get <a href=""https://huggingface.co/transformers/serialization.html?highlight=onnx"" rel=""nofollow noreferrer"">onnx</a> to export this particular model as it seems it will be supported at a later time (I would be glad to be wrong otherwise).</p>
<h2>Convert Transformer to a Pytorch trace:</h2>
<pre class=""lang-py prettyprint-override""><code>import torch
&quot;&quot;&quot; Model data  &quot;&quot;&quot;
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

model = MBartForConditionalGeneration.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;, torchscript= True)

tokenizer = MBart50TokenizerFast.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;)
tokenizer.src_lang = 'en_XX'

dummy = &quot;To celebrate World Oceans Day, we're swimming through a shoal of jack fish just off the coast of Baja, California, in Cabo Pulmo National Park. This Mexican marine park in the Sea of Cortez is home to the northernmost and oldest coral reef on the west coast of North America, estimated to be about 20,000 years old. Jacks are clearly plentiful here, but divers and snorkelers in Cabo Pulmo can also come across many other species of fish and marine mammals, including several varieties of sharks, whales, dolphins, tortoises, and manta rays.&quot;

model.config.forced_bos_token_id=250006
myTokenBatch = tokenizer(dummy, max_length=192, padding='max_length', truncation = True, return_tensors=&quot;pt&quot;)

torch.jit.save(torch.jit.trace(model, [myTokenBatch.input_ids,myTokenBatch.attention_mask]), &quot;././traced-model/mbart-many.pt&quot;)
</code></pre>
<h2>Inference Step:</h2>
<pre class=""lang-py prettyprint-override""><code>

import torch
 &quot;&quot;&quot; Model data  &quot;&quot;&quot;
from transformers import MBart50TokenizerFast

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
tokenizer = MBart50TokenizerFast.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;)
 
model = torch.jit.load('././traced-model/mbart-many.pt')
MAX_LENGTH =  192

tokenizer = MBart50TokenizerFast.from_pretrained(&quot;facebook/mbart-large-50-one-to-many-mmt&quot;)
 
model.to(device)
model.eval()

tokenizer.src_lang = 'en_XX'

dummy = &quot;To celebrate World Oceans Day, we're swimming through a shoal of jack fish just off the coast of Baja, California, in Cabo Pulmo National Park. This Mexican marine park in the Sea of Cortez is home to the northernmost and oldest coral reef on the west coast of North America, estimated to be about 20,000 years old. Jacks are clearly plentiful here, but divers and snorkelers in Cabo Pulmo can also come across many other species of fish and marine mammals, including several varieties of sharks, whales, dolphins, tortoises, and manta rays.&quot;

myTokenBatch = tokenizer(dummy, max_length=192, padding='max_length', truncation = True, return_tensors=&quot;pt&quot;)

encode, pool , norm  = model(myTokenBatch.input_ids,myTokenBatch.attention_mask)


</code></pre>
<h2>Expected Encoding Output:</h2>
<p>These are tokens that can be decoded to words with MBart50TokenizerFast.</p>
<pre><code>
tensor([[250004,    717, 176016,   6661,  55609,      7,  10013,      4,    642,
             25,    107, 192298,   8305,     10,  15756,    289,    111, 121477,
          67155,   1660,   5773,     70, 184085,    111, 118191,      4,  39897,
              4,     23, 143740,  21694,    432,   9907,   5227,      5,   3293,
         181815, 122084,   9201,     23,     70,  27414,    111,  48892,    169,
             83,   5368,     47,     70, 144477,   9022,    840,     18,    136,
          10332,    525, 184518,    456,   4240,     98,     70,  65272, 184085,
            111,  23924,  21629,      4,  25902,   3674,     47,    186,   1672,
              6,  91578,   5369,  10332,      5,  21763,      7,    621, 123019,
          32328,    118,   7844,   3688,      4,   1284,  41767,    136, 120379,
           2590,   1314,     23, 143740,  21694,    432,    831,   2843,   1380,
          36880,   5941,   3789, 114149,    111,  67155,    136, 122084,  21968,
           8080,      4,  26719,  40368,    285,  68794,    111,  54524,   1224,
              4,    148,  50742,      7,      4,  13111,  19379,   1779,      4,
          43807, 125216,      7,      4,    136,    332,    102,  62656,      7,
              5,      2,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1,      1,      1,      1,      1,      1,      1,
              1,      1,      1]])
</code></pre>
<h2>Actual Output:</h2>
<p>I don't know what this is... <code>print(encode)</code></p>
<pre><code>
(tensor([[[[-9.3383e-02, -2.0395e-01,  4.8226e-03,  ...,  1.8068e+00,
            1.1528e-01,  7.0406e-02],
          [-4.4630e-02, -2.2453e-01,  9.5264e-02,  ...,  1.6921e+00,
            1.4607e-01,  4.8238e-02],
          [-7.8206e-01,  1.2699e-01,  1.6467e+00,  ..., -1.7057e+00,
            8.7768e-01,  8.2230e-01],
          ...,
 [-1.2145e-02, -2.1855e-03, -6.0966e-03,  ...,  2.9296e-02,
            2.2141e-03,  3.2074e-02],
          [-1.4671e-02, -2.8995e-03, -5.8610e-03,  ...,  2.8525e-02,
            2.4620e-03,  3.1593e-02],
          [-1.5877e-02, -3.5165e-03, -4.8743e-03,  ...,  2.8930e-02,
            2.9877e-03,  3.3892e-02]]]], grad_fn=&lt;CopyBackwards&gt;))

</code></pre>
","transformer-model"
"67912652","Memory bottleneck with autoregressive transformer decoding","2021-06-09 23:01:02","","0","597","<pytorch><transformer-model>","<p>I am trying to train a transformer model for sequence modeling. Below is a standalone example:</p>
<pre><code>import torch
import torch.nn as nn

criterion = nn.MSELoss()

decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=12)
memory = torch.rand(10, 32, 512)
y = torch.rand(20, 32, 512)

start_token = torch.ones((1,32,512))
tgt_input = torch.cat((start_token,y[:-1,:]),axis=0)

optimizer = torch.optim.Adam(transformer_decoder.parameters())

###################Teacher forced
while(True):
    optimizer.zero_grad()
    out = transformer_decoder(tgt_input, memory, nn.Transformer.generate_square_subsequent_mask(20,20))

    loss = criterion(out,y)
    print(&quot;loss: &quot;, loss.item())
    
    loss.backward()
    optimizer.step()
</code></pre>
<p>For a 12 layer decoder, the model works fine on a personal machine with 8GB memory. The model is autoregressive and works with shifted targets. Given we provide targets above, I refer to this setting as &quot;teacher forced&quot;.</p>
<p>However, at inference stage, we will not have targets fed as above, and one would need to condition on targets generated on the go. This setting is as follows:</p>
<pre><code>###################Non Teacher forced
while(True):
    optimizer.zero_grad()
    predictions = torch.ones((1,32,512))
    for i in range(1,21):
        predictions = torch.cat((predictions, transformer_decoder(tgt_input[:i], memory, nn.Transformer.generate_square_subsequent_mask(i,i))[-1].unsqueeze(0)),axis=0)
        print(&quot;i: &quot;, i, &quot;predictions.shape: &quot;, predictions.shape)
        
    loss = criterion(predictions[1:],y)
    print(&quot;loss: &quot;, loss.item())
    
    loss.backward()
    optimizer.step()  
</code></pre>
<p>I wish to train the model with a hybrid training strategy with, without teacher forcing. However, the non-teacher forced strategy causes out-of-memory exception and doesn't work. For final inference (testing), usually, <code>with torch.no_grad()</code> it can work, but not in training. Can anyone explain as to why this causes memory bottlenecks exactly?</p>
","transformer-model"
"67884135","sequence encoding by MultiHeadAttention","2021-06-08 08:32:23","","1","150","<machine-learning><pytorch><transformer-model><attention-model>","<p>I am trying to encode a sequence of image embeddings to one bigger embedding using MultiHeadAttention. (order doesn't matter)</p>
<p>I want an operation similar to passing a sequence of shape <code>(batch_size, sequence_length, embedding_dim)</code> to an LSTM layer and taking the last hidden_state as an embedding holding all importatnt information about the sequence.. or we may say a sequence embedding.</p>
<p>I want to implement that using attention to get rid of the recurrent behavior..</p>
<pre><code># embeddings: shape(batch_size, sequence_length, embedding_dim)
multihead_attn = nn.MultiheadAttention(embedding_dim, num_heads)
attn_output, attn_output_weights = multihead_attn(embeddings, embeddings, embeddings)
</code></pre>
<p>but in this case the attention output will have a shape of <code>(batch_size, sequence_length, embedding_dim)</code> as well..
should I be doing</p>
<pre><code>attn_output = attn_output.mean(1)
</code></pre>
<p>or what if I pass the query as the <code>embeddings.mean(1)</code>.. will that give the intended behavior? will the output simulate a sequence embedding?</p>
<pre><code># embeddings: shape(batch_size, sequence_length, embedding_dim)
multihead_attn = nn.MultiheadAttention(embedding_dim, num_heads)
seq_emb, attn_output_weights = multihead_attn(embeddings.mean(1), embeddings, embeddings)
</code></pre>
","transformer-model"
"67862211","Getting 'ValueError: Could not find matching function to call loaded from the SavedModel.' on training a model","2021-06-06 18:05:54","","1","280","<tensorflow2.0><preprocessor><transformer-model><tf.data.dataset>","<p>I'm using Tensorflow 2 and LaBSE pre-trained model on tf-hub (not much familiar with both). (<a href=""https://tfhub.dev/google/LaBSE/2"" rel=""nofollow noreferrer"">https://tfhub.dev/google/LaBSE/2</a>). I am trying to train a multi class classifier with a custom text data set. I am also following this example on BERT classifier(<a href=""https://www.tensorflow.org/text/tutorials/classify_text_with_bert"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/classify_text_with_bert</a>), to get the idea of how the model is built. This is to check only I can train and run the model. I am using data set objects for my input texts obtained from csv data which looks like this,</p>
<pre><code>&quot;Sentence&quot;,&quot;label&quot;
&quot;sentence sample1&quot;, 0
&quot;sentence sample2&quot;, 3
</code></pre>
<p>and I split them into X, y sets as usual. However,
I get the above error when trying to train the model. Below is my code,</p>
<pre><code>def build_classifier_model():
  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
  encoder_inputs = preprocessing_layer(text_input)
  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='LaBSE_encoder')
  outputs = encoder(encoder_inputs)
  net = outputs['pooled_output']
  activation= tf.keras.activations.softmax#None
  net = tf.keras.layers.Dropout(0.1)(net)
  net = tf.keras.layers.Dense(4, activation=activation, name='classifier')(net)
  return tf.keras.Model(text_input, net)

loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
metrics = tf.keras.metrics.SparseCategoricalAccuracy()
epochs = 5
optimizer=tf.keras.optimizers.Adam()

train_dataset = tf.data.Dataset.from_tensor_slices(( # convert to dataset objects
    np.array(X_train),np.array(y_train,dtype='int32')
  
))

test_dataset = tf.data.Dataset.from_tensor_slices((
    np.array(X_test),np.array(y_test,dtype='int32')
    
))
</code></pre>
<p>These dataset objects' specs are, &lt;TensorSliceDataset shapes: ((), ()), types: (tf.string, tf.int32)&gt;</p>
<pre><code>
classifier_model.compile(optimizer=optimizer,
                         loss=loss,
                         metrics=metrics)

his = classifier_model.fit(train_dataset, validation_data=test_dataset,
                               epochs=epochs, batch_size=8) #ignore that I'm using test dataset for validation dataset


</code></pre>
<p>The last step gives the error;</p>
<pre><code>Epoch 1/5
WARNING:tensorflow:Model was constructed with shape (None,) for input KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='text'), name='text', description=&quot;created by layer 'text'&quot;), but it was called on an input with incompatible shape ().
WARNING:tensorflow:Model was constructed with shape (None,) for input KerasTensor(type_spec=TensorSpec(shape=(None,), dtype=tf.string, name='text'), name='text', description=&quot;created by layer 'text'&quot;), but it was called on an input with incompatible shape ().
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
&lt;ipython-input-141-3523a14b56f1&gt; in &lt;module&gt;()
      1 history = classifier_model.fit(train_dataset, validation_data=test_dataset,
----&gt; 2                                epochs=epochs, batch_size=8)

9 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    984           except Exception as e:  # pylint:disable=broad-except
    985             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 986               raise e.ag_error_metadata.to_exception(e)
    987             else:
    988               raise

ValueError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:855 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py:237 call  *
        result = smart_cond.smart_cond(training,
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/load.py:670 _call_attribute  **
        return instance.__call__(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:889 __call__
        result = self._call(*args, **kwds)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:924 _call
        results = self._stateful_fn(*args, **kwds)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3022 __call__
        filtered_flat_args) = self._maybe_define_function(args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3444 _maybe_define_function
        graph_function = self._create_graph_function(args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3289 _create_graph_function
        capture_by_value=self._capture_by_value),
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:999 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:672 wrapped_fn
        out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/function_deserialization.py:291 restored_function_body
        &quot;\n\n&quot;.join(signature_descriptions)))

    ValueError: Could not find matching function to call loaded from the SavedModel. Got:
      Positional arguments (3 total):
        * Tensor(&quot;inputs:0&quot;, shape=(), dtype=string)
        * False
        * None
      Keyword arguments: {}
    
    Expected these arguments to match one of the following 4 option(s):
    
    Option 1:
      Positional arguments (3 total):
        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')
        * True
        * None
      Keyword arguments: {}
    
    Option 2:
      Positional arguments (3 total):
        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')
        * False
        * None
      Keyword arguments: {}
    
    Option 3:
      Positional arguments (3 total):
        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')
        * True
        * None
      Keyword arguments: {}
    
    Option 4:
      Positional arguments (3 total):
        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')
        * False
        * None
      Keyword arguments: {}

</code></pre>
<p>I think this is an issue with the dataset object's specs provided to the input but, do not understand how to fix it or the exact reason. I do not get even though my dataset object has types of &quot;tf.string&quot; why it is incompatible with the input expected. I looked up in existing answers and since I'm not much familiar with TF, I want to know what is the reason and how could I fix this issue.</p>
","transformer-model"
"67855040","PyTorch Transformer Model, some values are always 0","2021-06-06 01:01:53","","0","314","<python><pytorch><transformer-model>","<p>As a bit of background I've implemented a slightly customized transformer model in PyTorch.</p>
<p>However, I'm consistently getting 0's in certain indices of my output :</p>
<p><a href=""https://i.sstatic.net/zMqrC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zMqrC.png"" alt=""Tensor with 0's in certain positions"" /></a></p>
<p>The target data is dense and has no zero values. Strangely, the loss is quite low as the non-zero values predicted are quite close to their target equivalents. Moreover, it's always the exact same indices that are set to 0:</p>
<p><a href=""https://i.sstatic.net/rkNhg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rkNhg.png"" alt=""Batch Tensor with 0's in same position"" /></a></p>
<p>This makes little sense to me as the model seems to work perfectly fine with another dataset (an english-french dataset where words are tokenized and passed into an embedding layer). My data is all between 0 and 1 so I doubt there is any clipping happening, and I am using ReLU as my activation function at the final layer of my model. While I have dropout layers in my model, the dropout value has already been set to 0.</p>
<p>My final layer:</p>
<pre><code>y = F.relu(self.linear4(y))
</code></pre>
<p>I originally thought it could've been the masking required for the attention mechanism, but I have this implemented within the attention mechanism right before the softmax portion of scaled dot-product attention. This means it shouldn't really have an effect on the output; moreover, the add &amp; normalize steps should propagate a gradient through no matter what. To confirm this I have tried removing the mask and seen that it does not affect the 0's in my output.</p>
<p>I'm wondering what could possibly be causing this?</p>
","transformer-model"
"67805117","MultiHeadAttention attention_mask [Keras, Tensorflow] example","2021-06-02 12:29:03","","10","7437","<tensorflow><machine-learning><keras><transformer-model><attention-model>","<p>I am struggling to mask my input for the MultiHeadAttention Layer. I am using the Transformer Block from Keras documentation with self-attention. I could not find any example code online so far and would appreciate if someone could give me a code snippet.</p>
<p>The transformer block from <a href=""https://keras.io/examples/nlp/text_classification_with_transformer/"" rel=""noreferrer"">this</a> page:</p>
<pre><code>class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation=&quot;relu&quot;), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)
</code></pre>
<p>The documentation for masking one can find under <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention"" rel=""noreferrer"">this</a> link:</p>
<blockquote>
<p>attention_mask: a boolean mask of shape [B, T, S], that prevents
attention to certain positions. The boolean mask specifies which query
elements can attend to which key elements, 1 indicates attention and 0
indicates no attention. Broadcasting can happen for the missing batch
dimensions and the head dimension.</p>
</blockquote>
<p>The only thing, I could get running is a mask created outside of the layer class as numpy array:</p>
<pre><code>mask = np.ones((observations, sequence_length, sequence_length))
mask[X[:observations,:,0]==0]=0
</code></pre>
<p>Then input while calling the layer, with the only change in the transformer block being:</p>
<pre><code>def call(self, inputs, mask, training):
    attn_output = self.att(inputs, inputs, attention_mask=mask)
</code></pre>
<p>However, this does of course not work when given a batch_size while fitting and does only work for 5 observations with my memory, so it doesn't make any sense.
Apart from that, I don't think this is masking the input properly - In general I am quite confused about how to mask, given the shape of the attention_mask (observations, sequence_length, sequence_length). The shape of my input is (observation, sequence_length, features). This input is being padded by zeros, however, when it comes to the transformer block, it has been already through an embedding layer and CNN.
I have tried various ways to write a function, which creates the mask while training with different Tensor or Keras objects. However I am running each time into errors.</p>
<p>I hope someone more fluent in Tensorflow/Keras will be able to provide an example.
Or somebody tells me that masking is useless given my architecture. The model is performing well. However, I hoped masking could help speed up the computing.
And it just buggs me that I cannot get my head around it.</p>
","transformer-model"
"67783283","What does ""fine-tuning of a BERT model"" refer to?","2021-06-01 05:46:24","","2","2146","<nlp><bert-language-model><huggingface-transformers><transformer-model>","<p>I was not able to understand one thing , when it says &quot;fine-tuning of BERT&quot;,  what does it actually mean:</p>
<ol>
<li>Are we retraining the entire model again with new data.</li>
<li>Or are we just training top few transformer layers with new data.</li>
<li>Or we are training the entire model but considering the pretrained weights as initial weight.</li>
<li>Or there is already few layers of ANN on top of transformer layers which is only getting trained keeping transformer weight freeze.</li>
</ol>
<p>Tried Google but I am getting confused, if someone can help me on this.</p>
<p>Thanks in advance!</p>
","transformer-model"
"67743498","How can/should we weight classes in HuggingFace token classification (entity recognition)?","2021-05-28 17:49:08","67744122","2","2108","<pytorch><huggingface-transformers><transformer-model>","<p>I'm training a <a href=""https://huggingface.co/transformers/task_summary.html#named-entity-recognition"" rel=""nofollow noreferrer"">token classification</a> (AKA named entity recognition) model with the <a href=""https://huggingface.co/transformers/index.html"" rel=""nofollow noreferrer"">HuggingFace Transformers</a> library, with a customized data loader.</p>
<p>Like most NER datasets (I'd imagine?) there's a pretty significant <strong>class imbalance</strong>: A large majority of tokens are <code>other</code> - i.e. <strong>not</strong> an entity - and of course there's a little variation between the different entity classes themselves.</p>
<p>As we might expect, my &quot;accuracy&quot; metrics are getting distorted quite a lot by this: It's no great achievement to get 80% token classification accuracy if 90% of your tokens are <code>other</code>... A trivial model could have done better!</p>
<p>I can calculate some additional and more insightful evaluation metrics - but it got me wondering... Can/should we somehow incorporate these weights into the training loss? How would this be done using a typical <code>*ForTokenClassification</code> model e.g. <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertfortokenclassification"" rel=""nofollow noreferrer"">BERTForTokenClassification</a>?</p>
","transformer-model"
"67740759","How to apply a pretrained transformer model from huggingface?","2021-05-28 14:27:04","67780432","3","10720","<huggingface-transformers><named-entity-recognition><transformer-model>","<p>I am interested in using pre-trained models from Hugging Face for named entity recognition (NER) tasks without further training or testing of the model.</p>
<p>On the <a href=""https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT"" rel=""nofollow noreferrer"">model page of Hugging Face</a>, the only information for reusing the model are as follows:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
</code></pre>
<p>I tried the following code, but I am getting a tensor output instead of class labels for each named entity.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

text = &quot;my text for named entity recognition here.&quot;

input_ids = torch.tensor(tokenizer.encode(text, padding=True, truncation=True,max_length=50, add_special_tokens = True)).unsqueeze(0)

with torch.no_grad():
  output = model(input_ids, output_attentions=True)
</code></pre>
<p>Any suggestions on how to apply the model on a text for NER?</p>
","transformer-model"
"67730887","beam search language translation pytorch","2021-05-27 22:58:23","","1","401","<pytorch><transformer-model>","<p>I want beam search code for making language translation PyTorch, I want to use it with sample code in this link.</p>
<p><a href=""https://pytorch.org/tutorials/beginner/translation_transformer.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/translation_transformer.html</a></p>
<p>Because this link is only presented how to greedy decode in the translation process.</p>
","transformer-model"
"67701984","Training loss remains nearly unchanged while validation loss fluctuates around","2021-05-26 09:17:05","","0","163","<pytorch><transformer-model><loss>","<p>I am using a pretrained Vision Transformer model to train a new model in which input is images and output is predicted prices of those images.</p>
<p>As the Vision Transformer model is used for classifying 1024 classes, I changed its last layer into nn.Linear(1024,1) and I am using MSELoss instead of softmax. However, the training loss and validation loss keep being high. As in the picture below, training loss remains nearly unchanged while validation loss fluctuates a bit around 800000.</p>
<p>Do you have any ideas about what should I do to improve the learning?</p>
<p><a href=""https://i.sstatic.net/Wpgrf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Wpgrf.png"" alt=""enter image description here"" /></a></p>
<p>[Update]
Here is part of the code</p>
<pre><code>#Import model
pip install timm
import timm
model = timm.create_model('vit_large_patch16_224_in21k', pretrained=True)
for param in model.parameters():
    param.requires_grad = False
    
model.head = nn.Sequential(nn.Linear(1024, 10), 
                           nn.ReLU(), 
                           nn.Linear(10, 1))
model.cuda()

#Training setting
device = 'cuda'
learning_rate = 0.0005
gamma = 0.1
seed = 42
use_gpu_number = 0
epochs = 30
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
scheduler = StepLR(optimizer, step_size=7, gamma=gamma)

#Training
loss_list = []
val_loss_list = []
val_corr_list = []
for epoch in range(epochs):
    epoch_loss = 0
    epoch_accuracy = 0
    
    model.train()
    for images, price in tqdm(train_loader):
        images = images.to(device)
        price = price.to(device)

        optimizer.zero_grad()
        
        output = model(images)
        loss = criterion(output, price)
        
        loss.backward()
        optimizer.step()

        epoch_loss += loss / len(train_loader)
        
    loss_list.append(epoch_loss.float())

    model.eval()    
    with torch.no_grad():
        epoch_val_accuracy = 0
        epoch_val_loss = 0
        for images, price in valid_loader:
            images = images.to(device)
            price = price.to(device)

            val_output = model(images)
            val_loss = criterion(val_output, price)

            epoch_val_loss += val_loss / len(valid_loader)
        _, _, _, val_corr = model_valid(valid_loader,
                              model,
                              use_gpu_number,
                              criterion)
        
        val_loss_list.append(epoch_val_loss.float())
        val_corr_list.append(val_corr)
</code></pre>
","transformer-model"
"67699354","Are these normal speed of Bert Pretrained Model Inference in PyTorch","2021-05-26 06:07:27","67712401","2","1647","<bert-language-model><huggingface-transformers><transformer-model><huggingface-tokenizers>","<p>I am testing Bert base and Bert distilled model in Huggingface with 4 scenarios of speeds, batch_size = 1:</p>
<pre><code>1) bert-base-uncased: 154ms per request
2) bert-base-uncased with quantifization: 94ms per request
3) distilbert-base-uncased: 86ms per request
4) distilbert-base-uncased with quantifization: 69ms per request
</code></pre>
<p>I am using the IMDB text as experimental data and set the max_length=512, so it's quite long. The cpu on Ubuntu 18.04 info is below:</p>
<pre><code>cat /proc/cpuinfo  | grep 'name'| uniq
model name  : Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz
</code></pre>
<p>The machine has 3 GPU available for use:</p>
<pre><code>Tesla V100-SXM2
</code></pre>
<p>It seems quite slow for realtime application. Are those speeds normal for bert base model?</p>
<p>The testing code is below:</p>
<pre><code>import pandas as pd
import torch.quantization

from transformers import AutoTokenizer, AutoModel, DistilBertTokenizer, DistilBertModel

def get_embedding(model, tokenizer, text):
    inputs = tokenizer(text, return_tensors=&quot;pt&quot;, max_length=512, truncation=True)
    outputs = model(**inputs)
    output_tensors = outputs[0][0]
    output_numpy = output_tensors.detach().numpy()
    embedding = output_numpy.tolist()[0]

def process_text(model, tokenizer, text_lines):
    for index, line in enumerate(text_lines):
        embedding = get_embedding(model, tokenizer, line)
        if index % 100 == 0:
            print('Current index: {}'.format(index))

import time
from datetime import timedelta
if __name__ == &quot;__main__&quot;:

    df = pd.read_csv('../data/train.csv', sep='\t')
    df = df.head(1000)
    text_lines = df['review']
    text_line_count = len(text_lines)
    print('Text size: {}'.format(text_line_count))

    start = time.time()

    tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
    model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)
    process_text(model, tokenizer, text_lines)

    end = time.time()
    print('Total time spent with bert base: {}'.format(str(timedelta(seconds=end - start))))

    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
    process_text(model, tokenizer, text_lines)

    end2 = time.time()
    print('Total time spent with bert base quantization: {}'.format(str(timedelta(seconds=end2 - end))))

    tokenizer = DistilBertTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)
    model = DistilBertModel.from_pretrained(&quot;distilbert-base-uncased&quot;)
    process_text(model, tokenizer, text_lines)

    end3 = time.time()
    print('Total time spent with distilbert: {}'.format(str(timedelta(seconds=end3 - end2))))

    model = DistilBertModel.from_pretrained(&quot;distilbert-base-uncased&quot;)
    model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)
    process_text(model, tokenizer, text_lines)

    end4 = time.time()
    print('Total time spent with distilbert quantization: {}'.format(str(timedelta(seconds=end4 - end3))))
</code></pre>
<p>EDIT: based on suggestion I changed to the following:</p>
<pre><code>inputs = tokenizer(text_batch, padding=True, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
</code></pre>
<p>Where text_batch is a list of text as input.</p>
","transformer-model"
"67668402","Why doesn't BertForMaskedLM generate right masked tokens?","2021-05-24 07:48:26","","3","584","<nlp><bert-language-model><huggingface-transformers><transformer-model>","<p>I am testing this piece of code:</p>
<pre><code>from transformers import BertTokenizer, BertModel, BertForMaskedLM

tokenizer = BertTokenizer.from_pretrained(&quot;hfl/chinese-roberta-wwm-ext&quot;)
model = BertForMaskedLM.from_pretrained(&quot;hfl/chinese-roberta-wwm-ext&quot;)

from transformers import pipeline

def check_model(model, tokenizer):
    fill_mask = pipeline(
        &quot;fill-mask&quot;,
        model=model,
        tokenizer=tokenizer
    )
    print('Fill blank: ')
    fill_mask(&quot;我喜欢 {nlp.tokenizer.mask_token}.&quot;)

    print('Fill blank: ')
    fill_mask(&quot;这个品牌的面膜 {nlp.tokenizer.mask_token}.&quot;)

print('Check model ...')
check_model(model, tokenizer)
</code></pre>
<p>But it prints out this error message:</p>
<pre><code>raceback (most recent call last):
  File &quot;/Users/congminmin/nlp/embedding/transformer/bert_roberta_wwm_test.py&quot;, line 21, in &lt;module&gt;
    check_model(model, tokenizer)
  File &quot;/Users/congminmin/nlp/embedding/transformer/bert_roberta_wwm_test.py&quot;, line 15, in check_model
    fill_mask(&quot;我喜欢 {nlp.tokenizer.mask_token}.&quot;)
  File &quot;/Users/congminmin/.venv/wbkg/lib/python3.7/site-packages/transformers/pipelines/fill_mask.py&quot;, line 162, in __call__
    self.ensure_exactly_one_mask_token(masked_index.numpy())
  File &quot;/Users/congminmin/.venv/wbkg/lib/python3.7/site-packages/transformers/pipelines/fill_mask.py&quot;, line 90, in ensure_exactly_one_mask_token
    f&quot;No mask_token ({self.tokenizer.mask_token}) found on the input&quot;,
transformers.pipelines.base.PipelineException: No mask_token ([MASK]) found on the input
</code></pre>
","transformer-model"
"67625349","Why doesn't trainer report evaluation metrics while training in the tutorial?","2021-05-20 17:31:59","","5","7810","<python><huggingface-transformers><transformer-model>","<p>I am following this tutorial to learn about the trainer API.
<a href=""https://huggingface.co/transformers/training.html"" rel=""noreferrer"">https://huggingface.co/transformers/training.html</a></p>
<p>I copied the code as below:</p>
<pre><code>from datasets import load_dataset

import numpy as np
from datasets import load_metric

metric = load_metric(&quot;accuracy&quot;)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

print('Download dataset ...')
raw_datasets = load_dataset(&quot;imdb&quot;)
from transformers import AutoTokenizer

print('Tokenize text ...')
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

print('Prepare data ...')
small_train_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(500))
small_eval_dataset = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(500))
full_train_dataset = tokenized_datasets[&quot;train&quot;]
full_eval_dataset = tokenized_datasets[&quot;test&quot;]

print('Define model ...')
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=2)

print('Define trainer ...')
from transformers import TrainingArguments, Trainer
training_args = TrainingArguments(&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

print('Fine-tune train ...')
trainer.evaluate()
</code></pre>
<p>However, it doesn't report anything about training metrics, but the following message:</p>
<pre><code>Download dataset ...
Reusing dataset imdb (/Users/congminmin/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)
Tokenize text ...
100%|██████████| 25/25 [00:06&lt;00:00,  4.01ba/s]
100%|██████████| 25/25 [00:06&lt;00:00,  3.99ba/s]
100%|██████████| 50/50 [00:13&lt;00:00,  3.73ba/s]
Prepare data ...
Define model ...
Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Define trainer ...
Fine-tune train ...
100%|██████████| 63/63 [08:35&lt;00:00,  8.19s/it]

Process finished with exit code 0
</code></pre>
<p>Isn't the tutorial updated? should I make some configuration changes to report the metrics?</p>
","transformer-model"
"67595500","How to download a model from huggingface?","2021-05-19 00:34:01","67599169","74","196408","<huggingface-transformers><transformer-model>","<p>For example, I want to download <code>bert-base-uncased</code> on <a href=""https://huggingface.co/models"" rel=""noreferrer"">https://huggingface.co/models</a>, but can't find a 'Download' link. Or is it not downloadable?</p>
","transformer-model"
"67595060","Do I need to train on my own data in using bert model as an embedding vector?","2021-05-18 23:14:33","","3","1026","<bert-language-model><huggingface-transformers><transformer-model>","<p>When I try the huggingface models and it gives the following error message:</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;bert-base-uncased&quot;)
inputs = tokenizer(&quot;Hello world!&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)
</code></pre>
<p>And the error message:</p>
<pre><code>Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<p>My purpose is to find a pretrained model to create embedding vectors for my text, so that it can be used in downstream text. I don't want to create my own pretrained models to generate the embedding vector. In this case, can I ignore those warning messages, or I need to continue to train on my own data? In another post I learn that &quot;Most of the official models don't have pretrained output layers. The weights are randomly initialized. You need to train them for your task.&quot;  My understanding is that I don't need to train if I just want to get generic embedding vector for my text based on the public models, like Huggingface. Is that right?</p>
<p>I am new to transformer and please comment.</p>
","transformer-model"
"67564014","BERT to XLNET train model","2021-05-17 04:29:19","67577114","0","177","<python><tensorflow><nlp><tf.keras><transformer-model>","<p>I'm trying to do something like this in XLNet but I can't find this part in the documentation, any help would be valuable, thanks!</p>
<pre class=""lang-py prettyprint-override""><code># we access the transformer model within our bert object using the bert attribute 
# (eg bert.bert instead of bert)

embeddings = bert.bert(input_ids, attention_mask=mask)[1]  # access pooled activations with [1]
</code></pre>
<p>(Instead of bert.bert I'm trying to do it with xlnet)</p>
","transformer-model"
"67547435","Error in Prediction results of Named Entity Recognition Task","2021-05-15 13:51:59","","1","143","<python><pytorch><named-entity-recognition><transformer-model>","<p>After training the NER Task with using RoBERTa Architecture, I got the below result</p>
<pre><code>{
 'eval_loss': 0.003242955543100834,
 'eval_precision': 0.9959672534053343,
 'eval_recall': 0.9959672534053343,
 'eval_f1': 0.9959672534053343,
 'eval_accuracy': 0.9995624335836689
}
</code></pre>
<p>The result generally is quite high, as I expected. But here is my confusion, when I randomly input a set of sentences (out of the training set) to really know the model's performance.</p>
<p>My pseudo code:</p>
<pre><code>def tokenize_and_align_labels_random(examples, tokenizer):
    tokenized_inputs = tokenizer(examples['tokens'], 
                                 truncation=True, 
                                 is_split_into_words=True)
    return tokenized_inputs
def preprocess_datasets(tokenizer, **datasets) -&gt; Dict[str, Dataset]:
    tokenize_ner = partial(tokenize_and_align_labels_random, 
                           tokenizer=tokenizer)
    return {k: ds.map(tokenize_ner) for k, ds in datasets.items()}

address=Testing_Dataset[Testing_Dataset['address']==1]['text'].apply(clean_doc).tolist()

da_datasets_random_Test = preprocess_datasets(tokenizer,
test=Dataset.from_dict({'tokens':address}))

results=da_trainer.predict(da_datasets_random_Test['test'])

predictions=results.predictions
predictions = np.argmax(predictions, axis=2)
# Remove ignored index (special tokens)
true_predictions = [
    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
    for prediction, label in zip(predictions, labels)
]
</code></pre>
<p>I input the sentences with some words <em>that don't exist in the tokenizer vocabulary</em>, and the model will handle that part for me by automatically generating their sub token.</p>
<blockquote>
<p>That means the 'input_ids' will generate more token ids for presenting these cases, the problem is their predicted tags will also be increasing (based on how many tokens was delivered to the model).</p>
</blockquote>
<p>For instance</p>
<ul>
<li><p>Input sentence: &quot;Giao tôi lê_lai phường hai tân_bình hcm&quot;</p>
</li>
<li><p>Value after tokenizer:</p>
<pre><code>{
  'input_ids': [0, 64003, 64003, 17489, 6115, 64139, 64151, 64003, 6446, 64313, 1340, 74780, 2], 
  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
</code></pre>
</li>
<li><p>Because tokenize of &quot;lê_lai&quot; is ['lê@@', '<em>l@@', 'ai']; of &quot;tân_bình&quot; is ['tân</em>@@', 'bình']; of &quot;hcm&quot; is ['h@@', 'cm']</p>
</li>
</ul>
<p>The result I got after all:</p>
<pre><code>['O','O','B-LOC','I-LOC','I-LOC','I-LOC','I-LOC','I-LOC','O','I-LOC','I-LOC', 'O']
</code></pre>
<p>In fact, their prediction should only have 7 tags for the input tokens, but now it was more than this. So do guys have any strategies for this (I got one that we can train the tokenizer with more tokens).</p>
","transformer-model"
"67531678","BERT transformer KeyError: 3","2021-05-14 08:58:46","","3","2254","<python><bert-language-model><huggingface-transformers><keyerror><transformer-model>","<p>I am quite new to the BERT language model. I am currently using the Huggingface transformer libraryand i'm  encountering an error when encoding the inputs. The goal of the model is to classify fake news.</p>
<p>First I downloaded the dataset which I turned into a pandas dataframe containing 3 columns. Index, tweet, label. The pretrained auto tokenizer from bert large uncased is used to encode the input.</p>
<pre><code>TOKENIZER = AutoTokenizer.from_pretrained(&quot;bert-large-uncased&quot;)
</code></pre>
<p>The following function is used:</p>
<pre><code>def bert_encode(data,maximum_len) :
input_ids = []
attention_masks = []


for i in range(len(data.tweet)):
    encoded = TOKENIZER.encode_plus(data.tweet[i],
                                    add_special_tokens=True,
                                    max_length=maximum_len,
                                    pad_to_max_length=True,
                                    return_attention_mask=True,
                                    truncation=True)
  
    input_ids.append(encoded['input_ids'])
    attention_masks.append(encoded['attention_mask'])
    
return np.array(input_ids),np.array(attention_masks)
</code></pre>
<p>The function is applied to the the data to get the train input id and the attention masks:</p>
<pre><code>train_input_ids,train_attention_masks = bert_encode(train,600)
test_input_ids,test_attention_masks = bert_encode(test,600)
</code></pre>
<p>However, calling the function gives me the following error: KeyError: 3
Provided beolow is the exact error message.</p>
<pre><code>---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2897             try:
-&gt; 2898                 return self._engine.get_loc(casted_key)
   2899             except KeyError as err:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.Int64HashTable.get_item()

KeyError: 3

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
4 frames
/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2898                 return self._engine.get_loc(casted_key)
   2899             except KeyError as err:
-&gt; 2900                 raise KeyError(key) from err
   2901 
   2902         if tolerance is not None:

KeyError: 3
</code></pre>
<p>Any insight on how to debug are welcome.</p>
","transformer-model"
"67381993","What is the right way to generate long sequence using PyTorch-Transformers?","2021-05-04 09:19:36","","0","317","<deep-learning><nlp><pytorch><huggingface-transformers><transformer-model>","<p>I am trying to generate a long sequence of text using PyTorch-Transformers from a sample text. I am following <a href=""https://www.analyticsvidhya.com/blog/2019/07/pytorch-transformers-nlp-python/"" rel=""nofollow noreferrer"">this tutorial</a> for this purpose. Because the original article only predicts one word from a given text, I modified that script to generate long sequence instead of one. This is the modified part of the code</p>
<pre><code># Encode a text inputs
text = &quot;&quot;&quot;An examination can be defined as a detailed inspection or analysis
 of an object or person. For example, an engineer will examine a structure,
  like a bridge, to see if it is safe. A doctor may conduct&quot;&quot;&quot;

indexed_tokens = tokenizer.encode(text)

# Convert indexed tokens in a PyTorch tensor
tokens_tensor = torch.tensor([indexed_tokens])
seq_len = tokens_tensor.shape[1]
tokens_tensor = tokens_tensor.to('cuda')


with torch.no_grad():
    for i in range(50):
        outputs = model(tokens_tensor[:,-seq_len:])
        predictions = outputs[0]
        predicted_index = torch.argmax(predictions[0, -1, :])
        tokens_tensor = torch.cat((tokens_tensor,predicted_index.reshape(1,1)),1)


pred = tokens_tensor.detach().cpu().numpy().tolist()
predicted_text = tokenizer.decode(pred[0])
print(predicted_text)
</code></pre>
<p>Output</p>
<blockquote>
<p>An examination can be defined as a detailed inspection or analysis
of an object or person. For example, an engineer will examine a
structure,   like a bridge, to see if it is safe. A doctor may conduct
an examination of a patient's body to see if it is safe.</p>
<p>The doctor may also examine a patient's body to see if it is safe. A
doctor may conduct an examination of a patient's body to see if it is
safe.</p>
</blockquote>
<p>As you can see the generated text does not generates any unique text sequence but it generates the same sentence over and over again with minor changes.</p>
<p>How should we create long sequence using  PyTorch-Transformers?</p>
","transformer-model"
"67375166","Why does Spacy 3 NER use different pipeline for GPU vs CPU?","2021-05-03 20:06:38","","1","3427","<gpu><transformer-model><spacy-3>","<p>Spacy 'train' command uses a command line option --gpu 0, allowing a 'last minute' choice between training with GPU and without it - using CPU only.</p>
<p>However, using the <a href=""https://spacy.io/usage/training#quickstart"" rel=""nofollow noreferrer"">https://spacy.io/usage/training#quickstart</a> to choose between GPU and CPU results in a major difference in (basic) configuration. In my case (dealing with NER), I get two different pipelines:</p>
<ul>
<li>for CPU: pipeline = [&quot;<strong>tok2vec</strong>&quot;,&quot;ner&quot;]</li>
<li>for GPU: pipeline = [&quot;<strong>transformer</strong>&quot;,&quot;ner&quot;]</li>
</ul>
<p>(with a very different following component setup).</p>
<p>Since my GPU has only 6GB of memory, I run out of GPU memory fairly fast - can't use it. But when I switch to using CPU only, the training behavior between the two pipelines is vastly different:</p>
<p>The [&quot;tok2vec&quot;,&quot;ner&quot;] pipeline runs pretty much on a single core, training my model (8,000 training, 2000 dev/validation docs) in couple hours. Notably faster than Spacy 2 (even with GPU), though at times using a lot of memory (up to 30G).</p>
<p>The [&quot;transformer&quot;,&quot;ner&quot;] pipeline explodes into using up to 20 cores (on a 40 logical core machine), so I would expect it to run fast. But it appears to run forever. In an hour I only get the first 'epoch' completed, then (on the next epoch) it crashes (see below). Since my data (DocBin files batching 100 'documents' each) is the same, the crash below (out-of-sequence B/I tag) is hard to explain.</p>
<p>My main question is <strong>WHY is the pipeline different when targeting GPU vs CPU</strong>? Where are the vectors in case of targeting GPU?</p>
<p>Crash:
...</p>
<pre><code> File &quot;C:\Work\ML\Spacy3\lib\site-packages\spacy\training\loop.py&quot;, line 98, in train
    for batch, info, is_best_checkpoint in training_step_iterator:
  File &quot;C:\Work\ML\Spacy3\lib\site-packages\spacy\training\loop.py&quot;, line 194, in train_while_improving
    nlp.update(
  File &quot;C:\Work\ML\Spacy3\lib\site-packages\spacy\language.py&quot;, line 1107, in update
    proc.update(examples, sgd=None, losses=losses, **component_cfg[name])
  File &quot;spacy\pipeline\transition_parser.pyx&quot;, line 350, in spacy.pipeline.transition_parser.Parser.update
  File &quot;spacy\pipeline\transition_parser.pyx&quot;, line 604, in spacy.pipeline.transition_parser.Parser._init_gold_batch
  File &quot;spacy\pipeline\_parser_internals\ner.pyx&quot;, line 273, in spacy.pipeline._parser_internals.ner.BiluoPushDown.init_gold
  File &quot;spacy\pipeline\_parser_internals\ner.pyx&quot;, line 53, in spacy.pipeline._parser_internals.ner.BiluoGold.__init__
  File &quot;spacy\pipeline\_parser_internals\ner.pyx&quot;, line 69, in spacy.pipeline._parser_internals.ner.create_gold_state
  File &quot;spacy\training\example.pyx&quot;, line 240, in spacy.training.example.Example.get_aligned_ner
  File &quot;spacy\tokens\doc.pyx&quot;, line 698, in spacy.tokens.doc.Doc.ents.__get__
ValueError: [E093] token.ent_iob values make invalid sequence: I without B
</code></pre>
","transformer-model"
"67334513","Is there an 'untrained' gpt model folder?","2021-04-30 13:09:03","","1","443","<huggingface-transformers><transformer-model><gpt-2>","<p>Crazy question maybe: but I want to download the gpt-2 model framework but I want the weights to be initialized randomly. So as if the model still has to be finetuned on the reddit content (including json, vocab, meta &amp; index files etc). Is this possible?</p>
<p>Kind regards!</p>
","transformer-model"
"67308013","Training a model on an entire dataset by dividing the dataset into chunks & loading the model back again untill all chunks of the dataset are trained","2021-04-28 21:16:47","","1","403","<pytorch><huggingface-transformers><transformer-model><pre-trained-model><nlp-question-answering>","<p>This is my first question on StackOverflow. I am working on the <strong>CUAD</strong>(Contract Understanding Atticus Dataset) which is a Q&amp;A based dataset. But training 80% of the dataset in one go is impossible due to resource constraints. I am using the boilerplate code provided by HuggingFace Transformer docs for Q&amp;A task <a href=""https://huggingface.co/transformers/examples.html"" rel=""nofollow noreferrer"">here</a>. My hands are tied with Google Colab Pro. So, it's not possible for me to use multiple GPU's in training the dataset. Inspite of using the hyperparameters below, I'm unable to avoid errors due to memory constraints like &quot;CUDA out of Memory&quot; etc.</p>
<pre><code>args = TrainingArguments(
    'cuad-roberta',
    evaluation_strategy = &quot;epoch&quot;,
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=2,
    weight_decay=0.01,
    save_steps=5000,
    logging_steps=5000,
    save_total_limit=100,
    gradient_accumulation_steps = 12,
    eval_accumulation_steps = 4,
)
</code></pre>
<p>Under these circumstances, I have divided my training set(80%) into 4 parts with each part holding 25% data. So, using any Q&amp;A supported pretrained model from Transformers, I've trained the first 25% of the training data and then saved the model in a directory of my drive. Then, I have loaded that tokenizer and model from the saved directory and trained the next 25% of my training data on the same model as shown below.</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/models/cuad-25%-roberta-base')
model = AutoModelForQuestionAnswering.from_pretrained('/content/drive/MyDrive/models/cuad-25%-roberta-base')
</code></pre>
<p>I repeated the step two more times to complete training the model on the entire training data.</p>
<p>Now, my question is that, <strong>Is this approach correct in terms of training a model when I have resource constraints? If it is correct, will this approach hurt the performance of my model?</strong> I'm relatively new to ML and NLP so please kindly consider any silly mistakes.</p>
<p>Also, any sources for understanding, visualising or implementing the Q&amp;A task through HuggingFace Transformers would be really helpful.</p>
","transformer-model"
"67299510","Understanding how gpt-2 tokenizes the strings","2021-04-28 11:38:49","67906030","1","1616","<python><huggingface-transformers><transformer-model><gpt-2>","<p>Using tutorials <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">here</a> , I wrote the following codes:</p>
<pre><code>from transformers import GPT2Tokenizer, GPT2Model
import torch

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2Model.from_pretrained('gpt2')

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state
</code></pre>
<p>So I realize that &quot;inputs&quot;, consists of tokenized items of my sentence.
But how can I get the values of tokenized items? (see for example [&quot;hello&quot;, &quot;,&quot;, &quot;my&quot;, &quot;dog&quot;, &quot;is&quot;, &quot;cute&quot;])</p>
<p>I am asking this because sometimes I think it separetes a word if that word is not in its dictionary (i.e., a word from another language). So I want to check that in my codes.</p>
","transformer-model"
"67237770","Transformer with reinforcement learning","2021-04-23 23:07:52","","1","342","<reinforcement-learning><transformer-model>","<p>I am training a Sequence to Sequence transformer model with RL and both the actor and critic have to be transfomers. But i dont see how i can have one output from the critic when a transformer outputs a sequence rather than just a single scalar reward.</p>
<p>I am using the pretrained t5 from huggingface.</p>
<p>The code for it is here:</p>
<p><a href=""https://huggingface.co/transformers/_modules/transformers/modeling_t5.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/_modules/transformers/modeling_t5.html</a></p>
","transformer-model"
"67193248","Can Anyone please help me in finding the solution to the error ""AttributeError: 'Example' object has no attribute 'src_len'""?","2021-04-21 09:44:05","","1","457","<python><transformer-model><machine-translation>","<p>I am working on a project which aims to make transformer neural network for machine translation using pytorch.
I am facing the error which says &quot;AttributeError: 'Example' object has no attribute 'src_len'&quot;..
Any help would be appreciated.
Thank you</p>
<pre><code>fields = {&quot;English&quot;: (&quot;eng&quot;, english), &quot;Urdu&quot;: (&quot;ur&quot;, urdu)}

train_data, test_data,valid_data= TabularDataset.splits(
path=&quot;&quot;, train=&quot;train.json&quot;, test=&quot;test.json&quot;,validation=&quot;val.json&quot;, format=&quot;json&quot;, fields=fields
)

english.build_vocab(train_data, max_size=10000, min_freq=2)
urdu.build_vocab(train_data, max_size=10000, min_freq=2)

train_iterator, valid_iterator, test_iterator = BucketIterator.splits(
(train_data, valid_data, test_data),
batch_size=32,
sort_within_batch=True,
sort_key=lambda x: (x.src_len),
device='cuda',
)

for batch in train_iterator:
print(batch) 
</code></pre>
<p>Error Message:</p>
<pre><code>AttributeError                            
Traceback (most recent call last)
&lt;ipython-input-27-9fe6d4873776&gt; in &lt;module&gt;()
----&gt; 1 for batch in train_iterator:
  2   print(batch)
  2 frames
&lt;ipython-input-26-1c01ecf39930&gt; in &lt;lambda&gt;(x)
   4     batch_size=32,
   5     sort_within_batch=True,
----&gt; 6     sort_key=lambda x: (x.src_len),
   7     device='cuda',
   8 )

   AttributeError: 'Example' object has no attribute 'src_len'`
</code></pre>
","transformer-model"
"67103062","Customizing TensorFlow Serving for BERT or Transformer","2021-04-15 06:05:50","","1","122","<tensorflow><tensorflow2.0><tensorflow-serving><bert-language-model><transformer-model>","<p>I would like to customize serving_default of BERT or Transformer to make the input as string type and get the prediction as string type. So the model's output should be converted to a sentence according to a dictionary. Are there some reference code or examples for this custom serving? I would appreciate your answers and please share your ideas.</p>
","transformer-model"
"67089849","AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'","2021-04-14 10:20:20","67089951","4","12669","<tokenize><huggingface-transformers><transformer-model><huggingface-tokenizers><gpt-2>","<p>I am just using the huggingface transformer library and get the following message when running run_lm_finetuning.py: AttributeError: 'GPT2TokenizerFast' object has no attribute 'max_len'. Anyone else with this problem or an idea how to fix it? Thanks!</p>
<p>My full experiment run:
mkdir experiments</p>
<p>for epoch in 5
do
python run_lm_finetuning.py <br />
--model_name_or_path distilgpt2 <br />
--model_type gpt2 <br />
--train_data_file small_dataset_train_preprocessed.txt <br />
--output_dir experiments/epochs_$epoch <br />
--do_train <br />
--overwrite_output_dir <br />
--per_device_train_batch_size 4 <br />
--num_train_epochs $epoch
done</p>
","transformer-model"
"67058277","Understanding repository gpt transformer","2021-04-12 12:21:59","","0","93","<github><nlp><transformer-model><gpt-2>","<p>For my project I need to understand and being able to execute <a href=""https://github.com/atcbosselut/comet-commonsense"" rel=""nofollow noreferrer"">this</a> github repository about commonsense generation using the GPT transformer language model. It is quite extensive and I don't have enough programming experience to make sense of it all. Is there anyone who is good with these subjects who can guide me through it/help me?</p>
<p>Or, is there another spot where I can post this question?</p>
","transformer-model"
"67047188","need a ""bag of words"" type of transformer","2021-04-11 16:01:43","","2","772","<nlp><word2vec><huggingface-transformers><transformer-model>","<p>I have a NLP project where a collection of words are encoded currently by <code>w2v</code>, to compare to other collections of words. I'd like to try <code>transformers</code> which could give a better encoding than <code>w2v</code>. However, due to the nature of the data, I won't need positional encoding at all (due to the fact that the collection of words have no order). <code>Is there a pretrained transformer that won't do positional encoding</code>?</p>
","transformer-model"
"66921933","tgt and src have to have equal features for a Transformer Network in Pytorch","2021-04-02 15:55:05","","2","2968","<machine-learning><pytorch><transformer-model><attention-model>","<p>I am attempting to train EEG data through a transformer network. The input dimensions are 50x16684x60 (seq x batch x features) and the output is 16684x2. Right now I am simply trying to run a basic transformer, and I keep getting an error telling me</p>
<pre><code>RuntimeError: the feature number of src and tgt must be equal to d_model
</code></pre>
<p>Why would the source and target feature number ever be equal? Is it possible to run such a dataset through a transformer?</p>
<p>Here is my basic model:</p>
<pre><code>input_size = 60 # seq x batch x features
hidden_size = 32
num_classes = 2
learning_rate = 0.001
batch_size = 64
num_epochs = 2
sequence_length = 50
num_layers = 2
dropout = 0.5

class Transformer(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(Transformer, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.transformer = nn.Transformer(60, 2)
        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)
    
    def forward(self, x, y):
        
        # Forward Propogation
        out, _ = self.transformer(x,y)
        out = out.reshape(out.shape[0], -1)
        out = self.fc(out)
        return out

model = Transformer(input_size, hidden_size, num_layers, num_classes)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

for epoch in range(num_epochs):
    for index in tqdm(range(16684)):
        X, y = (X_train[index], Y_train[index])
        print(X.shape, y.shape)
    
        output = model(X, y)

        loss = criterion(output, y)
        
        model.zero_grad()
        loss.backward()
        
        optimizer.step()
        
        if index % 500 == 0:
            print(f&quot;Epoch {epoch}, Batch: {index}, Loss: {loss}&quot;)

</code></pre>
","transformer-model"
"66909773","cannot import 'AutoModelForSequenceClassification' from 'transformers'","2021-04-01 18:26:56","","0","3990","<nlp><huggingface-transformers><transformer-model>","<p><strong>cannot import 'AutoModelForSequenceClassification' from 'transformers'</strong></p>
<p>The code is</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

t = AutoTokenizer.from_pretrained('/some/directory')
m = AutoModelForSequenceClassification.from_pretrained('/some/directory')
c2 = pipeline(task = 'sentiment-analysis', model=m, tokenizer=t)
</code></pre>
<p>The error is</p>
<pre><code>cannot import 'AutoModelForSequenceClassification' from 'transformers'
</code></pre>
","transformer-model"
"66867213","PEGASUS pre-training for summarisation tasks","2021-03-30 08:20:57","","2","269","<nlp><huggingface-transformers><transformer-model><summarization><huggingface-tokenizers>","<p>I am unsure of how the evaluation for large document summarisation is conducted for the recently introduced <a href=""http://scholar.google.com.sg/scholar_url?url=http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf&amp;hl=en&amp;sa=X&amp;ei=It5iYPzDPKuM6rQP-6OsuAc&amp;scisig=AAGBfm3TMc7EP3WzoI9nibV1Z0HLzzeb2w&amp;nossl=1&amp;oi=scholarr"" rel=""nofollow noreferrer"">PEGASUS model</a> for single document summarisation.</p>
<p>The author's show evaluation against large document datasets like Big Patent, PubMed etc with document lengths exceeding that of the input size to the transformer models.</p>
<p>To quote from the paper, they did talk about this but didn't really elaborate further.</p>
<blockquote>
<p>CNN/DailyMail, Multi-News, arXiv, PubMed, BIG- PATENT datasets contain input documents longer than the maximum input length (<code>L_input = 512 tokens</code>) in pre- training. This would present a problem for position em- beddings which would never be updated for longer input lengths, but we confirm the postulation that sinusoidal po- sitional encodings (Vaswani et al., 2017) generalize well when fine-tuning PEGASUSLARGE beyond the input lengths observed in training up to <code>L_input = 1024 tokens</code>. Since average input length in BIGPATENT, arXiv, PubMed and Multi-News are well beyond 1024 tokens, further scaling up <code>L_input</code> or applying a two-stage approach (Liu et al., 2018) may improve performance even more, although this is out- side the scope of this work.</p>
</blockquote>
<p>They did mention that the input length is up till 1024 tokens. In the PEGASUS Large model on huggingface the max input tokens is also 1024.</p>
<p>I am not sure how they managed to extend their document summarisations for more than 1024 tokens.</p>
<p>I would also like to do similar for my own long document summarisations that I want to try.</p>
","transformer-model"
"66821321","BERT: Weights of input embeddings as part of the Masked Language Model","2021-03-26 17:03:32","67065742","1","1401","<nlp><pytorch><bert-language-model><transformer-model><language-model>","<p>I looked through different implementations of BERT's Masked Language Model.
For pre-training there are <strong>two</strong> common versions:</p>
<ol>
<li>Decoder would simply take the final embedding of the [MASK]ed token and pass it throught a linear layer (without any modifications):</li>
</ol>
<pre><code>    class LMPrediction(nn.Module):
        def __init__(self, hidden_size, vocab_size):
            super().__init__()
            self.decoder = nn.Linear(hidden_size, vocab_size, bias = False)
            self.bias = nn.Parameter(torch.zeros(vocab_size))
            self.decoder.bias = self.bias
        def forward(self, x):
             return self.decoder(x)
</code></pre>
<ol start=""2"">
<li>Some implementations would use the weights of the input embeddings as weights of the decoder-linear-layer:</li>
</ol>
<pre><code>    class LMPrediction(nn.Module):
        def __init__(self, hidden_size, vocab_size, embeddings):
            super().__init__()
            self.decoder = nn.Linear(hidden_size, vocab_size, bias = False)
            self.bias = nn.Parameter(torch.zeros(vocab_size))
            self.decoder.weight = embeddings.weight ## &lt;- THIS LINE
            self.decoder.bias = self.bias
        def forward(self, x):
             return self.decoder(x)
</code></pre>
<p>Which one is correct? Mostly, I see the first implementation. However, the second one makes sense as well - but I cannot find it mentioned in any papers (I would like to see if the second version is somehow superior to the first one)</p>
","transformer-model"
"66820943","How to get word embeddings from the pretrained transformers","2021-03-26 16:39:46","66867147","0","1773","<python><tensorflow><huggingface-transformers><transfer-learning><transformer-model>","<p>I am working on a word-level classification task on multilingual data, I am using XLM-R, I know that XLM-R uses <code>sentencepiece</code>  as tokenizers which sometimes tokenizes words into subword.</p>
<blockquote>
<p>For example the sentence &quot;deception master&quot; is tokenized as <code>de</code> <code>ception</code> <code>master</code>, the word deception has been tokenized into two sub-words.</p>
</blockquote>
<p>How can I get the embedding of <code>deception</code>. I can take the mean of the subwords to get the embedding of the word as done <a href=""https://github.com/uhh-lt/bert-sense/blob/bfecb3c0e677d36ccfab4e2131ef9183995efaef/BERT_Model.py#L342"" rel=""nofollow noreferrer"">here</a>.  But I have to implement my code in TensorFlow and TensorFlow computational graph doesn't support NumPy.</p>
<p>I could store the final hidden embeddings after taking the mean of the subwords into a NumPy array and give this array as input to the model, but I want to fine-tune the transformer.</p>
<p>How to get the word embeddings from the sub-word embeddings given by the transformer</p>
","transformer-model"
"66722287","A language model with only one embedding layer in both encode and decode only predict <eos>","2021-03-20 13:42:43","","0","66","<nlp><pytorch><huggingface-transformers><transformer-model>","<p>I'm trying to make the model predict a word from a sentence using pretrained Huggingface's BERT as feature extractor. The model look like this</p>
<pre><code>class BertAutoEncoder(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        decoder_layer = nn.TransformerDecoderLayer(768, 2, 1024, dropout=0.1)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, 2)
        self.fc = nn.Linear(768, vocab_size)

    def forward(self, memory, embedded_word):
        output = self.transformer_decoder(embedded_word, memory)
        output = self.fc(output)
        return output
</code></pre>
<p>And when train/evaluate I call the model like this</p>
<pre><code>bert = BertModel.from_pretrained('bert-base-uncased')
bert.requires_grad_(False)
...
memory = bert(**src).last_hidden_state.transpose(0, 1)
embeded_word = bert.embeddings(trg.data['input_ids'][:, :-1], token_type_ids=trg.data['token_type_ids'][:, :-1]).transpose(0, 1)
output = model(memory, embeded_word)
</code></pre>
<p>The loss reduced nicely but turned out the model only predict <code>&lt;eos&gt;</code> token.</p>
<p>I tried train the model with 1 batch of 32 samples and it did work when loss reduced pass <code>8e-6</code> but when I trained it with all data the loss could go way beyond that but none of the saved models work. Even the one with eval or train loss around <code>4e-6</code> - <code>8e-6</code>.</p>
<p>Surprisingly the model would work if I use a separate decoder's Embedding like this</p>
<pre><code>class BertAutoEncoderOld(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        decoder_layer = nn.TransformerDecoderLayer(768, 2, 1024, dropout=0.1)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, 2)
        self.decoder = nn.Embedding(vocab_size, 768)
        self.pos_decoder = PositionalEncoding(768, 0.5)
        self.fc = nn.Linear(768, vocab_size)

    def forward(self, memory, word):
        tgt = self.decoder(word.data['input_ids'][:, :-1].transpose(0, 1))
        tgt = self.pos_decoder(tgt)
        output = self.transformer_decoder(tgt, memory)
        output = self.fc(output)
        return output
</code></pre>
<p>But I was asked to make it work with one Embedding and I have no idea how.</p>
<p>I tried</p>
<ul>
<li>Reduce/increase batch from 32 to 8-64</li>
<li>Also tried 2 and 1024 batch size</li>
<li>Remove <code>&lt;eos&gt;</code> token and change it's attention mask to 0</li>
</ul>
<p>But none of those work.</p>
<p>What did I do wrong and how to fix it?</p>
<p>Thanks</p>
<h1>Edit per @emily qeustion</h1>
<p>I change the data itself in collate function</p>
<pre><code>text.data['attention_mask'][text.data['input_ids'] == 102] = 0
text.data['input_ids'][text.data['input_ids'] == 102] = 0
word.data['attention_mask'][word.data['input_ids'] == 102] = 0
word.data['input_ids'][word.data['input_ids'] == 102] = 0
</code></pre>
<p>It only used in Bert though.</p>
","transformer-model"
"66666525","How to map token indices from the SQuAD data to tokens from BERT tokenizer?","2021-03-17 03:21:42","","3","4320","<bert-language-model><transformer-model><nlp-question-answering><huggingface-tokenizers><squad>","<p>I am using the SQuaD dataset for answer span selection. After using the <code>BertTokenizer</code> to tokenize the passages, for some samples, the start and end indices of the answer don't match the real answer span position in the passage tokens anymore. How to solve this problem? One way is to modify the answer indices (also the training targets) accordingly? But how to do it?</p>
","transformer-model"
"66656622","Python ImportError: cannot import name 'version' from 'packaging' (transformers)","2021-03-16 13:54:25","66661028","1","4454","<python><bert-language-model><huggingface-transformers><transformer-model>","<p>when I'm trying to simply <code>import transformers</code> I receive this error:</p>
<p>ImportError: cannot import name 'version' from 'packaging' (C:\Users\miria\packaging.py)</p>
<p>Can anyone help me solve this?</p>
<p><a href=""https://i.sstatic.net/RHMPL.png"" rel=""nofollow noreferrer"">traceback</a></p>
","transformer-model"
"66633664","label_smoothing in ""The Annotated Transformer""","2021-03-15 07:08:52","","1","122","<nlp><transformer-model>","<p>In <a href=""http://nlp.seas.harvard.edu/2018/04/03/attention.html"" rel=""nofollow noreferrer"">&quot;The Annotated Transformer&quot;</a>, label smoothing is implemented as the following:</p>
<pre><code>class LabelSmoothing(nn.Module):
    &quot;Implement label smoothing.&quot;
    def __init__(self, size, padding_idx, smoothing=0.0):
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(size_average=False)
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None

    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() &gt; 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        return self.criterion(x, Variable(true_dist, requires_grad=False))

</code></pre>
<p>In particular, why it is</p>
<pre><code>true_dist.fill_(self.smoothing / (self.size - 2))
</code></pre>
<p>Where does the 2 in <code>self.size - 2</code> come from?</p>
<p>And what is the purpose of <code>true_dist[:, self.padding_idx] = 0</code></p>
","transformer-model"
"66596142","BertModel or BertForPreTraining","2021-03-12 07:53:12","66597745","4","5055","<deep-learning><nlp><bert-language-model><huggingface-transformers><transformer-model>","<p>I want to use Bert only for embedding and use the Bert output as an input for a classification net that I will build from scratch.</p>
<p>I am not sure if I want to do finetuning for the model.</p>
<p>I think the relevant classes are BertModel or BertForPreTraining.</p>
<p><a href=""https://dejanbatanjac.github.io/bert-word-predicting/"" rel=""nofollow noreferrer"">BertForPreTraining</a>  head contains two &quot;actions&quot;:
self.predictions is MLM (Masked Language Modeling) head is what gives BERT the power to fix the grammar errors, and self.seq_relationship is NSP (Next Sentence Prediction); usually refereed as the classification head.</p>
<pre><code>class BertPreTrainingHeads(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.predictions = BertLMPredictionHead(config)
        self.seq_relationship = nn.Linear(config.hidden_size, 2)
</code></pre>
<p>I think the NSP isn't relevant for my task so I can &quot;override&quot; it.
what does the MLM do and is it relevant for my goal or should I use the BertModel?</p>
","transformer-model"
"66551537","RuntimeError: Unimplemented: DNN library is not found","2021-03-09 17:05:42","","1","3453","<python-3.x><tensorflow><deep-learning><transformer-model><jax>","<p>I was trying to implement vision transformer by google and I came across this error during inference:</p>
<pre><code>RuntimeError: Unimplemented: DNN library is not found.: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well).
</code></pre>
<p>I followed <a href=""https://github.com/google/jax/issues/4920"" rel=""nofollow noreferrer"">this</a> post but it didn't help. What should I do?</p>
","transformer-model"
"66542873","How to let the tar not be None?","2021-03-09 07:56:15","","0","44","<python><transformer-model><tensorflow2>","<blockquote>
<p><em>I don't know why does the last line show that the tar is 0 and its shape is (None, None).<br />
However, the second line in the end shows that the tar's shape is (64, 27).<br />
How to let the tar not be None?</em></p>
</blockquote>
<pre><code>train_step_signature = [
    tf.TensorSpec(shape=(None, None), dtype=tf.int64),
    tf.TensorSpec(shape=(None, None), dtype=tf.int64),
    ]

@tf.function(input_signature=train_step_signature)
def train_step(inp, tar):
    print(tar)
    
for epoch in range(EPOCHS):
    start = time.time()
  
    train_loss.reset_states()
    train_accuracy.reset_states()

    # inp -&gt; portuguese, tar -&gt; english
    for (batch, (inp, tar)) in enumerate(train_dataset):

        print(tar)
        train_step(inp, tar)
        break
</code></pre>
<blockquote>
<p><em><strong>The ouput is:</strong></em></p>
</blockquote>
<pre><code>tf.Tensor(  
[[1942  777 1186 ...    0    0    0]  
 [1942   22  164 ...    0    0    0]  
 [1942    1  410 ...    0    0    0]  
 ...   
 [1942  824  895 ...    0    0    0]  
 [1942  393  356 ...    0    0    0]  
 [1942 1518 1209 ...    0    0    0]], shape=(64, 27), dtype=int64)  
 Tensor(&quot;tar:0&quot;, shape=(None, None), dtype=int64)
</code></pre>
<blockquote>
<p><em>When adding two lines in ninth line, I get the wrong sentences:<br />
<strong>TypeError: unsupported operand type(s) for -: 'NoneType' and 'int'</strong><br />
How to solve this problem?</em></p>
</blockquote>
<pre><code>train_step_signature = [
    tf.TensorSpec(shape=(None, None), dtype=tf.int64),
    tf.TensorSpec(shape=(None, None), dtype=tf.int64),
    ]

@tf.function(input_signature=train_step_signature)
def train_step(inp, tar):
    print(tar)
    img=tf.pad(tar[:, 0:2], [[0, 0], [0, tar.shape[1]-2]])
    print(img）

for epoch in range(EPOCHS):
    start = time.time()
  
    train_loss.reset_states()
    train_accuracy.reset_states()

    # inp -&gt; portuguese, tar -&gt; english
    for (batch, (inp, tar)) in enumerate(train_dataset):

        print(tar)
        train_step(inp, tar)
        break
</code></pre>
","transformer-model"
"66518375","How is transformers loss calculated for blank token predictions?","2021-03-07 15:51:16","66526823","3","1580","<machine-learning><nlp><transformer-model><language-model>","<p>I'm currently trying to implement a transformer and have trouble understanding its loss calculation.</p>
<p>My encoders input looks for batch_size=1  and max_sentence_length=8 like:</p>
<pre><code>[[Das, Wetter, ist, gut, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>My decoders input looks like (german to english):</p>
<pre><code>[[&lt;start&gt;, The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>Let's say my transformer predicted those class probabilities (only showing the word for the class with the highest class probability):</p>
<pre><code>[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>Now I calculate the loss using:</p>
<pre><code>loss = categorical_crossentropy(
   [[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]],
   [[The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
)
</code></pre>
<p>Is this the correct way to calculate the loss? My transformer always predicts the blank token for the next word and I thought that's because I have a mistake in my loss calculation and have to do something with the blank tokens before calculating the loss.</p>
","transformer-model"
"66514811","Input dimension of MultiheadAttention varies between Pytorch and Tensorflow ""Transformer Model for language understanding""","2021-03-07 09:08:53","","1","274","<tensorflow><keras><deep-learning><pytorch><transformer-model>","<p>In Pytorch's implementation of MultiheadAttention, TransformerEncoder, the Batch size is in the middle. That is (L,N,E) for query dimension.
But in Tensorflow's  &quot;Transformer Model for language understanding&quot; article, the query dimension is (batch_size,seq_len,depth). That batch_size is at 0th index.
Why this differs?</p>
","transformer-model"
"66440065","Transformer Autoencoder with low loss and good accuracy can't reconstruct song without teacher forcing","2021-03-02 13:10:08","","2","437","<python><machine-learning><pytorch><generative-adversarial-network><transformer-model>","<p>I am working on an Adversarial Autoencoder with Compressive Transformer for music generation and interpolation.
The input for the decoder is a sequence of 8 bars, where each bars is made by 200 tokens.
Each bar has 4 tracks which are respectively: drums, bass, guitar and strings.
Each note is represented with a sequence <code>&lt;starting time&gt; &lt;note pitch&gt; &lt;note duration&gt;</code>.
The compressed representation is the first 16 latent of the encoder output on the last bar (the  8th one), which should capture information about all the 8 provided bars thanks to the memories of the Compressive Transformer.
The training is done with the WGAN-GP loss.</p>
<p>The encoder output distribution matches the prior distribution (a mixture of 4 gaussians), the accuracy reaches the value of 0.6 and the loss decrases well:</p>
<p><a href=""https://i.sstatic.net/B1iBx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B1iBx.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/f3OCg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/f3OCg.png"" alt=""enter image description here"" /></a></p>
<p>When I try to sample from the distribution, the song generated is kinda weird:</p>
<p><a href=""https://drive.google.com/file/d/13etvgnCzfTAfFY8YUOTEXWSEmYler-sX/view?usp=sharing"" rel=""nofollow noreferrer"">generated song</a></p>
<p>But the problem is when I try to reconstruct a song:</p>
<p><a href=""https://drive.google.com/file/d/1JeMvSTHJQlIuFQ3Ws2v-R3Ib4ZyM24ac/view?usp=sharing"" rel=""nofollow noreferrer"">original</a></p>
<p><a href=""https://drive.google.com/file/d/1tP4yP-NUhNNfSH5orRa1oyMH58ZOJscz/view?usp=sharing"" rel=""nofollow noreferrer"">reconstructed</a></p>
<p>I think that the problem is releated to what is discussed <a href=""https://forums.fast.ai/t/cheating-in-transformer-notebook/67219"" rel=""nofollow noreferrer"">here</a>: at training time, when the decoder needs to reconstruct the token in position k, the previous &lt;k token are the right ones because of the teacher forcing, while at testing time the previous token are feed in an autoregressive way and, because not all of them are the right ones, this confuses the Decoder.
The code for Greedy Decoding is the following:</p>
<pre><code>    def greedy_decode(self, latent, n_bars, desc):
        _, _, d_mems, d_cmems = get_memories(n_batch=1)
        outs = []
        for _ in tqdm(range(n_bars), position=0, leave=True, desc=desc):
            trg = np.full((4, 1, 1), config[&quot;tokens&quot;][&quot;sos&quot;])
            trg = torch.LongTensor(trg).to(config[&quot;train&quot;][&quot;device&quot;])
            for _ in range(config[&quot;model&quot;][&quot;seq_len&quot;] - 1):  # for each token of each bar
                trg_mask = create_trg_mask(trg.cpu().numpy())
                out, _, _, _, _, _ = self.decoder(trg, trg_mask, None, latent, d_mems, d_cmems)
                out = torch.max(out, dim=-2).indices
                out = out.permute(2, 0, 1)
                trg = torch.cat((trg, out[..., -1:]), dim=-1)
            trg_mask = create_trg_mask(trg.cpu().numpy())
            out, _, _, d_mems, d_cmems, _ = self.decoder(trg, trg_mask, None, latent, d_mems, d_cmems)
            out = torch.max(out, dim=-2).indices
            out = out.permute(2, 0, 1)
            outs.append(out)
        return outs
</code></pre>
<p>I also tried Beam Search, but it does not seem to solve the problem.
Now I am trying to use the previous generated token instead to the true one with a probability of 1/2, but the training is very slow.
Does anyone know a way to teach the transformer to be not too dependent on teacher forcing?</p>
","transformer-model"
"66389707","Why embed dimemsion must be divisible by num of heads in MultiheadAttention?","2021-02-26 16:45:13","74993496","16","5787","<python-3.x><pytorch><transformer-model><attention-model>","<p>I am learning the Transformer. Here is the pytorch document for <a href=""https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention"" rel=""noreferrer"">MultiheadAttention</a>. In their <a href=""https://github.com/pytorch/pytorch/blob/7a178a8a523d4653a3a2fa10c573b71e7fab1b9a/torch/nn/modules/activation.py#L874"" rel=""noreferrer"">implementation</a>, I saw there is a constraint:</p>
<pre><code> assert self.head_dim * num_heads == self.embed_dim, &quot;embed_dim must be divisible by num_heads&quot;
</code></pre>
<p>Why require the constraint: <code>embed_dim must be divisible by num_heads?</code>  If we go back to the equation</p>
<p><a href=""https://i.sstatic.net/T0niJ.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/T0niJ.png"" alt=""MultiHead(Q,K,V)=Concat(head1​,…,headh​)WOwhereheadi​=Attention(QWiQ​,KWiK​,VWiV​)"" /></a></p>
<p>Assume:
<code>Q</code>, <code>K</code>,<code>V</code> are <code>n x emded_dim</code> matrices; all the weight matrices <code>W</code> is <code>emded_dim x head_dim</code>,</p>
<p>Then, the concat <code>[head_i, ..., head_h]</code> will be a <code>n x (num_heads*head_dim)</code> matrix;</p>
<p><code>W^O</code> with size <code>(num_heads*head_dim) x embed_dim</code></p>
<p><code>[head_i, ..., head_h] * W^O</code> will become a <code>n x embed_dim</code> output</p>
<p>I don't know why we require <code>embed_dim must be divisible by num_heads</code>.</p>
<p>Let say we have <code>num_heads=10000</code>, the resuts are the same, since the matrix-matrix product will absort this information.</p>
","transformer-model"
"66309579","Spark The Definitive Guide: Chapter 25 - Preprocessing and Feature Engineering","2021-02-22 03:11:57","66339011","0","84","<apache-spark><pyspark><transformer-model>","<p>I do not understand when to use both 'fit' and 'transform' versus when to use 'transform' only.</p>
<p>The following transformers use both fit and transform:</p>
<ul>
<li>Rformula</li>
<li>QuantileDiscretizer</li>
<li>StandardScaler</li>
<li>MinMaxScaler</li>
<li>MaxAbsScaler</li>
<li>StringIndexer</li>
<li>VectorIndexer</li>
<li>CountVectorizer</li>
<li>PCA</li>
<li>ChiSqSelector</li>
</ul>
<p>The following transformers only use transform:</p>
<ul>
<li>SQLTransformer</li>
<li>VectorAssembler</li>
<li>Bucketizer</li>
<li>ElementWiseProduct</li>
<li>Normalizer</li>
<li>IndexToString</li>
<li>OneHotEncoder</li>
<li>Tokenizer</li>
<li>RegexTokenizer</li>
<li>StopWordsRemover</li>
<li>NGram</li>
</ul>
<p>I don't understand intuitively when to use both fit and transform versus when to use transform only.</p>
<p>Kindly explain.  Thanks.</p>
","transformer-model"
"66301608","How to build a dataset from a large text file without getting a memory error?","2021-02-21 11:06:02","","1","701","<python><memory><pytorch><out-of-memory><transformer-model>","<p>I have a text file with size &gt; 7.02 GB. I have already built a tokenizer based on this text file.  I want to build a dataset like so:</p>
<pre><code>from transformers import LineByLineTextDataset

dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=&quot;data.txt&quot;, block_size=128,)
</code></pre>
<p>Since the size of my data is very large, a memory error occurs. This is the source code:</p>
<pre><code>with open(file_path, encoding=&quot;utf-8&quot;) as f:
        lines = [line for line in f.read().splitlines() if (len(line) &gt; 0 and not line.isspace())]

    batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)
    print(batch_encoding)
    self.examples = batch_encoding[&quot;input_ids&quot;]
    self.examples = [{&quot;input_ids&quot;: torch.tensor(e, dtype=torch.long)} for e in self.examples]
</code></pre>
<p>Supposing that my text file has only 4 lines, the following will be printed:</p>
<pre><code>{'input_ids': [[49, 93, 1136, 1685, 973, 363, 72, 3130, 16502, 18], [44, 73, 1685, 279, 7982, 18, 225], [56, 13005, 1685, 4511, 3450, 18], [56, 19030, 1685, 7544, 18]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]}
</code></pre>
<p>I have changed the source code as the following so that the memory error doesn't appear:</p>
<pre><code>for line in open(file_path, encoding=&quot;utf-8&quot;):
        if (len(line) &gt; 0 and not line.isspace()):
            new_line = line.split()

            batch_encoding = tokenizer(new_line, add_special_tokens=True, truncation=True, max_length=block_size)
            print(batch_encoding)
            print(type(batch_encoding))
            self.examples = batch_encoding[&quot;input_ids&quot;]
            self.examples = [{&quot;input_ids&quot;: torch.tensor(e, dtype=torch.long)} for e in self.examples]
print(batch_encoding)
</code></pre>
<p>However, the following will be printed:</p>
<pre><code>{'input_ids': [[49, 93], [3074], [329], [2451, 363, 72, 3130, 16502, 18]], 'token_type_ids': [[0, 0], [0], [0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1], [1], [1], [1, 1, 1, 1, 1, 1]]}
&lt;class 'transformers.tokenization_utils_base.BatchEncoding'&gt;
{'input_ids': [[44, 73], [329], [69], [23788, 18]], 'token_type_ids': [[0, 0], [0], [0], [0, 0]], 'attention_mask': [[1, 1], [1], [1], [1, 1]]}
&lt;class 'transformers.tokenization_utils_base.BatchEncoding'&gt;
{'input_ids': [[56, 13005], [329], [7522], [7958, 18]], 'token_type_ids': [[0, 0], [0], [0], [0, 0]], 'attention_mask': [[1, 1], [1], [1], [1, 1]]}
&lt;class 'transformers.tokenization_utils_base.BatchEncoding'&gt;
{'input_ids': [[56, 19030], [329], [11639, 18]], 'token_type_ids': [[0, 0], [0], [0, 0]], 'attention_mask': [[1, 1], [1], [1, 1]]}
{'input_ids': [[56, 19030], [329], [11639, 18]], 'token_type_ids': [[0, 0], [0], [0, 0]], 'attention_mask': [[1, 1], [1], [1, 1]]}
</code></pre>
<p>How can I change the source code in order to be able to read the large text file line by line but get the same output as desired without a memory error?</p>
","transformer-model"
"66300671","How to handle memory error while building a dataset","2021-02-21 09:02:49","","0","285","<python><dataset><transformer-model>","<p>I have a text file with a size of more than 7.02 GB. I have already built a tokenizer based on this text file:</p>
<pre><code>from tokenizers.implementations import ByteLevelBPETokenizer
tokenizer = ByteLevelBPETokenizer(
    &quot;tokenizer model/vocab.json&quot;,
    &quot;tokenizer model/merges.txt&quot;,
)
</code></pre>
<p>I want to build a dataset as the following:</p>
<pre><code>from transformers import LineByLineTextDataset

dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=&quot;data.txt&quot;, block_size=128,)
</code></pre>
<p>Since the size of my data is very large, a memory error will appear.
This is the source code:</p>
<pre><code>with open(file_path, encoding=&quot;utf-8&quot;) as f:
        lines = [line for line in f.read().splitlines() if (len(line) &gt; 0 and not line.isspace())]

    batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)
    self.examples = batch_encoding[&quot;input_ids&quot;]
    self.examples = [{&quot;input_ids&quot;: torch.tensor(e, dtype=torch.long)} for e in self.examples]
</code></pre>
<p>For this reason, since the whole data can't be read at the same time and the lines can't be written to a list as the list gets so big that the program can't handle it, I have changed the source code in the following way:</p>
<pre><code>for line in open(file_path, encoding=&quot;utf-8&quot;):
        if (len(line) &gt; 0 and not line.isspace()):
            new_line = line.split()

            batch_encoding = tokenizer(new_line, add_special_tokens=True, truncation=True, max_length=block_size)
            self.examples = batch_encoding[&quot;input_ids&quot;]
            self.examples = [{&quot;input_ids&quot;: torch.tensor(e, dtype=torch.long)} for e in self.examples]
</code></pre>
<p>However, it gives me the following error:</p>
<pre><code>Traceback (most recent call last):
  File dfgd.py&quot;, line 21, in &lt;module&gt;
    file_path=&quot;data.txt&quot;, block_size=128, #max_length = int(1e30)
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\data\datasets\language_modeling.py&quot;, line 140, in __init__
    batch_encoding = tokenizer(new_line, add_special_tokens=True, truncation=True, max_length=block_size)
TypeError: 'ByteLevelBPETokenizer' object is not callable
</code></pre>
<p>How can I handle this problem?</p>
","transformer-model"
"66294076","How to determine the block size in training a dataset","2021-02-20 16:45:51","66294428","3","3417","<python><transformer-model>","<p>I want to build a training dataset by applying a previously trained tokenizer to my text file. The size of my text file is 7.02 GB (7,543,648,706 bytes). This is what I have written:</p>
<pre><code>from transformers import LineByLineTextDataset

dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=&quot;data.txt&quot;, block_size = ???
)
</code></pre>
<p>What does &quot;block size&quot; exactly mean here? How can I determine its value?</p>
","transformer-model"
"66293355","OSError: Can't load tokenizer","2021-02-20 15:33:21","66293832","4","11501","<python><transformer-model><huggingface-tokenizers>","<p>I want to train an XLNET language model from scratch. First, I have trained a tokenizer as follows:</p>
<pre><code>from tokenizers import ByteLevelBPETokenizer

# Initialize a tokenizer
tokenizer = ByteLevelBPETokenizer()
# Customize training
tokenizer.train(files='data.txt', min_frequency=2, special_tokens=[ #defualt vocab size
    &quot;&lt;s&gt;&quot;,
    &quot;&lt;pad&gt;&quot;,
    &quot;&lt;/s&gt;&quot;,
    &quot;&lt;unk&gt;&quot;,
    &quot;&lt;mask&gt;&quot;,
])
tokenizer.save_model(&quot;tokenizer model&quot;)
</code></pre>
<p>Finally, I will have two files in the given directory:</p>
<pre><code>merges.txt
vocab.json
</code></pre>
<p>I have defined the following config for the model:</p>
<pre><code>from transformers import XLNetConfig, XLNetModel
config = XLNetConfig()
</code></pre>
<p>Now, I want to recreate my tokenizer in transformers:</p>
<pre><code>from transformers import XLNetTokenizerFast

tokenizer = XLNetTokenizerFast.from_pretrained(&quot;tokenizer model&quot;)
</code></pre>
<p>However, the following error appears:</p>
<pre><code>File &quot;dfgd.py&quot;, line 8, in &lt;module&gt;
    tokenizer = XLNetTokenizerFast.from_pretrained(&quot;tokenizer model&quot;)
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\tokenization_utils_base.py&quot;, line 1777, in from_pretrained
    raise EnvironmentError(msg)
OSError: Can't load tokenizer for 'tokenizer model'. Make sure that:

- 'tokenizer model' is a correct model identifier listed on 'https://huggingface.co/models'

- or 'tokenizer model' is the correct path to a directory containing relevant tokenizer files
</code></pre>
<p>What should I do?</p>
","transformer-model"
"66286196","How to insert information on a word embedding?","2021-02-19 23:09:36","","0","74","<nlp><embedding><word-embedding><transformer-model>","<p>I saw that transformers inserted the positional embedding of a word on its word embedding. How can I insert another things like te word size for example? Is it possible to view the internal code of a transformer framework like BERT for example to edit something like that?</p>
","transformer-model"
"66244123","Why use multi-headed attention in Transformers?","2021-02-17 14:38:34","66259806","10","4208","<nlp><transformer-model><attention-model>","<p>I am trying to understand why transformers use multiple attention heads. I found the following <a href=""https://towardsdatascience.com/simple-explanation-of-transformers-in-nlp-da1adfc5d64f"" rel=""noreferrer"">quote</a>:</p>
<blockquote>
<p>Instead of using a single attention function where the attention can
be dominated by the actual word itself, transformers use multiple
attention heads.</p>
</blockquote>
<p>What is meant by &quot;the attention being dominated by the word itself&quot; and how does the use of multiple heads address that?</p>
","transformer-model"
"66207138","Errors appear when training an XLNET model","2021-02-15 11:23:51","66260061","0","176","<python><huggingface-transformers><transformer-model>","<p>I am trying to train an XLNET model as the following. I want to set the hyperparameters by myself without using any pretrained models.</p>
<pre><code>from transformers import XLNetConfig, XLNetModel
from transformers import Trainer, TrainingArguments
# Initializing an XLNet configuration
configuration = XLNetConfig(use_mems_train = True)
model = XLNetModel(configuration)
train_dataset = 'sentences.txt'
training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=3,              # total # of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
)
trainer = Trainer(
    model=model,                         # the instantiated Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
)
trainer.train()
</code></pre>
<p>However, the following errors appear:</p>
<pre><code>C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(&quot;qint8&quot;, np.int8, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(&quot;quint8&quot;, np.uint8, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(&quot;qint16&quot;, np.int16, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(&quot;quint16&quot;, np.uint16, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(&quot;qint32&quot;, np.int32, 1)])
C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(&quot;resource&quot;, np.ubyte, 1)])
  0%|          | 0/9 [00:00&lt;?, ?it/s]Traceback (most recent call last):
  File &quot;untitled1/dfgd.py&quot;, line 23, in &lt;module&gt;
    trainer.train()
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\trainer.py&quot;, line 925, in train
    for step, inputs in enumerate(epoch_iterator):
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\torch\utils\data\dataloader.py&quot;, line 435, in __next__
    data = self._next_data()
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\torch\utils\data\dataloader.py&quot;, line 475, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\torch\utils\data\_utils\fetch.py&quot;, line 47, in fetch
    return self.collate_fn(data)
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\data\data_collator.py&quot;, line 52, in default_data_collator
    features = [vars(f) for f in features]
  File &quot;C:\Users\DSP\AppData\Roaming\Python\Python37\site-packages\transformers\data\data_collator.py&quot;, line 52, in &lt;listcomp&gt;
    features = [vars(f) for f in features]
TypeError: vars() argument must have __dict__ attribute
Exception ignored in: &lt;function tqdm.__del__ at 0x0000014A17ABE828&gt;
Traceback (most recent call last):
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\std.py&quot;, line 1039, in __del__
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\std.py&quot;, line 1223, in close
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\std.py&quot;, line 555, in _decr_instances
  File &quot;C:\ProgramData\Anaconda3\lib\site-packages\tqdm\_monitor.py&quot;, line 51, in exit
  File &quot;C:\ProgramData\Anaconda3\lib\threading.py&quot;, line 522, in set
  File &quot;C:\ProgramData\Anaconda3\lib\threading.py&quot;, line 365, in notify_all
  File &quot;C:\ProgramData\Anaconda3\lib\threading.py&quot;, line 348, in notify
TypeError: 'NoneType' object is not callable
</code></pre>
<p>How should I handle these errors? How can I train my XLNET model?</p>
","transformer-model"
"66196236","TuneError: ('Trials did not complete')","2021-02-14 14:11:56","","0","1846","<keras><grid-search><hyperparameters><transformer-model><ray-tune>","<p>I wrote a program using keras that detects real texts from fake (I used 5000 training data and 10,000 test data), I used Transformer and 'distilbert-base-uncased' model for detection. Now I decide to hyperparameters tuning using the grid search , which I encountered the following error:</p>
<pre><code>    TuneError                                 Traceback (most recent call last)
    &lt;ipython-input-15-c4a44a2180d8&gt; in &lt;module&gt;()
        156     tune_iris,
        157     verbose=1,
    --&gt; 158     config=hyperparameter_space,
        159    )
        160 
    
    /usr/local/lib/python3.6/dist-packages/ray/tune/tune.py in run(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, fail_fast, restore, search_alg, scheduler, with_server, server_port, verbose, progress_reporter, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init)
        354     if incomplete_trials:
        355         if raise_on_failed_trial:
    --&gt; 356             raise TuneError(&quot;Trials did not complete&quot;, incomplete_trials)
        357         else:
        358             logger.error(&quot;Trials did not complete: %s&quot;, incomplete_trials)
    
    TuneError: ('Trials did not complete', [tune_iris_83131_00000, tune_iris_83131_00001, tune_iris_83131_00002, tune_iris_83131_00003, tune_iris_83131_00004, tune_iris_83131_00005, tune_iris_83131_00006, tune_iris_83131_00007, tune_iris_83131_00008, tune_iris_83131_00009, tune_iris_83131_00010, tune_iris_83131_00011, tune_iris_83131_00012, tune_iris_83131_00013, tune_iris_83131_00014, tune_iris_83131_00015, tune_iris_83131_00016, tune_iris_83131_00017])
</code></pre>
<p>The program I wrote is as follows:</p>
<pre><code>data = pd.concat([train_webtext,train_gen,valid_webtext,valid_gen])

sentences=data['text']
labels=labels1+labels2
len(sentences),len(labels)


DistilBertTokenizer = DistilBertTokenizer.from_pretrained(&quot;distilbert-base-cased&quot;,do_lower_case=False)


input_ids=[]
attention_masks=[]

for sent in sentences:
    bert_inp=DistilBertTokenizer.encode_plus(sent,add_special_tokens = True,max_length =64,pad_to_max_length = True,return_attention_mask = True)
    input_ids.append(bert_inp['input_ids'])
    attention_masks.append(bert_inp['attention_mask'])
    

input_ids=np.asarray(input_ids)
attention_masks=np.array(attention_masks)
labels=np.array(labels)


class TuneReporterCallback(keras.callbacks.Callback):
    &quot;&quot;&quot;Tune Callback for Keras.
    
    The callback is invoked every epoch.
    &quot;&quot;&quot;

    def __init__(self, logs={}):
        self.iteration = 0
        super(TuneReporterCallback, self).__init__()

    def on_epoch_end(self, batch, logs={}):
        self.iteration += 1
        tune.report(keras_info=logs, mean_accuracy=logs.get(&quot;accuracy&quot;), mean_loss=logs.get(&quot;loss&quot;))


def tune_gpt(config):
  train_inp,val_inp,train_label,val_label,train_mask,val_mask=train_test_split(input_ids,labels,attention_masks,test_size=0.6666666666666666)
  DistilBert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)
  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
  metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
  optimizer = tf.keras.optimizers.Adam(learning_rate=config[&quot;learning_rate&quot;],epsilon=1e-08)
  DistilBert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])
  checkpoint_callback = [tf.keras.callbacks.ModelCheckpoint( &quot;DistilBert_model.h5&quot;,monitor='val_loss',mode='min',save_best_only=True)]
  callbacks = [checkpoint_callback, TuneReporterCallback()]
  history=DistilBert_model.fit([train_inp,train_mask],train_label,batch_size=config[&quot;batch_size&quot;],epochs=config[&quot;epochs&quot;],validation_data=([val_inp,val_mask],val_label),callbacks=callbacks)
  assert len(inspect.getargspec(tune_gpt).args) == 1, &quot;The `tune_gpt` function needs to take in the arg `config`.&quot;



hyperparameter_space  ={
       &quot;batch_size&quot;: tune.grid_search([16, 32]),
       &quot;learning_rate&quot;: tune.grid_search([2e-5, 3e-5, 5e-5]),
       &quot;epochs&quot;: tune.grid_search([2, 3, 4])
    }


analysis = tune.run(
    tune_gpt, 
    verbose=1, 
    config=hyperparameter_space,
   )
</code></pre>
","transformer-model"
"66190946","Transformer/BERT token prediction vocabulary (filtering the special tokens out of the set of possible tokens)","2021-02-13 23:23:44","","3","1104","<bert-language-model><transformer-model>","<p>With the Transformer model, especially with the BERT, does it make sense (and would it be statistically correct) to programmatically forbid the model to result with the special tokens as predictions?
How is that in the original implementations? During convergence the models have to learn not to predict these but would this intervention help (or the opposite)?</p>
<ul>
<li>I would consider the [MASK], [CLS] tokens mainly</li>
<li>[PAD] token could have some sense as well (but that not in all situations)</li>
</ul>
","transformer-model"
"66189012","Sentiment analysis in a generic text with Transformers","2021-02-13 19:25:55","","-2","96","<deep-learning><sentiment-analysis><text-classification><bert-language-model><transformer-model>","<p>Is it correct to use BERT (or any other Transformers-based model) finetuned with a dataset like IMDb if I want to do sentiment analysis in a generic text (unrelated with movies)?</p>
<p>If not, how should I do?</p>
","transformer-model"
"66054042","Why is the timm visual transformer position embedding initializing to zeros?","2021-02-04 21:26:43","66590833","1","1088","<pytorch><transformer-model><vision-transformer>","<p>I'm looking at the <code>timm</code> implementation of visual transformers and for the positional embedding, he is initializing his position embedding with zeros as follows:</p>
<pre><code>self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
</code></pre>
<p>See here:
<a href=""https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py#L309"" rel=""nofollow noreferrer"">https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py#L309</a></p>
<p>I'm not sure how this actually embeds anything about the position when it is later added to the patch?</p>
<pre><code>x = x + self.pos_embed
</code></pre>
<p>Any feedback is appreciated.</p>
","transformer-model"
"66032426","Backpropagation in bert","2021-02-03 17:15:55","66050780","2","1246","<nlp><bert-language-model><transformer-model>","<p>i would like to know when people say pretrained bert model, is it only the final classification neural network is trained</p>
<p>Or</p>
<p>Is there any update inside transformer through back propagation along with classification neural network</p>
","transformer-model"
"65976748","Fastai v2 IndexError: Target 2 is out of bounds","2021-01-31 07:33:33","","3","698","<python-3.x><pytorch><transformer-model><fast-ai>","<h3>Package version and reference page</h3>
<p>Hi, I am referring to the page <a href=""https://github.com/ohmeow/blurr"" rel=""nofollow noreferrer"">Blurr</a> to combining the HuggingFace Transformers' models with the Fastai fine-tuning policies. Below are the versions of the packages:</p>
<ul>
<li>transformers : 4.2.2</li>
<li>fastai : 2.1.10</li>
<li>ohmeow-blurr  : 0.0.22</li>
</ul>
<p>The difference between the sample code in <a href=""https://github.com/ohmeow/blurr"" rel=""nofollow noreferrer"">Blurr</a> and mine is that I am trying to solve the <strong>multi-class single label text classification</strong> problem with 4 class of total labels while the tutorial is the <strong>binary-classification</strong> issue.</p>
<p>The dataset for both training and validation has 3 columns: texts, label, is_valid.</p>
<ul>
<li><strong>texts</strong> contains text data similar to the <a href=""https://github.com/ohmeow/blurr"" rel=""nofollow noreferrer"">Blurr</a>.</li>
<li><strong>label</strong> contains 4 classes of string labels, for each raw, there is only one label.</li>
<li><strong>is_valid</strong> contains bool value &quot;True and False&quot; that indicates if these raw are for training or validation.</li>
</ul>
<h3>Error message</h3>
<pre><code>---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
&lt;ipython-input-16-fc93db149c6f&gt; in &lt;module&gt;
     13 learn.freeze()
     14 
---&gt; 15 learn.fit_one_cycle(3, lr_max=1e-3)

~/fastai/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt)
    110     scheds = {'lr': combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final),
    111               'mom': combined_cos(pct_start, *(self.moms if moms is None else moms))}
--&gt; 112     self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd)
    113 
    114 # Cell

~/fastai/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt)
    204             self.opt.set_hypers(lr=self.lr if lr is None else lr)
    205             self.n_epoch = n_epoch
--&gt; 206             self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)
    207 
    208     def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None

~/fastai/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    153 
    154     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 155         try:       self(f'before_{event_type}')       ;f()
    156         except ex: self(f'after_cancel_{event_type}')
    157         finally:   self(f'after_{event_type}')        ;final()

~/fastai/fastai/learner.py in _do_fit(self)
    195         for epoch in range(self.n_epoch):
    196             self.epoch=epoch
--&gt; 197             self._with_events(self._do_epoch, 'epoch', CancelEpochException)
    198 
    199     def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False):

~/fastai/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    153 
    154     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 155         try:       self(f'before_{event_type}')       ;f()
    156         except ex: self(f'after_cancel_{event_type}')
    157         finally:   self(f'after_{event_type}')        ;final()

~/fastai/fastai/learner.py in _do_epoch(self)
    189 
    190     def _do_epoch(self):
--&gt; 191         self._do_epoch_train()
    192         self._do_epoch_validate()
    193 

~/fastai/fastai/learner.py in _do_epoch_train(self)
    181     def _do_epoch_train(self):
    182         self.dl = self.dls.train
--&gt; 183         self._with_events(self.all_batches, 'train', CancelTrainException)
    184 
    185     def _do_epoch_validate(self, ds_idx=1, dl=None):

~/fastai/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    153 
    154     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 155         try:       self(f'before_{event_type}')       ;f()
    156         except ex: self(f'after_cancel_{event_type}')
    157         finally:   self(f'after_{event_type}')        ;final()

~/fastai/fastai/learner.py in all_batches(self)
    159     def all_batches(self):
    160         self.n_iter = len(self.dl)
--&gt; 161         for o in enumerate(self.dl): self.one_batch(*o)
    162 
    163     def _do_one_batch(self):

~/fastai/fastai/learner.py in one_batch(self, i, b)
    177         self.iter = i
    178         self._split(b)
--&gt; 179         self._with_events(self._do_one_batch, 'batch', CancelBatchException)
    180 
    181     def _do_epoch_train(self):

~/fastai/fastai/learner.py in _with_events(self, f, event_type, ex, final)
    153 
    154     def _with_events(self, f, event_type, ex, final=noop):
--&gt; 155         try:       self(f'before_{event_type}')       ;f()
    156         except ex: self(f'after_cancel_{event_type}')
    157         finally:   self(f'after_{event_type}')        ;final()

~/fastai/fastai/learner.py in _do_one_batch(self)
    164         self.pred = self.model(*self.xb)
    165         self('after_pred')
--&gt; 166         if len(self.yb): self.loss = self.loss_func(self.pred, *self.yb)
    167         self('after_loss')
    168         if not self.training or not len(self.yb): return

~/fastai/fastai/losses.py in __call__(self, inp, targ, **kwargs)
     31         if targ.dtype in [torch.int8, torch.int16, torch.int32]: targ = targ.long()
     32         if self.flatten: inp = inp.view(-1,inp.shape[-1]) if self.is_2d else inp.view(-1)
---&gt; 33         return self.func.__call__(inp, targ.view(-1) if self.flatten else targ, **kwargs)
     34 
     35 # Cell

~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
    725             result = self._slow_forward(*input, **kwargs)
    726         else:
--&gt; 727             result = self.forward(*input, **kwargs)
    728         for hook in itertools.chain(
    729                 _global_forward_hooks.values(),

~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py in forward(self, input, target)
    959 
    960     def forward(self, input: Tensor, target: Tensor) -&gt; Tensor:
--&gt; 961         return F.cross_entropy(input, target, weight=self.weight,
    962                                ignore_index=self.ignore_index, reduction=self.reduction)
    963 

~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)
   2460         tens_ops = (input, target)
   2461         if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):
-&gt; 2462             return handle_torch_function(
   2463                 cross_entropy, tens_ops, input, target, weight=weight,
   2464                 size_average=size_average, ignore_index=ignore_index, reduce=reduce,

~/anaconda3/lib/python3.8/site-packages/torch/overrides.py in handle_torch_function(public_api, relevant_args, *args, **kwargs)
   1058         # Use `public_api` instead of `implementation` so __torch_function__
   1059         # implementations can do equality/identity comparisons.
-&gt; 1060         result = overloaded_arg.__torch_function__(public_api, types, args, kwargs)
   1061 
   1062         if result is not NotImplemented:

~/fastai/fastai/torch_core.py in __torch_function__(self, func, types, args, kwargs)
    317 #         if func.__name__[0]!='_': print(func, types, args, kwargs)
    318 #         with torch._C.DisableTorchFunction(): ret = _convert(func(*args, **(kwargs or {})), self.__class__)
--&gt; 319         ret = super().__torch_function__(func, types, args=args, kwargs=kwargs)
    320         if isinstance(ret, TensorBase): ret.set_meta(self, as_copy=True)
    321         return ret

~/anaconda3/lib/python3.8/site-packages/torch/tensor.py in __torch_function__(cls, func, types, args, kwargs)
    993 
    994         with _C.DisableTorchFunction():
--&gt; 995             ret = func(*args, **kwargs)
    996             return _convert(ret, cls)
    997 

~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)
   2466     if size_average is not None or reduce is not None:
   2467         reduction = _Reduction.legacy_get_string(size_average, reduce)
-&gt; 2468     return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)
   2469 
   2470 

~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)
   2262                          .format(input.size(0), target.size(0)))
   2263     if dim == 2:
-&gt; 2264         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
   2265     elif dim == 4:
   2266         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

IndexError: Target 2 is out of bounds.
</code></pre>
<h3>Code</h3>
<p>While in the <code>learn.fit_one_cycle(3, lr_max=1e-3)</code> it shows the error message above. I am sure that for both the training set and the validation set have the same labels by checking <code>len(df[df['is_valid']==False]['label'].unique()) == len(df[df['is_valid']==True]['label'].unique())</code> and print them respectively.</p>
<pre><code>import os
import torch
from transformers import *
from fastai.text.all import *

from blurr.data.all import *
from blurr.modeling.all import *

task = HF_TASKS_AUTO.SequenceClassification
pretrained_model_name = &quot;bert-base-uncased&quot;
hf_arch, hf_config, hf_tokenizer, hf_model = BLURR_MODEL_HELPER.get_hf_objects(pretrained_model_name,  task=task)

# data loader
blocks = (HF_TextBlock(hf_arch, hf_config, hf_tokenizer, hf_model),  CategoryBlock)
dblock = DataBlock(blocks=blocks,  
                   get_x=ColReader('texts'), 
                   get_y=ColReader('label'), 
                   splitter=ColSplitter('is_valid'))

dls = dblock.dataloaders(df, batch_size=4)
</code></pre>
<pre><code>
model = HF_BaseModelWrapper(hf_model)

learn = Learner(dls, 
                model,
                opt_func=partial(Adam, decouple_wd=True),
                loss_func=CrossEntropyLossFlat(),
                metrics=[accuracy, F1Score(average='macro')],
                cbs=[HF_BaseModelCallback],
                splitter=hf_splitter)

learn.create_opt()  
learn.freeze()

learn.fit_one_cycle(3, lr_max=1e-3)
</code></pre>
<p>Has anyone gotten some similar problems before? and is there anything I lost in the code?</p>
","transformer-model"
"65925640","Assigning weights during testing the bert model","2021-01-27 18:58:04","66193869","0","1587","<bert-language-model><huggingface-transformers><transformer-model><language-model>","<p>I have a basic conceptual doubt. When i train a bert model on sentence say:</p>
<pre><code>Train: &quot;went to get loan from bank&quot; 
Test :&quot;received education loan from bank&quot;
</code></pre>
<p>How does the test sentence assigns the weights for each token because i however dont pass exact sentence for testing and there is a slight addition  of words like &quot;education&quot; which change the context slightly</p>
<p>Assuming such context is not trained in my model how the weights are assigned for each token in my bert before i fine tune further</p>
<p>If i confuse with my question, simply put i am trying to understand how the weights get assigned during testing if a slight variation in context occurs that was not trained on.</p>
","transformer-model"
"65733681","Cannot deploy a small transformers model for prediction serving using Google Cloud AI Platform due to ""Model requires more memory than allowed""","2021-01-15 09:38:57","","0","485","<google-cloud-platform><pytorch><google-cloud-ml><huggingface-transformers><transformer-model>","<p>I have a fine tuned <code>distilgpt2</code> model that I want to deploy using GCP ai-platform.</p>
<p>I've followed all the documentation for deploying a custom prediction routine on GCP but when creating the model I get the error:</p>
<blockquote>
<p>Create Version failed. Bad model detected with error: Model requires more memory than allowed. Please try to decrease the model size and re-deploy.</p>
</blockquote>
<p>Here is my <code>setup.py</code> file:</p>
<pre><code>from setuptools import setup

setup(
    name=&quot;generator_package&quot;,
    version=&quot;0.2&quot;,
    include_package_data=True,
    scripts=[&quot;generator_class.py&quot;],
    install_requires=['transformers==2.8.0']
)
</code></pre>
<p>I then create a model version using:</p>
<pre><code>gcloud beta ai-platform versions create v1 --model my_model \
 --origin=gs://my_bucket/model/ \
 --python-version=3.7 \
 --runtime-version=2.3 \
 --package-uris=gs://my_bucket/packages/gpt2-0.1.tar.gz,gs://cloud-ai-pytorch/torch-1.3.1+cpu-cp37-cp37m-linux_x86_64.whl \
 --prediction-class=model_prediction.CustomModelPrediction
</code></pre>
<p>Following this answer: <a href=""https://stackoverflow.com/questions/60423140/pytorch-model-deployment-in-ai-platform"">PyTorch model deployment in AI Platform</a>, I figured out how to get pytorch installed on my custom prediction routine, but am still getting the above error. I believe it may have something to do with the <code>transformers</code> package, as it has <code>torch</code> as a dependency. Can that be causing the issue?</p>
<p>I have tried every suggested route and cant get this to work and I'm still getting the above error. I'm using the smallest gpt2 model and am well within memory.</p>
<p>Can anyone who have successfully deployed to GCP please give some insight here.</p>
<p><strong>Update:</strong></p>
<p>So to address the above concern of <code>transformers</code> also trying to install <code>torch</code> and that may be causing the problem I rebuilt the <code>.whl</code> file from source with the additional packages removed, below is the edited <code>setup.py</code> file and built using <code>python setup.py bdist_wheel</code>.</p>
<p>I then added this <code>whl</code> to the required dependencies when creating a model version in GCP and removed the <code>transformers==2.8.0</code> from my own <code>setup.py</code>. But it is still giving the same error of model requires more memory =(</p>
<pre><code>import shutil

from pathlib import Path



from setuptools import find_packages, setup





# Remove stale transformers.egg-info directory to avoid https://github.com/pypa/pip/issues/5466

stale_egg_info = Path(__file__).parent / &quot;transformers.egg-info&quot;

if stale_egg_info.exists():

    print(

        (

            &quot;Warning: {} exists.\n\n&quot;

            &quot;If you recently updated transformers to 3.0 or later, this is expected,\n&quot;

            &quot;but it may prevent transformers from installing in editable mode.\n\n&quot;

            &quot;This directory is automatically generated by Python's packaging tools.\n&quot;

            &quot;I will remove it now.\n\n&quot;

            &quot;See https://github.com/pypa/pip/issues/5466 for details.\n&quot;

        ).format(stale_egg_info)

    )

    shutil.rmtree(stale_egg_info)





extras = {}



extras[&quot;mecab&quot;] = [&quot;mecab-python3&quot;]

extras[&quot;sklearn&quot;] = [&quot;scikit-learn&quot;]

# extras[&quot;tf&quot;] = [&quot;tensorflow&quot;]

# extras[&quot;tf-cpu&quot;] = [&quot;tensorflow-cpu&quot;]

# extras[&quot;torch&quot;] = [&quot;torch&quot;]



extras[&quot;serving&quot;] = [&quot;pydantic&quot;, &quot;uvicorn&quot;, &quot;fastapi&quot;, &quot;starlette&quot;]

extras[&quot;all&quot;] = extras[&quot;serving&quot;] + [&quot;tensorflow&quot;, &quot;torch&quot;]



extras[&quot;testing&quot;] = [&quot;pytest&quot;, &quot;pytest-xdist&quot;]

extras[&quot;docs&quot;] = [&quot;recommonmark&quot;, &quot;sphinx&quot;, &quot;sphinx-markdown-tables&quot;, &quot;sphinx-rtd-theme&quot;]

extras[&quot;quality&quot;] = [

    &quot;black&quot;,

    &quot;isort&quot;,

    &quot;flake8&quot;,

]

extras[&quot;dev&quot;] = extras[&quot;testing&quot;] + extras[&quot;quality&quot;] + [&quot;mecab-python3&quot;, &quot;scikit-learn&quot;, &quot;tensorflow&quot;, &quot;torch&quot;]



setup(

    name=&quot;transformers&quot;,

    version=&quot;2.8.0&quot;,

    author=&quot;Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Sam Shleifer, Google AI Language Team Authors, Open AI team Authors, Facebook AI Authors, Carnegie Mellon University Authors&quot;,

    author_email=&quot;thomas@huggingface.co&quot;,

    description=&quot;State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch&quot;,

    long_description=open(&quot;README.md&quot;, &quot;r&quot;, encoding=&quot;utf-8&quot;).read(),

    long_description_content_type=&quot;text/markdown&quot;,

    keywords=&quot;NLP deep learning transformer pytorch tensorflow BERT GPT GPT-2 google openai CMU&quot;,

    license=&quot;Apache&quot;,

    url=&quot;https://github.com/huggingface/transformers&quot;,

    package_dir={&quot;&quot;: &quot;src&quot;},

    packages=find_packages(&quot;src&quot;),

    install_requires=[

        &quot;numpy&quot;,

        &quot;tokenizers == 0.5.2&quot;,

        # dataclasses for Python versions that don't have it

        &quot;dataclasses;python_version&lt;'3.7'&quot;,

        # accessing files from S3 directly

        &quot;boto3&quot;,

        # filesystem locks e.g. to prevent parallel downloads

        &quot;filelock&quot;,

        # for downloading models over HTTPS

        &quot;requests&quot;,

        # progress bars in model download and training scripts

        &quot;tqdm &gt;= 4.27&quot;,

        # for OpenAI GPT

        &quot;regex != 2019.12.17&quot;,

        # for XLNet

        &quot;sentencepiece&quot;,

        # for XLM

        &quot;sacremoses&quot;,

    ],

    extras_require=extras,

    scripts=[&quot;transformers-cli&quot;],

    python_requires=&quot;&gt;=3.6.0&quot;,

    classifiers=[

        &quot;Development Status :: 5 - Production/Stable&quot;,

        &quot;Intended Audience :: Developers&quot;,

        &quot;Intended Audience :: Education&quot;,

        &quot;Intended Audience :: Science/Research&quot;,

        &quot;License :: OSI Approved :: Apache Software License&quot;,

        &quot;Operating System :: OS Independent&quot;,

        &quot;Programming Language :: Python :: 3&quot;,

        &quot;Programming Language :: Python :: 3.6&quot;,

        &quot;Programming Language :: Python :: 3.7&quot;,

        &quot;Topic :: Scientific/Engineering :: Artificial Intelligence&quot;,

    ],

)
</code></pre>
","transformer-model"
"65696968","How to i get word embeddings for out of vocabulary words using a transformer model?","2021-01-13 06:51:24","","2","1803","<nlp><huggingface-transformers><transformer-model><huggingface-tokenizers><oov>","<p>When i tried to get word embeddings of a sentence using bio_clinical bert, for a sentence of 8 words i am getting 11 token ids(+start and end) because &quot;embeddings&quot; is an out of vocabulary word/token, that is being split into <code>em</code>, <code>bed</code> ,<code>ding</code>, <code>s</code>.</p>
<p>I would like to know if there is any aggregation strategies available that make sense apart from doing a mean of these vectors.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
# download and load model
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

sentences = ['This framework generates embeddings for each input sentence']


#Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')


#Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)

print(encoded_input['input_ids'].shape)
</code></pre>
<p>Output:
<code>torch.Size([1, 13])</code></p>
<pre class=""lang-py prettyprint-override""><code>for token in encoded_input['input_ids'][0]:
      print(tokenizer.decode([token]))
</code></pre>
<p>Output:</p>
<pre><code>[CLS]
this
framework
generates
em
##bed
##ding
##s
for
each
input
sentence
[SEP]
</code></pre>
","transformer-model"
"65690180","Reducing size of training dataset in tensorflow 2 tutorial (Transformer model for language understanding) with '.take(n)' method does not work","2021-01-12 18:51:06","","0","944","<python><tensorflow><tensorflow2.0><tensorflow-datasets><transformer-model>","<p>I am a Tensorflow-newbie, therefore bear with me if my question is too basic or stupid ;)</p>
<p>I tried to reduce the size of the training dataset in the &quot;Transformer model for language understanding&quot;-tutorial of the Tensorflow website (<a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/transformer</a>). My intention was to make my test runs faster, when playing around with the code.</p>
<p>I thought I could use the <code>dataset.take(n)</code> method to shorten the training datasets. I added two lines right after the original dataset is read from file:</p>
<pre><code>...

examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)
train_examples, val_examples = examples['train'], examples['validation']

# lines added to reduce dataset size
train_examples = train_examples.take(1000)
val_examples = val_examples.take(1000)

...
</code></pre>
<p>The resulting datasets (i.e., <code>train_examples</code>, <code>val_examples</code>) seem to have the intended size, and they seem to be working, e.g., with the tokenizer, which comes next in the turorial.</p>
<p>However, I get tons of error messages and warnings when I execute the code, more specifically when it enters training (i.e., <code>train_step(inp, tar)</code>). The error messages and warnings are too long to copy here, but perhaps an important part of it is this:</p>
<pre><code>...

    /home/kst/python/tf/tf_env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1105 set_shape
        raise ValueError(

    ValueError: Tensor's shape (8216, 128) is not compatible with supplied shape (4870, 128)

WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1

...
</code></pre>
<p>Some of the tensors in the training part seem to have an inappropriate size or shape.</p>
<p>Is there a good reason why <code>.take(n)</code> is not a good method to shorten datasets in Tensorflow?</p>
<p>Is there a better way to do it?</p>
<p>Thanks!
:)</p>
","transformer-model"
"65663731","Transformer for time series forecasting","2021-01-11 08:47:29","","1","572","<python><time-series><pytorch><transformer-model><pytorch-lightning>","<p>I have discrete daily features and a target time series and I'm trying to implement a basic Transformer for seq2seq modeling. I construct my supervised data as follows:</p>
<pre><code> . . .\. . .\. . . .
       |
\0 0 0 0\0 0 0 0 0 0
</code></pre>
<p>The next sequence is shifted by one position ahead. Overall I have the input data shape <code>[batch_size, in_sequence_len, num_features]</code> and the target is <code>[batch_size, out_sequence_len, 1]</code>. I understand that encoder input should be of shape <code>[batch_size, in_seq_len, num_features]</code> and decoder takes <code>[batch_size, out_seq_len, num_features]</code>. But how should I transform my batch to be suitable for the transformer input?</p>
","transformer-model"
"65651623","Multi-step time series forecasting","2021-01-10 09:07:05","","0","391","<python><time-series><pytorch><recurrent-neural-network><transformer-model>","<p>I have standard many-to-one different RNN models which I used for 1 target time series prediction using other time series as features. I use 14 as the input sequence length and 1 value which corresponds to the target time series at the time moment which corresponds to the end of the input sequence. I want to generalize this approach to have some forecast horizon and predict some predefined steps ahead. Then, I would construct my training batches as follows(with forecast horizon = 2 and in_seq_len = 4):</p>
<pre><code> . . ./. ./. . . . .
       |
/0 0 0 0/0 0 0 0 0 0 
</code></pre>
<p>The next sample is shifted by one position:</p>
<pre><code>  . . . ./. ./. . . .
          |
  0/0 0 0 0/0 0 0 0 0 
</code></pre>
<p>As expected, I train the network to predict the sequence instead of just one value. My question is: How should I get the final predictions of the network if I got the output of my network of shape <code>[batch_size, forecast_horizon]</code>. The output tensors will be overlapping because the samples are shifted by one position as Pytorch RNN expects it to be. Should I take only the last samples as the network predictions? i.e. <code>[batch_size, -1]</code></p>
","transformer-model"
"65646925","How to train BERT from scratch on a new domain for both MLM and NSP?","2021-01-09 19:46:24","65760008","13","12522","<deep-learning><nlp><bert-language-model><huggingface-transformers><transformer-model>","<p>I’m trying to train BERT model from scratch using my own dataset using HuggingFace library. I would like to train the model in a way that it has the exact architecture of the original BERT model.</p>
<p>In the original paper, it stated that: <em>“BERT is trained on two tasks: predicting randomly masked tokens (MLM) and predicting whether two sentences follow each other (NSP). SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text.”</em></p>
<p>I’m trying to understand how to train the model on two tasks as above. At the moment, I initialised the model as below:</p>
<pre><code>from transformers import BertForMaskedLM
model = BertForMaskedLM(config=config)
</code></pre>
<p>However, it would just be for MLM and not NSP. How can I initialize and train the model with NSP as well or maybe my original approach was fine as it is?</p>
<p>My assumptions would be either</p>
<ol>
<li><p>Initialize with <code>BertForPreTraining</code> (for both MLM and NSP), OR</p>
</li>
<li><p>After finish training with <code>BertForMaskedLM</code>,
initalize the same model and train again with
<code>BertForNextSentencePrediction</code> (but this approach’s computation and
resources would cost twice…)</p>
</li>
</ol>
<p>I’m not sure which one is the correct way. Any insights or advice would be greatly appreciated.</p>
","transformer-model"
"65627425","Need to understand the concept behind SubwordTextEncoder tokenizer","2021-01-08 10:35:55","","0","288","<python><tensorflow><deep-learning><nlp><transformer-model>","<p>I am currently using transformer model for my NLP task. I am looking into the transformer model <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">explanation</a> from Tensorflow.org.</p>
<p>I understood the concept behind the entire model but I am a bit stuck up at tokenization part.</p>
<p>Tokenization uses SubwordTextEncoder API in which we need to build vocabulary first and then for replacing sentences with the set of tokens (in order to be understood by the model), we use its .encode() function.</p>
<p>When I looked at its usage given on tensorflow website, I found it a bit puzzling as in how this kind of encoding may help in self-attention.</p>
<p>To verify my understanding about its implementation, I created my own vocabulary with 2 sentences - ['My name is xyz. I am doing experiments.'] as follows -</p>
<pre><code>my_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(
   (ex for ex in ['My name is xyz. I am doing experiments.']), target_vocab_size=258)
</code></pre>
<p>After this, I tried to encode few sentences.</p>
<p>First,</p>
<p>Code --&gt;</p>
<pre><code>tokenized_string = my_tokenizer.encode('I am doing xyz.')
for ts in tokenized_string:
  print ('{} ----&gt; {}'.format(ts, my_tokenizer.decode([ts])))
</code></pre>
<p>Output --&gt;</p>
<pre><code>8 ----&gt; I 
6 ----&gt; am 
5 ----&gt; doing 
1 ----&gt; xyz
56 ----&gt; .
</code></pre>
<p>Another string was -</p>
<p>Code --&gt;</p>
<pre><code>tokenized_string = my_tokenizer.encode('very nice.')
for ts in tokenized_string:
  print ('{} ----&gt; {}'.format(ts, my_tokenizer.decode([ts])))
</code></pre>
<p>Output --&gt;</p>
<pre><code>128 ----&gt; v
111 ----&gt; e
124 ----&gt; r
131 ----&gt; y
42 ----&gt;  
120 ----&gt; n
115 ----&gt; i
109 ----&gt; c
111 ----&gt; e
56 ----&gt; .
</code></pre>
<p>Can anyone tell me how this helps in achieving better self-attention? Or is this the old way of performing tokenization as I can see that this particular API is on the verge of getting deprecated?</p>
","transformer-model"
"65588829","Pytorch transformer forward function masks implementation for decoder forward function","2021-01-06 01:11:01","","1","4459","<pytorch><forward><transformer-model><encoder-decoder>","<p>I am trying to use and learn PyTorch Transformer with DeepMind math dataset. I have tokenized (char not word) sequence that is fed into model. Models forward function is doing once forward for encoder and multiple forwards for decoder (till all batch outputs reach  token, this is still TODO).
I am struggling with Transformer masks and decoder forward as it throws the error:</p>
<pre><code>    k = k.contiguous().view(-1, bsz * num_heads, head_dim).transpose(0, 1)
    RuntimeError: shape '[-1, 24, 64]' is invalid for input of size 819200.
</code></pre>
<p>Source is N = 32, S = 50, E = 512. Target is N = 32, S = 3, E = 512.
It is possible that I have wrong implementation of masks or that source and target lengths are different, not realy sure.</p>
<pre><code>class PositionalEncoding(nn.Module):   
# function to positionally encode src and target sequencies 
def __init__(self, d_model, dropout=0.1, max_len=5000):
    super(PositionalEncoding, self).__init__()
    self.dropout = nn.Dropout(p=dropout)
    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    pe = pe.unsqueeze(0).transpose(0, 1)
    self.register_buffer('pe', pe)

def forward(self, x):
    x = x + self.pe[:x.size(0), :]
    return self.dropout(x)

class MyTransformerModel(nn.Module):
# should implement init and forward function
# define separate functions for masks
# define forward function with
# implement:
#  embedding layer
#  positional encoding
#  encoder layer
#  decoder layer
#  final classification layer
# encoder -&gt; forward once
# decoder -&gt; forward multiple times (for one encoder forward)
# decoder output =&gt; concatenate to input e.g. decoder_input = torch.cat([decoder_input], [decoder_output])
# early stopping =&gt; all in batch reach &lt;eos&gt; token
def __init__(self, vocab_length = 30, sequence_length = 512, num_encoder_layers = 3, num_decoder_layers = 2, num_hidden_dimension = 256, feed_forward_dimensions = 1024, attention_heads = 8, dropout = 0.1, pad_idx = 3, device = &quot;CPU&quot;, batch_size = 32):
    super(MyTransformerModel, self).__init__()
    self.src_embedding = nn.Embedding(vocab_length, sequence_length)
    self.pos_encoder = PositionalEncoding(sequence_length, dropout)
    self.src_mask = None # attention mask
    self.memory_mask = None # attention mask
    self.pad_idx = pad_idx        
    self.device = device        
    self.batch_size = batch_size
    self.transformer = nn.Transformer(
        sequence_length,
        attention_heads,
        num_encoder_layers,
        num_decoder_layers,
        feed_forward_dimensions,
        dropout,
    )
    
def src_att_mask(self, src_len):
    mask = (torch.triu(torch.ones(src_len, src_len)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
    return mask


def no_peak_att_mask(self, batch_size, src_len, time_step):
    mask = np.zeros((batch_size, src_len), dtype=bool)
    mask[:, time_step: ] = 1 # np.NINF
    mask = torch.from_numpy(mask)
    return mask

def make_src_key_padding_mask(self, src):
    # mask &quot;&lt;pad&gt;&quot;
    src_mask = src.transpose(0, 1) == self.pad_idx
    return src_mask.to(self.device)

def make_trg_key_padding_mask(self, trg):
    tgt_mask = trg.transpose(0, 1) == self.pad_idx
    return tgt_mask.to(self.device)


def forward(self, src, trg):
    src_seq_length, N = src.shape
    trg_seq_length, N = trg.shape
    embed_src = self.src_embedding(src)
    position_embed_src =  self.pos_encoder(embed_src)
    embed_trg = self.src_embedding(trg)
    position_embed_trg = self.pos_encoder(embed_trg)        
    src_padding_mask = self.make_src_key_padding_mask(src)
    trg_padding_mask = self.make_trg_key_padding_mask(trg)
    trg_mask = self.transformer.generate_square_subsequent_mask(trg_seq_length).to(self.device)
    time_step = 1
    att_mask = self.no_peak_att_mask(self.batch_size, src_seq_length, time_step).to(self.device)
    encoder_output = self.transformer.encoder.forward(position_embed_src, src_key_padding_mask = src_padding_mask)
    # TODO : implement loop for transformer decoder forward fn, implement early stopping
    # where to feed decoder_output?
    decoder_output = self.transformer.decoder.forward(position_embed_trg, encoder_output, trg_mask, att_mask, trg_padding_mask, src_padding_mask)
    return decoder_output
    
</code></pre>
<p>Can anyone pin point where I have made a mistake?</p>
","transformer-model"
"65579177","Inputs of a transformer model","2021-01-05 12:43:50","","1","718","<nlp><sequence><transformer-model><attention-model>","<p>I am trying to understand the transformer model. Please consider my below example and help me to understand the concept.</p>
<p>Example: English to french conversion</p>
<p>My questions:</p>
<ol>
<li><p>Is the input word embedding is an English- french pretrained embedding?</p>
</li>
<li><p>In which step of the decoder the prediction of a french word is happening?</p>
</li>
<li><p>Is output embedding in decoder is just decoder's output till predicted, if so why should I mask the next word since that is unknown to me as I still yet not passed as output</p>
</li>
</ol>
<p>Please clarify me this doubt</p>
<p>I also referred to these links:</p>
<ul>
<li><a href=""https://datascience.stackexchange.com/questions/81727/what-would-be-the-target-input-for-transformer-decoder-during-test-phase"">https://datascience.stackexchange.com/questions/81727/what-would-be-the-target-input-for-transformer-decoder-during-test-phase</a></li>
<li><a href=""https://datascience.stackexchange.com/questions/51785/what-is-the-first-input-to-the-decoder-in-a-transformer-model"">https://datascience.stackexchange.com/questions/51785/what-is-the-first-input-to-the-decoder-in-a-transformer-model</a></li>
</ul>
","transformer-model"
"65557918","XLNetTokenizer requires the SentencePiece library but it was not found in your environment","2021-01-04 05:09:48","65599896","5","9446","<google-colaboratory><huggingface-transformers><transformer-model><huggingface-tokenizers>","<p>I am trying to implement the XLNET on Google Collaboratory. But I get the following issue.</p>
<pre><code>ImportError: 
XLNetTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment.
</code></pre>
<p>I have also tried the following steps:</p>
<pre><code>!pip install -U transformers
!pip install sentencepiece

from transformers import XLNetTokenizer
tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased-spiece.model')
</code></pre>
<p>Thank you for your help in advance.</p>
","transformer-model"
"65548397","fluentd add fields and output by json","2021-01-03 09:42:21","","1","860","<record><fluentd><transformer-model>","<p>I want to transform with fluentd something like this</p>
<pre><code>{
  &quot;source_app&quot;:&quot;client&quot;,
  &quot;location&quot;:&quot;cn&quot;
}
</code></pre>
<p>To this:</p>
<pre><code>{
  &quot;message&quot;:
  {
    &quot;source_app&quot;:&quot;client&quot;,
    &quot;location&quot;:&quot;cn&quot;
  }
}
</code></pre>
<p>My fluentd config is:</p>
<pre><code>&lt;filter **&gt;
  @type record_transformer
  enable_ruby true
  auto_typecast true
  &lt;record&gt;
    message ${{&quot;source_app&quot;=&gt;&quot;${record[&quot;source_app&quot;]}&quot;,&quot;location&quot;=&gt;&quot;${record[&quot;location&quot;]}&quot;}}
  &lt;/record&gt;
  remove_keys source_app,location
&lt;/filter&gt;
&lt;match **&gt;
    @type stdout
&lt;/match&gt;
</code></pre>
<p>But the result is：</p>
<pre><code>{&quot;message&quot;:&quot;{\&quot;source_app\&quot;=&gt;\&quot;client\&quot;, \&quot;location\&quot;=&gt;\&quot;cn\&quot;}&quot;}
</code></pre>
<p>the result is not json type. why? how to config fluentd to make my result as json type?</p>
","transformer-model"
"65543593","Why doesn't the transformer use positional encoding in every layer?","2021-01-02 20:10:20","74775203","2","1692","<machine-learning><artificial-intelligence><transformer-model>","<p>Positional encoding is added to the input before it is passed into the transformer model, because otherwise the attention mechanism would be order invariant. However, both the encoder and decoder are layered, with attention being used on each layer. So if order is important for the attention mechanism, shouldn't the positional encoding be added to the input of each multiheaded attention block, instead of just once at the input to the model?</p>
","transformer-model"
"65543178","Fairseq Transform model not working (Float can't be cast to long)","2021-01-02 19:26:22","65549557","3","2140","<pytorch><conda><transformer-model><fairseq>","<p>I've installed python 3.8, pytorch 1.7, and fairseq 0.10.1, on a new machine, then copied in a script and model from a machine with python 3.6, pytorch 1.4 and fairseq 0.9.0, where it is working.</p>
<p>The model is loaded and prepared with:</p>
<pre><code>model = TransformerModel.from_pretrained(...)
model.eval()
model.cuda()
</code></pre>
<p>Then used with:</p>
<pre><code>inputs = [model.binarize(encode(src, str)) for str in texts]
batched_hypos = model.generate(inputs, beam)
</code></pre>
<p><code>inputs</code> looks like e.g. <code>[tensor([ 116, 1864,  181,    6,    2]), tensor([    5,   432,     7,   2])]</code></p>
<p>It asserts, with the last bit of the call stack being:</p>
<pre><code>  ...
    batched_hypos = model.generate(inputs, beam)
  File &quot;/path/to/fairseq/hub_utils.py&quot;, line 125, in generate
    sample = self._build_sample(tokens)
  File &quot;/path/to/fairseq/hub_utils.py&quot;, line 196, in _build_sample
    assert torch.is_tensor(src_tokens)
</code></pre>
<p>If instead I use <code>fairseq-interactive</code> from the commandline it fails with <code>RuntimeError: result type Float can't be cast to the desired output type Long</code>. (Full stack trace below.)</p>
<p>As using the cli also fails, my hunch is that my model built with fairseq 0.9.x cannot be used with fairseq 0.10.x. If so, is there a way to update the model (i.e. without having to retrain it). And if not, what could the problem be, and how do I fix it?</p>
<p>BTW, exactly the same error if I add <code>--cpu</code> to the commandline args, so the GPU or cuda version can be eliminated as a possible cause.</p>
<pre><code>$ fairseq-interactive path/to/dicts --path models/big.pt --source-lang ja --target-lang en  --remove-bpe sentencepiece


  File &quot;/path/to/bin/fairseq-interactive&quot;, line 11, in &lt;module&gt;
    sys.exit(cli_main())
  File &quot;/path/to/lib/python3.8/site-packages/fairseq_cli/interactive.py&quot;, line 190, in cli_main
    main(args)
  File &quot;/path/to/lib/python3.8/site-packages/fairseq_cli/interactive.py&quot;, line 149, in main
    translations = task.inference_step(generator, models, sample)
  File &quot;/path/to/lib/python3.8/site-packages/fairseq/tasks/fairseq_task.py&quot;, line 265, in inference_step
    return generator.generate(models, sample, prefix_tokens=prefix_tokens)
  File &quot;/path/to/lib/python3.8/site-packages/torch/autograd/grad_mode.py&quot;, line 26, in decorate_context
    return func(*args, **kwargs)
  File &quot;/path/to/lib/python3.8/site-packages/fairseq/sequence_generator.py&quot;, line 113, in generate
    return self._generate(model, sample, **kwargs)
  File &quot;/path/to/lib/python3.8/site-packages/torch/autograd/grad_mode.py&quot;, line 26, in decorate_context
    return func(*args, **kwargs)
  File &quot;/path/to/lib/python3.8/site-packages/fairseq/sequence_generator.py&quot;, line 376, in _generate
    cand_scores, cand_indices, cand_beams = self.search.step(
  File &quot;/path/to/lib/python3.8/site-packages/fairseq/search.py&quot;, line 81, in step
    torch.div(self.indices_buf, vocab_size, out=self.beams_buf)
RuntimeError: result type Float can't be cast to the desired output type Long
</code></pre>
","transformer-model"
"65512421","Add a custom transformer of the predicted vector for regression in sklearn pipeline","2020-12-30 19:16:56","66523050","0","178","<python><scikit-learn><regression><transformer-model>","<p>I have constructed a sklearn pipeline with a preprocessor and a regressor for a regression problem.</p>
<pre><code>Regressor = GradientBoostingRegressor()

Model = Pipeline([(&quot;preprocessor&quot;, xgb_model_preprocessor),
              (&quot;reg&quot;, Regressor)])
</code></pre>
<p>The output of Model.predict(X_test) contains some negative values, but my target Y_test is a positive vector. To increase my score, I want to apply a very simple custom function that returns 0 for every negative prediction. I want to add this into my pipeline directly.</p>
<p>Example :</p>
<pre><code>Model.predict(X_test) = [5 , 10 , 1  , -2  , 8 , -1 ]
</code></pre>
<p>And I want my new pipeline Model_2 so that :</p>
<pre><code>Model_2.predict(X_test) = [5 , 10 , 1  , 0  , 8 , 0 ]
</code></pre>
<p>Can anyone help me achieving this ?</p>
<p>Thank you very much for your help.</p>
","transformer-model"
"65507784","Pytorch, standard layer to convert sequential output to binary?","2020-12-30 13:31:54","65508547","0","223","<python><neural-network><pytorch><sequential><transformer-model>","<p>I am working on a new Pytorch model which takes sequential data as input and I need to output just a single value, which I will then use a binary cross-entropy function to evaluate as a probability of 1 or 0.</p>
<p>To be more concrete, lets say my sequence is 1000 time steps and only 2 dimensions, like a 2-dimensional sine wave, so the data shape would be 1000 x 2.</p>
<p>I have done something like this before using an RNN, which there is a lot of content online. Because of the recurrent structure of the RNN, in order to do this we just look at final output of the RNN after processing the sequence. In this way the the final step output would be 2 dimensions, then we can apply a linear layer to convert 2 -&gt; 1 dimension, et voila, its done.</p>
<p>MY PROBLEM:</p>
<p>What I am attempting to do now is not using a recurrent network, but instead an encoder with attention (Transformer). So the output of the encoder is now still 1000 steps long and whatever my embedded dimension is, likes say 8. So the output of the sequential encoder is shape 1000 x 8. So my issue is that I need to convert this output to a single value, to which I can apply the binary cross-entropy function. I am not finding an obvious way to do this.</p>
<p>IDEAS:</p>
<p>Traditionally with this kind of sequential model, the encoder feeds into a decoder and the decoder can then output a variable length sequence (this is used to language translation problems). My problem is different in that I don't want to output another sequence but just a single value. Maybe I need to convert the decoder in such a way where this works? The decoder usually takes a target value as well as the output from the encoder as input. The output from the decoder then has the same shape as this target value. An idea would be to use the traditional decoder and give a 1 length target, I would then get a 1 length output and I could use a traditional linear layer to convert this to my desired output. However this doesn't seem entirely logical because I really am not interested in outputting a sequence but just 1 value.</p>
<p>Anyways just looking for some more ideas from the community, if you have any. Thanks!</p>
","transformer-model"
"65492844","What happens to the positional encodings in the output of of the Transformer model?","2020-12-29 13:22:43","","0","627","<machine-learning><time-series><artificial-intelligence><transformer-model>","<p>I've been learning about the <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">new popular Transformer model</a>, which can be used for sequence-to-sequence language applications. I am considering an application of time-series modeling, which is not necessarily language modeling. Thus I am modeling where the output layer maybe is not a probability, but could perhaps be a prediction of the next value of the time series.</p>
<p>If I consider the original language model presented in the paper (see Figure 1), we notice that positional encodings are applied to the embedded input data, however there is no indication of a position in the output. The output simply gives probabilities for value at the &quot;next&quot; time step. To me it seems like something is being lost here. The output assumes an iterative process, where the &quot;next&quot; output is just next because it is next. However in the input we feel the need to insert some positional information with the positional encodings. I would think we should also be interested in the positional encodings of the output as well. Is there a way to recover them?</p>
<p>This problem becomes more pronounced if we consider non-uniformly sampled time series data. This is really what I am interested in. It would be interesting to use non-uniformly sampled time series as input and predict the &quot;next&quot; value of the time series, where we also get the time position of that prediction. This comes down to somehow recovering the positional information from that output value. Since the positional encoding of the input is added to the input, it is not trivial how to extract this positional information from the output, perhaps it should be called &quot;positional decoding&quot;.</p>
<p>To sumarize, my question is, what happens to the positional information in the output? Is it still there but I am just missing it? Also, does anyone see a straightforward way of recovering this data if not immediately available by the model?</p>
<p>Thanks</p>
<p><a href=""https://i.sstatic.net/AxHkS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AxHkS.png"" alt=""The Transformer Schematic"" /></a></p>
","transformer-model"
"65424676","Pytorch's nn.TransformerEncoder ""src_key_padding_mask"" not functioning as expected","2020-12-23 12:55:17","66783445","2","4331","<python><nlp><pytorch><transformer-model>","<p>Im working with Pytorch's <code>nn.TransformerEncoder</code> module. I got input samples with (as normal) the shape (<code>batch-size, seq-len, emb-dim</code>). All samples in one batch have been zero-padded to the size of the biggest sample in this batch. Therefore I want the attention of the all zero values to be ignored.</p>
<p>The documentation says, to add an argument <code>src_key_padding_mask</code> to the <code>forward</code> function of the <code>nn.TransformerEncoder</code> module. This mask should be a tensor with shape (<code>batch-size, seq-len</code>) and have for each index either <code>True</code> for the pad-zeros or <code>False</code> for anything else.</p>
<p>I achieved that by doing:</p>
<pre class=""lang-py prettyprint-override""><code>. . .

def forward(self, x):
    # x.size -&gt; i.e.: (200, 28, 200)

    mask = (x == 0).cuda().reshape(x.shape[0], x.shape[1])
    # mask.size -&gt; i.e.: (200, 20)

    x = self.embed(x.type(torch.LongTensor).to(device=device))
    x = self.pe(x)

    x = self.transformer_encoder(x, src_key_padding_mask=mask)

    . . .
</code></pre>
<p>Everything works good when I dont set the <code>src_key_padding_mask</code>. But the error I get when I do is the following:</p>
<pre><code>File &quot;/home/me/.conda/envs/py37/lib/python3.7/site-packages/torch/nn/functional.py&quot;, line 4282, in multi_head_attention_forward
    assert key_padding_mask.size(0) == bsz
AssertionError
</code></pre>
<p>Seems seems like it is comparing the first dimension of the mask, which is the batch-size, with <code>bsz</code> which probably stands for batch-size. But why is it failing then? Help very much appreciated!</p>
","transformer-model"
"65414014","Use multiple softmax in transformers output layer and calculate loss","2020-12-22 18:32:17","","1","227","<python-3.x><pytorch><softmax><transformer-model><seq2seq>","<p>Can I use multiple softmax in the last output layer in transformers? If so, how can I calculate loss from that. I am working in pytorch.</p>
<p>And I am asking because my data is a sequence of tuples where, the elements have different dimensions. Like,</p>
<pre><code>[(2,1), (3,1), (3,1), (2,1), (2,1), (3,1), (3,0), (4,1)]
</code></pre>
<p>The first element of tuples has a vocab of <code>5</code> and the second element of tuples has a vocab of <code>2</code>.</p>
","transformer-model"
"65386351","How do I mask output in transformer model?","2020-12-21 01:06:36","","0","267","<tensorflow><keras><mask><transformer-model>","<p>I am applying the transformer model and I apply padding_mask + look_a_head_mask to the attention layer. But the masks are not propagated to outputs. Is there any way to apply padding_mask when calculating loss?</p>
","transformer-model"
"65359224","predict sequence of tuples using Transformer model","2020-12-18 14:57:43","","2","270","<python-3.x><pytorch><transformer-model><seq2seq>","<p>I am fairly new to seq2seq models and transformers.</p>
<p>Basically I am working on a sequence generation problem. I want to use the transformer. I am using python and pyTorch.</p>
<p>I know how the transformer model works for a sequence generation like given <code>[1,2,3]</code> it can generate <code>[1,2,3,4,5]</code>.
But the problem I am facing is that, in my dataset each point/element in the sequence has 2 attributes. So, the sequence looks like following:</p>
<pre><code>[(2,4), (1,8), (1,9)] 
</code></pre>
<p>and the generated sequence will be:</p>
<pre><code>[(2,4), (1,8), (1,9), (3,1), (2,9)] 
</code></pre>
<p>The first element of each tuple can be between 1-5 and the second element can be between 1-10.</p>
<p>I want to follow the general approach of Transformers like creating embedding for each element, pass it through a decoder block of multihead attention and pointwise feed forward network and finally use softmax to sample out the output.</p>
<p>My question is, as each data point contains 2 values, how do I change the transformer model to work with this kind of sequences? Any direction will be appreciated. Thanks.</p>
","transformer-model"
"65341363","What memory does Transformer Decoder Only use?","2020-12-17 13:08:59","","5","3363","<python><pytorch><decoder><transformer-model><gpt-2>","<p>I've been reading a lot about transformers and self attention and have seen both BERT and GPT-2 are a newer version that only use an encoder transformer (BERT) and decoder transformer (GPT-2). I've been trying to build a decoder only model for myself for next sequence prediction but am confused by one thing. I'm using PyTorch and have looked at there<a href=""https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"" rel=""noreferrer"">Seq2Seq tutorial</a> and then looked into the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#transformerdecoder"" rel=""noreferrer"">Transformer Decoder Block</a> which is made up of <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer"" rel=""noreferrer"">Transformer Decoder Layers</a>. My confusion comes from the memory these need to be passed as well. In the documentation they say memory is the last layer of the encoder block which makes sense for a Seq2Seq model but I'm wanting to make a decoder only model. So my question is what do you pass a decoder only model like GPT-2 for memory if you do not have an encoder?</p>
","transformer-model"
"65262928","Query padding mask and key padding mask in Transformer encoder","2020-12-12 08:19:34","65304255","2","3034","<python><pytorch><transformer-model><attention-model>","<p>I'm implementing self-attention part in transformer encoder using pytorch <code>nn.MultiheadAttention</code> and confusing in the padding masking of transformer.</p>
<p>The following picture shows the self-attention weight of the query (row) and key (column).</p>
<p>As you can see, there are some tokens &quot;&lt;PAD&gt;&quot; and I have already mask it in key. Therefore the tokens will not calculate the attention weight.</p>
<p><a href=""https://i.sstatic.net/GdTEw.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GdTEw.jpg"" alt=""enter image description here"" /></a></p>
<p>There are still two questions:</p>
<ol>
<li><p>In query part, can I also mask them(&quot;&lt;PAD&gt;&quot;) except for the red square part? Is this reasonable?</p>
</li>
<li><p>How can I mask &quot;&lt;PAD&gt;&quot; in the query?</p>
</li>
</ol>
<p>The attention weights also use the <code>softmax</code> function along the row by giving mask in <code>src_mask</code> or <code>src_key_padding_mask</code> argument. If I set all the &quot;&lt;PAD&gt;&quot; row into <code>-inf</code>, the <code>softmax</code> will return <code>nan</code> and the loss with be <code>nan</code></p>
","transformer-model"
"65190217","How to process TransformerEncoderLayer output in pytorch","2020-12-07 22:12:07","","4","3038","<pytorch><bert-language-model><transformer-model>","<p>I am trying to use bio-bert sentence embeddings for text classification of longer pieces of text.</p>
<p>As it currently stands I standardize the number of sentences in each piece of text (some sentences are only comprised of (&quot;[PAD]&quot;) and run each sentence through biobert to get sentence vectors as they do here:
<a href=""https://pypi.org/project/biobert-embedding/"" rel=""nofollow noreferrer"">https://pypi.org/project/biobert-embedding/</a></p>
<p>I then run those embeddings through a TrasnformerEncoder with 8 layers and 16 attention heads.</p>
<p>The TrasnformerEncoder outputs something of shape (batch_size, num_sentences, embedding_size).</p>
<p>I then try to decode this with a Linear layer and map it to my classes (of which there are 7) and softmax the output to get probabilities.</p>
<p>My loss function is simply nn.CrossEntropyLoss().</p>
<p>At first, I summed over dimensions 1 of the TransformerEncoder output to get something of size (batch_size, embedding_size). This invariable led to my network converging on always predicting one of the labels with absolute certainty. Usually the most common label in the dataset.</p>
<p>I then tried only taking the output for the last sentence of the TransformerEncoder output. i.e. TransformerEncoderOutput[:, -1, :].</p>
<p>This resulted in something similar.</p>
<p>I then tried running my Linear layer on each of the outputs of TransformerEncoder output to produce a tensor of size (batch_size, num_sentences, 7). I then sum over dim 1 (makes a tensor of size (batch_size, 7) and softmax as usual. The idea here is that every sentence gets to vote for the label after being informed about its place in the sequence.</p>
<p>This converged even more quickly to just predicting 1 for one of the labels and vanishingly small values for the others.</p>
<p>I feel like I am misunderstanding out to use the output of a pytorch Transformer somehow.
My learning rate is very low, 0.00001, and that helped delay the convergence but it converged eventually anyway.</p>
<p>What this suggests to me is that my network is incapable of figuring anything out about the text and is just learning to find the most common labels. I would guess that this is either a problem with my loss function or a problem with how I am using the Transformer.</p>
<p>Is there a glaring flaw in the architecture that I have laid out?</p>
","transformer-model"
"65167131","transformers: how to use hugging face EncoderDecoderModel to do machine translation task?","2020-12-06 10:47:29","","3","914","<python><pytorch><huggingface-transformers><transformer-model><machine-translation>","<p>I have trained a EncoderDecoderModel from huggging face to do english-German translation task. I tried to overfit a small dataset (100 parallel sentences), and use <code>model.generate()</code> then <code>tokenizer.decode()</code> to perform the translation. However, the output seems to be proper German sentences, but it is definitely not the correct translation.</p>
<p>Here are the code for building the model</p>
<pre><code>encoder_config = BertConfig()
decoder_config = BertConfig()
config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)
model = EncoderDecoderModel(config=config)
</code></pre>
<p>Here are the code for testing the model</p>
<pre><code>model.eval()
input_ids = torch.tensor(tokenizer.encode(input_text)).unsqueeze(0)
output_ids = model.generate(input_ids.to('cuda'), decoder_start_token_id=model.config.decoder.pad_token_id)
output_text = tokenizer.decode(output_ids[0])
</code></pre>
<p>Example input: &quot;iron cement is a ready for use paste which is laid as a fillet by putty knife or finger in the mould edges ( corners ) of the steel ingot mould .&quot;</p>
<p>Ground truth translation: &quot;iron cement ist eine gebrauchs ##AT##-##AT## fertige Paste , die mit einem Spachtel oder den Fingern als Hohlkehle in die Formecken ( Winkel ) der Stahlguss -Kokille aufgetragen wird .&quot;</p>
<p>What the model outputs after trained 100 epochs: &quot;[S] wenn sie den unten stehenden link anklicken, sehen sie ein video uber die erstellung ansprechender illustrationen in quarkxpress&quot; which is totally nonesense.</p>
<p>Where is the problem?</p>
","transformer-model"
"65024801","Input format for BERT fine-tuning on corpus","2020-11-26 15:19:24","","0","822","<python><nlp><bert-language-model><transformer-model>","<p>I want to fine-tune BERT on a specific language domain using the following git repo:</p>
<p><a href=""https://github.com/cedrickchee/pytorch-pretrained-BERT/blob/master/examples/lm_finetuning/README.md"" rel=""nofollow noreferrer"">https://github.com/cedrickchee/pytorch-pretrained-BERT/blob/master/examples/lm_finetuning/README.md</a></p>
<p>Regarding the input format, it says:</p>
<blockquote>
<p>The scripts in this folder expect a single file as input, consisting
of untokenized text, with one sentence per line, and one blank line
between documents. The reason for the sentence splitting is that part
of BERT's training involves a next sentence objective in which the
model must predict whether two sequences of text are contiguous text
from the same document or not, and to avoid making the task too easy,
the split point between the sequences is always at the end of a
sentence. The linebreaks in the file are therefore necessary to mark
the points where the text can be split.</p>
</blockquote>
<p>What do they mean with documents in this regard? From my understanding, the <code>.txt</code> file used for fine-tuning just contains a lot of domain specific text with one sentence per line. Just to be sure, is it the correct approach to use this repository if I want to fine tune BERT on a specific language domain?</p>
<p>Thank you for your help!</p>
","transformer-model"
"65021316","AttributeError: module 'tensorflow._api.v1.initializers' has no attribute 'TruncatedNormal'","2020-11-26 11:35:21","","0","2916","<python-3.x><tensorflow><nlp><transformer-model><sentence-transformers>","<p>I get the following error when I try to import sentence_transformers in <code>python 3.6.7</code> and <code>tensorflow==2.3.0</code>. Can someone please help me with this? Seems like it's a bug in the transformers package.</p>
<pre><code>import sentence_transformers
Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;/usr/local/lib/python3.6/site-packages/sentence_transformers/__init__.py&quot;, line 3, in &lt;module&gt;
    from .datasets import SentencesDataset, SentenceLabelDataset, ParallelSentencesDataset
  File &quot;/usr/local/lib/python3.6/site-packages/sentence_transformers/datasets/__init__.py&quot;, line 1, in &lt;module&gt;
    from .sampler import *
  File &quot;/usr/local/lib/python3.6/site-packages/sentence_transformers/datasets/sampler/__init__.py&quot;, line 1, in &lt;module&gt;
    from .LabelSampler import *
  File &quot;/usr/local/lib/python3.6/site-packages/sentence_transformers/datasets/sampler/LabelSampler.py&quot;, line 6, in &lt;module&gt;
    from ...datasets import SentenceLabelDataset
  File &quot;/usr/local/lib/python3.6/site-packages/sentence_transformers/datasets/SentenceLabelDataset.py&quot;, line 8, in &lt;module&gt;
    from .. import SentenceTransformer
  File &quot;/usr/local/lib/python3.6/site-packages/sentence_transformers/SentenceTransformer.py&quot;, line 11, in &lt;module&gt;
    import transformers
  File &quot;/usr/local/lib/python3.6/site-packages/transformers/__init__.py&quot;, line 124, in &lt;module&gt;
    from .pipelines import (
  File &quot;/usr/local/lib/python3.6/site-packages/transformers/pipelines.py&quot;, line 47, in &lt;module&gt;
    from .modeling_tf_auto import (
  File &quot;/usr/local/lib/python3.6/site-packages/transformers/modeling_tf_auto.py&quot;, line 45, in &lt;module&gt;
    from .modeling_tf_albert import (
  File &quot;/usr/local/lib/python3.6/site-packages/transformers/modeling_tf_albert.py&quot;, line 43, in &lt;module&gt;
    from .modeling_tf_utils import (
  File &quot;/usr/local/lib/python3.6/site-packages/transformers/modeling_tf_utils.py&quot;, line 943, in &lt;module&gt;
    def get_initializer(initializer_range: float = 0.02) -&gt; tf.initializers.TruncatedNormal:
  File &quot;/usr/local/lib64/python3.6/site-packages/tensorflow/python/util/module_wrapper.py&quot;, line 194, in __getattr__
    attr = getattr(self._tfmw_wrapped_module, name)
AttributeError: module 'tensorflow._api.v1.initializers' has no attribute 'TruncatedNormal'
</code></pre>
","transformer-model"
"64984627","The definition of ""heads"" in MultiheadAttention in Pytorch Transformer module","2020-11-24 10:32:49","","2","1636","<pytorch><transformer-model>","<p>I am a bit confused about the definition of Multihead.<br>
Are [1] and [2] below the same?</p>
<p>[1]
My understanding about multiplhead is the multiple attention patterns as below.<br>
<em>&quot;multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder).&quot;</em><br>
<a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">http://jalammar.github.io/illustrated-transformer/</a></p>
<p>But</p>
<p>[2] in class MultiheadAttention(Module): in Pytorch Transformer module,
it seems like embed_dim is DIVIDED by the number of heads.. WHy?</p>
<p>Or... the embed_dim is meant to be the feature dimension times the number of heads in the first place?</p>
<p>self.head_dim = embed_dim // num_heads
assert self.head_dim * num_heads == self.embed_dim, &quot;embed_dim must be divisible by num_heads&quot;</p>
<p><a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py</a></p>
","transformer-model"
"64930736","/srv/share/ykant3/pythia/vector_cache/wiki.en.bin cannot be opened for loading","2020-11-20 13:57:00","","1","630","<python><text><computer-vision><transformer-model>","<p>I want to run pre-trained data at <a href=""https://github.com/yashkant/sam-textvqa"" rel=""nofollow noreferrer"">https://github.com/yashkant/sam-textvqa</a></p>
<p><img src=""https://user-images.githubusercontent.com/41455920/99804206-fb2f0080-2b7d-11eb-9381-ce13df4bfd9b.png"" alt=""image"" /></p>
<p><img src=""https://user-images.githubusercontent.com/41455920/99803443-cec6b480-2b7c-11eb-8461-bbb7e848d354.png"" alt=""image"" /></p>
","transformer-model"
"64881638","Transformer PyTorch Error- ValueError: too many values to unpack (expected 2)","2020-11-17 19:00:08","","0","1415","<python><pytorch><transformer-model>","<p>I am having issues getting my model to run. I am not sure which model to use in the translate_sentence function, I have tried model.transformer, model.encoder_de, etc. It is based off of the Transformer class and the forward() function I believe but I am getting a type error. These are the directions:</p>
<ol>
<li><p>As in the forward(self, src, tgt) function of the
TransformerModel class, you need to create the appropriate
mask and encode the source sentence (just once).</p>
</li>
<li><p>You also need to create the appropriate mask and encode the
output sentence for sequential predictions. Unlike the source,
for every iteration, you need to re-encode the previous output and
pass both the source sentence and previous output into the
Transformer.</p>
</li>
</ol>
<pre><code>from torch.nn import Transformer
class TransformerModel(nn.Module):

    def __init__(self, ntoken_in, ntoken_out, ninp, nhead, npf_dim, nlayers, src_pad_idx, trg_pad_idx, dropout=0.5):
        super(TransformerModel, self).__init__()

        # --------------- param -----------------
        # ntoken_in: the idx of the input word after tokenization 
        # ntoken_out: the idx of the input word w.r.t. the tokenization 
        # ninp: the number of expected features in the encoder/decoder inputs 
        # nhead: the number of multiAttention heads 
        # npf_dim: the dimension of the feedforward layer 
        # src_pad_idx: the token for padding in source language
        # trg_pad_idx: the token for padding in target language 
        # ----------------------------------------

        self.model_type = 'Transformer'
        self.pos_encoder = PositionalEncoding(ninp, dropout)
        self.transformer = Transformer(d_model=ninp, nhead=nhead, num_encoder_layers=nlayers, num_decoder_layers=nlayers,
                                       dim_feedforward=npf_dim, dropout=dropout, activation='relu')
      
        self.encoder_en = nn.Embedding(ntoken_in, ninp)  # tok_embedding for input 
        self.encoder_de = nn.Embedding(ntoken_out, ninp) # tok_embedding for output 
        self.ninp = ninp
        self.decoder = nn.Linear(ninp, ntoken_out)

        self.src_pad_idx = src_pad_idx
        self.tgt_pad_idx = trg_pad_idx

        self.init_weights()

    def _generate_src_key_mask(self, src):
        # for key_padding_mask in transformer
        # the positions with the value of True will be ignored while the position
        # with the value of False will be unchanged. We mask all padding words. 
        # The output dim is b*s
        src_mask = (src == self.src_pad_idx)
        return src_mask.T

    def _generate_tgt_mask(self, tgt, sz):
        # Beside key_padding_mask in transformer, the output or teacher input 
        # should be masked sequentially to prevent the model get any information 
        # from the future words it is going to predict 
        tgt_key_mask = tgt == self.tgt_pad_idx

        # We provide FloatTensor attn_mask. It will be added to the attention weight.
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        attn_mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(tgt.device)
        return attn_mask, tgt_key_mask.T

    def init_weights(self):
        initrange = 0.1
        self.encoder_en.weight.data.uniform_(-initrange, initrange)
        self.encoder_de.weight.data.uniform_(-initrange, initrange)
        self.decoder.bias.data.zero_()
        self.decoder.weight.data.uniform_(-initrange, initrange)

    def forward(self, src, tgt):
        # src
        src_key_mask = self._generate_src_key_mask(src)
        src = self.encoder_en(src) * math.sqrt(self.ninp)  # use a learned encoder put stoi index to a feature space s*b --&gt; s*b*e
        src = self.pos_encoder(src)  # add the pos feature toward feature space

        # tgt
        tgt_mask, tgt_key_mask = self._generate_tgt_mask(tgt, tgt.size(0))
        tgt = self.encoder_de(tgt) * math.sqrt(self.ninp) 
        tgt = self.pos_encoder(tgt)

        output = self.transformer(src, tgt, tgt_mask=tgt_mask, 
                                  src_key_padding_mask = src_key_mask, 
                                  tgt_key_padding_mask = tgt_key_mask)
        output = self.decoder(output)
        return output

class PositionalEncoding(nn.Module):
    # The positional encoding as described in the paper 
    # https://arxiv.org/pdf/1706.03762.pdf
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

# Here we intialize our model
INPUT_DIM = len(SRC.vocab)
OUTPUT_DIM = len(TRG.vocab)
print(INPUT_DIM, OUTPUT_DIM)

HID_DIM = 256
N_LAYERS = 3
N_HEADS = 8
N_PF_DIM = 512
DROPOUT = 0.1

SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]
TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]

model =TransformerModel(ntoken_in = INPUT_DIM, ntoken_out=OUTPUT_DIM, ninp=HID_DIM, 
                        nhead=N_HEADS, npf_dim=N_PF_DIM, nlayers=N_LAYERS,
                        src_pad_idx=SRC_PAD_IDX, trg_pad_idx=TRG_PAD_IDX, dropout=DROPOUT).to(device)

def count_parameters(model: nn.Module):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f'The model has {count_parameters(model):,} trainable parameters')

def initialize_weights(m):
    if hasattr(m, 'weight') and m.weight.dim() &gt; 1:
        nn.init.xavier_uniform_(m.weight.data)

model.apply(initialize_weights)


----------
def translate_sentence(sentence, src_field, trg_field, model, device, max_len = 50):

    model.eval()
        
    if isinstance(sentence, str):
        nlp = spacy.load('de')
        tokens = [token.text.lower() for token in nlp(sentence)]
    else:
        tokens = [token.lower() for token in sentence]

    #tokens = [src_field.init_token] + tokens + [src_field.eos_token]  
    src_indexes = [src_field.vocab.stoi[token] for token in tokens]
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)

    with torch.no_grad():
        #model.?
        hidden, cell = model.encoder_en(src_tensor)

    # create a list to hold the output sentence, initialized with an &lt;sos&gt; token   
    
    trg_indexes = [trg_field.vocab.stoi[trg_field.init_token]]
    
    for i in range(max_len):

        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1).to(device)
        with torch.no_grad():
            #model.?
            output, hidden, cell = model.encoder_de(trg_tensor, hidden, cell)
            
        pred_token = output.argmax(1).item()
        
        trg_indexes.append(pred_token)

        if pred_token == trg_field.vocab.stoi[trg_field.eos_token]:
            break
    
    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes]
    
    return trg_tokens[1:]


----------
#getting error here

example_idx = 18

src = vars(train_data.examples[example_idx])['src']
trg = vars(train_data.examples[example_idx])['trg']

print(f'src = {src}')
print(f'trg = {trg}')

translation = translate_sentence(src, TRG, SRC, model, device)

print(f'predicted trg = {translation}')


</code></pre>
","transformer-model"
"64876788","How to use the PyTorch Transformer with multi-dimensional sequence-to-seqence?","2020-11-17 14:03:01","66519480","9","2424","<python><machine-learning><pytorch><transformer-model><sequence-to-sequence>","<p>I'm trying to go <code>seq2seq</code> with a Transformer model. My input and output are the same shape (<code>torch.Size([499, 128])</code> where 499 is the sequence length and 128 is the number of features.</p>
<p>My input looks like:
<a href=""https://i.sstatic.net/90IRq.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/90IRq.png"" alt=""enter image description here"" /></a></p>
<p>My output looks like:
<a href=""https://i.sstatic.net/C0qAY.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/C0qAY.png"" alt=""enter image description here"" /></a></p>
<p>My training loop is:</p>
<pre><code>    for batch in tqdm(dataset):
        optimizer.zero_grad()
        x, y = batch

        x = x.to(DEVICE)
        y = y.to(DEVICE)

        pred = model(x, torch.zeros(x.size()).to(DEVICE))

        loss = loss_fn(pred, y)
        loss.backward()
        optimizer.step()
</code></pre>
<p>My model is:</p>
<pre><code>import math
from typing import final
import torch
import torch.nn as nn

class Reconstructor(nn.Module):
    def __init__(self, input_dim, output_dim, dim_embedding, num_layers=4, nhead=8, dim_feedforward=2048, dropout=0.5):
        super(Reconstructor, self).__init__()

        self.model_type = 'Transformer'
        self.src_mask = None
        self.pos_encoder = PositionalEncoding(d_model=dim_embedding, dropout=dropout)
        self.transformer = nn.Transformer(d_model=dim_embedding, nhead=nhead, dim_feedforward=dim_feedforward, num_encoder_layers=num_layers, num_decoder_layers=num_layers)
        self.decoder = nn.Linear(dim_embedding, output_dim)
        self.decoder_act_fn = nn.PReLU()

        self.init_weights()

    def init_weights(self):
        initrange = 0.1
        nn.init.zeros_(self.decoder.weight)
        nn.init.uniform_(self.decoder.weight, -initrange, initrange)

    def forward(self, src, tgt):

        pe_src = self.pos_encoder(src.permute(1, 0, 2))  # (seq, batch, features)
        transformer_output = self.transformer_encoder(pe_src)
        decoder_output = self.decoder(transformer_output.permute(1, 0, 2)).squeeze(2)
        decoder_output = self.decoder_act_fn(decoder_output)
        return decoder_output
</code></pre>
<p>My output has a shape of <code>torch.Size([32, 499, 128])</code> where <code>32</code> is batch, <code>499</code> is my sequence length and <code>128</code> is the number of features. But the output has the same values:</p>
<pre><code>tensor([[[0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017],
         [0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017],
         [0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017],
         ...,
         [0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017],
         [0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017],
         [0.0014, 0.0016, 0.0017,  ..., 0.0018, 0.0021, 0.0017]]],
       grad_fn=&lt;PreluBackward&gt;)
</code></pre>
<p>What am I doing wrong? Thank you so much for any help.</p>
","transformer-model"
"64839614","How to get reproducible results of T5 transformer model","2020-11-14 23:13:13","","1","2394","<python><nlp><torch><huggingface-transformers><transformer-model>","<p>I'm trying to get reproducible results of T5 transformer model:</p>
<pre><code>import torch
from transformers import T5ForConditionalGeneration,T5Tokenizer


def set_seed(seed):
  torch.manual_seed(seed)
  if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

set_seed(42)

t5model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_paraphraser')
tokenizer = T5Tokenizer.from_pretrained('t5-base')

device = torch.device(&quot;cpu&quot;)
print (&quot;device &quot;,device)
t5model = t5model.to(device)

max_len = 256

text =  &quot;paraphrase: &quot; + txt + &quot; &lt;/s&gt;&quot;

encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=&quot;pt&quot;)
input_ids, attention_masks = encoding[&quot;input_ids&quot;].to(device), encoding[&quot;attention_mask&quot;].to(device)

beam_outputs = t5model.generate(
    input_ids=input_ids, attention_mask=attention_masks,
    do_sample=True,
    max_length=max_len,
    top_k=50,
    top_p=0.98,
    early_stopping=True,
    num_return_sequences=10,
)
</code></pre>
<p>Though I set a seed number, <code>t5model.generate</code> gives me different results each time I run it.</p>
<p>What is the right way to set the seed number, in order to get the same results of <code>t5model.generate</code> after multiple executions?</p>
","transformer-model"
"64799622","How is the GPT's masked-self-attention is utilized on fine-tuning/inference","2020-11-12 07:31:29","64800837","1","4156","<nlp><transformer-model><large-language-model>","<p>At training time, as far as I understand from the &quot;Attention is all you need&quot; paper, the way that masked-self-attention is used in the decoder is by feeding the output sequence multiple times, each time removing the mask from the next token.</p>
<p>Q1. At inference time, the expected output sequence length is not known. How do you decide on how many masked tokens to add? Do you always fill the max-length of your input with masked tokens and stop when an end of sequence symbol is predicted?</p>
<p>Q2. The GPT inference objective task is a little different. A &quot;query&quot; vector is injected to the model (for example [text1;text2] and [text2;text1] in the similarity task). How is the masking used in this scenario? I would expect that the whole sequence will be injected in only one step with no masking, however this contradicts the masked-self-attention methodology.</p>
","transformer-model"
"64789217","How can I do a seq2seq task with PyTorch Transformers if I am not trying to be autoregressive?","2020-11-11 15:20:25","66636234","10","1236","<python><pytorch><transformer-model>","<p>I may be mistaken, but it seems that PyTorch Transformers are autoregressive, which is what masking is for. However, I've seen some implementations where people use just the Encoder and output that directly to a <code>Linear</code> layer.</p>
<p>In my case, I'm trying to convert a spectrogram (rows are frequencies and columns are timesteps) to another spectrogram of the same dimensions. I'm having an impossible time trying to figure out how to do this.</p>
<p>For my model, I have:</p>
<pre><code>class TransformerReconstruct(nn.Module):
    def __init__(self, feature_size=250, num_layers=1, dropout=0.1, nhead=10, output_dim=1):
        super(TransformerReconstruct, self).__init__()
        self.model_type = 'Transformer'

        self.src_mask = None
        self.pos_encoder = PositionalEncoding(feature_size)
        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=nhead, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)
        self.decoder = nn.Linear(feature_size, output_dim)
        self.init_weights()

    def init_weights(self):
        initrange = 0.1
        self.decoder.bias.data.zero_()
        self.decoder.weight.data.uniform_(-initrange, initrange)

    def forward(self, src):
        if self.src_mask is None or self.src_mask.size(0) != len(src):
            device = src.device
            mask = self._generate_square_subsequent_mask(len(src)).to(device)
            self.src_mask = mask

        src = self.pos_encoder(src)
        output = self.transformer_encoder(src, self.src_mask)
        output = self.decoder(output)
        return output

    def _generate_square_subsequent_mask(self, sz):
        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask
</code></pre>
<p>And when training, I have:</p>
<pre><code>model = TransformerReconstruct(feature_size=128, nhead=8, output_dim=128, num_layers=6).to(device)
</code></pre>
<p>This returns the right shape, but doesn't seem to learn.</p>
<p>My basic training loop looks like:</p>
<pre><code>for i in range(0, len(data_source) - 1, input_window):
  data, target = get_batch(data_source, i, 1)
  output = recreate_model(data)
</code></pre>
<p>and I'm using an <code>MSELoss</code> and I'm trying to learn a very simple identity. Where the input and output are the same, however this is not learning. What could I be doing wrong? Thanks in advance.</p>
","transformer-model"
"64769430","Keras loadmodel for custom model with custom layers - Transformer documentation example","2020-11-10 13:01:55","","2","1176","<python><tensorflow><keras><serialization><transformer-model>","<p>I am running the following example:</p>
<p><a href=""https://keras.io/examples/nlp/text_classification_with_transformer/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/text_classification_with_transformer/</a></p>
<p>I have created and trained a model as described, and it works nicely:</p>
<pre><code>inputs = layers.Input(shape=(maxlen,))
embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)
x = embedding_layer(inputs)
transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
x = transformer_block(x,training=True)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.1)(x)
x = layers.Dense(20, activation=&quot;relu&quot;)(x)
x = layers.Dropout(0.1)(x)
outputs = layers.Dense(2, activation=&quot;softmax&quot;)(x)

model = keras.Model(inputs=inputs, outputs=outputs)


&quot;&quot;&quot;
## Train and Evaluate
&quot;&quot;&quot;

model.compile(&quot;adam&quot;, &quot;sparse_categorical_crossentropy&quot;, metrics=[&quot;accuracy&quot;])
history = model.fit(
    x_train, y_train, batch_size=1024, epochs=1, validation_data=(x_val, y_val)
)

model.save('SPAM.h5')
</code></pre>
<p>How to save and load such custom model correctly in Keras?</p>
<p>I have tried</p>
<pre><code> best_model=tf.keras.models.load_model('SPAM.h5')
 ValueError: Unknown layer: TokenAndPositionEmbedding
</code></pre>
<p>but the model seems to miss the custom Layers. However the following also does not work</p>
<pre><code>best_model=tf.keras.models.load_model('SPAM.h5',custom_objects={&quot;TokenAndPositionEmbedding&quot;: TokenAndPositionEmbedding()})
 
TypeError: __init__() missing 3 required positional arguments:
 'maxlen', 'vocab_size', and 'embed_dim'
</code></pre>
<p>also passing the class does not solve.</p>
<pre><code>best_model=tf.keras.models.load_model('SPAM.h5',
 custom_objects={&quot;TokenAndPositionEmbedding&quot;: TokenAndPositionEmbedding})
 TypeError: __init__() got an unexpected keyword argument 'name'



 best_model=tf.keras.models.load_model('SPAM.h5',
{&quot;TokenAndPositionEmbedding&quot;:
TokenAndPositionEmbedding,'TransformerBlock':TransformerBlock,
'MultiHeadSelfAttention':MultiHeadSelfAttention})
</code></pre>
","transformer-model"
"64739779","Cannot import name ""is_sklearn_available"" from ""transformers.data""","2020-11-08 15:26:53","","2","2262","<python><scikit-learn><transformer-model>","<p>I installed the package sentence_transformers via conda. It was successful; however, I cannot load them in jupyter notebook. I kept receiving the error as below. I have already upgraded all the relevant packages (sklearn, scipy, etc.) and I still receive the error.</p>
<pre><code>from sentence_transformers import SentenceTransformer

ImportError: cannot import name 'is_sklearn_available' from 'transformers.data' (/Users/KK/opt/anaconda3/lib/python3.7/site-packages/transformers/data/__init__.py)
</code></pre>
<p>Any suggestions would be greatly appreciated.</p>
","transformer-model"
"64719591","download the following model: distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es","2020-11-06 18:06:19","64733576","0","460","<bert-language-model><transformer-model>","<p>I have two PCs: one of them has an internet connection and the other PC does not have an internet connection, I need to download the following model: <code>distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es</code>, but not I find the link.</p>
<p>I do not have and i cannot install python on the pc with internet access, but I can use wget.</p>
<p>Where i can download the following model: <code>distill-bert-base-spanish-wwm-cased-finetuned-spa-squad2-es</code>?</p>
","transformer-model"
"64704428","Read CSV, Change 2 Columns using pyproj Transform and save to new CSV","2020-11-05 19:54:41","64707363","0","384","<python><csv><latitude-longitude><transformer-model><pyproj>","<p>I'm very new to Python so apologies for my lack of understanding.</p>
<p>I need to read in 2 columns (latitude &amp; longitude) of a 4 column CSV file.
<strong>Example below.</strong></p>
<p>ShopName    Misc    latitude    longitude
XXX        3       999999      999999</p>
<p>I then have to change the latitude and longitude using a pyproj transform scrypt that I have checked. I then need to save the tranformed latitude and longitude data into a new csv such that the column format is the same as the existing csv.
<strong>Example below.</strong></p>
<p>ShopName    Misc    latitude    longitude
XXX        3       49.12124    -2.32131</p>
<p>I'm a bit lost but this is where I got to. Thank you in advance</p>
<pre><code>import csv
from pyproj import Transformer

#2.2 Define function
def transformer = Transformer.from_crs(&quot;epsg:12345&quot;, &quot;epsg:9999&quot;)
    result = transformer.transform(old_longitude, old_latitude)
    return new_longitude, new latitude

#2.3 Set destination file to variable
with open('new.csv' ,'w') as csv_new2:

#2.4 Instruct write method to new file    
    fileWriter2 = csv.writer(csv_new2)
    
#2.5 Set existing file to variable
    with open('old.csv','r') as csv_old2:

#2.6 Instruct read method to new file
        fileReader2 = csv.reader(csv_old2)

        for row in fileReader2:
</code></pre>
","transformer-model"
"64702885","How to use pipeline from transformers, summarization? Python","2020-11-05 18:04:26","64710897","-1","681","<python><pipeline><huggingface-transformers><transformer-model>","<p>I am trying to use pipeline from transformers to summarize the text. But what I can get is only truncated text from original one.
My code is:</p>
<pre><code>from transformers import pipeline
summarizer = pipeline(&quot;summarization&quot;)
summarizer(&quot;The present invention discloses a pharmaceutical composition comprising therapeutically effective amount of, or an extract consisting essentially therapeutically effective amount of at least one cannabinoid selected from the group consisting of: Cannabidiol (CBD) or a derivative thereof, Tetrahydrocannabinol (THC) or a derivative thereof, and any combination thereof, for use in the treatment of multiple myeloma (MM). The present invention further discloses methods and uses of the aforementioned composition.&quot;, 
       min_length=5, max_length=10)
</code></pre>
<p>The output is</p>
<pre><code>[{'summary_text': ' The present invention discloses a pharmaceutical'}]
</code></pre>
<p>That is just a beginning of the text for analysis. What I am doing wrong?</p>
","transformer-model"
"64583794","How to get translations of one batch of sentences after batch_encode_plus?","2020-10-29 01:47:21","","2","1375","<nlp><translation><huggingface-transformers><transformer-model>","<p>I want to get translations of one batch of sentences using pretrained model.</p>
<pre><code>model = AutoModelWithLMHead.from_pretrained(&quot;Helsinki-NLP/opus-mt-es-en&quot;)
tokenizer = AutoTokenizer.from_pretrained(&quot;Helsinki-NLP/opus-mt-es-en&quot;)
batch_input_str = ((&quot;Mary spends $20 on pizza&quot;), (&quot;She likes eating it&quot;), (&quot;The pizza was great&quot;))
encoded = (tokenizer.batch_encode_plus(batch_input_str, pad_to_max_length=True))
</code></pre>
<p>The <code>encoded</code>is like:</p>
<pre><code>{'input_ids': [[4963, 10154, 5021, 9, 25, 1326, 2255, 35, 17462, 0], [552, 3996, 2274, 9, 129, 75, 2223, 25, 1370, 0], [42, 17462, 12378, 9, 25, 5807, 1949, 0, 65000, 65000]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]}
</code></pre>
<p>Then, should I just pass the <code>encoded</code> to</p>
<pre><code>output = model.generate(a)
</code></pre>
<p>And then use</p>
<pre><code>res = tokenizer.decode(output)
</code></pre>
<p>?</p>
<p>Thanks!</p>
","transformer-model"
"65137085","ML Code throws value error when transforming data","2020-10-28 17:29:09","","0","23","<python><scikit-learn><numpy><encoding><transformer-model>","<p>Data source can be found <a href=""https://drive.google.com/file/d/14-VaJqtkAy4A0BfEwsHhLKfn887Z-7hF/view?usp=sharing"" rel=""nofollow noreferrer"">here</a>.</p>
<p>I've hit a stumbling block in some code I'm writing because the fit_transform method continuously fails. It throws this error:</p>
<pre><code>Traceback (most recent call last):

  File &quot;/home/user/Datasets/CSVs/Working/Playstore/untitled0.py&quot;, line 18, in &lt;module&gt;
    data = data[oh_cols].apply(oh.fit_transform)

  File &quot;/usr/lib/python3.8/site-packages/pandas/core/frame.py&quot;, line 7547, in apply
    return op.get_result()

  File &quot;/usr/lib/python3.8/site-packages/pandas/core/apply.py&quot;, line 180, in get_result
    return self.apply_standard()

  File &quot;/usr/lib/python3.8/site-packages/pandas/core/apply.py&quot;, line 255, in apply_standard
    results, res_index = self.apply_series_generator()

  File &quot;/usr/lib/python3.8/site-packages/pandas/core/apply.py&quot;, line 284, in apply_series_generator
    results[i] = self.f(v)

  File &quot;/usr/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py&quot;, line 410, in fit_transform
    return super().fit_transform(X, y)

  File &quot;/usr/lib/python3.8/site-packages/sklearn/base.py&quot;, line 690, in fit_transform
    return self.fit(X, **fit_params).transform(X)

  File &quot;/usr/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py&quot;, line 385, in fit
    self._fit(X, handle_unknown=self.handle_unknown)

  File &quot;/usr/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py&quot;, line 74, in _fit
    X_list, n_samples, n_features = self._check_X(X)

  File &quot;/usr/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py&quot;, line 43, in _check_X
    X_temp = check_array(X, dtype=None)

  File &quot;/usr/lib/python3.8/site-packages/sklearn/utils/validation.py&quot;, line 73, in inner_f
    return f(**kwargs)

  File &quot;/usr/lib/python3.8/site-packages/sklearn/utils/validation.py&quot;, line 620, in check_array
    raise ValueError(

ValueError: Expected 2D array, got 1D array instead:
array=['Everyone' 'Everyone' 'Everyone' ... 'Everyone' 'Mature 17+' 'Everyone'].
Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>
<p>I've done some searching on this online and arrived at a few potential solutions, but they didn't seem to work.</p>
<p>Here's my code:</p>
<pre><code>import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from category_encoders import CatBoostEncoder,CountEncoder,TargetEncoder

data = pd.read_csv(&quot;/home/user/Datasets/CSVs/Working/Playstore/data.csv&quot;)


oh = OneHotEncoder()
cb = CatBoostEncoder()
ce = CountEncoder()
te = TargetEncoder()

obj = [i for i in data if data[i].dtypes==&quot;object&quot;]
unique = dict(zip(list(obj),[len(data[i].unique()) for i in obj]))
oh_cols = [i for i in unique if unique[i] &lt; 100]
te_cols = [i for i in unique if unique[i] &gt; 100]

data = data[oh_cols].apply(oh.fit_transform)
</code></pre>
<p>It throws the aforementioned error. A solution I saw advised me to use <code>.values</code> when transforming the data and I tried the following:</p>
<p><code>data = data[oh_cols].values.apply(oh.fit_transform)</code></p>
<p><code>data = data[oh_cols].apply(oh.fit_transform).values</code></p>
<pre><code>encoding = np.array(data[oh_cols])
encoding.apply(oh.fit_transform)
</code></pre>
<p>The first and the third threw the same error which is below,:</p>
<pre><code>AttributeError: 'numpy.ndarray' object has no attribute 'apply'
</code></pre>
<p>While the second threw the first error I mentioned again:</p>
<pre><code>ValueError: Expected 2D array, got 1D array instead:
</code></pre>
<p>I'm honestly stumped and I'm not sure where to go from here. The Kaggle exercise I learnt this from went smoothly, but for some reason things never do when I try my hand at things myself.</p>
","transformer-model"
"64572097","Cause of Exploding NLLLoss","2020-10-27 06:30:34","","1","620","<pytorch><transformer-model>","<p>I have been trying to make Transformer based language model, for the loss function Negative Log-likelihood is implemented. For some reason, after a few iterations, there is a steep increase in reconstruction loss.
Here is the log generated by the training program</p>
<pre><code>root - WARNING - Loss: 203.81146240234375
root - WARNING - Loss: 124.32596588134766
root - WARNING - Loss: 62.59440612792969
root - WARNING - Loss: 59.84109115600586
root - WARNING - Loss: 59.247005462646484
root - WARNING - Loss: 48.832725524902344
root - WARNING - Loss: 57.592288970947266
root - WARNING - Loss: 50.18443298339844
root - WARNING - Loss: 46.474849700927734
root - WARNING - Loss: 52.12908172607422
root - WARNING - Loss: 50.090736389160156
root - WARNING - Loss: 66.04253387451172
root - WARNING - Loss: 49.094024658203125
root - WARNING - Loss: 36.69044494628906
root - WARNING - Loss: 48.54591369628906
root - WARNING - Loss: 60.71137237548828
root - WARNING - Loss: 40.35478591918945
root - WARNING - Loss: 49.070556640625
root - WARNING - Loss: 54.33742141723633
root - WARNING - Loss: 47.14014434814453
root - WARNING - Loss: 55.043060302734375
root - WARNING - Loss: 47.63726043701172
root - WARNING - Loss: 46.314571380615234
root - WARNING - Loss: 41.330291748046875
root - WARNING - Loss: 48.85242462158203
root - WARNING - Loss: 50.59345245361328
root - WARNING - Loss: 48.508975982666016
root - WARNING - Loss: 43.35681915283203
root - WARNING - Loss: 45.875431060791016
root - WARNING - Loss: 51.701438903808594
root - WARNING - Loss: 39.1783561706543
root - WARNING - Loss: 30.14274024963379
root - WARNING - Loss: 44.33928680419922
root - WARNING - Loss: 40.88005447387695
root - WARNING - Loss: 62.682804107666016
root - WARNING - Loss: 45.18329620361328
root - WARNING - Loss: 39.7137451171875
root - WARNING - Loss: 47.31813049316406
root - WARNING - Loss: 50.755348205566406
root - WARNING - Loss: 40.52918243408203
root - WARNING - Loss: 49.48160934448242
root - WARNING - Loss: 58.29778289794922
root - WARNING - Loss: 45.660675048828125
root - WARNING - Loss: 55.13115692138672
root - WARNING - Loss: 50.72150421142578
root - WARNING - Loss: 33.377098083496094
root - WARNING - Loss: 48.404151916503906
root - WARNING - Loss: 60.24494934082031
root - WARNING - Loss: 46.290470123291016
root - WARNING - Loss: 9.493173539216099e+24
</code></pre>
<p>After the weight updation of the last iteration, the loss becomes <code>nan</code> from that point forth. What are the possible reasons for this to occur ?</p>
","transformer-model"
"64532940","Multi-Head attention layers - what is a warpper multi-head layer in Keras?","2020-10-26 07:25:57","","4","7928","<tensorflow><keras><deep-learning><transformer-model><attention-model>","<p>I am new to attention mechanisms and I want to learn more about it by doing some practical examples. I came across a Keras implementation for multi-head attention found it in this website <a href=""https://pypi.org/project/keras-multi-head/#description"" rel=""nofollow noreferrer"">Pypi keras multi-head</a>. I found two different ways to implement it in Keras.</p>
<ol>
<li>One way is to use a multi-head attention as a keras wrapper layer with either LSTM or CNN.
This is a snippet of implementating multi-head as a wrapper layer with LSTM in Keras. This example is taken from this website <a href=""https://pypi.org/project/keras-multi-head/#description"" rel=""nofollow noreferrer"">keras multi-head</a>&quot;</li>
</ol>
<pre><code>import keras
from keras_multi_head import MultiHead

model = keras.models.Sequential()
model.add(keras.layers.Embedding(input_dim=100, output_dim=20, name='Embedding'))
model.add(MultiHead(keras.layers.LSTM(units=64), layer_num=3, name='Multi-LSTMs'))
model.add(keras.layers.Flatten(name='Flatten'))
model.add(keras.layers.Dense(units=4, activation='softmax', name='Dense'))
model.build()
model.summary()
</code></pre>
<ol start=""2"">
<li>The other way is to use it separately as a stand-alone layer.
This is a snippet of the second implementation for multi-head as stand-alone laye, also taken from <a href=""https://pypi.org/project/keras-multi-head/#description"" rel=""nofollow noreferrer"">keras multi-head</a>&quot;</li>
</ol>
<pre><code>import keras
from keras_multi_head import MultiHeadAttention

input_layer = keras.layers.Input( shape=(2, 3), name='Input',)
att_layer = MultiHeadAttention( head_num=3, name='Multi-Head',)(input_layer)
model = keras.models.Model(inputs=input_layer, outputs=att_layer)
model.compile( optimizer='adam', loss='mse', metrics={},)
</code></pre>
<p>I have been trying to find some documents that explain this but I have not found yet.</p>
<p><strong>Update:</strong></p>
<p>What I have found was that the second implementation (MultiHeadAttention) is more like the Transformer paper &quot;<a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention All You Need</a>&quot;. However, I am still struggling to understand the first implementation which is the wrapper layer.</p>
<p>Does the first one (as a wrapper layer) would combine the output of multi-head with LSTM?.</p>
<p>I was wondering if someone could explain the idea behind them, especially, the wrapper layer.</p>
","transformer-model"
"64481106","Is there any way to self create Transformer to run on Coral board?","2020-10-22 11:17:37","","3","959","<tensorflow><deep-learning><tensorflow-lite><transformer-model><google-coral>","<p>I want to use a transformer applied on Coral Board edge device. But I cant find any source that apply this.</p>
<p>The transformer can be trained on CloudTPU, but I am not sured the op can operate on Coral board. If I have to self write ops, please tell me how.</p>
","transformer-model"
"64382393","Using Transformer's decoder to extract sentences","2020-10-16 03:10:42","","0","375","<neural-network><pytorch><recurrent-neural-network><huggingface-transformers><transformer-model>","<p>My question relates directly to the text summarization task. I know that there are a bunch of implementations with RNN networks (especially LSTMs) that use sentence-level attention to extract salient sentences of the source, using an attentive LSTM decoder. I have been digging into it to see if this is possible with the Transformer's networks, specifically the Transformer's decoder part, but do not really have an idea how to get this incorporated.</p>
<p>Look, for example, in terms of LSTM decoder, the LSTM encoder can produce contextualized encodings for the sentences that are in the source, then the last hidden state is passed to the LSTM decoder, and <em>at each decoding timestep</em>, the decoder attends to the source sentences (encoded by the encoder) to get an attention score over those. Finally, these scores and the sentence hidden states are combined to form the context vector, which is further processed with the decoder hidden state to predict the right sentence to pick up.</p>
<p>Assume that I have obtained the sentence encodings using Transformer's encoder, I'm just wondering how I can relate the scenario happening in LSTM network's decoder part to the Transformer's network decoder side.</p>
<p>Also, a question, how are these networks (both LSTM and Transformer's) that use sentence-level attention instead of word-level attention trained?</p>
<hr />
<p>Update: the behaviour that I intended to achieve is as follows: I want the Transformer's decoder gets in sentences (instead of tokens which is then regarded as abstractive summarization), compute attention on the source sentences considering the partial summary that has been selected in prior timesteps, and then give a probability distribution over the source sentences denoting how much it is probable that a sentence is being copied into the target. So to make it explicit, I'm looking for an extractive summarizer with decoder.</p>
","transformer-model"
"64356583","Transformer Image captioning model produces just padding rather than a caption","2020-10-14 15:24:07","","1","757","<python><neural-network><pytorch><resnet><transformer-model>","<p>I am trying to produce a model that will produce a caption for an image using resnet as the encoder, transformer as the decoder and COCO as the database.</p>
<p>After training my model for 10 epochs, my model failed to produce anything other than the word <code>&lt;pad&gt;</code> which implies that the only result after going through the model only produced tokens of 0 which corresponds to <code>&lt;pad&gt;</code>.</p>
<p>After using the debugger it seems that the error occurs at the argmax, where the output just becomes zero rather than anything else, but I don't know how to fix it, is it an issue with my model, or the way it is trained?</p>
<p>I based my model off of <a href=""https://github.com/EthanCDD/ImageCaptioning/blob/master/ImageCaptioning.ipynb"" rel=""nofollow noreferrer"">this</a> github if it helps.</p>
<p>The script to download the COCO model is here:</p>
<p>Download.sh</p>
<pre><code>mkdir data
wget http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip -P ./data/
wget http://images.cocodataset.org/zips/train2014.zip -P ./data/
wget http://images.cocodataset.org/zips/val2014.zip -P ./data/

unzip ./data/captions_train-val2014.zip -d ./data/
rm ./data/captions_train-val2014.zip
unzip ./data/train2014.zip -d ./data/
rm ./data/train2014.zip 
unzip ./data/val2014.zip -d ./data/ 
rm ./data/val2014.zip 
</code></pre>
<p>Any help is much appreciated.</p>
<p>Here is my code:</p>
<p>model.py</p>
<pre><code>import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math, copy, time
import torchvision.models as models
from torch.nn import TransformerDecoderLayer, TransformerDecoder
from torch.nn.utils.rnn import pack_padded_sequence
from torch.autograd import Variable

class EncoderCNN(nn.Module):
    def __init__(self, embed_size):
        super(EncoderCNN, self).__init__()
        resnet = models.resnet152(pretrained=True)
        self.resnet = nn.Sequential(*list(resnet.children())[:-2])
        self.conv1 = nn.Conv2d(2048, embed_size, 1)
        self.embed_size = embed_size

        self.fine_tune()
        
    def forward(self, images):
        features = self.resnet(images)
        batch_size, _,_,_ = features.shape
        features = self.conv1(features)
        features = features.view(batch_size, self.embed_size, -1)
        features = features.permute(2, 0, 1)

        return features

    def fine_tune(self, fine_tune=True):
        for p in self.resnet.parameters():
            p.requires_grad = False
        # If fine-tuning, only fine-tune convolutional blocks 2 through 4
        for c in list(self.resnet.children())[5:]:
            for p in c.parameters():
                p.requires_grad = fine_tune

class PositionEncoder(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):
        super(PositionEncoder, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)
    
class Embedder(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model)
    def forward(self, x):
        return self.embed(x)


class Transformer(nn.Module):
    def __init__(self, vocab_size, d_model, h, num_hidden, N, device, dropout_dec=0.1, dropout_pos=0.1):
        super(Transformer, self).__init__()
        decoder_layers = TransformerDecoderLayer(d_model, h, num_hidden, dropout_dec)
        self.source_mask = None
        self.device = device
        self.d_model = d_model
        self.pos_decoder = PositionalEncoder(d_model, dropout_pos)
        self.decoder = TransformerDecoder(decoder_layers, N)
        self.embed = Embedder(vocab_size, d_model)
        self.linear = nn.Linear(d_model, vocab_size)

        self.init_weights()

    def forward(self, source, mem):
        source = source.permute(1,0) 
        if self.source_mask is None or self.source_mask.size(0) != len(source):
            self.source_mask = nn.Transformer.generate_square_subsequent_mask(self=self, sz=len(source)).to(self.device)

        source = self.embed(source) 
        source = source*math.sqrt(self.d_model)  
        source = self.pos_decoder(source)
        output = self.decoder(source, mem, self.source_mask)
        output = self.linear(output)
        return output

    def init_weights(self):
        initrange = 0.1
        self.linear.bias.data.zero_()
        self.linear.weight.data.uniform_(-initrange, initrange)

    def pred(self, memory, pred_len):
        batch_size = memory.size(1)
        src = torch.ones((pred_len, batch_size), dtype=int) * 2
        if self.source_mask is None or self.source_mask.size(0) != len(src):
            self.source_mask = nn.Transformer.generate_square_subsequent_mask(self=self, sz=len(src)).to(self.device)
        output = torch.ones((pred_len, batch_size), dtype=int)
        src, output = src.cuda(), output.cuda()
        for i in range(pred_len):
            src_emb = self.embed(src) # src_len * batch size * embed size
            src_emb = src_emb*math.sqrt(self.d_model)
            src_emb = self.pos_decoder(src_emb)
            out = self.decoder(src_emb, memory, self.source_mask)
            out = out[i]
            out = self.linear(out) # batch_size * vocab_size
            out = out.argmax(dim=1)
            if i &lt; pred_len-1:
                src[i+1] = out
            output[i] = out
        return output
</code></pre>
<p>Data_Loader.py</p>
<pre><code>import torch
import torchvision.transforms as transforms
import torch.utils.data as data
import os
import pickle
import numpy as np
import nltk
from PIL import Image
from build_vocab import Vocabulary
from pycocotools.coco import COCO


class CocoDataset(data.Dataset):
    &quot;&quot;&quot;COCO Custom Dataset compatible with torch.utils.data.DataLoader.&quot;&quot;&quot;
    def __init__(self, root, json, vocab, transform=None):
        &quot;&quot;&quot;Set the path for images, captions and vocabulary wrapper.
        
        Args:
            root: image directory.
            json: coco annotation file path.
            vocab: vocabulary wrapper.
            transform: image transformer.
        &quot;&quot;&quot;
        self.root = root
        self.coco = COCO(json)
        self.ids = list(self.coco.anns.keys())
        self.vocab = vocab
        self.transform = transform

    def __getitem__(self, index):
        &quot;&quot;&quot;Returns one data pair (image and caption).&quot;&quot;&quot;
        coco = self.coco
        vocab = self.vocab
        ann_id = self.ids[index]
        caption = coco.anns[ann_id]['caption']
        img_id = coco.anns[ann_id]['image_id']
        path = coco.loadImgs(img_id)[0]['file_name']

        image = Image.open(os.path.join(self.root, path)).convert('RGB')
        if self.transform is not None:
            image = self.transform(image)

        # Convert caption (string) to word ids.
        tokens = nltk.tokenize.word_tokenize(str(caption).lower())
        caption = []
        caption.append(vocab('&lt;start&gt;'))
        caption.extend([vocab(token) for token in tokens])
        caption.append(vocab('&lt;end&gt;'))
        target = torch.Tensor(caption)
        return image, target

    def __len__(self):
        return len(self.ids)


def collate_fn(data):
    &quot;&quot;&quot;Creates mini-batch tensors from the list of tuples (image, caption).
    
    We should build custom collate_fn rather than using default collate_fn, 
    because merging caption (including padding) is not supported in default.
    Args:
        data: list of tuple (image, caption). 
            - image: torch tensor of shape (3, 256, 256).
            - caption: torch tensor of shape (?); variable length.
    Returns:
        images: torch tensor of shape (batch_size, 3, 256, 256).
        targets: torch tensor of shape (batch_size, padded_length).
        lengths: list; valid length for each padded caption.
    &quot;&quot;&quot;
    # Sort a data list by caption length (descending order).
    data.sort(key=lambda x: len(x[1]), reverse=True)
    images, captions = zip(*data)

    # Merge images (from tuple of 3D tensor to 4D tensor).
    images = torch.stack(images, 0)

    # Merge captions (from tuple of 1D tensor to 2D tensor).
    lengths = [len(cap) for cap in captions]
    targets = torch.zeros(len(captions), max(lengths)).long()
    for i, cap in enumerate(captions):
        end = lengths[i]
        targets[i, :end] = cap[:end]        
    return images, targets, lengths

def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):
    &quot;&quot;&quot;Returns torch.utils.data.DataLoader for custom coco dataset.&quot;&quot;&quot;
    # COCO caption dataset
    coco = CocoDataset(root=root,
                       json=json,
                       vocab=vocab,
                       transform=transform)
    
    # Data loader for COCO dataset
    # This will return (images, captions, lengths) for each iteration.
    # images: a tensor of shape (batch_size, 3, 224, 224).
    # captions: a tensor of shape (batch_size, padded_length).
    # lengths: a list indicating valid length for each caption. length is (batch_size).
    data_loader = torch.utils.data.DataLoader(dataset=coco, 
                                              batch_size=batch_size,
                                              shuffle=shuffle,
                                              num_workers=num_workers,
                                              collate_fn=collate_fn)
    return data_loader
</code></pre>
<p>Build_vocab.py</p>
<pre><code>import nltk
import pickle
import argparse
from collections import Counter
from pycocotools.coco import COCO

class Vocabulary(object):
    def __init__(self):
        self.word2idx = {}
        self.idx2word = {}
        self.idx = 0

    def add_word(self, word):
        if not word in self.word2idx:
            self.word2idx[word] = self.idx
            self.idx2word[self.idx] = word
            self.idx += 1

    def __call__(self, word):
        if not word in self.word2idx:
            return self.word2idx['&lt;unk&gt;']
        return self.word2idx[word]


    def __len__(self):
        return len(self.word2idx)

def build_vocab(json, threshold):
    coco = COCO(json)
    counter = Counter()
    ids = coco.anns.keys()
    for i, id in enumerate(ids):
        caption = str(coco.anns[id]['caption'])
        tokens = nltk.tokenize.word_tokenize(caption.lower())
        counter.update(tokens)

        if (i+1) % 1000 == 0:
            print(&quot;[{}/{}] Tokenized the captions.&quot;.format(i+1, len(ids)))

    # If the word frequency is less than 'threshold', then the word is discarded.
    words = [word for word, cnt in counter.items() if cnt &gt;= threshold]

    # Create a vocab wrapper and add some special tokens.
    vocab = Vocabulary()
    vocab.add_word('&lt;pad&gt;')
    vocab.add_word('&lt;start&gt;')
    vocab.add_word('&lt;end&gt;')
    vocab.add_word('&lt;unk&gt;')

    # Add the words to the vocabulary.
    for i, word in enumerate(words):
        vocab.add_word(word)
    return vocab

def main(args):
    vocab = build_vocab(json=args.caption_path, threshold=args.threshold)
    vocab_path = args.vocab_path
    with open(vocab_path, 'wb') as f:
        pickle.dump(vocab, f)
    print(&quot;Total vocabulary size: {}&quot;.format(len(vocab)))
    print(&quot;Saved the vocabulary wrapper to '{}'&quot;.format(vocab_path))


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--caption_path', type=str, 
                        default='./data/annotations/captions_train2014.json', 
                        help='path for train annotation file')
    parser.add_argument('--vocab_path', type=str, default='./data/vocab.pkl', 
                        help='path for saving vocabulary wrapper')
    parser.add_argument('--threshold', type=int, default=4, 
                        help='minimum word count threshold')
    args = parser.parse_args()
    main(args)

</code></pre>
<p>train.py</p>
<pre><code>import argparse
import torch
import torch.nn as nn
import numpy as np
import os
import pickle
import math
from tqdm import tqdm
from data_loader import get_loader 
from build_vocab import Vocabulary
from model import EncoderCNN, Decoder
from torch.nn.utils.rnn import pack_padded_sequence
from torchvision import transforms

# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def main(args):
    batch_size = 64
    embed_size = 512
    num_heads = 8
    num_layers = 6
    num_workers = 2
    num_epoch = 5
    lr = 1e-3
    load = False
    # Create model directory
    if not os.path.exists('models/'):
        os.makedirs('models/')
    
    # Image preprocessing, normalization for the pretrained resnet
    transform = transforms.Compose([ 
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(), 
        transforms.ToTensor(), 
        transforms.Normalize((0.485, 0.456, 0.406), 
                             (0.229, 0.224, 0.225))])
    
    # Load vocabulary wrapper
    with open('data/vocab.pkl', 'rb') as f:
        vocab = pickle.load(f)
    
    # Build data loader
    data_loader = get_loader('data/resized2014', 'data/annotations/captions_train2014.json', vocab, 
                             transform, batch_size,
                             shuffle=True, num_workers=num_workers) 

    encoder = EncoderCNN(embed_size).to(device)
    encoder.fine_tune(False)
    decoder = Decoder(len(vocab), embed_size, num_heads, embed_size, num_layers).to(device)
    
    if(load):
        encoder.load_state_dict(torch.load(os.path.join('models/', 'encoder-{}-{}.ckpt'.format(5, 5000))))
        decoder.load_state_dict(torch.load(os.path.join('models/', 'decoder-{}-{}.ckpt'.format(5, 5000))))
        print(&quot;Load Successful&quot;)

    # Loss and optimizer
    criterion = nn.CrossEntropyLoss()
    encoder_optim = torch.optim.Adam(encoder.parameters(), lr=lr)
    decoder_optim = torch.optim.Adam(decoder.parameters(), lr=lr)
    
    # Train the models
    for epoch in range(num_epoch):
        encoder.train()
        decoder.train()
        for i, (images, captions, lengths) in tqdm(enumerate(data_loader), total=len(data_loader), leave=False):
            
            # Set mini-batch dataset
            images = images.to(device)
            captions = captions.to(device)

            # Forward, backward and optimize
            features = encoder(images)
            cap_input = captions[:, :-1]
            cap_target = captions[:, 1:]
            outputs = decoder(cap_input, features)
            outputs = outputs.permute(1,0,2)
            outputs_shape = outputs.reshape(-1, len(vocab))
            loss = criterion(outputs_shape, cap_target.reshape(-1))
            decoder.zero_grad()
            encoder.zero_grad()
            loss.backward()
            encoder_optim.step()
            decoder_optim.step()
                
            # Save the model checkpoints
            if (i+1) % args.save_step == 0:
                torch.save(decoder.state_dict(), os.path.join(
                    'models/', 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))
                torch.save(encoder.state_dict(), os.path.join(
                    'models/', 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--log_step', type=int , default=10, help='step size for prining log info')
    parser.add_argument('--save_step', type=int , default=1000, help='step size for saving trained models')
        
    args = parser.parse_args()
    print(args)
    main(args)

</code></pre>
<p>sample.py</p>
<pre><code>
import torch
import matplotlib.pyplot as plt
import numpy as np 
import argparse
import pickle 
import os
from torchvision import transforms 
from build_vocab import Vocabulary
from data_loader import get_loader 
from model import EncoderCNN, Decoder
from PIL import Image


# Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#

def token_sentence(decoder_out, itos):
    tokens = decoder_out
    tokens = tokens.transpose(1, 0)
    tokens = tokens.cpu().numpy()
    results = []
    for instance in tokens:
        result = ' '.join([itos[x] for x in instance])
        results.append(''.join(result.partition('&lt;eos&gt;')[0])) # Cut before '&lt;eos&gt;'
    return results

def load_image(image_path, transform=None):
    image = Image.open(image_path).convert('RGB')
    image = image.resize([224, 224], Image.LANCZOS)
    
    if transform is not None:
        image = transform(image).unsqueeze(0)
    
    return image

def main(args):
    batch_size = 64
    embed_size = 512
    num_heads = 8
    num_layers = 6
    num_workers = 2
    
    # Image preprocessing
    transform = transforms.Compose([
        transforms.ToTensor(), 
        transforms.Normalize((0.485, 0.456, 0.406), 
                             (0.229, 0.224, 0.225))])
    
    # Load vocabulary wrapper
    with open(args.vocab_path, 'rb') as f:
        vocab = pickle.load(f)

    data_loader = get_loader('data/resized2014', 'data/annotations/captions_train2014.json', vocab, 
                             transform, batch_size,
                             shuffle=True, num_workers=num_workers) 

    # Build models
    encoder = EncoderCNN(embed_size).to(device)
    encoder.fine_tune(False)
    decoder = Decoder(len(vocab), embed_size, num_heads, embed_size, num_layers).to(device)

    # Load trained models
    encoder.load_state_dict(torch.load(os.path.join('models/', 'encoder-{}-{}.ckpt'.format(1, 4000))))
    decoder.load_state_dict(torch.load(os.path.join('models/', 'decoder-{}-{}.ckpt'.format(1, 4000))))
    encoder.eval()
    decoder.eval()
    
    itos = vocab.idx2word
    pred_len = 100
    result_collection = []

    # Decode with greedy
    # with torch.no_grad():
    #     for i, (images, captions, lengths) in enumerate(data_loader):
    #         images = images.to(device)
    #         features = encoder(images)
    #         output = decoder.generator(features, pred_len)
    #         result_caption = token_sentence(output, itos)
    #         result_collection.extend(result_caption)

# Decode with greedy
    with torch.no_grad():
        for batch_index, (inputs, captions, caplens) in enumerate(data_loader):
            inputs, captions = inputs.cuda(), captions.cuda()
            enc_out = encoder(inputs)
            captions_input = captions[:, :-1]
            captions_target = captions[:, 1:]
            output = decoder.pred(enc_out, pred_len)
            result_caption = token_sentence(output, itos)
            result_collection.extend(result_caption)
        
            
    print(&quot;Prediction-greedy:&quot;, result_collection[1])
    print(&quot;Prediction-greedy:&quot;, result_collection[2])
    print(&quot;Prediction-greedy:&quot;, result_collection[3])
    print(&quot;Prediction-greedy:&quot;, result_collection[4])
    print(&quot;Prediction-greedy:&quot;, result_collection[5])
    print(&quot;Prediction-greedy:&quot;, result_collection[6])
    print(&quot;Prediction-greedy:&quot;, result_collection[7])
    print(&quot;Prediction-greedy:&quot;, result_collection[8])
    print(&quot;Prediction-greedy:&quot;, result_collection[9])
    print(&quot;Prediction-greedy:&quot;, result_collection[10])
    print(&quot;Prediction-greedy:&quot;, result_collection[11])

    # # Prepare an image
    # image = load_image(args.image, transform)
    # image_tensor = image.to(device)
    
    # # Generate an caption from the image
    # feature = encoder(image_tensor)
    # sampled_ids = decoder.generator(feature, pred_len)
    # sampled_ids = sampled_ids[0].cpu().numpy()          # (1, max_seq_length) -&gt; (max_seq_length)
    
    # # Convert word_ids to words
    # sampled_caption = []
    # for word_id in sampled_ids:
    #     word = vocab.idx2word[word_id]
    #     sampled_caption.append(word)
    #     if word == '&lt;end&gt;':
    #         break
    # sentence = ' '.join(sampled_caption)
    
    # # Print out the image and the generated caption
    # print (sentence)
    # image = Image.open(args.image)
    # plt.imshow(np.asarray(image))
    
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--image', type=str, required=False, help='input image for generating caption')
    parser.add_argument('--vocab_path', type=str, default='data/vocab.pkl', help='path for vocabulary wrapper')
    args = parser.parse_args()
    main(args)

</code></pre>
<p>resize.py</p>
<pre><code>
import argparse
import os
from PIL import Image


def resize_image(image, size):
    &quot;&quot;&quot;Resize an image to the given size.&quot;&quot;&quot;
    return image.resize(size, Image.ANTIALIAS)

def resize_images(image_dir, output_dir, size):
    &quot;&quot;&quot;Resize the images in 'image_dir' and save into 'output_dir'.&quot;&quot;&quot;
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    images = os.listdir(image_dir)
    num_images = len(images)
    for i, image in enumerate(images):
        with open(os.path.join(image_dir, image), 'r+b') as f:
            with Image.open(f) as img:
                img = resize_image(img, size)
                img.save(os.path.join(output_dir, image), img.format)
        if (i+1) % 100 == 0:
            print (&quot;[{}/{}] Resized the images and saved into '{}'.&quot;
                   .format(i+1, num_images, output_dir))

def main(args):
    image_dir = args.image_dir
    output_dir = args.output_dir
    image_size = [args.image_size, args.image_size]
    resize_images(image_dir, output_dir, image_size)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--image_dir', type=str, default='./data/train2014/',
                        help='directory for train images')
    parser.add_argument('--output_dir', type=str, default='./data/resized2014/',
                        help='directory for saving resized images')
    parser.add_argument('--image_size', type=int, default=256,
                        help='size for image after processing')
    args = parser.parse_args()
    main(args)
</code></pre>
","transformer-model"
"64218678","what's the difference between ""self-attention mechanism"" and ""full-connection"" layer?","2020-10-06 02:50:36","64218982","11","2570","<pytorch><bert-language-model><transformer-model>","<p>I am confused with these two structures. In theory, the output of them are all connected to their input. what magic make 'self-attention mechanism' is more powerful than the full-connection layer?</p>
","transformer-model"
"64065478","XLM-RoBERTa token - id relationship","2020-09-25 13:51:25","64114644","2","1026","<transformer-model><roberta-language-model>","<p>I used the XLM-RoBERTa tokenizer in order to get the IDs for a bunch of sentences such as:</p>
<pre><code>[&quot;loving is great&quot;, &quot;This is another example&quot;]
</code></pre>
<p>I see that the IDs returned are not always as many as the number of whitespace-separated tokens in my sentences: for example, the first sentence corresponds to <code>[[0, 459, 6496, 83, 6782, 2]]</code>, with <code>loving</code> being <code>456</code> and <code>6496</code>. After getting the matrix for the word embeddings from the IDs, I was trying to identify only those word embeddings/vectors corresponding to some specific tokens: is there a way to do that? If the original tokens are sometimes assigned more than one ID and this cannot be predicted, I do not see how this is possible.</p>
<p>More in general, my task is to get word embeddings for some specific tokens within a sentence: my goal is therefore to use first the sentence so that word embeddings of single tokens can be calculated within the syntactic context, but then I would like to identify/keep the vectors of only some specific tokens and not those of all tokens in the sentence.</p>
","transformer-model"
"64057111","How to decide between NER and QA Model?","2020-09-25 02:51:11","64133467","2","1314","<python><nlp><extract><named-entity-recognition><transformer-model>","<p>I am completing a task involving NLP and transformers. I would like to identify relevant features in a corpus of text. If i was to extract the relevant features from job description for instance the tools that would be used at the job (powerpoint, excel, java, etc..) and the level of proficiency required would this task be better suited for a Named Entity Recognition model or a Question Answering model.</p>
<p>If I was to approach it like a NER task I would attach a label to all the relevant tools in the training data and hope it would generalize well. I could approach the problem simialrly as a QA model and ask things like &quot;what tools does this job require&quot; and supply a description as context.</p>
<p>I plan to use the transformers library unless I am missing a better tool for this task. There are many features I am looking to extract so not all may be as simple as grabbing keywords from a list (programming languages, microsoft office etc...).</p>
<p>Would one of these approaches be a better fit or am I missing a better way to approach the proble.</p>
<p>Any help appreciated. Thank you!</p>
","transformer-model"
"64040071","How is model parallelism implemented for GPT2 in MegatronLM?","2020-09-24 05:29:01","","2","509","<tensorflow><parallel-processing><nlp><distributed-computing><transformer-model>","<p>I am trying to understand the implementation details of <a href=""https://github.com/NVIDIA/Megatron-LM#inverse-cloze-task-ict-pretraining"" rel=""nofollow noreferrer"">MegatronLM</a>, which has both model and data parallel. On their <a href=""https://nv-adlr.github.io/MegatronLM"" rel=""nofollow noreferrer"">site</a> or in their research <a href=""https://arxiv.org/pdf/1909.08053.pdf"" rel=""nofollow noreferrer"">paper</a>, they mentioned how they used intra-layer parallel which is similar to mesh TensorFlow. I am confused with some details.</p>
<p>As shown in the picture below, my understanding is that the computation inside each of the 4 red circles can be parallelized by intra-layer splitting, but MLP must happen after self-attention, so only 2 red circled blocks can be parallelized at the same time. The paper says the model parallel is 8-way. My first question is, <strong>Does this indicate they split each block into 4 intra-layer parts (8/2)?</strong></p>
<p>(8-way divided by 2-blocks)
<a href=""https://i.sstatic.net/5agbD.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5agbD.jpg"" alt=""enter image description here"" /></a></p>
<p>The paper also mentioned</p>
<blockquote>
<p>To have consistent GEMM sizes in the self attention layer, the hidden size per attention head is kept constant at 96 while the number of heads and layers are varied to obtain configurations ranging from 1 billion to 8 billion parameters.</p>
</blockquote>
<p>My second question is <strong>What does the 96 hidden size refer to here?</strong></p>
<p>I am totally new to distributed training, I probably misunderstood something. Any clarification on this topic would be very appreciated! Thanks!</p>
","transformer-model"
"63970296","Encounter 'UnicodeDecodeError' when load BertModel with transformer package","2020-09-19 15:16:55","","1","187","<python><torch><transformer-model>","<p>I downloaded the pretrained file form google and converted it to the torch verion with the name 'pytorch_model.bin' using official scrip <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/convert_bert_original_tf_checkpoint_to_pytorch.py"" rel=""nofollow noreferrer"">convert_bert_original_tf_checkpoint_to_pytorch.py</a>:</p>
<pre><code>current_dir = os.path.dirname(__file__) # (1)
model_folder = os.path.join(current_dir, 'chinese_L-12_H-768_A-12')
checkpoint = os.path.join(model_folder, 'bert_model.ckpt')
configer = os.path.join(model_folder, 'bert_config.json')
save_to = os.path.join(current_dir, 'torch bert model', 'pytorch_model.bin')
convert_tf_checkpoint_to_pytorch(checkpoint, configer, save_to) 
</code></pre>
<p>Then I renamed 'bert_config.json' to 'config.json', and put 'pytorch_model.bin', 'config.json', and 'vocab.txt' into same folder 'torch_bert_model' in order to load this torch model with 'from_pertained()' method:</p>
<pre><code>current_dir = os.path.dirname(__file__)
model_folder = os.path.join(current_dir, 'torch_bert_model')
# configer_file = os.path.join(model_folder, 'config.json')
vocab_file = os.path.join(model_folder, 'vocab.txt')
model_file = os.path.join(model_folder, 'pytorch_model.bin')

tokenizer = BertTokenizer.from_pretrained(vocab_file)
# bert_config = BertConfig.from_json_file(configer_file)
model = TFBertModel.from_pretrained(model_file) # Automatically loads the config
</code></pre>
<p>When this code was run, the 'UnicodeDecodeError' occured:</p>
<pre><code>Traceback (most recent call last):
File &quot;c:/Users/hap/Desktop/Projects/cgt/BERT.py&quot;, line 47, in &lt;module&gt;
  model = TFBertModel.from_pretrained(model_file)
File &quot;C:\Users\hap\AppData\Local\Programs\Python\Python38\lib\site- packages\transformers\modeling_tf_utils.py&quot;, line 529, in from_pretrained
  config, model_kwargs = cls.config_class.from_pretrained(
File &quot;C:\Users\hap\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\configuration_utils.py&quot;, line 311, in from_pretrained
  config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
File &quot;C:\Users\hap\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\configuration_utils.py&quot;, line 354, in get_config_dict
  config_dict = cls._dict_from_json_file(resolved_config_file)
File &quot;C:\Users\hap\AppData\Local\Programs\Python\Python38\lib\site-packages\transformers\configuration_utils.py&quot;, line 436, in _dict_from_json_file
  text = reader.read()
File &quot;C:\Users\hap\AppData\Local\Programs\Python\Python38\lib\codecs.py&quot;, line 322, in decode
  (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte
</code></pre>
<p>Where am I going wrong here? Any suggestions are much appreciated. Thanks!</p>
","transformer-model"
"63904821","Using Transformer for Text-Summarization","2020-09-15 15:12:10","64529095","0","2136","<tensorflow><pytorch><huggingface-transformers><transformer-model><summarization>","<p>I am using huggingface transformer models for <strong>text-summarization</strong>.
Currently I am testing different models such as <strong>T5</strong> and <strong>Pegasus</strong>.
Now these models were trained for summarizing Big Texts into very short like a maximum of two sentences. Now I have the task, that I want summarizations, that are about half the size of the text, ergo the generated summaries are too small for my purpose.</p>
<p>My question now is, if there is a way to tell the model that another sentence came before?
Kind of similar to the logic inside stateful RNNs (although I know they work completly different).
If yes, I could summarize small windows over the sentences always with the information which content came before.</p>
<p>Is that just a thing of my mind? I cant believe that I am the only one, who wants to create shorter summaries, but not only 1 or two sentence long ones.</p>
<p>Thank you</p>
","transformer-model"
"63884856","trouble with using tokenizer.encode_plus","2020-09-14 13:00:27","","3","9200","<python><jupyter><transformer-model>","<p>#jupyter notebook</p>
<p>I'm trying to study BERT Classifier with <a href=""https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=2bBdb3pt8LuQ"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=2bBdb3pt8LuQ</a></p>
<p>In that colab, starting with &quot;Tokenize all of the sentence.....&quot;</p>
<p>At that part, I have a trouble &quot;TypeError: _tokenize() got an unexpected keyword argument 'pad_to_max_length'&quot;</p>
<pre><code>**
input_ids = []
attention_masks = []

for sent in sentences:
    encoded_dict = tokenizer.encode_plus(
                    sent,                      # Sentence to encode.
                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                    max_length = 64,           # Pad &amp; truncate all sentences.
                    pad_to_max_length = True,
                    return_attention_mask = True,   # Construct attn. masks.
                    return_tensors = 'pt',     # Return pytorch tensors.
               )
</code></pre>
","transformer-model"
"63869529","Training Loss and Accuracy both decreasing for my transformer model for Time Series Prediction","2020-09-13 10:07:33","63870857","2","1148","<python-3.x><machine-learning><time-series><pytorch><transformer-model>","<p>I am using a transformer model for predicting the forex market. I transformed the open price data and calculated the difference between each 30 min interval. And converted the difference into tokens. The tokens are obtained by applying log1.5 to the difference. I obtained 28 types of tokens for 6 years. 14-27 represents a bull market and 0-13 tokens represent bear market.
I created a transformer model in PyTorch and applied the data.</p>
<pre><code>import torch 
import math 
import numpy as np
import copy
from torch import nn
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
import ast 
from numpy import load
import torch.nn as nn
import random
import time
import matplotlib.pyplot as plt


class Embedder(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        # print(vocab_size,d_model)
        self.embed = nn.Embedding(vocab_size+1, d_model,padding_idx=0)
    def forward(self, x):
        # print(x.shape)
        # print(&quot;Embed&quot;,self.embed(x).shape)
        return self.embed(x)

class PositionalEncoder(nn.Module):
    def __init__(self, d_model, max_seq_len = 500):
        super().__init__()
        self.d_model = d_model
        
        # create constant 'pe' matrix with values dependant on 
        # pos and i
        pe = torch.zeros(max_seq_len, d_model)
        for pos in range(max_seq_len):
            for i in range(0, d_model, 2):
                pe[pos, i] = \
                math.sin(pos / (10000 ** ((2 * i)/d_model)))
                pe[pos, i + 1] = \
                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))
                
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
 
    
    def forward(self, x):
        x = x * math.sqrt(self.d_model)
        seq_len = x.size(1)
        x = x + torch.autograd.Variable(self.pe[:,:seq_len],requires_grad=False)
        return x

def attention(q, k, v, d_k, mask=None, dropout=None):
    
    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)
    if mask is not None:
        mask = mask.unsqueeze(1)
        scores = scores.masked_fill(mask == 0, -1e9)
        scores = torch.nn.functional.softmax(scores, dim=-1)
    
    if dropout is not None:
        scores = dropout(scores)
        
    output = torch.matmul(scores, v)
    return output


class MultiHeadAttention(nn.Module):
    def __init__(self, heads, d_model, dropout = 0.1):
        super().__init__()
        
        self.d_model = d_model
        self.d_k = d_model // heads
        self.h = heads
        
        self.q_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)
        self.out = nn.Linear(d_model, d_model)
    
    def forward(self, q, k, v, mask=None):
        
        bs = q.size(0)
        
        # perform linear operation and split into h heads
        
        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)
        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)
        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)
        
        # transpose to get dimensions bs * h * sl * d_model
       
        k = k.transpose(1,2)
        q = q.transpose(1,2)
        v = v.transpose(1,2)
        # calculate attention using function we will define next
        scores = attention(q, k, v, self.d_k, mask, self.dropout)
        
        # concatenate heads and put through final linear layer
        concat = scores.transpose(1,2).contiguous()\
        .view(bs, -1, self.d_model)
        
        output = self.out(concat)
    
        return output

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff=512, dropout = 0.1):
        super().__init__() 
        # We set d_ff as a default to 2048
        self.linear_1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)
    def forward(self, x):
        x = self.dropout(torch.nn.functional.relu(self.linear_1(x)))
        x = self.linear_2(x)
        return x

class Norm(nn.Module):
    def __init__(self, d_model, eps = 1e-6):
        super().__init__()
    
        self.size = d_model
        # create two learnable parameters to calibrate normalisation
        self.alpha = nn.Parameter(torch.ones(self.size))
        self.bias = nn.Parameter(torch.zeros(self.size))
        self.eps = eps
    def forward(self, x):
        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \
        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias
        return norm

class EncoderLayer(nn.Module):
    def __init__(self, d_model, heads, dropout = 0.1):
        super().__init__()
        self.norm_1 = Norm(d_model)
        self.norm_2 = Norm(d_model)
        self.attn = MultiHeadAttention(heads, d_model)
        self.ff = FeedForward(d_model)
        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)
        
    def forward(self, x, mask):
        x2 = self.norm_1(x)
        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))
        x2 = self.norm_2(x)
        x = x + self.dropout_2(self.ff(x2))
        return x
    
class DecoderLayer(nn.Module):
    def __init__(self, d_model, heads, dropout=0.1):
        super().__init__()
        self.norm_1 = Norm(d_model)
        self.norm_2 = Norm(d_model)
        self.norm_3 = Norm(d_model)
        
        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)
        self.dropout_3 = nn.Dropout(dropout)
        
        self.attn_1 = MultiHeadAttention(heads, d_model)
        self.attn_2 = MultiHeadAttention(heads, d_model)
        self.ff = FeedForward(d_model).cuda()
        # self.ff = FeedForward(d_model)

    def forward(self, x, e_outputs, src_mask, trg_mask):
        x2 = self.norm_1(x)
        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))
        x2 = self.norm_2(x)
        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,
        src_mask))
        x2 = self.norm_3(x)
        x = x + self.dropout_3(self.ff(x2))
        return x
# We can then build a convenient cloning function that can generate multiple layers:
def get_clones(module, N):
    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])

class Encoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads):
        super().__init__()
        self.N = N
        self.embed = Embedder(vocab_size, d_model)
        self.pe = PositionalEncoder(d_model)
        self.layers = get_clones(EncoderLayer(d_model, heads), N)
        self.norm = Norm(d_model)
        
    def forward(self, src, mask):
        x = self.embed(src)
        x = self.pe(x)
        for i in range(self.N):
            x = self.layers[i](x, mask)
        return self.norm(x)
    
class Decoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, heads):
        super().__init__()
        self.N = N
        self.embed = Embedder(vocab_size, d_model)
        self.pe = PositionalEncoder(d_model)
        self.layers = get_clones(DecoderLayer(d_model, heads), N)
        self.norm = Norm(d_model)
    def forward(self, trg, e_outputs, src_mask, trg_mask):
        x = self.embed(trg)
        x = self.pe(x)
        for i in range(self.N):
            x = self.layers[i](x, e_outputs, src_mask, trg_mask)
        return self.norm(x)

class Transformer(nn.Module):
    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):
        super().__init__()
        self.encoder = Encoder(src_vocab, d_model, N, heads)
        self.decoder = Decoder(trg_vocab, d_model, N, heads)
        self.out = nn.Linear(d_model, trg_vocab)

    def forward(self, src, trg, src_mask, trg_mask):
        e_outputs = self.encoder(src, src_mask)
        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)
        output = self.out(d_output)
        return output


def batchify(data, bsz):
    nbatch = data.size(0) // bsz
    data = data.narrow(0, 0, nbatch * bsz)
    data = data.view(bsz, -1).t().contiguous()
    return data

bptt = 128
class CustomDataLoader:
    def __init__(self,source):
        print(&quot;Source&quot;,source.shape)
        self.batches = list(range(0, source.size(0) - 2*bptt))
        # random.shuffle(self.batches)
        # print(self.batches)
        self.data = source
        self.sample = random.sample(self.batches,120)

    def batchcount(self):
        return len(self.batches)

    def shuffle_batches(self):
        random.shuffle(self.batches)

    def get_batch_from_batches(self,i):
        if i==0:
            random.shuffle(self.batches)
        ind = self.batches[i]
        seq_len = min(bptt,len(self.data)-1-ind)
        src = self.data[ind:ind+seq_len]
        tar = self.data[ind+seq_len-3:ind+seq_len-3+seq_len+1]
        return src,tar
        
    def get_batch(self,i):
        # print(i,len(self.batches))
        ind = self.sample[i]
        seq_len = min(bptt,len(self.data)-1-ind)
        src = self.data[ind:ind+seq_len]
        tar = self.data[ind+seq_len-3:ind+seq_len-3+seq_len+1]
        # tar = tar.view(-1)
        if(i==len(self.sample)-1):
            random.sample(self.batches,60)
            # print(&quot;Data shuffled&quot;,self.batches[:10])
        return src,tar

def get_batch(source, i):
    seq_len = min(bptt, len(source) - 1 - i)
    data = source[i:i+seq_len]
    target = source[i+seq_len-3:i+seq_len-3+seq_len]
    return data, target

def plot_multiple(data,legend):
    fig,ax = plt.subplots()
    for line in data:
        plt.plot(list(range(len(line))),line)
    plt.legend(legend)
    plt.show()


def plot_subplots(data,legends,name):
    names = ['Accuracy', 'Loss']
    plt.figure(figsize=(10, 5))
    for i in range(len(data)):
        plt.subplot(121+i)
        plt.plot(list(range(0,len(data[i])*3,3)),data[i])
        plt.title(legends[i])
        plt.xlabel(&quot;Epochs&quot;)
    plt.savefig(name)

def evaluate(eval_model, data_source):
    eval_model.eval() # Turn on the evaluation mode
    total_loss = 0.
    ntokens = 28
    count = 0
    with torch.no_grad():
        cum_loss = 0
        acc_count = 0
        accs = 0
        print(data_source.shape)
        for batch, i in enumerate(range(0, data_source.size(0) - bptt*2, bptt)):
            data, targets = get_batch(data_source, i)
            # data,targets = dataLoader.get_batch(i)
            data = data.transpose(0,1).contiguous()
            targets= targets.transpose(0,1).contiguous()
            trg_input = targets[:,:-1]
            trg_output = targets[:,1:].contiguous().view(-1)
            src_mask , trg_mask = create_masks(data,trg_input)
            output = model(data,trg_input,src_mask,trg_mask)
            output = output.view(-1,output.size(-1))
            loss = torch.nn.functional.cross_entropy(output,trg_output-1)
            accs += ((torch.argmax(output,dim=1)==trg_output).sum().item()/output.size(0))
            # accs += ((torch.argmax(output,dim=1)==targets).sum().item()/output.size(0))
            cum_loss += loss
            count+=1
        # print(epoch,&quot;Loss: &quot;,(cum_loss/count),&quot;Accuracy &quot;,accs/count)

    return cum_loss/ (count), accs/count

def nopeak_mask(size,cuda_enabled):
    np_mask = np.triu(np.ones((1, size, size)),
    k=1).astype('uint8')
    np_mask =  torch.autograd.Variable(torch.from_numpy(np_mask) == 0)

    if cuda_enabled:
      np_mask = np_mask.cuda()
    return np_mask

def create_masks(src, trg):
    src_mask = (src != 0).unsqueeze(-2)
    if trg is not None:
        trg_mask = (trg != 0).unsqueeze(-2)
        size = trg.size(1) # get seq_len for matrix
        # print(&quot;Sequence lenght in mask &quot;,size)
        np_mask = nopeak_mask(size,True)
        # print(np_mask.shape,trg_mask.shape)
        if trg.is_cuda:
            np_mask.cuda()
        trg_mask = trg_mask &amp; np_mask
    else:
        trg_mask = None
    return src_mask, trg_mask

def create_padding_mask(seq):
  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)
  
  # add extra dimensions to add the padding
  # to the attention logits.
  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)


if __name__ == '__main__':
    data = []
    dev = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    procsd_data = load(&quot;Eavg_open.npy&quot;)
    print(set(procsd_data[:,0]))
    train_data =torch.tensor(procsd_data)[:30000*2]
    print(train_data.shape)
    val_data = torch.tensor(procsd_data)[30000*2:35000*2]
    test_data = torch.tensor(procsd_data)[35000*2:]
    train_data = train_data.to(dev)
    val_data = val_data.to(dev)
    test_data = test_data.to(dev)

    # train_data = train_data.transpose(1,0).contiguous()
    # val_data = val_data.transpose(1,0).contiguous()

    batch_size = 32
    ntokens = 28
    train_data = batchify(train_data,batch_size)
    # print(train_data.shape)
    val_data = batchify(val_data,batch_size)
    test_data = batchify(train_data,batch_size)
    # model = Transformer(n_blocks=3,d_model=256,n_heads=8,d_ff=256,dropout=0.5)
    model = Transformer(28,28,64,3,4)
    # model = torch.load(&quot;modela&quot;)
    for p in model.parameters():
        if p.dim() &gt; 1:
            nn.init.xavier_uniform_(p)

    model.to(dev)
    criterion = nn.CrossEntropyLoss()
    lr = 0.00001 # learning rate
    
    optim = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)
    #########training starts###########

    accuracies = []
    lossies = []
    val_loss = []
    val_accuracy = []
    dataLoader = CustomDataLoader(train_data)
    _onehot = torch.eye(29)
    for epoch in range(500):
        count = 0
        cum_loss = 0
        acc_count = 0
        accs = 0
        s = time.time()
        # for i in range(len(range(0, train_data.size(0) - bptt))):
        model.train()
        # dataLoader.shuffle_batches()
        for i in range(300):
            # data, targets = get_batch(train_data, i)
            # d = time.time()
            hh = time.time()
            data,targets = dataLoader.get_batch_from_batches(i)
            data = data.transpose(0,1).contiguous()
            targets= targets.transpose(0,1).contiguous()
            # print(data.shape,targets.shape)
            trg_input = targets[:,:-1]
            trg_output = targets[:,1:].contiguous().view(-1)
            # print(data.shape,trg_input.shape)
            src_mask , trg_mask = create_masks(data,trg_input)
            # print(&quot;Source Mask&quot;,src_mask)
            # print(&quot;Target Mask&quot;,trg_mask)
            output = model(data,trg_input,src_mask,trg_mask)
            # output = output.view(-1,28)
            output = output.view(-1,output.size(-1))

            loss = torch.nn.functional.cross_entropy(output,trg_output-1)
            accuracy = ((torch.argmax(output,dim=1)==trg_output).sum().item()/output.size(0))
            accs += accuracy
            cum_loss += loss.item();
            loss.backward()
            optim.step()
            model.zero_grad()
            optim.zero_grad()
            print(i,&quot; Batch Loss&quot;, loss.item(),&quot; Batch Accuracy &quot;,accuracy,&quot; Time taken &quot;,time.time()-hh)
            
            count+=1
            
        data,targets = None,None
        print(epoch,&quot;Loss: &quot;,(cum_loss/count),&quot;Accuracy &quot;,accs/count,&quot; Time Taken: &quot;,time.time()-s)
        if(epoch%3==0):
            lossies.append(cum_loss/count)
            accuracies.append(accs/count)
            legend = [&quot;accuracy&quot;,&quot;Loss&quot;]
            plot_subplots([accuracies,lossies],legend,&quot;A&amp;L_v1&quot;)
            print(&quot;Valdata&quot;,val_data.shape)
            eval_loss,eval_acc = evaluate(model,val_data)
            val_accuracy.append(eval_acc)
            val_loss.append(eval_loss)
            plot_subplots([val_accuracy,val_loss],legend,&quot;Val A&amp;L_v1&quot;)
            print(epoch,&quot;Loss: &quot;,(cum_loss/count),&quot;Accuracy &quot;,accs/count,&quot; Valid_loss: &quot;,eval_loss,&quot; Valid_accuracy: &quot;,eval_acc)
            if len(val_loss)&gt;0 and eval_loss &lt; val_loss[-1]:
                val_loss.append(eval_loss)
                torch.save(model,&quot;evalModel&quot;)
            else:
                val_loss.append(eval_loss)
                torch.save(model,&quot;evalModel&quot;)
        if(epoch%5==0):
            torch.save(model,&quot;modela&quot;)
</code></pre>
<p>I got the following loss and accuracy while training:
<a href=""https://i.sstatic.net/aRgMw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aRgMw.png"" alt=""enter image description here"" /></a></p>
<p>What is causing this behaviour?
Am I wrong in my tokenization method?
Is it necessary to add any time embedding to the data?</p>
","transformer-model"
"63867124","Pytorch NLP sequence length of target in Transformer","2020-09-13 04:22:09","63867312","1","1499","<nlp><pytorch><mask><transformer-model>","<p>I'm trying to understand the code of Transformer (<a href=""https://github.com/SamLynnEvans/Transformer"" rel=""nofollow noreferrer"">https://github.com/SamLynnEvans/Transformer</a>).</p>
<p>If seeing the train_model function in &quot;train&quot; script, I wonder why need to use the different sequence length of trg_input from trg:</p>
<pre><code>trg_input = trg[:, :-1]
</code></pre>
<p>In this case, the sequence length of trg_input is &quot;seq_len(trg) - 1&quot;.
It means that trg is like:</p>
<pre><code>&lt;sos&gt; tok1 tok2 tokn &lt;eos&gt;
</code></pre>
<p>and trg_input is like:</p>
<pre><code>&lt;sos&gt; tok1 tok2 tokn    (no eos token)
</code></pre>
<p>Please let me know the reason.</p>
<p>Thank you.</p>
<p>The related code is like below:</p>
<pre><code>    for i, batch in enumerate(opt.train):
        src = batch.src.transpose(0, 1).to('cuda')
        trg = batch.trg.transpose(0, 1).to('cuda')

        trg_input = trg[:, :-1]
        src_mask, trg_mask = create_masks(src, trg_input, opt)
        preds = model(src, trg_input, src_mask, trg_mask)
        ys = trg[:, 1:].contiguous().view(-1)
        opt.optimizer.zero_grad()
        loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=opt.trg_pad)
        loss.backward()
        opt.optimizer.step()


def create_masks(src, trg, opt):
    
    src_mask = (src != opt.src_pad).unsqueeze(-2)

    if trg is not None:
        trg_mask = (trg != opt.trg_pad).unsqueeze(-2)
        size = trg.size(1) # get seq_len for matrix
        np_mask = nopeak_mask(size, opt)
        if trg.is_cuda:
            np_mask.cuda()
        trg_mask = trg_mask &amp; np_mask
        
    else:
        trg_mask = None
    return src_mask, trg_mask
</code></pre>
","transformer-model"
"63796350","Hello, two questions about sklearn.Pipeline with custom transformer for timeseries","2020-09-08 14:46:16","63809625","-5","623","<machine-learning><scikit-learn><python-3.7><pipeline><transformer-model>","<p>How should I to modify the code below to make it work:</p>
<p><s>target, predicted = pipe.fit_predict(df)</s></p>
<h2>EDIT:</h2>
<pre><code>target, predicted = pipe.fit_transform(df, df)
</code></pre>
<p>My code:</p>
<pre><code>import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator
from sklearn.base import TransformerMixin
from sklearn.pipeline import Pipeline 
np.random.seed(1)

rows,cols = 100,1
data = np.random.randint(100, size = (rows,cols))
tidx = pd.date_range('2019-01-01', periods=rows, freq='20min') 
df = pd.DataFrame(data, columns=['num_orders'], index=tidx)
      


class MakeFeatures(BaseEstimator, TransformerMixin):

def __init__(self, X, y = None, max_lag = None, rolling_mean_day = None, rolling_mean_month = None):
    self.X = X.resample('1H').sum()
    self.max_lag = max_lag
    self.rolling_mean_day = rolling_mean_day
    self.rolling_mean_month = rolling_mean_month
        
def fit(self, X, y = None):
    return self

def transform(self, X, y = None):
    data = pd.DataFrame(index = self.X.index)
    data['num_orders'] = self.X['num_orders']
    data['year'] = self.X.index.year
    data['month'] = self.X.index.month
    data['day'] = self.X.index.day
    data['dayofweek'] = self.X.index.dayofweek
    
    data['detrend'] = self.X.shift() - self.X
    
    if self.max_lag:
        for lag in range(1, self.max_lag + 1):
            data['lag_{}'.format(lag)] = data['detrend'].shift(lag)
    if self.rolling_mean_day:
        data['rolling_mean_24'] = data.detrend.shift().rolling(self.rolling_mean_day).mean()
    
    if self.rolling_mean_month:
        data['rolling_mean_24'] = data['detrend'].shift().rolling(self.rolling_mean_month).mean()
    
    if data['year'].mean() == data['year'][1]:
        data = data.drop('year', axis = 1)
    
    data = data.dropna()
    
    y = data.num_orders
    data = data.drop('num_orders', 1)
    
    return data, y

pipe = Pipeline([
                ('features', MakeFeatures(df, df, 2 , 24)),
                ('scaler', StandardScaler())  
    ])

target, predicted = pipe.fit_transform(df, df)  # where ‘Target’ is y - the output from the Class
</code></pre>
<p>Out:</p>
<pre><code>ValueError: could not broadcast input array from shape (9,7) into shape (9).
</code></pre>
<p>Each function inside the Pipeline is working fine.</p>
<p>I can run <strong>MakeFeatures(df, df)</strong> and <strong>StandardScaler().fit_transform(df, df)</strong> without a problem.</p>
<p>I can insert the product of MakeFeatures(df,df) into the StandardScaler and it has no mistake.</p>
","transformer-model"
"63722187","Tensorflow: Getting LSTM to 100% in converting date time formats","2020-09-03 10:56:22","","1","266","<python><tensorflow><lstm><autoencoder><transformer-model>","<p>I created a bidirectional seq2seq endcoder-decoder network, which aims at formating different datetypes to a german datetime format as: <em>day_written_out, day.month.year</em></p>
<p>So as an example I have the string <strong>12-27-1992</strong> and I expect the model to predict <strong>Sunday, 27.12.1992</strong>.</p>
<p>After training, the model reaches a valid_accuracy of <strong>98%</strong>. Further investigation showed, that the model nearly always predicted the date itself correct, <strong>but</strong> the model is not able to extract the correct <strong>day</strong>. So instead of predicting <strong>Sunday, 27.12.1992</strong> it predicts <strong>Wednesday, 27.12.1992</strong>.
Only 5% of the days are predicted correctly.</p>
<p>I think, that the issues lies in the LSTM I use, even though its bidirectional, but since the day is the first element that is predicted, the model does not have alot of information for predicting the correct day, since at timestep 0 the model hasnt seen any date values yet. Is this correct?</p>
<p>So my question is, why is this model not capable of predicting the correct day? And would a tranformer architecture, which is capable of reading the whole sequence at a time, be able to solve this issue?</p>
<p>Thank you</p>
","transformer-model"
"63680281","How to get stable output for torch.nn.Transformer","2020-09-01 01:53:57","63681883","0","918","<python><neural-network><pytorch><transformer-model>","<p>Looks like Transformer layers of pytorch give not reproducible outputs. It happens both for cpu and gpu. I know that it sometimes happens because of parallel computations on gpu.</p>
<pre><code>emb = nn.Embedding(10, 12).to(device)
inp1 = torch.LongTensor([1, 2, 3, 4]).to(device)
inp1 = emb(inp1).reshape(inp1.shape[0], 1, 12) #S N E

encoder_layer = nn.TransformerEncoderLayer(d_model=12, nhead=4)
transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)

out1 = transformer_encoder(inp1)
out2 = transformer_encoder(inp1)
</code></pre>
<p>out1 and out2 are different. It can be multiprocessing on cpu, but results looks too shaky. How to fix this?</p>
","transformer-model"
"63588991","Converting a tensor to a numpy 2D array","2020-08-26 00:50:15","","0","455","<numpy><tensorflow><bert-language-model><transformer-model>","<pre><code>from transformers import BertTokenizer, TFBertModel
import matplotlib.pyplot as plt
import tensorflow as tf
</code></pre>
<p>The code included below throws an error on the line:</p>
<pre><code>features = bert_encoder([input_word_ids, input_mask, input_type_ids])[0][:,0,:].numpy()
</code></pre>
<p>The error is:</p>
<pre><code>AttributeError: 'Tensor' object has no attribute 'numpy'
</code></pre>
<p>I am running this on a tensor flow version &gt; 2.0 and <code>tf.executing_eagerly()</code> returns <code>True</code></p>
<p>The dictionary items that I am retrieving information from before the numpy() operation are:</p>
<pre><code>{
bert_encoder_output: &lt;tf.Tensor 'strided_slice:0' shape=(None, 768) dtype=float32&gt;, 
embedding: &lt;tf.Tensor 'tf_bert_model/Identity:0' shape=(None, 50, 768) dtype=float32&gt;
}
</code></pre>
<p>TPU Session set up:</p>
<pre><code>try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
except ValueError:
    strategy = tf.distribute.get_strategy() # for CPU and single GPU
    print('Number of replicas:', strategy.num_replicas_in_sync)
</code></pre>
<p>Code:</p>
<pre><code>   from tensorflow import keras
from tensorflow.keras import layers
import tensorflow as tf

if (tf.executing_eagerly()):
    print (&quot;Yes&quot;)
    
tf.compat.v1.enable_eager_execution() 

max_len = 50

def get_bert_encoder_output(printInputs = False):
    model_inputs = {}
    bert_encoder = TFBertModel.from_pretrained(model_name)
    # Get Inputs
    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_word_ids&quot;)
    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_mask&quot;)
    input_type_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=&quot;input_type_ids&quot;)
    # last hidden-state - the model output - is the first element of the output tuple
    embedding = bert_encoder([input_word_ids, input_mask, input_type_ids])[0]
    bert_encoder_output = (embedding[:,0,:])
    model_inputs['input_word_ids'] = input_word_ids    
    model_inputs['input_mask'] = input_mask
    model_inputs['input_type_ids'] = input_type_ids
    model_inputs['bert_encoder_output'] = bert_encoder_output
    model_inputs['embedding'] = embedding
    if (tf.executing_eagerly()):
     print (&quot;Inside get_bert_encoder_output - Yes executing eagerly&quot;)
    features = bert_encoder([input_word_ids, input_mask, input_type_ids])[0][:,0,:].numpy()
    
    if (printInputs):
        print (model_inputs)
        print (features)
        
    return (model_inputs)    
</code></pre>
<p><a href=""https://i.sstatic.net/LvHSn.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LvHSn.jpg"" alt=""enter image description here"" /></a></p>
","transformer-model"
"63566232","RuntimeError: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 3","2020-08-24 17:53:16","63610104","4","31060","<python><pytorch><transformer-model><seq2seq>","<p>I am doing the following operation,</p>
<pre><code>energy.masked_fill(mask == 0, float(&quot;-1e20&quot;)) 
</code></pre>
<p>my python traces are below,</p>
<pre><code>    File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;seq_sum.py&quot;, line 418, in forward
    enc_src = self.encoder(src, src_mask)
  File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;seq_sum.py&quot;, line 71, in forward
    src = layer(src, src_mask)
  File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;seq_sum.py&quot;, line 110, in forward
    _src, _ = self.self_attention(src, src, src, src_mask)
  File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py&quot;, line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File &quot;seq_sum.py&quot;, line 191, in forward
    energy =  energy.masked_fill(mask == 0, float(&quot;-1e20&quot;))
RuntimeError: The size of tensor a (1024) must match the size of tensor b (512) at non-singleton dimension 3
</code></pre>
<p>These are my attention layers code,</p>
<pre><code>    Q = self.fc_q(query)
    K = self.fc_k(key)
    V = self.fc_v(value)
    
    #Q = [batch size, query len, hid dim]
    #K = [batch size, key len, hid dim]
    #V = [batch size, value len, hid dim]
            
    # Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)
    # K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)
    # V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)

    Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).view(-1, 1024)
    K = K.view(batch_size, -1, self.n_heads, self.head_dim).view(-1, 1024)
    V = V.view(batch_size, -1, self.n_heads, self.head_dim).view(-1, 1024)
    energy = torch.matmul(Q, K.transpose(1,0)) / self.scale
</code></pre>
<p>I am following below github code to do my seq to seq operation,<a href=""https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"" rel=""nofollow noreferrer"">seq2seq pytorch</a>
actual testing code is available on the below location, <a href=""https://github.com/VinACE/trans-vsumm/blob/master/seq_sum.py"" rel=""nofollow noreferrer"">code to test a seq of 1024 to 1024 output</a></p>
<p><a href=""https://github.com/VinACE/trans-vsumm/blob/master/vasnet_seq2seq_summ.py"" rel=""nofollow noreferrer"">2nd example tried</a> here I have commented out pos_embedding due CUDA error with large index (<a href=""https://www.gitmemory.com/issue/huggingface/pytorch-pretrained-BERT/97/498151254"" rel=""nofollow noreferrer"">RuntimeError: cuda runtime error (59) </a></p>
","transformer-model"
"63489716","How to separate the values of a multivalued field into dynamic fields","2020-08-19 14:54:18","","1","104","<solr><transformer-model><multivalue><dih>","<p>I have 1 multivalued date type field, its definition in the schema.xml is shown below:</p>
<pre><code>&lt;field name=&quot;fecha_referencia&quot; type=&quot;pdates&quot; uninvertible=&quot;true&quot; indexed=&quot;true&quot; stored=&quot;true&quot;/&gt;
</code></pre>
<p>The total of values it can take are three, here is an example where it is already indexed:</p>
<pre><code>fecha_referencia:[&quot;2015-12-04T00:00:00Z&quot;,
          &quot;2014-12-15T00:00:00Z&quot;,
          &quot;2014-02-03T00:00:00Z&quot;]
</code></pre>
<p><strong>I want to know is if you can divide the values at the time of indexing (I am indexing via DIH) into other dynamic fields or separate fields.</strong></p>
<p>Example of what you are looking for:</p>
<pre><code>fecha_referencia:[&quot;2015-12-04T00:00:00Z&quot;,
              &quot;2014-12-15T00:00:00Z&quot;,
              &quot;2014-02-03T00:00:00Z&quot;],
fecha1:2015-12-04T00:00:00Z,
fecha2:2014-12-15T00:00:00Z,
fecha3:2014-02-03T00:00:00Z
</code></pre>
<p>Note: I have tried to test regex but have had no luck.
Any contribution would be of great help and well received by your server...</p>
<p>This is my data.config.xml structure:</p>
<pre><code>&lt;dataConfig&gt;
&lt;dataSource  type=&quot;JdbcDataSource&quot; driver=&quot;org.postgresql.Driver&quot; url=&quot;jdbc:postgresql://10.152.11.47:5433/meta&quot; user=&quot;us&quot; password=&quot;ntm&quot; URIEncoding=&quot;UTF-8&quot; /&gt;
    &lt;document &gt;
       &lt;entity name=&quot;tr_ident&quot; query=&quot;SELECT id_ident, titulo,proposito,descripcion,palabra_cve
        FROM ntm_p.tr_ident&quot;&gt;
            &lt;field column=&quot;id_ident&quot; name=&quot;id_ident&quot; /&gt;
            &lt;field column=&quot;titulo&quot; name=&quot;titulo&quot; /&gt;
            &lt;field column=&quot;proposito&quot; name=&quot;proposito&quot; /&gt;
        &lt;field column=&quot;descripcion&quot; name=&quot;descripcion&quot; /&gt;
            &lt;field column=&quot;palabra_cve&quot; name=&quot;palabra_cve&quot; /&gt;

            &lt;entity name=&quot;tr_fecha_insumo&quot; query=&quot;select fecha_creacion,fech_ini_verif,
        fech_fin_verif from ntm_p.tr_fecha_insumo where id_fecha_insumo='${tr_ident.id_ident}'&quot;&gt;
                &lt;field name=&quot;fecha_creacion&quot; column=&quot;fecha_creacion&quot; /&gt;
        &lt;field name=&quot;fech_ini_verif&quot; column=&quot;fech_ini_verif&quot; /&gt;
        &lt;field name=&quot;fech_fin_verif&quot; column=&quot;fech_fin_verif&quot; /&gt;
            &lt;/entity&gt;

       &lt;entity name=&quot;ti_fecha_evento&quot;
              query=&quot;select tipo_fecha,fecha_referencia from ntm_p.ti_fecha_evento where id_fecha_evento='${tr_ident.id_ident}'&quot;&gt;
            &lt;field column=&quot;fecha_referencia&quot; name=&quot;fecha_referencia&quot; /&gt;
            &lt;entity name=&quot;tc_tipo_fecha&quot; query=&quot;select des_tipo_fecha,id_tipo_fecha from ntm_p.tc_tipo_fecha where id_tipo_fecha='${ti_fecha_evento.tipo_fecha}'&quot;&gt;
                            &lt;field column=&quot;des_tipo_fecha&quot; name=&quot;des_tipo_fecha&quot; /&gt;
                &lt;field column=&quot;id_tipo_fecha&quot; name=&quot;id_tipo_fecha&quot; /&gt;
                    &lt;/entity&gt;
           &lt;/entity&gt;
      &lt;/entity&gt;
    &lt;/document&gt;
&lt;/dataConfig&gt;
</code></pre>
","transformer-model"
"63455286","is it possible to use an Icon as a rotater in react-konva Transformer","2020-08-17 16:54:13","63510822","2","1703","<reactjs><transformer-model><react-konva>","<p>I want to use a custom icon as rotater in react-konva transformer as shown below :</p>
<p><img src=""https://i.sstatic.net/dLFIf.png"" alt=""https://i.sstatic.net/dLFIf.png"" /></p>
","transformer-model"
"63435621","PyTorch Adding src_key_padding_mask in TransformerEncoder leads to inf loss","2020-08-16 10:13:40","","3","1691","<python><deep-learning><pytorch><transformer-model>","<p>I'm using <a href=""https://colab.research.google.com/drive/1g4ZFCGegOmD-xXL-Ggu7K5LVoJeXYJ75#scrollTo=cflC2xVxKb5M"" rel=""nofollow noreferrer"">this</a> code as base to build an own transformer model, using different inputs as presented there. The part in <code>class TransformerModel(nn.Module):</code> or in the following my own implementation shows some problems:</p>
<pre><code>def make_len_mask(self, inp):
    return (inp == 0).transpose(0, 1)


class TransformerModel(nn.Module):
    def __init__(self):
        encoder_layer = TransformerEncoderLayer(ninp, nhead, nhid, dropout)
        encoder_norm = LayerNorm(ninp)
        self.encoder = TransformerEncoder(encoder_layer, nlayers, encoder_norm)

    def forward(self, src, trg):
        src.shape # (x, y, z)
        trg.shape # (x, y)
        # eliminate last dimension of source tensor, which is (batch_size, samples, features) to compute mask
        # resulting in a [true, false]-Vector indicating which elements are padding elements
        padding_tensor = src.mean(2) # padding_tensor.shape: (x,y)
        src_pad_mask = self.make_len_mask(padding_tensor)
        # self.src.mask = None
        output = self.encoder(src, mask=self.src_mask, src_key_padding_mask=src_pad_mask)
</code></pre>
<p>Using that <code>src_pad_mask</code> results in a <code>ValueError: The loss returned in training_step is nan or inf.</code>. If not using that mask in <code>EncoderLayer</code> there are results.</p>
<p>My inputs are</p>
<pre><code>model(source, target)
</code></pre>
<p>where source are continuous and target are words structured as follows:</p>
<pre><code>target = [1] + [4,2,3,8] + [99]  # 0 and 99 are start and end of sentence tokens
</code></pre>
<p>I tried to remove them with <code>target_tensor[target_tensor == 1] = 0</code> or <code>target_tensor[target_tensor == 99] = 0</code> this unfortunately lead to a <code>RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: ...</code> so I'm providing the sequence with sos and eos tokens, which doesnt feel right, maybe the problem arises there? But the sequence is already padded there so its not possible to remove the last or first index as suggested in the example.</p>
<p>If using <code>nn.Transformer()</code> instead of single <code>EncoderLayer ()</code> the results are strongly overfitting without a mask or the same error arises using said mask. Using just a target_mask doesnt produce correct inputs.</p>
<p>Is there any possibility to find out where this error arises or if my mask is calculated wrongly? More <a href=""https://github.com/pytorch/tutorials/issues/719"" rel=""nofollow noreferrer"">discussion</a> on github. Is it necessary to provide masks <strong>during</strong> training or inference? If so Im not doing it, maybe someone could help or point to a source if its true?</p>
<pre><code># Values != 0 =&gt; False
# Values == 0 =&gt; True
src_pad_mask:
tensor([[False],
        [False],
        ...
        [ True],
        [ True]], device='cuda:0')
</code></pre>
","transformer-model"
"63298454","Speed up generation of USE(universal sentence encoder) embeddings","2020-08-07 08:52:13","","0","823","<tensorflow><machine-learning><deep-learning><data-science><transformer-model>","<p>I am working on a semantic similarity problem using <a href=""https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder"" rel=""nofollow noreferrer"">universal sentence encoder</a>. The dataset contains abstracts of scholarly articles. The mean length is around 1500. There are ~300k records in data and it will take quite long to generate USE embedding for all of them. I am looking for ways to optimize this. Currently, generating embedding for 10k rows of data took ~15 mins.</p>
<pre><code>from tqdm import tqdm    
use_module_url = &quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot;

model = hub.load(use_module_url)
print (&quot;module %s loaded&quot; % use_module_url)

def embed(input):
  return model(input)

def get_features(texts):
        if type(texts) is str:
            texts = [texts]
        return embed(texts)    

def data_iterator(data):
  chunk_list = []
  for x in tqdm(range(0, len(data), 1000)):
    if x+1000 &gt; len(data):
      chunk_list.append(data[x:len(data)])
    else:
      chunk_list.append(data[x:x+1000])
  return chunk_list

data = df['text'][:10000].values
data_processed = list(map(process_text, data))
</code></pre>
<p>Here, I want to speed up the generation of USE embeddings for my data. I am experimenting in kaggle kernel and have turned on the GPU. The GPU utilization doesn`t go beyond 2-3% &amp; CPU utilization was ~120%</p>
<pre><code>%%time
BASE_VECTORS = []

chunk_list = data_iterator(data_processed)

for i in tqdm(chunk_list):
    BASE_VECTORS_tmp = get_features(i)
    BASE_VECTORS.extend(BASE_VECTORS_tmp)

BASE_VECTORS = np.asarray(BASE_VECTORS)
</code></pre>
<p>Time taken
CPU times: user 16min 48s, sys: 2min 59s, total: 19min 47s
Wall time: 15min 13s</p>
","transformer-model"
"63295569","Explanation about i//2 in positional encoding in tensorflow tutorial about transformers","2020-08-07 05:09:02","","1","2544","<tensorflow><encoding><nlp><transformer-model><attention-model>","<p>I was implementing the transformer architecture in tensorflow.</p>
<p>I was following the tutorial : <a href=""https://www.tensorflow.org/tutorials/text/transformer#setup_input_pipeline"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/transformer#setup_input_pipeline</a></p>
<p>They implement the positional encoding in this way:</p>
<pre><code>angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))
</code></pre>
<p>However in the paper i is not divided by 2 (i//2), is this a bug? , or why is the reason to make this operation?</p>
<p><a href=""https://i.sstatic.net/67ADh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/67ADh.png"" alt=""pos"" /></a></p>
<p>Thankyou</p>
","transformer-model"
"63216550","Positional Embedding in the Transformer model - does it change the word's meaning?","2020-08-02 13:27:04","63231334","0","724","<nlp><transformer-model>","<p>I am reading the Transformer paper, and the Positional Embeddings make me wonder a thing:</p>
<p>Assume that the word &quot;cat&quot; is pretrained to be embedded to the word vector <code>[2,3,1,4]</code>. If we use the positional encoding that turns the vector into a new one, like <code>[3,1,5,2]</code>, should not it change also the word's meaning in the word2vec matrix? Since the corpus is large, a slight change in the value can also change its meaning.</p>
","transformer-model"
"63198951","How can I efficiently convert a list of object ADto to a list of object BVo? Both objects have the same property values","2020-07-31 20:52:30","","0","36","<java><list><dto><transformer-model>","<p>I have one class:</p>
<pre><code>public class ADto
{
    private double c;
    private String d;
    private String e;
}
</code></pre>
<p>and another class</p>
<pre><code>public class BVo
{
    private double c;
    private String d;
    private String e;
}
</code></pre>
<p>I have a list of BVo:</p>
<pre><code>List&lt;BVo&gt; vo;
</code></pre>
<p>that I need to utilize a transformer to transform into a list of Dto's</p>
<pre><code>List&lt;ADto&gt; dto;
</code></pre>
<p>This list of ADtos will then be put into a wrapper DTO and returned from my endpoint.</p>
<p>Is there an easy way to efficiently do this? Efficiency is key in my implementation.</p>
","transformer-model"
"63197956","seq2seq model transformer model - what's the best way to batchify my inputs?","2020-07-31 19:22:38","","1","148","<nlp><pytorch><transformer-model><seq2seq>","<p>I'm trying to build a character-level model that matches diacritics for Hebrew characters (each character is decorated with a diacritic). Note that the correct diacritic is dependent on the word, the context and the part-of-speech (not trivial).</p>
<p>I built an LSTM based model which achieves 18% word-level accuracy (18% of the words were exactly right in all their characters, on an unseen test set)</p>
<p>Now I'm trying to beat that with a transformer model, following <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">pytorch seq-2-seq tutorial</a>, and I'm reaching far worse results (7% word-level accuracy).</p>
<p>My training dataset is 100K sentences, most with up to 30 characters, but some go all the way to 80 characters.</p>
<p>My question (finally) - what's the best way to batchify these inputs for the transformer? I prepared 30-characters chunks that cover each sentence (e.g. a 55 characters sentence =&gt; 30 + 25) and padded with zeros when a chunk is shorter than 30. I'm also trying to split the chunks between words (on spaces) and not in mid-word.</p>
<p>Is this the way to go? Am I missing some better (and better-known) technique?</p>
","transformer-model"
"63178631","Should the queries, keys and values of the transformer be split before or after being passed through the linear layers?","2020-07-30 17:48:33","75849699","4","630","<deep-learning><nlp><pytorch><transformer-model><attention-model>","<p>I have seen two different implementations of Multi-Head Attention.</p>
<ol>
<li>In one of the approaches the queries, keys and values are split into heads before being passed through the linear layers as shown below:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>
    def split_heads(self, x, batch_size):
        return x.reshape(batch_size, -1, self.heads, self.projection_dim)

    def forward(self, queries, keys, values, mask):
        batch_size = queries.size()[0]

        # split queries keys and values into heads
        queries = self.split_heads(queries, batch_size)
        keys = self.split_heads(keys, batch_size)
        values = self.split_heads(values, batch_size)

        queries = self.queries_linear(queries)
        keys = self.keys_linear(keys)
        values = self.values_linear(values)
        #...more code

</code></pre>
<ol start=""2"">
<li>The second approach is to split the queries, keys and values into heads after passing them through linear layers:</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>   def forward(self, queries, keys, values, mask=None):
        
        batch_size = q.size(0)
        
        # perform linear operation and split into h heads
        k = self.keys_linear(keys).view(batch_size, -1, self.heads, self.projection_dim)
        q = self.queries_linear(queries).view(batch_size, -1, self.heads, self.projection_dim)
        v = self.values_linear(values).view(batch_size, -1, self.heads, self.projection_dim)
        #...more code
        
</code></pre>
<p>According to the paper <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention Is All You Need</a>, from what I can deduce from the image the queries and keys should be split before being passed through the linear layers, but from most implementation online they are split after. <a href=""https://i.sstatic.net/dgwDr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dgwDr.png"" alt=""Multi-Head Attention"" /></a></p>
<p>Are the two approaches similar or is one better than the other?</p>
","transformer-model"
"63111754","Develop Question Answer System using BERT","2020-07-27 08:48:01","","0","486","<tensorflow><nlp><information-retrieval><transformer-model><nlp-question-answering>","<p>i'm currently developing a Question-Answer system (<strong>in Indonesian language</strong>) using BERT for my thesis.
The dataset and the questions given are in Indonesian.</p>
<p>The problem is, i'm still not clear on how the step-to-step process to develop the Question-Answer system in BERT.</p>
<p>From what I concluded after reading a number of research journals and papers, the process might be like this:</p>
<ol>
<li>Prepare main dataset</li>
<li>Load Pre-Train Data</li>
<li>Train the main dataset with the pre-train data (so that it produce &quot;fine-tuned&quot; model)</li>
<li>Cluster the fine-tuned model</li>
<li>Testing (giving questions to the system)</li>
<li>Evaluation</li>
</ol>
<p>What i want to ask are :</p>
<ul>
<li>Are those steps correct? Or maybe there any missing step(s)?</li>
<li>Also, if the default pre-train data that BERT provide is in English while my main dataset is in Indonesian, how can i create my own indonesian pre-train data?</li>
<li>Does it really need to perform data/model clustering in BERT?</li>
</ul>
<p>I appreciate any helpful answer(s).
Thank you very much in advance.</p>
","transformer-model"
"63045229","How to avoid iterating over Dataloader while resuming training in Huggingface Trainer class?","2020-07-23 00:54:12","67606464","5","1201","<pytorch><transformer-model><huggingface-transformers>","<p>I'm currently using Huggingface's Trainer class to train Distillbert for a regression problem using a custom loss function. I'm using their checkpoints to resume training due to the ephemeral nature of compute / unexpected errors.</p>
<p>The issue I'm facing is that each time I resume training from a checkpoint as per their Trainer class via the <code>model_path</code> in the <code>Trainer.train()</code> method, I noticed that the class iterates over the dataloader until it reaches the iteration count as saved in the checkpoint (<a href=""https://github.com/huggingface/transformers/blob/33d7506ea10ca92886fd1bb3b5306a1a720c58fe/src/transformers/trainer.py#L500"" rel=""noreferrer"">see the lines from the Trainer class that match the issue</a>).</p>
<p>This might usually not be a issue, but due to the nature of my dataloader's collate function and the size of the dataset, iterating for such a duration without any training is pretty expensive and slows down the overall training.</p>
<p>I planned on utilizing a custom sampler class <a href=""https://discuss.pytorch.org/t/resume-iterating-dataloader-from-checkpoint-batch-idx/60683"" rel=""noreferrer"">something along the lines of this</a> with a parameter to resume the indices from a given location but that too seems quite the hack for the given problem.</p>
<p>What could be an alternative that I could try to save on this wasted compute cycles?</p>
","transformer-model"
"63030692","How do I use BertForMaskedLM or BertModel to calculate perplexity of a sentence?","2020-07-22 09:07:00","66593509","9","8251","<nlp><pytorch><transformer-model><huggingface-transformers><bert-language-model>","<p>I want to use BertForMaskedLM or BertModel to calculate perplexity of a sentence, so I write code like this:</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertForMaskedLM
# Load pre-trained model (weights)
with torch.no_grad():
    model = BertForMaskedLM.from_pretrained('hfl/chinese-bert-wwm-ext')
    model.eval()
    # Load pre-trained model tokenizer (vocabulary)
    tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm-ext')
    sentence = &quot;我不会忘记和你一起奋斗的时光。&quot;
    tokenize_input = tokenizer.tokenize(sentence)
    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])
    sen_len = len(tokenize_input)
    sentence_loss = 0.

    for i, word in enumerate(tokenize_input):
        # add mask to i-th character of the sentence
        tokenize_input[i] = '[MASK]'
        mask_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])

        output = model(mask_input)

        prediction_scores = output[0]
        softmax = nn.Softmax(dim=0)
        ps = softmax(prediction_scores[0, i]).log()
        word_loss = ps[tensor_input[0, i]]
        sentence_loss += word_loss.item()

        tokenize_input[i] = word
    ppl = np.exp(-sentence_loss/sen_len)
    print(ppl)
</code></pre>
<p>I think this code is right, but I also notice BertForMaskedLM's paramaters <code>masked_lm_labels</code>, so could I use this paramaters to calculate PPL of a sentence easiler?
I know the input_ids argument is the masked input, the masked_lm_labels argument is the desired output. But I couldn't understand the actual meaning of its output loss, its code like this:</p>
<pre><code>if masked_lm_labels is not None:
    loss_fct = CrossEntropyLoss()  # -100 index = padding token
    masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), 
    masked_lm_labels.view(-1))
    outputs = (masked_lm_loss,) + outputs
</code></pre>
","transformer-model"
"62982346","How to finetune distillbart for abstractive summarization using Gigaword or Cnn dailymail?","2020-07-19 15:38:16","","2","590","<transformer-model><huggingface-transformers><summarization><bert-language-model>","<p>I would like to ask about how to finetune distillbart on gigaword and cnn dailymail with the starting checkpoint <a href=""https://huggingface.co/sshleifer/distilbart-cnn-12-6"" rel=""nofollow noreferrer"">distilbart-cnn-12-6</a>.
I did use the gigaword dataset provided by tensorflow but it replaces numbers by this character: &quot;#&quot;, as a result, my summaries have # instead of numbers, is it normal that it has those # ?
Also is it really possible to finetune distillbart from the checkpoint distilbart-cnn-12-6 with cnn daily mail?</p>
<pre><code>import os
os.environ['PYTHONPATH'] += &quot;:/content/transformers/examples&quot;
%cd &quot;/content/transformers/examples&quot;

!python /content/transformers/examples/seq2seq/finetune.py \
    --learning_rate=3e-5 \
    --fp16 \
    --gpus 1 \
    --do_train \
    --do_predict \
    --n_val 1000 \
    --val_check_interval 0.1 \
    --sortish_sampler \
    --data_dir '/content/dataset' \
    --train_batch_size=4 \
    --eval_batch_size=4 \
    --output_dir=distilbart_1300k_1400k \
    --num_train_epochs 1 \
    --model_name_or_path /content/transformers/examples/distilbart_1200k_1300k/best_tfmr
</code></pre>
<p>here is the link to gigaword: <a href=""https://www.tensorflow.org/datasets/catalog/gigaword"" rel=""nofollow noreferrer"">https://www.tensorflow.org/datasets/catalog/gigaword</a>
and here is the link to cnn dailymail: <a href=""https://www.tensorflow.org/datasets/catalog/cnn_dailymail"" rel=""nofollow noreferrer"">https://www.tensorflow.org/datasets/catalog/cnn_dailymail</a></p>
<p>For the code I followed the instruction of fine tuning distillabart here:
<a href=""https://github.com/Hildweig/transformers/tree/master/examples/seq2seq"" rel=""nofollow noreferrer"">https://github.com/Hildweig/transformers/tree/master/examples/seq2seq</a></p>
<p>For the outputs with gigawords I get something like this:
&quot;foreign exchange rates in hong kong sept. ## #### ; china 's defense minister says he 's ready to work with yugoslavia 's foreign minister on iraq 's role in iraq with bc-me-gen iraq&quot;</p>
","transformer-model"
"62961194","How does BertForSequenceClassification classify on the CLS vector?","2020-07-17 20:14:29","62981360","5","3205","<python><transformer-model><huggingface-transformers><bert-language-model>","<p><strong>Background:</strong></p>
<p>Following along with this <a href=""https://stackoverflow.com/questions/60876394/does-bertforsequenceclassification-classify-on-the-cls-vector"">question</a> when using bert to classify sequences the model uses the &quot;[CLS]&quot; token representing the classification task. According to the paper:</p>
<blockquote>
<p>The first token of every sequence is always a special classification
token ([CLS]). The final hidden state corresponding to this token is
used as the aggregate sequence representation for classification
tasks.</p>
</blockquote>
<p>Looking at the huggingfaces repo their BertForSequenceClassification utilizes the bert pooler method:</p>
<pre><code>class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        # We &quot;pool&quot; the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
</code></pre>
<p>We can see they take the first token (CLS) and use this as a representation for the whole sentence. Specifically they perform <code>hidden_states[:, 0]</code> which looks a lot like its taking the first element from each state rather than taking the first tokens hidden state?</p>
<p><strong>My Question:</strong></p>
<p>What I don't understand is how do they encode the information from the entire sentence into this token? Is the CLS token a regular token which has its own embedding vector that &quot;learns&quot; the sentence level representation? Why can't we just use the average of the hidden states (the output of the encoder) and use this to classify?</p>
<p><strong>EDIT</strong>: After thinking a little about it: Because we use the CLS tokens hidden state to predict, is the CLS tokens embedding being trained on the task of classification as this is the token being used to classify (thus being the major contributor to the error which gets propagated to its weights?)</p>
","transformer-model"
"62931082","GPU memory leakage when creating objects from sentence-transformers","2020-07-16 08:56:12","63502685","2","1956","<r><python-3.x><pytorch><transformer-model><reticulate>","<h1><strong>Description</strong></h1>
<p>I am creating a function in R that embeds sentences using the <code>sentence_transformers</code> library from Python.</p>
<p>For some unknown reason, creating the object multiple times under the same variable name ends up in insufficient memory space to allocate the transformer. To reproduce:</p>
<pre><code>sentence_transformers &lt;- reticulate::import(&quot;sentence_transformers&quot;)
for (i in 1:10) {
  print(i)
  bert_encoder &lt;- sentence_transformers$SentenceTransformer(&quot;bert-large-nli-stsb-mean-tokens&quot;)
}
</code></pre>
<p>However, doing the same operation directly on Python does not produce an error</p>
<pre><code>from sentence_transformers import SentenceTransformer
for i in range(10):
    print(i)
    bert_encoder = SentenceTransformer(&quot;bert-large-nli-stsb-mean-tokens&quot;)
}
</code></pre>
<p>This happens with any model that is allocated in GPU. On my NVIDIA GTX 1060 it reaches the 4th cycle, but on smaller GPUs it crashes earlier. One temporal solution is to create the model outside only once, and then pass the model as a parameter to the function as many times as wanted, but I would rather avoid that because it adds an extra step and in any case calling multiple models might just make it crash as well.</p>
<h1><strong>Expected behaviour</strong></h1>
<p>The for loop finishes without an error</p>
<h1><strong>Observed behaviour</strong></h1>
<p><em>Error in py_call_impl(callable, dots$args, dots$keywords) :
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 6.00 GiB total capacity; 2.95 GiB already allocated; 16.11 MiB free; 238.68 MiB cached)</em></p>
<h1><strong>Unsuccesful attemps at solving it</strong></h1>
<ol>
<li>The solutions proposed <a href=""https://github.com/tensorflow/tensorflow/issues/36465"" rel=""nofollow noreferrer"">here</a></li>
<li>Using numba as suggested <a href=""https://stackoverflow.com/a/60354785/5559413"">here</a></li>
<li>Declaring the variable explicitely on Python via <code>reticulate::py_run_string()</code> and then doing <code>del bert_encoder</code> and calling the garbage collector</li>
</ol>
<h1><strong>Details</strong></h1>
<p>Windows 10 Home</p>
<p>Python 3.7.4</p>
<p>R 4.0.1</p>
<p>Reticulate 1.16</p>
<p>Torch 1.3.1</p>
<p>Tensorflow 2.2.0</p>
<p>Transformers 2.11.0</p>
<p>sentence_transformers 0.2.6</p>
","transformer-model"
"62852940","How to get immediate next word probability using GPT2 model?","2020-07-11 18:19:30","62911442","14","5955","<transformer-model><huggingface-transformers>","<p>I was trying the hugging face gpt2 model. I have seen the <a href=""https://github.com/huggingface/transformers/blob/master/examples/text-generation/run_generation.py"" rel=""noreferrer""><code>run_generation.py</code> script</a>, which generates a sequence of tokens given a prompt. I am aware that we can use GPT2 for NLG.</p>
<p>In my use case, I wish to determine the probability distribution for (only) the immediate next word following the given prompt. Ideally this distribution would be over the entire vocab.</p>
<p>For example, given the prompt: &quot;How are &quot;, it should give a probability distribution where &quot;you&quot; or &quot;they&quot; have the some high floating point values and other vocab words have very low floating values.</p>
<p>How to do this using hugging face transformers? If it is not possible in hugging face, is there any other transformer model that does this?</p>
","transformer-model"
"62825520","Hierarchical transformer for document classification: model implementation error, extracting attention weights","2020-07-10 00:16:16","","1","631","<python><tensorflow><keras><transformer-model><attention-model>","<p>I am trying to implement a hierarchical transformer for document classification in Keras/tensorflow, in which:</p>
<p>(1) a word-level transformer produces a representation of each sentence, and attention weights for each word, and,</p>
<p>(2) a sentence-level transformer uses the outputs from (1) to produce a representation of each document, and attention weights for each sentence, and finally,</p>
<p>(3) the document representations produced by (2) are used to classify documents (in the following example, as belonging or not belonging to a given class).</p>
<p>I am attempting to model the classifier on Yang et al.'s approach here (<a href=""https://www.cs.cmu.edu/%7E./hovy/papers/16HLT-hierarchical-attention-networks.pdf"" rel=""nofollow noreferrer"">https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf</a>), but replacing the GRU and attention layers with transformers.</p>
<p>I am using Apoorv Nandan's transformer implementation from <a href=""https://keras.io/examples/nlp/text_classification_with_transformer/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/text_classification_with_transformer/</a>.</p>
<p>I have two issues for which I would be grateful for the community's help:</p>
<p><strong>(1) I get an error in the upper (sentence) level model that I can't resolve (details and code below)</strong></p>
<p><strong>(2) I don't know how to extract the word- and sentence-level attention weights, and value advice on how best to do this.</strong></p>
<p>I am new to both Keras and this forum, so apologies for obvious mistakes and thank you in advance for any help.</p>
<p>Here is a reproducible example, indicating where I encounter errors:</p>
<p>First, establish the multi-head attention, transformer, and token/position embedding layers, after Nandan.</p>
<pre><code>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import pandas as pd
import numpy as np

class MultiHeadSelfAttention(layers.Layer):
    def __init__(self, embed_dim, num_heads=8):
        super(MultiHeadSelfAttention, self).__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        if embed_dim % num_heads != 0:
            raise ValueError(
                f&quot;embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}&quot;
            )
        self.projection_dim = embed_dim // num_heads
        self.query_dense = layers.Dense(embed_dim)
        self.key_dense = layers.Dense(embed_dim)
        self.value_dense = layers.Dense(embed_dim)
        self.combine_heads = layers.Dense(embed_dim)

    def attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output, weights

    def separate_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, inputs):
        # x.shape = [batch_size, seq_len, embedding_dim]
        batch_size = tf.shape(inputs)[0]
        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)
        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)
        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)
        query = self.separate_heads(
            query, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        key = self.separate_heads(
            key, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        value = self.separate_heads(
            value, batch_size
        )  # (batch_size, num_heads, seq_len, projection_dim)
        attention, weights = self.attention(query, key, value)
        attention = tf.transpose(
            attention, perm=[0, 2, 1, 3]
        )  # (batch_size, seq_len, num_heads, projection_dim)
        concat_attention = tf.reshape(
            attention, (batch_size, -1, self.embed_dim)
        )  # (batch_size, seq_len, embed_dim)
        output = self.combine_heads(
            concat_attention
        )  # (batch_size, seq_len, embed_dim)
        return output

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate, name=None):
        super(TransformerBlock, self).__init__(name=name)
        self.att = MultiHeadSelfAttention(embed_dim, num_heads)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation=&quot;relu&quot;), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(dropout_rate)
        self.dropout2 = layers.Dropout(dropout_rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim, name=None):
        super(TokenAndPositionEmbedding, self).__init__(name=name)
        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions
</code></pre>
<p>For the purpose of this example, the data are 10,000 documents, each truncated to 15 sentences, each sentence with a maximum of 60 words, which are already converted to integer tokens 1-1000.</p>
<p>X is a 3-D tensor (10000, 15, 60) containing these tokens. y is a 1-D tensor containing the classes of the documents (1 or 0). For the purpose of this example there is no relation between X and y.</p>
<p>The following produces the example data:</p>
<pre><code>max_docs = 10000
max_sentences = 15
max_words = 60

X = tf.random.uniform(shape=(max_docs, max_sentences, max_words), minval=1, maxval=1000, dtype=tf.dtypes.int32, seed=1)

y = tf.random.uniform(shape=(max_docs,), minval=0, maxval=2, dtype=tf.dtypes.int32, seed=1)
</code></pre>
<p>Here I attempt to construct the word level encoder, after <a href=""https://keras.io/examples/nlp/text_classification_with_transformer/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/text_classification_with_transformer/</a>:</p>
<pre><code># Lower level (produce a representation of each sentence):

embed_dim = 100 # Embedding size for each token
num_heads = 2  # Number of attention heads
ff_dim = 64  # Hidden layer size in feed forward network inside transformer
L1_dense_units = 100 # Size of the sentence-level representations output by the word-level model
dropout_rate = 0.1
vocab_size=1000

word_input = layers.Input(shape=(max_words,), name='word_input') 
word_embedding = TokenAndPositionEmbedding(maxlen=max_words, vocab_size=vocab_size, 
                                           embed_dim=embed_dim, name='word_embedding')(word_input) 
word_transformer = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim, 
                                    dropout_rate=dropout_rate, name='word_transformer')(word_embedding)
word_pool = layers.GlobalAveragePooling1D(name='word_pooling')(word_transformer) 
word_drop = layers.Dropout(dropout_rate,name='word_drop')(word_pool)
word_dense = layers.Dense(L1_dense_units, activation=&quot;relu&quot;,name='word_dense')(word_drop)
word_encoder = keras.Model(word_input, word_dense) 

word_encoder.summary()
</code></pre>
<p>It looks as though this word encoder works as intended to produce a representation of each sentence. Here, run on the 1st document, it produces a tensor of shape (15, 100), containing the vectors representing each of 15 sentences:</p>
<pre><code>word_encoder(X[0]).shape
</code></pre>
<p>My problem is in connecting this to the higher (sentence) level model, to produce document representations.</p>
<p><strong>I get error &quot;NotImplementedError&quot; when trying to apply the word encoder to each sentence in a document. I would be grateful for any help in fixing this issue, since the error message is not informative as to the specific problem.</strong></p>
<p>After applying the word encoder to each sentence, the goal is to apply another transformer to produce attention weights for each sentence, and a document-level representation with which to perform classification. I can't determine whether this part of the model will work because of the error above.</p>
<p><strong>Finally, I would like to extract word- and sentence-level attention weights for each document, and would be grateful for advice on how to do so.</strong></p>
<p>Thank you in advance for any insight.</p>
<pre><code># Upper level (produce a representation of each document):

L2_dense_units = 100

sentence_input = layers.Input(shape=(max_sentences, max_words), name='sentence_input') 

# This is the line producing &quot;NotImplementedError&quot;:
sentence_encoder = tf.keras.layers.TimeDistributed(word_encoder, name='sentence_encoder')(sentence_input) 

sentence_transformer = TransformerBlock(embed_dim=L1_dense_units, num_heads=num_heads, ff_dim=ff_dim, 
                               dropout_rate=dropout_rate, name='sentence_transformer')(sentence_encoder)
sentence_dense = layers.TimeDistributed(Dense(int(L2_dense_units)),name='sentence_dense')(sentence_transformer)
sentence_out = layers.Dropout(dropout_rate)(sentence_dense)
preds = layers.Dense(1, activation='sigmoid', name='sentence_output')(sentence_out)

model = keras.Model(sentence_input, preds) 
model.summary()
</code></pre>
","transformer-model"
"62636875","Difference between testing and inference in seq2seq models","2020-06-29 11:12:45","","2","2061","<machine-learning><training-data><transformer-model><seq2seq>","<p>I am studying transformer model with <a href=""https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"" rel=""nofollow noreferrer"">this tutorial</a> and I am confused on the difference between evaluation and inference. In my understanding evaluation happens after the model is trained, by only giving it source and ask it to predict the target one by one (in the seq2seq problem).</p>
<p>However, in the tutorial it does evaluation in the same way as training, which is getting the loss from a forward pass to the model, given both source and target. And the inference step is more similar to what I understand as evaluation. In this case, I tried the model and it does really well with evaluation and testing, but at the inference step I found that it can't output anything meaningful.
Can anyone explain me the difference between evaluation and inference?</p>
","transformer-model"
"62629644","what the difference between att_mask and key_padding_mask in MultiHeadAttnetion","2020-06-29 00:31:03","62633542","22","10588","<python><deep-learning><pytorch><transformer-model><attention-model>","<p>What the difference between <code>att_mask</code> and <code>key_padding_mask</code> in <code>MultiHeadAttnetion</code> of pytorch:</p>
<blockquote>
<p><strong>key_padding_mask</strong> – if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. When given a byte mask and a value is non-zero, the corresponding value on the attention layer will be ignored</p>
</blockquote>
<blockquote>
<p><strong>attn_mask</strong> – 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.</p>
</blockquote>
<p>Thanks in advance.</p>
","transformer-model"
"62608037","TypeError: Expected any non-tensor type, got a tensor instead","2020-06-27 09:11:31","","5","13117","<tensorflow><keras><deep-learning><chatbot><transformer-model>","<p>I Was following a post on '<a href=""https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/transformer_chatbot.ipynb#scrollTo=K16BIGSKfkve"" rel=""noreferrer"">Training a transformer model for a chatbot with TensorFlow 2.0</a>'. I have encountered an error on my local machine although the code seems to work <strong>fine</strong> in colab. Below is the code snippet.</p>
<pre><code>def encoder_layer(units, d_model, num_heads, dropout, name=&quot;encoder_layer&quot;):
  inputs = tf.keras.Input(shape=(None, d_model), name=&quot;inputs&quot;)
  padding_mask = tf.keras.Input(shape=(1, 1, None), name=&quot;padding_mask&quot;)

  attention = MultiHeadAttention(
      d_model, num_heads, name=&quot;attention&quot;)({
          'query': inputs,
          'key': inputs,
          'value': inputs,
          'mask': padding_mask
      })
  attention = tf.keras.layers.Dropout(rate=dropout)(attention)
  attention = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(inputs + attention)

  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)
  outputs = tf.keras.layers.Dense(units=d_model)(outputs)
  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)
  outputs = tf.keras.layers.LayerNormalization(
      epsilon=1e-6)(attention + outputs)

  return tf.keras.Model(
      inputs=[inputs, padding_mask], outputs=outputs, name=name)
</code></pre>
<p>I called above function with the following function call;</p>
<pre><code>sample_encoder_layer = encoder_layer(
    units=512,
    d_model=128,
    num_heads=4,
    dropout=0.3,
    name=&quot;sample_encoder_layer&quot;)
</code></pre>
<p>Below is the traceback of the error:</p>
<pre><code>---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)
    323   try:
--&gt; 324     fn(values)
    325   except ValueError as e:

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in _check_not_tensor(values)
    275 def _check_not_tensor(values):
--&gt; 276   _ = [_check_failed(v) for v in nest.flatten(values)
    277        if isinstance(v, ops.Tensor)]

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in &lt;listcomp&gt;(.0)
    276   _ = [_check_failed(v) for v in nest.flatten(values)
--&gt; 277        if isinstance(v, ops.Tensor)]
    278 # pylint: enable=invalid-name

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in _check_failed(v)
    247   # it is safe to use here.
--&gt; 248   raise ValueError(v)
    249 

ValueError: Tensor(&quot;attention_1/Identity:0&quot;, shape=(None, None, 128), dtype=float32)

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
&lt;ipython-input-20-3fa05a9bbfda&gt; in &lt;module&gt;
----&gt; 1 sample_encoder_layer = encoder_layer(units=512, d_model=128, num_heads=4, dropout=0.3, name='sample_encoder_layer')
      2 
      3 tf.keras.utils.plot_model(
      4     sample_encoder_layer, to_file='encoder_layer.png', show_shapes=True)

&lt;ipython-input-18-357ca53de1c0&gt; in encoder_layer(units, d_model, num_heads, dropout, name)
     10           'mask': padding_mask
     11       })
---&gt; 12   attention = tf.keras.layers.Dropout(rate=dropout)(attention)
     13   attention = tf.keras.layers.LayerNormalization(
     14       epsilon=1e-6)(inputs + attention)

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    920                     not base_layer_utils.is_in_eager_or_tf_function()):
    921                   with auto_control_deps.AutomaticControlDependencies() as acd:
--&gt; 922                     outputs = call_fn(cast_inputs, *args, **kwargs)
    923                     # Wrap Tensors in `outputs` in `tf.identity` to avoid
    924                     # circular dependencies.

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, training)
    209     output = tf_utils.smart_cond(training,
    210                                  dropped_inputs,
--&gt; 211                                  lambda: array_ops.identity(inputs))
    212     return output
    213 

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/keras/utils/tf_utils.py in smart_cond(pred, true_fn, false_fn, name)
     63         pred, true_fn=true_fn, false_fn=false_fn, name=name)
     64   return smart_module.smart_cond(
---&gt; 65       pred, true_fn=true_fn, false_fn=false_fn, name=name)
     66 
     67 

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     57   else:
     58     return control_flow_ops.cond(pred, true_fn=true_fn, false_fn=false_fn,
---&gt; 59                                  name=name)
     60 
     61 

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--&gt; 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in cond(pred, true_fn, false_fn, strict, name, fn1, fn2)
   1175   if (util.EnableControlFlowV2(ops.get_default_graph()) and
   1176       not context.executing_eagerly()):
-&gt; 1177     return cond_v2.cond_v2(pred, true_fn, false_fn, name)
   1178 
   1179   # We needed to make true_fn/false_fn keyword arguments for

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/ops/cond_v2.py in cond_v2(pred, true_fn, false_fn, name)
     82             true_name, collections=ops.get_default_graph()._collections),  # pylint: disable=protected-access
     83         add_control_dependencies=add_control_dependencies,
---&gt; 84         op_return_value=pred)
     85     false_graph = func_graph_module.func_graph_from_py_func(
     86         false_name,

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    979         _, original_func = tf_decorator.unwrap(python_func)
    980 
--&gt; 981       func_outputs = python_func(*func_args, **func_kwargs)
    982 
    983       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py in dropped_inputs()
    205           noise_shape=self._get_noise_shape(inputs),
    206           seed=self.seed,
--&gt; 207           rate=self.rate)
    208 
    209     output = tf_utils.smart_cond(training,

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--&gt; 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in dropout(x, keep_prob, noise_shape, seed, name, rate)
   4341     raise ValueError(&quot;You must provide a rate to dropout.&quot;)
   4342 
-&gt; 4343   return dropout_v2(x, rate, noise_shape=noise_shape, seed=seed, name=name)
   4344 
   4345 

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py in dropout_v2(x, rate, noise_shape, seed, name)
   4422       raise ValueError(&quot;rate must be a scalar tensor or a float in the &quot;
   4423                        &quot;range [0, 1), got %g&quot; % rate)
-&gt; 4424     x = ops.convert_to_tensor(x, name=&quot;x&quot;)
   4425     x_dtype = x.dtype
   4426     if not x_dtype.is_floating:

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1339 
   1340     if ret is None:
-&gt; 1341       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1342 
   1343     if ret is NotImplemented:

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    319                                          as_ref=False):
    320   _ = as_ref
--&gt; 321   return constant(v, dtype=dtype, name=name)
    322 
    323 

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    260   &quot;&quot;&quot;
    261   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--&gt; 262                         allow_broadcast=True)
    263 
    264 

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    298       tensor_util.make_tensor_proto(
    299           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--&gt; 300           allow_broadcast=allow_broadcast))
    301   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    302   const_tensor = g._create_op_internal(  # pylint: disable=protected-access

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    449       nparray = np.empty(shape, dtype=np_dt)
    450     else:
--&gt; 451       _AssertCompatible(values, dtype)
    452       nparray = np.array(values, dtype=np_dt)
    453       # check to them.

~/anaconda3/envs/tf-chatbot/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py in _AssertCompatible(values, dtype)
    326     [mismatch] = e.args
    327     if dtype is None:
--&gt; 328       raise TypeError(&quot;Expected any non-tensor type, got a tensor instead.&quot;)
    329     else:
    330       raise TypeError(&quot;Expected %s, got %s of type '%s' instead.&quot; %

TypeError: Expected any non-tensor type, got a tensor instead.
</code></pre>
","transformer-model"
"62567796","Is the difference between autoregressive transformer and non-autoregressive transformer is only the attention masks in decoder?","2020-06-25 04:17:15","","2","518","<pytorch><transformer-model><huggingface-transformers>","<p>The way i think is that,</p>
<p>The attention mask (not padding mask) for decoder of an autoregressive transformer model is a subsequent generated mask like as</p>
<pre class=""lang-sh prettyprint-override""><code>[0, 1, 1, 1, 1]
[0, 0, 1, 1, 1]
[0, 0, 0, 1, 1]
[0, 0, 0, 0, 1]
</code></pre>
<p>While there's no attention mask being fed to the decoder of non-autoregressive transformer model, there's only a padding mask.</p>
<p>Am i correct?</p>
","transformer-model"
"62490113","ALBERT not converging - HuggingFace","2020-06-20 18:36:23","","4","836","<machine-learning><nlp><text-classification><transformer-model><huggingface-transformers>","<p>I'm trying to apply a pretrained HuggingFace ALBERT transformer model to my own text classification task, but the loss is not decreasing beyond a certain point.</p>
<p>Here's my code:</p>
<p><strong>There are four labels in my text classification dataset which are:</strong></p>
<pre><code>0, 1, 2, 3
</code></pre>
<p><strong>Define the tokenizer</strong></p>
<pre><code>maxlen=25
albert_path = 'albert-large-v1'
from transformers import AlbertTokenizer, TFAlbertModel, AlbertConfig
tokenizer = AlbertTokenizer.from_pretrained(albert_path, do_lower_case=True, add_special_tokens=True,
                                                max_length=maxlen, pad_to_max_length=True)
</code></pre>
<p><strong>Encode all sentences in text, using the tokenizer</strong></p>
<pre><code>encodings = []
for t in text:
  encodings.append(tokenizer.encode(t, max_length=maxlen, pad_to_max_length=True, add_special_tokens=True))
</code></pre>
<p><strong>Define the pretrained transformer model and add Dense layer on top</strong></p>
<pre><code>    from tensorflow.keras.layers import Input, Flatten, Dropout, Dense
    from tensorflow.keras import Model

    optimizer = tf.keras.optimizers.Adam(learning_rate= 1e-4)
    token_inputs = Input((maxlen), dtype=tf.int32, name='input_word_ids')
    config = AlbertConfig(num_labels=4, dropout=0.2, attention_dropout=0.2)
    albert_model = TFAlbertModel.from_pretrained(pretrained_model_name_or_path=albert_path, config=config)

    X = albert_model(token_inputs)[1] 
    X = Dropout(0.2)(X)
    output_= Dense(4, activation='softmax', name='output')(X)

    bert_model2 = Model(token_inputs,output_)
    print(bert_model2.summary())
    
    bert_model2.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')
</code></pre>
<p><strong>Finally, feed the encoded text and labels to the model</strong></p>
<pre><code>encodings = np.asarray(encodings)
labels = np.asarray(labels)
bert_model2.fit(x=encodings, y = labels, epochs=20, batch_size=128)


Epoch 11/20
5/5 [==============================] - 2s 320ms/step - loss: 1.2923
Epoch 12/20
5/5 [==============================] - 2s 319ms/step - loss: 1.2412
Epoch 13/20
5/5 [==============================] - 2s 322ms/step - loss: 1.3118
Epoch 14/20
5/5 [==============================] - 2s 319ms/step - loss: 1.2531
Epoch 15/20
5/5 [==============================] - 2s 318ms/step - loss: 1.2825
Epoch 16/20
5/5 [==============================] - 2s 322ms/step - loss: 1.2479
Epoch 17/20
5/5 [==============================] - 2s 321ms/step - loss: 1.2623
Epoch 18/20
5/5 [==============================] - 2s 319ms/step - loss: 1.2576
Epoch 19/20
5/5 [==============================] - 2s 321ms/step - loss: 1.3143
Epoch 20/20
5/5 [==============================] - 2s 319ms/step - loss: 1.2716
</code></pre>
<p>Loss has decreased from 6 to around 1.23 but doesn't seem to decrease any further, even after 30+ epochs.</p>
<p>What am I doing wrong?</p>
<p>All advice is greatly appreciated!</p>
","transformer-model"
"62485231","Why pytorch transformer src_mask doesn't block positions from attending?","2020-06-20 11:30:27","62496497","1","1018","<pytorch><word-embedding><transformer-model>","<p>I am trying to train word embedding with transformer encoder by masking the word itself with diagonal src_mask:</p>
<pre><code>def _generate_square_subsequent_mask(self, sz):
    mask = torch.diag(torch.full((sz,),float('-inf')))
    return mask

def forward(self, src):

    if self.src_mask is None or self.src_mask.size(0) != len(src):
        device = src.device
        mask = self._generate_square_subsequent_mask(len(src)).to(device)
        self.src_mask = mask
    
    src = self.embedding(src) * math.sqrt(self.ninp)
    src = self.dropout(src)
    src = self.pos_encoder(src)
    src = self.transformer_encoder(src, self.src_mask)
    output = self.decoder(src) # Linear layer
    return output
</code></pre>
<p>After training the model predicts exactly the same sentence from the input. If I change any word in the input - it predict the new word. So the model doesn't block according to the mask.</p>
<p>Why is it ?</p>
<p>I understand that there is a mistake in my logic because BERT would probably be much simpler if it worked. But where am I wrong ?</p>
<p><strong>Edit:</strong></p>
<p>I am using the a sequence of word indices as input. Output is the same sequence as input.</p>
","transformer-model"
"62482511","TFBertMainLayer gets less accuracy compared to TFBertModel","2020-06-20 06:37:47","64000378","3","1313","<keras><transformer-model><bert-language-model>","<p>I had a problem with saving weights of <code>TFBertModel</code> wrapped in <code>Keras</code>. the problem is described <a href=""https://github.com/huggingface/transformers/issues/2733"" rel=""nofollow noreferrer"">here in GitHub issue</a> and <a href=""https://stackoverflow.com/questions/60062624/save-model-wrapped-in-keras"">here in Stack Overflow</a>.The solution proposed in both cases is to use </p>

<pre><code> config = BertConfig.from_pretrained(transformer_model_name)
 bert = TFBertMainLayer(config=config,trainable=False)
</code></pre>

<p>instead of </p>

<pre><code> bert = TFBertModel.from_pretrained(transformer_model_name, trainable=False)
</code></pre>

<p>The problem is that when I change my model to the former code, the accuracy decreases by 10 percent.While the parameters count in both cases are the same. I wonder what is the reason and how can be prevented?</p>
","transformer-model"
"62436178","BERT HuggingFace gives NaN Loss","2020-06-17 18:44:19","","3","6220","<machine-learning><keras><text-classification><transformer-model><huggingface-transformers>","<p><strong>I'm trying to fine-tune BERT for a text classification task, but I'm getting NaN losses and can't figure out why.</strong></p>

<p>First I define a BERT-tokenizer and then tokenize my text:</p>

<pre><code>from transformers import DistilBertTokenizer, RobertaTokenizer
distil_bert = 'distilbert-base-uncased' 

tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,
                                                max_length=128, pad_to_max_length=True)

def tokenize(sentences, tokenizer):
    input_ids, input_masks, input_segments = [],[],[]
    for sentence in tqdm(sentences):
        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=25, pad_to_max_length=True, 
                                             return_attention_mask=True, return_token_type_ids=True)
        input_ids.append(inputs['input_ids'])
        input_masks.append(inputs['attention_mask'])
        input_segments.append(inputs['token_type_ids'])        

    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')

train = pd.read_csv('train_dataset.csv')
d = train['text']
input_ids, input_masks, input_segments = tokenize(d, tokenizer)
</code></pre>

<p>Next, I load my integer labels which are: 0, 1, 2, 3. </p>

<pre><code>d_y = train['label']
0    0
1    1
2    0
3    2
4    0
5    0
6    0
7    0
8    3
9    1
Name: label, dtype: int64
</code></pre>

<p>Then I load the pretrained Transformer model and put layers on top of it. I use SparseCategoricalCrossEntropy Loss when compiling the model:</p>

<pre><code>from transformers import TFDistilBertForSequenceClassification, DistilBertConfig, AutoTokenizer, TFDistilBertModel

  distil_bert = 'distilbert-base-uncased'
  optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0000001)

  config = DistilBertConfig(num_labels=4, dropout=0.2, attention_dropout=0.2)
  config.output_hidden_states = False
  transformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)

  input_ids_in = tf.keras.layers.Input(shape=(25,), name='input_token', dtype='int32')
  input_masks_in = tf.keras.layers.Input(shape=(25,), name='masked_token', dtype='int32') 

  embedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]
  X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)
  X = tf.keras.layers.GlobalMaxPool1D()(X)
  X = tf.keras.layers.Dense(50, activation='relu')(X)
  X = tf.keras.layers.Dropout(0.2)(X)
  X = tf.keras.layers.Dense(4, activation='softmax')(X)
  model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)

  for layer in model.layers[:3]:
    layer.trainable = False

  model.compile(optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['sparse_categorical_accuracy'],
    )
</code></pre>

<p><strong>Finally, I run the model using previously tokenized input_ids and input_masks as inputs to the model and get a NAN Loss after the first epoch:</strong></p>

<pre><code>model.fit(x=[input_ids, input_masks], y = d_y, epochs=3)

    Epoch 1/3
20/20 [==============================] - 4s 182ms/step - loss: 0.9714 - sparse_categorical_accuracy: 0.6153
Epoch 2/3
20/20 [==============================] - 0s 19ms/step - loss: nan - sparse_categorical_accuracy: 0.5714
Epoch 3/3
20/20 [==============================] - 0s 20ms/step - loss: nan - sparse_categorical_accuracy: 0.5714
&lt;tensorflow.python.keras.callbacks.History at 0x7fee0e220f60&gt;
</code></pre>

<p><strong>EDIT: The model computes losses on the first epoch but it starts returning NaNs 
at the second epoch. What could be causing that problem???</strong></p>

<p>Does anyone has any ideas about what I am doing wrong? 
All suggestions are welcomed!</p>
","transformer-model"
"62399243","TransformerEncoder with a padding mask","2020-06-16 00:43:00","62400355","5","17254","<pytorch><transformer-model><attention-model>","<p>I'm trying to implement torch.nn.TransformerEncoder with a src_key_padding_mask not equal to none. Imagine the input is of the shape <code>src = [20, 95]</code> and the binary padding mask has the shape <code>src_mask = [20, 95]</code>, 1 in the position of padded tokens and 0 for other positions. I make a transformer encoder with 8 layers, each of which contain an attention with 8 heads and hidden dimension 256:</p>

<pre><code>layer=torch.nn.TransformerEncoderLayer(256, 8, 256, 0.1)
encoder=torch.nn.TransformerEncoder(layer, 6)
embed=torch.nn.Embedding(80000, 256)
src=torch.randint(0, 1000, (20, 95))
src = emb(src)
src_mask = torch.randint(0,2,(20, 95))
output =  encoder(src, src_mask)
</code></pre>

<p>But I get the following error:</p>

<pre><code>---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-107-31bf7ab8384b&gt; in &lt;module&gt;
----&gt; 1 output =  encoder(src, src_mask)

~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    545             result = self._slow_forward(*input, **kwargs)
    546         else:
--&gt; 547             result = self.forward(*input, **kwargs)
    548         for hook in self._forward_hooks.values():
    549             hook_result = hook(self, input, result)

~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/transformer.py in forward(self, src, mask, src_key_padding_mask)
    165         for i in range(self.num_layers):
    166             output = self.layers[i](output, src_mask=mask,
--&gt; 167                                     src_key_padding_mask=src_key_padding_mask)
    168 
    169         if self.norm:

~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    545             result = self._slow_forward(*input, **kwargs)
    546         else:
--&gt; 547             result = self.forward(*input, **kwargs)
    548         for hook in self._forward_hooks.values():
    549             hook_result = hook(self, input, result)

~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/transformer.py in forward(self, src, src_mask, src_key_padding_mask)
    264         """"""
    265         src2 = self.self_attn(src, src, src, attn_mask=src_mask,
--&gt; 266                               key_padding_mask=src_key_padding_mask)[0]
    267         src = src + self.dropout1(src2)
    268         src = self.norm1(src)

~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)
    545             result = self._slow_forward(*input, **kwargs)
    546         else:
--&gt; 547             result = self.forward(*input, **kwargs)
    548         for hook in self._forward_hooks.values():
    549             hook_result = hook(self, input, result)

~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/activation.py in forward(self, query, key, value, key_padding_mask, need_weights, attn_mask)
    781                 training=self.training,
    782                 key_padding_mask=key_padding_mask, need_weights=need_weights,
--&gt; 783                 attn_mask=attn_mask)
    784 
    785 

~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py in multi_head_attention_forward(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)
   3250     if attn_mask is not None:
   3251         attn_mask = attn_mask.unsqueeze(0)
-&gt; 3252         attn_output_weights += attn_mask
   3253 
   3254     if key_padding_mask is not None:

RuntimeError: The size of tensor a (20) must match the size of tensor b (95) at non-singleton dimension 2
</code></pre>

<p>I was wondering if somebody could help me figure out this problem.</p>

<p>Thanks</p>
","transformer-model"
"62327803","Having 6 labels instead of 2 in Hugging Face BertForSequenceClassification","2020-06-11 15:23:32","62328920","3","7024","<python><transformer-model><huggingface-transformers><bert-language-model>","<p>I was just wondering if it is possibel to extend the HuggingFace <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification"" rel=""nofollow noreferrer"">BertForSequenceClassification</a> model to more than 2 labels. The docs say, we can pass positional arguments, but it seems like ""labels"" is not working. Does anybody has an idea?</p>

<h2>Model assignment</h2>

<pre class=""lang-py prettyprint-override""><code>labels = th.tensor([0,0,0,0,0,0], dtype=th.long).unsqueeze(0)
print(labels.shape)
modelBERTClass = transformers.BertForSequenceClassification.from_pretrained(
    'bert-base-uncased', 
    labels=labels
    )

l = [module for module in modelBERTClass.modules()]
l
</code></pre>

<h2>Console Output</h2>

<pre><code>torch.Size([1, 6])
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
&lt;ipython-input-122-fea9a36402a6&gt; in &lt;module&gt;()
      3 modelBERTClass = transformers.BertForSequenceClassification.from_pretrained(
      4     'bert-base-uncased',
----&gt; 5     labels=labels
      6     )
      7 

/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    653 
    654         # Instantiate model.
--&gt; 655         model = cls(config, *model_args, **model_kwargs)
    656 
    657         if state_dict is None and not from_tf:

TypeError: __init__() got an unexpected keyword argument 'labels'
</code></pre>
","transformer-model"
"62317931","How to predownload a transformers model","2020-06-11 05:52:57","62318184","7","14723","<machine-learning><flask><amazon-elastic-beanstalk><transformer-model><huggingface-transformers>","<p>I want to perform a text generation task in a flask app and host it on a web server however when downloading the GPT models the elastic beanstalk managed EC2 instance crashes because the download takes too much time and memory</p>

<pre><code>from transformers.tokenization_openai import OpenAIGPTTokenizer
from transformers.modeling_tf_openai import TFOpenAIGPTLMHeadModel
model = TFOpenAIGPTLMHeadModel.from_pretrained(""openai-gpt"")
tokenizer = OpenAIGPTTokenizer.from_pretrained(""openai-gpt"")
</code></pre>

<p>These are the lines in question causing the issue. GPT is approx 445 MB. I am using the transformers library. Instead of downloading the model at this line I was wondering if I could pickle the model and then bundle it as part of the repository. Is that possible with this library? Otherwise how can I preload this model to avoid the issues I am having?</p>
","transformer-model"
"62286188","How to use scripting to convert pytorch transformer?","2020-06-09 15:31:16","62294084","1","1026","<pytorch><transformer-model><torchscript>","<p>I am trying to compile pytorch transformer to run it in C++:</p>

<pre><code>from torch.nn import TransformerEncoder, TransformerEncoderLayer
encoder_layers = TransformerEncoderLayer(1000, 8, 512, 0.1)
transf = TransformerEncoder(encoder_layers, 6)
sm = torch.jit.script(transf)
</code></pre>

<p>But I am getting an error:</p>

<pre><code>RuntimeError:  Expected a default value of type Tensor on parameter
""src_mask"":   File ""C:\Program Files (x86)\Microsoft Visual
Studio\Shared\Python36_64\lib\site-packages\torch\nn\modules\transformer.py"",
line 271
    def forward(self, src, src_mask=None, src_key_padding_mask=None):
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~... &lt;--- HERE
        r""""""Pass the input through the encoder layer.
</code></pre>

<p>It looks like something wrong with pytorch transformer module. </p>

<p>Is there any way to run pytorch transformer in C++ ?</p>
","transformer-model"
"62276011","Training RoBerta using transformers on masked language task giving weird results?","2020-06-09 06:02:14","","3","249","<pytorch><transformer-model><huggingface-transformers><bert-language-model>","<p>I trained a RoBERTa model following this colab - <a href=""https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=XaFAsB_fnU3K"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=XaFAsB_fnU3K</a></p>

<p>Here is how my data looked: </p>

<pre><code>Merkel bemoans lack of rain as Germany fears for its forests .\n
Germany’s forests, covering a third of its territory and as much a part of its cultural landscape as its physical one, are in danger.\n
An aerial view shows a forest near Gummersbach, Germany, April 24, 2020, following an unusually warm, dry winter after a summer of record temperatures leaving forests dried out.\n
Picture taken with a drone.\n
The last two exceptionally hot and dry summers have weakened millions of trees, undermining their defences against the bark beetle, which can be fatal to ancient woodlands.\n
And after an exceptionally dry April, with summer still two months away, a forest fire has already had to be put out near the town of Gummersbach in western Germany this week.\n
“We’re already noticing these days that it’s not raining enough in many areas. 
</code></pre>

<p>After training the model I used <code>pipeline</code> from the transforms library for the fill_mask task</p>

<pre><code>from transformers import pipeline

fill_mask = pipeline(
    ""fill-mask"",
    model=""./output"",
    tokenizer=""./output""
fill_mask(""Merkel bemoans lack of rain as &lt;mask&gt; fears for its forests"")
)
</code></pre>

<p>These are the results: </p>

<pre><code>[{'sequence': '&lt;s&gt; Merkel bemoans lack of rain as. fears for its forests&lt;/s&gt;',
  'score': 0.040456026792526245,
  'token': 18},
 {'sequence': '&lt;s&gt; Merkel bemoans lack of rain as, fears for its forests&lt;/s&gt;',
  'score': 0.03502459451556206,
  'token': 16},
 {'sequence': '&lt;s&gt; Merkel bemoans lack of rain as the fears for its forests&lt;/s&gt;',
  'score': 0.03497963398694992,
  'token': 269},
 {'sequence': '&lt;s&gt; Merkel bemoans lack of rain as\n fears for its forests&lt;/s&gt;',
  'score': 0.03180328756570816,
  'token': 203},
 {'sequence': '&lt;s&gt; Merkel bemoans lack of rain as to fears for its forests&lt;/s&gt;',
  'score': 0.020796578377485275,
  'token': 288}]
</code></pre>

<p>As you can see there is no meaningful word(s) returned only punctuations and one other word (to) which doesn't make sense. What am i doing wrong here? Do I have to remove all punctuations?</p>
","transformer-model"
"62263428","Fine tuning BERT with my own entities/labels","2020-06-08 13:39:21","","1","628","<neural-network><transformer-model><bert-language-model>","<p>i would like to fine tune A BERT model with my own labels, like [COLOR, MATERIAL] and not the normal ""NAME"", ""ORG"".</p>

<p>I'm following this Colab: <a href=""https://colab.research.google.com/drive/14rYdqGAXJhwVzslXT4XIwNFBwkmBWdVV"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/14rYdqGAXJhwVzslXT4XIwNFBwkmBWdVV</a></p>

<p>I prepared train.txt, eval.txt, test.txt like this:</p>

<pre><code>-DOCSTART- -X- -X- O

blue B-COLOR
motorcicle B-CATEGORY
steel B-MATERIAL
etc.
</code></pre>

<p>But whene i execute this command</p>

<pre><code>!python run_ner.py --data_dir=data/ --bert_model=bert-base-multilingual-cased --task_name=ner --output_dir=out_ner --max_seq_length=128 --do_train --num_train_epochs 5 --do_eval --warmup_proportion=0.1
</code></pre>

<p>i get this error</p>

<pre><code>06/08/2020 13:30:27 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059
06/08/2020 13:30:33 - INFO - pytorch_transformers.modeling_utils -   Weights of Ner not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
06/08/2020 13:30:33 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in Ner: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']


File ""run_ner.py"", line 594, in
main()

File ""run_ner.py"", line 464, in main
train_examples, label_list, args.max_seq_length, tokenizer)

File ""run_ner.py"", line 210, in convert_examples_to_features
label_ids.append(label_map[labels[i]])

KeyError: 'B-COLOR'
</code></pre>

<p>Did i create wrongly train.txt file?</p>
","transformer-model"
"62170439","Difference between src_mask and src_key_padding_mask","2020-06-03 10:18:39","","30","31557","<pytorch><transformer-model>","<p>I am having a difficult time in understanding transformers. Everything is getting clear bit by bit but one thing that makes my head scratch is
what is the difference between src_mask and src_key_padding_mask which is passed as an argument in forward function in both encoder layer and decoder layer.</p>

<p><a href=""https://pytorch.org/docs/master/_modules/torch/nn/modules/transformer.html#Transformer"" rel=""noreferrer"">https://pytorch.org/docs/master/_modules/torch/nn/modules/transformer.html#Transformer</a></p>
","transformer-model"
"62154230","BPE vs WordPiece Tokenization - when to use / which?","2020-06-02 14:23:27","","6","1859","<machine-learning><nlp><lstm><transformer-model><huggingface-transformers>","<p>What's the general tradeoff between choosing BPE vs WordPiece Tokenization? When is one preferable to the other? Are there any differences in model performance between the two? I'm looking for a general overall answer, backed up with specific examples. Thanks!</p>
","transformer-model"
"62124961","TensorFlow1.15, multi-GPU-1-machine, how to set batch_size?","2020-06-01 02:43:13","65767832","1","269","<tensorflow><tensorflow-datasets><tensorflow-estimator><transformer-model><bert-language-model>","<p>I am pretraining BERT in 1 machine with 4 GPU.</p>

<p>The input function code:</p>

<pre><code>    def input_fn(params):
        """"""The actual input function.""""""
        batch_size = FLAGS.train_batch_size

        name_to_features = {
            ""input_ids"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""input_mask"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""segment_ids"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""masked_lm_positions"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
            ""masked_lm_ids"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
            ""masked_lm_weights"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.float32),
            ""next_sentence_labels"":
                tf.FixedLenFeature([1], tf.int64),
        }

        # For training, we want a lot of parallel reading and shuffling.
        # For eval, we want no shuffling and parallel reading doesn't matter.
        if is_training:
            d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))
            d = d.repeat()
            d = d.shuffle(buffer_size=len(input_files))

            # `cycle_length` is the number of parallel files that get read.
            cycle_length = min(num_cpu_threads, len(input_files))

            # `sloppy` mode means that the interleaving is not exact. This adds
            # even more randomness to the training pipeline.
            d = d.apply(
                tf.contrib.data.parallel_interleave(
                    tf.data.TFRecordDataset,
                    sloppy=is_training,
                    cycle_length=cycle_length))
            d = d.shuffle(buffer_size=100)
        else:
            d = tf.data.TFRecordDataset(input_files)
            # Since we evaluate for a fixed number of steps we don't want to encounter
            # out-of-range exceptions.
            d = d.repeat()

        # We must `drop_remainder` on training because the TPU requires fixed
        # size dimensions. For eval, we assume we are evaluating on the CPU or GPU
        # and we *don't* want to drop the remainder, otherwise we wont cover
        # every sample.
        d = d.apply(
            tf.contrib.data.map_and_batch(
                lambda record: _decode_record(record, name_to_features),
                batch_size=batch_size,
                num_parallel_batches=num_cpu_threads,
                drop_remainder=True))
        d = d.prefetch(10)
        return d

</code></pre>

<p>The mirrow strategy code:</p>

<pre><code>    distribution = tf.contrib.distribute.MirroredStrategy(
        devices=[""device:GPU:%d"" % i for i in range(FLAGS.n_gpus)],
        # num_gpus=4,
        cross_tower_ops=tf.distribute.HierarchicalCopyAllReduce())
    run_config = RunConfig(
        train_distribute=distribution,
        # eval_distribute=dist_strategy,
        log_step_count_steps=log_every_n_steps,
        model_dir=FLAGS.output_dir,
        save_checkpoints_steps=FLAGS.save_checkpoints_steps)

    model_fn = model_fn_builder(
        bert_config=bert_config,
        init_checkpoint=FLAGS.init_checkpoint,
        learning_rate=FLAGS.learning_rate,
        num_train_steps=FLAGS.num_train_steps,
        num_warmup_steps=FLAGS.num_warmup_steps,
        use_tpu=FLAGS.use_tpu,
        use_one_hot_embeddings=FLAGS.use_tpu)

    # If TPU is not available, this will fall back to normal Estimator on CPU
    # or GPU.
    estimator = Estimator(
        model_fn=model_fn,
        params={},
        config=run_config)
</code></pre>

<p>The problem is that I have 4 GPU. Each GPU could run 8 batchsize at most.</p>

<p>I set <code>train_batch_size = 8</code> not 32. Is OK but I don't know each GPU get different data in one training step.</p>

<p>If I set <code>train_batch_size = 32</code>, it will out of memory (OOM).</p>

<p>Is my code right now? Will the data be distributed to 4 GPU and each GPU get different data?</p>
","transformer-model"
"62109957","Why does the BERT NSP head linear layer have two outputs?","2020-05-30 23:40:13","62110180","1","539","<nlp><pytorch><transformer-model><huggingface-transformers><bert-language-model>","<p>Here's the code in question. </p>

<p><a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491</a></p>

<pre><code>class BertOnlyNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, pooled_output):
        seq_relationship_score = self.seq_relationship(pooled_output)
        return seq_relationship_score
</code></pre>

<p>I think it was just ranking how likely one sentence would follow another? Wouldn't it be one score?</p>
","transformer-model"
"62086878","How to get embedding from bert finetuned model?","2020-05-29 13:15:33","62089091","1","1266","<pytorch><transformer-model><bert-language-model>","<p>I have finedtuned 'bert-base-uncased' model using transformer and torch which gave me pytorch_model.bin, vocab.txt and other files as output.
After loading the model how to I get embedding for complete vocab, like a matrix which maps every word to its embedding vector </p>
","transformer-model"
"62063518","Python ""Can't pickle local object"" exception during BertModel training","2020-05-28 11:24:49","","2","1415","<python><nlp><text-classification><transformer-model><bert-language-model>","<p>I am using simpletransformers.classification to train a Bert moder to classify some text inputs. Here is my code.</p>
<pre><code>from simpletransformers.classification import ClassificationModel

import torch

import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from simpletransformers.classification import ClassificationModel

from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification

import parallelTestModule

# Lets import the csv file in pandas dataframe first
train_df = pd.read_csv('D:\\7allV03Small.csv', encoding='utf-8', header=None, names=['cat', 'text'])

# Check the df
print(train_df.head())

# unique categories
print(train_df.cat.unique())
print(&quot;Total categories&quot;,len(train_df.cat.unique()))

# convert string labels to integers
train_df['labels'] = pd.factorize(train_df.cat)[0]

print(train_df.head())

# Let's create a train and test set
from sklearn.model_selection import train_test_split

train, test = train_test_split(train_df, test_size=0.2, random_state=42)

print('Eğitim veri seti boyutu : ' + str(train.shape), ' Test eğitim seti : ' + str(test.shape))

if __name__ == &quot;__main__&quot;:
    from multiprocessing import freeze_support

    model = ClassificationModel('bert', 'bert-base-multilingual-uncased',  use_cuda=False,  num_labels=8, args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 1,'train_batch_size':1})
    
    freeze_support()
    
    # Now lets fine tune bert with the train set
    model.train_model(train)
</code></pre>
<p>Everything looks okay and it starts to training. But at the end of the training it gives an error like below.</p>
<pre><code>Traceback (most recent call last):
  File &quot;c:/Users/arslanom/Desktop/text/try.py&quot;, line 45, in &lt;module&gt;
    model.train_model(train)
  File &quot;C:\Users\arslanom\AppData\Roaming\Python\Python36\site-packages\simpletransformers\classification\classification_model.py&quot;, line 269, in train_model
    **kwargs,
  File &quot;C:\Users\arslanom\AppData\Roaming\Python\Python36\site-packages\simpletransformers\classification\classification_model.py&quot;, line 544, in train
    self._save_model(output_dir_current, optimizer, scheduler, model=model)
  File &quot;C:\Users\arslanom\AppData\Roaming\Python\Python36\site-packages\simpletransformers\classification\classification_model.py&quot;, line 1113, in _save_model
    torch.save(scheduler.state_dict(), os.path.join(output_dir, &quot;scheduler.pt&quot;))
  File &quot;C:\Users\arslanom\AppData\Roaming\Python\Python36\site-packages\torch\serialization.py&quot;, line 209, in save
    return _with_file_like(f, &quot;wb&quot;, lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File &quot;C:\Users\arslanom\AppData\Roaming\Python\Python36\site-packages\torch\serialization.py&quot;, line 134, in _with_file_like
    return body(f)
  File &quot;C:\Users\arslanom\AppData\Roaming\Python\Python36\site-packages\torch\serialization.py&quot;, line 209, in &lt;lambda&gt;
    return _with_file_like(f, &quot;wb&quot;, lambda f: _save(obj, f, pickle_module, pickle_protocol))
  File &quot;C:\Users\arslanom\AppData\Roaming\Python\Python36\site-packages\torch\serialization.py&quot;, line 282, in _save
    pickler.dump(obj)
AttributeError: Can't pickle local object 'get_linear_schedule_with_warmup.&lt;locals&gt;.lr_lambda'
</code></pre>
<p>Sounds like this problem is related to worker_count because it runs using multithreading. But I could not find any solution.</p>
<p>Operating System: Windows 10</p>
<p>RAM: 16 Gb</p>
","transformer-model"
"61994001","Unable to pip install -U sentence-transformers","2020-05-25 00:14:02","62004142","5","36854","<transformer-model><sentence><bert-language-model>","<p>I am unable to do: pip install -U sentence-transformers. I get this message on Anaconda Prompt:
ERROR: Could not find a version that satisfies the requirement torch>=1.0.1 (from sentence-transformers) (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2)
ERROR: No matching distribution found for torch>=1.0.1 (from sentence-transformers)
Can someone help?</p>
","transformer-model"
"61899118","Cannot load German BERT model in spaCy","2020-05-19 19:24:36","61899689","0","806","<python><spacy><transformer-model><bert-language-model>","<p>Here is my problem: I am working on the German text classification project. I use spacy for that and decided to fine-tune its pretrained BERT model to get better results. However, when I try to load it to the code, it shows me errors.</p>

<p>Here is what I've done:</p>

<ol>
<li>Installed spacy-transformers: <code>pip install spacy-transformers</code></li>
<li>Downloaded German BERT model: <code>python -m spacy download de_trf_bertbasecased_lg</code>. It was downloaded successfully and showed me: <code>✔ Download and installation successful
You can now load the model via spacy.load('de_trf_bertbasecased_lg')</code></li>
<li>Wrote the following code:</li>
</ol>

<p><code>import spacy
 nlp = spacy.load('de_trf_bertbasecased_lg')</code></p>

<p>And the output was:</p>

<pre><code>Traceback (most recent call last):
  File ""&lt;pyshell#1&gt;"", line 1, in &lt;module&gt;
nlp = spacy.load('de_trf_bertbasecased_lg')
  File ""C:\Python\Python37\lib\site-packages\spacy\__init__.py"", line 30, in load
return util.load_model(name, **overrides)
  File ""C:\Python\Python37\lib\site-packages\spacy\util.py"", line 164, in load_model
return load_model_from_package(name, **overrides)
  File ""C:\Python\Python37\lib\site-packages\spacy\util.py"", line 185, in load_model_from_package
return cls.load(**overrides)
  File ""C:\Python\Python37\lib\site-packages\de_trf_bertbasecased_lg\__init__.py"", line 12, in load
return load_model_from_init_py(__file__, **overrides)
  File ""C:\Python\Python37\lib\site-packages\spacy\util.py"", line 228, in load_model_from_init_py
return load_model_from_path(data_path, meta, **overrides)
  File ""C:\Python\Python37\lib\site-packages\spacy\util.py"", line 196, in load_model_from_path
cls = get_lang_class(lang)
  File ""C:\Python\Python37\lib\site-packages\spacy\util.py"", line 70, in get_lang_class
if lang in registry.languages:
  File ""C:\Python\Python37\lib\site-packages\catalogue.py"", line 56, in __contains__
has_entry_point = self.entry_points and self.get_entry_point(name)
  File ""C:\Python\Python37\lib\site-packages\catalogue.py"", line 140, in get_entry_point
return entry_point.load()
  File ""C:\Python\Python37\lib\site-packages\importlib_metadata\__init__.py"", line 94, in load
module = import_module(match.group('module'))
  File ""C:\Python\Python37\lib\importlib\__init__.py"", line 127, in import_module
return _bootstrap._gcd_import(name[level:], package, level)
  File ""&lt;frozen importlib._bootstrap&gt;"", line 1006, in _gcd_import
  File ""&lt;frozen importlib._bootstrap&gt;"", line 983, in _find_and_load
  File ""&lt;frozen importlib._bootstrap&gt;"", line 967, in _find_and_load_unlocked
  File ""&lt;frozen importlib._bootstrap&gt;"", line 677, in _load_unlocked
  File ""&lt;frozen importlib._bootstrap_external&gt;"", line 728, in exec_module
  File ""&lt;frozen importlib._bootstrap&gt;"", line 219, in _call_with_frames_removed
  File ""C:\Python\Python37\lib\site-packages\spacy_transformers\__init__.py"", line 1, in &lt;module&gt;
from .language import TransformersLanguage
  File ""C:\Python\Python37\lib\site-packages\spacy_transformers\language.py"", line 5, in &lt;module&gt;
from .util import is_special_token, pkg_meta, ATTRS, PIPES, LANG_FACTORY
  File ""C:\Python\Python37\lib\site-packages\spacy_transformers\util.py"", line 2, in &lt;module&gt;
import transformers
  File ""C:\Python\Python37\lib\site-packages\transformers\__init__.py"", line 20, in &lt;module&gt;
from .file_utils import (TRANSFORMERS_CACHE, PYTORCH_TRANSFORMERS_CACHE, PYTORCH_PRETRAINED_BERT_CACHE,
  File ""C:\Python\Python37\lib\site-packages\transformers\file_utils.py"", line 37, in &lt;module&gt;
import torch
  File ""C:\Python\Python37\lib\site-packages\torch\__init__.py"", line 81, in &lt;module&gt;
ctypes.CDLL(dll)
  File ""C:\Python\Python37\lib\ctypes\__init__.py"", line 356, in __init__
self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found
</code></pre>

<p>If I run the same code in PyCharm, it also shows me these two lines before all of those above:</p>

<pre><code>2020-05-19 18:00:55.132721: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not 
load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2020-05-19 18:00:55.132990: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart 
dlerror if you do not have a GPU set up on your machine.
</code></pre>

<p>If I got it right, these two lines complain that I don't have a GPU. However, according to the docs, I should be able to use BERT even without GPU.</p>

<p>So I am really stuck right now and looking for your help. </p>

<p>I should also mention, that I used <code>de_core_news_sm</code> model before and it worked fine.</p>

<p>I have also already tried several solutions, but none of them worked. I tried:
<a href=""https://www.kaggle.com/questions-and-answers/103976"" rel=""nofollow noreferrer"">this</a> and <a href=""https://stackoverflow.com/questions/1940578/windowserror-error-126-the-specified-module-could-not-be-found"">this</a>. I have also tried to uninstall all <code>spacy</code>-related libraries and installed them again. Didn't help either.</p>

<p>I am working with:</p>

<blockquote>
  <p>Windows 10 Home</p>
  
  <p>Python: 3.7.2</p>
  
  <p>Spacy: 2.2.4</p>
  
  <p>Spacy-transformers: 0.5.1</p>
</blockquote>

<p>Would appreciate any help or advice!</p>
","transformer-model"
"61802133","Join a few elements of the list in Python","2020-05-14 16:11:04","61834643","0","140","<python><nlp><tokenize><transformer-model>","<p>Please have a look into the code below</p>

<pre><code>from transformers import GPT2Tokenizer, GPT2Model

text = ""Here is the sentence I want embeddings for.""
#marked_text = ""[CLS] "" + text + "" [SEP]""
# Tokenize our sentence with the GPT2 tokenizer.
tokenized_text = tokenizer.tokenize(text)
print(tokenized_text)
</code></pre>

<p>Output of the above code is shown below :-</p>

<pre><code>['Here', 'Ġis', 'Ġthe', 'Ġsentence', 'ĠI', 'Ġwant', 'Ġembed', 'd', 'ings', 'Ġfor', '.']
</code></pre>

<p>But I want an output like this:-</p>

<pre><code>['Here', 'Ġis', 'Ġthe', 'Ġsentence', 'ĠI', 'Ġwant', 'Ġembeddings', 'Ġfor', '.']
</code></pre>

<p>So, while tokenizing the text, the tokenizer has splitted the word 'embeddings' because it doesn't have this word in its dictionary. But, I don't want this happening. I want the whole word 'embedding' to be tokenized as it is.</p>

<p>I don't know how to solve this. Also kindly note that tokenized_text is a List object.
Please help.</p>

<p><strong>EDIT 1 :</strong>
I came with this solution </p>

<pre><code>tokenized_text[6:9] = [''.join(tokenized_text[6:9])]
print(tokenized_text)
</code></pre>

<p>And it gave me the desired output as well but I don't want to give the numbers here specifically. I want the machine to figure it out for itself.
Like whichever element in the list doesn't start with that 'G' special character, that element needs to be joined with the previous element and so on.</p>

<p><strong>EDIT 2 :</strong>
I came across another approach and here's the code for it, but it doesn't work probably because of wrong for loops.</p>

<pre><code>for i in range(1, len(tokenized_text)):
  if tokenized_text[i].startswith('Ġ'):
    i += 1 
  else:
    for j in range(i, len(tokenized_text)):
      if tokenized_text[j].startswith(""Ġ"") :
        pass
      else :
        j += 1


tokenized_text[i-1:j] = [''.join(tokenized_text[i-1:j])]
print(tokenized_text)
</code></pre>
","transformer-model"
"61764582","How does the Transformer Model Compute Self Attention?","2020-05-13 00:58:33","","1","1082","<machine-learning><deep-learning><pytorch><transformer-model><attention-model>","<p>In the transformer model, <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1706.03762.pdf</a> there is self-attention which is computed using softmax on <code>Query (Q)</code> and <code>Key (K)</code> vectors:</p>

<p>I am trying to understand the matrix multiplications:</p>

<pre><code>Q = batch_size x seq_length x embed_size

K = batch_size x seq_length x embed_size

QK^T = batch_size x seq_length x seq_length

Softmax QK^T = Softmax (batch_size x seq_length x seq_length)
</code></pre>

<p>How is the softmax computed since there are <code>seq_length x seq_length</code> values per batch element?</p>

<p>A reference to Pytorch computation will be very helpful.</p>

<p>Cheers!</p>
","transformer-model"
"61667142","huggingface-transformers: Train BERT and evaluate it using different attentions","2020-05-07 20:27:57","61729996","0","395","<transformer-model><huggingface-transformers>","<p>This is a clarification question. I am trying to train <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">BERT provided by huggingface</a> using standard attention, and evaluate using a different attention definition.</p>

<p>The operation I was thinking about was change <code>bert-base-uncased</code> to the path of my trained model(using standard attention) in the following command, and run <code>--do_eval</code> under the installation of my customized attention version.</p>

<pre><code>export GLUE_DIR=/path/to/glue
export TASK_NAME=MRPC

python ./examples/run_glue.py \
    --model_name_or_path bert-base-uncased \
    --task_name $TASK_NAME \
    --do_eval \
    --data_dir $GLUE_DIR/$TASK_NAME \
    --max_seq_length 128 \
    --per_gpu_eval_batch_size=8   \
    --per_gpu_train_batch_size=8   \
    --learning_rate 2e-5 \
    --num_train_epochs 3.0 \
    --output_dir /tmp/$TASK_NAME/
</code></pre>

<p>However, I was getting unexpected results. So I want to make sure that I was using the right command. Could anyone confirm with me or correct me?</p>

<p>Edited: The version was 2.8.0.</p>
","transformer-model"
"61630765","Implementation of the Dense Synthesizer","2020-05-06 08:33:01","","15","367","<python><deep-learning><neural-network><pytorch><transformer-model>","<p>I’m trying to understand the Synthesizer paper (<a href=""https://arxiv.org/pdf/2005.00743.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/2005.00743.pdf</a> 1) and there’s a description of the dense synthesizer mechanism that should replace the traditional attention model as described in the Transformer architecture.</p>

<p><a href=""https://i.sstatic.net/DF09Y.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/DF09Y.png"" alt=""enter image description here""></a></p>

<p>The <strong>Dense Synthesizer</strong> is described as such:</p>

<p><a href=""https://i.sstatic.net/EK7I3.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/EK7I3.png"" alt=""enter image description here""></a></p>

<p>So I tried to implement the layer and it looks like this but I’m not sure whether I’m getting it right:</p>

<pre><code>class DenseSynthesizer(nn.Module):
    def __init__(self, l, d):
        super(DenseSynthesizer, self).__init__()
        self.linear1 = nn.Linear(d, l)
        self.linear2 = nn.Linear(l, l)

    def forward(self, x, v):
        # Equation (1) and (2)
        # Shape: l x l
        b = self.linear2(F.relu(self.linear1(x)))   
        # Equation (3)
        # [l x l] x [l x d] -&gt; [l x d]
        return torch.matmul(F.softmax(b), v) 
</code></pre>

<p>Usage:</p>

<pre><code>l, d = 4, 5

x, v =  torch.rand(l, d), torch.rand(l, d)

synthesis = DenseSynthesizer(l, d)
synthesis(x, v) 
</code></pre>

<p>Example:</p>

<p>x and v are tensors:</p>

<pre><code>x = tensor([[0.0844, 0.2683, 0.4299, 0.1827, 0.1188],
         [0.2793, 0.0389, 0.3834, 0.9897, 0.4197],
         [0.1420, 0.8051, 0.1601, 0.3299, 0.3340],
         [0.8908, 0.1066, 0.1140, 0.7145, 0.3619]])

v = tensor([[0.3806, 0.1775, 0.5457, 0.6746, 0.4505],
         [0.6309, 0.2790, 0.7215, 0.4283, 0.5853],
         [0.7548, 0.6887, 0.0426, 0.1057, 0.7895],
         [0.1881, 0.5334, 0.6834, 0.4845, 0.1960]])
</code></pre>

<p>And passing through a forward pass through the dense synthesis, it returns:</p>

<pre><code>&gt;&gt;&gt; synthesis = DenseSynthesizer(l, d)
&gt;&gt;&gt; synthesis(x, v) 

tensor([[0.5371, 0.4528, 0.4560, 0.3735, 0.5492],
        [0.5426, 0.4434, 0.4625, 0.3770, 0.5536],
        [0.5362, 0.4477, 0.4658, 0.3769, 0.5468],
        [0.5430, 0.4461, 0.4559, 0.3755, 0.5551]], grad_fn=&lt;MmBackward&gt;)
</code></pre>

<p><strong>Is the implementation and understanding of the dense synthesizer correct?</strong></p>

<p>Theoretically, <strong>how is that different from a multi-layered perceptron that takes in two different inputs and makes uses of it at different point in the forward propagation?</strong></p>
","transformer-model"
"61626779","PyTorch nn.Transformer learns to copy target","2020-05-06 03:18:52","","8","1514","<pytorch><transformer-model>","<p>I’m trying to train a Transformer Seq2Seq model using nn.Transformer class. I believe I am implementing it wrong, since when I train it, it seems to fit too fast, and during inference it repeats itself often. This seems like a masking issue in the decoder, and when I remove the target mask, the training performance is the same. This leads me to believe I am doing the target masking wrong. Here is my model code:</p>
<pre><code>class TransformerModel(nn.Module):
    def __init__(self, 
        vocab_size, input_dim, heads, feedforward_dim, encoder_layers, decoder_layers, 
        sos_token, eos_token, pad_token, max_len=200, dropout=0.5, 
        device=(torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;))):

        super(TransformerModel, self).__init__()
        self.target_mask = None
        self.embedding = nn.Embedding(vocab_size, input_dim, padding_idx=pad_token)
        self.pos_embedding = nn.Embedding(max_len, input_dim, padding_idx=pad_token)
        self.transformer = nn.Transformer(
            d_model=input_dim, nhead=heads, num_encoder_layers=encoder_layers, 
            num_decoder_layers=decoder_layers, dim_feedforward=feedforward_dim, 
            dropout=dropout)
        self.out = nn.Sequential(
            nn.Linear(input_dim, feedforward_dim), 
            nn.ReLU(), 
            nn.Linear(feedforward_dim, vocab_size))

        self.device = device
        self.max_len = max_len
        self.sos_token = sos_token
        self.eos_token = eos_token

    # Initialize all weights to be uniformly distributed between -initrange and initrange
    def init_weights(self): 
        initrange = 0.1
        self.encoder.weight.data.uniform_(-initrange, initrange)
        self.decoder.bias.data.zero_()
        self.decoder.weight.data.uniform_(-initrange, initrange)

    # Generate mask covering the top right triangle of a matrix
    def generate_square_subsequent_mask(self, size): 
        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))
        return mask

    def forward(self, src, tgt):
        # src: (Max source seq len, batch size, 1)
        # tgt: (Max target seq len, batch size, 1)

        # Embed source and target with normal and positional embeddings
        embedded_src = (self.embedding(src) + 
           self.pos_embedding(
           torch.arange(0, src.shape[1]).to(self.device).unsqueeze(0).repeat(src.shape[0], 1)))
        # Generate target mask
        target_mask = self.generate_square_subsequent_mask(size=tgt.shape[0]).to(self.device) 
        embedded_tgt = (self.embedding(tgt) + 
            self.pos_embedding(
            torch.arange(0, tgt.shape[1]).to(self.device).unsqueeze(0).repeat(tgt.shape[0], 1)))
        # Feed through model
        outputs = self.transformer(src=embedded_src, tgt=embedded_tgt, tgt_mask=target_mask)
        outputs = F.log_softmax(self.out(outputs), dim=-1)
        return outputs
</code></pre>
","transformer-model"
"61619007","Pytorch - How to add a self-attention to another architecture","2020-05-05 17:16:41","","1","2712","<python><pytorch><transformer-model>","<p>I'm a beginner with pytorch framework and I'm trying to add a multiheaded self attention on top of another architecture (BERT) (this is a simple question but I'm not familiar with PyTorch):</p>

<p><strong>UPDATE 1</strong> </p>

<pre><code>import math
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.d_model = d_model

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x, seq_len = 768, mask = None):
        pos_emb = self.pe[:, :seq_len]
        x = x * mask[:, :, None].float()
        x = x + pos_emb
        return x
</code></pre>

<p>The problem in how to add the transformer is in the following class:</p>

<pre><code>class CamemBERTQA(nn.Module):
   def __init__(self,bert_type, hidden_size, num_labels, num_inter_layers=1, heads = 12, do_lower_case = True):
       super(CamemBERTQA, self).__init__()
       self.do_lower_case = do_lower_case
       self.bert_type = bert_type
       self.hidden_size = hidden_size
       self.num_labels = num_labels       
       self.num_inter_layers = num_inter_layers
       self.camembert = CamembertModel.from_pretrained(self.bert_type)

       # ---------------- Transformer ------------------------------------------
       self.d_model = self.hidden_size # 768
       dropout = 0.1
       self.pos_emb = PositionalEncoding(d_model = self.d_model, dropout = dropout)
       self.transformer_inter = nn.ModuleList(
           [nn.TransformerEncoderLayer(d_model = self.d_model, nhead = heads, dim_feedforward = 2048, dropout = dropout)
            for _ in range(num_inter_layers)])
       # ---------------- Transformer ------------------------------------------

       self.qa_outputs = nn.Linear(self.hidden_size, self.num_labels)



   def forward(self, input_ids, mask=None):
       bert_output = self.camembert(input_ids = input_ids) # input_ids is a tensor

       # ---------------- Transformer ------------------------------------------
       seq_len = self.hidden_size
       x = self.pos_emb(x = bert_output, seq_len = seq_len, mask = None)

       for i in range(self.num_inter_layers):
           x = self.transformer_inter[i](i, x, x, 1 - mask)  # all_tokens * max_tokens * dim
       output = self.layer_norm(x)
       # ---------------- Transformer ------------------------------------------

       sequence_output = output[0]
       logits = self.qa_outputs(sequence_output)
       start_logits, end_logits = logits.split(1, dim=-1)
       start_logits = start_logits.squeeze(-1)
       end_logits = end_logits.squeeze(-1)
       outputs = (start_logits, end_logits,)
       return x
</code></pre>

<p>Thank you so much.</p>
","transformer-model"
"61588381","Speed up embedding of 2M sentences with RoBERTa","2020-05-04 08:50:15","61612256","6","2635","<python><nlp><word-embedding><transformer-model>","<p>I have roughly 2 million sentences that I want to turn into vectors using Facebook AI's RoBERTa-large,fine-tuned on NLI and STSB for sentence similarity (using the awesome <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""noreferrer"">sentence-transformers</a> package).</p>

<p>I already have a dataframe with two columns: ""utterance"" containing each sentence from the corpus, and ""report"" containing, for each sentence, the title of the document from which it is from.</p>

<p>From there, my code is the following:</p>

<pre class=""lang-py prettyprint-override""><code>from sentence_transformers import SentenceTransformer
from tqdm import tqdm

model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')

print(""Embedding sentences"")

data = pd.read_csv(""data/sentences.csv"")

sentences = data['utterance'].tolist()

sentence_embeddings = []

for sent in tqdm(sentences):
    embedding = model.encode([sent])
    sentence_embeddings.append(embedding[0])

data['vector'] = sentence_embeddings
</code></pre>

<p>Right now, tqdm estimates that the whole process will take around 160 hours on my computer, which is more than I can spare.</p>

<p>Is there any way I could speed this up by changing my code? Is creating a huge list in memory then appending it to the dataframe the best way to proceed here? (I suspect not).</p>

<p>Many thanks in advance!</p>
","transformer-model"
"61557024","NotImplementedError: Learning rate schedule must override get_config","2020-05-02 09:17:37","61927902","6","4033","<python><machine-learning><keras><tensorflow2.0><transformer-model>","<p>I have created a custom schedule using tf.keras and I am encountering this error while saving the model:</p>

<blockquote>
  <p>NotImplementedError: Learning rate schedule must override get_config</p>
</blockquote>

<p>The class looks like this:</p>

<pre class=""lang-py prettyprint-override""><code>class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):

    def __init__(self, d_model, warmup_steps=4000):
        super(CustomSchedule, self).__init__()

        self.d_model = d_model
        self.d_model = tf.cast(self.d_model, tf.float32)

        self.warmup_steps = warmup_steps

    def __call__(self, step):
        arg1 = tf.math.rsqrt(step)
        arg2 = step * (self.warmup_steps**-1.5)

        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)

    def get_config(self):
        config = {
            'd_model':self.d_model,
            'warmup_steps':self.warmup_steps

        }
        base_config = super(CustomSchedule, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
</code></pre>
","transformer-model"
"61550968","Implementation details of positional encoding in transformer model?","2020-05-01 21:18:57","61587579","4","4240","<encoding><deep-learning><nlp><transformer-model><attention-model>","<p>How exactly does this positional encoding being calculated?</p>

<p>Let's assume a machine translation scenario and these are input sentences,</p>

<pre><code>english_text = [this is good, this is bad]
german_text = [das ist gut, das ist schlecht]
</code></pre>

<p>Now our input vocabulary size is 4 and embedding dimension is 4.</p>

<pre><code>#words     #embeddings
this     - [0.5, 0.2, 0.3, 0.1]
is       - [0.1, 0.2, 0.5, 0.1]
good     - [0.9, 0.7, 0.9, 0.1]
bad      - [0.7, 0.3, 0.4, 0.1]
</code></pre>

<p>As per transformer paper we add the <strong>each word position encoding</strong> with <strong>each word embedding</strong> and then pass it to encoder like seen in the image below,</p>

<p><a href=""https://i.sstatic.net/E1aEA.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/E1aEA.jpg"" alt=""attention is all you need""></a></p>

<p>As far as the  paper is concerned they given this formula for calculating position encoding of each word,
<a href=""https://i.sstatic.net/vN3p5.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vN3p5.jpg"" alt=""attention paper""></a></p>

<p>So, this is how I think I can implement it,</p>

<pre><code>d_model = 4 # Embedding dimension

positional_embeddings = np.zeros((max_sentence_length, d_model))

max_sentence_length = 3 # as per my examples above

for position in range(maximum_sentence_length):
    for i in range(0, d_model, 2):
       positional_embeddings[position, i] = (
                                          sin(position / (10000 ** ( (2*i) / d_model) ) )
                                            )
       positional_embeddings[position, i + 1] = (
                                              cos(position / (10000 ** ( (2 * (i + 1) ) / d_model) ) )
                                                )
</code></pre>

<p>Then, the new embedding vector will be </p>

<pre><code>[[0.5, 0.2, 0.3, 0.1], 
 [0.1, 0.2, 0.5, 0.1], 
 [0.9, 0.7, 0.9, 0.1]] + positional_embeddings = NEW EMBEDDINGS

 ## shapes
  3 x 4                + 3 x 4                 = 3 x 4     
</code></pre>

<p>Is this how the calculation will be carried out in the implementation? Do correct me if there's any mistake in my above pseudo implementation.</p>

<p>If everything is correct then <strong><em>I have three doubts</em></strong> hope someone can clear them,</p>

<p>1) From the above implementation we are using sin formula for even positions and cos formula for odd positions but I couldn't understand the reason behind it? I read that it's taking use of cyclic properties but couldn't understand it.</p>

<p>2) Is there a reason behind choosing <code>10000/(2i/d)</code> or <code>10000/(2i+1/d)</code> as scaling factor in formula.</p>

<p>3) All the sentence will not be equal to max sentence length so we might have to padded the sentence so do we also calculate positional encondings to padding tokens.</p>
","transformer-model"
"61546891","Pytorch: TypeError: copy_(): argument 'other' (position 1) must be Tensor, not Vectors","2020-05-01 16:50:53","","0","865","<python><nlp><pytorch><transformer-model>","<p>I am building my model in google colab.</p>

<p>I have created a custom embedding matrix </p>

<pre><code>import torchtext.vocab as vocab

custom_embeddings = vocab.Vectors(name = 'custom_embeddings.txt')
TEXT.build_vocab(train_data, vectors = custom_embeddings)
</code></pre>

<p>Here is the code for the Encoder class:</p>

<pre><code>class Encoder(nn.Module):
    def __init__(self, 
                 input_dim, 
                 hid_dim, 
                 n_layers, 
                 n_heads, 
                 pf_dim,
                 dropout, 
                 device,
                 max_length = 100):
        super().__init__()

        self.device = device

        self.tok_embedding = nn.Embedding(input_dim, hid_dim)

        # step added for custom embedding
        self.tok_embedding.weight.data.copy_(custom_embeddings)

        self.pos_embedding = nn.Embedding(max_length, hid_dim)

        self.layers = nn.ModuleList([EncoderLayer(hid_dim, 
                                                  n_heads, 
                                                  pf_dim,
                                                  dropout, 
                                                  device) 
                                     for _ in range(n_layers)])

        self.dropout = nn.Dropout(dropout)

        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)

    def forward(self, src, src_mask):

        #src = [batch size, src len]
        #src_mask = [batch size, src len]

        batch_size = src.shape[0]
        src_len = src.shape[1]

        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)

        #pos = [batch size, src len]

        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))

        #src = [batch size, src len, hid dim]

        for layer in self.layers:
            src = layer(src, src_mask)

        #src = [batch size, src len, hid dim]

        return src
</code></pre>

<p>Now when I am trying to create the Encoder object, I am getting error for the custom embedding I have used.</p>

<pre><code>enc = Encoder(INPUT_DIM, 
              HID_DIM, 
              ENC_LAYERS, 
              ENC_HEADS, 
              ENC_PF_DIM, 
              ENC_DROPOUT, 
              device)
</code></pre>

<p>Error decscription:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-72-06d3631c029b&gt; in &lt;module&gt;()
     18               ENC_PF_DIM,
     19               ENC_DROPOUT,
---&gt; 20               device)
     21 
     22 dec = Decoder(OUTPUT_DIM, 

&lt;ipython-input-59-6c2f23451d01&gt; in __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length)
     16 
     17         # step added for custom embedding
---&gt; 18         self.tok_embedding.weight.data.copy_(custom_embeddings)
     19 
     20         self.pos_embedding = nn.Embedding(max_length, hid_dim)

TypeError: copy_(): argument 'other' (position 1) must be Tensor, not Vectors
</code></pre>

<p>Could you please help me to fix this error?</p>

<p>Thanks in advance!</p>
","transformer-model"
"61493753","I am trying to use pytorch's implementation of XLNet and got 'Trying to create tensor with negative dimension -1: [-1, 768]' when loading XLNet","2020-04-29 03:31:29","61493955","0","960","<pytorch><tensor><transformer-model>","<p>I started working on this about two months ago on Google Colab for a midterm project and everything worked perfectly. Now I am modifying it for a final project and keep getting the error 'RuntimeError: Trying to create tensor with negative dimension -1: [-1, 768]'. It looks like pytorch recently pushed a new version 1.5, so I downgraded to version 1.4 and still got the same error. Same with 1.3, and I know I wasn't using anything lower since that came out last year. I checked it with my midterm code and still got the same error, so I don't know what's going on. Here is the chunk of code related to downloading and using the model.</p>

<pre><code>    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(inputIds, 
                                                                                        labels, 
                                                                                        random_state=2020, 
                                                                                        test_size=0.2)
    train_masks, validation_masks, _, _ = train_test_split(attention_masks, inputIds, random_state=2020, 
                                                           test_size=0.2)
    # Turn data into torch tensors
    train_inputs = torch.tensor(train_inputs)
    validation_inputs = torch.tensor(validation_inputs)
    train_labels = torch.tensor(train_labels)
    validation_labels = torch.tensor(validation_labels)
    train_masks = torch.tensor(train_masks)
    validation_masks = torch.tensor(validation_masks)

    # Create Iterators of the datasets
    train_data = TensorDataset(train_inputs, train_masks, train_labels)
    train_sampler = RandomSampler(train_data)
    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)
    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)
    validation_sampler = SequentialSampler(validation_data)
    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)

    model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)
    # Loads model into GPU memory
    model.cuda()

    param_optimizer = list(model.named_parameters())
    no_decay = ['bias','gamma','beta']
    optimizer_grouped_parameters = [
        {'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],
         'weight_decay_rate':0.01},
        {'params':[p for n, p in param_optimizer if any(nd in n for nd in no_decay)],
         'weight_decay_rate':0.0}
    ]
    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)
</code></pre>

<p>The error happens on the line <code>model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)</code>. The packages I am using:</p>

<pre><code>from pandas import to_datetime
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
# MUST INSTALL PYTORCH-TRANSFORMERS
from pytorch_transformers import XLNetTokenizer, XLNetForSequenceClassification, AdamW
from tqdm import trange
from numpy import argmax, sum 
import nltk
nltk.download('punkt')
</code></pre>

<p>Thank you to anyone who tries to help.</p>
","transformer-model"
"61440281","Is positional encoding necessary for transformer in language modeling?","2020-04-26 11:54:02","63948329","6","6236","<transformer-model><language-model>","<p>I am developing a language model like <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""noreferrer"">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a>.</p>

<p>It is not clear for me - whether positional encoding is neccessary here ?
As far as I understand - it is necessary for language translation task because the decoder should be able to position  the word from the previous output within the sequence from encoder.
But is it necessary in language modeling without the decoder ?</p>

<p>Is it possible that the words in the encoder output are shuffled ?</p>

<p><strong>Edit:</strong> </p>

<p>there are no explanations in the original paper. And I didn't find explanations in tutorials (like here <a href=""https://kazemnejad.com/blog/transformer_architecture_positional_encoding/"" rel=""noreferrer"">https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</a>).</p>

<p>I don't understand this: </p>

<p>""As each word in a sentence simultaneously flows through the Transformer’s encoder/decoder stack, The model itself doesn’t have any sense of position/order for each word.""</p>

<p>From my point of view - transformer encoder has info about the order because its input is an ordered sequence (similar to RNN).</p>

<p>I tried to remove positional encoding from the model. It works, but with a worse performance.</p>

<p>Is it useful to add such positional encoding to RNN ? Could it improve its performance ?</p>
","transformer-model"
"61344110","How can PHP transformer parameter return a single value from an array","2020-04-21 12:57:22","","0","54","<php><algolia><transformer-model>","<pre><code> -&gt;transformer(function (\craft\elements\Entry $entry) {
                    return [
                        'Area of Study' =&gt; $entry-&gt;areaOfStudy,
                    ];
</code></pre>

<p>Above is the function I'm using to return </p>

<p>This array</p>

<pre><code>""Area of Study"": {
    ""label"": ""Public Administration/Social Services"",
    ""value"": ""Public Administration/Social Services"",
    ""selected"": true
  },
</code></pre>

<p>What I'd like for it to return is this: </p>

<pre><code>""Area of Study"": ""Public Administration/Social Services"",
</code></pre>

<p>The array is sent to an Algolia indices via a dropdown in a form. The full return doesn't play nice in the indices.  </p>
","transformer-model"
"61326892","Gradient of the loss of DistilBERT for measuring token importance","2020-04-20 16:04:42","61339096","0","928","<pytorch><transformer-model><attention-model><huggingface-transformers><bert-language-model>","<p>I am trying to access the gradient of the loss in DistilBERT with respect to each attention weight in the first layer. I could access the computed gradient value of the output weight matrix via the following code when <code>requires_grad=True</code> </p>

<pre><code>loss.backward()
for name, param in model.named_parameters():
    if name == 'transformer.layer.0.attention.out_lin.weight':
       print(param.grad)  #shape is [768,768]
</code></pre>

<p>where <code>model</code> is the loaded distilbert model.
My question is how to get the gradient with respect to [SEP] or [CLS] or other tokens' attention? I need it to reproduce the figure about the ""Gradient-based feature importance estimates for attention to [SEP]"" in the following link: 
<a href=""https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062"" rel=""nofollow noreferrer"">https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062</a></p>

<p>A similar question for the same purpose has been asked in the following, but it is not my issue:
<a href=""https://stackoverflow.com/questions/61286574/bert-token-importance-measuring-issue-grad-is-none"">BERT token importance measuring issue. Grad is none</a> </p>
","transformer-model"
"61286574","BERT token importance measuring issue. Grad is none","2020-04-18 08:58:12","","1","501","<deep-learning><pytorch><transformer-model><huggingface-transformers>","<p>I am trying to measure token importance for <code>BERT</code> via comparing token embedding grad value. So, to get the grad, I've copied the <code>2.8.0</code> forward of BertModel and changed it a bit: </p>

<p><code>huggingface transformers 2.8.0 BERT</code> <a href=""https://github.com/huggingface/transformers/blob/11c3257a18c4b5e1a3c1746eefd96f180358397b/src/transformers/modeling_bert.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/11c3257a18c4b5e1a3c1746eefd96f180358397b/src/transformers/modeling_bert.py</a></p>

<p>Code:</p>

<pre><code>        embedding_output = self.embeddings(
            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds
        )
        embedding_output = embedding_output.requires_grad_(True) # my code
        encoder_outputs = self.encoder(
            embedding_output,
            attention_mask=extended_attention_mask,
            head_mask=head_mask,
            encoder_hidden_states=encoder_hidden_states,
            encoder_attention_mask=encoder_extended_attention_mask,
        )
        sequence_output = encoder_outputs[0]
        sequence_output.mean().backward() # my code
        assert(embedding_output.grad is not None) # my code
</code></pre>

<p><code>Colab</code> link: <a href=""https://colab.research.google.com/drive/1MggBUaDWAAZNuXbTDM11E8jvdMGEkuRD"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1MggBUaDWAAZNuXbTDM11E8jvdMGEkuRD</a>
But it gives assertion error. I do not understand why and it seems to be a bug for me.
Please, help!</p>
","transformer-model"
"61202309","How can I get input shape of my transformer model","2020-04-14 07:03:51","","1","1085","<python><tensorflow><transformer-model>","<p>I am trying to convert my transformer model into a .tflite format but keep seeing errors about input shape no matter the values I put in. I have tried putting in batch sizes and model dimensions with no difference. The error I mainly get is 
<code>ValueError: None is only supported in the 1st dimension.</code></p>

<p>Converter code:</p>

<pre><code>import tensorflow as tf

model = tf.saved_model.load(""saved_model\d_1024\\b_128\E_440"")


concrete_func = model.signatures[
  tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY]
concrete_func.inputs[0].set_shape([the part i need help with])
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])

tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
</code></pre>

<p>and the saving code:</p>

<pre><code>
from model import transformer
from dataset import get_dataset, preprocess_sentence
from Main import evaluate

parser = argparse.ArgumentParser()
parser.add_argument(
    '--max_samples',
    default=25000,
    type=int,
    help='maximum number of conversation pairs to use')
parser.add_argument(
    '--max_length', default=40, type=int, help='maximum sentence length')
parser.add_argument('--batch_size', default=64, type=int)
parser.add_argument('--num_layers', default=2, type=int)
parser.add_argument('--num_units', default=512, type=int)
parser.add_argument('--d_model', default=1024, type=int)
parser.add_argument('--num_heads', default=8, type=int)
parser.add_argument('--dropout', default=0.1, type=float)
parser.add_argument('--activation', default='relu', type=str)
parser.add_argument('--epochs', default=20, type=int)

hparams = parser.parse_args()

dataset, tokenizer = get_dataset(hparams)

model = transformer(hparams)

model.load_weights('Weights_only/d_1024/b_128/e_200/cp.ckpt')

model.save(""saved_model\d_1024\\b_128\E_440"")
</code></pre>

<p>and the model summary:</p>

<pre><code>Model: ""transformer""
_______________________
Layer (type)                 Output Shape              Param #   
=================================================================
inputs (InputLayer)          [(None, None)]            0         
_______________________
dec_inputs (InputLayer)      [(None, None)]            0         
_______________________
enc_padding_mask (Lambda)    multiple                  0         
_______________________
encoder (Model)              multiple                  18848768  
_______________________
look_ahead_mask (Lambda)     multiple                  0         
_______________________
dec_padding_mask (Lambda)    multiple                  0         
_______________________
decoder (Model)              multiple                  27249664  
_______________________
outputs (Dense)              multiple                  8351700   
=================================================================
Total params: 54,450,132
Trainable params: 54,450,132
Non-trainable params: 0
_______________________
</code></pre>

<p>the model is specifically from 
<a href=""https://github.com/bryanlimy/tf2-transformer-chatbot"" rel=""nofollow noreferrer"">https://github.com/bryanlimy/tf2-transformer-chatbot</a> but I added</p>

<pre><code>checkpoint_path = ""Weights_only/d_1024/b_128/e_200/cp.ckpt""
    checkpoint_dir = os.path.dirname(checkpoint_path)

    # Create a callback that saves the model's weights
    cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                     save_weights_only=True,
                                                     verbose=1)

    model.fit(dataset, epochs=hparams.epochs, callbacks=[cp_callback])
</code></pre>

<p>to save the weights for the model</p>
","transformer-model"
"61196578","Whitespace tokenizer for training BERT language model from scratch with Huggingface","2020-04-13 20:55:50","","2","1257","<pytorch><transformer-model><huggingface-transformers>","<p>I am trying to train a BERT language model from scratch using <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">Huggingface API</a>. For that I need to build a tokenizer that tokenize the text data based on white spaces only, nothing else. I understand that there are multiple tonkenizers available in Huggingface (such as <code>BPE</code>, <code>WordPiece</code>) that produce good results for language models but for my use case I want to tokenize text input based on whitespace only and generate vocabs that should not have any kind of special characters viz ""##"" in front of words. </p>

<p>For example: the input <code>Hello, y'all! How are you?</code> should be tonkenized as:</p>

<p><code>Hello,</code>, <code>y'all!</code>, <code>How</code>, <code>are</code>, <code>you?</code></p>

<p>I checked the documentation <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">[1]</a> and <a href=""https://github.com/huggingface/tokenizers"" rel=""nofollow noreferrer"">[2]</a> but did not find a way to achieve this. </p>
","transformer-model"
"60997438","How to revert BERT/XLNet embeddings?","2020-04-02 17:23:43","","5","1973","<python><nlp><pytorch><huggingface-transformers><transformer-model>","<p>I've been experimenting with stacking language models recently and noticed something interesting: the output embeddings of BERT and XLNet are not the same as the input embeddings. For example, this code snippet:</p>

<pre><code>bert = transformers.BertForMaskedLM.from_pretrained(""bert-base-cased"")
tok = transformers.BertTokenizer.from_pretrained(""bert-base-cased"")

sent = torch.tensor(tok.encode(""I went to the store the other day, it was very rewarding.""))
enc = bert.get_input_embeddings()(sent)
dec = bert.get_output_embeddings()(enc)

print(tok.decode(dec.softmax(-1).argmax(-1)))
</code></pre>

<p>Outputs this for me:</p>

<pre><code>,,,,,,,,,,,,,,,,,
</code></pre>

<p>I would have expected the (formatted) input sequence to be returned since I was under the impression that the input and output token embeddings were tied.</p>

<p>What's interesting is that most other models do not exhibit this behavior. For example, if you run the same code snippet on GPT2, Albert or Roberta, it outputs the input sequence.</p>

<p>Is this a bug? Or is it expected for BERT/XLNet?</p>
","transformer-model"
"60975829","Issue when preprocessing text with Ktrain and DistilBERT","2020-04-01 16:33:22","60987753","1","1445","<python><keras><transformer-model><bert-language-model><distilbert>","<p>Following the example notebook here:</p>

<p><a href=""https://github.com/amaiya/ktrain/blob/master/examples/text/20newsgroup-distilbert.ipynb"" rel=""nofollow noreferrer"">https://github.com/amaiya/ktrain/blob/master/examples/text/20newsgroup-distilbert.ipynb</a></p>

<p>At STEP 1: Preprocess Data, I run into the errors listed below. When I do exactly the same in a Colab notebook, it works. What am I missing on my machine? I <strong>am</strong> able to run this with BERT, DistilBERT causes problems.</p>

<pre><code>trn, val, preproc = text.texts_from_array(x_train=x_train, y_train=y_train,
                                      x_test=x_test, y_test=y_test,
                                      class_names=class_names,
                                      preprocess_mode='distilbert',
                                      maxlen=350)
</code></pre>

<p>causes:</p>

<pre><code>    ---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
&lt;ipython-input-142-ff3842c91276&gt; in &lt;module&gt;
      3                                           class_names=class_names,
      4                                           preprocess_mode='distilbert',
----&gt; 5                                           maxlen=350)

/usr/local/lib/python3.7/site-packages/ktrain/text/data.py in texts_from_array(x_train, y_train, x_test, y_test, class_names, max_features, maxlen, val_pct, ngram_range, preprocess_mode, lang, random_state, verbose)
    337                            classes = class_names,
    338                            lang=lang, ngram_range=ngram_range)
--&gt; 339     trn = preproc.preprocess_train(x_train, y_train, verbose=verbose)
    340     val = preproc.preprocess_test(x_test,  y_test, verbose=verbose)
    341     return (trn, val, preproc)

/usr/local/lib/python3.7/site-packages/ktrain/text/preprocessor.py in preprocess_train(self, texts, y, mode, verbose)
    766                                       pad_on_left=bool(self.name in ['xlnet']),
    767                                       pad_token=self.tok.convert_tokens_to_ids([self.tok.pad_token][0]),
--&gt; 768                                       pad_token_segment_id=4 if self.name in ['xlnet'] else 0)
    769         self.set_multilabel(dataset, mode)
    770         return dataset

/usr/local/lib/python3.7/site-packages/ktrain/text/preprocessor.py in hf_convert_examples(texts, y, tokenizer, max_length, pad_on_left, pad_token, pad_token_segment_id, mask_padding_with_zero)
    280                                           pad_token=pad_token,
    281                                           pad_token_segment_id=pad_token_segment_id,
--&gt; 282                                           mask_padding_with_zero=mask_padding_with_zero)
    283             features_list.append(features)
    284             labels.append(y[idx] if y is not None else None)

/usr/local/lib/python3.7/site-packages/ktrain/text/preprocessor.py in hf_convert_example(text, tokenizer, max_length, pad_on_left, pad_token, pad_token_segment_id, mask_padding_with_zero)
    206         max_length=max_length,
    207     )
--&gt; 208     input_ids, token_type_ids = inputs[""input_ids""], inputs[""token_type_ids""]
    209 
    210     # The mask has 1 for real tokens and 0 for padding tokens. Only real

KeyError: 'token_type_ids'
</code></pre>

<p>Any ideas what's wrong here?</p>
","transformer-model"
"60942550","BERT training with character embeddings","2020-03-31 02:30:42","","5","2809","<nlp><pytorch><tokenize><transformer-model>","<p>Does it make sense to change the tokenization paradigm in the BERT model, to something else? Maybe just a simple word tokenization or character level tokenization?</p>
","transformer-model"
"60937617","How to reconstruct text entities with Hugging Face's transformers pipelines without IOB tags?","2020-03-30 18:58:03","61909224","12","12180","<nlp><tokenize><transformer-model><named-entity-recognition><huggingface-transformers>","<p>I've been looking to use Hugging Face's Pipelines for NER (named entity recognition). However, it is returning the entity labels in inside-outside-beginning (IOB) format but <a href=""https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)"" rel=""noreferrer"">without the IOB labels</a>. So I'm not able to map the output of the pipeline back to my original text. Moreover, the outputs are masked in BERT tokenization format (the default model is BERT-large).</p>

<p>For example: </p>

<pre class=""lang-py prettyprint-override""><code>from transformers import pipeline
nlp_bert_lg = pipeline('ner')
print(nlp_bert_lg('Hugging Face is a French company based in New York.'))
</code></pre>

<p>The output is:</p>

<pre><code>[{'word': 'Hu', 'score': 0.9968873858451843, 'entity': 'I-ORG'},
{'word': '##gging', 'score': 0.9329522848129272, 'entity': 'I-ORG'},
{'word': 'Face', 'score': 0.9781811237335205, 'entity': 'I-ORG'},
{'word': 'French', 'score': 0.9981815814971924, 'entity': 'I-MISC'},
{'word': 'New', 'score': 0.9987512826919556, 'entity': 'I-LOC'},
{'word': 'York', 'score': 0.9976728558540344, 'entity': 'I-LOC'}]
</code></pre>

<p>As you can see, New York is broken up into two tags.</p>

<p>How can I map Hugging Face's NER Pipeline back to my original text?</p>

<p>Transformers version: 2.7</p>
","transformer-model"
"60892539","Problem running Tensorflow Transformer Tutorial","2020-03-27 19:00:45","","2","259","<tensorflow><attributeerror><transformer-model>","<p>I was checking the TensorFlow tutorial ""Transformer model for language understanding,"" and I copied the code exactly as it is into my Spyder 4 environment. However, the code shows the following error when running:</p>

<pre><code>AttributeError: 'RepeatedCompositeFieldContainer' object has no attribute 'append'
</code></pre>

<p>I checked the code and realized that the error comes from the call function of the MultiHeadAttention class. However, I do not understand what the problem is since the code runs just fine in the Colab notebook.</p>

<pre><code>class MultiHeadAttention(tf.keras.layers.Layer):


  def __init__(self, d_model, num_heads):
    super(MultiHeadAttention, self).__init__()
    self.num_heads = num_heads
    self.d_model = d_model

    assert d_model % self.num_heads == 0

    self.depth = d_model // self.num_heads

    self.wq = tf.keras.layers.Dense(d_model)
    self.wk = tf.keras.layers.Dense(d_model)
    self.wv = tf.keras.layers.Dense(d_model)

    self.dense = tf.keras.layers.Dense(d_model)



def split_heads(self, x, batch_size):
    """"""Split the last dimension into (num_heads, depth).
    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)
    """"""
    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
    return tf.transpose(x, perm=[0, 2, 1, 3])

def call(self, v, k, q, mask):
    batch_size = tf.shape(q)[0]

    q = self.wq(q)  # (batch_size, seq_len, d_model)
    k = self.wk(k)  # (batch_size, seq_len, d_model)
    v = self.wv(v)  # (batch_size, seq_len, d_model)

    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)
    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)
    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)

    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)
    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)
    scaled_attention, attention_weights = scaled_dot_product_attention(
        q, k, v, mask)

    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, 
        num_heads, depth)

    concat_attention = tf.reshape(scaled_attention, 
                              (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)

    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)

    return output, attention_weights
</code></pre>

<p>The error shows when executing the line of <code>q = self.wq(q)</code> in the call function. Any help will be appreciated.</p>

<p>Thanks in advance.</p>
","transformer-model"
"60838718","Pytorch crashes on input in eval mode","2020-03-24 20:21:42","","0","513","<python><deep-learning><neural-network><pytorch><transformer-model>","<p>My model trains perfectly fine, but when I switch it to evaluation mode it does not like the data types of the input samples:</p>

<pre><code>Traceback (most recent call last):
  File ""model.py"", line 558, in &lt;module&gt;
    main_function(train_sequicity=args.train)
  File ""model.py"", line 542, in main_function
    out = model(user, bspan, response_, degree)
  File ""/home/memduh/git/project/venv/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File ""model.py"", line 336, in forward
    self.params['bspan_size'])
  File ""model.py"", line 283, in _greedy_decode_output
    out = decoder(input_, encoder_output)
  File ""/home/memduh/git/project/venv/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 541, in __call__
    result = self.forward(*input, **kwargs)
  File ""model.py"", line 142, in forward
    tgt = torch.cat([go_tokens, tgt], dim=0)  # concat GO_2 token along sequence lenght axis
RuntimeError: Expected object of scalar type Long but got scalar type Float for sequence element 1 in sequence argument at position #1 'tensors'
</code></pre>

<p>This seems to occur in a part of the code where concatenation happens. This is in an architecture similar to the pytorch transformer, just modified to have two decoders:</p>

<pre><code>      def forward(self, tgt, memory):
          """""" Call decoder
          the decoder should be called repeatedly

          Args:
              tgt: input to transformer_decoder, shape: (seq, batch)
              memory: output from the encoder

          Returns:
              output from linear layer, (vocab size), pre softmax

          """"""
          go_tokens = torch.zeros((1, tgt.size(1)), dtype=torch.int64) + 3  # GO_2 token has index 3

          tgt = torch.cat([go_tokens, tgt], dim=0)  # concat GO_2 token along sequence lenght axis

+
          mask = tgt.eq(0).transpose(0,1)  # 0 corresponds to &lt;pad&gt;
          tgt = self.embedding(tgt) * self.ninp
          tgt = self.pos_encoder(tgt)
          tgt_mask = self._generate_square_subsequent_mask(tgt.size(0))
          output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=mask)
          output = self.linear(output)
          return output
</code></pre>

<p>The concatenation bit in the middle of the codeblock is where the problem happens. The odd thing is that it works perfectly fine and trains, with loss going down in train mode. This issue only comes up in eval mode. What could the problem be?</p>
","transformer-model"
"60833301","Train huggingface's GPT2 from scratch : assert n_state % config.n_head == 0 error","2020-03-24 14:42:13","60833512","0","907","<python><nlp><huggingface-transformers><transformer-model><gpt-2>","<p>I am trying to use a GPT2 architecture for musical applications and consequently need to train it from scratch. After a bit of googling I found that the issue #1714 from huggingface's github already had ""solved"" the question. When I try the to run the propose solution :</p>

<pre><code>from transformers import GPT2Config, GPT2Model

NUMLAYER = 4
NUMHEAD = 4
SIZEREDUCTION = 10 #the factor by which we reduce the size of the velocity argument.
VELSIZE = int(np.floor(127/SIZEREDUCTION)) + 1 
SEQLEN=40 #size of data sequences.
EMBEDSIZE = 5 

config = GPT2Config(vocab_size = VELSIZE, n_positions = SEQLEN, n_embd = EMBEDSIZE, n_layer = NUMLAYER, n_ctx = SEQLEN, n_head = NUMHEAD)  
model = GPT2Model(config)
</code></pre>

<p>I get the following error : </p>

<pre><code>Traceback (most recent call last):

  File ""&lt;ipython-input-7-b043a7a2425f&gt;"", line 1, in &lt;module&gt;
    runfile('C:/Users/cnelias/Desktop/PHD/Swing project/code/script/GPT2.py', wdir='C:/Users/cnelias/Desktop/PHD/Swing project/code/script')

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 786, in runfile
    execfile(filename, namespace)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/cnelias/Desktop/PHD/Swing project/code/script/GPT2.py"", line 191, in &lt;module&gt;
    model = GPT2Model(config)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in __init__
    self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 355, in &lt;listcomp&gt;
    self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 223, in __init__
    self.attn = Attention(nx, n_ctx, config, scale)

  File ""C:\Users\cnelias\Anaconda3\lib\site-packages\transformers\modeling_gpt2.py"", line 109, in __init__
    assert n_state % config.n_head == 0
</code></pre>

<p>What does it mean and how can I solve it ?</p>

<p>Also more generally, is there a documentation on how to do a forward call with the GPT2 ? Can I define my own <code>train()</code> function or do I have to use the model's build-in function ? Am I forced to use a <code>Dataset</code> to do the training or can I feed it individual tensors ? 
I looked for it but couldn't find answer to these on the doc, but maybe I missed something.</p>

<p>PS : I already read the blogpost fron huggingface.co, but it omits too much informations and details to be usefull for my application.</p>
","transformer-model"
"60803757","How to do Sentence Similarity with XLNet?","2020-03-22 19:08:20","","5","1365","<python><nlp><embedding><cosine-similarity><transformer-model>","<p>I want to perform a sentence similarity task and tried the following:</p>

<pre><code>from transformers import XLNetTokenizer, XLNetModel
import torch
import scipy
import torch.nn as nn
import torch.nn.functional as F

tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')
model = XLNetModel.from_pretrained('xlnet-large-cased')

input_ids = torch.tensor(tokenizer.encode(""Hello, my animal is cute"", add_special_tokens=False)).unsqueeze(0)
outputs = model(input_ids)
last_hidden_states = outputs[0]

input_ids = torch.tensor(tokenizer.encode(""I like your cat"", add_special_tokens=False)).unsqueeze(0) 

outputs1 = model(input_ids)
last_hidden_states1 = outputs1[0]

cos = nn.CosineSimilarity(dim=1, eps=1e-6)
output = cos(last_hidden_states, last_hidden_states1)
</code></pre>

<p>However, I get the following error:</p>

<pre><code>RuntimeError: The size of tensor a (7) must match the size of tensor b (4) at non-singleton dimension 1
</code></pre>

<p>Can anybody tell me, what I am doing wrong? Is there a better way to do it?</p>
","transformer-model"
"60796625","Why multiply sqrt(dim) to the encoded input for Transformer in the Tensorflow tutorial?","2020-03-22 06:41:58","","1","69","<tensorflow><transformer-model>","<p>I'm trying to build up Speech Transformer models using Tensorflow 2.1.0.
There is a line I cannot understand in the Tensorflow tutorial.</p>

<p>sqrt(dim) is multiplied to encoded inputs to both encoders and decoders in the tutorial for Transformer on <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/transformer</a>.</p>

<p>In the Encoder class,</p>

<pre><code>  def call(self, x, training, mask):

    seq_len = tf.shape(x)[1]

    # adding embedding and position encoding.
    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))  &lt;&lt;== This the line I'm asking about now. (You can also find exact same line in the call function of Decoder class.)
    x += self.pos_encoding[:, :seq_len, :]

    x = self.dropout(x, training=training)

    for i in range(self.num_layers):
      x = self.enc_layers[i](x, training, mask)

    return x  # (batch_size, input_seq_len, d_model)
</code></pre>

<p>Could you explain the reason why?
(I could not find any lines multiplying sqrt(dim) to the encoded inputs in the official transformer model on <a href=""https://github.com/tensorflow/models/blob/master/official/nlp/transformer/transformer.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/official/nlp/transformer/transformer.py</a>.)</p>
","transformer-model"
"60769118","Torch.nn.Transformer Example Code Throwing Tensor Shape Errors","2020-03-20 05:43:30","","1","760","<python><pytorch><transformer-model>","<p>I was trying to implement a Transformer model with Pytorch and was experimenting with the example from <a href=""https://github.com/pytorch/examples/tree/master/word_language_model"" rel=""nofollow noreferrer"">this GitHub repo</a>, which was linked from <a href=""https://pytorch.org/docs/stable/nn.html#transformer-layers"" rel=""nofollow noreferrer"">here in the documentation</a>, and ran into a problem within the PositionalEncoding class, found within model.py.</p>

<p>The code for the class's <code>__init__()</code> function is as follows:</p>

<pre><code>def __init__(self, d_model, dropout=0.1, max_len=5000):
    super(PositionalEncoding, self).__init__()
    self.dropout = nn.Dropout(p=dropout)

    pe = torch.zeros(max_len, d_model)
    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)
    pe = pe.unsqueeze(0).transpose(0, 1)
    self.register_buffer('pe', pe)
</code></pre>

<p>This code, ran with d_model = 103, threw the following error on the forth last line (<code>pe[:, 0::2] =</code>...):</p>

<p>RuntimeError: The expanded size of the tensor (52) must match the existing size (51) at non-singleton dimension 1.  Target sizes: [5000, 52].  Tensor sizes: [5000, 51]</p>

<p>I've found this error fairly impenetrable and haven't had much success writing my own similarly effective implementation. 
My first guess would be that this is a problem with version changes within Python/PyTorch, but it could of course be something else I'm missing.</p>
","transformer-model"
"60765496","How to interpret the P numbers that fairseq generate produces?","2020-03-19 21:44:28","63108258","5","1189","<python><pytorch><transformer-model>","<p>Using fairseq-generate.py, with the transformer architecture, each translation produces a section like this:</p>

<pre><code>Why is it rare to discover new marine mammal species?
S-0     Why is it rare to discover new marine mam@@ mal species ?
H-0     -0.0643349438905716     Pourquoi est-il rare de découvrir de nouvelles espèces de mammifères marins?
P-0     -0.0763 -0.1849 -0.0956 -0.0946 -0.0735 -0.1150 -0.1301 -0.0042 -0.0321 -0.0171 -0.0052 -0.0062 -0.0015
</code></pre>

<p>With <a href=""https://fairseq.readthedocs.io/en/latest/getting_started.html#evaluating-pre-trained-models"" rel=""noreferrer"">this explanation</a>:</p>

<blockquote>
  <p>H is the hypothesis along with an average log-likelihood; and P is the positional score per token position, including the end-of-sentence marker</p>
</blockquote>

<p>I'm wondering if it is reasonable to say a low (absolute) number in the P row means higher confidence in that particular word? E.g. does -0.07 for ""Pourquoi"" means it was happier about that than it was (-0.1849) for ""est-il""? And the low -0.0015 at the end means it was really confident the sentence should end there.</p>

<p>Background: What I'm trying to work out is if I can use either the H number, or somehow to use the individual P numbers, to get a confidence measure in its translation. I've been analyzing a handful of translations against the H number and didn't notice much correspondence between it and my subjective opinion of translation quality. But I've a couple where I thought it was particularly poor - it had missed a bit of key information - and the final P number was a relatively high <code>-0.6099</code> and <code>-0.3091</code> (The final P number is <code>-0.11</code> or so on most of them.)</p>
","transformer-model"
"60682287","Tensorflow transformer with CNN","2020-03-14 11:36:25","","1","355","<tensorflow><ocr><transformer-model><conv-neural-network>","<p>I try to implement OCR in tensorflow. I would like to use transformer in combination with CNN for input image. I try this tutorial: <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/transformer</a> and now I need to remake it to use CNN for the input image. Do you know any implementations or tutorials? Because I can't find anything. I am new to tensorflow so I don't have an idea how to exchange input sentence instead of input image. Thank you</p>
","transformer-model"
"60662998","Using Model as a Layer in another Model, First model not training","2020-03-12 23:19:16","","0","41","<python><tensorflow><keras><transformer-model>","<p>I built a Keras model that uses another model as a layer, but the problem is the weights in the other model are not training.  How to I get around this?</p>

<p>For more details, I am using a transformer to encode sentences individually, then combining the set of sentences with another transformer.  </p>

<p>Here is the pseudo code:</p>

<pre><code>Class:
 def build_context_encoder(self):
     a = Input(sentences shape)
     #function stuff
     b = #transformer structure
     context_encoder = Model(inputs=[a], outputs=b)
     return context encoder

  def build_model(self):
    list_of _contexts = Input(list of contexts shape)
    context_embs = Lambda(lambda x: K.map_fn(fn=self.context_encoder, elems=x, dtype=tf.float32))(list_of_contexts)
    c = #rest of the model (context_embs)
    model = Model(inputs=[list_of _contexts], outputs=c)
    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[])

    return model

  def __init__():
    self.context_encoder = self.build_context_encoder()

    self.model = self.build_model()
</code></pre>

<p>Why don't the weights in context_encoder update when I call fit?  Is it due to the map_fn, or because I'm calling the model?  How do I fix this? </p>
","transformer-model"
"60548773","Pre-train BERT base for text classification","2020-03-05 15:25:52","","0","826","<deep-learning><nlp><transformer-model>","<p>I have a tweet corpus and I am trying to use BERT for classification. I have successfully pre-trained BERT using my corpus, and it has generated the checkpoint files. Now I need to use this new trained model, and add some more layers to it. 
I tried to use the ""load_trained_model_from_checkpoint"" function from keras_bert, but it's failing with error ""cls/predictions/transform/dense/kernel not found in checkpoint"". Can anyone please help me fix this bug. Thanks</p>
","transformer-model"
"60530393","GilBERTo (version Italian) from RoBERTa","2020-03-04 16:18:38","","0","468","<python><transformer-model><named-entity-recognition><huggingface-transformers>","<p>i tried to train a NER model with GilBERTo (tokenizer and Model) the italian version of RoBERTa (from CamemBert), but i have the error below, 
someone could help me to understand it and maybe a possible solution?</p>

<p>the solution replicate the: <a href=""https://github.com/monologg/R-BERT"" rel=""nofollow noreferrer"">GitHub: R-BERT entity relation</a>
(I just change the utils.py with: </p>

<pre><code>def load_tokenizer(args):
    tokenizer = AutoTokenizer.from_pretrained(""idb-ita/gilberto-uncased-from-camembert"", do_lower_case=True)
    #tokenizer = MODEL_CLASSES[args.model_type][2].from_pretrained(args.model_name_or_path)
    tokenizer.add_special_tokens({""additional_special_tokens"": ADDITIONAL_SPECIAL_TOKENS})
    return tokenizer
</code></pre>

<p>)
and then i use model ""roberta"" in main.py with just change the Tokenizer to GilBERTo. And i used a different Dataset with the same format like the original project.
Note: i disable the GPU for size problem in cache upload, and i use transformers 2.4.1
thanks a lot!</p>

<pre><code>03/04/2020 16:33:01 - INFO - trainer -   ***** Config loaded *****
03/04/2020 16:33:01 - INFO - transformers.modeling_utils -   loading weights file ./model\pytorch_model.bin
03/04/2020 16:33:01 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at C:\Users\ADMIN\.cache\torch\transformers\228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e
Traceback (most recent call last):
  File ""C:\Users\ADMIN\R-BERT\trainer.py"", line 196, in load_model
    self.model = self.model_class.from_pretrained(self.args.model_dir, config=self.bert_config, args=self.args)
  File ""C:\Users\ADMIN\Miniconda3\envs\envTransformers\lib\site-packages\transformers\modeling_utils.py"", line 463, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File ""C:\Users\ADMIN\R-BERT\model.py"", line 31, in __init__
    self.bert = PRETRAINED_MODEL_MAP[args.model_type].from_pretrained(args.model_name_or_path, config=bert_config)  # Load pretrained bert
  File ""C:\Users\ADMIN\Miniconda3\envs\envTransformers\lib\site-packages\transformers\modeling_utils.py"", line 555, in from_pretrained
    model.__class__.__name__, ""\n\t"".join(error_msgs)
RuntimeError: Error(s) in loading state_dict for RobertaModel:
        size mismatch for roberta.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50265, 768]) from checkpoint, the shape in current model is torch.Size([30522, 768]).
        size mismatch for roberta.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).
        size mismatch for roberta.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""main.py"", line 60, in &lt;module&gt;
    trainer.load_model()
  File ""C:\Users\ADMIN\R-BERT\trainer.py"", line 200, in load_model
    raise Exception(""Some model files might be missing..."")
Exception: Some model files might be missing...
</code></pre>
","transformer-model"
"60505798","Why can Bert's three embeddings be added?","2020-03-03 11:03:09","60525701","1","980","<vector><nlp><embedding><transformer-model><bert-language-model>","<p>I already know the meaning of Token Embedding, Segment Embedding, and Position Embedding. But why can these three vectors be added together? The Size and direction of vectors will change after the addition, and the semantics of the word will also change. (It's the same question for the Transformer model which has two Embeddings named Input Embedding and Position Embedding.)</p>
","transformer-model"
"60477299","How can I implement these bash commands in Google Colab","2020-03-01 16:40:04","60477370","0","1017","<bash><pytorch><google-colaboratory><transformer-model>","<p>I'm a beginner who is working on Neural Machine Translation, the transformer model. I want to implement <a href=""https://github.com/pytorch/fairseq/tree/master/examples/scaling_nmt"" rel=""nofollow noreferrer"">fairseq Scaling Neural Machine Translation</a> using Google Colab. I guess the commands shown in the README file is written in bash. I know that bash commands can be run in Google Colab by prefixing the command with <code>!</code>. Following commands are from the Github repository mentioned above.</p>

<pre><code>TEXT=wmt16_en_de_bpe32k
mkdir -p $TEXT
tar -xzvf wmt16_en_de.tar.gz -C $TEXT
</code></pre>

<p>These commands throw errors when I add the <code>!</code> as follows. </p>

<p><a href=""https://i.sstatic.net/V4mfj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/V4mfj.png"" alt=""enter image description here""></a></p>
","transformer-model"
"60378466","If BERT's [CLS] can be retrained for a variety of sentence classification objectives, what about [SEP]?","2020-02-24 14:54:01","60390369","1","1174","<transformer-model><bert-language-model><huggingface-transformers>","<p>In BERT pretraining, the [CLS] token is embedded into the input of a classifier tasked with the Next Sentence Prediction task (or, in some BERT variants, with other tasks, such as ALBERT's Sentence Order Prediction); this helps in the pretraining of the entire transformer, and it also helps to make the [CLS] position readily available for retraining to other ""sentence scale"" tasks.</p>

<p>I wonder whether [SEP] could also be retrained in the same manner.
While [CLS] will probably be easier to retrain as the transformer is already trained to imbue its embedding with meaning from across the sentence, while [SEP] does not have these ""connections"" (one would assume), this might still work with sufficient fine-tuning. </p>

<p>With this one could retrain the same model for two different classification tasks,   one using [CLS] and one using [SEP].</p>

<p>Am I missing anything? 
Is there a reason why this would not work? </p>
","transformer-model"
"60158032","Can't install pip package (No metadata found)","2020-02-10 20:36:32","","5","6658","<python><tensorflow><pip><anaconda><transformer-model>","<p>I'm trying to install <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">transformers</a> with <code>pip install transformers</code> inside an <a href=""https://www.anaconda.com/"" rel=""nofollow noreferrer"">Anaconda</a> environment running <a href=""https://www.tensorflow.org/"" rel=""nofollow noreferrer"">TensorFlow 2.1.0</a> and I get the following error:</p>

<pre><code>WARNING: No metadata found in c:\users\gamer\anaconda3\envs\tensorflow\lib\site-packages
ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'c:\\users\\gamer\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\chardet-3.0.4.dist-info\\METADATA'
</code></pre>

<p>There is a <code>metadata.json</code> file in that directory.</p>

<p>It also fails with the same error when installing other packages such as <code>pip install tf-nightly</code>.</p>

<p>I have tried enabling long paths as suggested in <a href=""https://stackoverflow.com/questions/54778630/could-not-install-packages-due-to-an-environmenterror-errno-2-no-such-file-or"">Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory \METADATA</a>.</p>

<p>How do I solve this?</p>

<p>Ps.: installing <code>transformers</code> from <a href=""https://anaconda.org/conda-forge/transformers"" rel=""nofollow noreferrer"">anaconda transformers</a> doesn't work for me, because it's using an old version (I mean, the installation works, but the version doesn't have what I need).</p>
","transformer-model"
"59945101","Parsing includes for nested tranformers","2020-01-28 08:56:24","59987476","2","560","<php><transformer-model><thephpleague-fractal>","<p>I'm trying to figure out how I can parse includes for nested transformers, given the scenario below:</p>

<p>I have a controller which sets my ArraySerializer, parses includes for the OrderTransformer and then creates the data:</p>

<pre class=""lang-php prettyprint-override""><code>$data = (new Manager())
    -&gt;setSerializer(new ArraySerializer())
    -&gt;parseIncludes('dispatches')
    -&gt;createData(new Collection($orders, new OrderTransformer()));
</code></pre>

<p>Inside my order transformer I have the include dispatches which I'm parsing from the above:</p>

<pre class=""lang-php prettyprint-override""><code>class OrderTransformer
{
    protected $availableIncludes = [
        'dispatches',
    ];

    public function transform($order)
    {
        return [];
    }

    public function includeDispatches($order)
    {
        return $this-&gt;collection($order-&gt;getDispatches(), new DispatchesTransformer());
    }
}
</code></pre>

<p>However where I'm getting stuck is inside my DispatchesTransformer:</p>

<pre class=""lang-php prettyprint-override""><code>class DispatchesTransformer
{
    protected $avaiableIncludes = [
        'product',
    ];

    public function transform($order)
    {
        return [];
    }

    public function includeProduct()
    {
        // ...
    }
}
</code></pre>

<p>I have an available include product I'd like to use. I don't want to make this a default include. How  can I use that include?</p>

<p>I have tried something like this from my controller:</p>

<p><code>-&gt;parseIncludes(['dispatches', 'product'])</code></p>
","transformer-model"
"59939549","Tensorflow Transformer Decoder output not giving the expected result","2020-01-27 22:03:48","","0","243","<python><tensorflow><transformer-model><attention-model>","<p>I have designed an transformer model using tensorflow. The aim of the model is to generate a sequence of text which is ideally an question followed by an answer given an input sentence.</p>

<p>I have datapoints ( around 15k ) whose format is as below</p>

<pre><code>SOURCE SENTENCE: &lt;@&gt;A man in the distance is walking past a brick wall painted with words and graffiti.&lt;#&gt;where&lt;%&gt;wall&lt;?&gt;brick
TARGET SENTENCE: &lt;^&gt;where is the man walking ?&lt;~&gt;A man is walking past a brick wall
</code></pre>

<p>I have trained the model using sentencepiece tokenizer. </p>

<p>For some reason, even after training the model upto 100 epochs, I am not getting the desired output. I expect the network to pick up the 
words from the source sentence and construct an Question Answer pair. But, in actual, the network constructs a question answer pair ( which is really good but the words which it user is not in the source sentence. </p>

<p>Below is the output from the network from the above source input after 50 Epochs for a beam search width of 15. </p>

<pre><code>PRED: &lt;^&gt;what does the woman?&lt;~&gt;the girls are young
PRED: &lt;^&gt;what was the girl holding ?&lt;~&gt;the girl was be
PRED: &lt;^&gt;what was the girl doing ?&lt;~&gt;the man are posing.
PRED: &lt;^&gt;what was the girl doing ?&lt;~&gt;the man are posing
PRED: &lt;^&gt;what was the girl holding ?&lt;~&gt;the girl was looking
PRED: &lt;^&gt;what was the girl holding ?&lt;~&gt;a man wearing a black shirt.
PRED: &lt;^&gt;what is the girls are ?&lt;~&gt;the girls are wearing a young man
PRED: &lt;^&gt;what is the girls are ?&lt;~&gt;the girls are wearing a
PRED: &lt;^&gt;what was the girl holding ?&lt;~&gt;the girl was be for the field
PRED: &lt;^&gt;what was the girl holding ?&lt;~&gt;the girl was holding a swing
PRED: &lt;^&gt;what was the girl doing ?&lt;~&gt;the man are tryings
PRED: &lt;^&gt;what is the girls are ?&lt;~&gt;the girls are wearing a brunette
PRED: &lt;^&gt;what was the girl holding ?&lt;~&gt;the girl was holding a peace man
PRED: &lt;^&gt;what is the girls are ?&lt;~&gt;the girls are wearing a older girl
PRED: &lt;^&gt;what is the girls are ?&lt;~&gt;the girls are wearing a older
</code></pre>

<p>I am not sure where I am going wrong. I am quite sure that the network is learning from the training which is very promising given  the way the output is constructed but the main issue here is the question answer is formed from words which are not in the source sentence. </p>

<p>Is there a way to instruct the network to mainly use the words from the source sentence only? Below is the decoder output function.</p>

<pre><code>def symbols_to_logits_fn(model, config, decoder_tensor, debug=False):
    '''We basically need to run the complete decoder function
    :param model: namespace returned from function
    :param decoder_tensor: [batch_size * beam_size, decoded_length]
    :return new_ids: [batch_size * beam_size, vocab_size]
    '''
    print('^^^^^^ decoder_tensor: {}'.format(decoder_tensor))
        decoder_gather = tf.gather(
        model.context_embedding, decoder_tensor
    ) * (config.embedding_dim ** 0.5)
    decoder_gather += tf.gather(model.position_embedding,
                            positions_for(decoder_tensor, past_length=0))
    print('&gt;&gt;&gt;&gt;&gt; {}'.format(decoder_gather))
    encoder_tiled = tf.tile(model.encoder_embedding, [config.beam_size, 1, 1])
    print('&gt;&gt;&gt; encoder_tiled: {}'.format(encoder_tiled))
    local_decoder_pad_mask = tf.math.equal(
        decoder_tensor, config.pad_id, name='beam_decoder_pad_mask')
    print('&gt;&gt;&gt;&gt; local_decoder_pad_mask: {}'.format(local_decoder_pad_mask))
    decoder_out_func = transformer_model.decoder_fn(config=config,
                                                dec_out=decoder_gather,
                                                enc_out=encoder_tiled,
                                                encoder_pad_mask=model.encoder_pad_mask,
                                                decoder_pad_mask=local_decoder_pad_mask)  # [bs,   None, embedding_dim]
    print('&gt;&gt;&gt;&gt; decoder_out_func: {}'.format(decoder_out_func))
    # [bs, None, vocab_size]
    decoder_out = tf.matmul(decoder_out_func, model.fproj_w, transpose_b=False)
    print('&gt;&gt;&gt;&gt; decoder_out: {}'.format(decoder_out))
    decoder_out_last_step = decoder_out[:, -1, :]  # [bs, vocab_size]
    print('&gt;&gt;&gt; decoder_out_last_step: {}'.format(decoder_out_last_step))
    return decoder_out_last_step
</code></pre>

<p>Could anyone help me in resolving this issue. I feel I am too close to quit. Any help to tweak the network would be very helpful.</p>
","transformer-model"
"59910494","Models passed to `fit` can only have `training` and the first argument in `call` as positional arguments, found","2020-01-25 15:07:12","","9","5346","<python><tensorflow><keras><compilation><transformer-model>","<p>I am trying to follow this code but on another data set:<a href=""https://www.tensorflow.org/tutorials/text/transformer#encoder_layer"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/text/transformer#encoder_layer</a>
I needed to compile and fit the model. however, I get this error while running; I don't know what it means:</p>

<pre><code> Models passed to `fit` can only have `training` and the first argument in `call` as positional arguments, found: ['tar', 'enc_padding_mask', 'look_ahead_mask', 'dec_padding_mask'].
</code></pre>

<p>Here it is the model:</p>

<pre><code>class Transformer(tf.keras.Model):
  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, 
               target_vocab_size, pe_input, pe_target, rate=0.1,**kwargs,):
    super(Transformer, self).__init__(**kwargs)

    self.encoder = Encoder(num_layers, d_model, num_heads, dff, 
                           input_vocab_size, pe_input, rate)

    self.decoder = Decoder(num_layers, d_model, num_heads, dff, 
                           target_vocab_size, pe_target, rate)

    self.final_layer = tf.keras.layers.Dense(target_vocab_size)
  def get_config(self):

        config = super().get_config().copy()
        config.update({
            'dff':self.dff,
            'input_vocab_size':self.input_vocab_size,
            'target_vocab_size':self.target_vocab_size,
            'pe_input':self.pe_input,
            'pe_target':self.pe_target,
            #'vocab_size': self.vocab_size,
            'num_layers': self.num_layers,
            #'units': self.units,
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'rate': self.rate,
        })
        return config

  def call(self, inp, tar, training, enc_padding_mask, 
           look_ahead_mask, dec_padding_mask):

    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)

    # dec_output.shape == (batch_size, tar_seq_len, d_model)
    dec_output, attention_weights = self.decoder(
        tar, enc_output, training, look_ahead_mask, dec_padding_mask)

    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)
    #    return final_output, attention_weights


    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)
</code></pre>

<p>and creating the model, compiling it, and fitting it as follows:</p>

<pre><code>transformer = Transformer(num_layers, d_model, num_heads, dff,
                          input_vocab_size, target_vocab_size, 
                          pe_input=input_vocab_size, 
                          pe_target=target_vocab_size,
                          rate=dropout_rate)

transformer.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])

transformer.fit(dataset, epochs=EPOCHS)
</code></pre>

<p>EDIT: Bases on @Geeocode updated the def function in transformer class to be:</p>

<pre><code>def call(self, inp, tar, enc_padding_mask,look_ahead_mask, dec_padding_mask, training=False,):

    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)

    # dec_output.shape == (batch_size, tar_seq_len, d_model)
    dec_output, attention_weights = self.decoder(
        tar, enc_output, training, look_ahead_mask, dec_padding_mask)

    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)
    return final_output, attention_weights
</code></pre>

<p>However, I still get the same error</p>
","transformer-model"
"59796343","Transformer model not able to be saved","2020-01-18 00:06:59","","1","3064","<python><tensorflow><keras><neural-network><transformer-model>","<p>I'm trying to follow this tutrial <a href=""https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/transformer_chatbot.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/tensorflow/examples/blob/master/community/en/transformer_chatbot.ipynb</a>, However, when I tried to save the model in order to load it again without training I got an error mentioned here <a href=""https://stackoverflow.com/questions/58678836/notimplementederror-layers-with-arguments-in-init-must-override-get-conf"">NotImplementedError: Layers with arguments in `__init__` must override `get_config`</a>
I understood from the answer that I need to make the encoder and decoder as classes and customise it(instead of leaving it as functions like the colab tutrial) so I went back to tensor flow documentation of this model here: <a href=""https://www.tensorflow.org/tutorials/text/transformer#encoder_layer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/transformer#encoder_layer</a> and tried to edit in it. I made the encoder layer as:</p>

<pre><code>class EncoderLayer(tf.keras.layers.Layer):
  def __init__(self, d_model, num_heads,  rate=0.1,**kwargs,):
    #super(EncoderLayer, self).__init__()
    super().__init__(**kwargs)
    self.mha = MultiHeadAttention(d_model, num_heads)
    self.ffn = point_wise_feed_forward_network(d_model, dff)

    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

    self.dropout1 = tf.keras.layers.Dropout(rate)
    self.dropout2 = tf.keras.layers.Dropout(rate)
  def get_config(self):

        config = super().get_config().copy()
        config.update({
            #'vocab_size': self.vocab_size,
            #'num_layers': self.num_layers,
            #'units': self.units,
            'd_model': self.d_model,
            'num_heads': self.num_heads,
            'dropout': self.dropout,
        })
        return config

  def call(self, x, training, mask):

    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)
    attn_output = self.dropout1(attn_output, training=training)
    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)

    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)
    ffn_output = self.dropout2(ffn_output, training=training)
    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)

    return out2
</code></pre>

<p>and same for the decoder layer class. Then the same encoder in the documentation of tf</p>

<pre><code>class Encoder(tf.keras.layers.Layer):
  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,
               maximum_position_encoding, rate=0.1):
    super(Encoder, self).__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
    self.pos_encoding = positional_encoding(maximum_position_encoding, 
                                            self.d_model)


    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) 
                       for _ in range(num_layers)]

    self.dropout = tf.keras.layers.Dropout(rate)

  def call(self, x, training, mask):

    seq_len = tf.shape(x)[1]

    # adding embedding and position encoding.
    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x += self.pos_encoding[:, :seq_len, :]

    x = self.dropout(x, training=training)

    for i in range(self.num_layers):
      x = self.enc_layers[i](x, training, mask)

    return x  # (batch_size, input_seq_len, d_model)
</code></pre>

<p>the function of the model as:</p>

<pre><code>def transformer(vocab_size,
                num_layers,
                units,
                d_model,
                num_heads,
                dropout,
                name=""transformer""):
  inputs = tf.keras.Input(shape=(None,), name=""inputs"")
  dec_inputs = tf.keras.Input(shape=(None,), name=""dec_inputs"")

  enc_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='enc_padding_mask')(inputs)
  # mask the future tokens for decoder inputs at the 1st attention block
  look_ahead_mask = tf.keras.layers.Lambda(
      create_look_ahead_mask,
      output_shape=(1, None, None),
      name='look_ahead_mask')(dec_inputs)
  # mask the encoder outputs for the 2nd attention block
  dec_padding_mask = tf.keras.layers.Lambda(
      create_padding_mask, output_shape=(1, 1, None),
      name='dec_padding_mask')(inputs)

  enc_outputs = Encoder(
      num_layers=num_layers, d_model=d_model, num_heads=num_heads, 
                         input_vocab_size=vocab_size,


  )(inputs=[inputs, enc_padding_mask])

  dec_outputs = Decoder(
      num_layers=num_layers, d_model=d_model, num_heads=num_heads, 
                          target_vocab_size=vocab_size,


  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])

  outputs = tf.keras.layers.Dense(units=vocab_size, name=""outputs"")(dec_outputs)

  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)
</code></pre>

<p>and calling the model:</p>

<pre><code>#the model itself with its paramters:
# Hyper-parameters
NUM_LAYERS = 3
D_MODEL = 256
#D_MODEL=tf.cast(D_MODEL, tf.float32)

NUM_HEADS = 8
UNITS = 512
DROPOUT = 0.1
model = transformer(
    vocab_size=VOCAB_SIZE,
    num_layers=NUM_LAYERS,
    units=UNITS,
    d_model=D_MODEL,
    num_heads=NUM_HEADS,
    dropout=DROPOUT)
</code></pre>

<p>However, I got that error:
<code>TypeError: __init__() missing 2 required positional arguments: 'dff' and 'maximum_position_encoding'</code>
I am really confused and I don't understand what dff and maximum position encoding mean in the documentation and when I removed them from the encoder and decoder classes, I got anther error as positional_encoding function takes maximum position as input and also dff is passed as input inside the class. I am not so sure what I should do as I am not sure whether I am following the right steps or not</p>
","transformer-model"
"59646954","What is attention penalty in speech transformer paper? (updated)","2020-01-08 13:29:18","59714855","0","206","<tensorflow><deep-learning><speech-recognition><tf.keras><transformer-model>","<p>github: <a href=""https://github.com/sephiroce/tfsr/tree/exprimental"" rel=""nofollow noreferrer"">https://github.com/sephiroce/tfsr/tree/exprimental</a></p>

<p>I'm trying to reproduce recognition accuracies described in the speech transformer paper [1]. 
The attention penalty is a technique I could not fully understand. 
This is the description of the attention penalty in the paper.</p>

<p>""In addition, we encouraged the model attending to closer positions by adding 
 bigger penalty on the attention weights of more distant position-pairs.""</p>

<p>I understood as it means adding smaller negative values for more away from the diagonal on scaled attention logits (before masking) except for the first multi-head attention in decoders.</p>

<p>This is a code snippet for computing attention weights.</p>

<pre><code>  # Q * trans(K): (..., seq_len_q, seq_len_k)
  matmul_qk = tf.matmul(query, key, transpose_b=True)

  # scaled matmul_qk: ( Q * trans(K) ) / sqrt(d_k)
  dimension_of_key = tf.cast(tf.shape(key)[-1], tf.float32)
  scaled_attention_logits = matmul_qk / tf.math.sqrt(dimension_of_key)

  # add the mask to the scaled tensor
  if mask is not None:
    scaled_attention_logits += (mask * -1e9)

  # softmax is normalized on the last axis (seq_len_k) so that the scores
  # add up to 1.
  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)

  # Adding penalty to attention weights and linearly re-normalize it.
  if attention_penalty is not None and att_penalty_scale &gt; 0:
    attention_weights += (attention_penalty * att_penalty_scale)
    attention_weights += tf.math.abs(tf.math.reduce_min(attention_weights))
    inv_sum = 1 / tf.math.reduce_sum(attention_weights, axis=-1)
    attention_weights = tf.einsum('ijlm,ijl-&gt;ijlm', attention_weights, inv_sum)
</code></pre>

<p>The source code snippet below is for creating an attention penalty matrix.
I could not find any efficient way to create an attention penalty matrix for the second multi-head attention weights in decoders since the attention maps are not diagonal. Thus first I am trying to apply the attention penalty to encoders.
The source code assigns linearly bigger penalties for more distant elements from diagonal.<br>
There are two hyper-parameters such as an attention_penalty_scale (this is similar to <code>penalty_values</code> which Jindřich suggested) and a width of the diagonal line.<br>
I might be able to add an option such as <code>stripe_step_size</code>. Currently <code>stripe_step_size</code> can be interpreted as 1.</p>

<pre><code>def create_attention_penalty(inp_len, tar_len, num_heads, attention_penalty_width):
  max_inp_len = tf.cast(tf.math.reduce_max(inp_len), tf.int32)
  n_batch = tf.shape(inp_len)[0]

  enc_att_penalty = tf.ones([n_batch, num_heads, max_inp_len, max_inp_len])

  accum = tf.zeros(([n_batch, num_heads, max_inp_len, max_inp_len]))
  for i in range(attention_penalty_width - 1, max_inp_len - 1):
    accum += tf.linalg.band_part(enc_att_penalty, i, i, name=None) - 1

  enc_att_penalty = accum

  return enc_att_penalty, None
</code></pre>

<p>Even though I implemented as I understand, I could not gain any accuracy improvement. And there is another down-side of this implementation. The training speed was getting slower.</p>

<p>Q) How to efficiently apply this attention penalty method for square and non-square attention weights?</p>

<p>Reference<br>
[1] Linhao Dong, Shuang Xu, Bo Xu, Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition, ICASSP 2018, <a href=""https://ieeexplore.ieee.org/document/8462506"" rel=""nofollow noreferrer"">https://ieeexplore.ieee.org/document/8462506</a></p>
","transformer-model"
"59602891","What is the training data input to the transformers (attention is all you need)?","2020-01-05 18:20:41","59614924","0","224","<deep-learning><transformer-model>","<p>Sorry I'm new to NLP. Please bear with me. Say I have two sentences: </p>

<p>French: Le chat mange.</p>

<p>English: The cat eats.</p>

<p>In the following text, I will denote a training data as a tuple <code>(x, y)</code>, where <code>x</code> is the input data and <code>y</code> is the annotation.</p>

<p>When I train a transformer network, do I A. input these two sentences synchronously as training data, i.e. <code>(Le chat mange, The cat eats)</code>? Or do I B. use
<code>((Le chat mange, ), The), ((Le chat mange, The), cat), ((Le chat mange, The cat), eats)</code> as training data?</p>

<p>If it's A, sounds like I have to wait for the network to produce the words one by one during training, which would not be parallelizable. So I guess it should be B? </p>
","transformer-model"
"59511029","How can you resolve module imports in a Typescript transformer","2019-12-28 12:48:25","","1","183","<typescript><import><module><transformer-model>","<p>I'm writing a Typescript transformer that needs to follow import declarations to understand what the import is. I'm having some trouble getting it working though.</p>

<p>Using <code>ts-loader</code> with the option <code>transpileOnly: true</code> ends up having the <code>resolvedModules</code> property on the <code>SourceFile</code> be undefined. If I set <code>transpileOnly: false</code> it is defined - however that brings other issues.</p>

<p>Is there any <em>official</em> way of resolving/visiting module imports? If you have any best practices or other handy hints for writing transformers I'm all ears as well :).</p>

<p>Cheers</p>
","transformer-model"
"59486629","BERT binary Textclassification get different results every run","2019-12-26 09:49:54","","1","1393","<python-3.x><text-classification><transformer-model><bert-language-model>","<p>I do binary text classification with BERT from the <a href=""https://github.com/ThilinaRajapakse/simpletransformers"" rel=""nofollow noreferrer"">Simpletransformer</a>.</p>

<p>I work in Colab with GPU runtime type. </p>

<p>I have generated train and test set with the sklearn StratifiedKFold Method. I have two files with the dictionaries containing my folds.</p>

<p>I run my classification in the following while loop:</p>

<pre><code>from sklearn.metrics import matthews_corrcoef, f1_score
import sklearn

counter = 0

resultatos = []

while counter != len(trainfolds):

    model = ClassificationModel('bert', 'bert-base-multilingual-cased',args={'num_train_epochs': 4, 'learning_rate': 1e-5, 'fp16': False, 
                                                                             'max_seq_length': 160, 'train_batch_size': 24,'eval_batch_size': 24 , 
                                                                             'warmup_ratio': 0.0,'weight_decay': 0.00, 
                                                                             'overwrite_output_dir': True})

    print(""start with fold_{}"".format(counter))
    trainfolds[""{}_fold"".format(counter)].to_csv(""/content/data/train.tsv"", sep=""\t"", index = False, header=False)
    print(""{}_fold Train als train.tsv exportiert"". format(counter))
    testfolds[""{}_fold"".format(counter)].to_csv(""/content/data/dev.tsv"", sep=""\t"", index = False, header=False)
    print(""{}_fold test als train.tsv exportiert"". format(counter))

    train_df =  pd.read_csv(""/content/data/train.tsv"", delimiter='\t', header=None)
    eval_df = df = pd.read_csv(""/content/data/dev.tsv"", delimiter='\t', header=None)

    train_df = pd.DataFrame({
    'text': train_df[3].replace(r'\n', ' ', regex=True),
    'label':train_df[1]})

    eval_df = pd.DataFrame({
    'text': eval_df[3].replace(r'\n', ' ', regex=True),
    'label':eval_df[1]})

    model.train_model(train_df)

    result, model_outputs, wrong_predictions = model.eval_model(eval_df, f1 = sklearn.metrics.f1_score)
    print(result)

    resultatos.append(result)

    shutil.rmtree(""outputs"")
    shutil.rmtree(""cache_dir"")
    #shutil.rmtree(""runs"")



    counter += 1
</code></pre>

<p>And i get different Results Running this code for the same Folds:</p>

<p>Here for example the F1 Scores for two runs:</p>

<pre><code>0.6237942122186495
0.6189111747851003
0.6172839506172839
0.632183908045977
0.6182965299684542
0.5942492012779553
0.6025641025641025
0.6153846153846154
0.6390532544378699
0.6627906976744187
The F1 Score is: 0.6224511646974427


0.6064516129032258
0.6282420749279539
0.6402439024390244
0.5971014492753622
0.6135693215339232
0.6191950464396285
0.6382978723404256
0.6388059701492537
0.6097560975609756
0.5956112852664576
The F1 Score is: 0.618727463283623

</code></pre>

<p>How can they be that diffeerent for the same folds?</p>

<p>What i tried already is give a fixed Random seed right before my loop starts:</p>

<pre><code>random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
torch.cuda.manual_seed_all(42)
</code></pre>

<p>I came up with approach of having the Model initialized in the loop because, when its outside the loop, it somehow remembers what it has learned - that means after the 2nd fold I get f1 score of almost one - despite the fact that i delete the cache..</p>
","transformer-model"
"59465885","Huggingface Transformers - AttributeError: 'MrpcProcessor' object has no attribute 'tfds_map'","2019-12-24 08:21:35","","2","2065","<transformer-model><bert-language-model><huggingface-transformers>","<p>When using Hugginface Transformers on GLUE task, I've got the error <code>AttributeError: 'MrpcProcessor' object has no attribute 'tfds_map'</code></p>

<p>I suspect a problem of compatibility.</p>
","transformer-model"
"59443408","How to improve the Tensorflow official transformer accuracy?","2019-12-22 10:26:31","","1","162","<tensorflow><transformer-model>","<p>every one:</p>

<p>I just run the <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">transformer in Tensorflow tutorial</a> with Google Colaboratory. After 20 epoch training, I got the model performance in :</p>

<pre><code>Epoch 20 Batch 700 Loss 0.5509 Accuracy 0.3449
Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4
Epoch 20 Loss 0.5510 Accuracy 0.3449
Time taken for 1 epoch: 63.83616662025452 secs
</code></pre>

<p>Why is the accuracy so low ? And how can I improve it ?</p>
","transformer-model"
"59435020","Get probability of multi-token word in MASK position","2019-12-21 09:24:46","59520904","12","4946","<python><pytorch><transformer-model><bert-language-model><huggingface-transformers>","<p>It is relatively easy to get a token's probability according to a language model, as the snippet below shows. You can get the output of a model, restrict yourself to the output of the masked token, and then find the probability of your requested token in the output vector. However, this only works with single-token words, e.g. words that are themselves in the tokenizer's vocabulary. When a word does not exist in the vocabulary, the tokenizer will chunk it up into pieces that it <em>does</em> know (see the bottom of the example). But since the input sentence consists of only one masked position, and the requested token has more tokens than that, how can we get its probability? Ultimately I am looking for a solution that works regardless of the number of subword units a word has.</p>

<p>In the code below I have added many comments explaining what is going on, as well as printing out the given output of print statements. You'll see that predicting tokens such as 'love' and 'hate' is straightforward because they are in the tokenizer's vocabulary. 'reprimand' is not, though, so it cannot be predicted in a single masked position - it consists of three subword units. So how can we predict 'reprimand' in the masked position?</p>

<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForMaskedLM
import torch

# init model and tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
model.eval()
# init softmax to get probabilities later on
sm = torch.nn.Softmax(dim=0)
torch.set_grad_enabled(False)

# set sentence with MASK token, convert to token_ids
sentence = f""I {tokenizer.mask_token} you""
token_ids = tokenizer.encode(sentence, return_tensors='pt')
print(token_ids)
# tensor([[ 101, 1045,  103, 2017,  102]])
# get the position of the masked token
masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero().item()

# forward
output = model(token_ids)
last_hidden_state = output[0].squeeze(0)
# only get output for masked token
# output is the size of the vocabulary
mask_hidden_state = last_hidden_state[masked_position]
# convert to probabilities (softmax)
# giving a probability for each item in the vocabulary
probs = sm(mask_hidden_state)

# get probability of token 'hate'
hate_id = tokenizer.convert_tokens_to_ids('hate')
print('hate probability', probs[hate_id].item())
# hate probability 0.008057191967964172

# get probability of token 'love'
love_id = tokenizer.convert_tokens_to_ids('love')
print('love probability', probs[love_id].item())
# love probability 0.6704086065292358

# get probability of token 'reprimand' (?)
reprimand_id = tokenizer.convert_tokens_to_ids('reprimand')
# reprimand is not in the vocabulary, so it needs to be split into subword units
print(tokenizer.convert_ids_to_tokens(reprimand_id))
# [UNK]

reprimand_id = tokenizer.encode('reprimand', add_special_tokens=False)
print(tokenizer.convert_ids_to_tokens(reprimand_id))
# ['rep', '##rim', '##and']
# but how do we now get the probability of a multi-token word in a single-token position?
</code></pre>
","transformer-model"
"59413468","How to use the pretrained transformer model (""en_trf_bertbaseuncased_lg"") in SpaCy?","2019-12-19 16:19:51","63722492","2","4685","<python><spacy><transformer-model>","<p>I was wondering, how I could use the pretrained transformer model <code>en_trf_bertbaseuncased_lg</code> from spacy for future NLP tasks (NER, POS, etc.). The documentation states, that the module can only be used for the following pipeline preprocessing modules (<a href=""https://spacy.io/models/en#en_trf_bertbaseuncased_lg"" rel=""nofollow noreferrer"">https://spacy.io/models/en#en_trf_bertbaseuncased_lg</a>): </p>

<ul>
<li>sentencizer  </li>
<li>trf_wordpiecer  </li>
<li>trf_tok2vec</li>
</ul>

<p>Can anyone explain to me, what these components are doing and in which tasks they can be used? Or does anyone know a good sources to read about it?</p>

<pre><code>&gt;&gt;&gt; import spacy
&gt;&gt;&gt; nlp = spacy.load(""en_trf_bertbaseuncased_lg"")
&gt;&gt;&gt; nlp.pipe_names
[sentencizer, trf_wordpiecer, trf_tok2vec]
</code></pre>
","transformer-model"
"59384146","Why do Transformers in Natural Language Processing need a stack of encoders?","2019-12-18 00:57:26","59390297","9","2652","<machine-learning><deep-learning><nlp><transformer-model>","<p>I am following this blog on transformers</p>

<p><a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">http://jalammar.github.io/illustrated-transformer/</a></p>

<p>The only thing I don't understand is why there needs to be a stack of encoders or decoders. I understand that the multi-headed attention layers capture different representation spaces of the problem. I don't understand why there needs to be a vertical stack of encoders and decoders. Wouldn't one encoder/decoder layer work?</p>
","transformer-model"
"59315138","How to get words from output of XLNet using Transformers library","2019-12-13 01:58:57","60255556","2","1201","<nlp><masking><transformer-model><language-model><huggingface-transformers>","<p>I am using Hugging Face's Transformer library to work with different NLP models. Following code does masking with XLNet. It outputs a tensor with numbers. How do I convert the output to words again?  </p>

<pre><code>import torch
from transformers import XLNetModel,  XLNetTokenizer, XLNetLMHeadModel

tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')
model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')

# We show how to setup inputs to predict a next token using a bi-directional context.
input_ids = torch.tensor(tokenizer.encode(""I went to &lt;mask&gt; York and saw the &lt;mask&gt; &lt;mask&gt; building."")).unsqueeze(0)  # We will predict the masked token
print(input_ids)

perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)
perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token

target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  # Shape [1, 1, seq_length] =&gt; let's predict one token
target_mapping[0, 0, -1] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)

outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)
next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]
</code></pre>

<p>The current output I get is: </p>

<p>tensor([[[ -5.1466, -17.3758, -17.3392,  ..., -12.2839, -12.6421, -12.4505]]],
       grad_fn=AddBackward0)</p>
","transformer-model"
"59300484","[Tensorflow 1.x]multi_label classification using BERT, Invalid argument: assertion failed: [`predictions` contains negative values]","2019-12-12 08:33:05","","1","611","<python><tensorflow><tensorflow-estimator><transformer-model><sigmoid>","<p>I am trying to fine tune BERT for multi-label classification.I have my own data processor,and using a pretrained BERT.I add a finetuning layer at end of pretrained BERT for my task.
I have a <code>create model</code> function that adds a finetuning layer at end of existing BERT.</p>

<pre><code>
def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,
                 labels, num_labels, use_one_hot_embeddings):
    """"""Creates a classification model.""""""
    model = modeling.BertModel(
        config=bert_config,
        is_training=is_training,
        input_ids=input_ids,
        input_mask=input_mask,
        token_type_ids=segment_ids,
        use_one_hot_embeddings=use_one_hot_embeddings)

    # In the demo, we are doing a simple classification task on the entire
    # segment.
    #
    # If you want to use the token-level output, use model.get_sequence_output()
    # instead.
    output_layer = model.get_pooled_output()

    hidden_size = output_layer.shape[-1].value

    output_weights = tf.get_variable(
        ""output_weights"", [num_labels, hidden_size],
        initializer=tf.truncated_normal_initializer(stddev=0.02))

    output_bias = tf.get_variable(
        ""output_bias"", [num_labels], initializer=tf.zeros_initializer())

    with tf.variable_scope(""loss""):
        if is_training:
            # I.e., 0.1 dropout
            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)

        logits = tf.matmul(output_layer, output_weights, transpose_b=True)
        logits = tf.nn.bias_add(logits, output_bias)

        # print(""labels:"",labels,"";logits:"",logits,""isinstance(labels,list):"",isinstance(labels,list))
        # mulit-label classification: 1.multi-hot==&gt; then use sigmoid to transform it to possibility
        probabilities = tf.nn.sigmoid(logits)
        # log_probs=tf.log(probabilities)
        labels = tf.cast(labels, tf.float32)
        #  below is for single label classification
        #  one-hot for single label classification
        #  probabilities = tf.nn.softmax(logits, axis=-1)
        # log_probs = tf.nn.log_softmax(logits, axis=-1)
        #  one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)

        tf.logging.debug(""num_labels = %s; logits = %s; labels = %s"" % (num_labels, logits, labels))
        # print(""log_probs:"",log_probs)
        # per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1) # 利用交叉熵就和
        per_example_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)

        loss = tf.reduce_mean(per_example_loss)
        return (loss, per_example_loss, logits, probabilities)

</code></pre>

<p>I use it inside model_fn_builder, where the logits returned are used in the estimator</p>

<pre><code>
def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,
                     num_train_steps, num_warmup_steps, use_tpu,
                     use_one_hot_embeddings):
    """"""Returns `model_fn` closure for TPUEstimator.""""""

    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument
        """"""The `model_fn` for TPUEstimator.""""""

        tf.logging.info(""*** Features ***"")
        for name in sorted(features.keys()):
            tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))

        input_ids = features[""input_ids""]
        input_mask = features[""input_mask""]
        segment_ids = features[""segment_ids""]
        label_ids = features[""label_ids""]
        is_real_example = None
        if ""is_real_example"" in features:
            is_real_example = tf.cast(features[""is_real_example""], dtype=tf.float32)
        else:
            is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)

        is_training = (mode == tf.estimator.ModeKeys.TRAIN)

        (total_loss, per_example_loss, logits, probabilities) = create_model(
            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,
            num_labels, use_one_hot_embeddings)

        tvars = tf.trainable_variables()
        initialized_variable_names = {}
        scaffold_fn = None
        if init_checkpoint:
            (assignment_map, initialized_variable_names
             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)
            if use_tpu:

                def tpu_scaffold():
                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
                    return tf.train.Scaffold()

                scaffold_fn = tpu_scaffold
            else:
                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)

        tf.logging.info(""**** Trainable Variables ****"")
        for var in tvars:
            init_string = """"
            if var.name in initialized_variable_names:
                init_string = "", *INIT_FROM_CKPT*""
            tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,
                            init_string)

        output_spec = None
        if mode == tf.estimator.ModeKeys.TRAIN:

            train_op = optimization.create_optimizer(
                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)
            logging_hook = tf.train.LoggingTensorHook({""loss"": total_loss,'precision:': t_precision,'recall:': t_recall}, every_n_iter=10)
            output_spec = tf.contrib.tpu.TPUEstimatorSpec(
                mode=mode,
                loss=total_loss,
                train_op=train_op,
                training_hooks=[logging_hook],
                scaffold_fn=scaffold_fn)

        elif mode == tf.estimator.ModeKeys.EVAL:

            def metric_fn(per_example_loss, label_ids, logits, is_real_example):
                # print(""###metric_fn.logits:"",logits.shape) # (?,80)
                # predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)
                # print(""###metric_fn.label_ids:"",label_ids.shape,"";predictions:"",predictions.shape) # label_ids: (?,80);predictions:(?,)
                print(logits)
                logits_split = tf.split(logits, args.num_classes,
                                        axis=-1)  # a list. length is num_classes
                label_ids_split = tf.split(logits, args.num_classes,
                                           axis=-1)  # a list. length is num_classes
                accuracy = tf.constant(0.0, dtype=tf.float64)

                for j, logits in enumerate(logits_split):  #
                    #  accuracy = tf.metrics.accuracy(label_ids, predictions)

                    label_id_ = tf.cast(label_ids_split[j], dtype=tf.int32)
                    current_auc, update_op_auc = tf.metrics.auc(label_id_, logits)
                    # TP = tf.count_nonzero(logits * label_id_)
                    # TN = tf.count_nonzero((logits - 1) * (label_id_ - 1))
                    # FP = tf.count_nonzero(logits * (label_id_ - 1))
                    # FN = tf.count_nonzero((logits - 1) * label_id_)
                    # current_precision,update_op_precision = tf.metrics.Precision(label_id_, logits)
                    # current_recall,update_op_recall = tf.metrics.Recall(label_id_, logits)
                    prec,prec_op=precision(label_id_,logits)
                    rec,rec_op=recall(label_id_,logits)
                    f_1=f1(label_id_,logits)

                eval_loss = tf.metrics.mean(values=per_example_loss)

                return {
                    ""eval_precision"":(prec,prec_op),
                    ""eval_recall"" : (rec_op,rec_op),
                    ""eval_auc"" : (current_auc, update_op_auc),
                    ""eval_loss"": eval_loss,
                }

            eval_metrics = (metric_fn,
                            [per_example_loss, label_ids, logits, is_real_example])
            output_spec = tf.contrib.tpu.TPUEstimatorSpec(
                mode=mode,
                loss=total_loss,
                eval_metrics=eval_metrics,
                scaffold_fn=scaffold_fn)
        else:
            output_spec = tf.contrib.tpu.TPUEstimatorSpec(
                mode=mode,
                predictions={""probabilities"": probabilities},
                scaffold_fn=scaffold_fn)
        return output_spec

    return model_fn


</code></pre>

<p>In my <code>model_fn</code> when estimator is in eval mode, I use the logits to calculate various metrics defined in <code>metric_fn</code> (defined inside <code>model_fn_builder</code>)</p>

<p>I am getting the following error in traceback:</p>

<pre><code>
ERROR:tensorflow:Error recorded from evaluation_loop: 2 root error(s) found.
  (0) Invalid argument: assertion failed: [`predictions` contains negative values] [Condition x &gt;= 0 did not holdelement-wise:] [x (Reshape:0) = ] [0 -1 -2...]
         [[node confusion_matrix/assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert (defined at /home/aditya_vartak/virtualenvs/anaconda3/envs/tf1/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
         [[confusion_matrix_2/ones_like/_1429]]
  (1) Invalid argument: assertion failed: [`predictions` contains negative values] [Condition x &gt;= 0 did not holdelement-wise:] [x (Reshape:0) = ] [0 -1 -2...]
         [[node confusion_matrix/assert_non_negative_1/assert_less_equal/Assert/AssertGuard/Assert (defined at /home/aditya_vartak/virtualenvs/anaconda3/envs/tf1/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1748) ]]
0 successful operations.
0 derived errors ignored.
</code></pre>

<p>I understand error is due to negative values in logits. My question is Why? and what is the workaround?</p>

<p><strong>Edit 1</strong>: If question is vague, I want to add that I did apply sigmoid activation on the weighted sum of the last layer of pretrained BERT with shape [hidden_dimension,num_classes] , the outputs are stored in <code>probablities</code>, following which applied <code>sigmoid_cross_entropy_with_logits</code>.(as showin in <code>create_model()</code>). According to <a href=""https://www.tensorflow.org/api_docs/python/tf/math/sigmoid"" rel=""nofollow noreferrer"">docs</a> it returns value between 0,1 for each input. So how do probablities get negative value?I feel the problem is in <code>metric_fn()</code>. but not understanding what exactly it is</p>
","transformer-model"
"59222579","squad2.0 training error: THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected","2019-12-07 02:51:21","","4","1712","<python><tensorflow><transformer-model><language-model>","<pre><code>!python -m torch.distributed.launch --nproc_per_node=8 /root/examples/run_squad.py \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file /root/DATA/train-v2.0.json \
    --predict_file /root/DATA/dev-v2.0.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ../root/result/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   \
</code></pre>

<p>I'm using google colab and I want to training my A&amp;Q dataset which downloaded from SQuad website.
But when I run the code above it return me an error.</p>

<p>Can some one help me fix this problem?The full error msg as following and I'll appreciate any suggestions:</p>

<p>this is error msg:
    [THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
        main()
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
      File ""/root/examples/run_squad.py"", line 469, in main
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch.cuda.set_device(args.local_rank)
        torch._C._cuda_setDevice(device)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/root/examples/run_squad.py"", line 575, in 
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
      File ""/root/examples/run_squad.py"", line 469, in main
        torch._C._cuda_init()
        main()
        torch.cuda.set_device(args.local_rank)
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
    Traceback (most recent call last):
      File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
        ""<strong>main</strong>"", mod_spec)
      File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
        exec(code, run_globals)
      File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py"", line 253, in 
        main()
      File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py"", line 249, in main
        cmd=cmd)
    subprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', '/root/examples/run_squad.py', '--local_rank=7', '--model_type', 'bert', '--model_name_or_path', 'bert-large-uncased-whole-word-masking', '--do_train', '--do_eval', '--do_lower_case', '--train_file', '/root/DATA/train-v2.0.json', '--predict_file', '/root/DATA/dev-v2.0.json', '--learning_rate', '3e-5', '--num_train_epochs', '2', '--max_seq_length', '384', '--doc_stride', '128', '--output_dir', '../root/result/', '--per_gpu_eval_batch_size=3', '--per_gpu_train_batch_size=3']' returned non-zero exit status 1.]</p>

<p><a href=""https://i.sstatic.net/DJJrX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DJJrX.png"" alt=""enter image description here""></a></p>
","transformer-model"
"59191144","How do attention network works?","2019-12-05 08:38:44","","4","130","<text><nlp><transformer-model><attention-model>","<p>Recently I was going through Attention is all you need paper, ongoing through it I found an issue regarding understanding the attention network if I ignore the maths behind it.
Can anyone make me understand the attention network with an example?</p>
","transformer-model"
"59080727","Using BERT in domain-specific corpus","2019-11-28 01:54:30","","3","216","<machine-learning><nlp><data-science><transformer-model>","<p>I am planning to use BERT in my domain-specific corpus. Should I retrain BERT from scratch (pre-training) or can I do fine-tuning instead? How can I add my new vocabulary if I need to do fine-tuning? Thanks!</p>
","transformer-model"
"59070740","Java Transformer converts Chinese character to ASCII value","2019-11-27 12:54:56","59071801","0","532","<java><ascii><transformer-model>","<p>Ok after lot of search I decided to ask question here. Below is the sample code to reproduce my problem. The document object is build with chinese character.</p>

<pre><code>String value= ""𧀠"";
DocumentBuilder builder = DocumentBuilderFactory.newInstance().newDocumentBuilder();
Document doc = builder.newDocument();
Element root = doc.createElement(""value"");      
root.setAttribute(""attribute"", value);
doc.appendChild(root);      
DOMSource source = new DOMSource(doc);  
</code></pre>

<p>I am trying to convert the document source to string using the Transformer class with the below code.</p>

<pre><code>ByteArrayOutputStream outStream = null;
Transformer transformer = TransformerFactory.newInstance().newTransformer();
StreamResult htmlStreamResult = new StreamResult( new ByteArrayOutputStream() );        
transformer.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");        
transformer.transform(source, htmlStreamResult);                    
outStream = (ByteArrayOutputStream) htmlStreamResult.getOutputStream();
String outPut = outStream.toString( ""UTF-8"" );
</code></pre>

<p>But I got output with converted Chinese characters as below. </p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;&lt;value attribute=""&amp;#159776;""/&gt;
</code></pre>

<p>I do not want the Chinese character to be converted but to be displayed as it is. Appreciate if anyone help me on this. </p>
","transformer-model"
"59024005","how to debug tf2 in general and transformer in particular","2019-11-25 01:06:20","","1","288","<tensorflow2.0><transformer-model>","<p>Looking for advice about a debugger for a TF2 model.
I would like to train the following transformer model
<a href=""https://github.com/tensorflow/models/blob/master/official/transformer/v2"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/official/transformer/v2</a>
on my own data.
My problem is that I have difficulty figuring out shape of data returned by the _parse_example function from the <a href=""https://github.com/tensorflow/models/blob/master/official/transformer/v2/data_pipeline.py"" rel=""nofollow noreferrer"">data_pipeline.py</a> file. To start with <a href=""https://www.pydev.org/"" rel=""nofollow noreferrer"">Pydev</a> doesn't brake inside _parse_example function, neither does <a href=""https://www.jetbrains.com/pycharm/"" rel=""nofollow noreferrer"">PyCharm</a>, which appears to be using <a href=""https://www.pydev.org/"" rel=""nofollow noreferrer"">Pydev</a> internally. The options offered by TensorBoard 2.0.0 seem to be applicable to TF1, not TF2:</p>

<blockquote>
  <ol>
  <li>sess = tf.Session()
  sess = tf_debug.TensorBoardDebugWrapperSession(sess, ""localhost:6064"")
  sess.run(my_fetches)</li>
  <li>hook = tf_debug.TensorBoardDebugHook(""localhost:6064"")
  my_estimator.fit(x=x_data, y=y_data, steps=1000, monitors=[hook])</li>
  <li>keras.backend.set_session(
  tf_debug.TensorBoardDebugWrapperSession(tf.Session(), ""localhost:6064""))
  model.fit(...)</li>
  </ol>
</blockquote>

<p>So what tool can I use to see tensor's data and its shape? Option 2 from the above list seems to make sense, except for the fact that I don't see a call for my_estimator.fit from transformer's implementation for TF2.</p>

<p>My environment was created in Ubuntu 18.04 using anaconda: 
conda create -n mytest tensorflow-gpu.
I use Eclipse with PyDev plugin.</p>

<p>Thanks.</p>
","transformer-model"
"58999876","Nested If in DataStage Transformer","2019-11-22 18:29:04","","0","1616","<if-statement><nested><datastage><transformer-model>","<p>I am using below nested if condition in transformer. How ever it is giving error. Can some one assist me to resolve the error: </p>

<p>IF  IsNotNull(lnkReadHabsClob.NUM_57_A)
THEN 
     ( IF  LEN57A &lt; 2 THEN Trim(lnkReadHabsClob.BENEFICIARY_FI_SKEY_57A)
       ELSE IF LEN57A > 2 THEN Trim(lnkReadHabsClob.BENEFICIARY_FI_SKEY_57A[LEN57A,11] ) )
ELSE 
( IF IsNotNull(lnkReadHabsClob.NUM_58_A) AND LEN58A &lt; 2 THEN TRIM(lnkReadHabsClob.BENEFICIARY_FI_SKEY_58A) 
  ELSE IF IsNotNull(lnkReadHabsClob.NUM_58_A) AND LEN58A > 2 THEN TRIM(lnkReadHabsClob.BENEFICIARY_FI_SKEY_58A[LEN58A,11]) )
  ELSE lnkReadHabsClob.BENEFICIARY_FI_SKEY_58A)</p>
","transformer-model"
"58991927","Can the HuggingFace GPT2DoubleHeadsModel be used for non-multiple-choice next token prediction?","2019-11-22 10:08:44","","2","526","<nlp><huggingface-transformers><transformer-model><gpt-2>","<p>According to the HuggingFace Transformer's website (<a href=""https://huggingface.co/transformers/model_doc/gpt2.html#gpt2doubleheadsmodel"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/gpt2.html#gpt2doubleheadsmodel</a>), <strong>GPT2DoubleHeadsModel</strong> (NOT <strong>GPT2LMHeadModel</strong> but <strong>GPT2DoubleHeadsModel</strong>) is the GPT-2 transformer model with a language modelling and a multiple-choice classification head on top e.g. for RocStories/SWAG tasks.</p>

<p>Does this mean that we can use the <strong>GPT2DoubleHeadsModel</strong> to process both non-multiple-choice-based language modelling tasks (i.e. next word prediction) as well as the multiple-choice questions, without making any adjustment to its head? Or would I need to adjust the head of the <strong>GPT2DoubleHeadsModel</strong> if I want to do the non-multiple-choice-based next word predictions because the <strong>GPT2DoubleHeadsModel</strong> is for answering multiple-choice type questions only?</p>

<p>I am a bit confused by this because the impression that I got from reading your GPT-2 paper is that GPT-2 uses language modelling process to process every type of language task (therefore GPT-2 would only have the regular language modelling head at the top), yet the name ""<strong>GPT2DoubleHeadsModel</strong>"" seem to suggest that I need to adjust the head of this GPT-2 for different types of language tasks.</p>

<p>Thank you,</p>
","transformer-model"
"58963513","TypeError: Cannot convert provided value to EagerTensor. Provided value: 0.0 Requested dtype: int64","2019-11-20 21:16:54","","-1","2154","<tensorflow><transformer-model>","<p>I am trying to train the transformer model available from the tensorflow official models. I am able to train in cpu without any error but when I try gpu I get the following error:</p>

<pre><code>models/official/transformer/v2/transformer.py:143 call  *
    encoder_outputs = self.encode(inputs, attention_bias, training)

models/official/transformer/v2/transformer.py:166 encode
    embedded_inputs = self.embedding_softmax_layer(inputs)

TypeError: Cannot convert provided value to EagerTensor. Provided value: 0.0 Requested dtype: int64
</code></pre>

<p>I tried tf.cast but it doesn't seem to help. </p>
","transformer-model"
"58947679","No gradients provided for any variable in tensorflow2.0","2019-11-20 06:00:02","58949183","3","1422","<tensorflow><gradient><transformer-model>","<p>I met a problem when I tried to use <code>tensorflow2.0</code> to create a transformer based on the official guidelines posted by the <code>TensorFlow</code> and when I add a full connected net it seems that both the classification loss and the translate loss as gradients on some of the variables. </p>

<p>But once I try to add the two loss the gradients to all variables disappear. I have no idea and I tried to figure to solved the problem for weeks. Could anyone give me some suggestions?</p>

<pre><code>@tf.function(input_signature=train_step_signature)
def train_step(group, inp, tar, label):
    tar_inp = tar[:, :-1]
    tar_real = tar[:, 1:]  # sess=tf.compat.v1.Session()
    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)
    with tf.GradientTape(persistent=True) as tape:
        classfication, predictions, _ = transformer(inp, tar_inp,
                                                    True,
                                                    enc_padding_mask,
                                                    combined_mask,
                                                    dec_padding_mask)
        loss = loss_function(tar_real, predictions)
        loss2 = tf.nn.softmax_cross_entropy_with_logits(label, classfication)

    #print(loss,loss2)
    a=tape.gradient(loss,trainsformer.trainable_variable)
    gradients = tape.gradient(loss+loss2, transformer.trainable_variables)

    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
    class_loss(loss2)
    train_loss(loss)
    train_accuracy(tar_real, predictions)
</code></pre>

<p>below is my error infomation</p>

<pre><code>    ValueError                                Traceback (most recent call last)
&lt;ipython-input-2-81054f0385cb&gt; in &lt;module&gt;()
    999     # inp -&gt; portuguese, tar -&gt; english
   1000     for (batch, (group, inp, tar, label)) in enumerate(train_dataset):
-&gt; 1001         train_step(group, inp, tar, label)
   1002         if batch % 50 == 0:
   1003             print(

8 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    455 
    456     tracing_count = self._get_tracing_count()
--&gt; 457     result = self._call(*args, **kwds)
    458     if tracing_count == self._get_tracing_count():
    459       self._call_counter.called_without_tracing()

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _call(self, *args, **kwds)
    501       # This is the first call of __call__, so we have to initialize.
    502       initializer_map = object_identity.ObjectIdentityDictionary()
--&gt; 503       self._initialize(args, kwds, add_initializers_to=initializer_map)
    504     finally:
    505       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    406     self._concrete_stateful_fn = (
    407         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 408             *args, **kwds))
    409 
    410     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1846     if self.input_signature:
   1847       args, kwargs = None, None
-&gt; 1848     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1849     return graph_function
   1850 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   2148         graph_function = self._function_cache.primary.get(cache_key, None)
   2149         if graph_function is None:
-&gt; 2150           graph_function = self._create_graph_function(args, kwargs)
   2151           self._function_cache.primary[cache_key] = graph_function
   2152         return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   2039             arg_names=arg_names,
   2040             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 2041             capture_by_value=self._capture_by_value),
   2042         self._function_attributes,
   2043         # Tell the ConcreteFunction to clean up its graph once it goes out of

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    913                                           converted_func)
    914 
--&gt; 915       func_outputs = python_func(*func_args, **func_kwargs)
    916 
    917       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    356         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    357         # the function a weak reference to itself to avoid a reference cycle.
--&gt; 358         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    359     weak_wrapped_fn = weakref.ref(wrapped_fn)
    360 

/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py in wrapper(*args, **kwargs)
    903           except Exception as e:  # pylint:disable=broad-except
    904             if hasattr(e, ""ag_error_metadata""):
--&gt; 905               raise e.ag_error_metadata.to_exception(e)
    906             else:
    907               raise

ValueError: in converted code:

    &lt;ipython-input-1-81054f0385cb&gt;:856 train_step  *
        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:427 apply_gradients
        grads_and_vars = _filter_grads(grads_and_vars)
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:1025 _filter_grads
        ([v.name for _, v in grads_and_vars],))

    ValueError: No gradients provided for any variable: ['transformer_1/encoder_1/embedding_2/embeddings:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_98/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_98/bias:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_99/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_99/bias:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_100/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_100/bias:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_101/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/multi_head_attention_18/dense_101/bias:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_102/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_102/bias:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_103/kernel:0', 'transformer_1/encoder_1/encoder_layer_6/sequential_12/dense_103/bias:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_30/gamma:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_30/beta:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_31/gamma:0', 'transformer_1/encoder_1/encoder_layer_6/layer_normalization_31/beta:0', 'transformer_1/encoder_1/encoder_layer_7/multi_head_attention_19/dense_104/kernel:0', 'transformer_1/encoder_1/encoder...
</code></pre>
","transformer-model"
"58826124","How can I move a JsArray to the root with Json Transformers within a Json object?","2019-11-12 20:22:41","58826768","0","41","<json><scala><playframework><transformer-model>","<p>How can I move a <code>JsArray</code> to the root with <a href=""https://www.playframework.com/documentation/2.7.x/ScalaJsonTransformers"" rel=""nofollow noreferrer"">Json Transformers</a>? Imagine a Json structure like so:</p>

<pre><code>   {
     ""key1"" : ""value1"",
     ""key2"" : {
       ""key21"" : ""catface"",
       ""key22"" : true,
       ""key23"" : [
           {
              ""key231"": ""alpha"", 
              ""key232"": ""beta"", 
              ""key232"": ""gamma""
           },
           ...
       ],
       ""key24"" : 234
    }
</code></pre>

<p>If I wanted to move <code>key21</code> to the root then I would use - <code>__.json.copyFrom((__ \ 'key2 \ 'key21).json.pick)</code> but if I wanted to move <code>key23</code> to the root; i.e. </p>

<pre><code>   {
     ""key23"" : [
       {
          ""key231"": ""alpha"", 
          ""key232"": ""beta"", 
          ""key232"": ""gamma""
       },
       ...
     ]
   }
</code></pre>

<p>but <code>__.json.copyFrom((__ \ 'key2 \ 'key23).json.pick)</code> causes the error:</p>

<blockquote>
  <p>A server errors occurred: when empty JsPath, expecting JsObject</p>
</blockquote>

<p>So then I try <code>__.json.copyFrom((__ \ 'key2 \ 'key23).json.pick[JsArray])</code> or <code>__.json.copyFrom[JsArray]((__ \ 'key2 \ 'key23).json.pick[JsArray])</code> - but the same error </p>

<p>I couldn't find anything out there that addresses this specifically but I am hoping this is an easy one for someone. Many thanks</p>
","transformer-model"
"58823325","Python Pipeline Custom Transformer","2019-11-12 16:58:42","58823700","0","196","<python><pipeline><transformer-model>","<p>I am trying to code a custom transformer to be used in a pipeline to pre-process data.</p>

<p>Here is the code I'm using (sourced - not written by me).  It takes in a dataframe, scales the features, and returns a dataframe:</p>

<pre><code>class DFStandardScaler(BaseEstimator,TransformerMixin):

    def __init__(self):

        self.ss = None

    def fit(self,X,y=None):

        self.ss = StandardScaler().fit(X)
        return self

    def transform(self, X):

        Xss = self.ss.transform(X)
        Xscaled = pd.DataFrame(Xss, index=X.index, columns=X.columns)
        return Xscaled
</code></pre>

<p>I have data that has both categorical and continuous features.  Obviously the transformer will not transform the categorical feature ('sex').  When I fit this pipeline with the dataframe below it throws an error because it is trying to scale the categorical labels in 'sex':</p>

<pre><code>     sex  length  diameter  height  whole_weight  shucked_weight  \
0      M   0.455     0.365   0.095        0.5140          0.2245   
1      M   0.350     0.265   0.090        0.2255          0.0995   
2      F   0.530     0.420   0.135        0.6770          0.2565   
3      M   0.440     0.365   0.125        0.5160          0.2155   
4      I   0.330     0.255   0.080        0.2050          0.0895   
5      I   0.425     0.300   0.095        0.3515          0.1410   
</code></pre>

<p>How do I pass a list of categorical / continuous features into the transformer so it will scale the proper features?  Or is it better to somehow code the feature type check inside the transformer?</p>
","transformer-model"
"58760179","<?xml version=""1.0"" encoding=""cp850"" standalone=""no""?> become <?xml version=""1.0"" encoding=""850"" standalone=""no""?> when using jdk1.8.0","2019-11-08 04:00:57","","1","167","<java><xml><encoding><java-8><transformer-model>","<p>In jdk1.6.0, ""cp850"" can show in the xml declaration </p>

<pre><code>&lt;?xml version=""1.0"" encoding=""cp850"" standalone=""no""?&gt;
</code></pre>

<p>however when i use jdk1.8.0, the encoding type become ""850"", without ""cp""</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""850"" standalone=""no""?&gt;
</code></pre>

<p>How can I make the encoding become ""cp850"" when using jdk1.8.0?</p>

<p>I have tried to hardcode the xml declaration. Although the encoding become ""cp850"", it seems not is the right ways. </p>

<pre><code>transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""yes"");  
writer.write(""&lt;?xml version=\""1.0\"" encoding=\""""+encoding+""\"" standalone=\""no\""?&gt;""); 
</code></pre>

<p>I also compared two java version library and found that both of them have rt.jar. 
In Java SE 8, <a href=""https://docs.oracle.com/javase/8/docs/technotes/guides/intl/encoding.doc.html"" rel=""nofollow noreferrer"">https://docs.oracle.com/javase/8/docs/technotes/guides/intl/encoding.doc.html</a> Cp850 have a new Alias or Aliases named ""850"" is this related?</p>

<p>Original Code</p>

<pre><code> String encoding = ""cp850"";
 transformer.setOutputProperty(OutputKeys.ENCODING, encoding);
</code></pre>

<p>Thanks in advance, Neko</p>
","transformer-model"
"58695954","Discard blanks in datastage","2019-11-04 14:58:07","","0","412","<datastage><identity-column><transformer-model><blank-line>","<p>I have a Datastage job which takes the data from a file to a Dataset and for a column I would like to make a transformation in order to exclude the rows where that columns has no value:
For example I use in the transfomer the following rule where I put 0 everytime I find no value in the column lcvInstalmentOriginalStr, but I need this row to be discarded from the begining.</p>

<p>If lcvInstalmentOriginalStr &lt;> """" Then StringToDecimal(lcvInstalmentOriginalStr)  Else 0</p>

<p>Thank you</p>
","transformer-model"
"58668528","Max Sequence length in Seq2Seq - Attention is all you need","2019-11-02 05:34:14","","3","1074","<nlp><transformer-model><seq2seq><attention-model>","<p>I have gone through the paper <a href=""http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""nofollow noreferrer"">Attention is all you need</a> and though I think I understood the overall idea behind what is happening, I am pretty confused with the way the input is being processed. Here are my doubts, and for simplicity, let's assume that we are talking about a Language translation task.</p>

<p>1) The paper states that the input embedding is of dimension 512, that would be the embedding vector of each word in the input sentence right? So if the input sentence is of length 25, then the input would be a 25*512 dimension matrix at each layer?</p>

<p>2) Does this model use a fixed ""MAX_LENGTH"" across all its batches? By this, I mean identify the longest sentence in your training set and pad all the other sentences to be equal to the MAX_LENGTH?</p>

<p>3) If the 2nd question does indeed use a concept of MAX_LENGTH, how does one process a test time query of length greater than the input query?</p>

<p>I have also referred to this video to get a better understanding <a href=""https://www.youtube.com/watch?v=z1xs9jdZnuY"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=z1xs9jdZnuY</a> and one of the frames that gives an overall idea of one single layer with 3 multi head attentions is this
<a href=""https://i.sstatic.net/Mx3HW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Mx3HW.png"" alt=""enter image description here""></a></p>

<p>here you can see that the input is of dimension 4*3(for simple representation the embedding size is 3 and the final output of one layer of attention and the Feed forward network is also 4*3). </p>
","transformer-model"
"58655207","PyTorch: Different Forward Methods for Train and Test/Validation","2019-11-01 06:48:54","58660175","5","7213","<python-3.x><neural-network><pytorch><transformer-model><seq2seq>","<p>I'm currently trying to extend <a href=""https://github.com/microsoft/MASS"" rel=""noreferrer"">a model</a> that is based on FairSeq/PyTorch. During training I need to train two encoders: one with the target sample, and the original one with the source sample.</p>

<p>So the current forward function looks like this:</p>

<pre><code>def forward(self, src_tokens=None, src_lengths=None, prev_output_tokens=None, **kwargs):
    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)
    decoder_out = self.decoder(prev_output_tokens, encoder_out=encoder_out, **kwargs)
    return decoder_out
</code></pre>

<p>And based on this <a href=""https://github.com/golsun/SpaceFusion"" rel=""noreferrer"">this idea</a> i want something like this:</p>

<pre><code>def forward_test(self, src_tokens=None, src_lengths=None, prev_output_tokens=None, **kwargs):
    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)
    decoder_out = self.decoder(prev_output_tokens, encoder_out=encoder_out, **kwargs)
    return decoder_out

def forward_train(self, src_tokens=None, src_lengths=None, prev_output_tokens=None, **kwargs):
    encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)
    autoencoder_out = self.encoder(tgt_tokens, src_lengths=src_lengths, **kwargs)
    concat = some_concatination_func(encoder_out, autoencoder_out)
    decoder_out = self.decoder(prev_output_tokens, encoder_out=concat, **kwargs)
    return decoder_out
</code></pre>

<p>Is there any way to do this? </p>

<p>Edit:
These are the constraints that I have, since I need to extend <em>FairseqEncoderDecoderModel</em>:</p>

<pre><code>@register_model('transformer_mass')
class TransformerMASSModel(FairseqEncoderDecoderModel):
    def __init__(self, encoder, decoder):
        super().__init__(encoder, decoder) 
</code></pre>

<p>Edit 2:
The parameters passed to the forward function in Fairseq can be altered by implementing your own Criterion, see for example <a href=""https://github.com/pytorch/fairseq/blob/master/fairseq/criterions/cross_entropy.py#L28"" rel=""noreferrer"">CrossEntropyCriterion</a>, where <code>sample['net_input']</code> is passed to the <code>__call__</code> function of the model, which invokes the <code>forward</code> method.</p>
","transformer-model"
"58562087","What is the opposite of middleware in Laravel?","2019-10-25 16:05:07","","1","764","<php><laravel><laravel-5><transformer-model>","<p>Middleware in Laravel can be used to add app wide logic (or specific to specific routes or group of routes) <em>before</em> any application/business logic is applied. I want to do the same, but <em>after</em> all the application/business logic is done. What I love about middleware is that it centralizes the place where said logic is applied. Is there a way to do that at the end of the request/response lifecycle?</p>

<p>One option is using <a href=""https://fractal.thephpleague.com/transformers/"" rel=""nofollow noreferrer"">transformers</a>, but I don't find it as clean as middleware for some reason (maybe because it's done by a third party?)</p>

<p>sample use case: I want to have a group of endpoints always return values in an alternate currency rather than USD only when such requests are made from a certain type of shoppers from a certain geographical area (which I already know). So I will need to perform business logic, and then right before I send the json response back, I want to ""hijack"" said response and replace all USD values with another currency of my choosing. </p>

<p>Ideas? (I'm using Laravel 5.5)</p>
","transformer-model"
"58532911","Why is the input size of the MultiheadAttention in Pytorch Transformer module 1536?","2019-10-24 01:28:33","58558888","2","4462","<pytorch><tensor><transformer-model><attention-model><huggingface-transformers>","<p>When using the <code>torch.nn.modules.transformer.Transformer</code> module/object, the first layer is the <code>encoder.layers.0.self_attn</code> layer that is a <code>MultiheadAttention</code> layer, i.e. </p>

<pre><code>from torch.nn.modules.transformer import Transformer
bumblebee = Transformer()

bumblee.parameters
</code></pre>

<p>[out]:</p>

<pre><code>&lt;bound method Module.parameters of Transformer(
  (encoder): TransformerEncoder(
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (linear1): Linear(in_features=512, out_features=2048, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (linear2): Linear(in_features=2048, out_features=512, bias=True)
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
      )
</code></pre>

<p>And if we print out the size of the layer, we see:</p>

<pre><code>for name in bumblebee.encoder.state_dict():
    print(name, '\t', bumblebee.encoder.state_dict()[name].shape)
</code></pre>

<p>[out]:</p>

<pre><code>layers.0.self_attn.in_proj_weight    torch.Size([1536, 512])
layers.0.self_attn.in_proj_bias      torch.Size([1536])
layers.0.self_attn.out_proj.weight   torch.Size([512, 512])
layers.0.self_attn.out_proj.bias     torch.Size([512])
layers.0.linear1.weight      torch.Size([2048, 512])
layers.0.linear1.bias    torch.Size([2048])
layers.0.linear2.weight      torch.Size([512, 2048])
layers.0.linear2.bias    torch.Size([512])
layers.0.norm1.weight    torch.Size([512])
layers.0.norm1.bias      torch.Size([512])
layers.0.norm2.weight    torch.Size([512])
layers.0.norm2.bias      torch.Size([512])
</code></pre>

<p>It seems like 1536 is 512 * 3 and somehow the <code>layers.0.self_attn.in_proj_weight</code> parameter might be storing all three QKV tensors in the transformer architecture in one matrix. </p>

<p>From <a href=""https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py#L649"" rel=""nofollow noreferrer"">https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/activation.py#L649</a> </p>

<pre><code>class MultiheadAttention(Module):
    def __init__(self, embed_dim, num_heads, dropout=0., bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None):
        super(MultiheadAttention, self).__init__()
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self._qkv_same_embed_dim = self.kdim == embed_dim and self.vdim == embed_dim

        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, ""embed_dim must be divisible by num_heads""

        if self._qkv_same_embed_dim is False:
            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))
            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))
            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))
        else:
            self.in_proj_weight = Parameter(torch.empty(3 * embed_dim, embed_dim))
</code></pre>

<p>And the note in the docstring of the <code>MultiheadAttention</code> says:</p>

<blockquote>
  <p>Note: if kdim and vdim are None, they will be set to embed_dim such that
  query, key, and value have the same number of features.</p>
</blockquote>

<p>Is that correct? </p>
","transformer-model"
"58518528","How to change the position of transformer rotation icon","2019-10-23 08:22:43","58526096","3","1534","<konvajs><transformer-model><react-konva>","<p>I've started react-konva. And I am trying to use Transfomer.</p>

<p>For now rotate handler is top-center and I want to place it to bottom-center, left or other side. How can I do that?</p>

<p><a href=""https://konvajs.org/docs/select_and_transform/Transformer_Styling.html"" rel=""nofollow noreferrer"">https://konvajs.org/docs/select_and_transform/Transformer_Styling.html</a></p>
","transformer-model"
"58346657","Can I customize the dictionary of a pre-trained transformer neural machine translation model?","2019-10-11 18:25:05","","0","424","<tensorflow><nlp><pytorch><transformer-model><machine-translation>","<p>There are many pre-trained machine translations models available, but it seems like they all need to be run with the dictionary they are trained with. The dictionaries sometimes can have less coverage for my data set (even BPE based ones), and sometimes miss important words as unknowns. What are the best ways to customize the pretrained models to a dictionary learned from my own data set? For example, some way to transfer learn, like unfreezing the encoder layers and retraining? </p>
","transformer-model"
"58346592","How to train a simple, vanilla transformers translation model from scratch with Fairseq","2019-10-11 18:19:39","","2","1083","<nlp><pytorch><transformer-model><machine-translation><seq2seq>","<p>I have been familiarizing myself with the fairseq library recently, and have tried a couple of pretrained models. I thought that a good way to teach myself would be to train a plain vanilla transformers model with the data I have, and then I can modify and maybe add bells and whistles like pre-training from there. The fairseq documentation has an example of this with fconv architecture, and I basically would like to do the same with transformers.</p>

<p>Below is the code I tried:</p>

<p>In data preparation, I cleaned the data with moses script, tokenized words, and then applied BPE using subword-nmt, where I set number of BPE tokens to 15000.</p>

<p>For preprocessing:</p>

<pre><code>fairseq-preprocess --source-lang zh --target-lang en \
    --trainpref data/train --validpref data/valid --testpref data/test \
    --joined-dictionary \
    --destdir data-bin \
    --workers 20
</code></pre>

<p>For training:</p>

<pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3

fairseq-train data-bin \
    --clip-norm 0.1 --dropout 0.2 --max-tokens 2048 \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
    --lr 5e-4 --lr-scheduler inverse_sqrt \
    --criterion label_smoothed_cross_entropy \
    --lazy-load \
    --update-freq 4 \
    --keep-interval-updates 100 --save-interval-updates 3000  --log-interval 50 \
    --arch transformer --save-dir checkpoints/transformer
</code></pre>

<p>I trained this on a data set of ~19M samples, on 4 NVIDIA P100 GPUs, for about 8 hours -- at that point I had completed 1 epoch and a bit more. I tested this against my checkpoints -- for the first checkpoint at update 3000, the prediction was all ""the the the""s -- but that might be ok because it was just the first checkpoint. However, I then tested this against the last checkpoint, and the prediction was the same sentence for all test samples!! -- The prediction was ""committee on the peaceful uses of outer space"" for everything, and the BLEU score was 0. My test set is not at all about outer space. </p>

<p>So after this extremely disappointing result, I realized that I should ask for some pointers on creating a basic transformers model:</p>

<ul>
<li><p>First of all, is my result actually within expectation? The paper on which transformer.py is based, Jointly Learning to Align and Translate, stated that state of the art results are achieved on 64 Volta GPUs for 30k updates (!!!) -- my set up was much smaller, so maybe the result was expected? However, I have achieved better results in less time with less data, so I doubt that. Is it just that the learning rate was not set right so that it got stuck in some weird local minima? Or are there more things wrong with my setup above?</p></li>
<li><p>When would the above model stop? max_epoch and max_update are not required parameters and are set to math.inf when not given. From train.py, it looks like training goes on until learning rate gets to below args.min_lr, however I can't find where min_lr is set, and it is not a parameter in the documentation, so what is min_lr? Is it 0?</p></li>
<li><p>What is the best architecture to use for the ""vanilla"" transformer model that I'm looking for? </p></li>
</ul>

<p>Thank you!</p>
","transformer-model"
"58334235","What is currently the best way to add a custom dictionary to a neural machine translator that uses the transformer architecture?","2019-10-11 04:24:24","58379239","0","497","<neural-network><nlp><transformer-model><machine-translation><seq2seq>","<p>It's common to add a custom dictionary to a machine translator to ensure that terminology from a specific domain is correctly translated. For example, the term server should be translated differently when the document is about data centers, vs when the document is about restaurants.</p>

<p>With a transformer model, this is not very obvious to do, since words are not aligned 1:1. I've seen a couple of papers on this topic, but I'm not sure which would be the best one to use. What are the best practices for this problem?</p>
","transformer-model"
"58305969","Training neural network with own dataset doesn't work due to cache strange problem","2019-10-09 14:03:24","","0","95","<python><caching><neural-network><transformer-model><transfer-learning>","<p>I faced a strange challenge trying to train neural network using code from github, it is huggingface conversational model.</p>

<p>What happens: even i use my own dataset for training result remains the same like with original dataset. My hypothesis that it is a somehow cache problem - old dataset continuously get loaded from cached and replace my.
Them when i launch actual interactive session with neural network it works, but without my data, even if i pass model checkpoint.</p>

<p>Why i think of cache: in this repo author use automatic downloading and caching neural network model in /home/joo/.cache/torch/pytorch_transformers/ if no parameter specified in terminal.</p>

<p><em>I have created an issue on Github. BUT i am not sure is that a problem specific for this repo only, or it is a common problem with retraining neural networks i faced first time.</em></p>

<p><a href=""https://github.com/huggingface/transfer-learning-conv-ai/issues/36"" rel=""nofollow noreferrer"">https://github.com/huggingface/transfer-learning-conv-ai/issues/36</a></p>

<p>Some copypaste from issue:</p>

<blockquote>
  <p>I am still curious, was not able to pass my dataset:</p>

<pre><code>I added to original 200mb json my personality
trained once more with --dataset_path ./my.json
invoke interact.py with new checkpoint and path python ./interact.py --model_checkpoint
</code></pre>
  
  <p>./runs/Oct08_18-22-53_joo-tf_openai-gpt/ --dataset_path ./my.json
      and it reports Gathered 18878 personalities (but not 18879, with my own).
      I changed the code in interact.py to choose my first perosnality this way</p>
  
  <p>was: personality = random.choice(personalities)</p>
  
  <p>become: personality = personalities[0]</p>
  
  <p>and this first personality is not mine.</p>
</blockquote>
","transformer-model"
"58158947","What are the input and the output of the Transformer?","2019-09-29 20:29:08","","1","583","<python><tensorflow><nlp><transformer-model>","<p>I have questions about google implementation of the Transformer <a href=""https://www.tensorflow.org/beta/tutorials/text/transformer"" rel=""nofollow noreferrer"">here</a>.</p>

<ul>
<li><p>In <code>train_step(input, tar)</code> function: the <code>inp</code> dimension is a 256*40 tensor and the transformer returns a 256*39*8089 tensor. Is each row in <code>inp</code> a sentence?  I expected Transformer to take a batch of sentences (a batch_size of 2D matrix in which each row is a word) and calculate attention weights and outputs at once and then pass them to decoder (see <a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">here</a>. ). However, I cannot see that being implemented in the code.  </p></li>
<li><p>In <code>train_step(input, tar)</code> function: the ""predictions"" is a 256*39*8089 tensor. Is it [batch size, max number of words in a sentence, target vocab size]? How does loss_function calculate loss while this format is different from ```tar_real`` which is [256 * 39]? </p></li>
<li><p>In <code>def evaluate(inp_sentence)</code>: Why in each iteration it sends the Transformer the entire encoder input? What I expect is that the encoder calculates attention weights and output once and then inside the for loop we send the output of the attentions and the predictions so far.</p></li>
</ul>

<p>Thank you</p>
","transformer-model"
"58149533","How to read and write XML files and treat the comment nodes as text nodes in Java when saving","2019-09-28 19:16:23","","1","200","<java><xml><dom><transformer-model>","<p>I'm reading an XML file in Java retrieved from an external system, then processing it and eventually save it locally and deploy it back.</p>

<p>The external system gives me an XML file that contains this node:</p>

<pre class=""lang-xml prettyprint-override""><code>    &lt;customApplications&gt;
        &lt;label&gt;&lt;!-- GDPR Management --&gt;&lt;/label&gt;
        &lt;name&gt;GDPR_Management&lt;/name&gt;
    &lt;/customApplications&gt;
</code></pre>

<p>The problem is the comment node. 
When I read the file and then just save it, the result looks like this:</p>

<pre class=""lang-xml prettyprint-override""><code>    &lt;customApplications&gt;
        &lt;label&gt;
            &lt;!-- GDPR Management --&gt;
        &lt;/label&gt;
        &lt;name&gt;GDPR_Management&lt;/name&gt;
    &lt;/customApplications&gt;
</code></pre>

<p>Which is a problem, because when I deploy the file back to the external system, it now thinks that the label has some text content. So I need the same result as it was, i.e. without the line breaks around the comment node.</p>

<p>I tried to remove all the comment nodes, which works well when deploying the file, but the file is also versioned using git and it produces many merge conflict as the file can be at any time retrieved again from the external system (the retrieved file is again with the comment nodes as you can see in the first example).</p>

<p>Then I tried to change all the comment nodes to text nodes before saving. The result is again not acceptable, because the label again has some text content:</p>

<pre class=""lang-xml prettyprint-override""><code>    &lt;customApplications&gt;
        &lt;label&gt;&amp;lt;!--  GDPR Management  --&amp;gt;&lt;/label&gt;
        &lt;name&gt;GDPR_Management&lt;/name&gt;
    &lt;/customApplications&gt;
</code></pre>

<p>How I read the document:</p>

<pre class=""lang-java prettyprint-override""><code>var docBuilder = DocumentBuilderFactory.newInstance().newDocumentBuilder();
var document = docBuilder.parse(inputStream);
document.getDocumentElement().normalize();
var xp = XPathFactory.newInstance().newXPath();
var nl = (NodeList) xp.evaluate(""//text()[normalize-space(.)='']"", document, XPathConstants.NODESET);
for (int i = 0; i &lt; nl.getLength(); ++i) {
    var node = nl.item(i);
    node.getParentNode().removeChild(node);
}
</code></pre>

<p>How I save the document:</p>

<pre class=""lang-java prettyprint-override""><code>var result = new StreamResult(outputStream);
var transformer = TransformerFactory.newInstance().newTransformer();
transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""no"");
transformer.setOutputProperty(OutputKeys.VERSION, ""1.0"");
transformer.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");
transformer.transform(new DOMSource(document), result);
</code></pre>

<p>I really need the same result as the first example, but I do not care about how the comment node will be represented in the dom when processing the file.</p>

<p>Thanks for any tips!</p>
","transformer-model"
"58127059","How to understand masked multi-head attention in transformer","2019-09-27 02:40:48","","32","30083","<tensorflow><deep-learning><transformer-model><attention-model>","<p>I'm currently studying code of transformer, but I can not understand the masked multi-head of decoder. The paper said that it is to prevent you from seeing the generating word, but I can not unserstand if the words after generating word have not been generated, how can them be seen? </p>

<p>I try to read the code of transformer (link:<a href=""https://github.com/Kyubyong/transformer"" rel=""noreferrer"">https://github.com/Kyubyong/transformer</a>). The code achieved mask is shown below. It uses the lower triangular matrix to mask, I can not understand why.</p>

<pre class=""lang-py prettyprint-override""><code>padding_num = -2 ** 32 + 1
diag_vals = tf.ones_like(inputs[0, :, :])  # (T_q, T_k)
tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  # (T_q, T_k)
masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])  # (N, T_q, T_k)
paddings = tf.ones_like(masks) * padding_num
outputs = tf.where(tf.equal(masks, 0), paddings, inputs)
</code></pre>
","transformer-model"
"58123393","How to use Transformers for text classification?","2019-09-26 19:18:15","","11","13907","<tensorflow><nlp><transformer-model><bert-language-model>","<p>I have two questions about how to use Tensorflow implementation of the Transformers for text classifications. </p>

<ul>
<li><strong>First</strong>, it seems people mostly used only the encoder layer to do the text classification task. However, encoder layer generates one prediction for each input word. Based on my understanding of transformers, the input to the encoder each time is one word from the input sentence. Then, the attention weights and the output is calculated using the current input word. And we can repeat this process for all of the words in the input sentence. As a result we'll end up with pairs of (attention weights, outputs) for each word in the input sentence. Is that correct? Then how would you use this pairs to perform a text classification?  </li>
<li><strong>Second</strong>, based on the Tensorflow implementation of transformer <a href=""https://www.tensorflow.org/beta/tutorials/text/transformer"" rel=""noreferrer"">here</a>, they embed the whole input sentence to one vector and feed a batch of these vectors to the Transformer. However, I expected the input to be a batch of words instead of sentences based on what I've learned from <a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">The Illustrated Transformer</a></li>
</ul>

<p>Thank you!</p>
","transformer-model"
"58092004","How to do sequence classification with pytorch nn.Transformer?","2019-09-25 06:02:53","","4","6569","<machine-learning><deep-learning><pytorch><text-classification><transformer-model>","<p>I am doing a sequence classification task using <code>nn.TransformerEncoder()</code>. Whose pipeline is similar to <code>nn.LSTM()</code>.</p>

<p>I have tried several temporal features fusion methods:</p>

<ol>
<li><p>Selecting the final outputs as the representation of the whole sequence.</p></li>
<li><p>Using an affine transformation to fuse these features.</p></li>
<li><p>Classifying the sequence frame by frame, and then select the max values to be the category of the whole sequence.</p></li>
</ol>

<p>But, all these 3 methods got a terrible accuracy, only <strong>25%</strong> for 4 categories classification. While using nn.LSTM with the last hidden state, I can achieve <strong>83%</strong> accuracy easily. I tried plenty of hyperparameters of <code>nn.TransformerEncoder()</code>, but without any improvement for the accuracy.</p>

<p>I have no idea about how to adjust this model now. Could you give me some practical advice? Thanks.</p>

<p>For <code>LSTM</code>: the <code>forward()</code> is:</p>

<pre class=""lang-py prettyprint-override""><code>    def forward(self, x_in, x_lengths, apply_softmax=False):

        # Embed
        x_in = self.embeddings(x_in)

        # Feed into RNN
        out, h_n = self.LSTM(x_in) #shape of out: T*N*D

        # Gather the last relevant hidden state
        out = out[-1,:,:] # N*D

        # FC layers
        z = self.dropout(out)
        z = self.fc1(z)
        z = self.dropout(z)
        y_pred = self.fc2(z)

        if apply_softmax:
            y_pred = F.softmax(y_pred, dim=1)
        return y_pred
</code></pre>

<p>For <code>transformer</code>:</p>

<pre class=""lang-py prettyprint-override""><code>    def forward(self, x_in, x_lengths, apply_softmax=False):

        # Embed
        x_in = self.embeddings(x_in)

        # Feed into RNN
        out = self.transformer(x_in)#shape of out T*N*D

        # Gather the last relevant hidden state
        out = out[-1,:,:] # N*D

        # FC layers
        z = self.dropout(out)
        z = self.fc1(z)
        z = self.dropout(z)
        y_pred = self.fc2(z)

        if apply_softmax:
            y_pred = F.softmax(y_pred, dim=1)
        return y_pred
</code></pre>
","transformer-model"
"58010126","Pyspark string array of dynamic length in dataframe column to onehot-encoded","2019-09-19 11:36:29","","1","1577","<pyspark><apache-spark-sql><transformer-model>","<p>I would like to convert a column which contains strings like:</p>

<pre><code> [""ABC"",""def"",""ghi""] 
 [""Jkl"",""ABC"",""def""]
 [""Xyz"",""ABC""]
</code></pre>

<p>Into a encoded column like this:</p>

<pre><code> [1,1,1,0,0]
 [1,1,0,1,0]
 [0,1,0,0,1]
</code></pre>

<p>Is there a class for that in pyspark.ml.feature?</p>

<p>Edit: In the encoded column the first entry always corresponds to the value ""ABC"" etc. 1 means ""ABC"" is present while 0 means it is not present in the corresponding row.</p>
","transformer-model"
"58007391","Attention Text Generation in Character-by-Character fashion","2019-09-19 09:01:32","58007592","1","610","<neural-network><nlp><pytorch><transformer-model><attention-model>","<p>I am searching the web for a couple of days for any <strong>text generation</strong> model that would use only attention mechanisms.</p>

<p>The <strong>Transformer</strong> architecture that made waves in the context of <strong>Seq-to-Seq</strong> models is actually based solely on <strong>Attention</strong> mechanisms but is mainly designed and used for translation or chat bot tasks so it doesn't fit to the purpose, but the principle does.</p>

<p>My question is:</p>

<p>Does anyone knows or heard of a text generation model <strong>based solely on Attention without any recurrence</strong>?</p>

<p>Thanks a lot!</p>

<p>P.S. I'm familiar with <strong>PyTorch</strong>.</p>
","transformer-model"
"57986783","embedding layer outputs nan","2019-09-18 06:59:52","","5","4151","<deep-learning><pytorch><transformer-model><seq2seq>","<p>I am trying to learn a seq2seq model.
An embedding layer is located in the encoder and it sometimes outputs nan value after some iterations.
I cannot identify the reason.
How can I solve this??
The problem is the first emb_layer in the forward function in the code below.</p>

<pre><code>
class TransformerEncoder(nn.Module):
    def __init__(self, vocab_size, hidden_size=1024, num_layers=6, dropout=0.2, input_pad=1, batch_first=False, embedder=None, init_weight=0.1):
        super(TransformerEncoder, self).__init__()
        self.input_pad = input_pad
        self.vocab_size = vocab_size
        self.num_layers = num_layers
        self.embedder = embedder

        if embedder is not None:
            self.emb_layer = embedder
        else:
            self.emb_layer = nn.Embedding(vocab_size, hidden_size, padding_idx=1)

        self.positional_encoder = PositionalEncoder()
        self.transformer_layers = nn.ModuleList()
        for _ in range(num_layers):
            self.transformer_layers.append(
                    TransformerEncoderBlock(num_heads=8, embedding_dim=1024, dropout=dropout))

    def set_mask(self, inputs):
        self.input_mask = (inputs == self.input_pad).unsqueeze(1)

    def forward(self, inputs):
        x = self.emb_layer(inputs)
        x = self.positional_encoder(x)
</code></pre>
","transformer-model"
"57614588","InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:Mul] name: mul/","2019-08-22 17:43:41","","2","8669","<tensorflow><transformer-model>","<p>I instantiate a class and get some errors when passing in parameters.saying that the data type received not the expected type</p>

<p>i try to convert the data to the type it requires but get the same error</p>

<pre><code>class PositionalEmbedding(tf.keras.Model):
def __init__(self, d_model, dropout, max_len=5000):
  super(PositionalEmbedding,self).__init__()
  self.dropout = tf.keras.layers.Dropout(dropout,dtype = 'float32')

  pe = tf.zeros([max_len,d_model],dtype = 'float32')
  position = tf.expand_dims(tf.range(max_len),axis = 1)
</code></pre>

<p>pe = PositionalEmbedding(20,0)</p>

<p>InvalidArgumentError                      Traceback (most recent call last)
 in ()
      1 plt.figure(figsize=(15,5))
      2 
----> 3 pe = PositionalEmbedding(20,0)
      4 
      5 s = pe(tf.Variable(tf.zeros(1,100,20)))</p>

<p>4 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)</p>

<p>InvalidArgumentError: cannot compute Mul as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:Mul] name: mul/</p>
","transformer-model"
"57604290","How to Convert Days into Years Months and Days in datastage?","2019-08-22 07:37:36","","0","1161","<date><datediff><datastage><date-difference><transformer-model>","<p>I need to calculate age in form of year , Month and date format in Datastage transformer </p>

<pre><code>Example       : SERVICE_DT = 02 Mar 1990
                Current date= 22 Aug 2019
Output        : 29 years 5 months 20 days
</code></pre>

<p>I have developed following formula in DS Transformer</p>

<pre><code>((DaysSinceFromDate(DSJobStartDate,TrimSpaces.SERVICE_DT))/365) :' Years ':( Mod(DaysSinceFromDate(DSJobStartDate,TrimSpaces.SERVICE_DT),365)/30):' Months ': mod( Mod(DaysSinceFromDate(DSJobStartDate,TrimSpaces.SERVICE_DT),365),30):' Days'
</code></pre>

<p><code>DSJobStartDate=Current date</code> 
The above formula is not working well because for 
service date 1997-08-25 it is populating 22 Years 0 Months 1 Days but the correct output is 21 years 11 months 28 days</p>

<pre><code>((DaysSinceFromDate(DSJobStartDate,TrimSpaces.SERVICE_DT))/365) :' Years ':( Mod(DaysSinceFromDate(DSJobStartDate,TrimSpaces.SERVICE_DT),365)/30):' Months ': mod( Mod(DaysSinceFromDate(DSJobStartDate,TrimSpaces.SERVICE_DT),365),30):' Days'
</code></pre>

<p>service date 1997-08-25 it is populating 22 Years 0 Months 1 Days but the correct output is 21 years 11 months 28 days</p>
","transformer-model"
"57467835","Why is the batch_size in XLNet and Transformer-XL not the first but the second dimension?","2019-08-12 20:27:24","","1","82","<machine-learning><neural-network><nlp><transformer-model>","<p>I'm looking into the XLNet and Transformer-XL source code and could not understand, why is the batch_size not the first dimension. Does someone have an idea?
Do you know any other neural network architecture where the batch_size is not the first dimension?</p>
","transformer-model"
"57265943","Javax XML Transformer removing inline style of XHTML","2019-07-30 06:56:12","","0","224","<java><ckeditor><transformer-model>","<p>I'm using javax xml Transformer to transform xhtml block which is generated by ck-editor, All inline styles are omitted.</p>

<pre><code>// xhtml which is generated by ck-editor
String xhtmlString = ""&lt;html &gt;&lt;head&gt;&lt;title&gt;&lt;/title&gt;&lt;/head&gt;&lt;body dir=""rtl""&gt;&lt;h1&gt;&lt;span style=""color:#c0392b""&gt;&lt;strong&gt;Test&lt;/strong&gt;&lt;/span&gt;&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;""

// create a StreamSource from the XHTML
StringReader reader = new StringReader(xhtmlString);
StreamSource source = new StreamSource(reader);

// Create StreamResult
StringWriter stringWriter = new StringWriter();
StreamResult result = new StreamResult(stringWriter);

StreamSource xslSource = new StreamSource(new 
File(config.getEditorXSLPath())); 
javax.xml.transform.Transformer xhtmlTransformer = 
transformerFactory.newTransformer(xslSource);
xhtmlTransformer.setOutputProperty(OutputKeys.ENCODING, ""ISO-10646-UCS- 
2"");
xhtmlTransformer.transform(source , result);

logger.debug(""String Result:\n"" + stringWriter.getBuffer().toString());
</code></pre>

<p>Output:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;body&gt;
&lt;span class=""body-paragraph""&gt;&lt;/span&gt;
&lt;strong&gt;Test&lt;/strong&gt;
&lt;span class=""body-paragraph""&gt;&lt;/span&gt;
</code></pre>

<h2>    </h2>

<p>Inline style is deleted : style=""color:#c0392b""</p>
","transformer-model"
"57099613","How is teacher-forcing implemented for the Transformer training?","2019-07-18 17:12:25","","9","9773","<tensorflow><machine-learning><nlp><transformer-model>","<p>In this part of Tensorflow's tutorial <a href=""https://www.tensorflow.org/beta/tutorials/text/transformer#training_and_checkpointing"" rel=""noreferrer"">here</a>, they mentioned that they are training with teacher-forcing. To my knowledge, teacher-forcing involves feeding the target output into the model so that it converges faster. So I'm curious as to how this is done here? The real target is <code>tar_real</code>, and as far as I can see, it is only used to calculate loss and accuracy. I'm curious as to how this code is implementing teacher-forcing?</p>

<p>Thanks in advance.</p>
","transformer-model"
"57085223","Considering The Transfomer modell: What is fed from the encoder stack to the decoder layers","2019-07-17 23:29:29","","1","85","<neural-network><transformer-model><attention-model>","<p>I'm trying to properly understand the transformer architecture. I have difficulties figuring out, what kind of data ist actually fed from the encoder stack to the decoder stack.</p>

<p>Considering the ""Attention is all you need"" Paper by Google(<a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1706.03762.pdf</a> (pdf)) and the blog post by Jay Allamar(<a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">http://jalammar.github.io/illustrated-transformer/</a>) I get that attention is performed ""over all the output of the encoder stack""(Paper). Now I am wondering about two things: 1. How is the decoder stack initialized, is there a fixed or trained decoder state D0 or something or is it also initialized by the output of the encoder stack. 2. What can I picture this ""output of the encoder state"" to be like? As this is an encoder-decoder structure I'd expect it to be some kind of context vector. In another blog post by Jay Allamar(<a href=""https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"" rel=""nofollow noreferrer"">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a>) it seems like it is a concatenation or stack of the hidden states of the encoder layers. However, in the blog post mentioned first as well as in the paper it sounds more like these hidden states are only fed to the subsequent encoder layer and not stored in any way. So what is the decoder actually attending to?</p>
","transformer-model"
"56639938","BERT output not deterministic","2019-06-17 23:17:12","56646351","9","3064","<deep-learning><nlp><transformer-model><bert-language-model>","<p>BERT output is not deterministic.
I expect the output values are deterministic when I put a same input, but my bert model the values are changing. Sounds awkwardly, the same value is returned twice, once. That is, once another value comes out, the same value comes out and it repeats.
How I can make the output deterministic?
let me show snippets of my code.
I use the model as below.</p>

<p>For the BERT implementation, I use huggingface implemented BERT pytorch implementation. which is quite fameous model ri implementation in the pytorch area. [link] <a href=""https://github.com/huggingface/pytorch-pretrained-BERT/"" rel=""noreferrer"">https://github.com/huggingface/pytorch-pretrained-BERT/</a></p>

<pre><code>        tokenizer = BertTokenizer.from_pretrained(self.bert_type, do_lower_case=self.do_lower_case, cache_dir=self.bert_cache_path)
        pretrain_bert = BertModel.from_pretrained(self.bert_type, cache_dir=self.bert_cache_path)
        bert_config = pretrain_bert.config
</code></pre>

<p>Get the output like this</p>

<pre><code>        all_encoder_layer, pooled_output = self.model_bert(all_input_ids, all_segment_ids, all_input_mask)

        # all_encoder_layer: BERT outputs from all layers.
        # pooled_output: output of [CLS] vec.

</code></pre>

<p>pooled_output</p>

<pre><code>tensor([[-3.3997e-01,  2.6870e-01, -2.8109e-01, -2.0018e-01, -8.6849e-02,

tensor([[ 7.4340e-02, -3.4894e-03, -4.9583e-03,  6.0806e-02,  8.5685e-02,

tensor([[-3.3997e-01,  2.6870e-01, -2.8109e-01, -2.0018e-01, -8.6849e-02,

tensor([[ 7.4340e-02, -3.4894e-03, -4.9583e-03,  6.0806e-02,  8.5685e-02,
</code></pre>

<p>for the all encoder layer, the situation is same, - same in twice an once.</p>

<p>I extract word embedding feature from the bert, and the situation is same. </p>

<pre><code>wemb_n
tensor([[[ 0.1623,  0.4293,  0.1031,  ..., -0.0434, -0.5156, -1.0220],

tensor([[[ 0.0389,  0.5050,  0.1327,  ...,  0.3232,  0.2232, -0.5383],

tensor([[[ 0.1623,  0.4293,  0.1031,  ..., -0.0434, -0.5156, -1.0220],

tensor([[[ 0.0389,  0.5050,  0.1327,  ...,  0.3232,  0.2232, -0.5383],
</code></pre>
","transformer-model"
"56422957","Comparing records in same column and performing concatenation","2019-06-03 07:37:50","","-1","229","<datastage><transformer-model>","<p>My sample file is</p>

<pre><code>101,name1,gold
102,name2,gold
101,name1,house.
</code></pre>

<p>I need to compare the names, if they are the same then the third column has to be concatenated using pipe deimiter<br>
For ex: <code>101,name1,gold|house</code><br>
I need to achieve this in datastage transformer.<br>
Please help on this  </p>
","transformer-model"
"56420878","PyTorch runtime error: expected argument to have type long, but got CPUType instead","2019-06-03 03:41:57","","2","929","<python-3.x><pycharm><pytorch><transformer-model><attention-model>","<p>I'm new to PyTorch and going through <a href=""http://nlp.seas.harvard.edu/2018/04/03/attention.html"" rel=""nofollow noreferrer"">this tutorial</a> on the transformer model. I'm using PyCharm on Win10.
For now, I've basically just copy-pasted the example code, but I'm getting the following error:</p>

<blockquote>
  <p>RuntimeError: Expected tensor for argument #1 'indices' to have scalar type Long; but got CPUType instead (while checking arguments for embedding)</p>
</blockquote>

<p>It seems to be coming from this line  </p>

<blockquote>
  <p>def encode(self, src, src_mask):<br>
         return self.encodder(self.src_embed(src), src_mask)</p>
</blockquote>

<p>Tbh, I'm not even sure what this means, let alone how I should go about fixing it.
What's a CPUType? When did I create a variable of such type? From looking at the code I'm only using tensors (or numpy arrays)</p>

<p>here's the full error message:</p>

<blockquote>
  <p>C:...\Python\Python37\lib\site-packages\torch\nn_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
    warnings.warn(warning.format(ret))
  C:/.../PycharmProjects/Transformer/all_the_code.py:263: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
    nn.init.xavier_uniform(p)
  Traceback (most recent call last):<br>
   File ""C:/.../PycharmProjects/Transformer/all_the_code.py"", line 421, in <br>
     SimpleLossCompute(model.generator, criterion, model_opt))<br>
   File ""C:/.../PycharmProjects/Transformer/all_the_code.py"", line 297, in run_epoch<br>
    batch.src_mask, batch.trg_mask)<br>
   File ""C:/.../PycharmProjects/Transformer/all_the_code.py"", line 30,  in forward<br>
    return self.decode(self.encode(src, src_mask), src_mask,<br>
  File ""C:/.../PycharmProjects/Transformer/all_the_code.py"", line 34, in encode<br>
     return self.encoder(self.src_embed(src), src_mask)<br>
  File ""C:...\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 493, in __call__<br>
    result = self.forward(*input, **kwargs)<br>
  File ""C:...\Python\Python37\lib\site-packages\torch\nn\modules\container.py"", line 92, in forward<br>
    input = module(input)<br>
  File ""C:...\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 493, in __call__<br>
    result = self.forward(*input, **kwargs)<br>
  File ""C:/.../PycharmProjects/Transformer/all_the_code.py"", line 218, in forward<br>
    return self.lut(x) * math.sqrt(self.d_model)<br>
  File ""C:...\Python\Python37\lib\site-packages\torch\nn\modules\module.py"", line 493, in __call__<br>
    result = self.forward(*input, **kwargs)<br>
  File ""C:...\Python\Python37\lib\site-packages\torch\nn\modules\sparse.py"", line 117, in forward<br>
    self.norm_type, self.scale_grad_by_freq, self.sparse)<br>
  File ""C:...\Python\Python37\lib\site-packages\torch\nn\functional.py"", line 1506, in embedding<br>
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)</p>
</blockquote>
","transformer-model"
"56278729","Transformer based decoding","2019-05-23 15:39:08","","2","1214","<deep-learning><transformer-model><seq2seq><encoder-decoder><sequence-modeling>","<p>Can the decoder in a transformer model be parallelized like the encoder? As far as I understand the encoder has all the tokens in the sequence to compute the self-attention scores. But for a decoder this is not possible (in both training and testing), as self attention is calculated based on previous timestep outputs. Even if we consider some technique like teacher forcing, where we are concatenating expected output with obtained, this still has a sequential input from the previous timestep. In this case, apart from the improvement in capturing long-term dependencies, is using a transformer-decoder better than say an lstm when comparing purely on the basis of parallelization?</p>
","transformer-model"
"56256192","Multilabel classification of a sequence, how to do it?","2019-05-22 11:51:04","56261068","0","464","<keras><deep-learning><classification><multilabel-classification><transformer-model>","<p>I am quite new to the deep learning field especially Keras. Here I have a simple problem of classification and I don't know how to solve it. What I don't understand is how the general process of the classification, like converting the input data into tensors, the labels, etc.</p>

<p>Let's say we have three classes, <code>1, 2, 3</code>.</p>

<p>There is a sequence of classes that need to be classified as one of those classes. The dataset is for example</p>

<ul>
<li>Sequence <code>1, 1, 1, 2</code> is labeled <code>2</code></li>
<li>Sequence <code>2, 1, 3, 3</code> is labeled <code>1</code></li>
<li>Sequence <code>3, 1, 2, 1</code> is labeled <code>3</code></li>
</ul>

<p>and so on.</p>

<p>This means the input dataset will be</p>

<pre class=""lang-py prettyprint-override""><code>[[1, 1, 1, 2],
 [2, 1, 3, 3],
 [3, 1, 2, 1]]
</code></pre>

<p>and the label will be</p>

<pre class=""lang-py prettyprint-override""><code>[[2],
 [1],
 [3]]
</code></pre>

<p>Now one thing that I do understand is to one-hot encode the class. Because we have three classes, every <code>1</code> will be converted into <code>[1, 0, 0]</code>, <code>2</code> will be <code>[0, 1, 0]</code> and <code>3</code> will be <code>[0, 0, 1]</code>. Converting the example above will give a dataset of 3 x 4 x 3, and a label of 3 x 1 x 3.</p>

<p>Another thing that I understand is that the last layer should be a softmax layer. This way if a test data like (e.g. <code>[1, 2, 3, 4]</code>) comes out, it will be softmaxed and the probabilities of this sequence belonging to class 1 or 2 or 3 will be calculated.</p>

<p>Am I right? If so, can you give me an explanation/example of the process of classifying these sequences?</p>

<p>Thank you in advance.</p>
","transformer-model"
"56176565","Pyspark Pipeline Custom Transformer","2019-05-16 21:01:04","","1","176","<pyspark><transformer-model>","<p>I'm having some trouble understanding the creation of custom transformers for  Pyspark pipelines.</p>

<p>I am writing a custom transformer that will take the dataframe column <code>Company</code> and remove stray commas:</p>

<pre><code>from pyspark.sql.functions import *

class DFCommaDropper(Transformer):

    def__init__(self, *args, **kwargs):
        self.name = CommaDropper

    def transform(self,df):
        df = df.withColumn('Company', regexp_replace('Company',',','')
        return df
</code></pre>

<p>The above code is obviously wrong.  I'm unsure what/how to initialize this and then how to use the initialized class instance in the transform function.</p>

<p>Thanks in advance for your help.,</p>
","transformer-model"
"56102201","How does BERT utilize TPU memories?","2019-05-12 18:26:08","","2","1036","<tensorflow><transformer-model><google-cloud-tpu><tpu><bert-language-model>","<p><a href=""https://github.com/google-research/bert/blob/master/README.md#out-of-memory-issues"" rel=""nofollow noreferrer"">README</a> in the Google's BERT repo says, even a single sentence of length 512 can not sit in a 12 GB Titan X for the BERT-Large model.</p>

<p>But in the BERT paper, it says 64 TPU chips are used to train BERT-Large
with a maximum length 512 and batch size 256. How could they fit a >256x larger batch into only 171x more memory? </p>

<p>From another point of view, we can compare these two configurations in a memory-usage-per-sample basis:</p>

<ul>
<li>TPU: Assume TPUv3 is used in pre-training, the total TPU memory is 32 GB/chip * 64 chips = 2048 GB. According to the paper, a batch size of 256 with maximum length 512 works well in this configuration, which means <strong>8 GB memory is able to hold a single sample.</strong> Furthermore, memory usage per sample will reduce to only 4 GB if GPUv2 is used.</li>
<li>GPU: <strong>A 12 GB Titan X can not hold even a single sample of length 512</strong>.</li>
</ul>

<p>Why is memory consumption on GPUs much larger? Does this mean memory consumption on TPUs is optimized way better than that on GPUs?</p>
","transformer-model"
"56081739","Size of positional encoding in a tensorflow tutorial","2019-05-10 16:44:59","","0","740","<tensorflow><transformer-model>","<p>I am trying to understand and play with this tensorflow tutorial about the transformer architecture and I find something I don't understand in the Class Decoder. Why is the self.pos_encoding = positional_encoding(target_vocab_size, self.d_model) called with targe_vocab_size instead of the maximum length of sequences? See this link and code for the class below. Any idea? <a href=""https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/transformer.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/transformer.ipynb</a> </p>

<pre><code>class Decoder(tf.keras.layers.Layer):
  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, 
               rate=0.1):
    super(Decoder, self).__init__()

    self.d_model = d_model
    self.num_layers = num_layers

    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)
    self.pos_encoding = positional_encoding(target_vocab_size, self.d_model)

    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) 
                       for _ in range(num_layers)]
    self.dropout = tf.keras.layers.Dropout(rate)
    def call(self, x, enc_output, training, 
           look_ahead_mask, padding_mask):

    seq_len = tf.shape(x)[1]
    attention_weights = {}

    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)
    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
    x += self.pos_encoding[:, :seq_len, :]

    x = self.dropout(x, training=training)

    for i in range(self.num_layers):
      x, block1, block2 = self.dec_layers[i](x, enc_output, training,
                                             look_ahead_mask, padding_mask)

      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1
      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2

    # x.shape == (batch_size, target_seq_len, d_model)
    return x, attention_weights
</code></pre>
","transformer-model"
"56063210","Extremely large gradient in last layer and small in the rest","2019-05-09 15:47:17","","4","301","<neural-network><pytorch><data-science><backpropagation><transformer-model>","<p>I’m very new to training neural nets but foolishly tried to implement my own novel architecture. It’s very similar to a transformer, in that the pipeline takes in a “batch of articles headlines” tensor, passes it through the encoder half of the Transformer, then through several transformations and versions of the transformer encoder. The output is then summed into a single vector and softmaxed. I also wrote a custom loss function.</p>

<p>When I graph the average gradients per layer, it looks like <a href=""https://i.sstatic.net/Tp4yx.png"" rel=""nofollow noreferrer"">this</a>.</p>

<p>(The last layer’s gradients are huge and the rest are barely existent. This is after clipping gradient to 0.25; before that the last layer's gradient was at around 1e7)</p>

<p>Here’s my relevant code:</p>

<pre><code>def forward(self, example):

        art_mask = Variable((example.Art[:,:,0] != 0).unsqueeze(-2)).cuda()
        Art = Variable(example.Art).cuda()

        # To give context to article embeddings, pass through Transformer Encoder block
        Art = self.encoder_block(self.position_layer(Art), art_mask).cuda()

        Tsf = example.Tsf.cuda().repeat((1,Art.shape[1],1)).cuda()
        # Concanetante Tsf to Art
        Art = torch.cat((Art, Tsf), dim=2).cuda()

        # Convert Art to Ent and construct ent_mask
        Ent = Art[example.EntArt[:,:,0], example.EntArt[:,:,1],:].cuda()
        ent_mask = (example.EntArt[:,:,0] == -1).unsqueeze(-2).cuda()

        # Pass to graph block, alternating layers of Relational Attn and Entity Self Attn
        Ent = self.graph_block(Ent, ent_mask, example.RelEnt).cuda()


        # Slice and reorder Ent into assets tensor
        A = len(self.assets_list)
        Ass = torch.full((A,1), -1, dtype=torch.long).cuda()
        for i,uri in enumerate(self.assets_list):
            if uri in example.AssetIndex:
                Ass[i] = example.AssetIndex[uri]

        Assets = Ent[Ass, :, :].squeeze(1).cuda()
        mask = Ass.unsqueeze(2).repeat(1,Assets.shape[1],Assets.shape[2]).cuda()
        Assets = Assets.masked_fill(mask == -1, -1e9).cuda()

        Assets = Assets.sum(dim = 1).squeeze(1)
        Assets = torch.matmul(Assets, self.W).cuda()

        bias = torch.zeros((1)).cuda()
        Assets = torch.cat((Assets, bias)).cuda()

        return self.softmax(Assets)

""""""
prices[i] is normalized closing / opening
:param prices &lt;torch.Tensor(batch_size, len(assets))&gt;
""""""
def loss_f(model, XY):
    examples, prices = XY
    portfolios = torch.stack([model.forward(ex) for ex in examples], dim=0)
    prices = Variable(prices)

    # safe asset (US Dollars) at prices[:,-1]
    prices = torch.cat((prices, torch.ones((prices.shape[0], 1), dtype=torch.float)), dim = 1).cuda()

    return -torch.sum(portfolios * prices) / 4 # batch size
</code></pre>

<p>The layer with the large gradients is the coefficients and biases for a LayerNorm layer at the end of the transformer-esque architecture</p>
","transformer-model"
"56047757","How to migrate read file from ftp flow to mule 4.1.4 Kernel version?","2019-05-08 19:26:33","","0","120","<migration><kernel><mule><transformer-model><mule-flow>","<p>people! I am migrating to mule 4 Kernel version.</p>

<p>I'm stuck in this moment: requirement is to read file from FTP and then process it. In old version it was like a few components: 
1. quartz, 
2. transformer 
3. transformer 
4. queue</p>

<p>Can somebody help me to migrate it to mule 4 kernel? </p>

<p>How to do this? How to put file content as string into queue like it was in older version? It would be nice if we could talking about Mule Kernel version. I'm new member of this community and of Mule developers, to pls dont hate me.</p>

<p>In next step I'm gonna split this file (splitter) but I know in Kernel there is not splitters anymore, so I have to use for each, right?</p>

<p>Now I've got 
1. http listener (but it should be job. For my own tests It is http listener, I'm gonna to change this). 
2. FTP read with FTP connector 
3. ????</p>

<pre><code>&lt;flow&gt; 
         &lt;quartz 
               with cronExpression 
               and with conector to FTP&gt;
          &lt;/quartz&gt;
          &lt;gzip-uncompress-transformer encoding=""UTF-8""&gt;&lt;/gzip-uncompress-transformer&gt;  
          &lt;byte-array-to-string-transformer encoding=""UTF-8""&gt;&lt;/byte-array-to-string-transformer&gt;  
          &lt;jms:outbound-endpoint queue=""xxx"" &gt;&lt;/jms:outbound-endpoint&gt;  
      &lt;/flow&gt;
</code></pre>
","transformer-model"
"55954232","Transfer learning with OpenNMT","2019-05-02 14:04:23","","0","560","<python><pytorch><transformer-model><transfer-learning><opennmt>","<p>I'm training a transformer model with OpenNMT-py on MIDI music files, but results are poor because I only have access to a small dataset pertaining to the style I want to study. To help the model learn something useful, I would like to use a much larger dataset of other styles of music for a pre-training and then fine-tune the results using the small dataset.</p>

<p>I was thinking of freezing the encoder side of the transformer after the pre-training and letting the decoder part free to do the fine-tuning. How would one do this with OpenNMT-py?</p>
","transformer-model"
"55787858","Scikit-learn transformer pipeline produces different results than running individually","2019-04-22 00:13:47","","2","578","<python><scikit-learn><pipeline><transformer-model>","<p>When I tried using the pipeline to combine a couple transformers, the second transformer (log) appears not be applied.</p>

<p>I have tried to simplify the log transformer to perform simple addition but the same problem persists.</p>

<pre><code>import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin

class Impute(BaseEstimator, TransformerMixin):
    def __init__(self, columns=None, value='mean'):
        """"""
        columns: A list of columns to apply the imputation to.
        value: 
            - ""mean"": Fills in missing values with mean of training data
            - number: Fills in values with that number
            - dictionary: Fills in values where dictionary keys are column names
        """"""
        self.columns = columns
        self.value = value

    def fit(self, X, y=None):
        if self.columns is None:
            self.columns = X.columns
        if isinstance(self.value, str):
            if self.value == ""mean"":
                self.value = X[self.columns].mean()
            elif self.value == 'median':
                self.value = X[self.columns].median()
        return self

    def transform(self, X):
        X[self.columns] = X[self.columns].fillna(self.value)
        return X

class Log(BaseEstimator, TransformerMixin):
    def __init__(self, columns=None, offset_value=0):
        """"""
        offset_value: a value to specify to handle invalid outputs such as log(0) or log(negative values)
        """"""
        self.columns = columns
        self.offset_value = offset_value

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        X_new = X.copy()
        X_new[self.columns] = np.log(X_new[self.columns] + self.offset_value)
        return X_new

###########################
temp = pd.DataFrame([[590,3,None, ""2018-01-01""],[0,2,3, ""2018-01-01""],
                     [590,2,4, ""2019-01-01""], [None ,None,4, ""2018-01-01""], 
                     [850 ,None,4, ""2018-01-01""]], columns=[""credit_score"", ""n_cats"", ""premium"", ""fix_date""])

print(temp)

impute = Impute(columns=[""credit_score"", ""n_cats"", ""premium""], value=""mean"")
impute.fit(temp)
temp = impute.transform(temp)

log = Log(columns=[""credit_score"", ""n_cats"", ""premium""], offset_value=1)
log.fit(temp)
temp = log.transform(temp)
temp


###########################
temp = pd.DataFrame([[590,3,None, ""2018-01-01""],[0,2,3, ""2018-01-01""],
                     [590,2,4, ""2019-01-01""], [None ,None,4, ""2018-01-01""], 
                     [850 ,None,4, ""2018-01-01""]], columns=[""credit_score"", ""n_cats"", ""premium"", ""fix_date""])

print(temp)

impute = Impute(columns=[""credit_score"", ""n_cats"", ""premium""], value=""mean"")
log = Log(columns=[""credit_score"", ""n_cats"", ""premium""], offset_value=1)

steps = [(""impute"", impute),
         (""log"", log)
        ]

pipe = Pipeline(steps)

pipe.fit(temp)
pipe.transform(temp)
temp
</code></pre>

<p>When transformer are applied separately, it shows:</p>

<pre><code>    credit_score    n_cats  premium fix_date
0   6.381816    1.386294    1.558145    2018-01-01
1   0.000000    1.098612    1.386294    2018-01-01
2   6.381816    1.098612    1.609438    2019-01-01
3   6.231465    1.203973    1.609438    2018-01-01
4   6.746412    1.203973    1.609438    2018-01-01
</code></pre>

<p>When I tried to use the pipeline, it shows</p>

<pre><code>    credit_score    n_cats  premium fix_date
0   590.0   3.000000    3.75    2018-01-01
1   0.0 2.000000    3.00    2018-01-01
2   590.0   2.000000    4.00    2019-01-01
3   507.5   2.333333    4.00    2018-01-01
4   850.0   2.333333    4.00    2018-01-01
</code></pre>
","transformer-model"
"55278935","Call to a member function createData() on null using middleware on Laravel Transformers","2019-03-21 11:00:25","55293126","4","798","<php><laravel><eloquent><transformer-model>","<p>So i created a controller for authentication with 2 methods (<code>token()</code> / <code>native)_</code>). Im using fractal transformer to return response. The token method works fine for me, but the <code>loginAndroid()</code> returns </p>

<blockquote>
  <p>""Call to a member function createData() on null"" error. </p>
</blockquote>

<p>Any help? Thank you.</p>

<pre><code>class AuthController extends RestController
{
    protected $transformer = UserTransformers::Class;

    public function __construct()
    {
        $this-&gt;middleware('auth:api', ['except' =&gt; ['login', 'loginAndroid']]);
    }

    public function login(Request $request)
    {
        $credentials = $request-&gt;only(['username', 'password']);

        if (!$token = auth()-&gt;attempt($credentials)) {
            return response()-&gt;json(['error' =&gt; 'Unauthorized'], 401);
        }

        return $this-&gt;respondWithToken($token);
    }

    public function loginAndroid(Request $request)
    {
        $credentials = $request-&gt;only(['username', 'password']);

        if (Auth::attempt($credentials)) {
            //$user = Auth::user()-&gt;with(['employees']);
            $userdata = User::with(['employees', 'employees.role', 'employees.branch'])-&gt;find(Auth::id());
            //$success['token'] =  $user-&gt;createToken('MyApp')-&gt;accessToken; 
            //return response()-&gt;json($userdata, 200); 
            //return $userdata;
            $response = $this-&gt;generateItem($userdata);

            return $this-&gt;sendResponse($response, 201);
        } else {
            return response()-&gt;json('gagal', 401);
        }
    }
}
</code></pre>

<p>this is my restcontroller</p>

<pre><code>abstract class RestController extends Controller
{
protected $manager;

protected $transformer;

public function __construct()
{
    $this-&gt;manager = new Manager();
}

protected function generateItem($model, $transformer = null)
{
    if (!is_null($transformer)) {
        return new Item($model, new $transformer);
    }

    return new Item($model, new $this-&gt;transformer);
}

protected function generateCollection($model, $transformer = null)
{
    if (!is_null($transformer)) {
        return new Collection($model, new $transformer);
    }

    return new Collection($model, new $this-&gt;transformer);
}

protected function sendResponse(ResourceInterface $data, $status = 200)
{
    return response()-&gt;json(
        $this-&gt;manager-&gt;createData($data)-&gt;toArray(),
        $status
    );
}

protected function sendNotFoundResponse($status)
{
    return response()-&gt;json($status, 404);
}

protected function sendIseResponse($status)
{
    return response()-&gt;json($status, 500);
}
}
</code></pre>
","transformer-model"
"55142677","Sklearn Pipeline : pass a parameter to a custom Transformer?","2019-03-13 13:10:02","55153569","9","4377","<scikit-learn><pipeline><transformer-model>","<p>I have a custom Transformer in my <code>sklearn</code> Pipeline and I wonder how to pass a parameter to my Transformer :</p>

<p>In the code below, you can see that I use a dictionary ""weight"" in my Transformer. I wish to not define this dictionary inside my Transformer but instead to pass it from the Pipeline, so that I can include this dictionary in a grid search . Is it possible to pass the dictionary as a parameter to my Transformer ?</p>

<pre><code># My custom Transformer
  class TextExtractor(BaseEstimator, TransformerMixin):
        """"""Concat the 'title', 'body' and 'code' from the results of 
        Stackoverflow query
        Keys are 'title', 'body' and 'code'.
        """"""
        def fit(self, x, y=None):
            return self

        def transform(self, x):
            # here is the parameter  I want to pass to my transformer
            weight ={'title' : 10, 'body': 1, 'code' : 1}
            x['text'] = weight['title']*x['Title'] +  
            weight['body']*x['Body'] +  
            weight['code']*x['Code']

            return x['text']

param_grid = {
    'min_df' : [10],
    'max_df' : [0.01],
    'max_features': [200],
    'clf' : [sgd]
    # here is the parameter  I want to pass to my transformer
    'weigth' : [{'title' : 10, 'body': 1, 'code' : 1}, {'title' : 1, 'body': 
     1, 'code' : 1}]

}

for g in ParameterGrid(param_grid) :   

    classifier_pipe = Pipeline(

    steps=[    ('textextractor', TextExtractor()), #is it possible to pass 
                my parameter ?
               ('vectorizer', TfidfVectorizer(max_df=g['max_df'], 
                     min_df=g['min_df'], max_features=g['max_features'])),
               ('clf', g['clf']), 
            ],
    )
</code></pre>
","transformer-model"
"55137103","XPath compiling behaviour","2019-03-13 08:12:55","55138180","1","372","<java><xml><exception><xpath><transformer-model>","<p>I am testing my application and realised that behaviour is different when compiling.</p>

<p>For example, if my expression to compile is :</p>

<pre><code>XPathExpression expr = xPath.compile(""/DocDetails/TransactionSignature"");
</code></pre>

<p>And :</p>

<pre><code>XPathExpression expr2 = xPath.compile(""/DocDetails/"" + x); 
</code></pre>

<p>x is declared as a String datatype.</p>

<p>Lets say that x in expr2 is ""abc"", XPathExpression is compiled with no issues.</p>

<p>But if x in expr2 is ""123abc"" OR ""123"", XPathExpression throws a : </p>

<blockquote>
  <p>javax.xml.transform.TransformerException: A location step was expected
  following the '/' or '//' token.</p>
</blockquote>

<p>Just curious regarding this behaviour..</p>

<p>Here is the full code for reference: </p>

<pre><code>        String document = ""C:/Users/Eunice/Documents/MITS/doc.xml"";
        String document2 = ""C:/Users/Eunice/Documents/MITS/doc2.xml"";

        DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
        DocumentBuilder builder = factory.newDocumentBuilder();
        Document doc = builder.parse(document);

        Document doc2 = builder.parse(document2);

        XPathFactory xPathFactory = XPathFactory.newInstance();
        XPath xPath = xPathFactory.newXPath();
        XPathExpression expr = xPath.compile(""/DocDetails/TransactionSignature"");
        Node node = (Node)expr.evaluate(doc, XPathConstants.NODE);

        String x = node.getTextContent();

        System.out.println(x);

        XPathExpression expr2 = xPath.compile(""/DocDetails/"" + x);
        Node node2 = (Node)expr2.evaluate(doc2, XPathConstants.NODE);

        if (node2 == null)
            System.out.println(""null"");
        else 
            System.out.println(""not null "" + node2.getTextContent());
</code></pre>

<p>And this is the XML file: </p>

<pre><code>&lt;DocDetails&gt;
    &lt;TransactionSignature&gt;abc123&lt;/TransactionSignature&gt;
&lt;/DocDetails&gt;
</code></pre>
","transformer-model"
"55114128","Uni-directional Transformer VS Bi-directional BERT","2019-03-12 04:23:33","55122596","8","4439","<nlp><transformer-model><pre-trained-model><bert-language-model>","<p>I just finished reading the <a href=""https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""noreferrer"">Transformer</a> paper and <a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">BERT</a> paper. But couldn't figure out why Transformer is uni-directional and BERT is bi-directional as mentioned in BERT paper. As they don't use recurrent networks, it's not so straightforward to interpret the directions. Can anyone give some clue? Thanks.</p>
","transformer-model"
"55031049","get parameters from a fitted custom pyspark transformer","2019-03-06 19:43:36","","0","565","<apache-spark><pyspark><transformer-model>","<p>Suppose the next Pyspark custom transformer:</p>

<pre><code>class CustomTransformer(MockTransformer, Identifiable, PysparkReaderWriter, MLReadable, MLWritable):

    def __init__(self, output_col):
        self.output_col = output_col
        self.feat_cols = None
        super(CustomTransformer, self).__init__()

    def _transform(self, df):

        self.feat_cols = get_match_columns(df, ""ops"")
        # Do something smart here with this feat_cols
        df = df.drop(*self.feat_cols)

        return df
</code></pre>

<p>where <code>feat_cols</code> is calculated and setter inside the <code>_transform()</code> method, and <code>get_match_columns</code> is a function which returns the column names that match some pattern. I need to access this parameter once the pipeline containing this transformer has been transformed, for example:</p>

<pre><code>pipeline = Pipeline(stages=[custom_transformer, assembler])
myPipe = pipeline.fit(data)
result = myPipe.transform(data)
</code></pre>

<p>with some method like:</p>

<pre><code>result.stages[0].getParam('feat_cols')
</code></pre>

<p>but, obviously, it doesn't work. I've tried to follow this <a href=""https://raufer.github.io/2018/02/08/custom-spark-models-with-python-wrappers/"" rel=""nofollow noreferrer"">wrapper</a>, coding this getter in my transformer:</p>

<pre><code>def getFeatCols(self):
        return self.getOrDefault(self.feat_cols)
</code></pre>

<p>but I still can not recover the parameter (either     <code>result.stages[0]._java_obj.getParam('feat_cols')</code> works).</p>

<p>Is there any way to solve this in Pyspark?</p>
","transformer-model"
"54850852","transformer.setOutputProperty() is not taking effect - java 8","2019-02-24 10:13:42","","1","898","<xml><apache-poi><transformer-model>","<p>I'm using poi library to write xml file using transformer and I have these properties sat:</p>

<p>java 1.8</p>

<pre><code>//for output to file, console
        TransformerFactory transformerFactory = TransformerFactory.newInstance();
        Transformer transformer = transformerFactory.newTransformer();
        //for pretty print
        transformer.setOutputProperty(OutputKeys.DOCTYPE_PUBLIC, ""yes""); //TODO
        transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
        transformer.setOutputProperty(OutputKeys.METHOD, ""xml"");     
        transformer.setOutputProperty(OutputKeys.STANDALONE, ""yes""); 
</code></pre>

<p>But the result appears with standalone=""no"" and without spacing/indentation!!!</p>

<p>Result:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;MainStruct&gt;
&lt;StringArray1 value=""H1""/&gt;
&lt;StringArray1 value=""H1""/&gt;
&lt;StringArray1 value=""H1""/&gt;
&lt;StringArray1 value=""H1""/&gt;
&lt;Table&gt;
&lt;IntVar1 value=""1""/&gt;
&lt;StringVar1 value=""String1""/&gt;
&lt;IntVar2 value=""2""/&gt;
&lt;StringVar2 value=""S""/&gt;
&lt;/Table&gt;
.
.
</code></pre>

<p>Expected:</p>

<pre><code>&lt;?xml version = '1.0' encoding = 'UTF-8' standalone = 'yes'?&gt;
&lt;MainStruct&gt;
   &lt;StringArray1 value=""H1""/&gt;
   &lt;StringArray1 value=""H1""/&gt;
   &lt;StringArray1 value=""H1""/&gt;
   &lt;StringArray1 value=""H1""/&gt;
   &lt;Table&gt;
      &lt;IntVar1 value=""1""/&gt;
      &lt;StringVar1 value=""String1""/&gt;
      &lt;IntVar2 value=""2""/&gt;
      &lt;StringVar2 value=""S""/&gt;
   &lt;/Table&gt;
.
.
</code></pre>
","transformer-model"
"54701986","Custom Transformer in sklearn","2019-02-15 02:42:57","54703831","1","359","<python><machine-learning><scikit-learn><data-science><transformer-model>","<p>I am building a transformer in sklearn which drops features that have a correlation coefficient lower than a specified threshold.</p>

<p>It works on the training set. However, when I transform the test set. All features on the test set disappear. I assume the transformer is calculating correlations between test data and training label and since those are all low, it is dropping all features. How do I make it only calculate correlations on the training set and drop those features from the test set on the transform?</p>

<pre><code>class CorrelatedFeatures(BaseEstimator, TransformerMixin): #Selects only features that have a correlation coefficient higher than threshold with the response label
    def __init__(self, response, threshold=0.1):
        self.threshold = threshold
        self.response = response
    def fit(self, X, y=None):
        return self
    def transform(self, X, y=None):
        df = pd.concat([X, self.response], axis=1)
        cols = df.columns[abs(df.corr()[df.columns[-1]]) &gt; self.threshold].drop(self.response.columns)
        return X[cols]
</code></pre>
","transformer-model"
"54298550","AttributeError when using ColumnTransformer into a pipeline","2019-01-21 22:20:16","54299376","19","30924","<python><pandas><scikit-learn><pipeline><transformer-model>","<p>This is my first machine learning project and the first time that I use ColumnTransformer. My aim is to perform two steps of data preprocessing, and use ColumnTransformer for each of them.</p>
<p>In the first step, I want to replace the missing values in my dataframe with the string 'missing_value' for some features, and the most frequent value for the remaining features. Therefore, I combine these two operations using ColumnTransformer and passing to it the corresponding columns of my dataframe.</p>
<p>In the second step, I want to use the just preprocessed data and apply OrdinalEncoder or OneHotEncoder depending on the features. For that I use again ColumnTransformer.</p>
<p>I then combine the two steps into a single pipeline.</p>
<p>I am using the Kaggle Houses Price dataset, I have scikit-learn version 0.20 and this is a simplified version of my code:</p>
<pre><code>cat_columns_fill_miss = ['PoolQC', 'Alley']
cat_columns_fill_freq = ['Street', 'MSZoning', 'LandContour']
cat_columns_ord = ['Street', 'Alley', 'PoolQC']
ord_mapping = [['Pave', 'Grvl'],                          # Street
               ['missing_value', 'Pave', 'Grvl'],         # Alley
               ['missing_value', 'Fa', 'TA', 'Gd', 'Ex']  # PoolQC
]
cat_columns_onehot = ['MSZoning', 'LandContour']


imputer_cat_pipeline = ColumnTransformer([
        ('imp_miss', SimpleImputer(strategy='constant'), cat_columns_fill_miss),  # fill_value='missing_value' by default
        ('imp_freq', SimpleImputer(strategy='most_frequent'), cat_columns_fill_freq),
])

encoder_cat_pipeline = ColumnTransformer([
        ('ordinal', OrdinalEncoder(categories=ord_mapping), cat_columns_ord),
        ('pass_ord', OneHotEncoder(), cat_columns_onehot),
])

cat_pipeline = Pipeline([
        ('imp_cat', imputer_cat_pipeline),
        ('cat_encoder', encoder_cat_pipeline),
])
</code></pre>
<p>Unfortunately, when I apply it to housing_cat, the subset of my dataframe including only categorical features,</p>
<pre><code>cat_pipeline.fit_transform(housing_cat)
</code></pre>
<p>I get the error:</p>
<blockquote>
<p>AttributeError: 'numpy.ndarray' object has no attribute 'columns'</p>
<p>During handling of the above exception, another exception occurred:</p>
<p>...</p>
<p>ValueError: Specifying the columns using strings is only supported for pandas DataFrames</p>
</blockquote>
<p>I have tried this simplified pipeline and it works properly:</p>
<pre><code>new_cat_pipeline = Pipeline([
        ('imp_cat', imputer_cat_pipeline),
        ('onehot', OneHotEncoder()),
])
</code></pre>
<p>However, if I try:</p>
<pre><code>enc_one = ColumnTransformer([
        ('onehot', OneHotEncoder(), cat_columns_onehot),
        ('pass_ord', 'passthrough', cat_columns_ord)
])

new_cat_pipeline = Pipeline([
        ('imp_cat', imputer_cat_pipeline),
        ('onehot_encoder', enc_one),
])
</code></pre>
<p>I start to get the same error.</p>
<p>I suspect then that this error is related to the use of ColumnTransformer in the second step, but I do not actually understand where it comes from. The way I identify the columns in the second step is the same as in the first step, so it remains unclear to me why only in the second step I get the Attribute Error...</p>
","transformer-model"
"53728595","How do I pass the value of the token inside the Transformer?","2018-12-11 16:39:10","","0","24","<php><laravel><transformer-model>","<p>I have a method where the user logs in, but I want to pass his token on the user transfomer, how to do?
I using Fractal.</p>

<p>before I used an ordinary json response, which would return the data plus the token.</p>

<p>// Authcontroller</p>

<p>public function authenticate(Request $request){</p>

<pre><code>    // grab credentials from the request
    $credentials = $request-&gt;only('email', 'password');

    try {
        // attempt to verify the credentials and create a token for the user
        if (!$token = JWTAuth::attempt($credentials)) {
            return $this-&gt;response-&gt;errorUnauthorized('dados incorretos');
        }
    } catch (JWTException $e) {
        // something went wrong whilst attempting to encode the token
        return $this-&gt;response-&gt;withError('could_not_create_token', 500);
    }

    $user = getAuthUser();

    if (!$user-&gt;active) {
        return $this-&gt;response-&gt;errorUnauthorized('usuário não ativo');
    }
    $user = $user-&gt;token;

    // all good so return the token
    return $this-&gt;response-&gt;withItem($user, new UserTransformer());
}
</code></pre>

<p>//UserTransformer</p>

<pre><code>public function transform(User $user)
{
    return [
        'id' =&gt; (int)$user-&gt;id,
        'name' =&gt; $user-&gt;name,
        'email' =&gt; $user-&gt;email,
        'photo_url' =&gt; $user-&gt;photo_url,
        'active' =&gt; $user-&gt;active,
        'phone_number' =&gt; $user-&gt;phone_number,
        'company_id' =&gt; $user-&gt;company_id,
        'token' =&gt; $user-&gt;token,
     ];
}
</code></pre>
","transformer-model"
"53448691","Typechecking after running Typescript compiler plugin/transformer","2018-11-23 14:39:48","53449470","3","992","<typescript><plugins><transformer-model>","<p>I'm following a blog (<a href=""https://dev.doctorevidence.com/how-to-write-a-typescript-transform-plugin-fc5308fdd943"" rel=""nofollow noreferrer"">https://dev.doctorevidence.com/how-to-write-a-typescript-transform-plugin-fc5308fdd943</a>) on how to write a Typescript compiler plugin/transformer.</p>

<p>After applying a first simple transformation which should introduce a type-error (some property accessed on an object that doesn't have that property) I noticed that the no type-error is shown. In fact, the compiler proceeds as normal.</p>

<pre><code>import * as ts from ""typescript"";

export const transformerFactory = (
  program: ts.Program
): ts.TransformerFactory&lt;ts.SourceFile&gt; =&gt; {
  return (context: ts.TransformationContext): ts.Transformer&lt;ts.SourceFile&gt; =&gt; {
    const visitor: ts.Visitor = (node: ts.Node): ts.VisitResult&lt;ts.Node&gt; =&gt; {
      if (ts.isCallExpression(node) &amp;&amp; ts.isIdentifier(node.expression)) {
        if (node.expression.escapedText === ""someCall"") {
          return ts.createCall(
            ts.createPropertyAccess(node.expression, ""nonExisting""),
            node.typeArguments,
            node.arguments
          );
        }
      }
      return ts.visitEachChild(node, visitor, context);
    };

    return (sf: ts.SourceFile) =&gt; ts.visitNode(sf, visitor);
  };
};
</code></pre>

<p>Applied to <code>index.ts</code>:</p>

<pre><code>declare function someCall(...args: any[]): string;

console.log(someCall(1, 2, true));
</code></pre>

<p>Yields <code>index.js</code>:</p>

<pre><code>console.log(someCall.nonExisting(1, 2, true));
</code></pre>

<p>(even with <code>noEmitOnError: true</code>)</p>

<p>Is this intended behavior? Is this something I can enable somewhere?</p>
","transformer-model"
"53310103","How can I create a const assignment in a typescript transformer?","2018-11-14 23:08:16","53323387","2","571","<typescript><transformer-model>","<p>I can create a variable assignment with this code:</p>

<pre><code>ts.createVariableStatement(undefined, 
     [ts.createVariableDeclaration('a', undefined, 
      ts.createStringLiteral('42'))])

/// yields: var a = 42
</code></pre>

<p>I cannot however create a const assignment. I am quite sure it <em>should</em> work like so:</p>

<pre><code>ts.createVariableStatement([ts.createModifier(ts.SyntaxKind.ConstKeyword)], 
     [ts.createVariableDeclaration('a', undefined, 
      ts.createStringLiteral('42'))])
</code></pre>

<p>but this yields an error:</p>

<pre><code>[!] Error: Unexpected keyword 'var'
</code></pre>

<p>an no javascript is emitted due to that error. The error message is very confusing as well.</p>
","transformer-model"
"53132182","How to understand this Java code that produces DOM output?","2018-11-03 14:16:55","","-1","244","<java><xml><dom><transformer-model>","<p>With an XML file as an input, I've parsed it and modified it with basic functions like deleting a node, adding one etc. I wanted to output the final document in the form of a DOM tree. I've found this code but struggle to really understand what is doing each line (even when reading the doc for each functions).</p>

<p>Could you, please, illustrate with an example how it works?</p>

<pre><code>  private static void toString(Document newDoc) throws Exception{
        DOMSource domSource = new DOMSource(newDoc);
        Transformer transformer = TransformerFactory.newInstance().newTransformer();
        StringWriter sw = new StringWriter();
        StreamResult sr = new StreamResult(sw);
        transformer.transform(domSource, sr);
        System.out.println(sw.toString());  
      }
</code></pre>

<p><a href=""https://i.sstatic.net/KSbOk.png"" rel=""nofollow noreferrer"">Output</a></p>
","transformer-model"
"52977150","InvalidArgumentError: Mismatch between the current graph and the graph from the checkpoint","2018-10-24 20:04:28","","12","7019","<python><tensorflow><tensor><transformer-model>","<p>So I am basically using this transformer implementation for my project: <a href=""https://github.com/Kyubyong/transformer"" rel=""noreferrer"">https://github.com/Kyubyong/transformer</a> . 
It works great on the German to English translation it was originally written for and I modified the processing python script in order to create vocabulary files for the languages that I want to translate. This seems to work fine.</p>

<p>However when it comes to training I get the following error:</p>

<blockquote>
  <p>InvalidArgumentError (see above for traceback): Restoring from
  checkpoint failed. This is most likely due to a mismatch between the
  current graph and the graph from the checkpoint. Please ensure that
  you have not altered the graph expected based on the checkpoint.
  Original error:</p>
  
  <p>Assign requires shapes of both tensors to match. lhs shape= [9796,512]
  rhs shape= [9786,512]      [[{{node save/Assign_412}} =
  Assign[T=DT_FLOAT, _class=[""loc:@encoder/enc_embed/lookup_table""],
  use_locking=true, validate_shape=true,
  _device=""/job:localhost/replica:0/task:0/device:CPU:0""](encoder/enc_embed/lookup_table/Adam_1,
  save/RestoreV2:412)]]</p>
</blockquote>

<p>Now I have no idea why I am getting the above error. I also reverted to the original code to translate from German to English and now I get the same error (except the lhs and rhs tensor shapes are different of course), when before it was working! </p>

<p>Any ideas on why this could be happening?</p>

<p>Thanks in advance</p>

<p>EDIT: This is the specific file in question here, the train.py when it is run: <a href=""https://github.com/Kyubyong/transformer/blob/master/train.py"" rel=""noreferrer"">https://github.com/Kyubyong/transformer/blob/master/train.py</a> 
Nothing has been modified other than the fact that the vocab loaded for de and en are differently (they're in fact vocab files with single letters as words). However as I mentioned that even when resorting back to the prevous working example I get the same error with different lhs and rhs dimensions. </p>
","transformer-model"
"52812496","REMOTE_ADDR disappears when running XSLT","2018-10-15 08:22:38","53203443","0","40","<xml><xslt><web-config><rules><transformer-model>","<p>I have created a block rule for my website where I'm listing a few IP-addresses that are allowed to access the Admin-page of my website.
The condition looks like this: </p>

<pre><code>&lt;xsl:template match=""/configuration/system.webServer/rewrite/rules/rule[@name='adminBlockRule']""&gt;
    &lt;xsl:comment&gt;Blockera Admin för alla utom vissa IP-adresser&lt;/xsl:comment&gt;
    &lt;rule name=""adminBlockRule"" enabled=""true"" patternSyntax=""ECMAScript"" stopProcessing=""true""&gt;
      &lt;match url=""^(Admin/|Sysadmin/).*$"" ignoreCase=""true""/&gt;
      &lt;conditions&gt;
        &lt;xsl:comment&gt;Generell&lt;/xsl:comment&gt;
        &lt;add input=""{REMOTE_ADDR}"" pattern=""10.*.*.*"" negate=""true"" /&gt;
        &lt;add input=""{REMOTE_ADDR}"" pattern=""194.103.31.*"" negate=""true"" /&gt;
        ... More rules
      &lt;/conditions&gt;
      &lt;action type=""AbortRequest"" /&gt;
    &lt;/rule&gt;
    &lt;xsl:comment&gt;Hit&lt;/xsl:comment&gt;
  &lt;/xsl:template&gt;
</code></pre>

<p>When I put it in my web.config file it works fine, but when I put it in my web.production.config-file and run it with XSLT it removes the ""{REMOTE_ADDR}"" so the output looks like this:</p>

<pre><code>&lt;rule name=""adminBlockRule"" enabled=""true"" patternSyntax=""ECMAScript"" stopProcessing=""true""&gt;
          &lt;match url=""^(Admin/|Sysadmin/).*$"" ignoreCase=""true"" /&gt;
          &lt;conditions&gt;
            &lt;!--Generell--&gt;
            &lt;add input="""" pattern=""10.*.*.*"" negate=""true"" /&gt;
            &lt;add input="""" pattern=""194.103.31.*"" negate=""true"" /&gt;
          &lt;/conditions&gt;
          &lt;action type=""AbortRequest"" /&gt;
        &lt;/rule&gt;
</code></pre>

<p>Anyone know how to fix this problem?</p>

<p>I'm using version: </p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;xsl:stylesheet version=""1.0""
                xmlns:saml=""urn:dk.nita.saml20.configuration""
                xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""
                xmlns:msxsl=""urn:schemas-microsoft-com:xslt""
                exclude-result-prefixes=""msxsl saml""&gt;
  &lt;xsl:output method=""xml""
              indent=""yes""/&gt;
  &lt;xsl:strip-space elements=""*""/&gt;
</code></pre>
","transformer-model"
"52210326","java there is no newline after xml decleration","2018-09-06 18:39:20","","1","330","<xml><parsing><dom><w3c><transformer-model>","<p>After modifying an <code>xml</code> document, I have written this document to a new file. In this <code>xml</code> file, however, a line break is missing after the <code>xml</code> declaration:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8"" standalone=""no""&gt;&lt;!DOCTYPE EMail SYSTEM ""../src/email.dtd""&gt;
&lt;Mail&gt;&lt;Person&gt;John&lt;/Person&gt;
&lt;Number&gt;7&lt;/Number&gt;
&lt;/Mail&gt;

public void createDoc(File outputFile) throws FileNotFoundException, IOException, TransformerConfigurationException,
    TransformerException, TransformerFactoryConfigurationError {

    TransformerFactory transformerFactory = TransformerFactory.newInstance();
    Transformer transformer = transformerFactory.newTransformer();

    transformer.setOutputProperty(OutputKeys.DOCTYPE_SYSTEM, this.dtdName);
    transformer.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");
    transformer.transform(new DOMSource(this.doc), new StreamResult(new OutputStreamWriter(new FileOutputStream(outputFile))));
}
</code></pre>

<p>why is there no line break between the xml decl and the doctype tag? if I open the file with Notepad and want to make a line break right now, Notepad crashes. is that a bug of Java?</p>
","transformer-model"
"52166113","SERE0014: Illegal HTML character - decimal 129 exception while parsing control characters","2018-09-04 12:13:48","","0","1617","<java><character-encoding><html-parsing><transformer-model><illegal-characters>","<p>Recently in a case, I found a string that has a control character in it which we are saving into the DB and trying to create an xml and an HTML file from it. It is getting saved properly in DB and showing as follows at different locations. <br> 
1) When querying into the DB it shows name as <a href=""https://i.sstatic.net/WdsLd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WdsLd.png"" alt=""enter image description here""></a>. <br/>
2) When I copy this on notepad++ (UTF-8 encoding) it is shown as <a href=""https://i.sstatic.net/JeixF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JeixF.png"" alt=""enter image description here""></a>.<br/>
3) In Eclipse IDE, debugging mode shows it as same as DB. <br/>
4) In table records in the HTML page (apache/tomcat) and as sysout output in console shows it as simple <a href=""https://i.sstatic.net/Zn4je.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Zn4je.png"" alt=""enter image description here""></a>, which I think is preferable and intended output. <br/></p>

<p>I am able to create the XML file with some junk character in it but when I am trying to create the HTML using javax TransformerFactory with UTF-8 encoding.
<code>transformer.transform(source, result); </code> 
<br/> <strong>throws the exception ""Illegal HTML character - decimal 129 ""</strong>.
<br/>
I understand that there is some control character in the string which is not supported by UTF-8 and thus parser is throwing this exception.
<br/>
I found its references here::
<a href=""https://www.fileformat.info/info/unicode/char/0081/index.htm"" rel=""nofollow noreferrer"">https://www.fileformat.info/info/unicode/char/0081/index.htm</a></p>

<p>To resolve it I tried many things but the one which results close to the intended one is to parse the strings manually before giving it to the parser and changed it to UTF-8 string as below : <br/>
<code>
String str = new String(nodeValue.getBytes(StandardCharsets.US_ASCII), StandardCharsets.UTF_8);
str = str.replaceAll(""[^\p{ASCII}]"", """");
</code></p>

<p>This solves the issue up to a certain level but I think parsing the whole content is not preferable to remove 1 control character from a String, and this is as well converting name <a href=""https://i.sstatic.net/5mE2b.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5mE2b.png"" alt=""enter image description here""></a> to <a href=""https://i.sstatic.net/gKd5L.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gKd5L.png"" alt=""enter image description here""></a> which is not preferable, I actually want it without any change.</p>

<p>Is there any standard way to do this, so that we can get the correct output in parsed HTML? <br/><br/>
How sysout and apache tomcat's HTML page is showing it correctly? Do they handle it explicitly?</p>
","transformer-model"
"51890184","Ensure the presence of a word/token/noun in Encoder-Decoder text generation deep learning models","2018-08-17 07:16:08","","0","52","<nlp><deep-learning><lstm><transformer-model><encoder-decoder>","<p>I am stuck with a problem where in I want to ensure that specific tokens/words are produced while decoding and generating abstractive-style sentences. </p>

<p>I am working with deep learning models like LSTM and transformer model for generating short sentences(100-200 characters). I want that some words like places or nouns(like brand names) be present in the generated texts.</p>

<p>I am not sure if there has been any research on this, I couldn't really find a paper after an extensive search on it.</p>

<p>TIA, any leads or suggestions are appreciated. :)</p>
","transformer-model"
"51074006","The positional encoding in official transformer's release is different from the original paper","2018-06-28 02:52:20","53532348","1","1366","<python><tensorflow><transformer-model>","<p>In the original paper <em>Attention is all you need</em>, the positional encoding is defined as:
<a href=""https://i.sstatic.net/fr878.gif"" rel=""nofollow noreferrer"">pe</a></p>
<p>but in <a href=""https://github.com/tensorflow/models/blob/master/official/transformer/model/model_utils.py"" rel=""nofollow noreferrer"">Transformer's model_utils.py</a>, I found that the formula is different at line 53. In the paper, the <code>sin</code> and <code>cos</code> functions appear alternately according to even or single dimension, while they are continuous in the half of the dimension respectively.</p>
","transformer-model"
"50447267","XML Writer Java","2018-05-21 10:52:14","50449163","2","551","<java><xml><transformer-model><xmlstreamreader>","<p>I am trying to implement an XML writer in Java. I am able to produce the file but i have one problem. The problem is that the writer adds the symbol (&amp;)(#)(13;) when it changes line. I want to remove that.</p>

<p>Here is my code for producing the XML:</p>

<pre><code>public class WriteXMLFile {
static Encryption encryption = new Encryption();

public void constructXmlFile(ArrayList&lt;String&gt; locationHash,ArrayList&lt;String&gt; encryptedValue,ArrayList&lt;String&gt; ids) throws Exception{
    try {

        DocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance();
        DocumentBuilder docBuilder = docFactory.newDocumentBuilder();

        // root elements
        Document doc = docBuilder.newDocument();
        Element rootElement = doc.createElement(""xxxxxxx"");
        doc.appendChild(rootElement);

        // body elements
        Element body = doc.createElement(""Body"");
        rootElement.appendChild(body);

        // message elements
        Element message = doc.createElement(""Message"");
        body.appendChild(message);

        // Records elements
        Element records = doc.createElement(""Records"");
        message.appendChild(records);
        for (int counter = 0; counter &lt; ids.size(); counter++) {              
            // ID elements
            Element id = doc.createElement(""ID"");
            id.appendChild(doc.createTextNode(ids.get(counter)));
            records.appendChild(id);

            // LocationInformation elements
            Element locationInformation = doc.createElement(""LocationInformation"");
            locationInformation.appendChild(doc.createTextNode(locationHash.get(counter)));
            records.appendChild(locationInformation);

            // BeneficiaryInformation elements
            Element beneficiaryInformation = doc.createElement(""BeneficiaryInformation"");
            beneficiaryInformation.appendChild(doc.createTextNode(encryptedValue.get(counter)));
            records.appendChild(beneficiaryInformation);


         }   

        TransformerFactory transformerFactory = TransformerFactory.newInstance();
        Transformer transformer = transformerFactory.newTransformer();
        //for pretty print
        transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
        DOMSource source = new DOMSource(doc);

        //write to console or file
        StreamResult file = new StreamResult(new File(""xxxxxxxxx\\BenInformation.xml""));

        //write data
        transformer.transform(source, file);
        System.out.println(""DONE"");


      } catch (ParserConfigurationException pce) {
        pce.printStackTrace();
      } catch (TransformerException tfe) {
        tfe.printStackTrace();
      }
}
</code></pre>

<p>And here is a sample of the result</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;xxxxxxxxx&gt;
&lt;Body&gt;
    &lt;Message&gt;
        &lt;Records&gt;
            &lt;ID&gt;1368523&lt;/ID&gt;
                &lt;LocationInformation&gt;[B@15db9742&lt;/LocationInformation&gt;
                &lt;BeneficiaryInformation&gt;WQt3I/XOkZx/o1q9xzUlPhbcdp3V1TafwVK4x+roT3OsI1aZ21s6H0h7ki8peQ2tFWrLbc3gB4Gi&amp;#13;
                    AVHkbPcHyfz7pZXOhmgoE+KiruI3yCc0qUHYZCxqNoAjxB6empiBDZEwcc1Dh22mTB2ZpaUsDhpf&amp;#13;
                    m4+EVPN7e6ey66rXT7+igJ7Qp/xfvOJrIwcHqCEkgTOnubAnwRrtUw2ejPe6qw==&lt;/BeneficiaryInformation&gt;
            &lt;ID&gt;853749&lt;/ID&gt;
                &lt;LocationInformation&gt;[B@2cfb4a64&lt;/LocationInformation&gt;
                &lt;BeneficiaryInformation&gt;pnlNRJIYiEWiQIPrUQc5hwFSCQAnCiNexcCjkxT395kdPE9iEf7Tr4BZ3rYvSJoQMYhQ7kGOf6Gb&amp;#13;
                AU4QymLqMPEOla95CuQXvBSNDXVPWgxCVNmU8TOyU28USaEMEVXLyotY+mrsl3DGTjNGIH256IAS&amp;#13;
                L/h4Fch/OVoV6a/pZ9w+HL7Xwvp/g6EixIW1g22Y&lt;/BeneficiaryInformation&gt;
        &lt;/Records&gt;
    &lt;/Message&gt;
&lt;Body&gt;
</code></pre>

<p></p>

<p>As you can see the (&amp;)(#)(13;) character appears at the end of the line. How can i remove that during export?</p>
","transformer-model"
"50275655","Java Transformer setOutputProperty()","2018-05-10 14:46:42","50275935","1","2197","<java><xml><apache><xslt><transformer-model>","<p>I am currently using the following code to indent XML:</p>

<pre><code>transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
transformer.setOutputProperty(""{http://xml.apache.org/xslt}indent-amount"", ""2"");
transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""yes"");
</code></pre>

<p>This indents the code perfectly well however, I am not sure what the <code>http://xml.apache.org/xslt}indent-amount""</code> is doing. The URL is essential for the indentation. Could someone explain what this URL does and how it works?</p>

<p>Thank you! :)</p>
","transformer-model"
"50138644","How to concatenate a flowVar to Json payload in Mule Dataweave","2018-05-02 15:47:14","","0","3396","<mule><dataweave><transformer-model>","<p>incoming payload:</p>

<pre><code>{
    ""Categories"": [
        {
            ""ID"": ""5a873ca3"",
            ""Code"": ""CTY""
        }, {
            ""ID"": ""89k873c8"",
            ""Code"": ""CTY""
        }
    ]
}
</code></pre>

<p>flowVar value is an ArrayList: ([84hkj569],[6j93hl9])</p>

<p>desired output payload:</p>

<pre><code>  {
        ""Categories"": [
            {
                ""ID"": ""5a873ca3"",
                ""Code"": ""CTY""
            }, {
                ""ID"": ""89k873c8"",
                ""Code"": ""CTY""
            }, {
                ""ID"": ""84hkj569"",
                ""Code"": ""CTY""
            }, {
                ""ID"": ""6j93hl9"",
                ""Code"": ""CTY""
            }
        ]
    }
</code></pre>

<p>I couldn't find a way to do in dataweave, 
Would you please help</p>
","transformer-model"
"49566157","How to feed cross-validation targets into custom transformers in pipeline","2018-03-29 22:52:00","49566523","0","810","<python><scikit-learn><pipeline><grid-search><transformer-model>","<p>I've been wrestling with an issue getting some custom transformers to work using sklearn's Pipeline and FeatureUnion classes.  I ultimately want to use GridsSearchCV to try a number of different parameters, but I get stuck here in the beginning.  I have the pipeline below:</p>

<pre><code>feature_selection = FeatureUnion([
(""fprfeatures"", SelectFprAttrib()),
(""modelfeatures"", 
    SelectModelAttrib(clf=RandomForestClassifier(n_estimators=150), on=True)),
])

full_pipeline = Pipeline([
    (""dataselector"", DataSelector(numcolumns)),
    (""scaler"", ScalerFlip()),
    (""features"", feature_selection),
    (""estimators"",estimator_pipe),
])
</code></pre>

<p>An example of my custom class is here (they both are essentially the same):</p>

<pre><code>#Custom SelectFromModel that allows me to mess with attribute numbers and 
toggle

class SelectModelAttrib(BaseEstimator, TransformerMixin):

from sklearn.feature_selection import SelectFromModel
def __init__(self, clf, attrib_number=20, on=True):
    self.attrib_number = attrib_number
    self.clf = clf
    self.on = on
def fit(self, X, y=None):
    self.y = y
    return self
def transform(self, X):
    if self.on:
        self.model = SelectFromModel(self.clf)
        return self.model.fit_transform(X,self.y)[:,:self.attrib_number]
    else:
        return np.empty_like(X)
def get_support(self):
    return self.model.get_support()
</code></pre>

<p>if I were to call</p>

<pre><code>full_pipeline.fit(features, targets)
</code></pre>

<p>I have no issues.  In fact, if I comment-out the estimator and run the following:</p>

<pre><code>full_pipeline.fit_transform(features, targets)
</code></pre>

<p>I get an array of features back just like I intended.  However, when I run full_pipeline through GridSearchCV as so:</p>

<pre><code>#X is in format rows=instances, columns=features

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

attrib_number = ss.randint(1,100)

param_grid = {""estimators"":[SVC(kernel=""rbf""), 
    SVC(kernel=""poly""),LinearSVC(), LogisticRegression()],}

pipe_grd = GridSearchCV(full_pipeline, param_grid, cv=4, scoring = 
""accuracy"", verbose=2)
pipe_grd.fit(X_train, y_train)
</code></pre>

<p>I get the following traceback....</p>

<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-72-86f7cf8d7839&gt; in &lt;module&gt;()
     16 
     17 pipe_grd = GridSearchCV(full_pipeline, param_grid, cv=4, scoring = ""accuracy"", verbose=2, n_jobs=1)
---&gt; 18 pipe_grd.fit(X_train, y_train)
     19 #full_pipeline.predict(X_test)

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\model_selection\_search.pyc in fit(self, X, y, groups)
    943             train/test set.
    944         """"""
--&gt; 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))
    946 
    947 

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\model_selection\_search.pyc in _fit(self, X, y, groups, parameter_iterable)
    562                                   return_times=True, return_parameters=True,
    563                                   error_score=self.error_score)
--&gt; 564           for parameters in parameter_iterable
    565           for train, test in cv_iter)
    566 

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.pyc in __call__(self, iterable)
    756             # was dispatched. In particular this covers the edge
    757             # case of Parallel used with an exhausted iterator.
--&gt; 758             while self.dispatch_one_batch(iterator):
    759                 self._iterating = True
    760             else:

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.pyc in dispatch_one_batch(self, iterator)
    606                 return False
    607             else:
--&gt; 608                 self._dispatch(tasks)
    609                 return True
    610 

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.pyc in _dispatch(self, batch)
    569         dispatch_timestamp = time.time()
    570         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--&gt; 571         job = self._backend.apply_async(batch, callback=cb)
    572         self._jobs.append(job)
    573 

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\_parallel_backends.pyc in apply_async(self, func, callback)
    107     def apply_async(self, func, callback=None):
    108         """"""Schedule a func to be run""""""
--&gt; 109         result = ImmediateResult(func)
    110         if callback:
    111             callback(result)

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\_parallel_backends.pyc in __init__(self, batch)
    324         # Don't delay the application, to avoid keeping the input
    325         # arguments in memory
--&gt; 326         self.results = batch()
    327 
    328     def get(self):

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.pyc in __call__(self)
    129 
    130     def __call__(self):
--&gt; 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
    132 
    133     def __len__(self):

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\model_selection\_validation.pyc in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)
    258     else:
    259         fit_time = time.time() - start_time
--&gt; 260         test_score = _score(estimator, X_test, y_test, scorer)
    261         score_time = time.time() - start_time - fit_time
    262         if return_train_score:

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\model_selection\_validation.pyc in _score(estimator, X_test, y_test, scorer)
    286         score = scorer(estimator, X_test)
    287     else:
--&gt; 288         score = scorer(estimator, X_test, y_test)
    289     if hasattr(score, 'item'):
    290         try:

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\metrics\scorer.pyc in __call__(self, estimator, X, y_true, sample_weight)
     89         super(_PredictScorer, self).__call__(estimator, X, y_true,
     90                                              sample_weight=sample_weight)
---&gt; 91         y_pred = estimator.predict(X)
     92         if sample_weight is not None:
     93             return self._sign * self._score_func(y_true, y_pred,

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\utils\metaestimators.pyc in &lt;lambda&gt;(*args, **kwargs)
     52 
     53         # lambda, but not partial, allows help() to work with update_wrapper
---&gt; 54         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
     55         # update the docstring of the returned function
     56         update_wrapper(out, self.fn)

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\pipeline.pyc in predict(self, X)
    324         for name, transform in self.steps[:-1]:
    325             if transform is not None:
--&gt; 326                 Xt = transform.transform(Xt)
    327         return self.steps[-1][-1].predict(Xt)
    328 

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\pipeline.pyc in transform(self, X)
    761         Xs = Parallel(n_jobs=self.n_jobs)(
    762             delayed(_transform_one)(trans, name, weight, X)
--&gt; 763             for name, trans, weight in self._iter())
    764         if not Xs:
    765             # All transformers are None

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.pyc in __call__(self, iterable)
    756             # was dispatched. In particular this covers the edge
    757             # case of Parallel used with an exhausted iterator.
--&gt; 758             while self.dispatch_one_batch(iterator):
    759                 self._iterating = True
    760             else:

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.pyc in dispatch_one_batch(self, iterator)
    606                 return False
    607             else:
--&gt; 608                 self._dispatch(tasks)
    609                 return True
    610 

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.pyc in _dispatch(self, batch)
    569         dispatch_timestamp = time.time()
    570         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--&gt; 571         job = self._backend.apply_async(batch, callback=cb)
    572         self._jobs.append(job)
    573 

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\_parallel_backends.pyc in apply_async(self, func, callback)
    107     def apply_async(self, func, callback=None):
    108         """"""Schedule a func to be run""""""
--&gt; 109         result = ImmediateResult(func)
    110         if callback:
    111             callback(result)

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\_parallel_backends.pyc in __init__(self, batch)
    324         # Don't delay the application, to avoid keeping the input
    325         # arguments in memory
--&gt; 326         self.results = batch()
    327 
    328     def get(self):

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\externals\joblib\parallel.pyc in __call__(self)
    129 
    130     def __call__(self):
--&gt; 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
    132 
    133     def __len__(self):

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\pipeline.pyc in _transform_one(transformer, name, weight, X)
    565 
    566 def _transform_one(transformer, name, weight, X):
--&gt; 567     res = transformer.transform(X)
    568     # if we have a weight for this transformer, multiply output
    569     if weight is None:

&lt;ipython-input-64-e7d7de2d62c1&gt; in transform(self, X)
     37         if self.on:
     38             self.fpr = SelectFpr()
---&gt; 39             return self.fpr.fit_transform(X,self.y)
     40         else:
     41             return np.empty_like(X)

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\base.pyc in fit_transform(self, X, y, **fit_params)
    495         else:
    496             # fit method of arity 2 (supervised transformation)
--&gt; 497             return self.fit(X, y, **fit_params).transform(X)
    498 
    499 

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\feature_selection\univariate_selection.pyc in fit(self, X, y)
    320             Returns self.
    321         """"""
--&gt; 322         X, y = check_X_y(X, y, ['csr', 'csc'], multi_output=True)
    323 
    324         if not callable(self.score_func):

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\utils\validation.pyc in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    529         y = y.astype(np.float64)
    530 
--&gt; 531     check_consistent_length(X, y)
    532 
    533     return X, y

C:\Users\philg\Anaconda2\lib\site-packages\sklearn\utils\validation.pyc in check_consistent_length(*arrays)
    179     if len(uniques) &gt; 1:
    180         raise ValueError(""Found input variables with inconsistent numbers of""
--&gt; 181                          "" samples: %r"" % [int(l) for l in lengths])
    182 
    183 

ValueError: Found input variables with inconsistent numbers of samples: [14, 38]
</code></pre>

<p>From what I have gathered through putting Print functions along the pipeline, the problem focuses on the custom transformer.  I did something perhaps a little funky with self.y = y.</p>

<p>As far as I can tell, when GridSearchCV starts doing the cross-validation, the following occurs [an example]:</p>

<pre><code>#Non-cv X.shape is (65,700)
#DataSelector is called 
X.shape = (38, 700) 
y.shape =(38L,)
#It passes to SelectModelAttrib the same shapes
X.shape = (38, 500)
y.shape = (38L,)
#DataSelector is called a second time
X.shape = (14, 700)
y.shape = (38L,)
#error occurs
</code></pre>

<p>And that's the error... is there any way that I can get y to update with X in this pipeline?   If I were to do this and replace my custom transformers with the actual SelectFromModel class on sklearn, the whole thing will run.  How do they pull it off?  I looked at their source code but it was beyond me.</p>
","transformer-model"
"49352717","java.lang.NoSuchMethodException: <Class>.<init>(java.lang.String) when copying custom Transformer","2018-03-18 20:54:34","49353146","0","1506","<scala><apache-spark><apache-spark-mllib><pipeline><transformer-model>","<p>Currently playing with custom tranformers in my spark-shell using both spark 2.0.1 and 2.2.1.</p>

<p>While writing a custom ml transformer, in order to add it to a pipeline, I noticed that there is an issue with the override of the copy method.</p>

<p>The copy method is called by the fit method of the TrainValidationSplit in my case.</p>

<p>The error I get :</p>

<pre><code>java.lang.NoSuchMethodException: Custom.&lt;init&gt;(java.lang.String)
  at java.lang.Class.getConstructor0(Class.java:3082)
  at java.lang.Class.getConstructor(Class.java:1825)
  at org.apache.spark.ml.param.Params$class.defaultCopy(params.scala:718)
  at org.apache.spark.ml.PipelineStage.defaultCopy(Pipeline.scala:42)
  at Custom.copy(&lt;console&gt;:16)
  ... 48 elided
</code></pre>

<p>I then tried to directly call the copy method but I still get the same error.</p>

<p>Here is myclass and the call I perform :</p>

<pre><code>import org.apache.spark.ml.Transformer
import org.apache.spark.sql.{Dataset, DataFrame}
import org.apache.spark.sql.types.{StructField, StructType, DataTypes}
import org.apache.spark.ml.param.{Param, ParamMap}

// Simple DF
val doubles = Seq((0, 5d, 100d), (1, 4d,500d), (2, 9d,700d)).toDF(""id"", ""rating"",""views"")


class Custom(override val uid: String) extends org.apache.spark.ml.Transformer {
  def this() = this(org.apache.spark.ml.util.Identifiable.randomUID(""custom""))

  def copy(extra: org.apache.spark.ml.param.ParamMap): Custom = {
    defaultCopy(extra)
  }

  override def transformSchema(schema: org.apache.spark.sql.types.StructType): org.apache.spark.sql.types.StructType = {
    schema.add(org.apache.spark.sql.types.StructField(""trending"", org.apache.spark.sql.types.IntegerType, false))
  }

   def transform(df: org.apache.spark.sql.Dataset[_]): org.apache.spark.sql.DataFrame = {

    df.withColumn(""trending"", (df.col(""rating"") &gt; 4 &amp;&amp; df.col(""views"") &gt; 40))
  }
}


val mycustom = new Custom(""Custom"")
// This call throws the exception. 
mycustom.copy(new org.apache.spark.ml.param.ParamMap())
</code></pre>

<p>Does anyone know if this is a known issue ? I cant seem to find it anywhere.</p>

<p>Is there another way to implement the copy method in a custom transformer ?</p>

<p>Thanks</p>
","transformer-model"
"49311339","Save custom transformers in pyspark","2018-03-16 00:16:30","","4","2021","<pyspark><persistence><databricks><transformer-model>","<p>When I implement this part of this python code in Azure Databricks:</p>

<pre><code>class clustomTransformations(Transformer):
    &lt;code&gt;

custom_transformer = customTransformations()
....
pipeline = Pipeline(stages=[custom_transformer, assembler, scaler, rf])
pipeline_model = pipeline.fit(sample_data)
pipeline_model.save(&lt;your path&gt;)
</code></pre>

<p>When I attempt to save the pipeline, I get this:</p>

<p><code>AttributeError: 'customTransformations' object has no attribute '_to_java'</code></p>

<p>Any work arounds?</p>
","transformer-model"
"49300130","Konva Transformer restrict shape size","2018-03-15 13:00:01","49318180","0","1824","<html5-canvas><transformer-model><konvajs>","<p>I don't think it's possible currently to restrict the size of a shape when using Transformer to make it bigger or smaller but I really need to add this capability.</p>

<p>What is the best way to go about doing this ourselves (giving the size of shapes min/max which Transformer will honour) ? </p>

<p>Thanks.</p>
","transformer-model"
"48348027","Define a list of strings in a Datatable cell for a field inside a Cucumber step in Java","2018-01-19 19:12:56","","3","1654","<datatable><cucumber><cucumber-java><transformer-model>","<p><strong>What I want</strong></p>

<p>I am working with DataTables in Cucumber.</p>

<p>I have the following situation:</p>

<p>Step in the feature file:</p>

<pre><code>I set the ingredients necessary for a meal
  | name           | ingredients        |
  | mac and cheese | pasta,cheese       |
  | hamburger      | bread,meat,lettuce | 
</code></pre>

<p>In the StepDefinition file I have</p>

<pre><code>@When(""I set the ingredients necessary for a meal"")
public void setIngredients(List&lt;Meal&gt; meals){
  //do things with it
}
</code></pre>

<p>And I have a class Meal</p>

<pre><code>public class Meal {
  String name;
  List&lt;String&gt; ingredients;
}
</code></pre>

<p>This doesn't work.</p>

<p><strong>what I know</strong></p>

<p>If I set my ingredients field as a simple String Cucumber ""magically"" matches both name and ingredients to the fields of the class and, in the step definition I will get a List of Meals correctly filled.
But as it currently is it doesn't automatically match.</p>

<p><strong>What I tried</strong></p>

<p>I tried defining the class as:</p>

<pre><code>public class Meal {
  String name;
  String ingredients;
  List&lt;String&gt; ingredientsList;
}
</code></pre>

<ul>
<li><p>And having a constructor Meal(String, String) that would parse the ingredients into the ingredients list, but it doesn't work.</p></li>
<li><p>I tried defining a setter for ingredients that would parse it and also define the ingredientsList but it also doesn't work.</p></li>
<li><p>I tried using a DataTable in the step definition but still I can't find a way to transform it into my list of ingredients.</p></li>
<li><p>I tried using Transformer but, AFAIK, I would have to define a step for each meal I want to serve and I would have to sent the values within the step itself.</p></li>
</ul>

<p><strong>What I don't want</strong></p>

<p>I don't want to be forced to parse the information anywhere but the Meal class.</p>

<p><strong>How I temporarily solved it</strong></p>

<p>Within the more complete Meal definition, a defined a setIngredientsList() that parses the ingredients into a list.</p>

<p>On the step definition, I iterate through the list of meals and call setIngredientsList for each of them. As I said, I don't want any of this processing done outside of the Meal class.</p>

<p><strong>Question</strong></p>

<p>Does anyone know how I can do this please?</p>
","transformer-model"
"48106397","Getting the attribute value of a tag in XML with Java","2018-01-05 02:28:28","","0","1140","<java><xml><transformer-model>","<p>I am trying to replace text2 to something else with Java. I am having trouble with this code:</p>

<pre><code>try {
    String filepath = ""c:\\path\\file.xml"";
    DocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance();
    DocumentBuilder docBuilder = docFactory.newDocumentBuilder();
    Document doc = docBuilder.parse(filepath);
    Node a = doc.getFirstChild();
    Node b = doc.getElementsByTagName(""b"").item(0);
    NodeList list = b.getChildNodes();

    for (int i = 0; i &lt; list.getLength(); i++) {

               Node node = list.item(i);
       if (""c"".equals(node.getNodeName())) { //Want to add '&amp;&amp; attribute value of key is 4'
        node.setTextContent(""new text"");
       }

    }

    // write the content into xml file
    TransformerFactory transformerFactory = TransformerFactory.newInstance();
    Transformer transformer = transformerFactory.newTransformer();
    DOMSource source = new DOMSource(doc);
    StreamResult result = new StreamResult(new File(filepath));
    transformer.transform(source, result);

    System.out.println(""Done"");

   } catch (ParserConfigurationException pce) {
    pce.printStackTrace();
   } catch (TransformerException tfe) {
    tfe.printStackTrace();
   } catch (IOException ioe) {
    ioe.printStackTrace();
   } catch (SAXException sae) {
    sae.printStackTrace();
   }
</code></pre>

<p>This is what the XML looks like:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""&gt;
&lt;a&gt;
    &lt;b id=""1""&gt;
        &lt;c key=""3""&gt;text1&lt;/c&gt;
        &lt;c key=""4""&gt;text2&lt;/c&gt; //Replace this
    &lt;/b&gt;
    &lt;b id=""2""&gt;
        &lt;c key=""5""&gt;text3&lt;/c&gt;
        &lt;c key=""6""&gt;text4&lt;/c&gt;
    &lt;/b&gt;
&lt;/a&gt;
</code></pre>

<p>I got this from a guide here: <a href=""https://www.mkyong.com/java/how-to-modify-xml-file-in-java-dom-parser/"" rel=""nofollow noreferrer"">https://www.mkyong.com/java/how-to-modify-xml-file-in-java-dom-parser/</a>
and is also currently giving me an error on this line <code>transformer.transform(source, result);</code></p>
","transformer-model"
"47799416","HDF5Data Processing with Caffe's Transformer for training","2017-12-13 17:55:08","","1","74","<python><io><caffe><hdf5><transformer-model>","<p>I am trying to load data to the network, since I need a custom data input (3 tops: 1 for data image, 2 for different labels) I load the data with HD5F files. It looks similar to this:</p>

<pre><code>layer {
  name: ""data""
  type: ""HDF5Data""
  top: ""img""
  top: ""alabels""
  top: ""blabels""
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: ""path_to_caffe/examples/hdf5_classification/data/train.txt""
    batch_size: 64
  }
} 
</code></pre>

<p>I want to preprocess the images using Caffe's own Transformer (for standard), how can I do this when I have to initialize the Transformer with data blob of a network model:</p>

<pre><code>transformer = caffe.io.Transformer({'data': net.blobs['data'].data.shape})
</code></pre>

<p>All the examples I have found first loads a pre-trained net and sets a transformer with its data blob/shape, but I couldn't find any for training. Thanks.</p>
","transformer-model"
"47417750","How to convert XSLT to plain text with encoded data?","2017-11-21 16:16:23","47419618","0","466","<java><xml><xslt><transformer-model>","<p>I am trying to convert a XSLT file to plain text. The problem is XSLT file has encoded values e.g, &amp; is  <code>&amp;amp;</code> and Transformer factory outputs encoded value. Is there a better other than using XML decoder and then covert to plain text?</p>

<pre><code>TransformerFactory tFactory = TransformerFactory.newInstance();
Transformer transformer = tFactory.newTransformer(new StreamSource(XSLT_Source));       
transformer.setOutputProperty(""media-type"", ""text/plain"");
transformer.transform(new StreamSource(XSLT_Source), new StreamResult(new FileOutputStream(outputTrager)));  
</code></pre>
","transformer-model"
"47341723","Javax Transformer fails on high concurrent environment","2017-11-17 01:06:27","47347485","0","1686","<java><xslt><singleton><transformer-model>","<p>I have a platform with a Singleton Bean where I load a Transformer instance (because the xslt is so big to create an instance on every request) .</p>

<p>At normal load everything works fine , but making a stress test with a huge number of concurrent requests the transformer starts to throw an ArrayIndexOutOfBoundsException and stops working so I have to restart my server instance or redeploy the application.</p>

<p>This is the way I create my instance when the application is deployed:</p>

<pre><code>private Transformer createCFDI33TransformerInstance() {

            InputStream in = new URL(""http://www.sat.gob.mx/sitio_internet/cfd/3/cadenaoriginal_3_3/cadenaoriginal_3_3.xslt"").openStream();

            TransformerFactory factory = TransformerFactory.newInstance();            
            Transformer transformer
                    = factory.newTransformer(new StreamSource(in));
            Logger.getLogger(PadeSingleton.class.getName()).log(Level.INFO, "" Se ha cargado la instancia de XSLT"");
            return transformer;
        } catch (TransformerConfigurationException | IOException ex) {
// Loading a remote instance was not possible so I will load a local instance
            Logger.getLogger(PadeSingleton.class.getName()).log(Level.SEVERE, ""No fue posible cargar una nueva instancia de cadena original, se usara la del sistema"", ex);
            InputStream in = CFDIv33Tools.class.getClassLoader()
                    .getResourceAsStream(""com/soft/cadenaoriginal_3_3.xslt"");

            TransformerFactory factory = TransformerFactory.newInstance();
            Transformer transformer;
            try {
                transformer = factory.newTransformer(new StreamSource(in));
                Logger.getLogger(PadeSingleton.class.getName()).log(Level.WARNING, ""No fue posible cargar la instancia, se cargara una local"");
            } catch (TransformerConfigurationException ex1) {
                Logger.getLogger(PadeSingleton.class.getName()).log(Level.SEVERE, null, ex1);
                // Estamos en problemas 
                throw new Exception(""Error critico"");
            }
            return transformer;
        }
    }
</code></pre>

<p>So in other bean I inject my Singleton Bean and call a method to get my Transformer instance.</p>

<pre><code>public static String transform(String xml, Transformer instance) throws Exception {

        StringWriter writer = new StringWriter();

        try {

            instance.transform(new StreamSource(new StringReader(xml)), new StreamResult(writer));

        } catch (TransformerException e) {
            throw new Exception(""El comprobante contiene simbolos no permitidos o esta mal formado"", e);
        }

        return writer.toString();
    }
</code></pre>

<p>In  this method is where I get an ArrayIndexOutOfBoundsException </p>

<pre><code>Caused by: javax.xml.transform.TransformerException: java.lang.ArrayIndexOutOfBoundsException: -1
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:746)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:351)
    at com.icontech.pade.common.xml.tools.CFDIv33Tools.calcularCadenaOriginalV2(CFDIv33Tools.java:93)
    ... 138 more
</code></pre>

<p>I repeat this happens when I making a stress test, when this exception is throwed the following requests fails with this same exception. I suppose my instance is corrupted.. or something.</p>
","transformer-model"
"46408139","How to pass all Eloquent (ORM) results throught a Laravel dataTransformer?","2017-09-25 14:49:07","46408420","0","533","<php><laravel><eloquent><transformer-model>","<p>I getting results from a query built using Eloquent (Laravel's ORM)</p>

<pre><code>$query = Lawyer::whereHas('user', function($q) use ($request) {
            $q-&gt;where('is_active', true);
        });
$result = $query-&gt;get()
</code></pre>

<p>I would like to pass the results I get throught a trasformer <code>class LawyerTransformer extends TransformerAbstract{}</code> to add some data to the results.</p>

<p>When I try this :</p>

<pre><code>$this-&gt;collection($query-&gt;get(), new LawyerTransformer())
</code></pre>

<p>I have the following issue : <code>Method [collection] does not exist.</code></p>

<p><strong>How can I transform all the results using a transformer ?</strong></p>
","transformer-model"
"46199395","ibm cognos transformer multiple fact table not supported by dimension","2017-09-13 13:50:31","","1","114","<cognos-10><transformer-model>","<p>i'm actually working on a solution that have two fact tables, and i need to add one more fact table
so the etl work fine
i add my new table to the pack in ibm framework manager , and i make relations between my fact table and other tables ""tables that are used for dimensions in transformer""
i make some tests in framework manager the relations are goud 
<a href=""https://i.sstatic.net/7FQbK.jpg"" rel=""nofollow noreferrer"">test relation between the fact table and the dimensions</a></p>

<p>when i create the cube i cant find aggregation between dimensions and my  new fact table it give the total sum of my fact table.
when i search suported  data sources in dimension i realize that the my fact table  is not supported by the dimensions
<a href=""https://i.sstatic.net/AePXc.jpg"" rel=""nofollow noreferrer"">my fact table not supported</a>
i dont know how to make that relation in transformer......</p>

<p>any help </p>

<p>sincerely </p>

<p>hope you have a goud day </p>
","transformer-model"
"45550155","XML - Trying to grasp the concept of namespace URI","2017-08-07 15:06:10","45551862","0","658","<xml><url><uri><transformer-model><urn>","<p>If my XSL have the following heading, I get no error :</p>

<pre><code>&lt;xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""
</code></pre>

<p>But if I change the xsl namespace URI... :</p>

<pre><code>&lt;xsl:stylesheet version=""1.0"" xmlns:xsl=""https://www.w3schools.com/xml/xml_namespaces.asp""
</code></pre>

<p>...I get an error :</p>

<pre><code>javax.xml.transform.TransformerConfigurationException 
</code></pre>

<p>I think I do not understand the concept of a namespace URI, because I though that it could be any URL (not to mention URNs) as long as it was unique in the XML/XSL document. Obviously not. I tried to change the URI to test this theory.</p>

<hr>

<p>At <a href=""https://www.w3schools.com/xml/xml_namespaces.asp"" rel=""nofollow noreferrer"">https://www.w3schools.com/xml/xml_namespaces.asp</a>, we can read the following statements :</p>

<blockquote>
  <p>The namespace URI is not used by the parser to look up information.</p>
  
  <p>The purpose of using an URI is to give the namespace a unique name.</p>
  
  <p>However, companies often use the namespace as a pointer to a web page containing namespace information.</p>
</blockquote>

<p>The first statement means that we don't care about what is the web page pointed by the URI, so why can't I set it to point any page I want, or even a page that does not exist ? Why does it have to specially be <a href=""http://www.w3.org/1999/XSL/Transform"" rel=""nofollow noreferrer"">http://www.w3.org/1999/XSL/Transform</a> ?</p>

<p>Furthermore, later on the above link we can read :</p>

<blockquote>
  <p>A Uniform Resource Identifier (URI) is a string of characters which <strong>identifies an Internet Resource</strong>.</p>
</blockquote>

<p>So, because  of this and because ""The purpose of using an URI is to give the namespace a unique name"", does it mean that <strong>a namespace URI has to be an URL</strong> (again, no to mention URNs which I absolutely can't fathom), which means that I can't set a namespace like this : </p>

<pre><code>xlmns:foo=""A_random_but_unique_string""
</code></pre>

<p>I would finish with this quote from the same link :</p>

<blockquote>
  <p>The namespace ""<a href=""http://www.w3.org/1999/XSL/Transform"" rel=""nofollow noreferrer"">http://www.w3.org/1999/XSL/Transform</a>"" identifies XSLT elements inside an HTML document</p>
</blockquote>

<p>So now it means that the parser must certainly use the namespace URL to look up for information (the elements of the namespace), no ?</p>
","transformer-model"
"45240990","Jxls: Transformer is null","2017-07-21 15:06:25","","0","1499","<java><android><maven><transformer-model><jxls>","<p>I am using Jxls to write data into an excel file. </p>

<p>For that I have a xls template <code>R.raw.object_collection_xmlbuilder_template_products_list</code> which is in <code>*/res&gt;raw</code> folder and xml builder file <code>R.xml.excel_template_products_list</code> is in <code>*/res&gt;xml</code> folder .</p>

<pre><code>try (InputStream is = mContext.getResources().openRawResource(R.raw.object_collection_xmlbuilder_template_products_list)) {
                try (OutputStream os = new FileOutputStream(excel_file)) {
                    Transformer transformer = TransformerFactory.createTransformer(is, os);

                    try (InputStream configInputStream = mContext.getResources().openRawResource(+R.xml.excel_template_products_list)) {

                        AreaBuilder areaBuilder = new XmlAreaBuilder(configInputStream, transformer);
                        List&lt;Area&gt; xlsAreaList = areaBuilder.build();
                        Area xlsArea = xlsAreaList.get(0);

                        org.jxls.common.Context context = new org.jxls.common.Context();
                        context.putVar(""products"", hashmap_list);

                        xlsArea.applyAt(new CellRef(""Result!A1""), context);
                        transformer.write();
                    }
                }
            } catch (Exception ex) {
                    aLog.error(ex);
        }
</code></pre>

<p>But Transformer Object is returning as null.
And for line <code>List&lt;Area&gt; xlsAreaList = areaBuilder.build();</code>, it throws :</p>

<pre><code>Method threw 'java.lang.NoClassDefFoundError'
exception.org.jxls.builder.xml.AreaAction
</code></pre>

<p>These are the <code>jar</code> files I had added:</p>

<pre><code>'libs/jxls-2.4.0.jar'
'libs/jxls-poi-1.0.12.jar'
'libs/slf4j.api-1.6.1.jar'
</code></pre>

<p>One solution suggested that I add the maven dependency, like so:</p>

<pre><code>maven {
        url ""https://repo1.maven.org/maven2/""
    }
</code></pre>

<p>But I still get the exception.</p>

<p>So, any idea why could the transformer be null and how can I fix the exception ? Thank you.</p>

<p>Edit:
This is xml content:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""utf-8""?&gt;
&lt;xls&gt;
    &lt;area ref=""Template!A1:T31""&gt;
        &lt;each items=""products"" var=""products"" ref=""Template!A4:T31""&gt;
            &lt;area ref=""Template!A4:T31""/&gt;
        &lt;/each&gt;
    &lt;/area&gt;
&lt;/xls&gt;
</code></pre>

<p>This is the xls template:<a href=""https://i.sstatic.net/YLx3k.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YLx3k.png"" alt=""enter image description here""></a></p>
","transformer-model"
"45224592","Jxls : How to define template programmatically and use in transformer?","2017-07-20 20:27:11","","1","813","<java><excel><templates><transformer-model><jxls>","<p>I am using Jxls to output data to excel. The no. of columns is not fixed and depends on who is calling it. So how to define template programatically?</p>

<p>In Jxls documentation, there are three ways of doing it, using excel markup, xml mark up and Java Apis. Using Java Apis, XmlArea is defined but one of the constructor takes transformer as a parameter</p>

<pre><code>public XlsArea(AreaRef areaRef, Transformer transformer);

 public XlsArea(String areaRef, Transformer transformer);

 public XlsArea(CellRef startCell, CellRef endCell, Transformer transformer);

 public XlsArea(CellRef startCellRef, Size size, List&lt;CommandData&gt; commandDataList, Transformer transformer);
</code></pre>

<p><code>Transformer constructor</code> requires a <code>inputstream</code>, which requires a <code>template</code> file.</p>

<pre><code>InputStream in = IdGenre.class.getResourceAsStream(""/xlsTemplates/IdGenre/IdGenreTemplate.xlsx"");

Transformer transformer = TransformerFactory.createTransformer(in, out);
</code></pre>

<p>From what I understand, <code>template</code> is where it is defined what value goes in columns.</p>

<p>Using <code>SimpleExporter</code>:</p>

<pre><code>try(OutputStream os1 = new FileOutputStream(""target/simple_export_output1.xls"")) {
        List&lt;Employee&gt; employees = generateSampleEmployeeData();
        List&lt;String&gt; headers = Arrays.asList(""Name"", ""Birthday"", ""Payment"");
        SimpleExporter exporter = new SimpleExporter();
        exporter.gridExport(headers, employees, ""name, birthDate, payment"", os1);
</code></pre>

<p><code>It requires a transformer and throws exception.</code></p>

<pre><code>java.lang.NullPointerException: Attempt to invoke interface method 'java.util.List org.jxls.transform.Transformer.getCommentedCells()' on a null object reference
</code></pre>

<p>How can I create a template programmatically or how can work with <code>SimpleExporter</code>? Any help is much appreciated. Thank you.</p>
","transformer-model"
"44941773","Can I use transformers to transform data coming from API rather than from database?","2017-07-06 06:56:32","45076360","11","5938","<php><laravel><transformer-model><dingo-api><thephpleague-fractal>","<p>I have been using laravel to build my APIs. I use transformers to tranform data from model object.</p>

<p>Now instead of database, I have a response coming from an API as the data source and I want to transform that data back to a user, but I am unable to do so. </p>

<p><strong>My Controller</strong></p>

<pre><code> public function rocByName(Request $request)
    {
        try {
            $this-&gt;roc_by_name_validator-&gt;with( $request-&gt;all() )-&gt;passesOrFail();
            $company_name = $request-&gt;input('company_name');
            $result = $this-&gt;my_service-&gt;getDetailsByName($company_name); //$result has the response object from the API which I want to transform and give it as a response.

             return $this-&gt;response-&gt;collection($result,new OnboardingTransformer()); //Tried using tranformer like this
        }
        catch (ValidatorException $e) {

            dd($e);

        }

    }
</code></pre>

<p><strong>My Transformer</strong></p>

<pre><code>&lt;?php

namespace Modules\Onboarding\Transformers;

use League\Fractal\TransformerAbstract;
use App\Entities\OnboardingEntity;  //I dont have an entity since the response is coming from an API!! What will give here?

/**
 * Class OnboardingTransformerTransformer
 * @package namespace App\Transformers;
 */
class OnboardingTransformer extends TransformerAbstract
{

    /**
     * Transform the \OnboardingTransformer entity
     * @param \OnboardingTransformer $model
     *
     * @return array
     */
    public function transform(OnboardingEntity $data_source) 
    {
        return [
            'company_name'         =&gt; $data_source-&gt;company_name,
        ];
    }
}
</code></pre>

<p>Here the OnboardingEntity refers to data coming from database ideally. Here I am not fetching data from database, instead my data is from an API source. How do I go about it. I am little consfused here. Can someone give a solution? </p>

<p>$result has the following response </p>

<pre><code>[
    [
        {
            ""companyID"": ""U72400MHTC293037"",
            ""companyName"": ""pay pvt LIMITED""
        },
        {
            ""companyID"": ""U74900HR2016PT853"",
            ""companyName"": ""dddd PRIVATE LIMITED""
        }
    ]
]
</code></pre>
","transformer-model"
"44833854","How to remove all the properties (INVOCATION/INBOUND/OUTBOUND/SESSION) in a specific scope in mule","2017-06-29 19:52:48","44838953","0","1150","<mule><transformer-model>","<p>I am having a problem where my rabbit mq message put is erroring and it does not tell me what the root cause for the put failure is. Looking at stackoverflow, I see that my problem is identical to what was reported here-<a href=""https://stackoverflow.com/questions/24674040/to-store-a-message-in-rabbitmq-from-mule"">to store a message in rabbitmq from mule</a>-as far as exception goes. 
Now I am trying to find out if there is a way to remove all the properties present in the message or if the removal can be performed per scope (i.e. INVOCATION/INBOUND/OUTBOUND/SESSION). Reason being, I dont know which property, if at all, is causing this.</p>

<p>I did look at <code>&lt;remove-property&gt;</code> &amp; <code>&lt;message-properties-transformer&gt;</code> but they work on a specific property only (&amp; not all).</p>
","transformer-model"
"44431563","Create a custom Transformer In Java spark ml","2017-06-08 09:18:19","","3","3345","<java><scala><apache-spark><apache-spark-mllib><transformer-model>","<p>I want to create a custom Spark Transformer in Java.</p>

<p>The Transformer is text preprocessor which acts like a Tokenizer. It takes an input column and an output column as parameters.</p>

<p>I looked around and I found 2 Scala Traits HasInputCol and HasOutputCol. </p>

<p>How can I create a class that extends Transformer and implements HasInputCol and OutputCol?</p>

<p>My goal is have something like this.</p>

<pre><code>   // Dataset that have a String column named ""text""
   DataSet&lt;Row&gt; dataset;

   CustomTransformer customTransformer = new CustomTransformer();
   customTransformer.setInputCol(""text"");
   customTransformer.setOutputCol(""result"");

   // result that have 2 String columns named ""text"" and ""result""
   DataSet&lt;Row&gt; result = customTransformer.transform(dataset);
</code></pre>
","transformer-model"
"44303090","Create composite transformer spark","2017-06-01 09:05:06","44307183","0","108","<apache-spark><composition><transformer-model>","<p>I am using an <code>NGram</code> <code>Transformer</code> then a <code>CountVectorizerModel</code>.</p>

<p>I need to be able to create a composite transformer for reuse later.</p>

<p>I was able to achieve this by making a <code>List&lt;Transformer&gt;</code> and looping through all elements but I want to know if it is possible to create a <code>Transformer</code> using 2 other <code>Transformer</code></p>
","transformer-model"
"44163943","Very Simple Mirth Functionality (Version 3.1.1.7461)","2017-05-24 16:29:44","","1","478","<javascript><mirth><transformer-model>","<p>Ok, trying to get the basics down as far as how Mirth interacts with the data.  Simple script below checking for a value and setting outbound to a hardcoded value when finished.  This is not a real life scenario, so please don't get hung up on the why....  When running this script, I receive an error:</p>

<pre><code>[2017-05-24 02:34:34,845]  ERROR (transformer:?): TypeError: Cannot read property ""EVN.1"" from undefined.
</code></pre>

<p>This must be something simple, but could use some interaction if anyone cares to share.  It seems to not want to identify my HL7.</p>

<p>Java Script</p>

<pre><code>var full_evn1 = msg['EVN']['EVN.1']['EVN.1.1'].toString();

if (full_evn1 = ""A01"" ) {
  tmp['EVN']['EVN.5']['EVN.5.1'] = ""MYID"" 
}
</code></pre>
","transformer-model"
"44052731","Websphere - Transformer.setParameter not working","2017-05-18 16:03:42","44137484","0","359","<java><xml><xslt><websphere><transformer-model>","<p>I have an XSLT which i use to transform an XML using Java. The code is working fine when i run it in eclipse and use Apache Tomcat. But when i deploy the ear file to WebSphere, the field is showing as blank. Does anyone have ideas? </p>

<p>The java variables 'reportId' and 'proposalId' are set as i used System.out.println() and could see the value is set.</p>

<p>Java Code
    // Use the factory to create a template containing the xsl file</p>

<pre><code>Templates template = factory.newTemplates(new StreamSource(is));              

// Use the template to create a transformer
Transformer xformer = template.newTransformer();
xformer.setParameter(""reportId"", reportId);
xformer.setParameter(""proposalId"", proposalId);


&lt;xsl:param name=""proposalId""/&gt;
&lt;xsl:param name=""reportId""/&gt;
</code></pre>

<p>I then use the following in the XSLT to read the parameter:</p>

<p><code>&lt;td align=""left""&gt;&lt;b&gt;Proposal Ref: &lt;/b&gt; &lt;xsl:value-of select=""$proposalId""/&gt;
    &lt;/td&gt;
    &lt;td align=""left""&gt;&lt;b&gt;Report Id: &lt;/b&gt; &lt;xsl:value-of select=""$reportId""/&gt;
    &lt;/td&gt;</code></p>
","transformer-model"
"44044992","Writing to file using getFilesDir() giving 'Read-only file system' error (Android)","2017-05-18 10:16:10","","2","1199","<java><android><xml><internal-storage><transformer-model>","<p>I want to write to my application's internal file directory but I am getting the following error:</p>

<p><code>java.io.FileNotFoundException: com.android.internal.os.AndroidPrintStream@47a0990 (Read-only file system)</code></p>

<p>So it says it is 'read-only' storage. I am using <code>getFilesDir()</code> to retrieve the path and  I have seen lots of examples online of people using this to write to the internal storage. Why am I seeing this error?</p>

<p>My code is below:</p>

<pre><code>DocumentBuilderFactory dbFactory = DocumentBuilderFactory.newInstance();
DocumentBuilder dBuilder = dbFactory.newDocumentBuilder();
Document doc = dBuilder.newDocument();

// Root element
Element rootElement = doc.createElement(""license"");
doc.appendChild(rootElement);

// Write content to XML file.
TransformerFactory transformerFactory = TransformerFactory.newInstance();
Transformer transformer = transformerFactory.newTransformer();
DOMSource source = new DOMSource(doc);
StreamResult result = new StreamResult(new File(context.getFilesDir().getPath() + ""/license.xml""));
System.out.println(""Saving to: "" + context.getFilesDir().getPath());
transformer.transform(source, result);
</code></pre>

<p>The exception is thrown at the final line of my code.</p>

<p>I have the following permissions in my Manifest: </p>

<pre><code>&lt;uses-permission android:name=""android.permission.WRITE_INTERNAL_STORAGE"" /&gt;
&lt;uses-permission android:name=""android.permission.WRITE_EXTERNAL_STORAGE"" /&gt;
&lt;uses-permission android:name=""android.permission.READ_EXTERNAL_STORAGE"" /&gt;
</code></pre>

<p>Note, I don't want to write to external storage as this file should not be visible to users.</p>

<p>I can write to the same directory using a <code>FileOutputStream</code> without it throwing any exception...so why does the <code>Transformer</code> encounter this read-only issue?</p>
","transformer-model"
"43976347","Symfony - Data transformer doesn't do anything?","2017-05-15 09:50:55","","1","1364","<php><forms><symfony><tags><transformer-model>","<p>I have been working on this for a solid week now and I just don't really know how to do it. So hopefully you can give me some pointers. </p>

<p>The image below shows how I can upload a file and add as many tags as I want to it. The entity MainMedia tags field has a many to many relationship with the Tag entity.<a href=""https://i.sstatic.net/yftHF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yftHF.png"" alt=""enter image description here""></a></p>

<p>MainMedia entity:</p>

<p>class MainMedia
   {
    /**
     * @var int
     *
     * @ORM\Column(name=""id"", type=""integer"")
     * @ORM\Id
     * @ORM\GeneratedValue(strategy=""AUTO"")
     */
    private $id;</p>

<pre><code>/**
 * @var string
 *
 * @ORM\Column(name=""title"", type=""string"", length=255)
 */
private $title;

/**
 * @var string
 *
 * @ORM\Column(name=""fileName"", type=""string"", length=255)
 */
private $fileName;

/**
 * @var Tag[]|ArrayCollection
 *
 * @ORM\ManyToMany(targetEntity=""Tag"", cascade={""persist""})
 * @ORM\JoinTable(name=""tags"")
 * @ORM\Column(name=""tags"")
 *
 */
private $tags;

/**
 * @var bool
 *
 * @ORM\Column(name=""public"", type=""boolean"")
 */
private $public;

/**
 * @var string
 *
 * @ORM\Column(name=""country"", type=""string"", length=255)
 */
private $country;

/**
 * @var string
 *
 * @ORM\Column(name=""publicationNumber"", type=""decimal"", scale=1, nullable=true)
 */
private $publicationNumber;

/**
 * @var int
 *
 * @ORM\Column(name=""leafletCode"", type=""integer"", nullable=true)
 */
private $leafletCode;

/**
 * @var \MyThorluxBundle\Entity\MediaType
 *
 * @ORM\ManyToOne(targetEntity=""MyThorluxBundle\Entity\MediaType"")
 * @ORM\JoinColumns({
 *   @ORM\JoinColumn(name=""type"", referencedColumnName=""id"")
 * })
 */
private $type;

/**
 * @var \DateTime
 *
 * @ORM\Column(name=""dateCreated"", type=""datetime"")
 */
private $dateCreated;

/**
 * @var \DateTime
 *
 * @ORM\Column(name=""dateModified"", type=""datetime"")
 */
private $dateModified;

// ...

/**
 * @ORM\Column(type=""string"")
 *
 * @Assert\NotBlank(message=""Please, upload the file"")
 * @Assert\File(mimeTypes = {""application/pdf"", ""image/png"", ""jpeg"", ""image/bmp"", ""application/msword"", ""video/mp4"", ""text/csv""})
 *  maxSize = ""1024k"",
 *
 */
private $file;

/**
 * Get id
 *
 * @return int
 */
public function getId()
{
    return $this-&gt;id;
}

/**
 * Set title
 *
 * @param string $title
 *
 * @return MainMedia
 */
public function setTitle($title)
{
    $this-&gt;title = $title;

    return $this;
}


/**
 * Get title
 *
 * @return string
 */
public function getTitle()
{
    return $this-&gt;title;
}

/**
 * Set fileName
 *
 * @param string $fileName
 *
 * @return MainMedia
 */
public function setFileName($fileName)
{
    $this-&gt;fileName = $fileName;

    return $this;
}

/**
 * Get fileName
 *
 * @return string
 */
public function getFileName()
{
    return $this-&gt;fileName;
}

/**
 * Set public
 *
 * @param boolean $public
 *
 * @return MainMedia
 */
public function setPublic($public)
{
    $this-&gt;public = $public;

    return $this;
}

/**
 * Get public
 *
 * @return bool
 */
public function getPublic()
{
    return $this-&gt;public;
}

/**
 * Set country
 *
 * @param string $country
 *
 * @return MainMedia
 */
public function setCountry($country)
{
    $this-&gt;country = $country;

    return $this;
}

/**
 * Get country
 *
 * @return string
 */
public function getCountry()
{
    return $this-&gt;country;
}

/**
 * Set publicationNumber
 *
 * @param string $publicationNumber
 *
 * @return MainMedia
 */
public function setPublicationNumber($publicationNumber)
{
    $this-&gt;publicationNumber = $publicationNumber;

    return $this;
}

/**
 * Get publicationNumber
 *
 * @return string
 */
public function getPublicationNumber()
{
    return $this-&gt;publicationNumber;
}

/**
 * Set leafletCode
 *
 * @param integer $leafletCode
 *
 * @return MainMedia
 */
public function setLeafletCode($leafletCode)
{
    $this-&gt;leafletCode = $leafletCode;

    return $this;
}

/**
 * Get leafletCode
 *
 * @return int
 */
public function getLeafletCode()
{
    return $this-&gt;leafletCode;
}

/**
 * Set type
 *
 * @param string $type
 *
 * @return MainMedia
 */
public function setType($type)
{
    $this-&gt;type = $type;

    return $this;
}

/**
 * Get type
 *
 * @return string
 */
public function getType()
{
    return $this-&gt;type;
}

/**
 * Set dateCreated
 *
 * @param \DateTime $dateCreated
 * @ORM\PrePersist
 * @return MainMedia
 */
public function setCreatedAt()
{

    if(!$this-&gt;dateCreated){
        $this-&gt;dateCreated = new \DateTime();
    }

    return $this;
}

/**
 * Get dateCreated
 *
 * @return \DateTime
 */
public function getDateCreated()
{
    return $this-&gt;dateCreated;
}

/**
 * Set dateModified
 *
 * @param \DateTime $dateModified
 * @ORM\PrePersist
 * @return MainMedia
 */
public function setUpdatedAt()
{
    $this-&gt;dateModified = new \DateTime();

    return $this;
}

/**
 * Get dateModified
 *
 * @return \DateTime
 */
public function getDateModified()
{
    return $this-&gt;dateModified;
}

public function __construct()
{
    $this-&gt;types = new ArrayCollection();
    $this-&gt;tags = new ArrayCollection();
}

public function addType(MediaType $mediaType)
{
    $mediaType-&gt;setMediaType($this);
    $this-&gt;types-&gt;add($mediaType);

    return $this;
}

public function getFile()
{
    return $this-&gt;file;
}

public function setFile($file)
{
    $this-&gt;file = $file;

    return $this;
}

public function addTags(Tag $tags)
{
    $this-&gt;tags-&gt;add($tags);
}

public function removeTag(Tag $tags)
{
    $this-&gt;tags-&gt;removeElement($tags);
}

public function getTags()
{
    return $this-&gt;tags;
}
}
</code></pre>

<p>Tag entity:</p>

<pre><code>class Tag
{
/**
 * @var int
 *
 * @ORM\Column(name=""id"", type=""integer"")
 * @ORM\Id
 * @ORM\GeneratedValue(strategy=""AUTO"")
 */
private $id;

/**
 * @var string
 *
 * @ORM\Column(name=""name"", type=""string"")
 */
private $name;

/**
 * @var \DateTime
 *
 * @ORM\Column(name=""dateCreated"", type=""datetime"")
 */
private $dateCreated;

/**
 * @var \DateTime
 *
 * @ORM\Column(name=""dateModified"", type=""datetime"")
 */
private $dateModified;

/**
 * Get name
 *
 * @return string
 */

public function getName()
{
    return $this-&gt;name;
}

/**
 * Set name
 *
 * @param string $name
 *
 * @return Tag
 */
public function setName($name)
{
    $this-&gt;name = $name;
}
</code></pre>

<p>I then have 2 forms, TagType and AddMedia. The AddMedia form tags field uses the TagType form. In the TagType form as shown below I use the datatransformer. </p>

<pre><code>class AddMedia extends AbstractType
{
public function buildForm(FormBuilderInterface $builder, array $options)
{

    $builder
        -&gt;add('file', FileType::class, array('label' =&gt; 'File Upload: ', 
'data_class' =&gt; null))
        -&gt;add('title')
        -&gt;add('fileName')
        -&gt;add('public')
        -&gt;add('country')
        -&gt;add('type', EntityType::class, array(
            'class' =&gt; 'MyThorluxBundle\Entity\MediaType',
            'choice_label' =&gt; function (MediaType $mediaType) {
                return $mediaType-&gt;getMediaType();
            }))
        -&gt;add('publicationNumber')
        -&gt;add('leafletCode')
        -&gt;add('tags', CollectionType::class, array(
            'entry_type' =&gt; TagType::class,
            'allow_add' =&gt; true,
            'by_reference' =&gt; false,
            'allow_delete' =&gt; true,
            'label' =&gt; false,
        ))
        -&gt;add('Submit', SubmitType::class);

}
public function configureOptions(OptionsResolver $resolver)
{
    $resolver-&gt;setDefaults(array(
        'data_class' =&gt; MainMedia::class,
    ));
}
}


   class TagType extends AbstractType
   {

private $manager;

public function __construct(ObjectManager $manager)
{
    $this-&gt;manager = $manager;
}

public function buildForm(FormBuilderInterface $builder, array $options)
{
    $builder-&gt;add('name', TextType::class, [
        'label' =&gt; 'Tag: ',
    ]);

    $builder-&gt;get('name')
        -&gt;addModelTransformer(new CollectionToArrayTransformer(), true)
        -&gt;addModelTransformer(new TagArrayToStringTransformer($this-&gt;manager), true);
}

public function configureOptions(OptionsResolver $resolver)
{
    $resolver-&gt;setDefaults(array(
        'data_class' =&gt; Tag::class,
    ));
}
</code></pre>

<p>Transformer:</p>

<p>I cant use code tags it is so broken, it just wont post the code indented. <a href=""https://i.sstatic.net/rW9k1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rW9k1.png"" alt=""enter image description here""></a></p>

<pre><code>    /**
 * Load media database add page
 *
 * @Route(""/media-database-add"", name=""media_add_route"")
 * @Template()
 * @return array
 */
public function mediaAddAction(Request $request)
{
    $mediaType = new MediaType();
    $media = new MainMedia();
    $media-&gt;addType($mediaType);

    $manager = $this-&gt;getDoctrine()-&gt;getManager();
    $mediaForm = $this-&gt;createForm(AddMedia::class, $media);

    $mediaForm-&gt;handleRequest($request);
    if ($mediaForm-&gt;isSubmitted() &amp;&amp; $mediaForm-&gt;isValid()) {

        $file = $media-&gt;getFile();
        $fileName = $this-&gt;get('file_uploader')-&gt;upload($file);

        $media-&gt;setFile($fileName);
        $em = $this-&gt;getDoctrine()-&gt;getManager();
        $em-&gt;persist($media);
        $em-&gt;flush();

        $this-&gt;addFlash(
            'success-upload',
            'Successfully uploaded ' . $media-&gt;getTitle() . ' to the intranet'
        );

        return $this-&gt;redirectToRoute('media_add_route');
    }

    return [
        'mediaForm' =&gt; $mediaForm-&gt;createView()
    ];
}
</code></pre>

<p>The above controller allows me to upload the files into the database. This is not where i get the error.</p>

<p>The error I get is on the page where I try to edit a file uploaded, so I should be able to edit its tags. </p>

<p>Controller:</p>

<pre><code>`    /**
 * edit media database page
 *
 * @Route(""/media-database-update/edit/{id}"", name=""media_update_route"")
 * @Template(""MyThorluxBundle:Database:mediaUpdate.html.twig"")
 * @param Request $request
 * @param MainMedia $media
 * @return array
 */
public function updateAction(MainMedia $media,Request $request)
{
    $editForm = $this-&gt;createForm(AddMedia::class, $media);

    $editForm-&gt;handleRequest($request);

    if ($editForm-&gt;isSubmitted() &amp;&amp; $editForm-&gt;isValid()) {
        $em = $this-&gt;getDoctrine()-&gt;getManager();
        $media-&gt;setUpdatedAt();
        $em-&gt;persist($media);
        $em-&gt;flush();

        $this-&gt;addFlash(
            'media-updated',
            'Successfully updated ' . $media-&gt;getTitle()
        );
        return $this-&gt;redirectToRoute('media_database_route');
    }

    return ['editForm' =&gt; $editForm-&gt;createView()];
}`
</code></pre>

<p>Twig:</p>

<pre><code>`    &lt;div class=""content-padding""&gt;
    &lt;h1&gt;Update Media&lt;/h1&gt;

    {% form_theme editForm 'MyThorluxBundle:Forms:fields.html.twig' %}
    {{ form_start(editForm) }}

    {{ form_start(editForm) }}
    {{ form_row(editForm.file) }}
    {{ form_row(editForm.title) }}
    {{ form_row(editForm.fileName) }}
    {{ form_row(editForm.public) }}
    {{ form_row(editForm.country) }}
    {{ form_row(editForm.type) }}
    {{ form_row(editForm.publicationNumber) }}
    {{ form_row(editForm.leafletCode) }}
    {{ form_row(editForm.tags.name) }}

    {{ form_end(editForm) }}

&lt;/div&gt;`
</code></pre>

<p>I know this is a lot of code but I just struggle to explain all this. So the error. I don't really understand why my twig files are still receiving an array when the transformer should be turning it into a string? Thanks!</p>

<p>I forgot to add the error:</p>

<pre><code>Expected argument of type ""array or (\Traversable and \ArrayAccess)"", ""string"" given
</code></pre>
","transformer-model"
"43608700","Grouping Object Array","2017-04-25 10:54:44","43619371","1","681","<laravel><eloquent><transformer-model>","<p>Hello I have the following JSON:</p>

<pre><code>[
  {
    ""id"": 1,
    ""company_id"": 2,
    ""route_id"": 16,
    ""class"": ""Standard"",
    ""fare"": 35,
    ""weekday"": ""monday"",
    ""reporting_time"": ""06:00 PM"",
    ""departure_time"": ""07:00 PM"",
    ""extras"": []
  },
  {
    ""id"": 2,
    ""company_id"": 2,
    ""route_id"": 16,
    ""class"": ""Standard"",
    ""fare"": 35,
    ""weekday"": ""tuesday"",
    ""reporting_time"": ""06:00 PM"",
    ""departure_time"": ""07:00 PM"",
    ""extras"": []
  },
  {
    ""id"": 3,
    ""company_id"": 2,
    ""route_id"": 16,
    ""class"": ""Standard"",
    ""fare"": 35,
    ""weekday"": ""wednesday"",
    ""reporting_time"": ""06:00 PM"",
    ""departure_time"": ""07:00 PM"",
    ""extras"": []
  },
  {
    ""id"": 4,
    ""company_id"": 2,
    ""route_id"": 16,
    ""class"": ""VIP"",
    ""fare"": 35,
    ""weekday"": ""thursday"",
    ""reporting_time"": ""06:00 PM"",
    ""departure_time"": ""07:00 PM"",
    ""extras"": []
  },
  {
    ""id"": 5,
    ""company_id"": 2,
    ""route_id"": 16,
    ""class"": ""VIP"",
    ""fare"": 35,
    ""weekday"": ""friday"",
    ""reporting_time"": ""06:00 PM"",
    ""departure_time"": ""07:00 PM"",
    ""extras"": []
  },
  {
    ""id"": 6,
    ""company_id"": 2,
    ""route_id"": 16,
    ""class"": ""Business"",
    ""fare"": 35,
    ""weekday"": ""saturday"",
    ""reporting_time"": ""06:00 PM"",
    ""departure_time"": ""07:00 PM"",
    ""extras"": []
  }
]
</code></pre>

<p>This is how the data is being returned by my eloquent model, I format the output with transformers to hide certain fields etc.</p>

<p>I would like to group this data by the <code>class</code> property.  This is how I want the data to look like:</p>

<pre><code>[
   {
      ""class"":""Standard"",
      ""schedules"":[
         {
            ""id"":1,
            ""company_id"":2,
            ""route_id"":16,
            ""fare"":35,
            ""weekday"":""monday"",
            ""reporting_time"":""06:00 PM"",
            ""departure_time"":""07:00 PM"",
            ""extras"":[

            ]
         },
         {
            ""id"":2,
            ""company_id"":2,
            ""route_id"":16,
            ""fare"":35,
            ""weekday"":""tuesday"",
            ""reporting_time"":""06:00 PM"",
            ""departure_time"":""07:00 PM"",
            ""extras"":[

            ]
         },
         {
            ""id"":3,
            ""company_id"":2,
            ""route_id"":16,
            ""fare"":35,
            ""weekday"":""wednesday"",
            ""reporting_time"":""06:00 PM"",
            ""departure_time"":""07:00 PM"",
            ""extras"":[

            ]
         }
      ]
   },
   {
      ""class"":""VIP"",
      ""schedules"":[
         {
            ""id"":4,
            ""company_id"":2,
            ""route_id"":16,
            ""fare"":35,
            ""weekday"":""thursday"",
            ""reporting_time"":""06:00 PM"",
            ""departure_time"":""07:00 PM"",
            ""extras"":[

            ]
         },
         {
            ""id"":5,
            ""company_id"":2,
            ""route_id"":16,
            ""fare"":35,
            ""weekday"":""friday"",
            ""reporting_time"":""06:00 PM"",
            ""departure_time"":""07:00 PM"",
            ""extras"":[

            ]
         }
      ]
   },
   {
      ""class"":""Business"",
      ""schedules"":[
         {
            ""id"":6,
            ""company_id"":2,
            ""route_id"":16,
            ""fare"":35,
            ""weekday"":""saturday"",
            ""reporting_time"":""06:00 PM"",
            ""departure_time"":""07:00 PM"",
            ""extras"":[

            ]
         }
      ]
   }
]
</code></pre>

<p>How do I do this without altering the eloquent query or database structure? Is there a way I can use transformers to achieve this? If not and my only option is to change the eloquent query how would I do that to get the data looking like the way I want it?</p>
","transformer-model"
"43476569","Fluentd time_key setting not working","2017-04-18 15:39:24","","2","2758","<elasticsearch><record><fluentd><transformer-model>","<p>Working on trying to use fluentd to centralize mysql logs into elastic search.</p>

<p>The issue I'm seeing is that time timestamp in the search is not matching the timestamp in the log.</p>

<p>Here's my fluentd config:</p>

<pre class=""lang-bash prettyprint-override""><code>&lt;source&gt;
  @type tail
  path /logs/mysql/audit.log
  pos_file /var/log/td-agent/audit.log.pos
  tag mysql.audit
  format json
&lt;/source&gt;

&lt;match mysql.audit&gt;
  type elasticsearch
  host isolinear-eg.corp.apple.com
  port 9200
  index_name mysql_audit
  include_tag_key true
  logstash_format true
  logstash_prefix mysql_audit
  time_key audit_record.timestamp
  time_format %Y-%m-%dT%H:%M:%S %Z
  flush_interval 10s # for testing
&lt;/match&gt;
</code></pre>

<p>And here's the output:</p>

<pre class=""lang-bash prettyprint-override""><code>{
  ""_index"": ""mysql_audit-2017.04.18"",
  ""_type"": ""fluentd"",
  ""_id"": ""AVuBrutMy6H0rNsJZZHy"",
  ""_score"": null,
  ""_source"": {
    ""audit_record"": {
      ""name"": ""Connect"",
      ""record"": ""447474053_2017-04-11T22:30:21"",
      ""timestamp"": ""2017-04-18T15:29:01 UTC"",
      ""connection_id"": ""21450"",
      ""status"": 0,
      ""user"": ""solver"",
      ""priv_user"": ""solver"",
      ""os_login"": """",
      ""proxy_user"": """",
      ""host"": """",
      ""ip"": ""10.108.251.201"",
      ""db"": ""solver""
    },
    ""tag"": ""mysql.audit"",
    ""@timestamp"": ""2017-04-18T10:29:02-05:00""
  },
  ""fields"": {
    ""@timestamp"": [
      1492529342000
    ]
  },
  ""sort"": [
    1492529342000
  ]
}
</code></pre>

<p>The issue I'm seeing is that the timestamp is not matching the audit record timestamp.</p>
","transformer-model"
"43095366","Which transformerClasses to use for complex Magnolia forms","2017-03-29 13:46:11","43133516","4","1059","<dialog><magnolia><transformer-model>","<p>I need to define a dialog, consisting of a several basic fields (text) nested in a switchable field, which itself is nested in a composite field, which itself is nested in a multivalue field. </p>

<pre><code>Multivalue 
  -&gt; Composite
    -&gt; Switchable
       -&gt; textField1
       -&gt; textField2
    -&gt; generic textField (belonging to  composite, but not to switchable)
</code></pre>

<p>However i cant manage to find the correct and working combination of transformerClasses, which I need to use. If using the corresponding <code>DelegatingXXTransformer</code> classes on multiValue and composite field, it nearly worked as expected, but those delegating transformers hide the required UI-controls (arrow down/up) buttons (see image).
But I need those controls.
I need the information which transformerClass to use for any element, to dont lose the ui controls, but still be able to handle nested fields with a higher level than two.
Anyway to solve this ? </p>

<p><a href=""https://i.sstatic.net/BBKaz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BBKaz.png"" alt=""enter image description here""></a></p>

<p>relevant yaml-config:</p>

<pre><code>form:
  tabs:
   - name: tabMain
     fields:
      - name: mainNav
        class: info.magnolia.ui.form.field.definition.MultiValueFieldDefinition
        transformerClass: info.magnolia.ui.form.field.transformer.multi.DelegatingMultiValueSubnodeTransformer
        field:
          name: composite
          class: info.magnolia.ui.form.field.definition.CompositeFieldDefinition
          transformerClass: info.magnolia.ui.form.field.transformer.composite.DelegatingCompositeFieldTransformer
          layout: vertical
          fields:
            - !include /module-ui/dialogs/common/link.yaml
            - !include /module-ui/common/link-title.yaml
            - !include /module-ui/dialogs/common/link-target.yaml
actions: !include /module-ui/dialogs/actions/default.yaml
</code></pre>

<p>link.yaml:</p>

<pre><code>name: link
class: info.magnolia.ui.form.field.definition.SwitchableFieldDefinition
transformerClass: info.magnolia.ui.form.field.transformer.composite.SwitchableTransformer
options:
 - name: internal
   value: internal
   selected: true
 - name: external
   value: external
   fields:
    - name: internal
      class: info.magnolia.ui.form.field.definition.LinkFieldDefinition
      appName: pages
      identifierToPathConverter:
        class:    info.magnolia.ui.form.field.converter.BaseIdentifierToPathConverter
 - name: external
   class: info.magnolia.ui.form.field.definition.TextFieldDefinition
   defaultValue: ""http://""
</code></pre>
","transformer-model"
"42902229","How to use hl7 encoding transformer connector in mule?","2017-03-20 11:18:12","","0","324","<encoding><mule><hl7><transformer-model>","<p>As I am new to mule anypointstudio, can anyone help in the usage of hl7 encoding transformer connector. I have created the application for this by using hl7 encoding transformer but it is just transferring the file from one directory to another without transforming it to XML.
The Flow I have made is as follows:</p>

<p>FILE - logger - hl7 encoding transformer connector - logger - file</p>
","transformer-model"
"42772395","How do I retrieve payload in one transformer to another transformer via vm","2017-03-13 20:00:35","42835205","0","50","<mule><transformer-model>","<p>I have a flow which transforms one payload and sends it via a vm to another flow. I want to be able to retrieve the list from the previous transformer. Unfortunately, only the original message that was put on it gets sent to the inbound vm of the other flow. Whether I set the payload or set property neither gives me a list. How can I get the list to the second transformer using the vm? </p>

<pre><code>  &lt;flow name=""Flow1""&gt;
    &lt;custom-transformer class=""com.nek.transformer.MyXmlToListTransformer""&gt;
     &lt;set-property propertyName=""listProp"" value=""#[payload]""/&gt;
    &lt;set-payload value=#[payload]/&gt;
    &lt;vm:outbound path=""listHandler""&gt;
  &lt;/flow&gt;

   &lt;flow name=""Flow1""&gt;
     &lt;vm:inbound path=""listHandler""&gt;
     &lt;custom-transformer class=""com.nek.transformer.MyListToMapTransformer""&gt;
   &lt;/flow&gt;

   //This transformer wants the list from the previous transformer
   public MyListToMapTransformer extends AbstractTransformer{
     public Object doTransform(MuleMessage message, String outputEncoding){
      //Neither gives me the list from the previous transformer
      Object obj=message.getInboundProperty(""listProp"");
       obj-message.getPayload();
     }

   }  
</code></pre>
","transformer-model"
"42333079","Symfony: Why is persist trying to insert an object retrieved by object manager?","2017-02-19 21:26:59","42333240","0","4286","<php><doctrine><duplicates><symfony><transformer-model>","<p>Apologies if this has been asked before. I'm still trying to grasp Symfony terms to my searching isn't that good yet!</p>

<p>I have (for the purpose of this question) two entities: Article and Author.</p>

<p>I'm embedding the Author form into the add Article form. I'd like to check if the email already exists, and, if it does, just update that record (the corresponding name) instead of adding a duplicate. I'm using a transformer to do this.</p>

<p>I am able to find an existing Author in my transformer. However, when I persist the Article in the controller, I get the error:</p>

<blockquote>
  <p>""An exception occurred while executing 'INSERT INTO author...
  Integrity constraint violation: 1062 Duplicate entry"" ... etc...</p>
</blockquote>

<p>I'm really confused because my understanding is that persist should update the record that I just retrieved from the database!</p>

<p><strong>Article Controller:</strong></p>

<pre><code>    public function newAction(Request $request){
    $article = new Article();
    $form = $this-&gt;createForm('AppBundle\Form\ArticleType', $article);
    $form-&gt;handleRequest($request);

    if ($form-&gt;isSubmitted() &amp;&amp; $form-&gt;isValid()) {

        //Set the timestamps on article/author.
        $date = new \DateTime(""now"");
        $article-&gt;setCreatedDate($date);
        $article-&gt;getAuthor()-&gt;setCreatedDate($date);

        //Persist to database.
        $em = $this-&gt;getDoctrine()-&gt;getManager();
        $em-&gt;persist($article);
        $em-&gt;flush($article);

        return $this-&gt;redirectToRoute('article_show', array('id' =&gt; $article-&gt;getId()));
    }

    return $this-&gt;render('article/new.html.twig', array(
        'article' =&gt; $article,
        'form' =&gt; $form-&gt;createView(),
    ));}
</code></pre>

<p><strong>Article Type:</strong></p>

<pre><code>namespace AppBundle\Form;

use Symfony\Component\Form\AbstractType;
use Symfony\Component\Form\FormBuilderInterface;
use Symfony\Component\OptionsResolver\OptionsResolver;
use Symfony\Component\Form\Extension\Core\Type\CollectionType;

class ArticleType extends AbstractType
{
    /**
     * {@inheritdoc}
     */
    public function buildForm(FormBuilderInterface $builder, array $options)
    {
        $builder-&gt;add('name')-&gt;add('description')-&gt;add('thumbnail');

        //We'll handle dates. Don't want users to access that.
        //$builder-&gt;add('createdDate');

        $builder-&gt;add('author', AuthorType::class, array(""label"" =&gt; FALSE));
    }

    /**
     * {@inheritdoc}
     */
    public function configureOptions(OptionsResolver $resolver)
    {
        $resolver-&gt;setDefaults(array(
            'data_class' =&gt; 'AppBundle\Entity\Article'
        ));
    }

    /**
     * {@inheritdoc}
     */
    public function getBlockPrefix()
    {
        return 'appbundle_article';
    }


}
</code></pre>

<p><strong>Article Entity:</strong></p>

<pre><code>&lt;?php

namespace AppBundle\Entity;

use Doctrine\ORM\Mapping as ORM;
use Doctrine\Common\Collections\ArrayCollection;
use Symfony\Component\Validator\Constraints as Assert;
/**
 * Article
 *
 * @ORM\Table(name=""article"")
 * @ORM\Entity(repositoryClass=""AppBundle\Repository\ArticleRepository"")
 */
class Article
{
    /**
     * @var int
     *
     * @ORM\Column(name=""id"", type=""integer"")
     * @ORM\Id
     * @ORM\GeneratedValue(strategy=""AUTO"")
     */
    private $id;

    /**
     * @var string
     *
     * @ORM\Column(name=""name"", type=""string"", length=255, unique=true)
     */
    private $name;

    /**
     * @var string
     *
     * @ORM\Column(name=""description"", type=""text"", nullable=true)
     */
    private $description;

    /**
     * @var string
     *
     * @ORM\Column(name=""thumbnail"", type=""string"", length=255, nullable=true)
     */
    private $thumbnail;

    /**
     * @var \DateTime
     *
     * @ORM\Column(name=""created_date"", type=""datetime"")
     */
    private $createdDate;

    /**
     * @ORM\ManyToOne(targetEntity=""Author"", inversedBy=""articles"", cascade={""persist""})
     * @ORM\JoinColumn(name=""author_id"", referencedColumnName=""id"")
     * @Assert\Valid()
     */
    private $author;


    /**
     * @ORM\OneToMany(targetEntity=""Review"", mappedBy=""article"")
     */
    private $reviews;

    public function __construct()
    {
        $this-&gt;reviews = new ArrayCollection();
    }

    /**
     * Get id
     *
     * @return int
     */
    public function getId()
    {
        return $this-&gt;id;
    }

    /**
     * Set name
     *
     * @param string $name
     *
     * @return Article
     */
    public function setName($name)
    {
        $this-&gt;name = $name;

        return $this;
    }

    /**
     * Get name
     *
     * @return string
     */
    public function getName()
    {
        return $this-&gt;name;
    }

    /**
     * Set description
     *
     * @param string $description
     *
     * @return Article
     */
    public function setDescription($description)
    {
        $this-&gt;description = $description;

        return $this;
    }

    /**
     * Get description
     *
     * @return string
     */
    public function getDescription()
    {
        return $this-&gt;description;
    }

    /**
     * Set thumbnail
     *
     * @param string $thumbnail
     *
     * @return Article
     */
    public function setThumbnail($thumbnail)
    {
        $this-&gt;thumbnail = $thumbnail;

        return $this;
    }

    /**
     * Get thumbnail
     *
     * @return string
     */
    public function getThumbnail()
    {
        return $this-&gt;thumbnail;
    }

    /**
     * Set createdDate
     *
     * @param \DateTime $createdDate
     *
     * @return Article
     */
    public function setCreatedDate($createdDate)
    {
        $this-&gt;createdDate = $createdDate;

        return $this;
    }

    /**
     * Get createdDate
     *
     * @return \DateTime
     */
    public function getCreatedDate()
    {
        return $this-&gt;createdDate;
    }

    /**
     * Set authorId
     *
     * @param integer $authorId
     *
     * @return Article
     */
    public function setAuthorId($authorId)
    {
        $this-&gt;authorId = $authorId;

        return $this;
    }

    /**
     * Get authorId
     *
     * @return int
     */
    public function getAuthorId()
    {
        return $this-&gt;authorId;
    }

    /**
     * Set author
     *
     * @param \AppBundle\Entity\Author $author
     *
     * @return Article
     */
    public function setAuthor(\AppBundle\Entity\Author $author = null)
    {
        $this-&gt;author = $author;

        return $this;
    }

    /**
     * Get author
     *
     * @return \AppBundle\Entity\Author
     */
    public function getAuthor()
    {
        return $this-&gt;author;
    }

    /**
     * Add review
     *
     * @param \AppBundle\Entity\Review $review
     *
     * @return Article
     */
    public function addReview(\AppBundle\Entity\Review $review)
    {
        $this-&gt;reviews[] = $review;

        return $this;
    }

    /**
     * Remove review
     *
     * @param \AppBundle\Entity\Review $review
     */
    public function removeReview(\AppBundle\Entity\Review $review)
    {
        $this-&gt;reviews-&gt;removeElement($review);
    }

    /**
     * Get reviews
     *
     * @return \Doctrine\Common\Collections\Collection
     */
    public function getReviews()
    {
        return $this-&gt;reviews;
    }

    public function __toString() {
        return $this-&gt;name;
    }

}
</code></pre>

<p><strong>Author Entity:</strong></p>

<pre><code>&lt;?php

namespace AppBundle\Entity;

use Doctrine\ORM\Mapping as ORM;
use Doctrine\Common\Collections\ArrayCollection;
use Symfony\Bridge\Doctrine\Validator\Constraints\UniqueEntity;

/**
 * Author
 *
 * @ORM\Table(name=""author"")
 * @ORM\Entity(repositoryClass=""AppBundle\Repository\AuthorRepository"")
 * @UniqueEntity(fields={""email""}, message=""Note: author already existed. Using that record"")
 */
class Author
{
    /**
     * @var int
     *
     * @ORM\Column(name=""id"", type=""integer"")
     * @ORM\Id
     * @ORM\GeneratedValue(strategy=""AUTO"")
     */
    private $id;

    /**
     * @var string
     *
     * @ORM\Column(name=""email"", type=""string"", length=255, unique=true)
     */
    private $email;

    /**
     * @var string
     *
     * @ORM\Column(name=""first_name"", type=""string"", length=255)
     */
    private $firstName;

    /**
     * @var string
     *
     * @ORM\Column(name=""last_name"", type=""string"", length=255)
     */
    private $lastName;

    /**
     * @var \DateTime
     *
     * @ORM\Column(name=""created_date"", type=""datetime"")
     */
    private $createdDate;

    /**
     * @ORM\OneToMany(targetEntity=""Review"", mappedBy=""author"")
     */
    private $reviews;

    /**
     * @ORM\OneToMany(targetEntity=""article"", mappedBy=""author"")
     */
    private $articles;

    public function __construct()
    {
        $this-&gt;reviews = new ArrayCollection();
        $this-&gt;articles = new ArrayCollection();
    }


    /**
     * Get id
     *
     * @return int
     */
    public function getId()
    {
        return $this-&gt;id;
    }

    /**
     * Set email
     *
     * @param string $email
     *
     * @return Author
     */
    public function setEmail($email)
    {
        $this-&gt;email = $email;

        return $this;
    }

    /**
     * Get email
     *
     * @return string
     */
    public function getEmail()
    {
        return $this-&gt;email;
    }

    /**
     * Set firstName
     *
     * @param string $firstName
     *
     * @return Author
     */
    public function setFirstName($firstName)
    {
        $this-&gt;firstName = $firstName;

        return $this;
    }

    /**
     * Get firstName
     *
     * @return string
     */
    public function getFirstName()
    {
        return $this-&gt;firstName;
    }

    /**
     * Set lastName
     *
     * @param string $lastName
     *
     * @return Author
     */
    public function setLastName($lastName)
    {
        $this-&gt;lastName = $lastName;

        return $this;
    }

    /**
     * Get lastName
     *
     * @return string
     */
    public function getLastName()
    {
        return $this-&gt;lastName;
    }

    /**
     * Set createdDate
     *
     * @param \DateTime $createdDate
     *
     * @return Author
     */
    public function setCreatedDate($createdDate)
    {
        $this-&gt;createdDate = $createdDate;

        return $this;
    }

    /**
     * Get createdDate
     *
     * @return \DateTime
     */
    public function getCreatedDate()
    {
        return $this-&gt;createdDate;
    }

    /**
     * Add review
     *
     * @param \AppBundle\Entity\Review $review
     *
     * @return Author
     */
    public function addReview(\AppBundle\Entity\Review $review)
    {
        $this-&gt;reviews[] = $review;

        return $this;
    }

    /**
     * Remove review
     *
     * @param \AppBundle\Entity\Review $review
     */
    public function removeReview(\AppBundle\Entity\Review $review)
    {
        $this-&gt;reviews-&gt;removeElement($review);
    }

    /**
     * Get reviews
     *
     * @return \Doctrine\Common\Collections\Collection
     */
    public function getReviews()
    {
        return $this-&gt;reviews;
    }

    /**
     * Add article
     *
     * @param \AppBundle\Entity\article $article
     *
     * @return Author
     */
    public function addarticle(\AppBundle\Entity\article $article)
    {
        $this-&gt;articles[] = $article;

        return $this;
    }

    /**
     * Remove article
     *
     * @param \AppBundle\Entity\article $article
     */
    public function removearticle(\AppBundle\Entity\article $article)
    {
        $this-&gt;articles-&gt;removeElement($article);
    }

    /**
     * Get articles
     *
     * @return \Doctrine\Common\Collections\Collection
     */
    public function getarticles()
    {
        return $this-&gt;articles;
    }

    /**
     *Return boolean depending on if the author has already reviewed the article
     * @param \AppBundle\Entity\Author $author
     * @return bool
     */
    public function hasarticle(\AppBundle\Entity\article $article)
    {
        return $this-&gt;getarticles()-&gt;contains($article);
    }

    public function __toString() {
        return $this-&gt;email;
    }

}
</code></pre>

<p>Thanks so much in advance for any help you can give. Also, is this the best way to handle duplicate emails? I'd like to update the firstname/lastname as well instead of just letting leaving the existing name in the database. Should I update the name in the controller before persisting or should that somehow be done in the transformer?</p>

<p>Thanks!</p>
","transformer-model"
"42294870","XML white space formatting in Java","2017-02-17 10:00:06","","1","1044","<java><xml><dom><xml-parsing><transformer-model>","<p>I've got problem with formatting white space in Java. I add new attribute to my file, but it hasn't correct format.</p>

<pre><code>    Attr attr = doc.createAttribute(""name"");
    attr.setValue(""name"");
    element.setAttributeNode(attr);
    element.setTextContent(""Something="" + this.thumbnailCacheSize);

    NodeList items = doc.getElementsByTagName(""mbean"");
    Node e = items.item(0);
    e.appendChild(element);
    TransformerFactory transformerFactory = TransformerFactory.newInstance();
    Transformer transformer = transformerFactory.newTransformer();
    transformer.setOutputProperty(OutputKeys.INDENT,""yes"");
    DOMSource source = new DOMSource(doc);
    StreamResult result = new StreamResult(filepath);

    transformer.transform(source,result);
</code></pre>

<p>It should look like this:</p>

<pre><code> &lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
 &lt;server&gt;
   &lt;mbean code=""code"" name=""name""&gt;
     &lt;attribute name=""name3""&gt;Something3=500&lt;/attribute&gt;
     &lt;attribute name=""name2""&gt;Something2=500&lt;/attribute&gt;
     &lt;attribute name=""name1""&gt;Something1=500&lt;/attribute&gt;
     &lt;attribute name=""name""&gt;Something=500&lt;/attribute&gt;
   &lt;/mbean&gt;
 &lt;/server&gt;
</code></pre>

<p>But is like:</p>

<pre><code> &lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
 &lt;server&gt;
 &lt;mbean code=""code"" name=""name""&gt;
     &lt;attribute name=""name3""&gt;Something3=500&lt;/attribute&gt;
     &lt;attribute name=""name2""&gt;Something2=500&lt;/attribute&gt;
     &lt;attribute name=""name1""&gt;Something1=500&lt;/attribute&gt;
   &lt;attribute name=""name""&gt;Something=500&lt;/attribute&gt;
 &lt;/mbean&gt;
 &lt;/server&gt;
</code></pre>

<p>When I used:
     transformer.setOutputProperty(""{<a href=""http://xml.apache.org/xslt"" rel=""nofollow noreferrer"">http://xml.apache.org/xslt</a>}indent-amount"", ""4"");</p>

<p>I get:</p>

<pre><code> &lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
 &lt;server&gt;
 &lt;mbean code=""code"" name=""name""&gt;
     &lt;attribute name=""name3""&gt;Something3=500&lt;/attribute&gt;
     &lt;attribute name=""name2""&gt;Something2=500&lt;/attribute&gt;
     &lt;attribute name=""name1""&gt;Something1=500&lt;/attribute&gt;
   &lt;attribute name=""name""&gt;Something=500&lt;/attribute&gt;
     &lt;/mbean&gt;
 &lt;/server&gt;
</code></pre>
","transformer-model"
"42008920","CSV encoding issue using javax.xml.transform.Transformer on Linux but not on windows","2017-02-02 17:43:46","","0","305","<java><xml><linux><transformer-model>","<p>I'm generating a CSV file from a XSL and XML files. </p>

<p>Both original files have <code>&lt;?xml version=""1.0"" encoding=""ISO-8859-1""?&gt;</code> as header.</p>

<pre><code>String csvPath = ""myCsvPath"";
String xmlPath = ""xmlPath.xml"";
String xslPath = ""xslPath.xsl""

File xmlFile = new File(xmlPath);
File xslFile = new File(xslPath);

Source xmlSource = new StreamSource(xmlFile);
Source xslSource = new StreamSource(xslFile);
Result result = new StreamResult(new ByteArrayOutputStream());

TransformerFactory transFac = new TransformerFactoryImpl();
Transformer trans = transFac.newTransformer(xslSource);
trans.setParameter(""CSV_PATH"",""file:///"" + csvPath);
trans.transform(xmlSource,result);
</code></pre>

<p>I'm using net.sf.saxon.Controller as Transformer.
The version of Saxon is 9.1.0.8 but I also tried with 9.4 without any luck.
In my XSL file one of the label is ""Disponibilité""</p>

<p>If I launch the generation on my Dev tomcat which is on windows, the CSV header is ""Disponibilité"", no problem.
However if I launch the generation on a linux VM, the ""é"" is not encoded correctly : ""Disponibili<code>tï¿½</code>""</p>

<p>I've checked with vim and log, the original file shows ""é"" correctly.
But once the generation is done, if I parse the first line of my file through vim or log I see the <code>é</code> is changed to <code>□</code></p>

<p>I checked similar question like : 
<a href=""https://stackoverflow.com/questions/15592025/transformer-setoutputpropertyoutputkeys-encoding-utf-8-is-not-working"">transformer.setOutputProperty(OutputKeys.ENCODING, &quot;UTF-8&quot;) is NOT working</a></p>

<p>But setting <code>trans.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");</code> is not working.
Nor does using StringWriter instead of ByteArrayOutputStream.</p>
","transformer-model"
"41710428","Error when trying to create custom Json to XML transformer in Mule","2017-01-18 02:29:36","","0","1095","<json><xml><mule><anypoint-studio><transformer-model>","<p>I'm trying to create a custom Json to XML conversion in mule (custom transformer), its just a one to one direct mapping values, not a complex transformation.</p>

<p><strong>here is my flow:</strong></p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;

&lt;mule xmlns:metadata=""http://www.mulesoft.org/schema/mule/metadata"" xmlns:json=""http://www.mulesoft.org/schema/mule/json"" xmlns:http=""http://www.mulesoft.org/schema/mule/http"" xmlns=""http://www.mulesoft.org/schema/mule/core"" xmlns:doc=""http://www.mulesoft.org/schema/mule/documentation""
    xmlns:spring=""http://www.springframework.org/schema/beans"" 
    xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-current.xsd
http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
http://www.mulesoft.org/schema/mule/json http://www.mulesoft.org/schema/mule/json/current/mule-json.xsd
http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd""&gt;
    &lt;http:listener-config name=""HTTP_Listener_Configuration"" host=""0.0.0.0"" port=""8081"" basePath=""/test"" doc:name=""HTTP Listener Configuration""/&gt;
    &lt;flow name=""test_finalFlow""&gt;
        &lt;http:listener config-ref=""HTTP_Listener_Configuration"" path=""/"" doc:name=""HTTP""/&gt;
        &lt;json:json-to-xml-transformer metadata:id=""57c0cd3d-ece4-48fe-9adf-79fc36a31b12"" doc:name=""JSON to XML""/&gt;
        &lt;logger level=""INFO"" doc:name=""Logger""/&gt;
    &lt;/flow&gt;
&lt;/mule&gt;
</code></pre>

<p><a href=""https://i.sstatic.net/YSJcd.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YSJcd.jpg"" alt=""enter image description here""></a></p>

<p>I'm using theses files to configure my <code>Json to XML</code> component</p>

<p><strong>jsonFile.json:</strong></p>

<pre><code>{
  ""inData"": ""value""
}
</code></pre>

<p><strong>xml file: outData.xml</strong></p>

<pre><code>&lt;?xml version='1.0' encoding='UTF-8'?&gt;
&lt;root&gt;
    &lt;outData&gt;someValue&lt;/outData&gt;
&lt;/root&gt;
</code></pre>

<p><strong>setting the metadata:</strong></p>

<p><a href=""https://i.sstatic.net/ZCo1o.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZCo1o.jpg"" alt=""enter image description here""></a></p>

<p><a href=""https://i.sstatic.net/yOdlr.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yOdlr.jpg"" alt=""enter image description here""></a></p>

<p><strong>right now when I deploy the application I do not have any problem,</strong></p>

<p>but <strong>when I use postman</strong> to send a POST data with payload <strong>I got these errors:</strong></p>

<p><a href=""https://i.sstatic.net/6aYP6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6aYP6.jpg"" alt=""enter image description here""></a></p>

<p>and this is the full output of the console:</p>

<pre><code>*******************************************************************************************************
*            - - + APPLICATION + - -            *       - - + DOMAIN + - -       * - - + STATUS + - - *
*******************************************************************************************************
* test_final                                    * default                        * DEPLOYED           *
*******************************************************************************************************

ERROR 2017-01-17 21:23:29,014 [[test_final].HTTP_Listener_Configuration.worker.01] org.mule.exception.DefaultMessagingExceptionStrategy: 
********************************************************************************
Message               : java.io.IOException: Illegal character: &lt;i&gt; (javax.xml.stream.XMLStreamException)
Payload               : org.glassfish.grizzly.utils.BufferInputStream@526ed3db
Transformer           : JsonToXml{this=6993c8df, name='JsonToString', ignoreBadInput=false, returnClass=SimpleDataType{type=java.lang.String, mimeType='text/xml', encoding='null'}, sourceTypes=[SimpleDataType{type=java.lang.String, mimeType='*/*', encoding='null'}, SimpleDataType{type=java.io.InputStream, mimeType='*/*', encoding='null'}, SimpleDataType{type=[B, mimeType='*/*', encoding='null'}, SimpleDataType{type=java.io.Reader, mimeType='*/*', encoding='null'}, SimpleDataType{type=java.net.URL, mimeType='*/*', encoding='null'}, SimpleDataType{type=java.io.File, mimeType='*/*', encoding='null'}]}
Element               : /test_finalFlow/processors/0 @ test_final:test_final.xml:13 (JSON to XML)
Element XML           : &lt;json:json-to-xml-transformer metadata:id=""57c0cd3d-ece4-48fe-9adf-79fc36a31b12"" doc:name=""JSON to XML""&gt;&lt;/json:json-to-xml-transformer&gt;
--------------------------------------------------------------------------------
Root Exception stack trace:
java.io.IOException: Illegal character: &lt;i&gt;
    at de.odysseus.staxon.json.stream.impl.Yylex.yylex(Yylex.java:641)
    at de.odysseus.staxon.json.stream.impl.Yylex.nextSymbol(Yylex.java:271)
    at de.odysseus.staxon.json.stream.impl.JsonStreamSourceImpl.next(JsonStreamSourceImpl.java:107)
    at de.odysseus.staxon.json.stream.impl.JsonStreamSourceImpl.peek(JsonStreamSourceImpl.java:250)
    at de.odysseus.staxon.json.JsonXMLStreamReader.consume(JsonXMLStreamReader.java:128)
    at de.odysseus.staxon.json.JsonXMLStreamReader.consume(JsonXMLStreamReader.java:161)
    at de.odysseus.staxon.base.AbstractXMLStreamReader.initialize(AbstractXMLStreamReader.java:216)
    at de.odysseus.staxon.json.JsonXMLStreamReader.&lt;init&gt;(JsonXMLStreamReader.java:65)
    at de.odysseus.staxon.json.JsonXMLInputFactory.createXMLStreamReader(JsonXMLInputFactory.java:139)
    at de.odysseus.staxon.json.JsonXMLInputFactory.createXMLStreamReader(JsonXMLInputFactory.java:120)
    at de.odysseus.staxon.json.JsonXMLInputFactory.createXMLStreamReader(JsonXMLInputFactory.java:44)
    at org.mule.module.json.transformers.JsonToXml.doTransform(JsonToXml.java:55)
    at org.mule.transformer.AbstractTransformer.transform(AbstractTransformer.java:415)
    at org.mule.transformer.AbstractTransformer.transform(AbstractTransformer.java:366)
    at org.mule.DefaultMuleMessage.transformMessage(DefaultMuleMessage.java:1589)
    at org.mule.DefaultMuleMessage.applyAllTransformers(DefaultMuleMessage.java:1488)
    at org.mule.DefaultMuleMessage.applyTransformers(DefaultMuleMessage.java:1462)
    at org.mule.DefaultMuleMessage.applyTransformers(DefaultMuleMessage.java:1454)
    at org.mule.transformer.AbstractTransformer.process(AbstractTransformer.java:114)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:27)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:108)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:98)
    at org.mule.processor.BlockingProcessorExecutor.execute(BlockingProcessorExecutor.java:59)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:27)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:98)
    at org.mule.processor.BlockingProcessorExecutor.execute(BlockingProcessorExecutor.java:59)
    at org.mule.processor.AsyncInterceptingMessageProcessor.process(AsyncInterceptingMessageProcessor.java:102)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:27)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:108)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:98)
    at org.mule.processor.BlockingProcessorExecutor.execute(BlockingProcessorExecutor.java:59)
    at org.mule.construct.DynamicPipelineMessageProcessor.process(DynamicPipelineMessageProcessor.java:55)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:27)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:108)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:88)
</code></pre>

<p>Any idea about what causing the error ?</p>

<blockquote>
  <p>java.io.IOException: Illegal character: i ?</p>
</blockquote>

<p><strong>Note:</strong></p>

<p>I'm trying to create this custom transformer because I'm using just <code>Mule server 3.8.1 CE</code> right now , I'll get access soon to an enterprise edition version that's why I'm trying to deploy in a community edition server at this time.</p>
","transformer-model"
"41156045","how to transform result to map in hibernate5.2","2016-12-15 03:38:44","","8","680","<hibernate-5.x><transformer-model>","<p>I'm using hibernate5.2 now.I want to transform result to map  with native query,
but <code>setResultTransformer</code> is deprecated.when i see the doc :<code>@todo develop a new approach to result transformers</code> in the method.so what is the new approach to result transformers with NativeQuery?</p>

<p>this is the code,and setResultTransformer() is deprecated in fact</p>

<pre><code> NativeQuery nativeQuery = session.createNativeQuery(sql);
            for (int i = 0; i &lt; params.length; i++)
            {
                nativeQuery.setParameter(i, params[i]);
            }
            List res = nativeQuery.setResultTransformer(Transformers.ALIAS_TO_ENTITY_MAP).list();
</code></pre>
","transformer-model"
"40655278","RavenDB session.Advanced.IsLoaded with transformer","2016-11-17 12:45:42","","0","78","<session><load><ravendb><transformer-model>","<p>I am using the Server and Client Build #30155.</p>

<p>Assuming that docId is a string, I am a bit confused about the following API (<a href=""https://ravendb.net/docs/article-page/3.5/csharp/client-api/session/loading-entities#load"" rel=""nofollow noreferrer"">https://ravendb.net/docs/article-page/3.5/csharp/client-api/session/loading-entities#load</a>):</p>

<pre><code>TResult Load&lt;TResult&gt;(
string id,
string transformer,
Action&lt;ILoadConfiguration&gt; configure);
</code></pre>

<p>When I have loaded (via call to the <code>session.Load&lt;object&gt;(docId)</code>) document in a session, and executed the above call, RavenDB returns null for this in-session-only-loaded document, although the document is in a session - <code>session.IsLoaded(docId)</code> returns true.</p>

<p>Is the call to:
<code>session.Load&lt;object&gt;(docId, transformer: transformerName, configure: null)</code>
aimed to ""forcibly"" go to the server, cause transformer is provided to run server side, or is it a bug?</p>

<p>However, session.Load(docId) first checks if the document with docId is already contained/loaded in a session, and first then if not, goes to the server and loads it into the session.</p>
","transformer-model"
"39859064","Convert stream transformer function into Mealy automaton in Haskell","2016-10-04 18:11:23","39861542","2","204","<haskell><stream><transformer-model>","<p>I work with streams:</p>

<pre><code>data Stream a = Cons a (Stream a)
</code></pre>

<p>Particularly, stream transformer functions:</p>

<pre><code>f :: Stream a -&gt; Stream b
</code></pre>

<p>I would like to make a Mealy automaton out of such a function:</p>

<pre><code>data Mealy a b = Mealy (a -&gt; (b, Mealy a b))
</code></pre>

<p>Is there a way to write such a function?</p>

<pre><code>toMealy :: (Stream a -&gt; Stream b) -&gt; Mealy a b
</code></pre>

<p>I can't find a way. Although the other way works easily:</p>

<pre><code>toStreamTransformer :: Mealy a b -&gt; Stream a -&gt; Stream b
</code></pre>

<p>Maybe I'm missing something trivial?</p>
","transformer-model"
"39564355","How to write a fit_transformer with two inputs and include it in a pipeline in python sklearn?","2016-09-19 01:56:34","","1","338","<python><scikit-learn><pipeline><transformer-model>","<p>Given some fake data:</p>

<pre><code>X = pd.DataFrame( np.random.randint(1,10,28).reshape(14,2) )
y = pd.Series( np.repeat([0,1], [10,4]) ) # imbalanced with more 0s than 1s
</code></pre>

<p>I write a sklearn fit-transformer that under-samples the majority of y to match the length of the minority label. I want to use it in a pipeline. </p>

<pre><code>from sklearn.base import BaseEstimator, TransformerMixin

class UnderSampling(BaseEstimator, TransformerMixin):
    def fit(self, X, y): # I don't need fit to do anything
        return self

    def transform(self, X, y):
        is_pos = y == 1
        idx_pos = y[is_pos].index
        random.seed(random_state)
        idx_neg = random.sample(y[~is_pos].index, is_pos.sum())
        idx = sorted(list(idx_pos) + list(idx_neg))
        X_resampled = X.loc[idx]
        y_resampled = y.loc[idx]
        return X_resampled, y_resampled

    def fit_transform(self, X, y):
        return self.transform(X,y)
</code></pre>

<p>Most unfortunately, I cannot use it in a pipeline. </p>

<pre><code>from sklearn.pipeline import make_pipeline
us = UnderSampling()
rfc = RandomForestClassifier()
model = make_pipeline(us, rfc)
model.fit(X,y)
</code></pre>

<p>How can I make this pipeline work?</p>
","transformer-model"
"39406539","sklearn function transformer in pipeline","2016-09-09 07:53:27","39429857","5","10226","<python><scikit-learn><pipeline><transformer-model>","<p>Writing my first pipeline for sk-learn I stumbled upon some issues when only a subset of columns is put into a pipeline:</p>

<pre><code>mydf = pd.DataFrame({'classLabel':[0,0,0,1,1,0,0,0],
                   'categorical':[7,8,9,5,7,5,6,4],
                   'numeric1':[7,8,9,5,7,5,6,4],
                   'numeric2':[7,8,9,5,7,5,6,""N.A""]})
columnsNumber = ['numeric1']
XoneColumn = X[columnsNumber]
</code></pre>

<p>I use the <code>functionTransformer</code> like:</p>

<pre><code>def extractSpecificColumn(X, columns):
    return X[columns]

pipeline = Pipeline([
    ('features', FeatureUnion([
        ('continuous', Pipeline([
            ('numeric', FunctionTransformer(columnsNumber)),
            ('scale', StandardScaler())
        ]))
    ], n_jobs=1)),
    ('estimator', RandomForestClassifier(n_estimators=50, criterion='entropy', n_jobs=-1))
])

cv.cross_val_score(pipeline, XoneColumn, y, cv=folds, scoring=kappaScore)
</code></pre>

<p>This results in: <code>TypeError: 'list' object is not callable</code> when the function transformer is enabled.</p>

<h1>edit:</h1>

<p>If I instantiate a <code>ColumnExtractor</code> like below no error is returned. But isn't the <code>functionTransformer</code> meant just for simple cases like this one and should just work?</p>

<pre><code>class ColumnExtractor(TransformerMixin):
    def __init__(self, columns):
        self.columns = columns

    def transform(self, X, *_):
        return X[self.columns]

    def fit(self, *_):
        return self
</code></pre>
","transformer-model"
"39352848","Fractal Transformers Optional Model Columns","2016-09-06 15:35:19","","1","588","<php><laravel><laravel-5><transformer-model>","<p>I am using Fractal Transformers in Laravel 5. I have:</p>

<pre><code>namespace App\Transformers;

use App\Models\Cake;
use League\Fractal\TransformerAbstract;

class CakeTransformer extends TransformerAbstract
{
    protected $availableIncludes = [
        'user',
        'description'
    ];
    public function transform(Cake $cake)
    {
        $ar = [
            'name' =&gt; $cake-&gt;name,
            'url_name' =&gt; $cake-&gt;url_name,
            'user' =&gt; $cake-&gt;user-&gt;screenname,
            'date_created' =&gt; $cake-&gt;created_at
        ];

        return $ar;
    }

    public function includeUser(Cake $cake)
    {
        return $this-&gt;item($cake-&gt;user, new UserTransformer());
    }

    public function includeDescription(Cake $cake) {
        return $cake-&gt;description;
    }
}
</code></pre>

<p>The above doesn't work because <code>includeDescription</code> doesn't return the right kind of object, but from the above you can see what I'm trying to do.</p>

<p>For instance in my search I want to bring back much less data than if I were to load a whole page about the search item. E.g. for search I don't want to load the description, but for the page that contains details about the product I would want to.</p>

<p>How can I achieve this?</p>
","transformer-model"
"38852188","How can we use XSLT to achieve an XML which removes unwanted data based on a pattern of string","2016-08-09 13:29:11","38852660","2","32","<xml><xslt><transformer-model>","<p><strong>Request XML:</strong></p>

<pre><code>&lt;SchoolName&gt;
  &lt;name&gt;ABC International, Wisconsin&lt;/name&gt;
  &lt;rank&gt;10&lt;/rank&gt;
&lt;/SchoolName&gt;
&lt;SchoolName&gt;
  &lt;name&gt;XYZ Primary, Las Vegas&lt;/name&gt;
  &lt;rank&gt;4&lt;/rank&gt;
&lt;/SchoolName&gt;
&lt;SchoolName&gt;
  &lt;name&gt;Ryan Academy, Wisconsin&lt;/name&gt;
  &lt;rank&gt;6&lt;/rank&gt;
&lt;/SchoolName&gt;
&lt;SchoolName&gt;
  &lt;name&gt;Advanced Elementary, Houston&lt;/name&gt;
  &lt;rank&gt;15&lt;/rank&gt;
&lt;/SchoolName&gt;
</code></pre>

<p><strong>Target XML:</strong>
Must only consider the schools with name containing ""Wisconsin"" but name should not contain ""Academy"". So it will just return the below:</p>

<pre><code>&lt;SchoolName&gt;
  &lt;name&gt;ABC International, Wisconsin&lt;/name&gt;
  &lt;rank&gt;1&lt;/rank&gt;
&lt;/SchoolName&gt;
</code></pre>

<p>Whats the best way to achieve this? How should the XSLT file for achieving this look like?</p>
","transformer-model"
"38735805","How to add CDATA sections with transformers in Scala?","2016-08-03 06:18:19","","1","159","<xml><scala><cdata><transformer-model>","<p>I'm trying to add CDATA sections around text in XML document with Scala transformer. I'm using the following transformer:</p>

<pre><code>object AddCDATATransformer extends RewriteRule {
  override def transform(n: Node): Seq[Node] = n match {
    case n: Text =&gt; scala.xml.PCData(n.text)
    case _ =&gt; n
  }
}
</code></pre>

<p>but this does not work. The text is there but the CDATA section is not, i.e., I want to have <code>&lt;![CDATA[text]]&gt;</code> rather that <code>text</code>. Any ideas?</p>

<p>PS1. I googled out that there is some <code>-Xxml:coalescing</code> flag but cannot find any documentation on how to use it.</p>

<p>PS2. Strangely enough in another transformer I can produce CDATA sections with node copying:</p>

<pre><code>n.copy(child = scala.xml.PCData(v) ++ n.child.tail)
</code></pre>
","transformer-model"
"37945949","Laravel transformer collection error","2016-06-21 13:34:09","","1","1889","<php><laravel><transformer-model>","<p>I'm using fractal in Laravel 5.2. I'm using a transformer on a collection like this:</p>

<pre><code>public function allFromCompany()
{
    $users = UserModel::all();
    return $this-&gt;response-&gt;collection($users, new UserTransformer);

}
</code></pre>

<p><strong>UserTransformer</strong></p>

<pre><code>class UserTransformer extends Fractal\TransformerAbstract
{
    public function transform(UserModel $user)
    {
        return [
            'user' =&gt; [
                'id'            =&gt; $user-&gt;id,
                'role'          =&gt;
                    [
                        'role_id'       =&gt; $user-&gt;role_id,
                        'name'          =&gt; $user-&gt;role-&gt;name
                    ],
                'company'       =&gt;
                    [
                        'company_id'    =&gt; $user-&gt;company_id,
                        'company'       =&gt; $user-&gt;company-&gt;name,
                    ],
                'active'        =&gt; $user-&gt;active,
                'name'          =&gt; $user-&gt;name,
                'lastname'      =&gt; $user-&gt;lastname,
                'address'       =&gt; $user-&gt;address,
                'zip'           =&gt; $user-&gt;zip,
                'email'         =&gt; $user-&gt;email
            ]
        ];
    }
}
</code></pre>

<p>But when I do it like that I receive an error:</p>

<pre><code>{
  ""status_code"": 500,
  ""debug"": {
    ""line"": 10,
    ""file"": ""/home/vagrant/Code/forum/app/Src/v1/User/UserTransformer.php"",
    ""class"": ""Symfony\\Component\\Debug\\Exception\\FatalThrowableError"",
    ""trace"": [
      ""#0 /home/vagrant/Code/forum/vendor/league/fractal/src/Scope.php(338): Src\\v1\\User\\UserTransformer-&gt;transform(Object(Src\\v1\\User\\User))"",
</code></pre>

<p>When I try this with one item:</p>

<pre><code>return $this-&gt;response-&gt;item($user, new UserTransformer);
</code></pre>

<p>It works. </p>
","transformer-model"
"37935374","java XML to string issue","2016-06-21 04:03:28","","2","177","<java><xml><transformer-model>","<p>I tried to solve my xml issue but I am stuck... I am getting the xml as <code>System.Out</code> at console quite well. However when I tried to get it as a return string value, it gives me only half of the xml (the returned string does not contain error but it's broken xml). The code is below. (ide: androidstudio, tried jdk 1.7/1.8 same result)</p>

<pre><code>Transformer transformer = TransformerFactory.newInstance().newTransformer();
transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
//initialize StreamResult with File object to save to file
StreamResult result = new StreamResult(new StringWriter());
DOMSource source = new DOMSource(doc);
transformer.transform(source, result);

// Output to console for testing
StreamResult consoleResult = new StreamResult(System.out);
transformer.transform(source, consoleResult); // this give all data to as System.Out as correct every line

xmlString = result.getWriter().toString();
Log.v(""XML OUTPUT  xmlString"", xmlString.toString()); // but this gives only part of xml like broken from half part...
</code></pre>

<p>Update:
Solution, there is nothing wrong but Log.v cuts string... this means code is working. thanks for tip to @wero</p>
","transformer-model"
"37920739","Remove key data in dingo - fractal","2016-06-20 10:56:39","","1","1360","<transformer-model><dingo-api>","<p>I already looked into several example on how to remove the key ""data"" in response, but i cannot fix it. </p>

<p>I tried to use the callback provided by dingo</p>

<pre><code>return $this-&gt;collection($users, new UserTransformer, function ($resource, $fractal) {
    $fractal-&gt;setSerializer(new ArraySerializer);
});
</code></pre>

<p>In change the ""$fractal->setSerializer(new CustomSerializer);"" to ""$fractal->setSerializer(new ArraySerializer);"" since I dont have CustomSerializer(And how to make this custom serializer?) based on the fractal documentation <a href=""http://fractal.thephpleague.com/serializers/"" rel=""nofollow"">array serializer</a> but the out put has the key ""data"".</p>

<p>I also tested the fractal library in different project and implementing the ArraySerializer as the Serializer, and it works.</p>

<p>What am I missing in the setup of dingo-fractal?</p>

<p><strong>UPDATE*</strong>
I included the setup in the <a href=""https://joshhornby.co.uk/blog/laravel-dingo-json-api-spec"" rel=""nofollow"">config</a> </p>

<pre><code>$this-&gt;app-&gt;bind('Dingo\Api\Transformer\Adapter\Fractal', function($app) {
        $fractal = $app-&gt;make('\League\Fractal\Manager');
        $serializer = new \League\Fractal\Serializer\ArraySerializer();

        $fractal-&gt;setSerializer($serializer);
        return new \Dingo\Api\Transformer\Adapter\Fractal($fractal);
    });
</code></pre>

<p>And in my controller</p>

<pre><code>    $obj = \App\EloquentModel\User::find(1);
    return $this-&gt;response-&gt;item($obj, new UserTransformer);
</code></pre>

<p>And in my UserTransformer</p>

<pre><code>public function transform(User $trans)
{
    return [
        'id'            =&gt; (int) $trans-&gt;id,
        'name'          =&gt; $trans-&gt;name,
        'description'   =&gt; $trans-&gt;description
    ];
}
</code></pre>

<p>Applying those things removed the key ""data"" for single item.</p>

<pre><code>{
   ""id"": 1,
   ""name"": ""Juan"",
   ""description"": ""The BOss""
}
</code></pre>

<p>But when I try to make it an Array. User::all(), the response has the ""data"" key.</p>

<p>Thanks.</p>
","transformer-model"
"37679827","Java Transformer class duplicates some xml node attributes when transforming","2016-06-07 12:47:14","37720709","1","302","<java><xml><attributes><duplicates><transformer-model>","<p>I have created this simple function in Java:</p>

<pre><code>public static String prettify(Document xml) {
   String resultValue;
   StreamResult xmlOutput;

    try {
        StringWriter stringWriter = new StringWriter();
        xmlOutput = new StreamResult(stringWriter);
        TransformerFactory transformerFactory = TransformerFactory.newInstance();
        Transformer transformer = transformerFactory.newTransformer(); 
        transformer.setOutputProperty(""{http://xml.apache.org/xslt}indent-amount"", ""5"");
        transformer.setOutputProperty(OutputKeys.ENCODING, ""ISO-8859-1"");
        transformer.setOutputProperty(OutputKeys.METHOD, ""xml"");
        transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""no"");
        transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
        transformer.setOutputProperty(OutputKeys.STANDALONE, ""no"");

        Source source = new DOMSource(xml);

        transformer.transform(source, xmlOutput); 
        resultValue = xmlOutput.getWriter().toString();
    } catch (Exception e) {
        resultValue = """";
    }
    return resultValue;
}
</code></pre>

<p>It is a very simple function so I can get an indented XML. The problem is that with this specific case, it is duplicating the attributes at the document element... weird. The case is an XML Document with this XML:</p>

<pre><code>&lt;?xml version='1.0'?&gt;&lt;Document xmlns='urn:iso:std:iso:20022:tech:xsd:pain.008.001.02' xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance'&gt;&lt;CstmrDrctDbtInitn&gt;&lt;GrpHdr&gt;A&lt;/GrpHdr&gt;&lt;/CstmrDrctDbtInitn&gt;&lt;/Document&gt;
</code></pre>

<p>I get:</p>

<pre><code>""&lt;?xml version=""1.0"" encoding=""ISO-8859-1"" standalone=""no""?&gt;
&lt;Document xmlns=""urn:iso:std:iso:20022:tech:xsd:pain.001.001.03"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns=""urn:iso:std:iso:20022:tech:xsd:pain.001.001.03"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""&gt;
 &lt;CstmrCdtTrfInitn&gt;
      &lt;GrpHdr&gt;
        A
      &lt;/GrpHdr&gt;
 &lt;/CstmrCdtTrfInitn&gt;
</code></pre>

<p></p>

<p>As you can see, both attributes at the  are duplicated! I have tried playing with output properties, and nothing. 
If I add, for example, another attribute to the , say ""test='whatever'"", that specific attribute is NOT duplicated.
Any help will be greatly appreciated</p>
","transformer-model"
"37651102","laravel eloquent models one to many relationship with dingo transformer","2016-06-06 06:47:20","","0","1947","<laravel><transformer-model><dingo-api>","<p>my codes are constructed by Laravel+dingo.</p>

<p>I have two models which are one to many relationship:</p>

<p>Reservation.php (Master)</p>

<pre><code>namespace App\Models;

use Illuminate\Database\Eloquent\Model;

class Reservation extends Model
{
    protected  $table = 'dbo.Reservation';

    public function hasManyReservationDetails()
    {
        return $this-&gt;hasMany('App\Models\ReservationDetail', 'ReservationID', 'ReservationID');
    }
}
</code></pre>

<p>ReservationDetail.php (Detail)</p>

<pre><code>namespace App\Models;

use Illuminate\Database\Eloquent\Model;

class ReservationDetail extends Model
{
    protected  $table = 'dbo.ReservationDetail';

    public function belongsToReservation()
    {
        return $this-&gt;belongsTo('App\Models\Reservation', 'ReservationID', 'ReservationDetailID');
    }

}
</code></pre>

<p>And two Transformers for the two models as following:</p>

<p>ReservationTransformer</p>

<pre><code>public function transform(Reservation $reservation)
{
    return [
        'reservation_id'        =&gt; (int) $reservation-&gt;ReservationID,
        'reservation_no'        =&gt; $reservation-&gt;ReservationNo,
    ];
}
</code></pre>

<p>ReservationDetail Transformer</p>

<pre><code>public function transform(ReservationDetail $reservation_detail)
{
    return [
        'reservation_detail_id' =&gt; (int) $reservation_detail-&gt;ReservationDetailID,
        'reservation_id'        =&gt; (int) $reservation_detail-&gt;ReservationID,
        'room_no'               =&gt; $reservation_detail-&gt;RoomNo,
    ];
}
</code></pre>

<p>My controller and inquire </p>

<pre><code>    $reservation = Reservation::where('ReservationNo', '=', $reservation_no)
        -&gt;with('ReservationDetails')
        -&gt;get();
     return $reservation;
</code></pre>

<p>I get the following return</p>

<pre><code>{
  ""Reservations"": [
    {
      ""ReservationID"": ""1"",
      ""ReservationNo"": ""2016-06-01 16:50:59.0659"",
      ""reservation_details"": [
        {
          ""ReservationDetailID"": ""1"",
          ""ReservationID"": ""1"",
          ""RoomNo"": ""001"",
        },
        {
          ""ReservationDetailID"": ""2"",
          ""ReservationID"": ""1"",
          ""RoomNo"": ""002"",
        }
      ]
    }
  ]
}
</code></pre>

<p>I try the following but only return the translation of master table.</p>

<pre><code>$reservation = $this-&gt;collection($reservation, new ReservationTransformer());
</code></pre>

<p>**</p>

<blockquote>
  <p>How can I transform the the data of master and detail table together?</p>
</blockquote>

<p>**</p>

<p>I am not really understand how 'Custom Transformation Layer' works, anyone who can give me an example?</p>

<p>Many Thanks.</p>
","transformer-model"
"37332029","Mule Message / Thread management","2016-05-19 19:04:57","","2","464","<mule><transformer-model>","<p>We are running on mule Community Edition 3.4.1 Investment to make code work there currently outweighs upgrade options.</p>

<p>The Mongo connector at that version leaks resources so bad, that is basically unusable for mass data processing. We are processing approximately 10M-100M records a day (which is not all that huge) and with the Mongo connector out of the box, we would have to restart Mule approximately every three hours. Now you can do that intelligently and not, and we can invest time to make it relatively painless however with the amount of automation and dependent processes we instead opted to write our own Mongo connector.</p>

<p>For usability purposes though and to fit the skills of our existing talent pool, we wanted to create something that provides easy usability and NO leaks on resources.</p>

<p>We basically created a simple Mule Transformer in Java, that exposes it's functionality through static methods. Each static method receives an instance reference, and the instance has the current MuleEvent. 
Basically, we simply override the process method to save the MuleEvent and the doProcess method to save the payload to a private property, then return the instance reference as the payload (this).</p>

<p>The next element in the flow is an Expression transformer, that calls the static method with the payload as the instance, and contains all expressions as simple MEL expressions, just as you could do in the Mongo connector.</p>

<p>We are using a POJO, and Mule Studio is able to pick up the static method references, and the use of this connector is very simple, and easy. It can also process hundreds of millions of calls without any issue whatsoever, since we are very carefully managing resources (actually, too conservative) and all is great.
Except.
When the processing is really fast, sometimes Mule loses its cool and throws a null exception. We know for a fact there is no null exception possible at a normal scenario at that point. We actually even explicitly added expression filters that would not allow a null expression to raise, however it still happens. But only, if there is an asynchronous flow. 
Now before we would jump on that, inside an asycnhronous flow the components still execute in sequence, and our solution puts the instance of the Mongo connector on the message payload and picks it right up. 
We want to have some input how this issue happens and why.</p>

<p>Here is some parts of the code, I can add more.
<strong>EDIT: I added the line in the flow before. You will see, currentSku cannot be null</strong></p>

<pre><code>&lt;set-session-variable variableName=""currentSku"" value=""#[payload.?sku]"" doc:name=""currentSku""/&gt;
&lt;logger message=""#[currentSku]"" level=""INFO"" category=""fiuze.plugins.contentproviders.ExtraData.asycnhFlow"" doc:name=""Logger""/&gt;
&lt;expression-filter expression=""#[currentSku != null ]"" doc:name=""filterForNoSKUinpayload""/&gt; 
&lt;custom-transformer class=""com.fiuze.components.Mongo"" doc:name=""mongoInit"" /&gt;
&lt;expression-transformer expression=""#[com.fiuze.components.Mongo.many(payload, &amp;quot;fizueDemoMongo&amp;quot;,&amp;quot;products&amp;quot;,&amp;quot;{'productChannelData.default.ExtraData':{$exists:false}}&amp;quot;,100)]"" doc:name=""mongoGetProductsWithoutExtraData""/&gt;
</code></pre>

<p>add here is the Java code that reflects this call:</p>

<pre><code>    public static synchronized Object many(Mongo instance, String configRef, String collectionName, String query, Integer limit) throws TransformerException {
      return Mongo.exec(instance, configRef, collectionName, query, limit, null, null, false);
    }
</code></pre>

<p>and here us the Exec implementation:</p>

<pre><code>    private static synchronized Object exec(Mongo instance, String configRef, String collectionName, Object query, Integer limit, Integer skip, Object sort, Boolean count) throws TransformerException {

    instance.getMuleMessage().setPayload(instance.getPayload());
    instance.setConfigref(configRef);
    instance.setCollectionName(collectionName);
    instance.setLimit(limit);
    instance.setSkip(skip);
    sort = instance.evaluate(sort);
    instance.setQuery(instance.evaluate(query));
    if (sort instanceof BasicDBObject) {
        instance.setSort((BasicDBObject)sort);
    } else if (sort instanceof String) {
        instance.setSort((String)sort);
    } else if (sort == null){
        instance.setSort((String)null);
    } else {
        throw new TransformerException( (Message) instance.getMuleMessage() );
    }
    instance.setCount(count);
    Object result = instance.execFind();
    instance.dispose();
    return result;
}
</code></pre>

<p>as you noticed, we use synchronized calls, so the static methods don't get confused.</p>

<p>Since this is a complicated issue, and I am not sure I am expressing it clearly, please do not hesitate to ask me details, I will be more than happy to provide.</p>

<p>EDIT:</p>

<p>Here is part of the log, I will include the part with no issues, the error, and then no issues further. In the section, you will see that the process continues, but a particular Asynch thread is messed up.</p>

<pre><code>INFO  2016-05-20 21:22:00,760 [[fiuze].getExtraProductData.async1.1579] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
INFO  2016-05-20 21:22:00,769 [[fiuze].getExtraProductData.async1.1581] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
ERROR 2016-05-20 21:22:00,812 [[fiuze].getExtraProductData.async1.1590] org.mule.api.processor.LoggerMessageProcessor: no match found
INFO  2016-05-20 21:22:00,825 [[fiuze].getExtraProductData.async1.01] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
ERROR 2016-05-20 21:22:00,831 [[fiuze].getExtraProductData.async1.1584] org.mule.api.processor.LoggerMessageProcessor: no match found
ERROR 2016-05-20 21:22:00,887 [[fiuze].getExtraProductData.async1.1581] org.mule.api.processor.LoggerMessageProcessor: no match found
ERROR 2016-05-20 21:22:00,959 [[fiuze].getExtraProductData.async1.1581] org.mule.api.processor.LoggerMessageProcessor: no match found
ERROR 2016-05-20 21:22:00,964 [[fiuze].getExtraProductData.async1.1580] org.mule.api.processor.LoggerMessageProcessor: no match found
ERROR 2016-05-20 21:22:00,964 [[fiuze].getExtraProductData.async1.1584] org.mule.api.processor.LoggerMessageProcessor: no match found
INFO  2016-05-20 21:22:00,995 [[fiuze].getExtraProductData.async1.1580] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84422
INFO  2016-05-20 21:22:00,996 [[fiuze].getExtraProductData.async1.1579] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84436
INFO  2016-05-20 21:22:00,998 [[fiuze].getExtraProductData.async1.1578] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84454
INFO  2016-05-20 21:22:00,999 [[fiuze].getExtraProductData.async1.1587] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84455
INFO  2016-05-20 21:22:01,000 [[fiuze].getExtraProductData.async1.1590] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84446
INFO  2016-05-20 21:22:01,002 [[fiuze].getExtraProductData.async1.1586] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84445
INFO  2016-05-20 21:22:01,006 [[fiuze].getExtraProductData.async1.1589] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84470
INFO  2016-05-20 21:22:01,006 [[fiuze].getExtraProductData.async1.1577] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84477
INFO  2016-05-20 21:22:01,004 [[fiuze].getExtraProductData.async1.1591] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84462
INFO  2016-05-20 21:22:01,004 [[fiuze].getExtraProductData.async1.1588] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84464
ERROR 2016-05-20 21:22:01,010 [[fiuze].getExtraProductData.async1.1577] org.mule.exception.DefaultMessagingExceptionStrategy:
********************************************************************************
Message               : Execution of the expression ""com.fiuze.components.Mongo.update(payload, ""fizueDemoMongo"",""products"",true,true,""{ sku: #[currentSku] }"",""{$set: { 'productChannelData.default.Extra.attemptedAt':'#[server.dateTime]'}}"")"" failed. (org.mule.api.expression.ExpressionRuntimeException)
Code                  : MULE_ERROR--2
--------------------------------------------------------------------------------
Exception stack is:
1. null (java.lang.NullPointerException)
2. cannot invoke method: update (java.lang.RuntimeException)
  org.mvel2.optimizers.impl.refl.nodes.MethodAccessor:88 (null)
3. Execution of the expression ""com.fiuze.components.Mongo.update(payload, ""fizueDemoMongo"",""products"",true,true,""{ sku: #[currentSku] }"",""{$set: { 'productChannelData.default.Extra.attemptedAt':'#[server.dateTime]'}}"")"" failed. (org.mule.api.expression.ExpressionRuntimeException)
  org.mule.el.mvel.MVELExpressionLanguage:218 (http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/expression/ExpressionRuntimeException.html)
4. Execution of the expression ""com.fiuze.components.Mongo.update(payload, ""fizueDemoMongo"",""products"",true,true,""{ sku: #[currentSku] }"",""{$set: { 'productChannelData.default.Extra.attemptedAt':'#[server.dateTime]'}}"")"" failed. (org.mule.api.expression.ExpressionRuntimeException) (org.mule.api.transformer.TransformerException)
  org.mule.expression.transformers.ExpressionTransformer:66 (http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/transformer/TransformerException.html)
--------------------------------------------------------------------------------
Root Exception stack trace:
java.lang.NullPointerException
    + 0 more (set debug level logging or '-Dmule.verbose.exceptions=true' for everything)
********************************************************************************

INFO  2016-05-20 21:22:01,004 [[fiuze].getExtraProductData.async1.1583] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84447
INFO  2016-05-20 21:22:01,003 [[fiuze].getExtraProductData.async1.1585] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84456
ERROR 2016-05-20 21:22:01,014 [[fiuze].getExtraProductData.async1.1577] org.mule.exception.DefaultMessagingExceptionStrategy:
********************************************************************************
Message               : Execution of the expression ""com.fiuze.components.Mongo.update(payload, ""fizueDemoMongo"",""products"",true,true,""{ sku: #[currentSku] }"",""{$set: { 'productChannelData.default.Extra.attemptedAt':'#[server.dateTime]'}}"")"" failed. (org.mule.api.expression.ExpressionRuntimeException)
Code                  : MULE_ERROR--2
--------------------------------------------------------------------------------
Exception stack is:
1. null (java.lang.NullPointerException)
2. cannot invoke method: update (java.lang.RuntimeException)
  org.mvel2.optimizers.impl.refl.nodes.MethodAccessor:88 (null)
3. Execution of the expression ""com.fiuze.components.Mongo.update(payload, ""fizueDemoMongo"",""products"",true,true,""{ sku: #[currentSku] }"",""{$set: { 'productChannelData.default.Extra.attemptedAt':'#[server.dateTime]'}}"")"" failed. (org.mule.api.expression.ExpressionRuntimeException)
  org.mule.el.mvel.MVELExpressionLanguage:218 (http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/expression/ExpressionRuntimeException.html)
4. Execution of the expression ""com.fiuze.components.Mongo.update(payload, ""fizueDemoMongo"",""products"",true,true,""{ sku: #[currentSku] }"",""{$set: { 'productChannelData.default.Extra.attemptedAt':'#[server.dateTime]'}}"")"" failed. (org.mule.api.expression.ExpressionRuntimeException) (org.mule.api.transformer.TransformerException)
  org.mule.expression.transformers.ExpressionTransformer:66 (http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/transformer/TransformerException.html)
--------------------------------------------------------------------------------
Root Exception stack trace:
java.lang.NullPointerException
    + 0 more (set debug level logging or '-Dmule.verbose.exceptions=true' for everything)
********************************************************************************

INFO  2016-05-20 21:22:01,015 [[fiuze].getExtraProductData.async1.1582] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84482
INFO  2016-05-20 21:22:01,015 [[fiuze].getExtraProductData.async1.01] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84483
INFO  2016-05-20 21:22:01,023 [[fiuze].ConnectorWithoutMuleSessionHTTP.receiver.64] org.mule.api.processor.LoggerMessageProcessor: starting ForEach
WARN  2016-05-20 21:22:01,024 [[fiuze].ConnectorWithoutMuleSessionHTTP.receiver.64] org.mule.module.mongo.api.MongoCollection: Method toArray needs to consume all the element. It is inefficient and thus should be used with care
ERROR 2016-05-20 21:22:01,034 [[fiuze].getExtraProductData.async1.1584] org.mule.api.processor.LoggerMessageProcessor: no match found
ERROR 2016-05-20 21:22:01,035 [[fiuze].getExtraProductData.async1.1581] org.mule.api.processor.LoggerMessageProcessor: no match found
ERROR 2016-05-20 21:22:01,145 [[fiuze].getExtraProductData.async1.1581] org.mule.api.processor.LoggerMessageProcessor: no match found
ERROR 2016-05-20 21:22:01,153 [[fiuze].getExtraProductData.async1.1584] org.mule.api.processor.LoggerMessageProcessor: no match found
ERROR 2016-05-20 21:22:01,268 [[fiuze].getExtraProductData.async1.1581] org.mule.api.processor.LoggerMessageProcessor: no match found
ERROR 2016-05-20 21:22:01,269 [[fiuze].getExtraProductData.async1.1584] org.mule.api.processor.LoggerMessageProcessor: no match found
WARN  2016-05-20 21:22:01,385 [[fiuze].ConnectorWithoutMuleSessionHTTP.receiver.64] org.mule.routing.Foreach$CollectionMapSplitter: Splitter returned no results. If this is not expected, please check your split expression
WARN  2016-05-20 21:22:01,586 [[fiuze].ConnectorWithoutMuleSessionHTTP.receiver.64] org.mule.module.mongo.api.MongoCollection: Method toArray needs to consume all the element. It is inefficient and thus should be used with care
WARN  2016-05-20 21:22:01,600 [[fiuze].ConnectorWithoutMuleSessionHTTP.receiver.64] org.mule.routing.Foreach$CollectionMapSplitter: Splitter returned no results. If this is not expected, please check your split expression
INFO  2016-05-20 21:22:01,667 [[fiuze].getExtraProductData.async1.01] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
INFO  2016-05-20 21:22:01,667 [[fiuze].getExtraProductData.async1.1580] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
INFO  2016-05-20 21:22:01,675 [[fiuze].getExtraProductData.async1.1583] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
INFO  2016-05-20 21:22:01,675 [[fiuze].getExtraProductData.async1.1586] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
INFO  2016-05-20 21:22:01,820 [[fiuze].getExtraProductData.async1.1581] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84485
INFO  2016-05-20 21:22:01,821 [[fiuze].getExtraProductData.async1.1584] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84488
INFO  2016-05-20 21:22:01,821 [[fiuze].getExtraProductData.async1.1577] fiuze.plugins.contentproviders.Extra.asycnhFlow: 84490
ERROR 2016-05-20 21:22:01,892 [[fiuze].getExtraProductData.async1.1580] org.mule.api.processor.LoggerMessageProcessor: no match found
INFO  2016-05-20 21:22:01,911 [[fiuze].getExtraProductData.async1.1585] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
INFO  2016-05-20 21:22:02,136 [[fiuze].getExtraProductData.async1.1591] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
INFO  2016-05-20 21:22:02,231 [[fiuze].getExtraProductData.async1.1588] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
INFO  2016-05-20 21:22:02,356 [[fiuze].getExtraProductData.async1.1587] org.mule.transport.jdbc.sqlstrategy.SelectSqlStatementStrategy: SQL query received a result
ERROR 2016-05-20 21:22:02,425 [[fiuze].getExtraProductData.async1.1587] org.mule.api.processor.LoggerMessageProcessor: no match found
</code></pre>
","transformer-model"
"37262705","Mule flow only processing 1 record from csv file","2016-05-16 20:27:14","","0","81","<json><csv><mule><workflow><transformer-model>","<p>I have a mule flow which goes FILE -> CHOICE -> DATAMAPPER -> LOGGER</p>

<p>The FILE endpoint processes a csv file with 40 records </p>

<p>The DATAMAPPER converts the csv into JSON and shows all 40 records in JSON format when run in preview</p>

<p>The LOGGER shows that the payload as a byte and not JSON and when I convert JSON to object it shows the payload in JSON format but there is only 1 record.</p>

<p>My queries:
1. How do you setup the workflow to process all of the 40 records and convert all to JSON and output all from the flow?</p>

<ol start=""2"">
<li>Is it correct that the output is a byte because the datamapper output is JSON so I was expecting to see all records in JSON format in the payload when looking at it in debug mode?</li>
</ol>
","transformer-model"
"37258614","How to use APIGuard with Fractal Transformer in Laravel?","2016-05-16 16:21:07","37271921","0","3125","<php><web-services><api><laravel><transformer-model>","<p>I'm trying to create a web API in Laravel.
I'm using the following packages for me to manage the REST return: <a href=""http://fractal.thephpleague.com/"" rel=""nofollow"">fractal</a>, spatie/fractal and also <a href=""https://github.com/chrisbjr/api-guard"" rel=""nofollow"">ApiGuard</a>.</p>

<p>My controller has the following code:</p>

<pre><code>&lt;?php

namespace App\Http\Controllers;

use Chrisbjr\ApiGuard\Http\Controllers\ApiGuardController;
use App\CSV;
use App\CSVTransformer;

class ApiController extends ApiGuardController
{
    public function info()
    {
        $csvs = CSV::all();
        $estado = [0,0,0,0];

        foreach ($csvs as $csv) {
            switch ($csv-&gt;estado) {
                case ""Renovado"":
                    $estado[0]++;
                    break;
                case ""Expirado"":
                    $estado[1]++;
                    break;
                case ""Aguardar Pagamento"":
                    $estado[2]++;
                    break;
                case ""Não Renovado"":
                    $estado[3]++;
                    break;
            }
        }            

        return $this-&gt;response-&gt;withCollection($csvs, new CSVTransformer);
    }

    public function renovacoes() {
        $csvs = CSV::all();

        return json_encode([ ""data"" =&gt; $csvs ]);
    }
}
</code></pre>

<p>This is what the transformer looks like:
    

<pre><code>namespace App;

use App\CSV;
use League\Fractal\TransformerAbstract;

class CSVTransformer extends TransformerAbstract
{
    public function transform(CSV $csv)
    {
        return [
            'id'      =&gt; (int) $csv-&gt;id,
            'renovacao'   =&gt; $csv-&gt;renovacao
        ];
    }
}
</code></pre>

<p>The problem is, when accessing the chosen POST route to get the JSON return, the following error is thrown:</p>

<pre><code>Class 'League\Fractal\TransformerAbstract' not found.
</code></pre>

<p>How do I solve this, so that my transformer works as it is supposed to?</p>

<p>EDIT:</p>

<p>Also, here's the <code>CSV</code> class:</p>

<pre><code>&lt;?php

namespace App;

use Illuminate\Auth\Authenticatable;
use Illuminate\Database\Eloquent\Model;
use Illuminate\Auth\Passwords\CanResetPassword;
use Illuminate\Contracts\Auth\Authenticatable as AuthenticatableContract;
use Illuminate\Contracts\Auth\CanResetPassword as CanResetPasswordContract;

class CSV extends Model
{
    protected $table = ""csv"";
}
</code></pre>

<p>Routes file:</p>

<pre><code>Route::group([""middleware"" =&gt; [""apiguard""]], function () {
    Route::group(['prefix' =&gt; 'api'], function () {
        Route::group(['prefix' =&gt; 'v1'], function () {
            Route::post(""/renovations/info"",""ApiController@info"");

            Route::post(""/renovations"",""ApiController@renovacoes"");
        });
    });
});
</code></pre>
","transformer-model"
"37251050","Mule rounding issue with JSON to object transformer","2016-05-16 09:49:56","","0","370","<mule><rounding><transformer-model><anypoint-studio>","<p>I have a JSON to object transformer that follows the datamapper in a Mule 3.6 workflow and found that the JSON to object transformer removes trailing zero in decimal fields so 11.00 becomes 11.0.</p>

<p>How can I prevent the JSON to Object transformer from dropping the last decimal place if it is a zero and enforce 2 decimal places so it will return 11.00 as this is a requirement?</p>

<p>An example flow showing the problem:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;

&lt;mule xmlns:file=""http://www.mulesoft.org/schema/mule/file"" xmlns:json=""http://www.mulesoft.org/schema/mule/json"" xmlns:data-mapper=""http://www.mulesoft.org/schema/mule/ee/data-mapper"" xmlns=""http://www.mulesoft.org/schema/mule/core"" xmlns:doc=""http://www.mulesoft.org/schema/mule/documentation""
    xmlns:spring=""http://www.springframework.org/schema/beans"" version=""EE-3.6.1""
    xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-current.xsd
http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
http://www.mulesoft.org/schema/mule/file http://www.mulesoft.org/schema/mule/file/current/mule-file.xsd
http://www.mulesoft.org/schema/mule/ee/data-mapper http://www.mulesoft.org/schema/mule/ee/data-mapper/current/mule-data-mapper.xsd
http://www.mulesoft.org/schema/mule/json http://www.mulesoft.org/schema/mule/json/current/mule-json.xsd""&gt;
    &lt;data-mapper:config name=""CSV_To_JSON"" transformationGraphPath=""csv_to_json.grf"" doc:name=""CSV_To_JSON""/&gt;
    &lt;flow name=""deimal-point-flowFlow""&gt;
        &lt;file:inbound-endpoint path=""${file.unprocessed.location}"" moveToPattern=""#[message.inboundProperties['originalFilename']]"" moveToDirectory=""${file.unprocessed.location}"" responseTimeout=""10000"" doc:name=""File""&gt;
            &lt;file:filename-regex-filter pattern=""test.csv"" caseSensitive=""true""/&gt;
        &lt;/file:inbound-endpoint&gt;
        &lt;data-mapper:transform config-ref=""CSV_To_JSON"" doc:name=""CSV To JSON""/&gt;
        &lt;json:json-to-object-transformer returnClass=""java.util.Map"" doc:name=""JSON to Object""/&gt;
        &lt;json:object-to-json-transformer returnClass=""java.lang.String"" mimeType=""application/json"" doc:name=""Object to JSON""/&gt;
        &lt;logger level=""INFO"" doc:name=""Logger""/&gt;
    &lt;/flow&gt;
&lt;/mule&gt;
</code></pre>

<p>The test.csv file looks like this:</p>

<pre><code>DeptID,Dept,Staff
5LL/A,Human Resources,4.00
</code></pre>
","transformer-model"
"37198099","Java transformer w3c.dom.document to inputstream","2016-05-12 21:55:20","37198259","0","1888","<java><dom><xpath><inputstream><transformer-model>","<p>My scenario is this:</p>

<p>I have a HTML which I loaded into a w3c.dom.Document, after loading it as a doc, I parsed through its nodes and made a few changes in their values, but now I need to transform this document into a String, or preferably into a InputStream directly.</p>

<p>And I managed to do so, however, to the ends I need this HTML it must keep some properties of the initial file, for instance (and this is the one thing I'm struggling a lot trying to solve), all tags must be closed.</p>

<p>Say, I have a link tag on the header, <code>&lt;link .... /&gt;</code> I NEED the dash (/) at the end. However after the transformer transform my doc into a outputStream (which then I proceed to send to an inputStream) all the '/' before the > disappear. All my tags, which ended in <code>/&gt;</code> are changed into simple <code>&gt;</code>.</p>

<p>The reason I need this structure is that one of the libraries I'm using (and I'm afraid I can't go looking for another one, specially not at this point) require all tags to be closed, if not it throws exceptions everywhere and my program crashes....</p>

<p>Does anyone have any good ideas or solutions for me? This is my first contact with the Transform class, so I might be missing something that could help me.</p>

<p>Thank you all so very much,</p>

<p>Warm regards</p>

<p>Some bit of the code to explain the scenario a little bit</p>

<pre><code>DocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance();
DocumentBuilder docBuilder = docFactory.newDocumentBuilder();
org.w3c.dom.Document doc = docBuilder.parse(his); // his = the HTML inputStream

XPath xPath = XPathFactory.newInstance().newXPath();
String expression = ""//*[@id='pessoaNome']"";
org.w3c.dom.Element pessoaNome = null;

try 
{
    pessoaNome = (org.w3c.dom.Element) (Node) xPath.compile(expression).evaluate(doc, XPathConstants.NODE);
} 
catch (Exception e) 
{
    e.printStackTrace();
}

pessoaNome.setTextContext(""The new values for the node"");
ByteArrayOutputStream outputStream = new ByteArrayOutputStream();
Source xmlSource = new DOMSource(doc);
Result outputTarget = new StreamResult(outputStream);

Transformer transformer = TransformerFactory.newInstance().newTransformer();
transformer.setOutputProperty(OutputKeys.DOCTYPE_SYSTEM, ""HTML"");
transformer.transform(xmlSource, outputTarget);
InputStream is = new ByteArrayInputStream(outputStream.toByteArray()); // At this point outputStream is already all messed up, not just the '/'. but this is the only thing causing me problems
</code></pre>

<p>as @Lee pointed out, I changed it to use Jsoup. Code got a lot cleaner, just had to set up the outputSettings for it to work like a charm. Code below</p>

<pre><code>org.jsoup.nodes.Document doc = Jsoup.parse(new File(HTML), ""UTF-8"");

org.jsoup.nodes.Element pessoaNome = doc.getElementById(""pessoaNome"");

pessoaNome.html(""My new html in here"");

OutputSettings oSettings = new OutputSettings();
oSettings.syntax(org.jsoup.nodes.Document.OutputSettings.Syntax.xml);
doc.outputSettings(oSettings);
InputStream is = new ByteArrayInputStream(doc.outerHtml().getBytes());
</code></pre>
","transformer-model"
"37077673","Symfony: use DataTransformer in formType via Dependency injection","2016-05-06 16:53:30","","0","401","<symfony><service><sonata-admin><transformer-model>","<p>I want to use my DataTransformer to convert base64 strings to fileSystem images.</p>

<p>my code:</p>

<pre><code>services:
lion_visionomie_media_bundle_base64_data_transformer:
    class: Lion\Visionomie\MediaBundle\Transformer\Base64DataTransformer
    arguments: ['@sonata.media.generator.default', '@sonata.media.manager.media', '@sonata.media.pool', '@doctrine.orm.entity_manager']

lion_visionomie_media_bundle_slide_type:
    class: Lion\Visionomie\MediaBundle\Form\SlideType
    arguments: ['@lion_visionomie_media_bundle_base64_data_transformer']
    tags:
        - {name: form.type}

class Base64DataTransformer implements DataTransformerInterface
{
/**
 * @var GeneratorInterface
 */
private $pathGenerator;

/**
 * @var MediaManager
 */
private $mediaManager;

/**
 * @var Pool
 */
private $pool;

/**
 * @var EntityManagerInterface
 */
private $entityManager;

public function __construct(GeneratorInterface $generator, MediaManager $mediaManager, Pool $pool, EntityManagerInterface $entityManager)
{
    $this-&gt;pathGenerator = $generator;
    $this-&gt;mediaManager = $mediaManager;
    $this-&gt;pool = $pool;
    $this-&gt;entityManager = $entityManager;
}

/**
 * @param mixed $value
 * @return mixed
 */
public function transform($value)
{
    // [...]
    return $media;
}

/**
 * @param mixed $value
 * @return mixed
 */
public function reverseTransform($value)
{
    /** @var Media $media */
    $media = $value;

    return base64_encode(file_get_contents($media-&gt;getPreviousProviderReference()));
}
}

class SlideType extends AbstractType
{
/**
 * @var Base64DataTransformer
 */
private $base64DataTransformer;

public function __construct(Base64DataTransformer $base64DataTransformer)
{
    dump($base64DataTransformer);
    die;
    $this-&gt;base64DataTransformer = $base64DataTransformer;
}

/**
 * @param FormBuilderInterface $builder
 * @param array $options
 */
public function buildForm(FormBuilderInterface $builder, array $options)
{
    $builder
        -&gt;add('name')
        -&gt;add('active')
        -&gt;add('background')
            -&gt;addModelTransformer($this-&gt;base64DataTransformer)
        -&gt;add('thumbnail')

    ;
}
</code></pre>

<p>My current problem is, that the base64DataTransformer variable in SlideType is null.</p>

<p>Can you tell me why is this null?</p>

<p>Maybe I can use the ContainerAwareInterface in my SlideType. But I don't want to put my dependencies manually to my DataTransformer Class.</p>
","transformer-model"
"36941366","Java Transformer How to keep xml structure inside a CDATA","2016-04-29 14:48:07","","0","806","<java><xml><cdata><removing-whitespace><transformer-model>","<p>When I try to transform my document using the <strong>javax.xml.transform.Transformer</strong>, the API is inserting some spaces between tags inside of CDATASection.</p>

<p>Not sure why, I'm already using the output property <strong>INDENT</strong> as <strong>""no""</strong>.</p>

<hr>

<p>I want this:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;root&gt;
    &lt;infoDoc&gt;
        &lt;xml1&gt;&lt;![CDATA[&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;el1&gt;
    &lt;Header&gt;
        &lt;Success&gt;false&lt;/Success&gt;
    &lt;/Header&gt;
    &lt;Detail&gt;
        &lt;Key&gt;
            &lt;Number&gt;4721&lt;/Number&gt;
        &lt;/Key&gt;
    &lt;/Detail&gt;
&lt;/el1&gt;]]&gt;&lt;/xml1&gt;
    &lt;/infoDoc&gt;
&lt;/root&gt;
</code></pre>

<hr>

<p>Or this:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;root&gt;
    &lt;infoDoc&gt;
        &lt;xml1&gt;&lt;![CDATA[&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;&lt;el1&gt;&lt;Header&gt;&lt;Success&gt;false&lt;/Success&gt;&lt;/Header&gt;&lt;Detail&gt;&lt;Key&gt;&lt;Number&gt;4721&lt;/Number&gt;&lt;/Key&gt;&lt;/Detail&gt;&lt;/el1&gt;]]&gt;&lt;/xml1&gt;
    &lt;/infoDoc&gt;
&lt;/root&gt;
</code></pre>

<hr>

<p>But I'm receiving this:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;&lt;root&gt;&lt;infoDoc&gt;&lt;xml1&gt;&lt;![CDATA[&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;

&lt;el1&gt;

    &lt;Header&gt;

        &lt;Success&gt;false&lt;/Success&gt;

    &lt;/Header&gt;

    &lt;Detail&gt;

        &lt;Key&gt;

            &lt;Number&gt;4721&lt;/Number&gt;

        &lt;/Key&gt;

    &lt;/Detail&gt;

&lt;/el1&gt;]]&gt;&lt;/xml1&gt;&lt;/infoDoc&gt;&lt;/root&gt;
</code></pre>

<hr>

<p>Is there another property that do this?</p>

<p>Or another API, that can do it right?</p>

<p><strong>EDIT:</strong>
I'm using as follow:</p>

<pre><code>...
final Element xml1 = doc.createElement(""xml1"");
final CDATASection xml1Content = doc.createCDATASection(new String(bytes, Charset.forName(encoding)));
xml1.appendChild(xml1Content);
infoDoc.appendChild(xml1);
...


...
final DOMSource source = new DOMSource(document);
final ByteArrayOutputStream baos = new ByteArrayOutputStream();
transformer.setOutputProperty(OutputKeys.ENCODING, encoding);
transformer.setOutputProperty(OutputKeys.INDENT, ""no"");
transformer.transform(source, result);
...
</code></pre>
","transformer-model"
"36871384","Decimal Values scaling and precision in Datastage 11.5","2016-04-26 17:06:38","","0","4310","<datastage><transformer-model><ibm-infosphere>","<p>I have a column of decimal (5,5). I want to put default value as ""+000.00000"". I tried with several options such in meta data such as setting precision=5 or using the option ""Packed to no(separate)"", setting sign position=leading, but all I am getting value as ""+000000"". How can I correct this?</p>
","transformer-model"
"36739885","Can we create map with out creating tranformer class on gateway in spring integration","2016-04-20 09:45:02","","1","145","<spring><spring-integration><transformer-model>","<pre><code>&lt;int:gateway id=""com_java__TestService"" service-interface=""com.java.svc.TestService""&gt;
    &lt;int:method name=""testOperation"" payload-expression=""@convertObjectToMap.transformer(#args[0],#args[1])""  reply-channel=""replyChannel_testOperation"" request-channel=""requestChannel_testOperation""/&gt;

  &lt;/int:gateway&gt;
</code></pre>

<p>In above code snipped two parameter is coming from testOpeartion method of gateway and the reply-channel of this gateway is expecting map. So we want to convert object type parameters into map.</p>

<p>I have written transformer which returning map. And the requirement is to create map in spring configuration xml itself. I don't want to write any transformer class to convert map.</p>

<p>currently we have created custom convertObjectToMap bean which has transformer method which expected two parameter and it returns map.</p>

<p>can we cretae map with out using java transformer? </p>
","transformer-model"
"36711112","Mapping XML to another XML","2016-04-19 07:15:18","","3","661","<c#><xml><xslt><transformer-model>","<p>Our system acts as a middleware between several different systems. This requires working with several differently formatted and laid out XML files that contain (roughly) the same information. Currently this is handled via middleware knowing about all of those formats. This is becoming unmanageable and we want to replace it with a path which allows our system to just work with the standard format, and convert our client files to and from using a transformer layer. However, we have a lot of clients coming on board (hence the rework), and would like to reduce the amount of extra developer work to a minimum per client.</p>

<p>On investigation, I can see three primary ways of doing this:</p>

<p>a) Use an XSLT transformer. Handle each client and XML file separately. Pros: very clear to understand, little overhead during runtime, simple text conversion, can be just dropped and uploaded, can largely be reused if client formats happen to be very similar. Cons: Requires a lot of work to get all XML files converted per client. Can technically be done by a non-developer.</p>

<p>b) Utilise AutoMapper: Create initial classes for our format, then run all client files through an XML -> POCO generator, and create profiles for each of them. Pros: Fairly simple to understand, maps could potentially be reused for different clients, strong typing for clarity and maintenance ease. Cons: Can only be done by developer, requires actual redeployment (I can't see us implementing a plugin system for that, but I can see it being a microservice).</p>

<p>c) Create a ""visual"" custom mapper and mapping engine. Use reflection to loop through properties and customize the mappings through a web-based interface. Expose this interface to customers and tell them which information they want mapped where after they have uploaded the XML and the system has autogenerated the POCOs. Pros: Very flexible, self-maintaining. Cons: Requires .... a LOT of work and time. </p>

<p>d) ??? Maybe something better?</p>

<p>So I'm trying to pick between one of those three, but not quite sure which one to go for. I'm starting to incline towards the XSLT because it'll technically allow anybody to do the transformation. Of course we would need to implement a verification system, but that's... an implementation detail.</p>
","transformer-model"
"36622146","League\Fractal transform item inside array","2016-04-14 11:47:48","","1","1063","<php><api><laravel><transformer-model>","<p>need a little help with League\Fractal, i'm trying to create object to imitate FeatureCollection of GoogleMapApi DataLayer</p>

<pre><code>FeatureCollection
+ type
+ features =&gt; [
  Feature
  {
     type,
     geometry =&gt; 
          {
          type,
          coordinates =&gt; [latitude, longitudes]
          }
   }
]
</code></pre>

<p>I've successfully create Transformer for Geometry and Feature, yet having a problem with FeatureCollection because attributes features is an array with element of Feature.</p>

<pre><code>{
    ""type"": ""FeatureCollection"",
    ""features"": [
        [],
        [],
    ]
}
</code></pre>

<p>How can i transform the inside of features element correctly?</p>
","transformer-model"
"36263247","Trying to remove data in Fractal by implementing ArraySerializer in Laravel 5.2","2016-03-28 13:19:44","","8","3959","<php><laravel><serialization><jsonserializer><transformer-model>","<p>I've got the API working using the standard process, but I want to remove the <code>data</code> namespace from the JSON output. I see I need to implement ArraySerializer, I have been through the Fractal docs, but I can't work out where I need to added it in Laravel 5.2</p>

<p>I found <a href=""http://stackoverflow.hex1.ru/a/34041003"" rel=""noreferrer"">this answer</a> but I'm just getting the same output at the line of code I commented out:</p>

<pre><code>class TrackController extends ApiController
{
    public function index()
    {
        $tracks = Track::all();
        //return $this-&gt;respondWithCollection($tracks, new TrackTransformer);
        // Same response as the commented out line above
        $response = new \League\Fractal\Resource\Collection($tracks, new TrackTransformer);
        $manager = new \League\Fractal\Manager();
        $manager-&gt;setSerializer(new \League\Fractal\Serializer\ArraySerializer());
        return response()-&gt;json($manager-&gt;createData($response)-&gt;toArray());
    }

    public function show($id)
    {
        $track = Track::find($id);
        return $this-&gt;respondWithItem($track, new TrackTransformer);
    }
}
</code></pre>

<p>Also, I'm implementing this on a specific controller, even if I got this working, where do I add the code/class so I can get ArraySerializer output for all my controllers?</p>

<p>I've <a href=""https://github.com/jackbarham/laravel-fractal-api/blob/master/app/Http/Controllers/TrackController.php"" rel=""noreferrer"">posted this on Github</a> if that helps.</p>
","transformer-model"
"36237680","Mule 3.7 transformer with annotations for custom Objects","2016-03-26 16:18:06","","1","1391","<mule><transformer-model>","<p>I have two custom Classes : Person, Manager. I wrote the following transformer to transform Person to Manager object.
    package com.learning.transformers;</p>

<pre><code>import org.mule.api.annotations.ContainsTransformerMethods;
import org.mule.api.annotations.Transformer;

import com.learning.beans.Manager;
import com.learning.beans.Person;

@ContainsTransformerMethods
public class AnnotatedTransformer {

    @Transformer
    public Manager transformToManager(@Payload Person person){
        Manager manager = new Manager();
        manager.setPerson(person);
        manager.setDesignation(""Manager"");
        return manager;
    }

}
</code></pre>

<p>My configuraiton file is as follows.</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;

&lt;mule xmlns:json=""http://www.mulesoft.org/schema/mule/json"" xmlns:http=""http://www.mulesoft.org/schema/mule/http"" xmlns=""http://www.mulesoft.org/schema/mule/core"" xmlns:doc=""http://www.mulesoft.org/schema/mule/documentation""
    xmlns:spring=""http://www.springframework.org/schema/beans"" 
    xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-current.xsd
http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
http://www.mulesoft.org/schema/mule/json http://www.mulesoft.org/schema/mule/json/current/mule-json.xsd""&gt;
    &lt;spring:beans&gt;
        &lt;spring:bean id=""anTransformer"" name=""anT"" class=""com.learning.transformers.AnnotatedTransformer""/&gt;
    &lt;/spring:beans&gt;

    &lt;flow name=""mule-learning-transformersFlow1""&gt;
        &lt;http:listener config-ref=""GlobalHTTPConnector"" path=""/customTransformation"" doc:name=""HTTP""/&gt;
        &lt;json:json-to-object-transformer returnClass=""com.learning.beans.Person"" doc:name=""JSON to Object""/&gt;
        &lt;!-- &lt;auto-transformer returnClass=""com.learning.beans.Manager"" /&gt; --&gt;
        &lt;component class=""com.learning.components.ManagerLoggingComponent"" doc:name=""Java"" /&gt;
    &lt;/flow&gt;
&lt;/mule&gt;
</code></pre>

<p>My ManagerLoggingComponent looks like : </p>

<pre><code>package com.learning.components;

import com.learning.beans.Manager;
import com.learning.beans.Person;
import org.mule.api.annotations.param.Payload;

public class ManagerLoggingComponent {
    public void logManager(@Payload Manager manager){
        Person person = manager.getPerson();
        System.out.println(""Name: ""+person.getName());
        System.out.println(""Age: ""+person.getAge());
        System.out.println(""Designation: ""+manager.getDesignation());
    }
}
</code></pre>

<p>I first send some json input to the flow. That is converted to Person object using json-to-object transformer. Then my component is placed which expects Manager object. I have registered my annotated transformer as spring bean. Hence I expect Person object to be converted to Manager object before passing it to Component. But it is throwing following exception.</p>

<blockquote>
  <p>Message               : Failed to transform from ""json"" to
  ""com.learning.beans.Manager"" Type                  :
  org.mule.api.transformer.TransformerException Code                  :
  MULE_ERROR-109 JavaDoc               :
  <a href=""http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/transformer/TransformerException.html"" rel=""nofollow"">http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/transformer/TransformerException.html</a>
  Transformer           : JsonToObject{this=d61a0b,
  name='JsonToManager', ignoreBadInput=false,
  returnClass=SimpleDataType{type=com.learning.beans.Manager,
  mimeType='<em>/</em>', encoding='null'},
  sourceTypes=[SimpleDataType{type=java.io.Reader, mimeType='<em>/</em>',
  encoding='null'}, SimpleDataType{type=java.net.URL, mimeType='<em>/</em>',
  encoding='null'}, SimpleDataType{type=java.io.File, mimeType='<em>/</em>',
  encoding='null'}, SimpleDataType{type=java.lang.String,
  mimeType='<em>/</em>', encoding='null'},
  SimpleDataType{type=java.io.InputStream, mimeType='<em>/</em>',
  encoding='null'}, SimpleDataType{type=[B, mimeType='<em>/</em>',
  encoding='null'}]}</p>
</blockquote>

<p>Also, while starting the server I found that it is throwing the following error by which I understand that it is un-registering the transformToManager transformer. That might be the reason for not picking up my custom transformer. This is being observed in mule 3.7. 3.5 is working smoothly.</p>

<blockquote>
  <p>WARN  2016-03-27 14:01:06,574 [main] org.mule.config.spring.SpringRegistry: Could not apply shutdown lifecycle to object 'AnnotatedTransformer.transformToManager' after being unregistered.
   org.mule.api.lifecycle.InitialisationException: An Expression Evaluator for ""payload"" is not registered with Mule. Make sure you have the the module for this expression type on your classpath. for example, if you are using an xpath expression you need to have the Mule XML module on your classpath.
       at org.mule.expression.transformers.AbstractExpressionTransformer.initialise(AbstractExpressionTransformer.java:85) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.config.expression.ExpressionAnnotationsHelper.getTransformerForMethodWithAnnotations(ExpressionAnnotationsHelper.java:53) ~[mule-module-annotations-3.7.3.jar:3.7.3]
       at org.mule.config.transformer.AnnotatedTransformerProxy.initialise(AnnotatedTransformerProxy.java:109) ~[mule-module-annotations-3.7.3.jar:3.7.3]
       at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source) ~[?:?]
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_72]
       at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_72]
       at org.mule.lifecycle.phases.DefaultLifecyclePhase.applyLifecycle(DefaultLifecyclePhase.java:237) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.lifecycle.phases.MuleContextInitialisePhase.applyLifecycle(MuleContextInitialisePhase.java:71) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.lifecycle.RegistryLifecycleManager.applyPhase(RegistryLifecycleManager.java:183) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.registry.AbstractRegistry.unregisterObject(AbstractRegistry.java:163) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.config.spring.SpringRegistry$ConfigurableRegistrationDelegate.doRegisterObject(SpringRegistry.java:451) ~[mule-module-spring-config-3.7.3.jar:3.7.3]
       at org.mule.config.spring.SpringRegistry$ConfigurableRegistrationDelegate.registerObject(SpringRegistry.java:405) ~[mule-module-spring-config-3.7.3.jar:3.7.3]
       at org.mule.config.spring.SpringRegistry.registerObject(SpringRegistry.java:253) ~[mule-module-spring-config-3.7.3.jar:3.7.3]
       at org.mule.registry.AbstractRegistryBroker.registerObject(AbstractRegistryBroker.java:249) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.registry.AbstractRegistryBroker.registerObject(AbstractRegistryBroker.java:262) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.registry.MuleRegistryHelper.registerObject(MuleRegistryHelper.java:816) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.registry.MuleRegistryHelper.registerTransformer(MuleRegistryHelper.java:458) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.config.transformer.AnnotatedTransformerObjectProcessor.process(AnnotatedTransformerObjectProcessor.java:76) ~[mule-module-annotations-3.7.3.jar:3.7.3]
       at org.mule.config.spring.processors.TransformerAnnotatedBeanProcessor.postProcessBeforeInitialization(TransformerAnnotatedBeanProcessor.java:30) ~[mule-module-spring-config-3.7.3.jar:3.7.3]
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsBeforeInitialization(AbstractAutowireCapableBeanFactory.java:408) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
       at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:753) ~[mule-module-spring-config-3.7.3.jar:4.1.6.RELEASE]
       at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757) ~[spring-context-4.1.6.RELEASE.jar:4.1.6.RELEASE]
       at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480) ~[spring-context-4.1.6.RELEASE.jar:4.1.6.RELEASE]
       at org.mule.config.spring.SpringRegistry.doInitialise(SpringRegistry.java:108) ~[mule-module-spring-config-3.7.3.jar:3.7.3]
       at org.mule.registry.AbstractRegistry.initialise(AbstractRegistry.java:104) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.config.spring.SpringXmlConfigurationBuilder.createSpringRegistry(SpringXmlConfigurationBuilder.java:172) ~[mule-module-spring-config-3.7.3.jar:3.7.3]
       at org.mule.config.spring.SpringXmlConfigurationBuilder.doConfigure(SpringXmlConfigurationBuilder.java:95) ~[mule-module-spring-config-3.7.3.jar:3.7.3]
       at org.mule.config.builders.AbstractConfigurationBuilder.configure(AbstractConfigurationBuilder.java:43) ~[mule-core-3.7.3.jar:3.7.3]
       at org.mule.config.builders.AbstractResourceConfigurationBuilder.configure(AbstractResourceConfigurationBuilder.java:69) ~[mule-core-3.7.3.jar:3.7.3]
       ............
  My request look like</p>
</blockquote>

<pre><code>{
    ""name"":""Sai"",
    ""age"" : ""100000""
}
</code></pre>

<p>Any ideas on what is happening here?</p>

<p><strong>Everything is working as expected in Mule 3.5. Problem is with mule 3.7.</strong></p>

<blockquote>
  <p>Update 1 : Full log of Excetion:</p>
</blockquote>

<pre><code>    Message               : Failed to transform from ""json"" to ""com.learning.beans.Manager""
Type                  : org.mule.api.transformer.TransformerException
Code                  : MULE_ERROR-109
JavaDoc               : http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/transformer/TransformerException.html
Transformer           : JsonToObject{this=13de431, name='JsonToManager', ignoreBadInput=false, returnClass=SimpleDataType{type=com.learning.beans.Manager, mimeType='*/*', encoding='null'}, sourceTypes=[SimpleDataType{type=java.io.Reader, mimeType='*/*', encoding='null'}, SimpleDataType{type=java.net.URL, mimeType='*/*', encoding='null'}, SimpleDataType{type=java.io.File, mimeType='*/*', encoding='null'}, SimpleDataType{type=java.lang.String, mimeType='*/*', encoding='null'}, SimpleDataType{type=java.io.InputStream, mimeType='*/*', encoding='null'}, SimpleDataType{type=[B, mimeType='*/*',  encoding='null'}]}
********************************************************************************
Exception stack is:
1. null (java.lang.NullPointerException)
  java.io.Reader:78 (null)
2. Failed to transform from ""json"" to ""com.learning.beans.Manager"" (org.mule.api.transformer.TransformerException)
  org.mule.module.json.transformers.JsonToObject:133 (http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/transformer/TransformerException.html)
********************************************************************************
Root Exception stack trace:
java.lang.NullPointerException
    at java.io.Reader.&lt;init&gt;(Reader.java:78)
    at java.io.InputStreamReader.&lt;init&gt;(InputStreamReader.java:97)
    at org.mule.module.json.transformers.JsonToObject.transformMessage(JsonToObject.java:119)
    at org.mule.transformer.AbstractMessageTransformer.transform(AbstractMessageTransformer.java:141)
    at org.mule.transformer.AbstractMessageTransformer.transform(AbstractMessageTransformer.java:69)
    at org.mule.DefaultMuleMessage.getPayload(DefaultMuleMessage.java:425)
    at org.mule.DefaultMuleMessage.getPayload(DefaultMuleMessage.java:373)
    at org.mule.expression.MessagePayloadExpressionEvaluator.evaluate(MessagePayloadExpressionEvaluator.java:79)
    at org.mule.expression.DefaultExpressionManager.evaluate(DefaultExpressionManager.java:318)
    at org.mule.expression.transformers.ExpressionArgument.evaluate(ExpressionArgument.java:116)
    at org.mule.expression.transformers.ExpressionTransformer.transformMessage(ExpressionTransformer.java:51)
    at org.mule.transformer.AbstractMessageTransformer.transform(AbstractMessageTransformer.java:141)
    at org.mule.transformer.AbstractMessageTransformer.transform(AbstractMessageTransformer.java:69)
    at org.mule.transformer.AbstractTransformer.transform(AbstractTransformer.java:366)
    at org.mule.impl.model.resolvers.AnnotatedEntryPointResolver.getPayloadFromMessageWithAnnotations(AnnotatedEntryPointResolver.java:175)
    at org.mule.impl.model.resolvers.AnnotatedEntryPointResolver.getPayloadForMethod(AnnotatedEntryPointResolver.java:161)
    at org.mule.impl.model.resolvers.AnnotatedEntryPointResolver.invoke(AnnotatedEntryPointResolver.java:130)
    at org.mule.model.resolvers.DefaultEntryPointResolverSet.invoke(DefaultEntryPointResolverSet.java:36)
    at org.mule.component.DefaultComponentLifecycleAdapter.invoke(DefaultComponentLifecycleAdapter.java:339)
    at org.mule.component.AbstractJavaComponent.invokeComponentInstance(AbstractJavaComponent.java:82)
    at org.mule.component.AbstractJavaComponent.doInvoke(AbstractJavaComponent.java:73)
    at org.mule.component.AbstractComponent.invokeInternal(AbstractComponent.java:120)
    at org.mule.component.AbstractComponent.access$000(AbstractComponent.java:55)
    at org.mule.component.AbstractComponent$1$1.process(AbstractComponent.java:236)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:24)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:107)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:88)
    at org.mule.processor.BlockingProcessorExecutor.execute(BlockingProcessorExecutor.java:59)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:24)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:107)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.component.AbstractComponent.process(AbstractComponent.java:154)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:24)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:107)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:98)
    at org.mule.processor.BlockingProcessorExecutor.execute(BlockingProcessorExecutor.java:59)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:24)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:98)
    at org.mule.processor.BlockingProcessorExecutor.execute(BlockingProcessorExecutor.java:59)
    at org.mule.processor.AsyncInterceptingMessageProcessor.process(AsyncInterceptingMessageProcessor.java:102)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:24)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:107)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:98)
    at org.mule.processor.BlockingProcessorExecutor.e...
********************************************************************************
</code></pre>

<blockquote>
  <p>Update 2 : I started getting the below exception suddenly. I didn't change any code. It is showing the below exception some times and above exception some other times.</p>
</blockquote>

<pre><code>    Message               : The object transformed is of type: ""SimpleDataType{type=java.lang.String, mimeType='*/*', encoding='null'}"", but the expected return type is ""SimpleDataType{type=com.learning.beans.Manager, mimeType='application/json', encoding='null'}"". The current MuleMessage is null! Please report this to mule-esb@mulesoft.com
Type                  : org.mule.api.transformer.TransformerMessagingException
Code                  : MULE_ERROR--2
JavaDoc               : http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/transformer/TransformerMessagingException.html
Payload               : {NullPayload}
********************************************************************************
Exception stack is:
1. The object transformed is of type: ""SimpleDataType{type=java.lang.String, mimeType='*/*', encoding='null'}"", but the expected return type is ""SimpleDataType{type=com.learning.beans.Manager, mimeType='application/json', encoding='null'}"". The current MuleMessage is null! Please report this to mule-esb@mulesoft.com (org.mule.api.transformer.TransformerMessagingException)
  org.mule.transformer.AbstractMessageTransformer:179 (http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/transformer/TransformerMessagingException.html)
********************************************************************************
Root Exception stack trace:
org.mule.api.transformer.TransformerMessagingException: The object transformed is of type: ""SimpleDataType{type=java.lang.String, mimeType='*/*', encoding='null'}"", but the expected return type is ""SimpleDataType{type=com.learning.beans.Manager, mimeType='application/json', encoding='null'}"". The current MuleMessage is null! Please report this to mule-esb@mulesoft.com
    at org.mule.transformer.AbstractMessageTransformer.checkReturnClass(AbstractMessageTransformer.java:179)
    at org.mule.transformer.AbstractMessageTransformer.transform(AbstractMessageTransformer.java:158)
    at org.mule.transformer.AbstractMessageTransformer.transform(AbstractMessageTransformer.java:69)
    at org.mule.DefaultMuleMessage.getPayload(DefaultMuleMessage.java:425)
    at org.mule.DefaultMuleMessage.getPayload(DefaultMuleMessage.java:373)
    at org.mule.expression.MessagePayloadExpressionEvaluator.evaluate(MessagePayloadExpressionEvaluator.java:79)
    at org.mule.expression.DefaultExpressionManager.evaluate(DefaultExpressionManager.java:318)
    at org.mule.expression.transformers.ExpressionArgument.evaluate(ExpressionArgument.java:116)
    at org.mule.expression.transformers.ExpressionTransformer.transformMessage(ExpressionTransformer.java:51)
    at org.mule.transformer.AbstractMessageTransformer.transform(AbstractMessageTransformer.java:141)
    at org.mule.transformer.AbstractMessageTransformer.transform(AbstractMessageTransformer.java:69)
    at org.mule.transformer.AbstractTransformer.transform(AbstractTransformer.java:366)
    at org.mule.impl.model.resolvers.AnnotatedEntryPointResolver.getPayloadFromMessageWithAnnotations(AnnotatedEntryPointResolver.java:175)
    at org.mule.impl.model.resolvers.AnnotatedEntryPointResolver.getPayloadForMethod(AnnotatedEntryPointResolver.java:161)
    at org.mule.impl.model.resolvers.AnnotatedEntryPointResolver.invoke(AnnotatedEntryPointResolver.java:130)
    at org.mule.model.resolvers.DefaultEntryPointResolverSet.invoke(DefaultEntryPointResolverSet.java:36)
    at org.mule.component.DefaultComponentLifecycleAdapter.invoke(DefaultComponentLifecycleAdapter.java:339)
    at org.mule.component.AbstractJavaComponent.invokeComponentInstance(AbstractJavaComponent.java:82)
    at org.mule.component.AbstractJavaComponent.doInvoke(AbstractJavaComponent.java:73)
    at org.mule.component.AbstractComponent.invokeInternal(AbstractComponent.java:120)
    at org.mule.component.AbstractComponent.access$000(AbstractComponent.java:55)
    at org.mule.component.AbstractComponent$1$1.process(AbstractComponent.java:236)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:24)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:107)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:88)
    at org.mule.processor.BlockingProcessorExecutor.execute(BlockingProcessorExecutor.java:59)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:24)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:107)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.component.AbstractComponent.process(AbstractComponent.java:154)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:24)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:107)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:98)
    at org.mule.processor.BlockingProcessorExecutor.execute(BlockingProcessorExecutor.java:59)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:24)
    at org.mule.execution.MessageProcessorExecutionTemplate.execute(MessageProcessorExecutionTemplate.java:44)
    at org.mule.processor.BlockingProcessorExecutor.executeNext(BlockingProcessorExecutor.java:98)
    at org.mule.processor.BlockingProcessorExecutor.execute(BlockingProcessorExecutor.java:59)
    at org.mule.processor.AsyncInterceptingMessageProcessor.process(AsyncInterceptingMessageProcessor.java:102)
    at org.mule.execution.ExceptionToMessagingExceptionExecutionInterceptor.execute(ExceptionToMessagingExceptionExecutionInterceptor.java:24)
    at org.mule.execution.MessageProcessorNotificationExecutionInterceptor.execute(MessageProcessorNotificationExecutionInterceptor.java:107)
    at org.m...
********************************************************************************
</code></pre>
","transformer-model"
"36218833","how to remove header when passing view with function using transfromer(with data as transformer data) in laravel","2016-03-25 11:12:47","","1","133","<php><laravel><transformer-model>","<p>I am get data from model and then passed to transformer while get trasformer's be passed to with view page when the to printed using echo.the output will be come with header how to remove header </p>

<p>this is my code:</p>

<pre><code>public function productView($sulg)
    {
        $productcount = Product::where('deletestatus','=',0)-&gt;where('slug','=',$sulg)-&gt;count();
        if($productcount != 0 )
        {
            $productdata = Product::where('deletestatus','=',0)-&gt;where('slug','=',$sulg)-&gt;get();
            $review_product_information = $this-&gt;response-&gt;withCollection($productdata, new ProductTransformer);
            return view('frontview.productview')-&gt;with('productdata', $review_product_information);
        }
        else
        {
            return view('frontview.404');
        }

    }
</code></pre>

<p>and I got output like this:</p>

<pre><code>HTTP/1.0 200 OK Cache-Control: no-cache Content-Type: application/json Date: Fri, 25 Mar 2016 10:54:53 GMT {""data"":[{""product_id"":6,""name"":""Saree"",""sku"":""h5f6uadl"",""slug"":""saree"",""description"":""Blue Saree"",""special_price_mrp"":""5600.00"",""regular_price"":""8000.00"",""wsprice"":""5500.00"",""quantity"":50,""offer_percentage"":""40%"",""min_pay_for_rp"":null,""discount_for_pp"":null,""pro_tax"":""1"",""category_id"":""6"",""product_images"":[{""image_id"":1}]}]} 
</code></pre>
","transformer-model"
"35835961","Why does Transformer return &lt and &gt instead of < and >?","2016-03-07 03:59:13","35837419","1","1678","<java><xml><transformer-model>","<pre><code>trnsformer.transform(DomSource, streamResult);
</code></pre>

<p>Input in DomSource contains many <code>&lt;br&gt;</code> tags, but instead I get <code>&amp;gt</code> and <code>&amp;lt</code> instead of <code>&lt;</code> and <code>&gt;</code>  <code>&lt;br&gt;</code> return as <code>&amp;lt br &amp;gt</code> </p>

<p>I know <code>&amp;lt &amp;gt</code> are equivalent to <code>&lt;&gt;</code>. How can I make transformer class to change the encoding and return <code>&lt;br&gt;</code> instead ? </p>

<p><strong>XML creator</strong> </p>

<pre><code>public class CreatXML 
{ 

  public static void main(String[] args){

  try {
    File article = new File(""article.txt"");
    Scanner scan = new Scanner (article);
    StringBuilder str = new StringBuilder();
    while (scan.hasNext())
    { 
      str.append(scan.nextLine());
      str.append(""&lt;br&gt;"");
    }
    DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
    DocumentBuilder builder = factory.newDocumentBuilder();
    Document doc = builder.newDocument();

    Element body  = doc.createElement(""div"");
    doc.appendChild(body);

    Attr classAttr = doc.createAttribute(""class"");
    classAttr.setValue(""code"");
    body.setAttributeNode(classAttr);

    Element p = doc.createElement(""p"");
    p.appendChild(doc.createTextNode(str.toString()));
    body.appendChild(p);

    TransformerFactory transFatory = TransformerFactory.newInstance();
    Transformer transformer = transFatory.newTransformer();
    DOMSource dom = new DOMSource(doc);

    StringWriter writer = new StringWriter();
    StreamResult result = new StreamResult(writer);
    transformer.transform(dom, result);
    System.out.println(writer.toString());

  }catch (Exception e){e.printStackTrace();}  
  }
}
</code></pre>

<p><strong>input sample</strong></p>

<p><code>&lt;br&gt;</code>this is an input sample<code>&lt;br&gt;</code></p>

<p><strong>output</strong> </p>

<p><code>&lt;?xml [stuff] &gt;&lt;div&gt;&lt;p&gt;&amp;lt;br&amp;gt;</code>this is an input sample<code>&amp;lt;br&amp;gt;&amp;lt;br&amp;gt;&lt;/p&gt;&lt;/div&gt;</code></p>
","transformer-model"
"35702868","How to access record variables in a java transaformer","2016-02-29 15:06:37","35703831","0","850","<java><mule><transformer-model>","<p>I'm converting a flow from straight single thread, to use a batch processor. So, I'm converting most of the flow variables to recordVars. Some questions i can't find in the docs:</p>

<ul>
<li>How do i access a record var in a java transformer? I'm used to message.getInvocationProperty for flow vars</li>
<li>What happens when i change a flow var - can other threads in the batch see the changes? </li>
</ul>
","transformer-model"
"35571413","How to prettify XML String in Java","2016-02-23 07:35:00","35572090","0","2821","<java><xml><dom><pretty-print><transformer-model>","<p>I have an XML String which is not formatted properly. I would like to do proper indentation using Java. There are a lot of answers on SO regarding this problem. One of the most widely accepted and brilliant answers is this one:</p>

<p><a href=""https://stackoverflow.com/questions/25864316/pretty-print-xml-in-java-8/33541820#33541820"">Pretty print XML in java 8</a></p>

<p>But the problem with this answer is that, the code always needs a root element whereas I have XML tags in my String as follows: </p>

<pre><code>&lt;person&gt;
&lt;address&gt;New York&lt;/address&gt;
&lt;/person&gt;
&lt;person&gt;
&lt;address&gt;Ottawa&lt;/address&gt;
&lt;/person&gt;
</code></pre>

<p>As you can see there is no root element here. Just multiple tags of person. </p>

<p>I have tried to see any methods available from the libraries used in the above answer. But to no use. </p>

<p>I don't know if there is a way out. But if someone can think of something I would really appreciate it. </p>

<p><strong>P.S. Please please before you mark this as Duplicate, read the question. I know there are similar questions on SO but my problem is quite different.</strong> </p>
","transformer-model"
"35460701","Bind a variable in query using a Transformer","2016-02-17 15:20:40","","1","39","<java><jena><transformer-model>","<p>I want to bind a specific subject variable within a query.  For instance, supposing that the Op is <code>subOp</code>, the following is the transformer that I'm trying to use.  However, at the end, when I print <code>query</code>, my subject variable is the same and has not been bound.  Why?</p>

<pre><code>Op BindedQueryWithKey = Transformer.transform(new TransformCopy(){
        public Op transform(OpTriple optp, Op subOp){
            Triple tp=optp.getTriple();
            if (tp.getSubject().isVariable() &amp;&amp; tp.getSubject().toString().equalsIgnoreCase(""userid""))
                {Triple newtp=new Triple(nodeId, tp.getPredicate(), tp.getObject());
            return new OpTriple(newtp);}
            else return optp;
        }
    }, subOp);

    Query query = OpAsQuery.asQuery(BindedQueryWithKey);
</code></pre>
","transformer-model"
"35222950","Mule - JSON to XML - Unbound prefix","2016-02-05 11:24:27","35226083","0","754","<json><xml><mule><prefix><transformer-model>","<p>With my <code>mule flow</code> I get a <code>JSON</code> message and I use a <code>JSON to XML transformer</code> to send the XML to a Web Service.</p>

<p><code>HTTP</code> => <code>JSON to XML</code> => <code>WS Consumer</code></p>

<p>The XML needs a prefix ""<strong>int:</strong>"" : </p>

<pre><code>&lt;int:contact&gt;Name&lt;/int:contact&gt;
</code></pre>

<p>And the JSON format is like this:</p>

<pre><code>{  
   ""Modify"":{  
      ""int:contact"":""Name""
   }
}
</code></pre>

<p>The <code>JSON to XML transformer</code> return an error:</p>

<blockquote>
  <p>javax.xml.stream.XMLStreamException: Unbound prefix: int</p>
</blockquote>

<p>How can I pass the prefix?</p>
","transformer-model"
"35170401","Changing XML to POJO using data weave transformer in mule","2016-02-03 06:55:14","","0","773","<mule><transformer-model><anypoint-studio>","<p>I am trying to convert a XML file to a POJO using data weave in mule, but it throws this exception -: </p>

<p>Message               : Exception while executing: 
        time: payload.deliveryMessageDate.time
                      ^
Type mismatch
     found :name, :binary
  required :name, :object
Type                  : com.mulesoft.weave.mule.exception.WeaveExecutionException
Code                  : MULE_ERROR--2</p>

<hr>

<p>Exception stack is:
1. Type mismatch
     found :name, :binary
  required :name, :object (com.mulesoft.weave.engine.ast.dynamic.DynamicDispatchException)
  com.mulesoft.weave.engine.ast.dynamic.DynamicDispatchNode:65 (null)
2. Exception while executing: 
        time: payload.deliveryMessageDate.time
                      ^
Type mismatch
     found :name, :binary
  required :name, :object (com.mulesoft.weave.mule.exception.WeaveExecutionException)
  com.mulesoft.weave.mule.WeaveMessageProcessor:124 (null)</p>

<hr>

<p>Root Exception stack trace:
com.mulesoft.weave.engine.ast.dynamic.DynamicDispatchException: Type mismatch
     found :name, :binary
  required :name, :object
    at com.mulesoft.weave.engine.ast.dynamic.DynamicDispatchNode.dispatchNode(DynamicDispatchNode.scala:65)
    at com.mulesoft.weave.engine.ast.dynamic.DynamicDispatchNode.valueType(DynamicDi...</p>

<p>I haven't used any annotation on my POJO and the xml has some attributes inside the tags. </p>
","transformer-model"
"34017140","Transform eloquent model using fractal","2015-12-01 09:40:47","34041003","2","2837","<php><laravel><eloquent><transformer-model>","<p>I am trying to use <a href=""http://fractal.thephpleague.com/"" rel=""nofollow"">fractal</a> to convert dates in my <a href=""http://laravel.com/docs/5.1/eloquent"" rel=""nofollow"">eloquent</a> model when sending it to a web page via ajax. However, I'm not getting the expected transformed data object. Here's my code:</p>

<p>Transformer:</p>

<pre><code>&lt;?php

namespace App\Transformers\Models;

use App\Models\Student;
use League\Fractal;

class StudentTransformer extends Fractal\TransformerAbstract
{
    public function transform(Student $student)
    {
        return [
            'name'           =&gt; $student-&gt;name,
            'birth_date'     =&gt; date('d-m-Y', strtotime($student-&gt;birth_date)),
            'start_date'     =&gt; date('d-m-Y', strtotime($student-&gt;start_date)),
            'is_active'      =&gt; $student-&gt;is_active ? 'Yes' : 'No',
            'course_title'   =&gt; $student-&gt;course_title,
            'university_id'  =&gt; $student-&gt;university_id,
            'institution_id' =&gt; $student-&gt;institution_id,
            'course_id'      =&gt; $student-&gt;course_id,
        ];
    }
}
</code></pre>

<p>Routes file:</p>

<pre><code>Route::get('/', function () {

    $student = App\Models\Student::first();

    $response = new League\Fractal\Resource\Item($student, new App\Transformers\Models\StudentTransformer);

    return response()-&gt;json([$response]);

});
</code></pre>

<p>returns this:</p>

<pre><code>[
    { }
]
</code></pre>

<p>if I <code>dd()</code>, like so:</p>

<pre><code>Route::get('/', function () {

    $student = App\Models\Student::first();

    $response = new League\Fractal\Resource\Item($student, new App\Transformers\Models\StudentTransformer);

    dd($response);

});
</code></pre>

<p>I get the following, with no apparent transformations made</p>

<pre><code>Item {#299 ▼
  #data: Student {#305 ▼
    #fillable: array:10 [▼
      0 =&gt; ""name""
      1 =&gt; ""birth_date""
      2 =&gt; ""start_date""
      3 =&gt; ""is_active""
      4 =&gt; ""course_title""
      5 =&gt; ""university_id""
      6 =&gt; ""institution_id""
      7 =&gt; ""course_id""
    ]
    #connection: null
    #table: null
    #primaryKey: ""id""
    #perPage: 15
    +incrementing: true
    +timestamps: true
    #attributes: array:13 [▼
      ""id"" =&gt; 1
      ""name"" =&gt; ""Michel Jast""
      ""birth_date"" =&gt; ""1979-12-22""
      ""start_date"" =&gt; ""2015-08-02""
      ""is_active"" =&gt; 1
      ""course_title"" =&gt; ""quia""
      ""university_id"" =&gt; ""10954""
      ""institution_id"" =&gt; 1044
      ""course_id"" =&gt; 1
      ""created_at"" =&gt; ""2015-11-23 09:40:35""
      ""updated_at"" =&gt; ""2015-11-26 10:24:14""
    ]
    #original: array:13 [▼
      ""id"" =&gt; 1
      ""name"" =&gt; ""Michel Jast""
      ""birth_date"" =&gt; ""1979-12-22""
      ""start_date"" =&gt; ""2015-08-02""
      ""is_active"" =&gt; 1
      ""course_title"" =&gt; ""quia""
      ""university_id"" =&gt; ""10954""
      ""institution_id"" =&gt; 1044
      ""course_id"" =&gt; 1
      ""created_at"" =&gt; ""2015-11-23 09:40:35""
      ""updated_at"" =&gt; ""2015-11-26 10:24:14""
    ]
    #relations: []
    #hidden: []
    #visible: []
    #appends: []
    #guarded: array:1 [▶]
    #dates: []
    #dateFormat: null
    #casts: []
    #touches: []
    #observables: []
    #with: []
    #morphClass: null
    +exists: true
  }
  #meta: []
  #resourceKey: null
  #transformer: StudentTransformer {#301 ▼
    #availableIncludes: []
    #defaultIncludes: []
    #currentScope: null
  }
}
</code></pre>

<p>Any ideas what I'm missing here?</p>
","transformer-model"
"33720090","Exception being thrown when using Transform from xml to text file via xslt","2015-11-15 13:14:45","33720192","2","717","<java><xml><xslt><transformer-model>","<p>My code looks as the following:</p>

<pre><code>  protected String[] parseXMLRecievedMsg(StringBuffer iSB_plainMessage) throws Exception {

    Source xslt = new StreamSource(new File(""sample.xslt""));
    TransformerFactory transFact = TransformerFactory.newInstance();
    Transformer transformer = transFact.newTransformer(xslt);
    StringWriter outWriter = new StringWriter();
    StreamResult lS_outputMsgBody = new StreamResult( outWriter );
    iSB_plainMessage =iSB_plainMessage.delete(0,iSB_plainMessage.indexOf(""\n""));
    iSB_plainMessage =iSB_plainMessage.reverse();
    iSB_plainMessage =iSB_plainMessage.delete(0, iSB_plainMessage.indexOf(""\n""));
    iSB_plainMessage =iSB_plainMessage.reverse();
    StringBuilder SB_plainMessage = new StringBuilder(iSB_plainMessage.toString());
    SB_plainMessage.insert(0,""&lt;?xml version=\""1.0\"" encoding=\""UTF-8\""?&gt;\r"");

    Source lSB_MessageInput = new StreamSource(SB_plainMessage.toString());
    transformer.setOutputProperty(OutputKeys.METHOD, ""text"");
    transformer.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");




    transformer.transform(lSB_MessageInput, lS_outputMsgBody);

    return new String []{ lS_outputMsgBody.getWriter().toString()};
</code></pre>

<p>`</p>

<p>I'm trying to transform xml message i get to a text file.
I keep get the following exception</p>

<blockquote>
  <p>java.io.FileNotFoundException: C:\alltra_apps\BIC\base_domain\allNETTImport\
  (The filename, directory name, or volume label syntax is incorrect)
      at java.io.FileInputStream.open(Native Method)
      at java.io.FileInputStream.(FileInputStream.java:120)
      at java.io.FileInputStream.(FileInputStream.java:79)
      at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:70)
      at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:161)
      at org.apache.xerces.impl.XMLEntityManager.setupCurrentEntity(Unknown Source)
      at org.apache.xerces.impl.XMLVersionDetector.determineDocVersion(Unknown Source)
      at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
      at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
      at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
      at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
      at com.sun.org.apache.xalan.internal.xsltc.dom.XSLTCDTMManager.getDTM(XSLTCDTMManager.java:440)
      at com.sun.org.apache.xalan.internal.xsltc.dom.XSLTCDTMManager.getDTM(XSLTCDTMManager.java:234)
      at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.getDOM(TransformerImpl.java:524)
      at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:709)
      at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:313)</p>
</blockquote>
","transformer-model"
"33335820","symfony : how to render edit a class json_array attribute with a list of checkboxes?","2015-10-25 22:27:55","33341971","1","1077","<php><forms><symfony><checkbox><transformer-model>","<p>In my symfony2 application, I have an attribute of type json_array :</p>

<pre><code>/**
 * @ORM\Column(name=""rights"", type=""json_array"", nullable=true)
 */
protected $rights = array();
</code></pre>

<p>The data for this attributeis an associative array as follows :</p>

<pre><code>    $allRights = array(
        Associate::READ_PROFILE =&gt; array('all' =&gt; false),
        Associate::UPDATE_PROFILE =&gt; array('all' =&gt; false),
        Associate::READ_CONTACT =&gt; array('all', 'created' =&gt; false),
);
</code></pre>

<p>I want to be able to edit this attribute with a collection of checkboxes collections ie. I want one line per key of the first level and then one checkbox per key of the second level.</p>

<p>I have starter a form type which calls a custom type :</p>

<pre><code>&lt;?php

namespace AppBundle\Form\User;

use AppBundle\Entity\User\Associate;
use AppBundle\Form\DataTransformer\RightsToArrayTransformer;
use Symfony\Component\Form\AbstractType;
use Symfony\Component\Form\FormBuilderInterface;
use Symfony\Component\Form\FormEvent;
use Symfony\Component\Form\FormEvents;
use Symfony\Component\OptionsResolver\OptionsResolverInterface;
use Symfony\Component\Validator\Constraints as Assert;

class AssociateRightsType extends AbstractType
{
    /**
     * @param FormBuilderInterface $builder
     * @param array $options
     */
    public function buildForm(FormBuilderInterface $builder, array $options)
    {
        $builder
            -&gt;add('rights', 'fmu_rights', array(
                'label' =&gt; false,
            ));
    }

    /**
     * @param OptionsResolverInterface $resolver
     */
    public function setDefaultOptions(OptionsResolverInterface $resolver)
    {
        $resolver-&gt;setDefaults(array(
            'data_class' =&gt; 'AppBundle\Entity\User\Associate',
            'validation_groups' =&gt; array('Default', 'rights'),
            'cascade_validation' =&gt; true,
        ));
    }

    /**
     * @return string
     */
    public function getName()
    {
        return 'appbundle_user_associate_rights';
    }
}
</code></pre>

<p>In this custom type I have added a dataTransformer :</p>

<pre><code>&lt;?php

namespace AppBundle\Form\Type;

use AppBundle\Application\User\AssociateManager;
use AppBundle\Entity\User\Associate;
use AppBundle\Form\DataTransformer\RightsToArrayTransformer;
use Symfony\Component\Form\AbstractType;
use Symfony\Component\Form\FormBuilderInterface;
use Symfony\Component\Form\FormInterface;
use Symfony\Component\Form\FormView;
use Symfony\Component\Validator\Constraints as Assert;
use Symfony\Component\OptionsResolver\OptionsResolverInterface;

class RightsType extends AbstractType
{
    /**
     * @var AssociateManager
     */
    private $associateManager;

    public function __construct(AssociateManager $associateManager)
    {
        $this-&gt;associateManager = $associateManager;
    }

    /**
     * @param FormBuilderInterface $builder
     * @param array $options
     */
    public function buildForm(FormBuilderInterface $builder, array $options)
    {
        $transformer = new RightsToArrayTransformer();
        $builder-&gt;addModelTransformer($transformer);


    }

    /**
     * @return string
     */
    public function getName()
    {
        return 'fmu_rights';
    }

}
</code></pre>

<p>In this data transformer I make sure the data is ouputed in the right format :</p>

<pre><code>&lt;?php

namespace AppBundle\Form\DataTransformer;

use AppBundle\Entity\User\Associate;
use Symfony\Component\Form\DataTransformerInterface;

class RightsToArrayTransformer implements DataTransformerInterface
{
    /**
     * @param mixed $data
     * @return array|mixed
     */
    public function transform($data)
    {
        return is_array($data) ? $data + $this-&gt;getRights() : $this-&gt;getRights();
    }

    /**
     * @param mixed $data
     * @return array
     */
    public function reverseTransform($data)
    {
        return $data;
    }

    private function getRights()
    {
        $allRights = array(
            Associate::READ_PROFILE =&gt; array('all'),
            Associate::UPDATE_PROFILE =&gt; array('all'),
            Associate::READ_CONTACT =&gt; array('all', 'created'),
            Associate::UPDATE_CONTACT =&gt; array('all', 'created'),
            Associate::DELETE_CONTACT =&gt; array('all', 'created'),
            Associate::IS_BOSS =&gt; array('all'),
        );

        foreach ($allRights as $right =&gt; $parameters) {
            $allRights[$right] = array();
            foreach ($parameters as $parameter) {
                $allRights[$right][$parameter] = false;
            }

        }

        return $allRights;
    }
}
</code></pre>

<p>and I have a custom view that makes the right visual ouput :</p>

<pre><code>{% block fmu_rights_widget %}

    {% if attr.class is defined %}
        {% set attr = attr|merge({'class': attr.class ~ ' input-sm form-control fmu_rights'}) %}
    {% endif %}

    {% set rights = form.vars.data %}

    {% for right, parameters in rights %}
        &lt;div class=""row padding-v""&gt;
            &lt;div class=""col-md-2""&gt;
                {{ right }}
            &lt;/div&gt;
            &lt;div class=""col-md-10""&gt;
                {% for parameter, value in parameters %}
                    {% set name = id ~ '[' ~ loop.index ~ ']' ~ right ~ '[' ~ parameter ~ ']' %}
                        &lt;label for=""{{ name }}""&gt;{{ parameter }}&lt;/label&gt;
                        &lt;input type=""checkbox"" id=""{{ name }}"" name=""{{ name }}"" {% if value %}checked{% endif %}&gt;
                {% endfor %}
            &lt;/div&gt;
        &lt;/div&gt;
    {% endfor %}

{% endblock %}
</code></pre>

<p>However, the data returned when I send my form is the original data output, not the modified data.
I guess I've done it wrong in the view. How can I edit this data correctly and be able to manipulate it the reverse transform function of my data transformer ?</p>
","transformer-model"
"33200134","Conditional Transformer Routing based on enviroment property in spring integration xml","2015-10-18 16:10:31","","0","989","<java><xml><spring-integration><transformer-model>","<p>I am using spring-integration , i have a requirement that based on system environment variable i want to switch message to either of the transformer.</p>

<p>I tried with the question answered here :<a href=""https://stackoverflow.com/questions/25393767/routing-to-a-different-channels-based-on-condition"">Condition recipient-list-router</a><br>
but as recipient-list-router solution would be only applicable if i want to direct to different channels but here the problem is that the channel configured for both the transformers is <strong>same</strong> also both transformers have <strong>same input channel</strong> only based on environment property i want to route them to <strong>different output channel</strong>.<br>
Like</p>

<pre><code>&lt;int:transformer id=""messageTransformer"" ref=""messageTransformerBean"" 
    input-channel=""validMessageChannel"" method=""transform"" output-channel=""Channel-A"" 
    /&gt;
</code></pre>

<p>so if environement variable is true then output this transformed message to channel-A otherwise to channel-B</p>

<pre><code>&lt;int:transformer id=""messageTransformer"" ref=""messageTransformerBean""
    input-channel=""validMessageChannel"" method=""transform"" output-channel=""Channel -B"" /&gt;
</code></pre>

<p>Is there any way to achieve this ,please let me know.</p>
","transformer-model"
"32984079","transformer to socket not working","2015-10-07 04:56:20","","0","157","<java><sockets><printwriter><transformer-model>","<p>Its driving me nuts trying days to grind this problem.</p>

<p>I am writing a java server to work with an android application. They communicate text string and xml.</p>

<p>Currently simple socket communication seems fine, however it breaks down when I tried to use transformer instead of simple println.</p>

<p>Server code as below</p>

<pre><code>import java.io.BufferedReader;
import java.io.BufferedWriter;
import java.io.IOException;
import java.io.InputStreamReader;
import java.io.OutputStreamWriter;
import java.io.PrintStream;
import java.io.PrintWriter;
import java.net.ServerSocket;
import java.net.Socket;
import java.nio.charset.Charset;

import javax.xml.transform.Transformer;
import javax.xml.transform.TransformerFactory;
import javax.xml.transform.dom.DOMSource;
import javax.xml.transform.stream.StreamResult;


public class javamultithreadedserver implements Runnable {
Socket csocket; 
static int listenport = 6021;
public static final String endOfCom = ""endofcom"";
String connectionname;

public javamultithreadedserver(Socket csocket, String connectionname) {
    this.csocket = csocket;
    this.connectionname = connectionname;
}

public static void main(String[] args) throws Exception {
    int listentoport = 0;
    //System.out.println(args.length);
    if (args.length == 1){
        listentoport = Integer.parseInt(args[0]);
    } else{
        listentoport = listenport;
    }
    ServerSocket ssock = new ServerSocket(listentoport);
    System.out.println(""Listening at port "" + listentoport);
    while (true) {
         Socket sock = ssock.accept();
         System.out.println(""Connected. from: "" + sock.getInetAddress().getHostAddress() + "" "" + sock.getInetAddress().getHostName() + "" port: "" + sock.getPort() + "" "" + sock.getInetAddress().getCanonicalHostName());
         new Thread(new javamultithreadedserver(sock, sock.getInetAddress().getHostAddress() + "" "" + sock.getInetAddress().getHostName() + "" "" + sock.getInetAddress().getCanonicalHostName())).start();
    }

}

@Override
public void run() {
      try {
          PrintWriter out = new PrintWriter(csocket.getOutputStream(), true);
          BufferedReader in = new BufferedReader(new InputStreamReader(csocket.getInputStream()));
          PrintStream pstream = new PrintStream (csocket.getOutputStream(), true);

          String inputLine, outputLine;

          System.out.println(connectionname + "": listening for query information."");
          while ((inputLine = in.readLine()) != null) {
              System.out.println(connectionname + "": "" + inputLine);
              //out.write(""received something"");
              DBQuery q = new DBQuery(inputLine + DBQuery.regexsplit + ""connectionname"" + DBQuery.regexsplit + connectionname);
              DOMSource dmsource = q.returns();

              ***//StreamResult consoleResult = new StreamResult(System.out); //debug***
              StreamResult consoleResult = new StreamResult(pstream);
              TransformerFactory transformerFactory = TransformerFactory.newInstance();
              Transformer transformer = transformerFactory.newTransformer();
              Thread.sleep(100);
              ***transformer.transform(dmsource, consoleResult);***

              //out.println(""println this is a testprogram""); // client receive ok

              pstream.flush();
              System.out.println(connectionname + "": reply sent."");

              if (inputLine == endOfCom){
                  System.out.println(connectionname + "": is satisfied and terminated communication."");
                  break;
              }
          }           
          pstream.close();
          csocket.close();
       }catch (IOException e) {
           System.out.println(e);
       }catch (Exception e) {
           System.out.println(e);
       }        
}
}
</code></pre>

<p>And the android client code as below:</p>

<pre><code>public void run() {
if (barcode.length() &gt; 0) {
    Socket socket = null;
    try {
        InetAddress serverAddr = InetAddress.getByName(SERVER_IP);
        socket = new Socket(serverAddr, SERVERPORT);
        PrintWriter out = new PrintWriter(new BufferedWriter(new OutputStreamWriter(socket.getOutputStream())), true);

        socket.setSoTimeout(10000);
        BufferedReader in = new BufferedReader(new InputStreamReader(socket.getInputStream()));

        out.write(barcode + ENDL);
        if(out.checkError()){
            showToUI(""PrintWriter had error"");
        } else {
            showToUI(""Query sent"");
        }
        int i = 0;
        while (true) {
            try{
                if (**(serverResponse = in.readLine()) != null**) {
                    if(serverResponse == endOfCom){
                        Log.i(""communicated all data"", ""bye"");
                        out.write(endOfCom + ENDL);
                        break;
                    } else{
                    }
                    Log.i(""server says"", serverResponse);
                }
                //Log.i(""server says"", (serverResponse == null? ""null"" :serverResponse ));
            }catch(SocketTimeoutException e){
                showToUI(barcode + "": Server did not respond in time"");
                if (i++ &gt; 2) {
                    break;
                }
            }
        }

    } catch (final UnknownHostException e1) {
        showToUI(e1.toString());
    } catch (final IOException e1) {
        showToUI(e1.toString());
    } finally{
        try {
            socket.close();
        } catch (IOException e) {
            showToUI(e.toString());
        }
    }
}else{
    runOnUiThread(new Runnable() {
        public void run() {
            Toast.makeText(getApplicationContext(), ""Please enter something in barcode"", Toast.LENGTH_SHORT).show();
        }
    });
}
}   
</code></pre>

<p>I have tested that the xml result returned was ok, if the Stream Result was set to <code>System.out</code> instead of the PrintStream <code>pstream</code>, everything prints perfectly to the console.</p>

<p>However with or without <code>pstream.flush()</code>, client received no response at <code>(serverResponse = in.readLine()) != null</code>.</p>

<p>Also the PrintWriter <code>out</code> which was based on the same Socket <code>socket</code> of <code>pstream</code> successfully gave feedback to the client. When <code>out.println(""println this is a testprogram"");</code> was called, the client received log at the <code>Log.i(""server says"", serverResponse);</code> line OK.</p>

<p>One thing to note is that the reply is usually quite long, can be up to 65536 characters or even more, I do not know if that has any implication, I have tried to create a Buffered reader on the android app larger than the replying xml, it still did not work. The machines are communicating on local network, java server program is running on a different machine from the android emulator.</p>

<p>Any help is appreciated, this had been going on for days and nowhere.</p>
","transformer-model"
"32442732","Symfony2 (2.7) Form Entity Data Transformer","2015-09-07 16:18:59","","0","830","<forms><symfony><entity><transformer-model>","<p>I'm trying to customize a selection list's text while using the entity's ID. This is because I want the list options to be specific to the authenticated user. The database text values are Full Name, By City and State, and Anonymous, but I want it to actually display the user's full name (John Smith), User in Denver, CO, and Anonymous. I'm attempting to use a view data transformer to achieve this, but with no luck. I'd rather not use Javascript to achieve this if possible.</p>

<p>Here's my main form type:</p>

<pre><code>&lt;?php

namespace Members\MessagesBundle\Form;

use Symfony\Component\Form\AbstractType;
use Symfony\Component\Form\FormBuilderInterface;
use Symfony\Component\OptionsResolver\OptionsResolver;
use Symfony\Component\Security\Core\SecurityContext;


class MessageType extends AbstractType
{
    /**
     * @param FormBuilderInterface $builder
     * @param array $options
     */
    public function buildForm(FormBuilderInterface $builder, array $options)
    {
        $builder
            -&gt;add('viewability', 'viewability_entity', array(
                'class' =&gt; 'MessagesBundle:Viewability',
                'property' =&gt; 'name',
                'required' =&gt; true,
            ))
            -&gt;add('body', new MessageBodyType())
        ;
    }

    /**
     * @param OptionsResolver $resolver
     */
    public function configureOptions(OptionsResolver $resolver)
    {
        $resolver-&gt;setDefaults(array(
            'data_class' =&gt; 'Members\MessagesBundle\Entity\Message',
        ));
    }

    /**
     * @return string
     */
    public function getName()
    {
        return 'members_messages_message';
    }
}
</code></pre>

<p>Here's my custom form type for Viewability (the entity which I would like to transform):</p>

<pre><code>&lt;?php

namespace Members\MessagesBundle\Form;

use Symfony\Component\Form\AbstractType;
use Symfony\Component\Form\FormBuilderInterface;
use Symfony\Component\OptionsResolver\OptionsResolver;
use Symfony\Component\Security\Core\SecurityContext;
use Members\MessagesBundle\Form\DataTransformer\MessageNameTransformer;


class ViewabilityType extends AbstractType
{
    private $context;

    /**
     * @param SecurityContext $context
     */
    public function __construct(SecurityContext $context)
    {
        $this-&gt;context = $context;
    }

    /**
     * @param FormBuilderInterface $builder
     * @param array                $options
     */
    public function buildForm(FormBuilderInterface $builder, array $options)
    {
        $transformer = new MessageNameTransformer($this-&gt;context);
        $builder-&gt;addViewTransformer($transformer);
    }

    /**
     * @param OptionsResolver $resolver
     */
    public function configureOptions(OptionsResolver $resolver)
    {
        $resolver-&gt;setDefaults(array(
            'invalid_message' =&gt; 'The selected issue does not exist',
        ));
    }

    /**
     * @return string
     */
    public function getParent()
    {
        return 'entity';
    }

    /**
     * @return string
     */
    public function getName()
    {
        return 'viewability_entity';
    }
}
</code></pre>

<p>Here's my service which defines the Viewability Type:</p>

<pre><code>members.messages.form.type.viewability_entity:
        class: Members\MessagesBundle\Form\ViewabilityType
        tags:
            - { name: form.type, alias: viewability_entity }
        arguments: [@security.context]
</code></pre>

<p>Here's my Viewability Entity:</p>

<pre><code>&lt;?php

namespace Members\MessagesBundle\Entity;

use Doctrine\ORM\Mapping as ORM;

/**
 * @ORM\Entity()
 */
class Viewability
{
    /**
     * @ORM\Column(type=""integer"")
     * @ORM\Id
     * @ORM\GeneratedValue(strategy=""AUTO"")
     */
    protected $id;

    /**
     * @ORM\Column(type=""string"", length=255)
     */
    private $name;

    public function __construct()
    {
    }

    /**
     * @return mixed
     */
    public function getName()
    {
        return $this-&gt;name;
    }

    /**
     * @param mixed $name
     */
    public function setName($name)
    {
        $this-&gt;name = $name;
    }

    /**
     * @return mixed
     */
    public function getId()
    {
        return $this-&gt;id;
    }

    /**
     * @param mixed $id
     */
    public function setId($id)
    {
        $this-&gt;id = $id;
    }
}
</code></pre>

<p>Finally, here's my data transformer:</p>

<pre><code>&lt;?php

namespace Members\MessagesBundle\Form\DataTransformer;

use Symfony\Component\Form\DataTransformerInterface;
use Members\MessagesBundle\Entity\Viewability;
use Symfony\Component\Security\Core\SecurityContext;

class MessageNameTransformer implements DataTransformerInterface
{
    private $user;

    /**
     * @param SecurityContext $context
     */
    public function __construct(SecurityContext $context)
    {
        $this-&gt;user = $context-&gt;getToken()-&gt;getUser();
    }

    /**
     * @param Viewability|null $viewability
     * @return string
     */
    public function transform($viewability)
    {
        if (null === $viewability) {
            return '';
        }

        if($viewability === 'Full Name')
            return sprintf('%s %s', $this-&gt;user-&gt;getInfo()-&gt;getFirstName(), $this-&gt;user-&gt;getInfo()-&gt;getLastName());
        if($viewability === 2)
            return sprintf('Lawyer in %s,  %s', $this-&gt;user-&gt;getInfo()-&gt;getAddress()-&gt;getCity(), $this-&gt;user-&gt;getInfo()-&gt;getAddress()-&gt;getState());
        if($viewability === 3)
            return 'Anonymous';
    }

    /**
     * @param Viewability $viewability
     * @return Viewability
     */
    public function reverseTransform($viewability)
    {
        return $viewability;
    }
}
</code></pre>

<p>The data passed into transform() always seems to be null or """" (empty string).</p>

<p>Thanks for any help.</p>
","transformer-model"
"31685941","How can update my xml file in android?","2015-07-28 19:58:07","31689183","0","186","<android><xml><transformer-model>","<p>Below code run without any error, but my xml file in assets folder does not change. why? </p>

<p>Is there a better way?.</p>

<pre><code>              try {
                    InputStream is = getAssets().open(""file.xml"");



                    DocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance();
                    DocumentBuilder docBuilder = docFactory.newDocumentBuilder();
                    Document doc = docBuilder.parse(is);

                    Node earth = doc.getFirstChild();
                    NamedNodeMap earthAttributes = earth.getAttributes();
                    Attr galaxy = doc.createAttribute(""galaxy"");
                    galaxy.setValue(""milky way"");
                    earthAttributes.setNamedItem(galaxy);

                    Node canada = doc.createElement(""country"");
                    canada.setTextContent(""ca"");
                    earth.appendChild(canada);

                    Transformer transformer = TransformerFactory.newInstance().newTransformer();
                    transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");

                    //Initialize StreamResult with File object to save to file
                    StreamResult result = new StreamResult(new StringWriter());
                    DOMSource source = new DOMSource(doc);
                    transformer.transform(source, result);

                    String xmlString = result.getWriter().toString();
                    dialog.setMessage(xmlString);
                    dialog.show();

                } catch (IOException e) {
                    // TODO Auto-generated catch block
                    e.printStackTrace();
                } catch (ParserConfigurationException e) {
                    // TODO Auto-generated catch block
                    e.printStackTrace();
                } catch (SAXException e) {
                    // TODO Auto-generated catch block
                    e.printStackTrace();
                } catch (TransformerException e) {
                    // TODO Auto-generated catch block
                    Toast.makeText(getApplicationContext(), e.getMessage(), Toast.LENGTH_LONG).show();
                }
</code></pre>

<p>I use this permission in manifest file:</p>

<pre><code>&lt;uses-permission android:name=""android.permission.WRITE_EXTERNAL_STORAGE""/&gt;
</code></pre>
","transformer-model"
"31070444","Using parseincludes in Laravel5 Fractal","2015-06-26 10:07:19","31075244","0","4262","<php><laravel><transformer-model><thephpleague><thephpleague-fractal>","<p>Struggling using parseIncludes in <a href=""https://github.com/thephpleague/fractal"" rel=""nofollow"">https://github.com/thephpleague/fractal</a>.</p>

<p>I have two tables, Property and Weeks. Each property has many weeks. Using Fractal I can return my property item with a collection of weeks. What I want to do is use parseIncludes, so that the return of weeks is optional.</p>

<p>PropertyTransformer.php</p>

<pre><code>&lt;?php
namespace App\Transformer;

use App\Models\Property;
use League\Fractal\TransformerAbstract;


class PropertyTransformer extends TransformerAbstract
{

    protected $availableIncludes = [
        'week'
    ];

    public function transform(Property $property)
    {
        return [
            'id'                =&gt; (int) $property['PropertyID'],
            'PropertyName'      =&gt; $property['PropertyName'],
            'ExactBeds'         =&gt; (int) $property['ExactBeds'],
            'weeks'             =&gt; $property-&gt;week
        ];
    }

    /**
     * Include Week
     *
     * @return League\Fractal\ItemResource
     */
    public function includeWeek( Property $property )
    {
        $week = $property-&gt;week;

        return $this-&gt;item($week, new WeekTransformer);
    }
}
</code></pre>

<p>WeekTransformer.php</p>

<pre><code>&lt;?php
namespace App\Transformer;

use App\Models\Week;
use League\Fractal;

class WeekTransformer extends Fractal\TransformerAbstract
{


    public function transform(Week $week)
    {
        return [
            'Week'          =&gt; $week['week'],
            'Available'     =&gt; $week['available'],
            'Price'         =&gt; (int) $week['price'],

        ];
    }

}
</code></pre>

<p>My PropertyController.php</p>

<pre><code>&lt;?php namespace App\Http\Controllers\Api\v1;

use App\Http\Requests;
use App\Models\Week;
use Illuminate\Support\Facades\Response;

use App\Models\Property;

use League\Fractal;
use League\Fractal\Manager;
use League\Fractal\Resource\Collection as Collection;
use League\Fractal\Resource\Item as Item;

use App\Transformer\PropertyTransformer;


class PropertyController extends \App\Http\Controllers\Controller {

public function show($id)
{
    $property = Property::with('bedroom')-&gt;with('week')-&gt;find($id);

    $fractal = new Fractal\Manager();

    if (isset($_GET['include'])) {
        $fractal-&gt;parseIncludes($_GET['include']);
    }

    $resource = new Fractal\Resource\Item($property, new PropertyTransformer);
    //$resource = new Fractal\Resource\Collection($properies, new PropertyTransformer);

    return $fractal-&gt;createData( $resource )-&gt;parseIncludes('weeks')-&gt;toJson();

}
</code></pre>

<p>I get the following error on the parseIncludes:-</p>

<p>Method 'parseIncludes' not found in class \League\Fractal\Scope</p>

<p>I'm following the guide here on transformers - <a href=""http://fractal.thephpleague.com/transformers/"" rel=""nofollow"">http://fractal.thephpleague.com/transformers/</a></p>

<p>I think I am going wrong somewhere here where it says:-</p>

<p>These includes will be available but can never be requested unless the Manager::parseIncludes() method is called:</p>

<pre><code>&lt;?php
use League\Fractal;

$fractal = new Fractal\Manager();

if (isset($_GET['include'])) {
    $fractal-&gt;parseIncludes($_GET['include']);
}
</code></pre>

<p>If I remove the parseIncludes, I don't get an error, I also get my property data with my collection of weeks, but ?include=week doesn't work to optionally get it.</p>
","transformer-model"
"30607744","dart pub test... can you exclude files / directories?","2015-06-02 22:31:43","","3","870","<testing><dart><transformer-model>","<p>dart/pub v1.10</p>

<p>I have a test/e2e folder that has webdriver.dart tests.  'pub run test' is trying to run the dart files in e2e.</p>

<pre><code>pubspec.yaml
dev_dependencies:
    test: '&gt;=0.12.1 &lt;0.13.0'
</code></pre>

<p>I've been running the webdriver.dart test as </p>

<pre><code>dart test/e2e/some_test.dart
</code></pre>

<p>Was hoping pub/test implemented a transformer... so I just just exclude it in pubspec.yaml. No joy.  Can I build a transformer for test?</p>

<p>Ideas? (besides moving the directory out from under test :-))</p>

<p><strong>update</strong>
looks like my options are:</p>

<ol>
<li>pub run test test/unit (specify the directory)</li>
<li>move the e2e folder out of 'test'</li>
</ol>
","transformer-model"
"30562483","Using Transformer in java for multiple outputs from an XSLT?","2015-05-31 20:44:13","","0","1919","<java><xml><xslt><xslt-2.0><transformer-model>","<p>I'm currently trying to get my code to call in an xml file and an xsl - then perform the transformation and output multiple outcome files depending on the xml content. </p>

<pre><code>import javax.xml.transform.*;
import javax.xml.transform.stream.StreamResult;
import javax.xml.transform.stream.StreamSource;
import java.io.File;
import java.io.IOException;
import java.net.URISyntaxException;

public class TestTransformation {

public static void main(String[] args) throws TransformerException {

System.setProperty(""javax.xml.transform.TransformerFactory"",""net.sf.saxon.TransformerFactoryImpl"");
    TransformerFactory tFactory = TransformerFactory.newInstance();

    Source xslt = new StreamSource(new File(""transformer.xslt""));

    Transformer transformer = tFactory.newTransformer(xslt);

    Source xmlText = new StreamSource(new File(""data.xml""));

    transformer.transform(xmlText, new StreamResult(new File(""output.xml"")));
</code></pre>

<p>But i want the transform to produce multiple output files.. Any ideas would be greatly appreciated!!</p>
","transformer-model"
"29741346","Mule ESB error >> ""There are two transformers that are an exact match for input","2015-04-20 07:10:03","","5","1679","<xml><cxf><mule><jax-ws><transformer-model>","<p>I am a newbie in Mule. </p>

<p>I try to create a simple login flow with SOAP that will return XML Response. My method is send the data with XML and then i save the login data in the transformation code, after save the data, the return data will be generated to xml response. but i have errors with "" There are two transformers that are an exact match for input"". I dont understand why it happen. 
Please Help me to solve this problems. </p>

<p>This is my flow code :</p>

<pre><code>&lt;http:listener-config name=""HTTP_Listener_Configuration"" host=""0.0.0.0"" 

port=""8081"" doc:name=""HTTP Listener Configuration""/&gt;
&lt;cxf:configuration name=""CXF_Configuration"" enableMuleSoapHeaders=""true"" initializeStaticBusInstance=""true"" doc:name=""CXF Configuration""/&gt;
&lt;mulexml:jaxb-context name=""myJaxb"" packageNames=""com.test.service"" doc:name=""JAXB Context""/&gt;

&lt;flow name=""LoginFlow"" initialState=""started""&gt;
&lt;http:listener config-ref=""HTTP_Listener_Configuration"" path=""/Login"" doc:name=""HTTP""/&gt;
&lt;cxf:proxy-service configuration-ref=""CXF_Configuration"" doc:name=""CXF"" namespace=""http://www.test.co.id/SOA/service/1.0"" payload=""body"" port=""LoginPort"" service=""LoginService"" wsdlLocation=""service/login-test.wsdl""/&gt;
&lt;logger message=""#[payload]"" level=""INFO"" doc:name=""Logger""/&gt;
&lt;mulexml:dom-to-xml-transformer returnClass=""java.lang.String"" doc:name=""DOM to XML""/&gt;
&lt;logger message=""#[payload]"" level=""INFO"" doc:name=""Logger""/&gt;
&lt;custom-transformer class=""com.test.transform.XMLTransform"" doc:name=""XML To Java"" returnClass=""com.test.service.LoginResponse""/&gt;
&lt;mulexml:jaxb-object-to-xml-transformer jaxbContext-ref=""myJaxb"" doc:name=""JAXB Object to XML""/&gt;
&lt;mulexml:xml-to-dom-transformer doc:name=""XML to DOM""/&gt;
&lt;/flow&gt;
</code></pre>

<p>Error Stack :</p>

<pre><code>1. There are two transformers that are an exact match for input: ""class org.mule.module.cxf.CxfInboundMessageProcessor$1"", output: ""class [B"". Transformers are: ""XmlToByteArray(class org.mule.module.xml.transformer.XmlToDomDocument)"" and ""_ObjectToByteArray(class org.mule.transformer.simple.ObjectToByteArray)"" (org.mule.api.registry.ResolverException)
org.mule.registry.TypeBasedTransformerResolver:166 (http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/registry/ResolverException.html)
--------------------------------------------------------------------------------
Root Exception stack trace:
org.mule.api.registry.ResolverException: There are two transformers that are an exact match for input: ""class org.mule.module.cxf.CxfInboundMessageProcessor$1"", output: ""class [B"". Transformers are: ""XmlToByteArray(class org.mule.module.xml.transformer.XmlToDomDocument)"" and ""_ObjectToByteArray(class org.mule.transformer.simple.ObjectToByteArray)""
at org.mule.registry.TypeBasedTransformerResolver.getNearestTransformerMatch(TypeBasedTransformerResolver.java:166)
at org.mule.registry.TypeBasedTransformerResolver.resolve(TypeBasedTransformerResolver.java:100)
at org.mule.registry.MuleRegistryHelper.resolveTransformer(MuleRegistryHelper.java:283)
+ 3 more (set debug level logging or '-Dmule.verbose.exceptions=true' for everything)
</code></pre>

<p>Thanks for the answers. :)</p>
","transformer-model"
"29496431","Symfony Form, ViewTransformer with ResizeformListener and Validation","2015-04-07 16:24:35","","1","325","<php><forms><validation><symfony><transformer-model>","<p>I'm using Symfony Form standalone (i.e. not the full stack framework) to build an ""images"" type that allows users to upload 1-x images and give each one a title, change the order of the uploaded images and so on.</p>

<p>I've done this by creating an images type which contains a file and a text field. I've also subclassed the CollectionType to act as a holder (like in the cookbook example). Lastly, I have a dedicated storage class associated with the CollectionType which takes care of persisting the uploaded data (I can't persist the individual images separately since I also need to save the order).</p>

<p>This all works more or less, i.e. I can add/update/delete images normally. The only problem I have is when there is a validation error in some other field in the same form (e.g. if I don't fill out a required field). In this situation, the uploaded file never reaches the storage class, and when the form is rendered again, the view data is in the wrong format. So I've added a ViewTransformer to my custom CollectionType that detects this case and performs the necessary transformation. Which basically works, but unfortunately, it puts the transformed data into the wrong place: The FormView instance corresponding to the CollectionType has the correct (i.e. transformed) data, while the child instance (i.e. the image type) has the untransformed values. So the question is: Can I make it pass the transformed data to the correct child, or shouldn't this happen automatically even? </p>

<p>I can provide some example code if it helps, but it might take a moment to extract only the relevant parts. In the meantime I wanted to ask: Am I even approaching this problem in the right way? The documentation on the inner workings of Symfony Form is a bit wanting, so I'm not sure if there wouldn't be an easier way to do what I am trying to do. </p>

<p>EDIT: After a lot of debugging I found out that my problem is that Transformers are called differently during <code>submit()</code> and <code>setData()</code>: During <code>submit()</code> (i.e. when data is loaded from the storage backend), the view transformer runs on the parent (i.e. <code>CollectionType</code>), and afterwards the converted <code>$viewData</code> is passed to the children. During <code>submit()</code>, it's the other way around: First, the child data is mapped, and afterwards the view transformer is run. I think I'll open a ticket on gh...</p>
","transformer-model"
"29454187","Java XML Pretty Print with Commented blocks","2015-04-05 04:45:34","34313750","0","278","<pretty-print><transformer-model>","<p>I have below code to pretty print a given XML.</p>

<pre><code>public void prettyPrintXML(String xmlString) {
        try {
            Source xmlInput = new StreamSource(new StringReader(xmlString));
            StringWriter stringWriter = new StringWriter();
            StreamResult xmlOutput = new StreamResult(stringWriter);
            TransformerFactory transformerFactory = TransformerFactory.newInstance();
            Transformer transformer = transformerFactory.newTransformer();
            transformer.setOutputProperty(OutputKeys.METHOD, ""xml"");
            transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
            transformer.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");
            transformer.setOutputProperty(""{http://xml.apache.org/xslt}indent-amount"", ""4"");
            transformer.transform(xmlInput, xmlOutput);
            System.out.println(""OutPutXML : "");
            System.out.println(xmlOutput.getWriter().toString());
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
</code></pre>

<p>Here is an input and output of the above code:</p>

<pre><code>InputXML :
&lt;employees&gt;&lt;employee&gt;&lt;name&gt;John&lt;/name&gt;&lt;age&gt;18&lt;/age&gt;&lt;/employee&gt;&lt;!--employee&gt;&lt;name&gt;Smith&lt;/name&gt;&lt;age&gt;27&lt;/age&gt;&lt;/employee--&gt;&lt;/employees&gt;

OutPutXML : 
&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;employees&gt;
    &lt;employee&gt;
        &lt;name&gt;John&lt;/name&gt;
        &lt;age&gt;18&lt;/age&gt;
    &lt;/employee&gt;
    &lt;!--employee&gt;&lt;name&gt;Smith&lt;/name&gt;&lt;age&gt;27&lt;/age&gt;&lt;/employee--&gt;
&lt;/employees&gt;
</code></pre>

<p>I need to get the commented block in above output in below format</p>

<pre><code>&lt;!--employee&gt;
   &lt;name&gt;Smith&lt;/name&gt;
   &lt;age&gt;27&lt;/age&gt;
&lt;/employee--&gt;
</code></pre>

<p>Is there a way to do this in Java without using any external libraries?</p>
","transformer-model"
"29308614","What pattern should I use with transformers?","2015-03-27 19:21:01","","0","1573","<java><generics><inheritance><design-patterns><transformer-model>","<p>I have my transformer abstract class:</p>

<pre><code>public abstract class Transformer&lt;T, S&gt; {

    public abstract S transform(T object);

    public abstract T revert(S object);

    public List&lt;S&gt; transformArray(Collection&lt;T&gt; iterable) {
        try {
            List&lt;S&gt; transformed = new ArrayList&lt;S&gt;();
            Iterator&lt;T&gt; iterator = iterable.iterator();
            while (iterator.hasNext()) {
                T element = iterator.next();
                transformed.add(this.transform(element));
            }
            return transformed;
        } catch (Exception e) {
            e.printStackTrace();
            return null;
        }

    }

    public List&lt;T&gt; revertArray(Collection&lt;S&gt; iterable) {
        try {
            List&lt;T&gt; transformed = new ArrayList&lt;&gt;();
            Iterator&lt;S&gt; iterator = iterable.iterator();
            while (iterator.hasNext()) {
                S element = iterator.next();
                transformed.add(this.revert(element));
            }
            return transformed;
        } catch (Exception e) {
            e.printStackTrace();
            return null;
        }
    }

}
</code></pre>

<p>And I want to have my DTOTransformer, which is like</p>

<pre><code>public abstract class DTOTransformer&lt;DTOREQ, OTHER, DTORES&gt; {

    Transformer&lt;DTOREQ, OTHER&gt; forwardTransformer;
    Transformer&lt;OTHER, DTORES&gt; backwardTransformer;

    // Not overriding, since isn't and hasn't a transformer
    public OTHER transform(DTOREQ object) {
         return forwardTransformer.transform(object);
    }

    // Not overriding, since isn't and hasn't a transformer
    public DTORES revert(OTHER object) {
         return backwardTransformer.transform(object);
    }

}
</code></pre>

<p>The problem is that I have not actually any implementation for backwardTransformer and forwardTransformer's transform() method. What I want is that when I create a subclass from DTOTransformer, for an example, ClientDTOTransformer then I have to implement the transform and revert methods (which are not the same of the methods that Transformer's class offers since I have a ""triangular"" transformation). </p>

<p>I have already tried using the adapter pattern but I couldn't find the way to implement it. </p>

<p>Do you know how I could do this? Or if you know a better way to do this, I am open to suggestions.</p>
","transformer-model"
"29192680","Attributes for TransformerFactory","2015-03-22 09:13:23","","2","4915","<java><xml><xslt><jaxp><transformer-model>","<p>where can I find the supported attributes I can set to Java's 7- javax.xml.transform.TransformerFactory?
TransformerFactory.newInstance().<strong>setAttribute</strong>(<strong>name</strong>, value)</p>

<p>I'm referring to Java's own's JAXP (coming out-of-the-box), and NOT external implementation.</p>
","transformer-model"
"29084013","How to set parameters in XSL stylesheet with the help of Transformer object when i have my xml generated dynamically in jsp","2015-03-16 18:10:57","","0","671","<java><xml><jsp><xslt><transformer-model>","<p>I need to set Parameters in my XSL stylesheet.I am retrieving all values in a list and using it to create XML dynamically in jsp. So there is no external  xml file created in this process..But now I need to set variable in XSLt stylesheet.  But as there is no xml file created so I am finding it difficult to use transform Object.Here  is MY code</p>

<pre><code>    &lt;%@ page contentType=""text/xml"" %&gt;
    &lt;%@page import=""java.util.ArrayList,aj.model.BriefBoardDetail,
     javax.xml.transform.Transformer,javax.xml.transform.TransformerFactory,
       javax.xml.transform.stream.StreamSource"" %&gt;
     &lt;% ArrayList&lt;BriefBoardDetail&gt; list =(ArrayList)
       request.getAttribute(""boardList""); 
       int countofPages =(Integer) request.getAttribute(""countOfPages"");
      int pageNumber = (Integer) request.getAttribute(""currentPageNumber"");
     Transformer transformer =  
               TransformerFactory.newInstance().newTransformer(new 
  StreamSource(""/xslt/indexPageStyle.xsl""));
     transformer.setParameter(""currentPage"" , pageNumber);
     transformer.setParameter(""lastPage"" , countofPages);
     transformer.transform(???, ???);//what parameter i need to set here?
   %&gt; 
   &lt;?xml-stylesheet type=""text/xsl"" href=""xslt/indexPageStyle.xsl""    
    version=""1.0"" encoding =""UTF-8"" ?&gt; 
   &lt;AlgorithmHome&gt;
    &lt;%
    for(BriefBoardDetail bbs : list) {
     %&gt;
    &lt;subject id = '&lt;%=bbs.getBoardId() %&gt;' &gt;
      &lt;name&gt; &lt;%= 
       bbs.getBoardName() %&gt;&lt;/name&gt;
       &lt;description&gt; &lt;%= 
       bbs.getDescription() %&gt;&lt;/description&gt;
     &lt;/subject&gt;
 &lt;%} %&gt;
&lt;/AlgorithmHome&gt;
</code></pre>

<p>So I need to set countOfPages and pageNumber in indexPageStyle.xsl.But as xml is generated dynamically in this jsp so what I need to set as <strong>XMLSource</strong> and <strong>outputTarget</strong> in transformer.transform(XMLSource, outputTarget) if i need to display it in browser.</p>
","transformer-model"
"29072751","Creating a XML file using Transformer object having "":"" in the file name","2015-03-16 08:51:53","","0","74","<xml><transformer-model>","<p>How can I create a XML file in linux server which have the "":"" in the file name, like:</p>

<pre><code>outputXMLFile = new File(""/data/file/Test_2015-03-16T09:34:171"");
outputXMLFile.createNewFile();
StreamResult result1 = new StreamResult(outputXMLFile);
transformer.transform(source1, result1);
</code></pre>

<p>I am getting the error : no such file or directory.</p>

<p>Any other workaround, I have a Document object which is possibly very big in size.</p>
","transformer-model"
"28976263","Pretty print and write XML with UTF-8 encoding in Java 8","2015-03-11 00:00:10","28976328","-1","2624","<java><xml><dom><pretty-print><transformer-model>","<p>I have a XML document in a String and would like to prettyPrint (newlines after tags and indentation) it to a file, so far the solution at <a href=""https://stackoverflow.com/a/25865324/3229995"">https://stackoverflow.com/a/25865324/3229995</a> is working for me almost as I want.</p>

<p>The problem is, it is using System.out, and I would like to write the output to a file, ensuring that the UTF-8 encoding is kept.</p>

<p>Below is how I modified the code. It runs and outputs an XML file on a test XML string as shown in <a href=""https://stackoverflow.com/a/25865324/3229995"">https://stackoverflow.com/a/25865324/3229995</a>. </p>

<p>My questions are:</p>

<p>Do I need to flush or close the writer or out? 
If so, at what part of the code?</p>

<p>I am worried that without flushing or closing, there could be cases where not the whole XML will be output. </p>

<pre><code>    public static void main(String[] args){
        String xmlString = ""&lt;hello&gt;&lt;from&gt;ME&lt;/from&gt;&lt;/hello&gt;"";
        DocumentBuilderFactory documentBuilderFactory = DocumentBuilderFactory.newInstance();
        DocumentBuilder documentBuilder = documentBuilderFactory.newDocumentBuilder();
        Document document = documentBuilder.parse(new InputSource(new StringReader(xmlString)));
        // NEW: using FileOutputStream and Writer
        OutputStream out = new FileOutputStream(""/path/to/output.xml"");
        Writer writer = new OutputStreamWriter(out, ""UTF-8"");
        pretty(document, writer, 2);
    }

    private static void pretty(Document document, Writer writer, int indent) throws Exception {
    document.setXmlStandalone(true);
    TransformerFactory transformerFactory = TransformerFactory.newInstance();
    Transformer transformer = transformerFactory.newTransformer();
    transformer.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");
    if (indent &gt; 0) {
        transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
        transformer.setOutputProperty(""{http://xml.apache.org/xslt}indent-amount"", Integer.toString(indent));
    }
    // NEW: passing the writer
    Result result = new StreamResult(writer);
    Source source = new DOMSource(document);
    transformer.transform(source, result);
}
</code></pre>
","transformer-model"
"28808745","Class not Found exception for saxon parser implementation","2015-03-02 11:31:10","","1","7959","<java><xslt-2.0><saxon><transformer-model>","<p>I am using ""Saxon-HE 9.6.0-4"" to convert xml data to HTML.
I am getting the following error. 
java.lang.ClassNotFoundException: net.sf.saxon.TransformerFactoryImpl</p>

<p>[javax.xml.transform.Source xmlSource = new javax.xml.transform.stream.StreamSource(results.getDirectory()
                + ""\results.xml"");
        javax.xml.transform.Source xsltSource = new javax.xml.transform.stream.StreamSource(xsltFile);
        StringWriter sw = new StringWriter();</p>

<pre><code>    javax.xml.transform.Result result = new javax.xml.transform.stream.StreamResult(sw);
    System.setProperty(""javax.xml.transform.TransformerFactory"", ""net.sf.saxon.TransformerFactoryImpl"");
    TransformerFactory transFact = TransformerFactory.newInstance(""net.sf.saxon.TransformerFactoryImpl"", null);
    javax.xml.transform.Transformer trans = transFact.newTransformer(xmlSource);

    trans.transform(xsltSource, result);]
</code></pre>

<p>Can anybody please help me out with this issue</p>
","transformer-model"
"28796319","Update an entity that has a datatransformer on a collection field on Symfony","2015-03-01 16:57:15","","1","490","<symfony><collections><entity><transformer-model>","<p>My project is on Symfony 2.6.</p>

<p>I have an Entity <code>Request</code> with a collection of <code>User</code> <code>contributers</code>. </p>

<p>I have a form in order to add a <code>contributer</code> to a <code>request</code>. This form contains a data transformer in order to add the user by its <code>email</code>.</p>

<p>Everything is working just fine When I want to :</p>

<ul>
<li><p>CREATE a new <code>request</code> with an <code>email</code> corresponding to :</p>

<ul>
<li>An existing <code>user</code>.</li>
<li>A NOT existing <code>user</code> : an error message appears to inform user that this <code>email</code> doesn't match any existing <code>user</code>.</li>
</ul></li>
<li><p>UPDATE an existing <code>request</code> by </p>

<ul>
<li>UPDATING an already existing <code>contributer</code> with an <code>email</code> corresponding to
<ul>
<li>An existing <code>user</code>.</li>
<li>A NOT existing <code>user</code> : an error message appears to inform user that this <code>email</code> doesn't match any existing <code>user</code>.</li>
</ul></li>
<li>ADDING a new <code>contributer</code> with an <code>email</code> corresponding to 
<ul>
<li>An existing <code>user</code>.</li>
</ul></li>
</ul></li>
</ul>

<p>BUT when I want to update an existing <code>request</code> by adding a new <code>contributer</code> with an <code>email</code> corresponding to a NOT existing <code>user</code>then it crashes :  <code>Catchable Fatal Error: Argument 1 passed to AppBundle\Entity\Request::addContributer() must be an instance of AppBundle\Entity\User, null given, called in C:\Users\Utilisateur\Workspace\repositories\tvjp\web-app-ws\vendor\symfony\symfony\src\Symfony\Component\PropertyAccess\PropertyAccessor.php on line 512 and defined</code></p>

<p>Here is a workaround to handle this problem : In my <code>Request</code> entity, I modify my <code>addContributer</code> method in order to accep null <code>User</code> :</p>

<p>This code doesn't work :</p>

<pre><code>public function addContributer(User $contributer){
    $this-&gt;contributers[] = $contributer;
    return $this;
}
</code></pre>

<p>This code works : </p>

<pre><code>public function addContributer(User $contributer = null){
    if($contributer != null){
        $this-&gt;contributers[] = $contributer;
    }
    return $this;
}
</code></pre>

<p>But I think this a really dirty workaround. Any ideas of what am I missing here ?</p>

<p>For information, here are parts of my code.</p>

<p>the datatransformer</p>

<pre><code>class UserByEmailTransformer implements DataTransformerInterface{

private $entityManager;

public function __construct(EntityManager $entityManager){
    $this-&gt;entityManager = $entityManager;
}

public function reverseTransform($email) {
    if (!$email) { return new User(); }
    $user = $this-&gt;entityManager-&gt;getRepository('AppBundle:user')-&gt;findOneBy(array('email' =&gt; $email));
    if ($user === null) {
        throw new TransformationFailedException('Il n\'existe aucun utilisateur '.$email.'.');
    }
    return $user;
}

public function transform($user) {
    if ($user === null) { return ''; }
    return $user-&gt;getEmail();

}

}
</code></pre>

<p>The user selector</p>

<pre><code>class UserSelectorType extends AbstractType{

private $entityManager;

public function __construct(EntityManager $entityManager){
    $this-&gt;entityManager = $entityManager;
}

public function buildForm(FormBuilderInterface $builder, array $options){
    $transformer = new UserByEmailTransformer($this-&gt;entityManager);
    $builder-&gt;addModelTransformer($transformer);

}

public function setDefaultOptions(OptionsResolverInterface $resolver){
    $defaults = array();
    $resolver-&gt;setDefaults($defaults);
}

public function getParent(){
    return 'email';
}

public function getName(){
    return 'userSelector';
}
}
</code></pre>

<p>The service </p>

<pre><code>user_selector_form_type:
    class: AppBundle\Form\Selector\UserSelectorType
    arguments: [""@doctrine.orm.entity_manager""]
    tags:
        - { name: form.type, alias: userSelector }
</code></pre>

<p>The form</p>

<pre><code>class RequestType extends AbstractType{

public function buildForm(FormBuilderInterface $builder, array $options){
    $isAddForm = $options['isAddForm'];
    $builder-&gt;add('contributers','collection',array(
            'label'                 =&gt; 'request.form.contributers', 
            'type'                  =&gt; 'userSelector',
            'allow_add'             =&gt; true,
            'allow_delete'          =&gt; true,
            'by_reference'          =&gt; false,
            'required'              =&gt; false,
            'options'               =&gt; array('label' =&gt; false,'invalid_message' =&gt; 'request.contributer.404',)));

    if ($isAddForm){
        $builder-&gt;add('save', 'submit', array('label' =&gt; 'request.form.add.submit'));
    }else{
        $builder-&gt;add('save', 'submit', array('label' =&gt; 'request.form.edit.submit'));
    }
}

public function setDefaultOptions(OptionsResolverInterface $resolver){
    $defaults = array(
        'data_class'            =&gt; 'AppBundle\Entity\Request',
        'intention'             =&gt; 'request',
        'translation_domain'    =&gt; 'request',
        'isAddForm'             =&gt; ''
    );
    $resolver-&gt;setDefaults($defaults);
}

public function getName(){
    return 'request';
}
}
</code></pre>
","transformer-model"
"28683628","Difference between Cognos Powerplay Client, Powerplay server and Cognos Transformer","2015-02-23 21:17:32","28699932","0","1698","<cognos><transformer-model>","<p>Can someone please explain the difference between Cognos Powerplay client, Powerplay server and Cognos transformer.</p>

<p>I am trying to install Cognos 10.2 on a server. I have the installation software for the below three, but not sure if these need to be installed and if so in which order.</p>

<p>1.Cognos Powerplay Client
2.Cognos Powerplay server and
3.Cognos Transformer.</p>

<p>I do not have much experience with Cubes. Do I need to install all the above 3 to build OLAP cubes? or Just Cognos transformer is sufficient?</p>

<p>Appreciate your response.</p>

<p>Thanks.</p>
","transformer-model"
"28570770","Dart transformer for packages","2015-02-17 20:44:12","","1","67","<dart><transformer-model>","<p>I am trying to make a dart transformer that also runs on packages, but I cant figure out how I currently have the following</p>



<pre class=""lang-dart prettyprint-override""><code>class MyTransformer extends Transformer implements LazyTransformer {
  MyTransformer.asPlugin();

  String get allowedExtensions =&gt; "".dart"";

  void declareOutputs(DeclaringTransform transform) {
    // Just transforms a Dart file in place.
    transform.declareOutput(transform.primaryId);
  }

  Future apply(Transform transform) {
    //Only prints files that are in project how to include packages?
    print(transform.primaryInput.id.path);
    return //do work here
  }
}
</code></pre>

<p>Thanks in advance I have know idea how to make it work</p>
","transformer-model"
"27801219","How to unit test a Symfony2 form when it uses a transformer linked to a database","2015-01-06 14:55:44","27801491","2","848","<forms><unit-testing><symfony><phpunit><transformer-model>","<p>TLDR: I am new to unit tests and I have few questions:</p>

<ol>
<li>Are my transformer tests well written?</li>
<li>Is there a way to decoupled my transformer tests from the database?</li>
<li>How to test my form with the transformer using the database?</li>
<li>Should I decouple my form from my transformer?</li>
</ol>

<hr>

<p>I don't know if my classes are too coupled, if my design is flawed or if my understanding of the unit tests is bad.</p>

<p>Here is some background.<br>
I have a form object with different widgets. One of them is used within a model transformer.<br>
This model transformer uses a connection to the database to retrieve the proper object.</p>

<p>Here is my code:</p>

<pre><code>class BookToStringTransformer implements DataTransformerInterface {

    private $om;

    public function __construct(ObjectManager $om) {
        $this-&gt;om = $om;
    }

    public function transform($book) {
        if (!$book instanceof Book) {
            return """";
        }

        return $book-&gt;getName();
    }

    public function reverseTransform($string) {
        if (!is_string($string) || !$string) {
            return null;
        }

        $book = $this-&gt;om
                -&gt;getRepository('MyBundle:Book')
                -&gt;findOneBy(array('name' =&gt; $string))
        ;

        if (null === $book) {
            throw new TransformationFailedException(sprintf(
                    'The book ""%s"" does not exist!', $string
            ));
        }

        return $book;
    }

}


class ItemType extends AbstractType {

    private $om;

    public function __construct(ObjectManager $om) {
        $this-&gt;om = $om;
    }

    public function buildForm(FormBuilderInterface $builder, array $options) {
        $bookTransformer = new BookToStringTransformer($this-&gt;om);
        $builder-&gt;add($builder-&gt;create('book', 'text', array(
                    'required' =&gt; false,
                ))-&gt;addModelTransformer($bookTransformer));
    }

    public function setDefaultOptions(OptionsResolverInterface $resolver) {
        $resolver-&gt;setDefaults(array(
            'data_class' =&gt; 'MyBundle\Entity\Item',
        ));

    }

    public function getName() {
        return 'mybundle_item';
    }

}
</code></pre>

<p>I wrote unit tests for the transformer using the KernelTestCase</p>

<pre><code>class BookToStringTransformerTest extends KernelTestCase {

    private $name = 'existing name';
    private $em;

    public function setUp() {
        static::$kernel = static::createKernel();
        static::$kernel-&gt;boot();
        $this-&gt;em = static::$kernel-&gt;getContainer()
                -&gt;get('doctrine')
                -&gt;getManager();
    }

    public function testReverseTransform_whenNameExists_returnsBookObject() {
        $transformer = new BookToStringTransformer($this-&gt;em);
        $book = $transformer-&gt;reverseTransform($this-&gt;name);
        $this-&gt;assertInstanceOf('MyBundle\Entity\Book', $book, 'Should return a Book object');
        $this-&gt;assertEquals($this-&gt;name, $book-&gt;getName(), 'Should return a Book object with the selected name');
    }

    /**
     * @expectedException Symfony\Component\Form\Exception\TransformationFailedException
     */
    public function testReverseTransform_whenNameDoesNotExist_throwsException() {
        $transformer = new BookToStringTransformer($this-&gt;em);
        $transformer-&gt;reverseTransform('unknown name');
    }

    /**
     * @param mixed $invalid_parameter
     * @dataProvider provideInvalidParameter
     */
    public function testReverseTransform_whenParameterIsInvalid_returnsNull($invalid_parameter) {
        $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();
        $transformer = new BookToStringTransformer($om);
        $this-&gt;assertNull($transformer-&gt;reverseTransform($invalid_parameter), 'Should return a NULL value');
    }

    /**
     * @return array
     */
    public function provideInvalidParameter() {
        return [
            [null],
            [false],
            [true],
            [''],
            [[]],
            [new \stdClass()],
        ];
    }

    public function testTransform_whenParameterIsBookObject_returnsName() {
        $book = $this-&gt;em-&gt;getRepository('MyBundle:Book')
                -&gt;findOneBy(array('name' =&gt; $this-&gt;name));

        $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();
        $transformer = new BookToStringTransformer($om);
        $this-&gt;assertEquals($this-&gt;name, $transformer-&gt;transform($book), 'Should return a string containing the name');
    }

    /**
     * @param mixed $not_book
     * @dataProvider provideInvalidBookObject
     */
    public function testTransform_whenParameterIsNotBookObject_returnsEmptyString($not_book) {
        $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();
        $transformer = new BookToStringTransformer($om);
        $this-&gt;assertEquals("""", $transformer-&gt;transform($not_book), 'Should return an empty string to be chained');
    }

    /**
     * @return array
     */
    public function provideInvalidBookObject() {
        return [
            [null],
            [123],
            ['123'],
            [[]],
            [true],
            [new \stdClass()],
        ];
    }

}
</code></pre>

<p>As I am new to unit tests, I don't even know if it is the proper way to test that transformer.<br>
I start writing tests for the form object. I am using the TypeTestCase but there is no simple way to get the connection to the database and I can't use the KernelTestCase.</p>

<pre><code>class ItemTypeTest extends TypeTestCase {

    /**
     * @expectedException \PHPUnit_Framework_Error
     */
    public function test_whenCreatedWithNoParameters_raiseException() {
        new ItemType();
    }

    /**
     * @expectedException \PHPUnit_Framework_Error
     */
    public function test_whenCreatedWithBadParameters_raiseException() {
        new ItemType(123);
    }

    public function test_whenCreatedWithGoodParameters_createsFormObject() {
        $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();
        $type = new ItemType($om);
        $form = $this-&gt;factory-&gt;create($type);
        $this-&gt;assertInstanceOf('Symfony\Component\Form\Form', $form);
    }

    public function test_whenSubmittedWithGoodData() {
        $formData = array(
            'name' =&gt; 'existing name',
        );

        $om = $this-&gt;getMockBuilder('Doctrine\Common\Persistence\ObjectManager')-&gt;getMock();
        $type = new ItemType($om);
        $form = $this-&gt;factory-&gt;create($type);

        $form-&gt;submit($formData);
    }

}
</code></pre>

<p>The last test fails because the transformer does get access to the database since I am passing a mock to the form. So should I get a real object (meaning classes are too coupled) or should I find an other way.</p>

<p>Thank you</p>
","transformer-model"
"27765878","How to create a StreamTransformer in Dart?","2015-01-04 13:15:02","27767068","26","17651","<stream><dart><transformer-model>","<p>Trying to build a custom StreamTransformer class, however a lot of the examples out there seem to be out of date, and the one found in the documentation isn't (what some typed languages might consider anyway) as a class (found here: <a href=""https://api.dartlang.org/apidocs/channels/stable/dartdoc-viewer/dart:async.StreamTransformer"" rel=""noreferrer"">https://api.dartlang.org/apidocs/channels/stable/dartdoc-viewer/dart:async.StreamTransformer</a>). This doesn't seem like a very Dart-like way of approaching it and rather more of a Javascript-like way (which I'm using Dart to avoid). </p>

<p>Many online sources say this is how you create a StreamTransformer, however there errors when extending it.</p>

<pre><code>class exampleStreamTransformer extends StreamTransformer
{
  //... (This won't work)
}
</code></pre>

<p>'Implements' seems to be the way to go, along with implementing the bind function needed:</p>

<pre><code>class exampleStreamTransformer implements StreamTransformer
{
  Stream bind(Stream stream)
  {
    //... (Go on to return new stream, etc)
  }
}
</code></pre>

<p>I can't seem to find any examples of this way, but have thrown something together myself (which is accepted in my IDE, but isn't accepted at runtime, I get a null object error when it tries to use pause getter):</p>

<pre><code>class exampleStreamTransformer implements StreamTransformer
{
  StreamController&lt;String&gt; _controller;
  StreamSubscription&lt;String&gt; _subscription;

  Stream bind(Stream stream)
  {
    _controller = new StreamController&lt;String&gt;(
        onListen: ()
        {
          _subscription = stream.listen((data)
          {
            // Transform the data.
            _controller.add(data);
          },
          onError: _controller.addError,
          onDone: _controller.close,
          cancelOnError: true); // Unsure how I'd pass this in?????
        },
        onPause: _subscription.pause,
        onResume: _subscription.resume,
        onCancel: _subscription.cancel,
        sync: true
    );

    return _controller.stream;
  }
}
</code></pre>

<p>Would like to achieve it this way, as in the 'typed' way of producing the class, any help is much appreciated, thank you.</p>
","transformer-model"
"27545853","Using FormBuilder to generate an edit form for a many-to-many join-entity with extra fields","2014-12-18 11:46:41","","0","767","<forms><symfony><collections><many-to-many><transformer-model>","<h2>tl;dr</h2>

<p>I need an edit form for a uni-directional many-to-many relationship (doctrine), where the resulting join table has extra inherited properties. Is there a way to achieve this through the FormBuilder?</p>

<hr>

<h2>Entities</h2>

<p>I have a uni-directional many-to-many relationship (doctrine), where the resulting join table has a relation to a third entity. </p>

<p><img src=""https://i.sstatic.net/03wM1.png"" alt=""ER""></p>

<h2>Form</h2>

<p>The User edit-form should allow (un)crossing Deals easily through a checkbox, while also showing the relations' inherited fields from Discount. The discount's properties should be inherited from Deal by default and overridden on the UserDeal object if modified.</p>

<p><img src=""https://i.sstatic.net/rrhsk.png"" alt=""Form""></p>

<hr>

<h2>My approach</h2>

<p>The way I have approached this, is by creating an auxiliary object UserDealState with two properties: a boolean <code>active</code> and a reference to the UserDeal. UserDealState has its own Type (checkbox and Discount fields).</p>

<p>I also added a <a href=""http://pastebin.com/embed_iframe.php?i=cpbG93nE"" rel=""nofollow noreferrer"">transformer</a> that converts the array of UserDeals to an array of UserDealStates, adding all the non-joined Deals (and setting <code>active</code> to false).</p>

<p>When I check the form inspector in the Symfony Profiler, I can see the transformation has taken place (the <em>view data</em> holds a collection of UserDealStates and is a much longer collection than the <em>model data</em> that holds a collection of UserDeals, which makes sense), but the form object only holds the amount of UserDealStates formViews that the original collection had of UserDeals had, not the much larger number it should.</p>

<p>Just to give a concrete example: say we have 10 Deals, and only one crossed UserDeals. On my form <code>{{ form.userDeals.vars.value|length }}</code> returns 10, <code>{{ form.userDeals.children|length }}</code> returns 1. Only one subform is rendered instead of 10.</p>

<p>Hera are my form-types: <a href=""http://pastebin.com/embed_iframe.php?i=bkEUgHTe"" rel=""nofollow noreferrer"">UserType</a>, <a href=""http://pastebin.com/embed_iframe.php?i=Dbue0ZYP"" rel=""nofollow noreferrer"">UserDealStateType</a>, <a href=""http://pastebin.com/embed_iframe.php?i=26dyHmxS"" rel=""nofollow noreferrer"">UserDealType</a>, <a href=""http://pastebin.com/embed_iframe.php?i=PKnsvVtz"" rel=""nofollow noreferrer"">DiscountType</a>.
<br>Transformer: <a href=""http://pastebin.com/embed_iframe.php?i=cpbG93nE"" rel=""nofollow noreferrer"">UserDealStateToUserDealTransformer</a></p>

<hr>

<h2>The question</h2>

<p>How do you render a form with FormBuilder to edit the mapping between two entities in a many-to-many relationship <em>and</em> the relations' extra fields (as shown on the form mock-up above)?</p>
","transformer-model"
"27355146","How to configure a STOMP Transformer for OpenMQ/Glassfish","2014-12-08 09:39:49","","1","507","<glassfish-3><stomp><transformer-model><openmq>","<p>I have wrote a basic Message Transformer to Transform object messages to Text messages.
I am not Java/OpenMQ/Glassfish Expert</p>

<p>The Transformer compiles fine BUT now I need to configure the STOMP Bridge to use it...
I cant find any examples online on how to do it.</p>

<p>I copied my StompTransformer.class to 
C:\glassfish3\glassfish\domains\domain1\lib\ext
and All the required jars to: 
C:\glassfish3\glassfish\domains\domain1\lib\applibs  (not sure if this is the right place)</p>

<p>I added the following into config.properties:</p>

<pre><code>imq.bridge.admin.user=admin
imq.bridge.stomp.messageTransformer=StompTransformer
imq.bridge.admin.password=admin
imq.bridge.activelist=stomp
imq.bridge.enabled=true
</code></pre>

<p>I tried to read the documentation:
<a href=""https://docs.oracle.com/cd/E19587-01/821-0027/gjdnl/index.html"" rel=""nofollow"">https://docs.oracle.com/cd/E19587-01/821-0027/gjdnl/index.html</a> >>> <em>Configuring a JMS Bridge</em></p>

<p><strong>but it is confusing to me :( I don't know what should be in the XML file, what it should be called, where I should put it and what else is needed for configuration.....</strong></p>

<p><strong>Here is the code for the Transformer:</strong></p>

<pre><code>import java.util.*;
import javax.jms.*;
import com.sun.messaging.bridge.service.MessageTransformer;
import com.thoughtworks.xstream.XStream;

 public class StompTransformer extends MessageTransformer &lt;Message, Message&gt; {

 public Message transform(Message message, 
                          boolean readOnly,
                          String charsetName,
                          String source, 
                          String target,
                          Properties properties)
                          throws Exception {

    Message m = message;
    if (source.equals(SUN_MQ)) { //from Java Message Queue to STOMP client

        if (message instanceof ObjectMessage) {

            //create a new TextMessage for message to be transformed to
            TextMessage tm = (TextMessage)createJMSMessage(JMSMessageType.TEXTMESSAGE);

            //convert message to the TextMessage
            XStream xstream = new XStream();
            tm.setText(xstream.toXML(message));
            m = tm;
        }
    }
    return m;
 }
}
</code></pre>
","transformer-model"
"27004888","Modifying python AST while preserving comments","2014-11-18 21:59:00","","4","2247","<python><abstract-syntax-tree><transformer-model>","<p>I am currently working with the AST in python. I take in a python file, generate its AST, modify it, and then recompile back to source code. I'm using a transformer that adds a getter to a class (I am using a visitor pattern with ast.NodeTransformer). Currently my code works as expected but does not preserve comments, which is my issue. Below is my code:</p>

<pre><code>#visits nodes and generates getters or setters
def genGet(file,type,func):
    global things
    things['func'] = func
    things['type'] = type
    with open(file) as f:
        code = f.read()             #get the code
    tree = ast.parse(code)          #make the AST from the code
    genTransformer().visit(tree)    #lets generate getters or setters depending on type argument given in our transformer so the genTransformer function
    source = meta.asttools.dump_python_source(tree) #recompile the modified ast to source code
    newfile = ""{}{}"".format(file[:-3],""_mod.py"")
    print ""attempting to write source code new file: {}"".format(newfile) #tell everyone we will write our new source code to a file
    outputfile = open(newfile,'w+')
    outputfile.write(source)        #write our new source code to a file
    outputfile.close()


class genTransformer(ast.NodeTransformer):
    ...
</code></pre>

<p>I have done some research on lib2to3 which apparently can preserve comments but have not found anything as of yet that helps with my problem. For example, I found the code below but don't really understand it. It appears to preserve comments but not allow my modifications. I get a missing attribute error when it runs.</p>

<pre><code>import lib2to3
from lib2to3.pgen2 import driver
from lib2to3 import pygram, pytree
import ast

def main():
    filename = ""%s"" % (""exfunctions.py"")
    with open(filename) as f:
        code = f.read()
    drv = driver.Driver(pygram.python_grammar, pytree.convert)
    tree = drv.parse_string(code, True)
    # ast transfomer breaks if it is placed here
    print str(tree)
    return
</code></pre>

<p>I am having trouble finding a package or strategy to preserve comments whilst transforming an AST. Thus far my research has not helped me. What can I use that will allow me to modify an AST but also preserve the comments? </p>
","transformer-model"
"26869462","Cognos Framework manager alternatives on Linux only","2014-11-11 16:20:45","","0","579","<linux><cognos><transformer-model>","<p>I asked a question before about if i needed FM installed for Cognos reports to be made.</p>

<p>The response was that i needed at least one FWM package to be published to run a report or that i could use cognos transformer instead..</p>

<p>After looking into this i dont think Transformer is an option, I only have a linux os and cannot use any Windows applications.. To install Transformer i need to install IBM Cognos Transformer client which is on Windows.</p>

<p>Are there any components i could use which only require Linux installations? Windows really isnt an option here.</p>
","transformer-model"
"26414792","Modifying a payload in Mule","2014-10-16 22:14:24","","0","1873","<json><mule><esb><payload><transformer-model>","<p>I've a JSON response based on a webservice request.</p>

<pre><code>     [
       {
        ""type"": "" --T::00""
       },
    {
       ""address"": ""10049 College Way N"",
       ""longitude"": ""-122.335022"",
       ""latitude"": ""47.701756"",
      ""incident_number"": ""F110104009"",
       ""type"": ""Aid Response"",
       ""report_location"": {
       ""needs_recoding"": false,
       ""longitude"": ""-122.335022"",
       ""latitude"": ""47.701756""
      }
     },
      {
      ""address"": ""5929 Beach Dr Sw"",
      ""longitude"": ""-122.397816"",
      ""latitude"": ""47.550431"",
      ""incident_number"": ""F110104008"",
       ""type"": ""Aid Response"",
       ""report_location"": {
       ""needs_recoding"": false,
       ""longitude"": ""-122.397816"",
       ""latitude"": ""47.550431""
       }
      }
</code></pre>

<p>Is there a way to manipulate the payload to remove this from the header  {
       ""type"": "" --T::00""
       } or just add it in the footer. I currently use a JSON to Object transformer     and  added java.util.List. I've seen posts of using a groovy transformer to add to the message but how about removing elements such as the header?</p>

<p>Thanks!</p>
","transformer-model"
"26157533","v8.5 - Error Compiling Parallel Transformer","2014-10-02 09:01:27","26202592","0","3361","<c++><transformer-model><datastage>","<p>I would appreciate if i can get some help in solving this issue for which i have been looking for information for over 2 days through various forums.  Crying or Very sad  Crying or Very sad </p>

<p>I have installed Infosphere Datastage v8.5 on windows environment for testing and have run a couple of jobs, but when i run a job containing a transformer i get compilation error. </p>

<p>Job: Row generator -> Tfx -> Sequential File </p>

<p>Based on my search through various forums i find that Similar issue has been faced by others as well.But i am unable to find a concrete answer regarding the following: 
1. A Compiler that i need to install and the related SDK that needs to be installed along with it 
2. or A combined pack that has both compiler+SDK </p>

<p>The Installation Guide talks about Microsoft Visual Studio .NET 2008 Express Edition C++: but does not mention which is the associated SDK that needs to be installed ? 
or is there a newer version of C++ compiler+SDK that will work on my environment </p>

<h1>My Environment Details:</h1>

<p>OS  -   Windows Server 2008 R2(Standard) SP1 - 64 bit </p>

<p>IIS -   Infosphere Information Server v8.5 64bit Multilingual </p>

<p>--================</p>

<p>.Net framework</p>

<p>--================ </p>

<p>Microsoft .Net framework 1.1</p>

<p>Microsoft .Net framework 4 Client Profile</p>

<p>Microsoft .Net framework 4 Extended </p>

<p>--=====</p>

<p>SDK</p>

<p>--=====</p>

<p>Microsoft Windows SDK for Windows 7 and .NET Framework 3.5 SP1 </p>

<p>This SDK created 2 directories path , which are as follows:</p>

<p>C:\Program Files\Microsoft SDKs </p>

<p>C:\Program Files (x86)\Microsoft Visual Studio 9.0 </p>

<h1>Windows Environment Variables:</h1>

<p>INCLUDE - C:\IBM\SQLLIB\INCLUDE;C:\IBM\SQLLIB\LIB;C:\Program Files\Microsoft SDKs\Windows\v7.0\Include </p>

<p>LIB - C:\Program Files\Microsoft SDKs\Windows\v7.0\Lib;C:\IBM\SQLLIB\LIB </p>

<p>CLASSPATH   - .;C:\IBM\SQLLIB\java\db2java.zip;C:\IBM\SQLLIB\java\db2jcc.jar;C:\IBM\SQLLIB\java\sqlj.zip; 
C:\IBM\SQLLIB\java\db2jcc_license_cu.jar;C:\IBM\SQLLIB\bin;C:\IBM\SQLLIB\java\common.jar </p>

<p>Path    - C:\IBM\InformationServer\Server\DSComponents\bin;C:\Program Files (x86)\MKS Toolkit\mksnt;C:\PROGRA~2\MKSTOO~1\bin64;C:\PROGRA~2\MKSTOO~1\bin; 
C:\PROGRA~2\MKSTOO~1\bin\X11;C:\PROGRA~2\MKSTOO~1\mksnt;C:\IBM\InformationServer\ASBNode\apps\jre\bin\classic;C:\IBM\InformationServer\ASBNode\lib\cpp; 
C:\IBM\InformationServer\ASBNode\apps\proxy\cpp\vc60\MT_dll\bin;%systemroot%\system32;%systemroot%; 
%systemroot%\system32\wbem;%systemroot%\system32\windowspowershell\v1.0\;c:\program files (x86)\microsoft visual studio 9.0\vc\bin; 
c:\program files (x86)\microsoft visual studio 9.0\common7\ide;c:\program files\ibm\gsk8\lib64;C:\IBM\SQLLIB\BIN;C:\IBM\SQLLIB\FUNCTION; 
C:\IBM\SQLLIB\SAMPLES\REPL;C:\Windows/SysWOW64 </p>

<h1>Compiler information from windows command prompt:</h1>

<p>c:\Users\skh>cl.exe </p>

<p>Microsoft (R) 32-bit C/C++ Optimizing Compiler Version 15.00.30729.01 for 80x86 
Copyright (C) Microsoft Corporation. All rights reserved. </p>

<p>usage: cl [ option... ] filename... [ /link linkoption... ] </p>

<h1>Datastage Environment Variables:</h1>

<p>APT_COMPILEOPT (old):   -W/TP -W/EHa -DAPT_USE_ANSI_IOSTREAMS -c -W/Zc:wchar_t- 
APT_COMPILEOPT (new):   -W/TP -W/EHa -DAPT_USE_ANSI_IOSTREAMS -c </p>

<p>APT_LINKOPT(old):   -s -W/dll -W/base:0x50000000 -W/Zc:wchar_t- </p>

<p>APT_LINKOPT(new):   -s -W/dll -W/base:0x50000000 </p>

<p>The (old) represent the default values that DS Environment variables had. I ran my job which had a transformer but i got the compilation error. 
I changed the Environment variable values to what is stated in (new), still i get the compilation error mentioned below </p>

<hr>

<p>Output from transformer compilation follows: </p>

<h2>I IIS-DSEE-TFCN-00001 12:55:36(000) </h2>

<p>IBM WebSphere DataStage Enterprise Edition 8.5.0.5746 
Copyright (c) 2001, 2005-2008 IBM Corporation. All rights reserved </p>

<pre><code>##I IIS-DSEE-TFCN-00006 12:55:36(001) &lt;main_program&gt; conductor uname: -s=Windows_NT; -r=1; -v=6; -n=IN-MUM-IBMQLT; -m=Pentium 
##I IIS-DSEE-TOSH-00002 12:55:36(002) &lt;main_program&gt; orchgeneral: loaded 
##I IIS-DSEE-TOSH-00002 12:55:36(003) &lt;main_program&gt; orchsort: loaded 
##I IIS-DSEE-TOSH-00002 12:55:36(004) &lt;main_program&gt; orchstats: loaded 
##W IIS-DSEE-TOSH-00049 12:55:36(007) &lt;main_program&gt; Parameter specified but not used in flow: DSPXWorkingDir 
##E IIS-DSEE-TBLD-00076 12:55:39(000) &lt;main_program&gt; Error when checking composite operator: Subprocess command failed with exit status 256. 
##E IIS-DSEE-TFSR-00019 12:55:39(001) &lt;main_program&gt; Could not check all operators because of previous error(s) 
##W IIS-DSEE-TFTM-00012 12:55:39(002) &lt;transform&gt; Error when checking composite operator: The number of reject datasets ""0"" is less than the number of input datasets ""1"". 
##I IIS-DSEE-TBLD-00000 12:55:39(003) &lt;main_program&gt; Error when checking composite operator: Output from subprocess: cl : Command line warning D9035 : option 'GX' has been deprecated and will be removed in a future release 
cl : Command line warning D9036 : use 'EHsc' instead of 'GX' 
cl : Command line warning D9025 : overriding '/GX' with '/EHa' 

##I IIS-DSEE-TBLD-00000 12:55:39(004) &lt;main_program&gt; Error when checking composite operator: Output from subprocess: C:\IBM\InformationServer\Server\PXEngine\include\apt_util/ints.h(83) : fatal error C1083: Cannot open include file: 'strstream': No such file or directory 

##W IIS-DSEE-TFEV-00025 12:55:39(005) &lt;transform&gt; Error when checking composite operator: Converting string to number. 
##W IIS-DSEE-TFEV-00023 12:55:39(006) &lt;transform&gt; Error when checking composite operator: Implicit conversion from source type ""String"" to result type ""DFloat"". 
##W IIS-DSEE-TFEV-00025 12:55:39(007) &lt;transform&gt; Error when checking composite operator: Converting number to string. 
##W IIS-DSEE-TFEV-00023 12:55:39(008) &lt;transform&gt; Error when checking composite operator: Implicit conversion from source type ""DFloat"" to result type ""String"". 
##W IIS-DSEE-TBLD-00000 12:55:39(009) &lt;main_program&gt; Error when checking composite operator: Output from subprocess: ld: script execution error in file 'C:/PROGRA~2/MKSTOO~1/etc/nutccg/ld.ccg' on line 1119 
&gt;&gt;&gt; execv: could not run 'link32': The system cannot find the file specified. 

##I IIS-DSEE-TBLD-00079 12:55:39(010) &lt;transform&gt; Error when checking composite operator: cxx -LC:/IBM/InformationServer/Server/Projects/PRJ_QSPOC/RT_BP2.O/ -LC:/IBM/InformationServer/Server/PXEngine/lib -LC:/IBM/InformationServer/Server/PXEngine/user_lib -s -W/dll -W/base:0x50000000 -lliborchnt -lliborchcorent -lliborchbuildopnt C:/IBM/InformationServer/Server/Projects/PRJ_QSPOC/RT_BP2.O/V0S3_TestJob_Tfx_Transformer.tmp.o -o C:/IBM/InformationServer/Server/Projects/PRJ_QSPOC/RT_BP2.O/V0S3_TestJob_Tfx_Transformer.dll. 
##E IIS-DSEE-TCOS-00029 12:55:39(011) &lt;main_program&gt; Creation of a step finished with status = FAILED. (TestJob_Tfx.Transformer) 

*** Internal Generated Transformer Code follows: 
0001: // 
0002: // Generated file to implement the V0S3_TestJob_Tfx_Transformer transform operator. 
0003: // 
0004: 
0005: // define our input/output link names 
0006: inputname 0 DSLink2; 
0007: outputname 0 DSLink5; 
0008: 
0009: initialize { 
0010: // define our control variables 
0011: int8 RowRejected0; 
0012: int8 NullSetVar0; 
0013: 
0014: } 
0015: 
0016: mainloop { 
0017: 
0018: // declare our intermediate variables for this section 
0019: dfloat InterVar0_0; 
0020: 
0021: // initialise the rejected row variable 
0022: RowRejected0 = 1; 
0023: 
0024: // evaluate columns (no constraints) for link: DSLink5 
0025: InterVar0_0 = DSLink2.Dummy1; 
0026: DSLink5.Dummy1 = (InterVar0_0 + 5); 
0027: writerecord 0; 
0028: RowRejected0 = 0; 
0029: } 
0030: 
0031: finish { 
0032: } 
0033: 
*** End of Internal Generated Transformer Code
</code></pre>
","transformer-model"
"25728885","Java produce version ""1.1"" in XML header with transformer","2014-09-08 16:30:00","25733664","1","1556","<java><xml><version><domdocument><transformer-model>","<p>I want to produce xml files with 1.1 as version in the header, since i get SaxparserExceptions when parsing my xml-files with version 1.0:</p>

<pre><code>""Character reference ""&amp;#3"" is an invalid XML character"". 
</code></pre>

<p>When I manually change the header, i don't get any errors. Changing the version via outputkeys doesn't seem to work. The file has still the wrong header:</p>

<pre><code>""&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;""
</code></pre>

<p>example code:</p>

<pre><code>    try {
        DocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance();
        DocumentBuilder docBuilder = docFactory.newDocumentBuilder();
        Document doc = docBuilder.newDocument();

        TransformerFactory transformerFactory = TransformerFactory
                .newInstance();
        Transformer transformer = transformerFactory
                .newTransformer();

        transformer.setOutputProperty(
                OutputKeys.INDENT, ""yes"");
        transformer.setOutputProperty(OutputKeys.METHOD, ""xml"");
        transformer.setOutputProperty(OutputKeys.VERSION, ""1.1"");
        transformer
                .setOutputProperty(
                        ""{http://xml.apache.org/xslt}indent-amount"",
                        ""2"");


        DOMSource source = new DOMSource(doc);
        StreamResult result = new StreamResult(
                 ""processed_.xml"");
        transformer.transform(source, result);

    } catch (TransformerConfigurationException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    } catch (TransformerException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    } catch (ParserConfigurationException e) {
        // TODO Auto-generated catch block
        e.printStackTrace();
    }
</code></pre>
","transformer-model"
"25315486","Filter payload that passes to the transformer in Webservice Proxy pattern in Mule ESB","2014-08-14 18:48:23","25320435","0","221","<web-services><proxy><mule><transformer-model>","<p>When using a web service proxy pattern in Mule, you have the ability to pass the message through 1 or more transformers.  Is there anyway to avoid passing ?Wsdl gets or other messages filtered on Content-Type for example?  My transformer is manipulating the XML payload prior to passing it off to the web service, but I've found my wsdl calls are also being processed by the transformer and failing.  </p>

<p>I've put a check in my transformer code, but this doesn't seem like the right way to go about solving this.  </p>

<pre><code>if(message.getOriginalPayload().toString().endsWith(""wsdl"")||!(xmlString.startsWith(""&lt;"") &amp;&amp; xmlString.endsWith(""&gt;""))){return message; }
</code></pre>

<p>The Proxy config:</p>

<pre><code>&lt;pattern:web-service-proxy name=""SR-Proxy"" 
    doc:name=""SR-Proxy"" 
    transformer-refs=""enrichPayloadWithSFSession"" 
    wsdlFile=""service/SR_Webservice.wsdl""&gt;
&lt;http:inbound-endpoint exchange-pattern=""request-response"" host=""${hostname}"" port=""${http.port}"" path=""service/SRProxy"" doc:name=""HTTP"" /&gt;
&lt;https:outbound-endpoint exchange-pattern=""request-response"" address=""${sfdc.wsUrl}SR_Webservice"" /&gt;
&lt;/pattern:web-service-proxy&gt;
</code></pre>
","transformer-model"
"25279917","How to make TransformerFactory thread safe?","2014-08-13 06:56:51","","0","3543","<java><multithreading><transformer-model>","<p>I have the following code to transform the xml file to html file.This is accessed by many threads.The transform method just appends the content every time.That is the first threads content is retained in all subsequent thread's html files.</p>

<pre><code>public class CreateHTML
{
    TransformerFactory tFactory;
    Source xslDoc;
    Source xmlDoc;
    OutputStream htmlFile;
    public CreateHTML(String xslDocFileName,String xmlDocFileName,String outputFileName) throws FileNotFoundException
    {
        xslDoc=new StreamSource(xslDocFileName);
        xmlDoc=new StreamSource(xmlDocFileName);
        htmlFile=new FileOutputStream(outputFileName);
    }
    public synchronized void createOutputFile() throws Exception
    {
        try
        {
            tFactory=TransformerFactory.newInstance();
            tFactory.setAttribute(""indent-number"",new Integer(2));
            Transformer transformer = tFactory.newTransformer(xslDoc);
            transformer.setOutputProperty(OutputKeys.INDENT,""yes"");
            transformer.transform(xmlDoc, new StreamResult(htmlFile));
        }
        catch (TransformerException ex)
        {
            Logger.getLogger(CreateHTML.class.getName()).log(Level.SEVERE, null, ex);
            throw ex;
        }
    }
}
</code></pre>
","transformer-model"
"25214433","Java XML Transformer","2014-08-09 01:36:54","25214560","-1","2649","<java><xml><transformer-model>","<p>I have been trying to create an XML document, but keep getting a Null Pointer Exception error, and I have spent several hours trying to figure out why.</p>

<p>The path is correct because it works when reading the file.</p>

<p>Main:</p>

<pre><code>    public static final String PATH = ""res/config.xml"";

public Main() {

}
public static void main(String args[]) throws FileNotFoundException {
    new WriteConfig(PATH);
}
</code></pre>

<p>WriteConfig:</p>

<pre><code>import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.parsers.ParserConfigurationException;
import javax.xml.transform.OutputKeys;
import javax.xml.transform.Transformer;
import javax.xml.transform.TransformerException;
import javax.xml.transform.TransformerFactory;
import javax.xml.transform.dom.DOMSource;
import javax.xml.transform.stream.StreamResult;
import org.w3c.dom.Document;
import org.w3c.dom.Element;

public class WriteConfig {
Document dom;
Element e = null;
String name;
String gender;
String race;
String cclass;

public WriteConfig(String xml) {
    DocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance();       
    try {
        DocumentBuilder docBuilder = docFactory.newDocumentBuilder();
        dom = docBuilder.newDocument();
        Element mainRootElement = dom.createElement(""character"");

        e = dom.createElement(""name"");
        e.appendChild(dom.createTextNode(name));
        mainRootElement.appendChild(e);

        e = dom.createElement(""gender"");
        e.appendChild(dom.createTextNode(gender));
        mainRootElement.appendChild(e);

        e = dom.createElement(""race"");
        e.appendChild(dom.createTextNode(race));
        mainRootElement.appendChild(e);

        dom.appendChild(mainRootElement);

        try {
            Transformer tr = TransformerFactory.newInstance().newTransformer();
            tr.setOutputProperty(OutputKeys.INDENT, ""yes"");
            tr.setOutputProperty(OutputKeys.METHOD, ""xml"");
            tr.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");
            tr.setOutputProperty(OutputKeys.DOCTYPE_SYSTEM, ""res/config.dtd"");
            tr.setOutputProperty(""{http://xml.apache.org/xslt}indent-amount"", ""4"");
            DOMSource dSource = new DOMSource(dom);
            StreamResult sResult = new StreamResult(xml);
            tr.transform(dSource, sResult);
        } catch (TransformerException te) {
            te.printStackTrace();
        } 
    } catch (ParserConfigurationException pce) {
        pce.printStackTrace();
    }
   }
 }
</code></pre>

<p>Error:</p>

<pre><code>    ERROR:  ''
javax.xml.transform.TransformerException: java.lang.NullPointerException
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:752)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:357)
    at bamberger.com.engine.WriteConfig.&lt;init&gt;(WriteConfig.java:64)
    at bamberger.com.engine.Main.main(Main.java:13)
Caused by: java.lang.NullPointerException
    at com.sun.org.apache.xml.internal.serializer.ToStream.characters(ToStream.java:1612)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:244)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:230)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:230)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:136)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:98)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transformIdentity(TransformerImpl.java:699)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:743)
    ... 3 more
---------
java.lang.NullPointerException
    at com.sun.org.apache.xml.internal.serializer.ToStream.characters(ToStream.java:1612)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:244)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:230)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:230)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:136)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:98)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transformIdentity(TransformerImpl.java:699)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:743)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:357)
    at bamberger.com.engine.WriteConfig.&lt;init&gt;(WriteConfig.java:64)
    at bamberger.com.engine.Main.main(Main.java:13)
</code></pre>

<p>WriteConfig.java:64</p>

<pre><code>tr.transform(dSource, sResult);
</code></pre>

<p>Thanks!</p>
","transformer-model"
"24519326","Adding flow variable to Mule map payload","2014-07-01 21:11:38","24519655","2","3407","<mule><transformer-model>","<p>What am I doing wrong here?  I want to set the payload of a Mule message to a map containing multiple values, one of them the contents of a flow variable.</p>

<pre><code>&lt;set-variable variableName=""myVariable"" value=""foo""/&gt;
&lt;set-payload value=""#[['STATUS':'OKAY','NEXT':'Test','TEXT':flowVars['myVariable']]]""/&gt;
&lt;logger level=""INFO"" message=""#[payload]""/&gt;
</code></pre>

<p>The output of this is:</p>

<pre><code>{STATUS=OKAY, NEXT=Test}
</code></pre>

<p>I was expecting:</p>

<pre><code>{STATUS=OKAY, NEXT=Test, TEXT=foo}
</code></pre>

<p>I know I can do this with a series of expression transformers but, if I want to use this kind of notation, why isn't it pulling the value of the flow variable in?</p>

<p><strong>Edit: to remove typo in source code</strong>  This solved the problem as noted by Ryan's answer below.</p>
","transformer-model"
"24494032","Spring-Integration-Transformer","2014-06-30 15:45:38","","0","1737","<spring-integration><transformer-model>","<p>I am publishing a MapMessage on IBM MQ and using SI i am receiving that message (using jms:inbound-gateway). I want to transform this Map into an object so i am using map-to-object-transformer. When i receive a message and transformer tries to transform this map into an object i am getting ClassCastException.</p>

<h2>MapMessage Publisher</h2>

<pre><code>public void sendMapMessage(){
        jmsTemplate102.send(sendQueue,
            new MessageCreator() {
                public Message createMessage(Session session) throws JMSException {

                    return getEventMap(session);
                }

                public Message getEventMap(Session session){
                    MapMessage mapMsg = null;
                    try {
                        mapMsg = session.createMapMessage();    
                        mapMsg.setStringProperty(""batchId"", ""123987"");
                        mapMsg.setStringProperty(""eventType"", ""ABCD"");
                        mapMsg.setStringProperty(""client"", ""CORP"");

                    } catch (JMSException e) {
                        // TODO Auto-generated catch block
                        e.printStackTrace();
                    }
                    return mapMsg;
                }
            });
    }
</code></pre>

<hr>

<h2>JMS Inbound Gateway</h2>

<pre><code>&lt;int-jms:inbound-gateway id=""jmsInGateway"" request-destination=""receiverQueue"" 
                                           request-channel=""events""
                                           concurrent-consumers=""1""
                                           max-concurrent-consumers=""4""
                                           acknowledge=""auto""
                                           /&gt;

&lt;int:channel id=""etlEvents""/&gt;

&lt;int:map-to-object-transformer input-channel=""etlEvents"" output-channel=""etlEvents"" ref=""etlEvent""/&gt;
</code></pre>

<hr>

<p>Exception</p>

<pre><code>JMS Message class: jms_text
  JMSType:         null
  JMSDeliveryMode: 2
  JMSExpiration:   0
  JMSPriority:     4
  JMSMessageID:    ID:414d5120414344534430513720202020537502dd20130f02
  JMSTimestamp:    1404142195240
  JMSCorrelationID:null
  JMSDestination:  null
  JMSReplyTo:      null
  JMSRedelivered:  false
  JMS_IBM_PutDate:20140630
  JMSXAppID:Websphere MQ Client for Java
  JMS_IBM_Format:MQSTR   
  JMS_IBM_PutApplType:28
  JMS_IBM_MsgType:8
  JMSXUserID:mqcls1      
  JMS_IBM_PutTime:15295524
  JMSXDeliveryCount:1
&lt;map&gt;&lt;/map&gt;] to integration Message payload [&lt;map&gt;&lt;/map&gt;]
2014-06-30 10:48:13,356 DEBUG [org.springframework.jms.listener.DefaultMessageListenerContainer#0-1] channel.DirectChannel - preSend on channel 'etlEvents', message: [Payload String content=&lt;map&gt;&lt;/map&gt;][Headers={JMS_IBM_Format=MQSTR   , errorChannel=org.springframework.messaging.core.GenericMessagingTemplate$TemporaryReplyChannel@581bce9a, jms_timestamp=1404142195240, replyChannel=org.springframework.messaging.core.GenericMessagingTemplate$TemporaryReplyChannel@581bce9a, JMSXUserID=mqcls1      , JMS_IBM_PutApplType=28, JMS_IBM_MsgType=8, JMS_IBM_PutDate=20140630, jms_messageId=ID:414d5120414344534430513720202020537502dd20130f02, JMS_IBM_PutTime=15295524, JMSXDeliveryCount=1, jms_redelivered=false, priority=4, JMSXAppID=Websphere MQ Client for Java, id=f8aacfea-e655-e19d-1b7f-fd99389bf095, timestamp=1404139693356}]
2014-06-30 10:48:13,356 DEBUG [org.springframework.jms.listener.DefaultMessageListenerContainer#0-1] transformer.MessageTransformingHandler - org.springframework.integration.transformer.MessageTransformingHandler#0 received message: [Payload String content=&lt;map&gt;&lt;/map&gt;][Headers={JMS_IBM_Format=MQSTR   , errorChannel=org.springframework.messaging.core.GenericMessagingTemplate$TemporaryReplyChannel@581bce9a, jms_timestamp=1404142195240, replyChannel=org.springframework.messaging.core.GenericMessagingTemplate$TemporaryReplyChannel@581bce9a, JMSXUserID=mqcls1      , JMS_IBM_PutApplType=28, JMS_IBM_MsgType=8, JMS_IBM_PutDate=20140630, jms_messageId=ID:414d5120414344534430513720202020537502dd20130f02, JMS_IBM_PutTime=15295524, JMSXDeliveryCount=1, jms_redelivered=false, priority=4, JMSXAppID=Websphere MQ Client for Java, id=f8aacfea-e655-e19d-1b7f-fd99389bf095, timestamp=1404139693356}]
2014-06-30 10:48:13,356 WARN  [org.springframework.jms.listener.DefaultMessageListenerContainer#0-1] jms.ChannelPublishingJmsMessageListener$GatewayDelegate - failure occurred in gateway sendAndReceive
org.springframework.integration.transformer.MessageTransformationException: failed to transform message
    at org.springframework.integration.transformer.AbstractTransformer.transform(AbstractTransformer.java:44)
    at org.springframework.integration.transformer.MessageTransformingHandler.handleRequestMessage(MessageTransformingHandler.java:68)
    at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
    at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
    at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
    at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
    at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
    at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
    at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
    at org.springframework.messaging.core.GenericMessagingTemplate.doSendAndReceive(GenericMessagingTemplate.java:154)
    at org.springframework.messaging.core.GenericMessagingTemplate.doSendAndReceive(GenericMessagingTemplate.java:44)
    at org.springframework.messaging.core.AbstractMessagingTemplate.sendAndReceive(AbstractMessagingTemplate.java:75)
    at org.springframework.integration.gateway.MessagingGatewaySupport.doSendAndReceive(MessagingGatewaySupport.java:250)
    at org.springframework.integration.gateway.MessagingGatewaySupport.sendAndReceiveMessage(MessagingGatewaySupport.java:224)
    at org.springframework.integration.jms.ChannelPublishingJmsMessageListener$GatewayDelegate.sendAndReceiveMessage(ChannelPublishingJmsMessageListener.java:485)
    at org.springframework.integration.jms.ChannelPublishingJmsMessageListener.onMessage(ChannelPublishingJmsMessageListener.java:330)
    at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:537)
    at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:497)
    at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:468)
    at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:325)
    at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:263)
    at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1102)
    at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1094)
    at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:991)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to java.util.Map
    at org.springframework.integration.transformer.MapToObjectTransformer.transformPayload(MapToObjectTransformer.java:41)
    at org.springframework.integration.transformer.AbstractPayloadTransformer.doTransform(AbstractPayloadTransformer.java:33)
    at org.springframework.integration.transformer.AbstractTransformer.transform(AbstractTransformer.java:33)
    ... 25 more
2014-06-30 10:48:13,387 WARN  [org.springframework.jms.listener.DefaultMessageListenerContainer#0-1] listener.DefaultMessageListenerContainer - Execution of JMS message listener failed, and no ErrorHandler has been set.
org.springframework.integration.transformer.MessageTransformationException: failed to transform message
    at org.springframework.integration.transformer.AbstractTransformer.transform(AbstractTransformer.java:44)
    at org.springframework.integration.transformer.MessageTransformingHandler.handleRequestMessage(MessageTransformingHandler.java:68)
    at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
    at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
    at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
    at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
    at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
    at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
    at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
    at org.springframework.messaging.core.GenericMessagingTemplate.doSendAndReceive(GenericMessagingTemplate.java:154)
    at org.springframework.messaging.core.GenericMessagingTemplate.doSendAndReceive(GenericMessagingTemplate.java:44)
    at org.springframework.messaging.core.AbstractMessagingTemplate.sendAndReceive(AbstractMessagingTemplate.java:75)
    at org.springframework.integration.gateway.MessagingGatewaySupport.doSendAndReceive(MessagingGatewaySupport.java:250)
    at org.springframework.integration.gateway.MessagingGatewaySupport.sendAndReceiveMessage(MessagingGatewaySupport.java:224)
    at org.springframework.integration.jms.ChannelPublishingJmsMessageListener$GatewayDelegate.sendAndReceiveMessage(ChannelPublishingJmsMessageListener.java:485)
    at org.springframework.integration.jms.ChannelPublishingJmsMessageListener.onMessage(ChannelPublishingJmsMessageListener.java:330)
    at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:537)
    at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:497)
    at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:468)
    at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:325)
    at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:263)
    at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1102)
    at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1094)
    at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:991)
    at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassCastException: java.lang.String cannot be cast to java.util.Map
    at org.springframework.integration.transformer.MapToObjectTransformer.transformPayload(MapToObjectTransformer.java:41)
    at org.springframework.integration.transformer.AbstractPayloadTransformer.doTransform(AbstractPayloadTransformer.java:33)
    at org.springframework.integration.transformer.AbstractTransformer.transform(AbstractTransformer.java:33)
    ... 25 more
</code></pre>

<hr>

<p>Sorry for the bad formatting.</p>

<p>Can anyone please tell me what wrong i am doing here ?</p>
","transformer-model"
"23744064","transform returning null pointer exception but the parameters arent null?","2014-05-19 17:48:15","","1","3199","<java><nullpointerexception><kml><transformer-model>","<p>I'm generating a KML file and at the point where I use the transformer API, I get the NullPointerException exception. Here's the code I'm refering too:</p>

<pre><code>TransformerFactory transformerFactory = TransformerFactory.newInstance();
            Transformer transformer = transformerFactory.newTransformer();
            DOMSource source = new DOMSource(kml);
            StreamResult result = new StreamResult(new File(""src/mapa/mapa.kml""));
            System.out.print(""DESPRES DE CREAR RESULT\n"");
            if(source == null)
                System.out.print(""SOURCE IS NULL"");
            if(result == null)
                System.out.print(""RESULT IS NULL"");
            transformer.transform(source, result);
            System.out.print(""AFTER TRASNFORM\n"");
</code></pre>

<p>It doesn't print any of the NULL System.outs, however it doesn't print the last one either. Why is it giving me NPE?</p>

<p>Exception:</p>

<pre><code>DESPRES DE CREAR RESULT
ERROR:  ''
javax.xml.transform.TransformerException: java.lang.NullPointerException
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(Unknown Source)
    at Joc.crearMapa(Joc.java:199)
    at MastersOfWar.main(MastersOfWar.java:18)
Caused by: java.lang.NullPointerException
    at com.sun.org.apache.xml.internal.serializer.ToUnknownStream.characters(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transformIdentity(Unknown Source)
    ... 4 more
---------
java.lang.NullPointerException
    at com.sun.org.apache.xml.internal.serializer.ToUnknownStream.characters(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transformIdentity(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(Unknown Source)
    at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(Unknown Source)
    at Joc.crearMapa(Joc.java:199)
    at MastersOfWar.main(MastersOfWar.java:18)
</code></pre>
","transformer-model"
"23129991","Can I access solr dataimporter.request variables in a javascript transformer","2014-04-17 09:56:59","26336444","2","986","<javascript><solr><dataimporthandler><transformer-model><dih>","<p>I have a dataconfig.xml file that collects data from an Oracle database. In the Datasource element, I use variables such as ${dataimporter.request.dbname} that return successfully the custom value I passed via the dataimport url. </p>

<p>I am now writing a javascript transformer in this same dataconfig file to add values in a mutivalued field and that includes the database name. Is it possible to refer to the variable ${dataimporter.request.dbname} from within the javascript transformer? If so, what would be the correct syntax? </p>

<p>This is what I have tried, but dbname does not get populated:</p>

<pre><code>function relatedItems(row) {
    var relatedItemsArray = new java.util.ArrayList();
    var dbname=${dataimporter.request.db_name};
    relatedItemsArray.add('type=DOCUMENT;datasource:PB||' + dbname);
    row.put('relation', relatedItemsArray);
    return row;
  }
</code></pre>

<p>Any help is greatly appreciated!</p>

<p>Thanks in advance.</p>
","transformer-model"
"22828133","Should we use Java Component in Mule, if component is already available? Mule Performance and Optimization","2014-04-03 04:52:38","22867411","0","218","<java><mule><transformer-model>","<p>It is accepted that we should use available Mule Components to achieve particular functionality rather then, using Custom Java Transformers.</p>

<p>The case is that i already have Mule component to perform particular functionality, 
But instead of using that, i introduced Java Transformer to achieve the same functionality.</p>

<p>I need a comparison between performance of two, as i am unable to find some speed issue.</p>

<p>but still i want to know:</p>

<p>Will it deteriorates performance? Will it make application slow in future?</p>

<p>I am unable to get answer for the same in forums. 
If anyone could comment on same.</p>
","transformer-model"
"22539158","Converting a org.w3c.dom.Document in Java to String using Transformer","2014-03-20 16:23:47","22539424","0","11091","<java><xml><string><dom><transformer-model>","<p>I'm trying to convert a XML org.w3c.dom.Document to a String using a Transformer:</p>

<pre><code>DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();

    factory.setNamespaceAware(true);
    DocumentBuilder builder = null;
    try {
        builder = factory.newDocumentBuilder();
    } catch (ParserConfigurationException e) {

        e.printStackTrace();
    }

    Document doc = builder.newDocument();

    Element gameNode = doc.createElement(""Game"");
    gameNode.setAttribute(""gameID"", String.valueOf(game.getGameID()));
    gameNode.setAttribute(""username"", game.getUsername());
    gameNode.setAttribute(""gameStartTime"", String.valueOf(game.getGameStartTime()));
    gameNode.setAttribute(""gameStartDate"", String.valueOf(game.getGameStartDate()));
    gameNode.setAttribute(""totalScore"", String.valueOf(game.getTotalScore()));
    gameNode.setAttribute(""totalTimeInSeconds"", String.valueOf(game.getTotalTimeInSeconds()));
    gameNode.setAttribute(""lastLevel"", String.valueOf(game.getLastLevel()));
    gameNode.setAttribute(""firstLevel"", String.valueOf(game.getFirstLevel()));

    doc.appendChild(gameNode);



    for(int i = 0 ; i &lt; game.getScores().size(); i++)
    {
        Element scoreNode = doc.createElement(""Score"");
        scoreNode.setAttribute(""gameID"", String.valueOf(game.getScores().get(i).getGameID()));
        scoreNode.setAttribute(""points"", String.valueOf(game.getScores().get(i).getPoints()));
        scoreNode.setAttribute(""levelOfScore"", String.valueOf(game.getScores().get(i).getLevelOfScore()));
        scoreNode.setAttribute(""timeToScore"", String.valueOf(game.getScores().get(i).getTimeToScore()));

        gameNode.appendChild(scoreNode);
    }


    return DocumentToString(doc);
</code></pre>

<p>I'm using my objects called Game and Score, which shouldn't interfere with the process.</p>

<pre><code>public static String DocumentToString(Document doc)
{
    try {
        TransformerFactory transformerFactory = TransformerFactory.newInstance();
        Transformer transformer = transformerFactory.newTransformer();
        DOMSource source = new DOMSource(doc);
        StreamResult result = new StreamResult(System.out);  
        transformer.transform(source, result); 
        return ""yeah"";
    } catch (Exception ex) {
        return ""Error converting to String"";
    }

}
</code></pre>

<p>For some reason the transformer.transform method does not complete successfully.</p>
","transformer-model"
"22522558","Create empty text node in DOM Document","2014-03-20 02:56:03","22522755","5","20378","<java><xml><domdocument><string><transformer-model>","<p>How can we create an empty node in a DOM document without getting <code>java.lang.NullPointerException</code> while writing XML files with <code>Transformer</code>?</p>

<p>I am writing an XML Document with some nodes possibly with empty values using <code>element.appendChild(documnet.createTextNode(nodeValue));</code></p>

<p>Here this nodeValue could be an empty String or lets say null or empty value, then how could we force it to write this empty text node like <code>&lt;emptyTextNode/&gt; or &lt;emptyTextNode&gt;&lt;emptyTextNode&gt;</code></p>

<p>If I just simply write this node with an empty value, <code>Transformer</code> throws <code>NullPointerException</code>, while transforming the Document to XML.</p>

<p>This nullpointer exception needs to be avoided and allow this empty node in output XML anyhow condition.</p>
","transformer-model"
"22066520","datamapper in mule- facing issue to map many to single because the child elements of input are disabled","2014-02-27 11:02:41","","0","646","<xml><mule><datamapper><transformer-model>","<p>mule-datamapper:
I am giving an input xml file as input.xml using this i am creating a  schema  and as output i am giving output xml in mule for this also i am creating xsd.</p>

<p>Input xml:</p>

<pre><code>&lt;abcd xmlns:ns0=""http://a.b.com/d""&gt;
&lt;temporaryaddress&gt;

&lt;address id=""1""&gt;
&lt;name&gt;x&lt;/name&gt;
&lt;country&gt;india&lt;/country&gt;
&lt;zipcode&gt;890765&lt;/xipcode&gt;
&lt;/address&gt;


&lt;address id=""2""&gt;
&lt;name&gt;y&lt;/name&gt;
&lt;country&gt;india&lt;/country&gt;
&lt;zipcode&gt;890766&lt;/xipcode&gt;
&lt;/address&gt;
&lt;/temporaryaddress&gt;
&lt;/abcd&gt;
</code></pre>

<p>and current output xml is like</p>

<pre><code>&lt;abcd xmlns:ns1=""http://e.f.com/g""&gt;
&lt;temporaryaddress&gt;
&lt;address&gt;
&lt;name&gt;y&lt;/name&gt;
&lt;country&gt;india&lt;/country&gt;
&lt;zipcode&gt;890766&lt;/xipcode&gt;
&lt;/address&gt;
&lt;/temporaryaddress&gt;
&lt;/abcd&gt;
</code></pre>

<p>when i tried mapping in mule it is showing the elements under temporaryaddress is disabled in input i am unable to map those.</p>

<p>Desired output xml:</p>

<pre><code>&lt;abcd&gt;
&lt;temporaryaddress&gt;
&lt;address&gt;
&lt;name&gt;x&lt;/name&gt;
&lt;country&gt;india&lt;/country&gt;
&lt;zipcode&gt;890765&lt;/xipcode&gt;
&lt;/address&gt;
&lt;/temporaryaddress&gt;

&lt;temporaryaddress&gt;
&lt;address&gt;
&lt;name&gt;y&lt;/name&gt;
&lt;country&gt;india&lt;/country&gt;
&lt;zipcode&gt;890766&lt;/xipcode&gt;
&lt;/address&gt;
&lt;/temporaryaddress&gt;
&lt;/abcd&gt;
</code></pre>

<p>i need to map many to single and get the output differently with root element.</p>

<p>In crisp: I have two address elements inside temporary address. In the output I need two temporary address elements with 1st address copied into  1st temporary address element and 2nd address of input into 2nd temporary address of output.
Any links/suggestions are appreciated.</p>
","transformer-model"
"21919725","Mule XSLT Transformer is adding namespace after transformation","2014-02-20 21:13:17","21919902","0","1495","<xml><xslt><namespaces><mule><transformer-model>","<p><strong>Question:</strong>
I am trying to add a uuid as a new element to incoming xml message. I can see that added as I am logging the result, but mule adds its namespace and the java util uuid namespace to the result which, causes another service where I route this message, NOT recognize it as it has a namespace which it doesn't know about.</p>

<p>Is there a way to configure the xslt transformer to achieve what I am trying to do here? Any other suggestions that could be an alternative?</p>

<p>Goal is to have xslt generate a uuid and tag on to the incoming message and pass it to a service end point.
Appreciate all the help in this regard.</p>

<p><strong>Mule Config:</strong>
    </p>

<pre><code>&lt;mule xmlns:mulexml=""http://www.mulesoft.org/schema/mule/xml""
  xmlns:tracking=""http://www.mulesoft.org/schema/mule/ee/tracking""
  xmlns:http=""http://www.mulesoft.org/schema/mule/http"" xmlns=""http://www.mulesoft.org/schema/mule/core""
  xmlns:doc=""http://www.mulesoft.org/schema/mule/documentation""
  xmlns:spring=""http://www.springframework.org/schema/beans"" version=""EE-3.4.1""
  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""
  xmlns:billing=""http://mycompany.com/billing""
  xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-current.xsd
http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/current/mule-http.xsd
http://www.mulesoft.org/schema/mule/ee/tracking http://www.mulesoft.org/schema/mule/ee/tracking/current/mule-tracking-ee.xsd
http://www.mulesoft.org/schema/mule/xml http://www.mulesoft.org/schema/mule/xml/current/mule-xml.xsd""&gt;

  &lt;mulexml:xslt-transformer name=""xslt""
    doc:name=""XSLT""&gt;
    &lt;mulexml:xslt-text&gt;
      &lt;xsl:stylesheet version=""2.0"" xmlns:uuid=""java:java.util.UUID""&gt;
        &lt;xsl:variable name=""uid"" select=""uuid:randomUUID()"" /&gt;
        &lt;xsl:template match=""/""&gt;
          &lt;xsl:apply-templates /&gt;
        &lt;/xsl:template&gt;

        &lt;xsl:template match=""node()|@*""&gt;
          &lt;xsl:copy&gt;
            &lt;xsl:apply-templates select=""node()|@*"" /&gt;
          &lt;/xsl:copy&gt;
        &lt;/xsl:template&gt;

        &lt;xsl:template match=""Request""&gt;
          &lt;Request&gt;
            &lt;xsl:apply-templates select=""node()|@*"" /&gt;
            &lt;RequestId&gt;
              &lt;xsl:value-of select=""$uid"" /&gt;
            &lt;/RequestId&gt;
          &lt;/Request&gt;
        &lt;/xsl:template&gt;
      &lt;/xsl:stylesheet&gt;
    &lt;/mulexml:xslt-text&gt;
  &lt;/mulexml:xslt-transformer&gt;

  &lt;flow name=""rsi_invoiceFlow1"" doc:name=""rsi_invoiceFlow1""&gt;
    &lt;http:inbound-endpoint exchange-pattern=""request-response""
      address=""${listener.hostname}:${listener.port}/${listener.path.invoice.rsi}""
      doc:name=""HTTP"" transformer-refs=""xslt"" /&gt;


    &lt;logger message=""#[message.payloadAs(java.lang.String)]"" level=""ERROR""
      doc:name=""Logger"" /&gt;

    &lt;http:outbound-endpoint exchange-pattern=""request-response""
      method=""POST"" address=""${destination.dev2.url}/"" doc:name=""HTTP""
      doc:description=""The '/' at the end of the URL is required on the RSI outbound call"" /&gt;
  &lt;/flow&gt;
&lt;/mule&gt;
</code></pre>

<p><strong>Incoming XML:</strong></p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;billing:RQ xmlns:billing=""http://mycompany.com/billing""&gt;
  &lt;Request&gt;
    &lt;CallingHostOrWeblogicInstance&gt;SOAPUI&lt;/CallingHostOrWeblogicInstance&gt;
  &lt;/Request&gt;
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;billing:RQ xmlns:billing=""http://mycompany.com/billing""&gt;
&lt;Request xmlns=""http://www.mulesoft.org/schema/mule/core"" xmlns:uuid=""java:java.util.UUID""&gt;&lt;CallingHostOrWeblogicInstance xmlns=""""&gt;SOAPUI&lt;/CallingHostOrWeblogicInstance&gt;
&lt;RequestId&gt;bff3e1d6-ecdd-41ae-8807-ec04085a2b54&lt;/RequestId&gt;
&lt;/Request&gt;
</code></pre>
","transformer-model"
"21916541","When using the jar file xmlparserv2.jar, transforming a doesn't work when the file path has whitespace. How can i fix this?","2014-02-20 18:32:12","","0","205","<java><xml-parsing><transformer-model>","<p>Normally when using ...</p>

<pre><code>Transformer t = TransformerFactory.newInstance().newTransformer(); 
t.transform(source,result);
</code></pre>

<p>(without the xmlparserv2.jar file) a File Not Found Exception looks like this.</p>

<pre><code>Exception in thread ""main"" java.io.FileNotFoundException: C:\Documents and Settings\username\nonExistentFile.xml (The system cannot find the file specified)
</code></pre>

<p>when you include the xmlparserv2.jar, the Exception turns to this</p>

<pre><code>Caused by: java.io.FileNotFoundException: C:\Documents%20and%20Settings\username\existingFile.xml (The system cannot find the path specified)
</code></pre>

<p>The file is actually there (the transform method finds it when i dont include the jar)  but when i include the jar, the transform method cant find it due to the %20 that is inserted for whitespace. Can someone please tell me how to fix this?</p>
","transformer-model"
"21827071","Prestashop : remove shopping cart from homepage only (transformer theme)","2014-02-17 10:52:27","","1","2895","<shopping-cart><prestashop><transformer-model>","<p>I'm working on a transformer theme and I would like to remove the shopping cart from the home page only (without a display:none if possible)
I tried editing the module ""Right bar cart block"" but to no effect (even deactivating it doesn't change anything)
Do you have any idea of how to do this ?
I had a look in the index.tpl index.php and the header.tpl but couldn't find any include shopping cart in the code.</p>

<p>Thank you</p>
","transformer-model"
"21697991","Ember JS, Error while parsing Data in Transformer handeling event `didCommit` in state root.loaded.updated.uncommitted.""","2014-02-11 09:34:40","","3","701","<javascript><ember.js><ember-data><transformer-model>","<p>I am getting an error in an Ember Transformer trying a parse a date in the serialize function. </p>

<p>Error message:
""Attempted to handle event <code>didCommit</code> on &lt;(subclass of DS.Model):ember1597:8260357> while in state root.loaded.updated.uncommitted.""</p>

<p>Strangely enough, the data is transmitted correctly parsed to the server.</p>

<p>Code:</p>

<pre><code>DS.Transform.extend({

    deserialize : function(serialized) {

        var array = [];

        if (Ember.isArray(serialized)) {

            serialized.forEach(function(item) {
                if (item.feldTyp === ""DATE_FIELD"" &amp;&amp; item.value) {
                    Ember.set(item, ""value"", moment(item.value, ""DD.MM.YYYY""));
                }
                array.addObject(Ember.Object.create(item));
            });
        }

        return array;
    },

    serialize : function(deserialized) {
        if (Ember.isArray(deserialized)) {
            deserialized.forEach(function(item) {
                if (item.get('feldTyp') === ""DATE_FIELD"" &amp;&amp; item.get('value')) {
                    item.set('value', moment(item.get('value')).format(""DD.MM.YYYY""));
                }
            });
            return deserialized;
        }

        return [];
    }
});
</code></pre>

<p>The line <code>item.set('value', moment(item.get('value')).format(""DD.MM.YYYY""));</code> causes the error as commented out the error vanishes.  I tried other things like setting a static value or setting the value using <code>Ember.set</code> but without success. I do not quite know what went wrong here and thus cannot think of a solution. Can someone help? Thanks in advance.</p>

<p><em>edit</em></p>

<p>Workaround:
I moved the serialization into the controller. Does not look as elegant but works for now...</p>
","transformer-model"
"21036765","TransformerConfigurationException:Could not compile sytlesheet","2014-01-10 05:26:45","21039388","0","2096","<java><xml><xslt><transformer-model>","<p>When I try to transform xml with xslt in my web application, the TransformerConfigurtionException exception is thrown in weblogic 10.3 server. The same web application code works fine in Tomcat 7.0. I have no clue what could be cause for this exception.</p>

<pre><code>Exception:
javax.xml.transform.TransformerConfigurationException: Could not compile stylesheet
ERROR: 'Syntax error in 'format-date($date,'[MNn][D],[Y]','en',(),())'.'
FATAL ERROR: 'Could not compile stylesheet'
</code></pre>

<p>After removing the format-date function in xslt, i got another exception (javax.xml.tranform.TransformerException:java.lang.ArrayIndexOutOfBoundsException</p>

<p>Code:</p>

<pre><code>TransformerFactory factory = TransformerFactory.newInstance();

  try
  {
   factory.newTransformer().transform( new StreamSource( new StringReader( xml ) ), new StreamResult( transformResult ) );

   Source documentInfoSource = new StringSource( new String( transformResult.toByteArray() ) );
   transformResult.reset();

   factory.setURIResolver( new URIResolver()
   {
    @Override
    public Source resolve(String href, String base) throws TransformerException
    {
     try
     {
      return new StreamSource( EcrionDocumentRenderServiceImpl.class.getClassLoader().getResourceAsStream( href ) );
     }

     catch( Exception e )
     {
      throw new TransformerException( e );
     }
    }

   } );
   factory.newTransformer( new StreamSource( Thread.currentThread().getContextClassLoader().getResourceAsStream( ""template.xsl"" ) ) ).transform( documentInfoSource, new StreamResult( transformResult ) );TransformerFactory factory = TransformerFactory.newInstance();

  try
  {
   factory.newTransformer().transform( new StreamSource( new StringReader( xml ) ), new StreamResult( transformResult ) );

   Source documentInfoSource = new StringSource( new String( transformResult.toByteArray() ) );
   transformResult.reset();

   factory.setURIResolver( new URIResolver()
   {
    @Override
    public Source resolve(String href, String base) throws TransformerException
    {
     try
     {
      return new StreamSource( EcrionDocumentRenderServiceImpl.class.getClassLoader().getResourceAsStream( href ) );
     }

     catch( Exception e )
     {
      throw new TransformerException( e );
     }
    }

   } );
   factory.newTransformer( new StreamSource( Thread.currentThread().getContextClassLoader().getResourceAsStream( ""template.xsl"" ) ) ).transform( documentInfoSource, new StreamResult( transformResult ) );
</code></pre>
","transformer-model"
"20861321","Custom XML Formatting with Java DOM","2013-12-31 17:48:58","","0","273","<java><xml><dom><format><transformer-model>","<p>This question is regarding pretty-printing with Java and XML. The data itself is stored in XML but I am using Java to manipulate the data. I have the following code</p>

<pre><code>&lt;ParentElement&gt;
  &lt;ChildElement&gt;&lt;HeaderNum&gt;34&lt;/HeaderNum&gt;&lt;LineNum&gt;21&lt;/LineNum&gt;&lt;/ChildElement&gt;
  &lt;ChildElement&gt;&lt;HeaderNum&gt;42&lt;/HeaderNum&gt;&lt;LineNum&gt;54&lt;/LineNum&gt;&lt;/ChildElement&gt;
&lt;/ParentElement&gt;
</code></pre>

<p>I want to insert another ChildElement in between the two existing ChildElement lines. I have no problem actually doing this, but I need to maintain that formatting. If I use</p>

<pre><code>transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
</code></pre>

<p>It will completely reformat the file. If I set that property to ""no"", it inserts the new , but not on its own line. It comes on the same line as the first .</p>

<p>Personally I'd just make both of these values attributes, but I can't change the formatting of the file. Is there a way to insert a new  on its own line, but without any other formatting?</p>
","transformer-model"
"20353097","How to change color and stroke of one type of edges","2013-12-03 13:56:40","20366792","0","405","<java><jung><transformer-model><jung2>","<p>I have a Graph&lt; Potter, Edge> g where Edges belongs to three different classes that extends the Edge class (I use it to represent different type of interactions, like begin related, ask advices and so on...). I would like to color edges accordingly to what they represent, like having all Parental edges turn green and not display arrows. I have this transformer to change color, but it appears to change colors to all the edges:</p>

<pre><code>Transformer&lt;Edge, Paint&gt; parental_color_yes = new Transformer&lt;Edge, Paint&gt;() {
        @Override
        public Paint transform(Edge s) {
            return Color.GREEN;
        }
    };
</code></pre>

<p>If I change the type of the transformer to Transformer&lt; Parental, Paint>, then my VisualizationViewer&lt; Potter, Edge> vv complains that cannot accept such a transformer... should I add a new visualization viewer? or there is something wrong in the transformer?</p>

<p>EDITED after the reply:</p>

<pre><code> parental_color_yes = new Transformer&lt;Edge, Paint&gt;() {
        @Override
        public Paint transform(Edge s) {
            if (s instanceof Parental){
                return Color.GREEN;
            } else if (s instanceof Innovation) {
                return Color.RED;
            } else {
                return Color.BLACK;
            }
        }
    };
</code></pre>

<p>Thank you for the help!</p>

<p>Best regards,
Simone</p>
","transformer-model"
"20331852","Java Hibernate Transformer AliasToBeanNestedResultTransformer","2013-12-02 15:28:26","","1","3094","<java><hibernate><transformer-model>","<p>i have a query like this. i pass the student ID i need some fields from Student as well as their parent as well some fields from the parent->Address[here is the main problem i am facing] i am using <code>AliasToBeanNestedResultTransformer</code> transformer by <a href=""https://stackoverflow.com/users/1129044/sami-andoni"">Sami Andoni</a> </p>

<p>here is the implementation of it <a href=""https://github.com/samiandoni/AliasToBeanNestedResultTransformer/blob/master/AliasToBeanNestedResultTransformer.java"" rel=""nofollow noreferrer"">CODE</a></p>

<p>here is my code.</p>

<pre><code>public List&lt;Student&gt;searchForStudent(Integer studentId)
{           
    Projection p=Projections.projectionList().create()
    .add(Projections.property(""name""),""name"")//the student name it works O.K
    .add(Projections.property(""lastname""),""lastname"")//the student name it works O.K
    .add(Projections.property(""age""),""age"")//the student AGE it works O.K                
    .add(Projections.property(""p.phone""),""parent.phone"")//the parent phone it works O.K                
    .add(Projections.property(""address.state"").as(""parent.Address.state"")); // i need a field from address.state here is the problem...  
    Session session = ......
    Criteria like = session.createCriteria(Student.class).add(prepareForSelect())//some filters..
    .createAlias(""parent"",""p"")//the parent of the student. a student have one parent
    .createAlias(""parent.Address"",""address"")//the address of the parent.... a parent have one address.
    .setProjection(p)                
    .setResultTransformer(new AliasToBeanNestedResultTransformer(Student.class));    
    List&lt;Student&gt;results=like.list();   
    return results;     
}         
</code></pre>

<p>it throws </p>

<pre><code>Exception in thread ""main"" org.hibernate.PropertyAccessException:   IllegalArgumentException occurred while calling setter of com.generic.model.Parent.Address
</code></pre>

<p>FYI is some type mismatch i have done some tracing in <code>SAMI</code> code and i see this </p>

<pre><code>[MyState]
[Address]
</code></pre>

<p>seems that Hibernate is returning a <code>String State</code> <code>MyState</code> in this case  and the transformer is using a <code>Address Object</code> and this is the <code>type Mismatch.</code></p>

<p>is any help is hugely needed it</p>

<p>thanks a lot.</p>
","transformer-model"
"19716796","Concat Children Nodes XPathEntityProcessor Solr DIH","2013-10-31 21:19:02","","1","1009","<xpath><solr><transformer-model><dih>","<p>I am trying to make a solr field's value contain information about a parent child relationship in the XML via DIH.</p>

<p>Here is the dataConfig:</p>

<pre><code>&lt;dataConfig&gt;
   &lt;script&gt;
      &lt;![CDATA[
         function doSomething(row){
             //logic
             return row;
         }
      ]]&gt;
   &lt;/script&gt;
   &lt;dataSource type=""URLDataSource""/&gt;
    &lt;document&gt;
         &lt;entity name=""getModels""
            pk=""id""
            url=""pathpathpath""
            processor=""XPathEntityProcessor""
            forEach=""/path""
            transformer=""script:doSomething""
            &gt;
                &lt;field column=""spec"" xpath=""/Group/name/SubGroup"" transformer=""script:doSomething""/&gt; 
                &lt;field column = ... /&gt;
          &lt;/entity&gt;
    &lt;/document&gt;
</code></pre>

<p></p>

<p>Sample XML:</p>

<pre><code>&lt;Group&gt;
   &lt;name&gt;
      Vehicle
   &lt;/name&gt;
   &lt;SubGroup&gt;
        &lt;name&gt;Car&lt;/name&gt;
   &lt;/SubGroup&gt;
   &lt;SubGroup&gt;
        &lt;name&gt;Bike&lt;/name&gt;
   &lt;/SubGroup&gt;
&lt;/Group&gt;

&lt;Group&gt;
   &lt;name&gt;
      Fruit
   &lt;/name&gt;
   &lt;SubGroup&gt;
        &lt;name&gt;Apple&lt;/name&gt;
   &lt;/SubGroup&gt;
   &lt;SubGroup&gt;
        &lt;name&gt;Banana&lt;/name&gt;
   &lt;/SubGroup&gt;
&lt;/Group&gt;
</code></pre>

<p>How would I specify in:</p>

<pre><code> &lt;field column=""spec"" xpath=""/Group/name/SubGroup"" transformer=""script:doSomething""/&gt; 
</code></pre>

<p>So that I can have the corresponding instances of spec be:</p>

<pre><code>Vehicle Car
</code></pre>

<p>and</p>

<pre><code>Vehicle Bike
</code></pre>

<p>and</p>

<pre><code>Fruit Apple
</code></pre>

<p>and</p>

<pre><code>Fruit Banana
</code></pre>

<p>Ideally with a delimiter between name and subname like:</p>

<pre><code>Fruit::Banana
</code></pre>
","transformer-model"
"19711465","Java Xml Transformation and Surrogates","2013-10-31 16:06:35","19731551","0","1392","<java><android><xml><transformer-model><surrogate-pairs>","<p>The code below is not correctly transforming the input data to XML. I think so because I don't expect <strong>Transformer</strong> to generate output with non-valid xml characters in it (I'm talking about the &amp;).</p>

<p>Here is the code:</p>

<pre><code>package com.example.test.formatter;

import java.io.StringWriter;
import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.transform.OutputKeys;
import javax.xml.transform.Transformer;
import javax.xml.transform.TransformerFactory;
import javax.xml.transform.dom.DOMSource;
import javax.xml.transform.stream.StreamResult;
import org.w3c.dom.Document;
import org.w3c.dom.Element;
import android.test.AndroidTestCase;
import android.util.Log;

public class XmlTest extends AndroidTestCase {

    public void testFormat() {

        try {
            String arbitraryInput = ""Arbitrary input: \uD83D""; // we don't have control over this input

            DocumentBuilderFactory documentBuilderFactory = DocumentBuilderFactory.newInstance();
            DocumentBuilder documentBuilder = documentBuilderFactory.newDocumentBuilder();
            Document document = documentBuilder.newDocument();

            TransformerFactory transformerFactory = TransformerFactory.newInstance();
            Transformer transformer = transformerFactory.newTransformer();
            transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""yes"");
            transformer.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");
            transformer.setOutputProperty(OutputKeys.INDENT, ""true"");

            StringWriter stringWriter = new StringWriter();
            StreamResult result = new StreamResult(stringWriter);
            DOMSource source = new DOMSource(document);

            Element root = document.createElement(""root"");
            Element subElement = document.createElement(""key"");
            subElement.setTextContent(arbitraryInput);
            root.appendChild(subElement);

            document.appendChild(root);

            stringWriter.getBuffer().setLength(0);
            transformer.transform(source, result);

            String parsed = stringWriter.toString(); // &lt;root&gt;&lt;key&gt;Arbitrary input: &amp;#55357;&lt;/key&gt;&lt;/root&gt;
            Log.e(""parsed"", parsed);
        }
        catch(Throwable ex) {
            ex.printStackTrace();
        }

    }

}
</code></pre>

<p>I was expecting to get something like</p>

<pre><code>&lt;root&gt;&lt;key&gt;Arbitrary input: &amp;amp; #55357;&lt;/key&gt;&lt;/root&gt;
</code></pre>

<p>But instead I get:</p>

<pre><code>&lt;root&gt;&lt;key&gt;Arbitrary input: &amp;#55357;&lt;/key&gt;&lt;/root&gt;
</code></pre>

<p>So, what should I do if I want to get valid XML output of Transformer?</p>

<p>Thanks!</p>

<p><strong>EDIT:</strong></p>

<p>I think that the output is invalid because when I'm trying to process the produced XML output with PHP like this:</p>

<pre><code>&lt;?php

$data = ""&lt;root&gt;&lt;key&gt;Arbitrary input: &amp;#55357;&lt;/key&gt;&lt;/root&gt;"";

$xmlDocument = new \DOMDocument();
$xmlDocument-&gt;loadXML($data);
</code></pre>

<p>I get a warning (or exception if the environment was configured to throw exceptions on warnings):</p>

<pre><code>PHP Warning:  DOMDocument::loadXML(): xmlParseCharRef: invalid xmlChar value 55357 in Entity, line: 1 in /tmp/test.php on line 6
PHP Stack trace:
PHP   1. {main}() /tmp/test.php:0
PHP   2. DOMDocument-&gt;loadXML() /tmp/test.php:6
</code></pre>

<p>Please note that if the I was trying to process with DOMDocument (PHP) the following code everything would be just fine:</p>

<pre><code>$data = "" &lt;root&gt;&lt;key&gt;Arbitrary input: &amp;amp; #55357;&lt;/key&gt;&lt;/root&gt;"";
</code></pre>

<p>Either the Java Transformer or the DOMDocument (PHP) is doing something wrong. Can you point me out?</p>

<p>Thanks!</p>
","transformer-model"
"19710996","How can I insert a HTML tag before its parent tag using the Transformer from Genshi?","2013-10-31 15:46:28","","1","237","<trac><transformer-model><genshi>","<p>I need to modify the file table in my trac browser view by creating a class which implements the <code>ITemplateStreamfilter</code> class. I tried using the <code>Transformer</code> from <code>genshi.filters.transform</code>. My table looks like</p>

<p><code>&lt;tbody&gt;<br>
    &lt;tr class=""even""&gt;<br>
        &lt;td class=""name""&gt;<br>
            &lt;a class=""partent"" title=""Parent Directory"" ..&gt;..&lt;/a&gt;<br>
        &lt;/td&gt;<br>
        ..<br>
    &lt;/tr&gt;<br>
    ..<br>
&lt;/tbody&gt;</code></p>

<p>I now need to insert a <code>&lt;/td&gt;</code> tag just before the frist cell in the first row of the table. The problem is that I only can identify the position of column where I want to put the new cell befor by searching for the <em>""Parent Directory""</em> title: <code>Transformer('//*[@title=""Parent Directory""]')</code>.  How can I step one tag up than put the new cell before the first <code>&lt;td class=""name""&gt;</code> tag?</p>
","transformer-model"
"19248950","TransformerConfigurationException: Could not compile stylesheet, error - no protocol","2013-10-08 13:25:18","19249092","2","8003","<java><xslt><transformer-model>","<p>I have an XMl which I trying to convert using transformer.</p>

<p>I am getting the following error:</p>

<pre><code>ERROR:  'no protocol:
FATAL ERROR:  'Could not compile stylesheet'
javax.xml.transform.TransformerConfigurationException: Could not compile stylesheet
at    com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl.newTemplates(TransformerFactoryImpl.java:885)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerFactoryImpl.newTransformer(TransformerFactoryImpl.java:671)
</code></pre>

<p>My code which performs this transform is:</p>

<pre><code>Source xslInput = new StreamSource(finalStr);
        Source xmlInput = new StreamSource(str);

        Transformer transformer = factory.newTransformer(xslInput);
        ByteArrayOutputStream bos = new ByteArrayOutputStream();
        Result result = new StreamResult(bos);
        transformer.transform(xmlInput, result);
        String s = new String(bos.toByteArray());
</code></pre>

<p>The XLS file which I am using is located at the following location: <a href=""https://code.google.com/p/ccd-bluebutton/source/browse/trunk/Bluebutton/ccd/xslt/CCDtoBlueButtonTxt_Resource.xsl?r=103"" rel=""nofollow"">https://code.google.com/p/ccd-bluebutton/source/browse/trunk/Bluebutton/ccd/xslt/CCDtoBlueButtonTxt_Resource.xsl?r=103</a></p>

<p>Can you please tell where I am going wrong?</p>

<p>Regards,
Radhika</p>
","transformer-model"
"18760453","How to get the last day of the current month in DataStage?","2013-09-12 09:27:59","18760552","0","9801","<date><transformer-model><datastage>","<p>I have explored all the functions avialable in the trasformer, but could not found the exact function to get the last day of current month by passing date in same default format i.e. yyyy-mm-dd.Please help me in this regard.</p>
","transformer-model"
"18602397","Mule returning a MessageCollection from component","2013-09-03 22:17:41","18617189","0","1493","<mule><transformer-model>","<p>I'm having difficulty returning a collection of MuleMessage instances from a component.  </p>

<p>Mule 3.3.1.</p>

<p>The following code works (i.e., after the component, a foreach component with a logger dumps out ""abc"" and ""def"" as I expect).</p>

<pre><code>public Object onCall( MuleEventContext eventContext ) throws Exception 
{
    MuleMessage message = eventContext.getMessage();

    MuleMessageCollection collection = new DefaultMessageCollection( message.getMuleContext() );

    String s1 = ""abc"";
    String s2 = ""def"";
    DefaultMuleMessage m1 = new DefaultMuleMessage( s1, message.getMuleContext() );
    DefaultMuleMessage m2 = new DefaultMuleMessage( s2, message.getMuleContext() );
    List&lt;MuleMessage&gt; list = new ArrayList&lt;MuleMessage&gt;();
    list.add( m1 );
    list.add( m2 );

    collection.addMessages( list );

    return collection;
}
</code></pre>

<p>If, however, I substitute a class of my own in place of the Strings, like so:</p>

<pre><code>public Object onCall( MuleEventContext eventContext ) throws Exception 
{
    MuleMessage message = eventContext.getMessage();

    MuleMessageCollection collection = new DefaultMessageCollection( message.getMuleContext() );

    LicenseRequest s1 = new LicenseRequest();
    LicenseRequest s2 = new LicenseRequest();
    DefaultMuleMessage m1 = new DefaultMuleMessage( s1, message.getMuleContext() );
    DefaultMuleMessage m2 = new DefaultMuleMessage( s2, message.getMuleContext() );
    List&lt;MuleMessage&gt; list = new ArrayList&lt;MuleMessage&gt;();
    list.add( m1 );
    list.add( m2 );

    collection.addMessages( list );

    return collection;
}
</code></pre>

<p>I get an exception:</p>

<pre><code>org.mule.transport.http.HttpsConnector Work caused exception on 'workCompleted'. Work being executed was: org.mule.transport.http.HttpsMessageReceiver$HttpsWorker@7b921c57
org.mule.exception.DefaultSystemExceptionStrategy Caught exception in Exception Strategy: Payload was invalidated calling setPayload and the message is not collection anymore.
java.lang.IllegalStateException: Payload was invalidated calling setPayload and the message is not collection anymore.
    at org.mule.DefaultMessageCollection.checkValidPayload(DefaultMessageCollection.java:107)
    at org.mule.DefaultMessageCollection.newThreadCopy(DefaultMessageCollection.java:312)
    at org.mule.DefaultMuleEvent.newThreadCopy(DefaultMuleEvent.java:779)
    at org.mule.RequestContext.newEvent(RequestContext.java:140)
    at org.mule.RequestContext.setExceptionPayload(RequestContext.java:121)
    at org.mule.exception.AbstractSystemExceptionStrategy.handleException(AbstractSystemExceptionStrategy.java:54)
    at org.mule.exception.AbstractSystemExceptionStrategy.handleException(AbstractSystemExceptionStrategy.java:77)
    at org.mule.transport.http.HttpMessageReceiver$HttpWorker.run(HttpMessageReceiver.java:220)
    at org.mule.work.WorkerContext.run(WorkerContext.java:311)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
Exception in thread ""[license-generation].HTTPSConnector.receiver.02"" java.lang.IllegalStateException: Payload was invalidated calling setPayload and the message is not collection anymore.
    at org.mule.DefaultMessageCollection.checkValidPayload(DefaultMessageCollection.java:107)
    at org.mule.DefaultMessageCollection.newThreadCopy(DefaultMessageCollection.java:312)
    at org.mule.DefaultMuleEvent.newThreadCopy(DefaultMuleEvent.java:779)
    at org.mule.RequestContext.newEvent(RequestContext.java:140)
    at org.mule.RequestContext.setExceptionPayload(RequestContext.java:121)
    at org.mule.exception.AbstractSystemExceptionStrategy.handleException(AbstractSystemExceptionStrategy.java:54)
    at org.mule.exception.AbstractSystemExceptionStrategy.handleException(AbstractSystemExceptionStrategy.java:77)
    at org.mule.transport.AbstractConnector.handleWorkException(AbstractConnector.java:2099)
    at org.mule.transport.AbstractConnector.workCompleted(AbstractConnector.java:2067)
    at org.mule.work.WorkerContext.run(WorkerContext.java:338)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
    at java.lang.Thread.run(Thread.java:722)
</code></pre>

<p>At this point in my testing, LicenseRequest is just an empty class whose toString() method returns ""ghi"".  </p>

<p>What am I doing wrong?</p>

<p>I should add that my goal is to return a collection of objects which are created from the incoming payload.  I can then iterate over those objects in the calling flow and take appropriate action for each.</p>

<p><strong>EDIT:</strong>  It appears I can do what I want in a transformer, just not a component.  Why is this?</p>
","transformer-model"
"17812882","How to transform javax.mail.internet.MimeMessage to java.io.InputStream in Mule","2013-07-23 14:19:17","","0","850","<email><mule><transformer-model>","<p>I tried to read a file and email through Mule. But it throws the following exception.</p>

<pre><code>  org.mule.api.transformer.TransformerException: Could not find a transformer to    transform ""SimpleDataType{type=javax.mail.internet.MimeMessage, mimeType='*/*'}"" to ""SimpleDataType{type=java.io.InputStream, mimeType='*/*'}"".
  at  org.mule.registry.MuleRegistryHelper.lookupTransformer(MuleRegistryHelper.java:252)
  at org.mule.DefaultMuleMessage.getPayload(DefaultMuleMessage.java:355)
  at org.mule.DefaultMuleMessage.getPayload(DefaultMuleMessage.java:313)
 + 3 more (set debug level logging or '-Dmule.verbose.exceptions=true' for everything)
</code></pre>

<p><strong>My mule file is as follows:</strong> 
    </p>

<pre><code>&lt;flow name=""Exception_HandlingFlow1"" doc:name=""Exception_HandlingFlow1""&gt;
    &lt;file:inbound-endpoint path=""D:\WorkArea\COPA\In"" responseTimeout=""10000"" doc:name=""File Writer""&gt;
        &lt;file:file-to-string-transformer/&gt;
    &lt;/file:inbound-endpoint&gt;
    &lt;component doc:name=""Java"" class=""com.exhnlr.Handler.ExceptionHandler""/&gt;
    &lt;smtps:outbound-endpoint host=""192.168.131.139"" responseTimeout=""10000"" doc:name=""SMTP"" 
        from=""anbu1@bibs.com"" subject=""test"" to=""anbu1@bibs.com"" user=""anbu1"" mimeType=""text/plain""&gt;
        &lt;email:string-to-email-transformer/&gt;
    &lt;/smtps:outbound-endpoint&gt;        
&lt;/flow&gt;
</code></pre>

<p>I have read a similiar link <a href=""https://stackoverflow.com/questions/14757677/how-to-parse-inbound-e-mail-when-using-mules-imap-transport"">How to parse inbound e-mail when using Mule&#39;s IMAP transport?</a>, but it didn't work for me.</p>

<p>Please help.</p>
","transformer-model"
"17639143","Symfony2 particular data transformer","2013-07-14 11:54:18","","1","1132","<forms><symfony><transformer-model>","<p>I have an entity Calendar with dateFrom and dateTo properties.</p>

<p>Now in my form I have one hidden input with date formatted like this: 2010-01-01,2011-01-01.</p>

<p>How can I write a data transformer in Symfony2 which will allow me to transform this date to TWO properties?</p>
","transformer-model"
"17594715","Solr : importing dynamic field names from XML with DIH and xpath","2013-07-11 13:29:20","","0","502","<regex><xpath><solr><dataimporthandler><transformer-model>","<p>I'm indexing data from a XML file, with many fields like these declared in DataImportHandler's <strong>dataconfig.xml</strong> :</p>

<pre><code>&lt;field column=""pos_A"" xpath=""/positions/pos_A/@pos"" /&gt;
&lt;field column=""pos_B"" xpath=""/positions/pos_B/@pos"" /&gt;
&lt;field column=""pos_C"" xpath=""/positions/pos_C/@pos"" /&gt;
...
</code></pre>

<p>And one matching <strong>dynamicField</strong> declaration in <strong>schema.xml</strong> :</p>

<pre><code>&lt;dynamicField name=""pos_*"" type=""sint"" indexed=""true"" stored=""true"" /&gt;
</code></pre>

<p>I'm wondering if it's possible to use a <a href=""http://docs.lucidworks.com/display/solr/Uploading+Structured+Data+Store+Data+with+the+Data+Import+Handler#UploadingStructuredDataStoreDatawiththeDataImportHandler-Transformers"" rel=""nofollow"">transformer</a> to dynamically generate the field names in <strong>dataconfig.xml</strong>, and have a single line, kinda like :</p>

<pre><code>&lt;field column=""pos_{$1}"" xpath=""/positions/pos_(*)/@pos"" /&gt;
</code></pre>

<p>(pardon my xpath and regex syntax :)</p>
","transformer-model"
"17517765","xml save and parse : org.xml.sax.SAXParseException: Content is not allowed in prolog","2013-07-08 00:35:17","","0","1804","<java><xml><transformer-model><saxparseexception>","<p>I've handling XML file with java.</p>

<p>If i handle xml file once, that is ok. Always successfully done.</p>

<p>But, if i handle xml file more than twice(save - read - save), always get error like this.</p>

<pre><code>org.xml.sax.SAXParseException: Content is not allowed in prolog.
at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:264)
at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:292)
at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:172)
</code></pre>

<p>this is my code. what is the problem this code?</p>

<pre><code>public static boolean save1(String baseDir) throws Exception {
    boolean result = true;

    File file = new File(baseDir + ""myfile.xml"");
    Document document;

    DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
    factory.setNamespaceAware(true);
    DocumentBuilder builder = factory.newDocumentBuilder();
    document = builder.parse(file);

    Element root = document.getDocumentElement();

    NodeList nodeList = root.getElementsByTagName(""root"");
    for (int i = 0; i &lt; nodeList.getLength(); i++) {
        Element node = (Element) nodeList.item(i);
        NamedNodeMap attrs = node.getAttributes();
        boolean isDefault = Boolean.valueOf(getText(attrs, ""default""));
        if (isDefault) {
            attrs.getNamedItem(""default_value"").setNodeValue(Boolean.valueOf(""false"");
            break;
        }
    }
    save(document, file);
    return result;
}

private static void save(Document doc, File xmlFile) throws Exception {
    TransformerFactory tranFactory = TransformerFactory.newInstance();
    Transformer transformer = tranFactory.newTransformer();

    transformer.setOutputProperty(OutputKeys.ENCODING, ""utf-8"");
    transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");

    OutputFormat format = new OutputFormat(doc);
    format.setIndenting(true);
    format.setIndent(4);
    format.setEncoding(""utf-8"");
    format.setPreserveSpace(false);

    BufferedWriter out = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(xmlFile), ""UTF-8""));
    XMLSerializer serializer = new XMLSerializer(out, format);
    serializer.serialize(doc.getDocumentElement());
    out.close();
}
</code></pre>
","transformer-model"
"17416370","Error Transforming Document to String","2013-07-02 02:11:35","","0","703","<java><xml><dom><transform><transformer-model>","<p>Let's see this link :</p>

<p><a href=""https://stackoverflow.com/q/11340779/2458559"">HTML DOM Tree to String - Transformer NullPointerException</a></p>

<p>I got the same problem with him..but he can't solve it. I don't want to change from JBrowser to DJ Project. I'm curious with this problem. Any idea what's wrong with this problem?</p>

<p>Thanks!</p>

<p><strong>Edit:</strong></p>

<p>HTML file : <a href=""http://www.uploadmb.com/dw.php?id=1372739472"" rel=""nofollow noreferrer"">http://www.uploadmb.com/dw.php?id=1372739472</a></p>

<p>This is method to transform document to string</p>

<pre><code>public String getStringFromDocument(org.w3c.dom.Document doc) {
    StringWriter sw = new StringWriter();
    try {
         doc = browser.getDocument();
         DOMSource domSource = new DOMSource(doc);
         StreamResult result = new StreamResult(sw);
         TransformerFactory tf = TransformerFactory.newInstance();
         Transformer transformer = tf.newTransformer();
         transformer.transform(domSource, result);

    } catch (TransformerConfigurationException e) {
         e.printStackTrace();
    } catch (TransformerException e) {
         e.printStackTrace();
    } catch (TransformerFactoryConfigurationError e) {
         e.printStackTrace();
    }

    return sw.getBuffer().toString();
}
</code></pre>

<p>This is method to get specific element :</p>

<pre><code>String html=getStringFromDocument(browser.getDocument());
//org.jsoup.nodes.Document doc;                    
this.doc=Jsoup.parse(html);
org.jsoup.select.Elements tableElements=doc.select(""table"");
org.jsoup.select.Elements rowElements = tableElements.select(""tr"");    
int k = rowElements.size();
for (org.jsoup.nodes.Element td : rowElements) {
    System.out.println(td.text());
    k = k - 1;
} 
</code></pre>

<p><strong>Error</strong> : </p>

<pre><code>ERROR:  'Namespace for prefix 'collapse;table-layout' has not been declared.'
javax.xml.transform.TransformerException: java.lang.RuntimeException: Namespace for       prefix 'collapse;table-layout' has not been declared.
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:736)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:340)
at browser.component.JBrowser.getStringFromDocument(JBrowser.java:428)
at browser.component.JBrowser$9.actionPerformed(JBrowser.java:133)
at javax.swing.AbstractButton.fireActionPerformed(AbstractButton.java:2018)
at javax.swing.AbstractButton$Handler.actionPerformed(AbstractButton.java:2341)
at javax.swing.DefaultButtonModel.fireActionPerformed(DefaultButtonModel.java:402)
at javax.swing.DefaultButtonModel.setPressed(DefaultButtonModel.java:259)
at javax.swing.plaf.basic.BasicButtonListener.mouseReleased(BasicButtonListener.java:252)
at java.awt.Component.processMouseEvent(Component.java:6505)
at javax.swing.JComponent.processMouseEvent(JComponent.java:3321)
at java.awt.Component.processEvent(Component.java:6270)
at java.awt.Container.processEvent(Container.java:2229)
at java.awt.Component.dispatchEventImpl(Component.java:4861)
at java.awt.Container.dispatchEventImpl(Container.java:2287)
at java.awt.Component.dispatchEvent(Component.java:4687)
at java.awt.LightweightDispatcher.retargetMouseEvent(Container.java:4832)
at java.awt.LightweightDispatcher.processMouseEvent(Container.java:4492)
at java.awt.LightweightDispatcher.dispatchEvent(Container.java:4422)
at java.awt.Container.dispatchEventImpl(Container.java:2273)
at java.awt.Window.dispatchEventImpl(Window.java:2719)
at java.awt.Component.dispatchEvent(Component.java:4687)
at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:729)
at java.awt.EventQueue.access$200(EventQueue.java:103)
at java.awt.EventQueue$3.run(EventQueue.java:688)
at java.awt.EventQueue$3.run(EventQueue.java:686)
at java.security.AccessController.doPrivileged(Native Method)
at java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)
at java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:87)
at java.awt.EventQueue$4.run(EventQueue.java:702)
at java.awt.EventQueue$4.run(EventQueue.java:700)
at java.security.AccessController.doPrivileged(Native Method)
at java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)
at java.awt.EventQueue.dispatchEvent(EventQueue.java:699)
at java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:242)
at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:161)
at java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:150)
at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:146)
at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:138)
at java.awt.EventDispatchThread.run(EventDispatchThread.java:91)
Caused by: java.lang.RuntimeException: Namespace for prefix 'collapse;table-layout' has not been declared.
at com.sun.org.apache.xml.internal.serializer.SerializerBase.getNamespaceURI(SerializerBase.java:914)
at com.sun.org.apache.xml.internal.serializer.SerializerBase.addAttribute(SerializerBase.java:431)
at com.sun.org.apache.xml.internal.serializer.ToUnknownStream.addAttribute(ToUnknownStream.java:316)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:201)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:230)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:230)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:230)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:136)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:98)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transformIdentity(TransformerImpl.java:683)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:727)
</code></pre>

<h2>    ... 39 more</h2>
","transformer-model"
"17187521","How can I access the parent row from dataimporthandler","2013-06-19 09:20:57","","2","385","<java><solr><dataimporthandler><transformer-model>","<p>I want to make a custom transformer for dataimporthandler. I want to access the row of the parent entity. How can I do that. Is it possible?</p>

<pre><code>  @Override
  public Object transformRow(Map&lt;String, Object&gt; row, Context context) {

    LOG.info(""##gyk: row: {}"",row);

    //******What to do here? ********
    LOG.info(""##gyk: PARENT-ROW: {}"", ????);


    return row;
  }
</code></pre>

<p><strong>UPDATE</strong></p>

<p>My current workaround is to use session variables</p>

<pre><code>  @Override
  public Object transformRow(Map&lt;String, Object&gt; row, Context context) {

    if (context.isRootEntity()) {

      Object tmp = row.get(""Keywords"");
      if (tmp == null) return row;

      String[] kwArray = tmp.toString().split("","");

      HashSet&lt;String&gt; kwSet = new HashSet&lt;String&gt;(64);

      for (String kw : kwArray) {
        kwSet.add(kw.trim().toLowerCase());
      }
      LOG.info(""##gyk: root entity kw: {}"",kwSet);

      context.setSessionAttribute(""company-keywords-set"", kwSet, ""document"");

    } else {
      LOG.info(""##gyk: row: {}"",row);

      HashSet&lt;String&gt; companyKeywordSet = (HashSet&lt;String&gt;)context
          .getParentContext().getParentContext()
          .getSessionAttribute(""company-keywords-set"", ""document"");

      if (companyKeywordSet != null) {
        LOG.info(""##gyk: keywords from parent: {}"", companyKeywordSet);
      }

    }

    return row;
  }
</code></pre>

<p><strong>Update</strong></p>

<p>When I try to log this <code>context.getParentContext().getParentContext().getAllEntityFields()</code>, I get the following:
<code>[{multiValued=false, toWrite=true, indexed=true, name=id, column=FIR_NR, boost=1.0, defaultValue=null, stored=true, type=string}, {multiValued=false, toWrite=true, indexed=true, name=fir_name, column=FIR_NAME, boost=1.0, defaultValue=null, stored=true, type=text_general}]</code></p>

<p>These are just the field  definitions for that entity.</p>
","transformer-model"
"17170733","symfony2 apply transformer to entity form field - empty array?","2013-06-18 13:52:06","","2","2931","<symfony><transformer-model>","<p>I am trying to get a dataTransformer to work on an entity field in symfony 2.</p>

<p>context:</p>

<ul>
<li><p>form displays sails that user can select (checkboxes)</p></li>
<li><p>this is the first step in a multi-step sail ordering process (later steps display options available for each sail, colors, etc)</p></li>
</ul>

<p>This is my form type class:</p>

<pre><code>use Symfony\Component\Form\AbstractType;
use Symfony\Component\OptionsResolver\OptionsResolverInterface;
use Symfony\Component\Form\FormBuilderInterface;
use Co\QuoteBundle\Form\DataTransformer\SailCollectionToStringsTransformer;

class PartsTypeStep1 extends AbstractType {

public function setDefaultOptions(OptionsResolverInterface $resolver)
{   
    $resolver-&gt;setDefaults(array(
        'data_class' =&gt; 'Co\QuoteBundle\Entity\Parts',));

    $resolver-&gt;setRequired(array('sailsAvailable', 'em'));
}

public function buildForm(FormBuilderInterface $formBuilder, array $options)
{        
    $transformer = new SailCollectionToStringsTransformer($options['em']);

    $formBuilder-&gt;add(
        $formBuilder-&gt;create('mainsailparts', 'entity', array(
            'class' =&gt; 'CoQuoteBundle:Mainsail',
            'choices' =&gt; $options['sailsAvailable']['mains'],
            'multiple' =&gt; true,
            'expanded' =&gt; true,
            'label' =&gt; 'Mainsails',))
       -&gt;addModelTransformer($transformer)); //line 58
}

public function getName() {
    return 'partsStep1';
}
}
</code></pre>

<p>The above works with no errors, but does not display the transformed data. The view is:</p>

<pre><code>__ Race main
__ Cruising main
</code></pre>

<p>(<code>__</code> stands for checkbox)</p>

<p>However, the view I want is:</p>

<pre><code>__ Race main ($1400)
__ Cruising main ($800)
</code></pre>

<p>The transformer I have is:</p>

<pre><code>use Symfony\Component\Form\DataTransformerInterface;
use Symfony\Component\Form\Exception\TransformationFailedException;
use Doctrine\Common\Persistence\ObjectManager;
use Co\QuoteBundle\Entity\Sail;
use Doctrine\Common\Collections\ArrayCollection;

class SailCollectionToStringsTransformer implements DataTransformerInterface
{
/**
 * @var ObjectManager
 */
private $om;

/**
 * @param ObjectManager $om
 */
public function __construct(ObjectManager $om)
{
    $this-&gt;om = $om;
}

/**
 * Transforms a collection of sails to a collection of strings.
 * @param  ISail|null $sail
 * @return string
 */
public function transform($sailCollection)
{
    if (null === $sailCollection) {
        return null;
    }

    $labels = new ArrayCollection();

    foreach($sailCollection as $sail){
        $labels[] = $sail-&gt;getName().' ($'.$sail-&gt;getBuildPrice().')';
    }

    return $labels;
}

//reverse transformer... not the issue (yet) because the forward transformer doesn't work

}
</code></pre>

<p>When running this through the netbeans debugger, an empty array is passed to the transformer. However, if I change line 58 to <code>-&gt;addViewTransformer($transformer));</code> and debug, it correctly passes two booleans with the sail id's as the array keys to the transformer. Unfortunately, I can't use the <code>ViewTransformer</code> because that no longer contains the original strings to change.</p>

<p>Why does the ArrayCollection that should contain the main sails get passed to the transformer as an empty ArrayCollection? The function returns an empty <code>$labels</code> collection.</p>

<p>I'm not sure what I am doing wrong... Help is much appreciated!!!!
Thanks.</p>
","transformer-model"
"16788479","How to merge two XML files have the same parameters?","2013-05-28 09:26:58","","1","9635","<java><xml><dom><jaxb><transformer-model>","<p>I want to merge two XML files(source file &amp; temp file) and put the resulted file in the source file and both source and temp files have the same elements but with different values like :</p>

<p><strong>Source.xml:</strong></p>

<pre><code>&lt;Main&gt;
   &lt;source&gt;
        &lt;param&gt;
            &lt;entry&gt;
                &lt;key&gt; bla1 &lt;/key&gt;
                &lt;value&gt; bla1 &lt;/value&gt;
            &lt;/entry&gt; 
        &lt;/param&gt;
        &lt;Name&gt; name1 &lt;/Name&gt;
   &lt;/Source&gt; 
&lt;/Main&gt;
</code></pre>

<p><strong>And temp.xml:</strong></p>

<pre><code>&lt;Main&gt;
   &lt;source&gt;
        &lt;param&gt;
           &lt;entry&gt;
               &lt;key&gt; bla2 &lt;/key&gt;
               &lt;value&gt; bla2 &lt;/value&gt;
           &lt;/entry&gt;
           &lt;entry&gt;
               &lt;key&gt; bla3 &lt;/key&gt;
               &lt;value&gt; bla3 &lt;/value&gt;
           &lt;/entry&gt;  
        &lt;/param&gt;
        &lt;Name&gt; name2 &lt;/Name&gt;
   &lt;/Source&gt; 
&lt;/Main&gt;
</code></pre>

<p><strong>And the desired output i want it like :</strong></p>

<pre><code>&lt;Main&gt;
  &lt;source&gt;
        &lt;param&gt;
            &lt;entry&gt;
                &lt;key&gt; bla1 &lt;/key&gt;
                &lt;value&gt; bla1 &lt;/value&gt;
            &lt;/entry&gt; 
        &lt;/param&gt;
        &lt;Name&gt; name1 &lt;/Name&gt;
   &lt;/Source&gt; 
   &lt;source&gt;
        &lt;param&gt;

           &lt;entry&gt;
               &lt;key&gt; bla2 &lt;/key&gt;
               &lt;value&gt; bla2 &lt;/value&gt;
           &lt;/entry&gt;
           &lt;entry&gt;
               &lt;key&gt; bla3 &lt;/key&gt;
               &lt;value&gt; bla3 &lt;/value&gt;
           &lt;/entry&gt;  
        &lt;/param&gt;
        &lt;Name&gt; name2 &lt;/Name&gt;
   &lt;/Source&gt; 
&lt;/Main&gt;
</code></pre>

<p>I'm using this code but it does't affect the <strong>source.xml</strong> at all :</p>

<pre><code>import java.io.BufferedWriter;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import java.io.StringWriter;
import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.parsers.ParserConfigurationException;
import javax.xml.transform.OutputKeys;
import javax.xml.transform.Transformer;
import javax.xml.transform.TransformerException;
import javax.xml.transform.TransformerFactory;
import javax.xml.transform.dom.DOMSource;
import javax.xml.transform.stream.StreamResult;
import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.w3c.dom.Node;
import org.w3c.dom.NodeList;
import org.xml.sax.SAXException;

public class MergeXml {

    private static final String fileName = ""Source.xml"";
    private static final String tempName = ""temp.xml"";
    private static final String mainTag = ""XmlSource"";
    private static final String YES = ""yes"";


    public void mergeXML() throws ParserConfigurationException, SAXException,
            IOException, TransformerException {

        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();
        DocumentBuilder db = null;
        Document doc = null;
        Document doc2 = null;

        db = dbf.newDocumentBuilder();
        doc = db.parse(new File(fileName));
        doc2 = db.parse(new File(tempName));
        Element tag = doc.createElement(mainTag);

        NodeList nodeList = doc2.getElementsByTagName(""*"");

        for(int i =0 ; i &lt; nodeList.getLength(); i++){

            Node node = nodeList.item(i);
            if (node.getNodeType() == Node.ELEMENT_NODE) {
                String nodeName = node.getNodeName();
                Element tagChild = doc.createElement((nodeName));

                tag.appendChild(tagChild);
            }
        }

        TransformerFactory tFactory = TransformerFactory.newInstance();
        Transformer transformer = tFactory.newTransformer();
        transformer.setOutputProperty(OutputKeys.INDENT, YES);

        DOMSource source = new DOMSource(doc);
        StreamResult result = new StreamResult(new StringWriter());
        transformer.transform(source, result);

        BufferedWriter output = new BufferedWriter(new FileWriter(fileName));
        String xmlOutput = result.getWriter().toString();
        output.write(xmlOutput);
        output.close();

    }
}
</code></pre>

<p><strong>My XML Original File if needed:</strong></p>

<pre><code>   &lt;XmlSource&gt;
         &lt;hostName&gt;api.worldweatheronline.com&lt;/hostName&gt;
         &lt;parameters&gt;
             &lt;entry&gt;
                 &lt;key&gt;num_of_days&lt;/key&gt;
                 &lt;value&gt;1&lt;/value&gt;
             &lt;/entry&gt;
             &lt;entry&gt;
                 &lt;key&gt;q&lt;/key&gt;
                 &lt;value&gt;Cairo&lt;/value&gt;
            &lt;/entry&gt;
            &lt;entry&gt;
                 &lt;key&gt;format&lt;/key&gt;
                 &lt;value&gt;xml&lt;/value&gt;
            &lt;/entry&gt;
            &lt;entry&gt;
                 &lt;key&gt;key&lt;/key&gt;
                 &lt;value&gt;wd63kxr294rcgvbynhaf2z4r&lt;/value&gt;
            &lt;/entry&gt;
       &lt;/parameters&gt;
       &lt;URL&gt;
        http://api.worldweatheronline.com/free/v1/weather.ashx?q=Cairo&amp;format=xml&amp;num_of_days=1&amp;key=wd63kxr294rcgvbynhaf2z4r
      &lt;/URL&gt;
      &lt;URLPath&gt;/free/v1/weather.ashx&lt;/URLPath&gt;
</code></pre>

<p></p>
","transformer-model"
"16648357","Unable to evaluate expression in XPath","2013-05-20 11:19:02","","5","20625","<java><xml><exception><xpath><transformer-model>","<p>I;m using XPath to parse XML document returned by a URL, when i run my code with given inputs it works but when giving it inputs as a user input it throws an exception.
<strong>The Code:</strong></p>

<pre><code>    class{
        private String generalQuery =  ""//@*"";
    method(){
        System.out.println(""Enter URL"");
        url = scan.nextLine();
        URL oracle = new URL(url);
        InputStream is = oracle.openStream();

        org.w3c.dom.Document doc = null;
        DocumentBuilderFactory domFactory;
        DocumentBuilder builder;

        try {
            domFactory = DocumentBuilderFactory.newInstance();
            domFactory.setNamespaceAware(true);
            builder = domFactory.newDocumentBuilder();
            doc = builder.parse(is);
        } catch (Exception ex) {
            System.err.println(""unable to load XML: "" + ex);
        }

    Map &lt;String, String&gt; params = new HashMap&lt;String, String&gt; ();

            XPathFactory factory = XPathFactory.newInstance();
            XPath xpath = factory.newXPath();
            xpath.setNamespaceContext(new NameSpaces(doc));
            XPathExpression expr = xpath.compile(generalQuery);
            Object result = expr.evaluate(doc, XPathConstants.NODESET); // exception thrown here

            NodeList nl = (NodeList) result;

            for (int i = 0 ; i &lt; nl.getLength() ; i++){
                Node n = (Node)nl.item(i);
                params.put(n.getNodeName(), n.getNodeValue());
            }

            return params;
}
}
</code></pre>

<p><strong>The Exception:</strong></p>

<pre><code>javax.xml.transform.TransformerException: Unable to evaluate expression using this context
</code></pre>

<p><strong>The class NameSpaces :</strong></p>

<pre><code>import java.util.Iterator;
import javax.xml.XMLConstants;
import javax.xml.namespace.NamespaceContext;
import org.w3c.dom.Document;

public class NameSpaces implements NamespaceContext {
    private Document sourceDocument;

    public NameSpaces(Document document) {
        sourceDocument = document;
    }

    @Override
    public String getNamespaceURI(String prefix) {
        if (prefix.equals(XMLConstants.DEFAULT_NS_PREFIX)) {
            return sourceDocument.lookupNamespaceURI(null);
        } else {
            return sourceDocument.lookupNamespaceURI(prefix);
        }
    }

    @Override
    public String getPrefix(String namespaceURI) {
        return sourceDocument.lookupPrefix(namespaceURI);
    }

    @Override
    public Iterator&lt;String&gt; getPrefixes(String namespaceURI) {
        return null;
    }
}
</code></pre>
","transformer-model"
"16641835","Strange XML indentation","2013-05-20 02:47:19","16651243","2","4316","<java><xml><dom><transformer-model>","<p>I'm writing an XML file, and the tabbing is coming out slightly wrong :</p>

<pre><code>&lt;BusinessEvents&gt;

&lt;MailEvent&gt;
          &lt;to&gt;Wellington&lt;/to&gt;
          &lt;weight&gt;10.0&lt;/weight&gt;
          &lt;priority&gt;air priority&lt;/priority&gt;
          &lt;volume&gt;10.0&lt;/volume&gt;
          &lt;from&gt;Christchurch&lt;/from&gt;
          &lt;day&gt;Mon May 20 14:30:08 NZST 2013&lt;/day&gt;
          &lt;PPW&gt;8.0&lt;/PPW&gt;
          &lt;PPV&gt;2.5&lt;/PPV&gt;
     &lt;/MailEvent&gt;
&lt;DiscontinueEvent&gt;
          &lt;to&gt;Wellington&lt;/to&gt;
          &lt;priority&gt;air priority&lt;/priority&gt;
          &lt;company&gt;Kiwi Co&lt;/company&gt;
          &lt;from&gt;Sydney&lt;/from&gt;
     &lt;/DiscontinueEvent&gt;
&lt;RoutePriceUpdateEvent&gt;
          &lt;weightcost&gt;3.0&lt;/weightcost&gt;
          &lt;to&gt;Wellington&lt;/to&gt;
          &lt;duration&gt;15.0&lt;/duration&gt;
          &lt;maxweight&gt;40.0&lt;/maxweight&gt;
          &lt;maxvolume&gt;20.0&lt;/maxvolume&gt;
          &lt;priority&gt;air priority&lt;/priority&gt;
          &lt;company&gt;Kiwi Co&lt;/company&gt;
          &lt;day&gt;Mon May 20 14:30:08 NZST 2013&lt;/day&gt;
          &lt;frequency&gt;3.0&lt;/frequency&gt;
          &lt;from&gt;Wellington&lt;/from&gt;
          &lt;volumecost&gt;2.0&lt;/volumecost&gt;
     &lt;/RoutePriceUpdateEvent&gt;
&lt;CustomerPriceUpdateEvent&gt;
          &lt;weightcost&gt;3.0&lt;/weightcost&gt;
          &lt;to&gt;Wellington&lt;/to&gt;
          &lt;priority&gt;air priority&lt;/priority&gt;
          &lt;from&gt;Sydney&lt;/from&gt;
          &lt;volumecost&gt;2.0&lt;/volumecost&gt;
     &lt;/CustomerPriceUpdateEvent&gt;
&lt;/BusinessEvents&gt;
</code></pre>

<p>As you can see, the first child node is not indented at all, but that nodes child is indented twice?
and then the close tag is only indented once?</p>

<p>I suspect it might have to do with adding the root not to the document through <code>doc.appendChild(root)</code>, but when I do that then I get an error </p>

<p><strong>""An attempt was made to insert a node where it is not permitted. ""</strong></p>

<p>Here is my parser:</p>

<pre><code>DocumentBuilderFactory icFactory = DocumentBuilderFactory.newInstance();
        DocumentBuilder icBuilder;
        try {
            icBuilder = icFactory.newDocumentBuilder();
            String businessEventsFile = System.getProperty(""user.dir"") + ""/testdata/businessevents/businessevents.xml"";
            Document doc = icBuilder.parse (businessEventsFile);

            Element root = doc.getDocumentElement();

            Element element;

            if(event instanceof CustomerPriceUpdateEvent){
                element = doc.createElement(""CustomerPriceUpdateEvent"");
            }
            else if(event instanceof DiscontinueEvent){
                element = doc.createElement(""DiscontinueEvent"");
            }
            else if(event instanceof MailEvent){
                element = doc.createElement(""MailEvent"");
            }
            else if(event instanceof RoutePriceUpdateEvent){
                element = doc.createElement(""RoutePriceUpdateEvent"");
            }
            else{
                throw new Exception(""business event isnt valid"");
            }

            for(Map.Entry&lt;String, String&gt; field : event.getFields().entrySet()){
                Element newElement = doc.createElement(field.getKey());
                newElement.appendChild(doc.createTextNode(field.getValue()));
                element.appendChild(newElement);
            }

            root.appendChild(element);


            // output DOM XML to console
            Transformer transformer = TransformerFactory.newInstance().newTransformer();
//            transformer.setOutputProperty(OutputKeys.METHOD, ""xml"");
            transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
            transformer.setOutputProperty(""{http://xml.apache.org/xslt}indent-amount"", ""5"");
            DOMSource source = new DOMSource(doc);
            StreamResult console = new StreamResult(businessEventsFile);
            transformer.transform(source, console);
</code></pre>

<p>Any insight would be appreciated.</p>
","transformer-model"
"16186990","adding content without delete original content in XML java","2013-04-24 08:23:52","","0","125","<java><xml><dom><transformer-model>","<p>I want to add new content inside a XML file, without delete the original content. How can I do that. </p>

<p>Original XML:</p>

<pre><code>&lt;collection&gt;
&lt;data&gt;
&lt;type&gt;CHD&lt;/type&gt;
&lt;values&gt;
&lt;entry key=""LOGICALCARDID""&gt;0000000&lt;/entry&gt;
&lt;entry key=""PRIMARYKEYVALUE""&gt;001490499026060000000&lt;/entry&gt;
&lt;entry key=""PRIMARYKEYOFFSET""&gt;53&lt;/entry&gt;
&lt;/values&gt;&lt;/data&gt;
&lt;data&gt;
&lt;type&gt;RECORD&lt;/type&gt;
&lt;values&gt;
&lt;entry key=""LOGICALCARDID""&gt;0000000&lt;/entry&gt;
&lt;entry key=""MAP_DP_EMV_SR""&gt;DDA_411&lt;/entry&gt;
&lt;entry key=""MAP_SK_SVC_PERSO_SR""&gt;9864591&lt;/entry&gt;
&lt;entry key=""MAP_DP_GRAPH_SR""&gt;GRAPH_PROFILE_1&lt;/entry&gt;
&lt;/values&gt;&lt;/data&gt;
&lt;data&gt;
&lt;type&gt;LOTPACK&lt;/type&gt;
&lt;values&gt;
&lt;entry key=""LOT_PACK_GROUP""&gt;1&lt;/entry&gt;
&lt;entry key=""GROUP_ID""&gt;GRP001&lt;/entry&gt;
&lt;entry key=""GROUP_INDEX""&gt;1&lt;/entry&gt;
&lt;entry key=""GROUP_QTY""&gt;3&lt;/entry&gt;
&lt;entry key=""LOTPACKTYPE""&gt;64&lt;/entry&gt;
&lt;/values&gt;&lt;/data&gt;
&lt;/collection&gt;
</code></pre>

<p>Java:</p>

<pre><code>public class program {

/**
 * @param args
 */
public static void main(String[] args) {
    try {
        String xmlFile = ""order.xml"";
        String xmlFile2 = ""order2.xml"";
        DocumentBuilderFactory docBuilderFactory = DocumentBuilderFactory
                .newInstance();
        DocumentBuilder docBuilder = docBuilderFactory.newDocumentBuilder();
        Document doc = docBuilder.parse(xmlFile);

        Node LOGICALCARDID = doc.getElementsByTagName(""entry"").item(0);
        Node PRIMARYKEYVALUE = doc.getElementsByTagName(""entry"").item(1);
        Node LOGICALCARDID1 = doc.getElementsByTagName(""entry"").item(3);

        for(int i = 1; i &lt; 20; i++){
            String number = Integer.toString(i);
            int length = number.length();
            if(length == 1){
                String digit = ""000000""+number;
                LOGICALCARDID.setTextContent(digit);
                PRIMARYKEYVALUE.setTextContent(""00149049902606""+digit);
                LOGICALCARDID1.setTextContent(digit);
            }else if (length == 2){
                String digit = ""00000""+number;
                LOGICALCARDID.setTextContent(digit);
                PRIMARYKEYVALUE.setTextContent(""00149049902606""+digit);
                LOGICALCARDID1.setTextContent(digit);
            }else if (length == 3){
                String digit = ""0000""+number;
                LOGICALCARDID.setTextContent(digit);
                PRIMARYKEYVALUE.setTextContent(""00149049902606""+digit);
                LOGICALCARDID1.setTextContent(digit);
            }else if (length == 4){
                String digit = ""000""+number;
                LOGICALCARDID.setTextContent(digit);
                PRIMARYKEYVALUE.setTextContent(""00149049902606""+digit);
                LOGICALCARDID1.setTextContent(digit);
            }

        }
    TransformerFactory transformerFactory = TransformerFactory.newInstance();
    Transformer transformer = transformerFactory.newTransformer();
    DOMSource source = new DOMSource(doc);
    StreamResult result = new StreamResult(new File(""order2.xml""));
    transformer.transform(source, result);

}catch (SAXParseException err) {
        System.out.println(""** Parsing error"" + "", line ""
                + err.getLineNumber() + "", uri "" + err.getSystemId());
        System.out.println("" "" + err.getMessage());

    } catch (SAXException e) {
        Exception x = e.getException();
        ((x == null) ? e : x).printStackTrace();

    } catch (Throwable t) {
        t.printStackTrace();
    }

}
</code></pre>

<p>Expected XML:</p>

<pre><code>&lt;collection&gt;
&lt;data&gt;
&lt;type&gt;CHD&lt;/type&gt;
&lt;values&gt;
&lt;entry key=""LOGICALCARDID""&gt;0000000&lt;/entry&gt;
&lt;entry key=""PRIMARYKEYVALUE""&gt;001490499026060000000&lt;/entry&gt;
&lt;entry key=""PRIMARYKEYOFFSET""&gt;53&lt;/entry&gt;
&lt;/values&gt;&lt;/data&gt;
&lt;data&gt;
&lt;type&gt;RECORD&lt;/type&gt;
&lt;values&gt;
&lt;entry key=""LOGICALCARDID""&gt;0000000&lt;/entry&gt;
&lt;entry key=""MAP_DP_EMV_SR""&gt;DDA_411&lt;/entry&gt;
&lt;entry key=""MAP_SK_SVC_PERSO_SR""&gt;9864591&lt;/entry&gt;
&lt;entry key=""MAP_DP_GRAPH_SR""&gt;GRAPH_PROFILE_1&lt;/entry&gt;
&lt;/values&gt;&lt;/data&gt;
&lt;data&gt;
&lt;type&gt;LOTPACK&lt;/type&gt;
&lt;values&gt;
&lt;entry key=""LOT_PACK_GROUP""&gt;1&lt;/entry&gt;
&lt;entry key=""GROUP_ID""&gt;GRP001&lt;/entry&gt;
&lt;entry key=""GROUP_INDEX""&gt;1&lt;/entry&gt;
&lt;entry key=""GROUP_QTY""&gt;3&lt;/entry&gt;
&lt;entry key=""LOTPACKTYPE""&gt;64&lt;/entry&gt;
&lt;/values&gt;&lt;/data&gt;
&lt;/collection&gt;

&lt;collection&gt;
&lt;data&gt;
&lt;type&gt;CHD&lt;/type&gt;
&lt;values&gt;
&lt;entry key=""LOGICALCARDID""&gt;0000001&lt;/entry&gt;
&lt;entry key=""PRIMARYKEYVALUE""&gt;001490499026060000001&lt;/entry&gt;
&lt;entry key=""PRIMARYKEYOFFSET""&gt;53&lt;/entry&gt;
&lt;/values&gt;&lt;/data&gt;
&lt;data&gt;
&lt;type&gt;RECORD&lt;/type&gt;
&lt;values&gt;
&lt;entry key=""LOGICALCARDID""&gt;0000001&lt;/entry&gt;
&lt;entry key=""MAP_DP_EMV_SR""&gt;DDA_411&lt;/entry&gt;
&lt;entry key=""MAP_SK_SVC_PERSO_SR""&gt;9864591&lt;/entry&gt;
&lt;entry key=""MAP_DP_GRAPH_SR""&gt;GRAPH_PROFILE_1&lt;/entry&gt;
&lt;/values&gt;&lt;/data&gt;
&lt;data&gt;
&lt;type&gt;LOTPACK&lt;/type&gt;
&lt;values&gt;
&lt;entry key=""LOT_PACK_GROUP""&gt;1&lt;/entry&gt;
&lt;entry key=""GROUP_ID""&gt;GRP001&lt;/entry&gt;
&lt;entry key=""GROUP_INDEX""&gt;1&lt;/entry&gt;
&lt;entry key=""GROUP_QTY""&gt;3&lt;/entry&gt;
&lt;entry key=""LOTPACKTYPE""&gt;64&lt;/entry&gt;
&lt;/values&gt;&lt;/data&gt;
&lt;/collection&gt;
</code></pre>

<p>Until 0000019.. But I dont know how to add the XML without delete original content. Anyone can please help me? </p>
","transformer-model"
"16123781","Racket with-hash macro and rename transformers","2013-04-20 18:30:47","16128241","4","393","<macros><scheme><racket><renaming><transformer-model>","<p>I created this:</p>

<pre><code>(define-syntax (with-hash stx)
  (syntax-parse stx
    [(_ obj:id ((~or key:id [new-key:id hash-key:id]) ...) body:expr ...+)
     #'(let ([key (hash-ref obj 'key)] ...
             [new-key (hash-ref obj 'hash-key)] ...)
         (begin body ...))]))
</code></pre>

<p>So that I can do this:</p>

<pre><code>  (require rackunit)
  (define h (hash 'id 1 'name ""scott""))
  (with-hash h (id [new-name name])
    (check-equal? id 1)
    (check-equal? new-name ""scott""))
</code></pre>

<p>How can I add an alternative pattern that automatically binds all the hash keys locally without the client specifying them in the call?</p>

<p>ie:</p>

<pre><code>(define h (hash 'id 1 'name ""scott""))
(with-hash h
  (check-equal? id 1)
  (check-equal? name ""scott""))
</code></pre>

<p>I suspect it involves renaming transformers, but am I able to declare syntax parameters and rename them dynamically, based on the runtime hash?  </p>

<p>Also, I thought something like this might be on the right track:</p>

<pre><code>(define-syntax (with-hash stx)
  (syntax-parse stx
    [(_ obj:id (key:id ...) body:expr ...+)
     #'(let ([key (hash-ref obj 'key)] ...)
         (begin body ...))]
    [(_ obj:id body:expr ...+)
     #'(with-hash obj (id title) body ...)]))
</code></pre>

<p>where I recall the macro and parse out the datums to be bound, but in that case, the id and title variables are not bound, even though the macro works otherwise.</p>

<p>Clearly I'm missing something in my understanding.</p>

<p>Any insights are appreciated.</p>

<p>Thanks.</p>
","transformer-model"
"16069519","TransformerFactoryConfigurationError when trying to use Saxon with a web service","2013-04-17 20:19:20","16069763","2","2104","<java><wso2><saxon><transformer-model><wso2-wsas>","<p>My web service, living on Axis2 inside a WSO2 WSAS application server, contains the following code:</p>

<pre><code>String prop = ""javax.xml.transform.TransformerFactory"";
String val = ""net.sf.saxon.TransformerFactoryImpl"";

public static TransformerFactory getTransformerFactory() {
    System.setProperty(prop, val); // This line executes normally
    return TransformerFactory.newInstance(); // Error occurs inside this call
}
</code></pre>

<p>When run, the service produces this error:</p>

<pre><code>[2013-04-15 12:22:41,263] ERROR
  {org.apache.axis2.transport.base.threads.NativeWorkerPool} -  Uncaught
  exception
javax.xml.transform.TransformerFactoryConfigurationError: Provider
  net.sf.saxon.TransformerFactoryImpl not found
        at javax.xml.transform.TransformerFactory.newInstance(Unknown Source)
        at com.example.Setup.getTransformerFactory(Setup.java:40)
        at com.example.ProcessFiles.init(ProcessFiles.java:336)
        at com.example.TorgApp.incoming(TorgApp.java:229)
        ...
</code></pre>

<p>I do have <code>Saxon-HE-9.4.jar</code> in my classpath, and it is readable. It is also available to Axis2 (<code>/foo/bar/wso2as/wso2as-4.0.0/tmp/axis2-tmp-6436419317930731973.tmp/axis67573163200472779114Saxon-HE-9.4.jar</code>).</p>

<p>According to <a href=""http://docs.oracle.com/javase/6/docs/api/javax/xml/transform/TransformerFactory.html#newInstance%28%29"" rel=""nofollow"">the Javadoc for <code>newInstance()</code></a>, the system property I'm setting should overrule all other methods of identifying which <code>TransformerFactory</code> to use, so precedence is not the issue.</p>

<p>What could be causing this error, and what can I do to get my desired Saxon transformer factory?</p>
","transformer-model"
"15598701","Spring Data Transformers.aliasToBean(AgentRecordDTO.class)","2013-03-24 12:54:49","","6","3497","<hibernate><spring-data><hibernate-criteria><transformer-model>","<p><strong>I want to use <a href=""http://docs.jboss.org/hibernate/orm/3.3/api/org/hibernate/transform/Transformers.html"" rel=""noreferrer"">Hibernate Transformation</a>  with Spring Data.</strong></p>

<p>I have an entity <code>AgentRecord</code> with attributes as </p>

<pre><code>@Entity
public class AgentRecord extends AbstractEntity&lt;Long&gt; {
        @ManyToOne
        private User processedBy;

        private String description;

        private RecordType recordType;

        private AgentRecordStatus status;
}
</code></pre>

<p>I am following the practice of setting required attributes to a different DTO called <code>AgentRecordDTO</code> and return it to Client-side(<code>gwt</code>).</p>

<pre><code>public class AgentRecordDTO implements IsSerializable {

    private long processedBy;

    private String description;

    private RecordType recordType;

    private AgentRecordStatus status;
}
</code></pre>

<p>Instead of fetching all attributes of an entity, I want to fetch few attributes and set them to <code>AgentRecordDTO</code> like that of <code>new AgentRecordDTO()</code> I can do in hql, BUT want to do with <strong>Spring Data Specification</strong>.</p>

<p><img src=""https://i.sstatic.net/zExJ3.png"" alt=""Transformation""></p>

<p>My <code>AgentRepository</code> is </p>

<pre><code>public interface AgentRecordRepository extends CrudRepository&lt;AgentRecord, Long&gt;, JpaSpecificationExecutor&lt;AgentRecord&gt; {

}
</code></pre>

<h2>My Transformation incomplete code looks like</h2>

<pre><code>public Page&lt;AgentRecordDTO&gt; getAgentRecords(final long userId) {
    SimplePageable page = new SimplePageable(1, 10); //my custom object
    Page&lt;AgentRecord&gt; pages = agentRecordRepository.findAll(new Specification&lt;AgentRecord&gt;() {

        @Override
        public Predicate toPredicate(Root&lt;AgentRecord&gt; root, CriteriaQuery&lt;?&gt; query, CriteriaBuilder cb) {
            //Projection plus Transformers.aliasToBean(AgentRecordDTO) missing here
            Predicate predicate = cb.equal(root.get(""processedBy"").get(""id""), userId);
            if (null != predicate) {
                predicate = cb.or(predicate, cb.equal(root.get(""recordType""), RecordType.VERIFICATION_REQUEST));
                predicate = cb.or(predicate, cb.equal(root.get(""recordType""), RecordType.VERIFICATION));
            }

            return predicate;
        }
    }, new PageRequest(page.getPage(), page.getSize(), new Sort(new Order(Direction.DESC, ""id""))));
    return null;
}
</code></pre>

<p><a href=""http://relation.to/Bloggers/Hibernate32TransformersForHQLAndSQL"" rel=""noreferrer"">Hibernate 3.2: Transformers for HQL and SQL</a> dated 03 Jun 2008 was the brilliant post, but <h2>I couldn't win over it in Spring Data Specification.</h2></p>
","transformer-model"
"15119571","Unit testing is not working with XSLT tansformer","2013-02-27 18:31:26","15120839","0","685","<java><xslt><junit><mule><transformer-model>","<p>I have written unit test cases to test the message processors individually in my mule flow.</p>

<p>But the unit test fails with error </p>

<pre><code>org.mule.api.transformer.TransformerMessagingException: Property ""xsl-file or xsl-text"" not set.  
One or more of them must be set (org.mule.api.lifecycle.InitialisationException).
 Message payload is of type: String    
 (org.mule.api.transformer.TransformerMessagingException). Message payload is of type: String
</code></pre>

<p>One of the transformers is an XSLT as shown below.</p>

<pre><code>&lt;mule-xml:xslt-transformer  maxIdleTransformers=""2"" maxActiveTransformers=""5""   xsl-file=""C:\EWS\myproj\src\main\resources\xslt\DataAdder.xsl""
         name=""AdderXSLT""   &gt;
    &lt;/mule-xml:xslt-transformer&gt;
</code></pre>

<p>The unit test method looks as below.</p>

<pre><code>    MessageProcessor subFlow = muleContext.getRegistry().lookupObject(""AdderXSLT"");
    MuleEvent result = subFlow.process(getTestEvent(getFileAsString(""SamplePayloads/input.xml"")));  

    System.out.println(""The output from Event is "" + result.getMessageAsString());
    System.out.println(""The converted XML is "" + result.getMessage().getPayloadAsString()); 

    assertNotNull(result);
    assertNull(result.getMessage().getExceptionPayload());
    assertFalse(result.getMessage().getPayload() instanceof NullPayload);
</code></pre>

<p>Please help me understand what's going wroong here.</p>
","transformer-model"
"14782777","Java XML Transformer replacing ""\n"" with a space","2013-02-08 23:05:05","14817313","1","621","<java><html><xml><transformer-model>","<p>I'm using a Java Transformer to convert XML into HTML. When I display the output, everywhere I had a newline string literal delimiter of ""\n"" in a field, there's simply a space. The only suggestion I've been able to find is: </p>

<pre><code>transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
</code></pre>

<p>which didn't work. I cannot find anything in the JavaDocs for the TransformerFactory, or Transformer classes that addresses this issue. Any suggestions?</p>
","transformer-model"
"14696352","Connecting Oracle 11g in Powerplay Transformer 7.3 via ODBC","2013-02-04 21:54:43","","-2","517","<oracle><cognos><transformer-model>","<p>I am using Cognos Powerplay Transformer 7.3 and wants to connect Oracle 11g through ODBC, but i can not see the ODBC as a data source options, the options i see are :
- Impromptu Query Definition
- Delimited-field text with column titles
- Delimeted-field text
- dBase tabel
etc.</p>
","transformer-model"
"14204428","Change DOM parser to not use short notation for empty nodes","2013-01-07 21:34:35","","1","1579","<java><xml><dom><transformer-model>","<p>Is there anyway to change DOM Parser Empty notation from short to long form?</p>

<p>I need</p>

<pre><code> &lt;book&gt;&lt;/book&gt;
</code></pre>

<p>instead of book </p>

<pre><code>&lt;book/&gt;
</code></pre>

<p>We have a third party XML reader which won't work with short notation.All my XML object are DOM. What is best way going about this?</p>

<pre><code>TransformerFactory transformerFactory = TransformerFactory.newInstance();
Transformer transformer = transformerFactory.newTransformer();
DOMSource source = new DOMSource(doc);
StreamResult result = new StreamResult(XMLFile);
transformer.transform(source, result);
</code></pre>

<p>Thanks</p>
","transformer-model"
"14193662","Transformer object appends namespace automatically to child elements","2013-01-07 10:06:21","14196109","1","488","<java><xml><saxparser><xmlreader><transformer-model>","<p>In order to make some changes on XML file, I use the code below:</p>

<pre><code>  public boolean run() throws Exception {
    XMLReader xr = new XMLFilterImpl(XMLReaderFactory.createXMLReader()) {

     public void startElement(String uri, String localName, String qName, Attributes atts) throws SAXException {
          if(AddRuleReq&amp;&amp; qName.equalsIgnoreCase(""cp:ruleset""))
           {
             attributeList.clear();
             attributeList.addAttribute(uri, localName, ""id"", ""int"", Integer.toString(getNewRuleId()));
             super.startElement(uri, localName, ""cp:rule"", attributeList);
             attributeList.clear();
             super.startElement(uri, localName, ""cp:conditions"", attributeList);
             super.startElement(uri, localName, ""SOMECONDITION"", attributeList);
             super.endElement(uri, localName, ""SOMECONDITION"");
             super.endElement(uri, localName, ""cp:conditions"");
             super.startElement(uri, localName, ""cp:actions"", attributeList);
             super.startElement(uri, localName, ""allow"", attributeList);
             super.characters(BooleanVariable.toCharArray(), 0, BooleanVariable.length());
             super.endElement(uri, localName, ""allow"");
             super.endElement(uri, localName, ""cp:actions"");
           }
      }
};
  Source source = new SAXSource(xr, new InputSource(new StringReader(xmlString)));
  stringWriter = new StringWriter();
  StreamResult result = new StreamResult(stringWriter);
  Transformer transformer = TransformerFactory.newInstance().newTransformer();
  transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""yes"");
  transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
  transformer.setOutputProperty(""{http://xml.apache.org/xslt}indent-amount"", ""3"");
  transformer.transform(source, result);
  return stringWriter.toString();
}
</code></pre>

<p>I have pasted a little part of it and it works. But with little differences.</p>

<p>What I expect to see is:</p>

<pre><code>&lt;cp:rule id=""1""&gt;
  &lt;cp:conditions&gt;
    &lt;SOMECONDITION/&gt;
  &lt;/cp:conditions&gt;
  &lt;cp:actions&gt;
  &lt;allow&gt;
    true
  &lt;/allow&gt;
  &lt;/cp:actions&gt;
&lt;/cp:rule&gt;
</code></pre>

<p>What I see is:</p>

<pre><code>&lt;cp:rule id=""1""&gt;
  &lt;cp:conditions&gt;
    &lt;SOMECONDITION xmlns=""urn:ietf:params:xml:ns:common-policy""/&gt;
  &lt;/cp:conditions&gt;
  &lt;cp:actions&gt;
  &lt;allow xmlns=""urn:ietf:params:xml:ns:common-policy""&gt;
    true
  &lt;/allow&gt;
  &lt;/cp:actions&gt;
&lt;/cp:rule&gt;
</code></pre>

<p>The processed XML is also invalid according to my schema and unusable for the next time. </p>

<p>My question is that, how can I prevent this namespaces (as in this example, &lt; SOMECONDITION xmlns=""urn:ietf:params:xml:ns:common-policy""/>) being added to child elements?</p>

<p>Thanks in advance..</p>
","transformer-model"
"14137229","Parsing Mixed Markup with MOXy, maybe using Transformers","2013-01-03 10:41:38","14137518","2","324","<jaxb><eclipselink><moxy><transformer-model>","<p>Using MOXy 2.3.1, but could upgrade if it would help.</p>

<p>I have the following XML:</p>

<pre><code>&lt;myelement&gt;
text content &lt;b&gt;mixed&lt;/b&gt; with tags
&lt;/myelement&gt;
</code></pre>

<p>Which I would like to be stored in a <strong>String</strong> field containing:</p>

<pre><code>text content &lt;b&gt;mixed&lt;/b&gt; with tags
</code></pre>

<p>I've been going on the idea I'd need to use the XML Transformation feature within MOXy, and my code looks like this:</p>

<pre><code>// Not a root element, it's own mapping annotation
// is defined in another class.
@XmlAccessorType(XmlAccessType.NONE)
class MyElement {

    @XmlTransformation
    @XmlReadTransformer(transformerClass=TempTrans.class)
    @XmlValue
    String markup
}

public class TempTrans implements AttributeTransformer {

    private AbstractTransformationMapping mapping;

    public void initialize(AbstractTransformationMapping mapping) {
        this.mapping = mapping;
    }

    public Object buildAttributeValue(Record record, Object instance, Session session) {
        return null;
    }
}
</code></pre>

<p>I've been debugging on the <strong>return null;</strong> line to see what's available to me in the <strong>mapping</strong> object. I haven't found it very useful, am I on the right track?</p>

<p>I'm hoping for a mechanism similar to XStream's <strong>HierarchicalStreamReader</strong>, something to give me DOM-like access to the source XML. Any workaround would be greatly appreciated.</p>

<p>I wouldn't mind ending up with:</p>

<pre><code>text content &amp;lt;b&amp;gt;mixed&amp;lt;/b&amp;gt; with tags
</code></pre>

<p>But the source XML would need to have unescaped markup in it.</p>

<p>Thanks, Mike</p>
","transformer-model"
"13656132","Java XML file fail to write","2012-12-01 03:41:38","13656202","4","7605","<java><xml><file><nullpointerexception><transformer-model>","<p>Okay, I have a class createUser which is supposed to create an XML file to store data on a user. Problem is when I run it I'm getting this error</p>

<blockquote>
<pre><code>&gt;        ERROR:  ''
&gt;     javax.xml.transform.TransformerException: java.lang.NullPointerException
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(Unknown
&gt; Source)
&gt;         at CreateUser.makeUser(CreateUser.java:156)
&gt;         at Welcomeclass.welcome(Welcomeclass.java:48)
&gt;         at Welcomeclass.main(Welcomeclass.java:32)
&gt;     Caused by: java.lang.NullPointerException
&gt;         at com.sun.org.apache.xml.internal.serializer.ToUnknownStream.characters(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transformIdentity(Unknown
&gt; Source)
&gt;         ... 5 more
&gt;     ---------
&gt;     java.lang.NullPointerException
&gt;         at com.sun.org.apache.xml.internal.serializer.ToUnknownStream.characters(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transformIdentity(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(Unknown
&gt; Source)
&gt;         at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(Unknown
&gt; Source)
&gt;         at CreateUser.makeUser(CreateUser.java:156)
&gt;         at Welcomeclass.welcome(Welcomeclass.java:48)
&gt;         at Welcomeclass.main(Welcomeclass.java:32)
</code></pre>
</blockquote>

<p>which means it is incapable of transforming my doc into an xml file.</p>

<p>Here is the code it is from.</p>

<pre><code>/*imports*/
import java.util.Scanner;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;

import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.parsers.ParserConfigurationException;
import javax.xml.transform.Transformer;
import javax.xml.transform.TransformerException;
import javax.xml.transform.TransformerFactory;
import javax.xml.transform.dom.DOMSource;
import javax.xml.transform.stream.StreamResult;

import org.w3c.dom.Attr;
import org.w3c.dom.Document;
import org.w3c.dom.Element;
import org.w3c.dom.NodeList;
import org.xml.sax.SAXException;
/*A class to create a user object and store it in a XML file for later retrieval
public class CreateUser {   
    static Scanner input = new Scanner(System.in);

    /*objects note: must be strings due to being stored in XML table*/
    static String name;
    static String age;
    static String bday;
    static String gender;
    static String location;
    static String orientation;
    static String relationship;
    static String hobbies;
    static String choice;
    static String username;
    static String password;

    static String fileLocation = ""C:/Users/Steven/Workspace/twitter/src/users.xml"";

    int count = 0;
    int maxId = 0;
    static int nextId  = 0;

    public static void makeUser() throws SAXException, IOException {
        /*gets user input to fill String objects*/
        System.out.println(""Hello, to register we will need some information about you."");
        System.out.println(""What is your name?"");
        name = input.nextLine();
        System.out.println(""How old are you(e.g. 45)?"");
        age = input.nextLine();
        System.out.println(""When is your birthday(MM/DD/YYYY)?"");
        bday = input.nextLine();
        System.out.println(""What is your gender?"");
        gender = input.nextLine();
        System.out.println(""Where do you live?"");
        location = input.nextLine();
        System.out.println(""What is your orientation?"");
        orientation = input.nextLine();
        System.out.println(""Are you in a relationship? (y/n)"");
        choice = input.nextLine();
        if(choice.equals(""y""))
            relationship = ""In a relationship."";
        if(choice.equals(""y""))  
            relationship = ""Single."";
        System.out.println(""What are your hobbies?"");
        hobbies = input.nextLine();
        System.out.println(""What will be your username?"");
        username = input.nextLine();
        System.out.println(""What will be your password?"");
        password = input.nextLine();    

        /*create XML file to store the data*/
        try{
            DocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance();
            DocumentBuilder docBuilder = docFactory.newDocumentBuilder();
            Document userslist = docBuilder.newDocument();
            /*create user element*/
            Element users = userslist.createElement(""users"");
            userslist.appendChild(users);

            Element user = userslist.createElement(""user"");
            users.appendChild(user);

            /*get the max id to set the next id if the file exists*/
            File xmlFile = new File(fileLocation);
            if(xmlFile.exists())
            {
                DocumentBuilderFactory dbFactory = DocumentBuilderFactory.newInstance();
                DocumentBuilder dBuilder = dbFactory.newDocumentBuilder();
                Document idgetter = dBuilder.parse(xmlFile);
                idgetter.getDocumentElement().normalize();
                NodeList nodes = idgetter.getElementsByTagName(""id"");
                int maxId = 0;
                for(int i = 0; i &lt; nodes.getLength(); i++){
                    if(Integer.parseInt(nodes.item(i).getTextContent()) &gt; maxId )
                    {
                        maxId = Integer.parseInt(nodes.item(i).getTextContent());
                    }
                }
                nextId = maxId +1;
            }
            /*else create the file*/
            else
            {
                /*create the id attribute*/
                Attr attr = userslist.createAttribute(""id"");
                attr.setValue(String.valueOf(nextId));
                user.setAttributeNode(attr);

                /*fill in doc with objects*/
                Element dname = userslist.createElement(""name"");
                dname.appendChild(userslist.createTextNode(name));
                user.appendChild(dname);
                Element dgender = userslist.createElement(""gender"");
                dgender.appendChild(userslist.createTextNode(gender));
                user.appendChild(dgender);
                Element dlocation = userslist.createElement(""location"");
                dlocation.appendChild(userslist.createTextNode(location));
                user.appendChild(dlocation);
                Element dorientation = userslist.createElement(""orientation"");
                dorientation.appendChild(userslist.createTextNode(orientation));
                user.appendChild(dorientation);
                Element drelationship = userslist.createElement(""relationship"");
                drelationship.appendChild(userslist.createTextNode(relationship));
                user.appendChild(drelationship);
                Element dhobbies = userslist.createElement(""hobbies"");
                dhobbies.appendChild(userslist.createTextNode(hobbies));
                user.appendChild(dhobbies);
                Element dchoice = userslist.createElement(""choice"");
                dchoice.appendChild(userslist.createTextNode(choice));
                user.appendChild(dchoice);
                Element dusername = userslist.createElement(""username"");
                dusername.appendChild(userslist.createTextNode(username));
                user.appendChild(dusername);
                Element dpassword = userslist.createElement(""password"");
                dpassword.appendChild(userslist.createTextNode(password));
                user.appendChild(dpassword);
                Element dbday = userslist.createElement(""birthday"");
                dbday.appendChild(userslist.createTextNode(bday));
                user.appendChild(dbday);
                Element dage = userslist.createElement(""age"");
                dage.appendChild(userslist.createTextNode(age));
                user.appendChild(dage);

                /*transfer document to XML*/
                TransformerFactory transformerFactory = TransformerFactory.newInstance();
                Transformer transformer = transformerFactory.newTransformer();
                DOMSource source = new DOMSource(users);

                /*create the document in append mode */
                //StreamResult result = new StreamResult(new FileWriter(fileLocation, true));
                StreamResult result = new StreamResult(System.out);

                transformer.transform(source, result);
            }
        } catch (ParserConfigurationException pce) {
            pce.printStackTrace();
        } catch (TransformerException tfe) {
            tfe.printStackTrace();
        }
    }
}
</code></pre>

<p>If you don't want to take the time to trouble shoot it yourself or look it over that's fine but if you have and idea about how to troubleshoot transformer issues that would be fantastic. Because I'm having a hard time figuring out exactly what is causing this issue.</p>
","transformer-model"
"13592539","Transformer Java XML CDATA Duplicate Lines","2012-11-27 20:32:42","","1","437","<java><xml><transformer-model>","<p>I am using transformer in JDK 7 to write out some XML and I have in the CDATA a freemarker template as follows:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;fileExport append=""false"" automaticExport=""false"" automaticExportWithErrors=""true"" exportDir=""C:\Users\bÃ¤nn\Desktop"" exportFileName=""&amp;lt;date&amp;gt; &amp;lt;time&amp;gt; &amp;lt;barcode&amp;gt;"" exportType=""Excel"" splitFiles=""false""&gt;
  &lt;exportTemplate&gt;&lt;![CDATA[Date,RackBarcode,Row,Col,tubeBarcode,OrientationBarcode
&lt;#list racks as rack&gt;
&lt;#list rack.containers as container&gt;
${scandate?datetime},${rack.barcode},${container.textRow},${container.col + 1},${container.barcode},${rack.orientationBarcode}
&lt;/#list&gt;
&lt;/#list&gt;]]&gt;&lt;/exportTemplate&gt;
  &lt;scanTimeQuestionsTemplate&gt;&lt;![CDATA[&lt;#if scanTimeQuestionsEnabled&gt;
Scan Time Questions

&lt;#list scanTimeQuestions as question&gt;
${question.shortName} : ${question.answer}
&lt;/#list&gt;

&lt;/#if&gt;]]&gt;&lt;/scanTimeQuestionsTemplate&gt;
  &lt;excelExportTemplate writeHeader=""true""&gt;
    &lt;excelExportColumn columnDataFormatting=""d/M/yyyy HH:mm"" columnDataFromString=""DATETIME"" columnHeader=""Scan Time""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""RACK_BARCODE"" columnHeader=""Rack Barcode""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""ORIENTATION_BARCODE"" columnHeader=""Orientation Barcode""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""TUBE_ROW_TEXT"" columnHeader=""Tube Row""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""TUBE_COLUMN"" columnHeader=""Tube Column""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""TUBE_BARCODE"" columnHeader=""Tube Barcode""/&gt;
    &lt;excelExportColumn columnDataFormatting=""d/M/yyy HH:mm:ss"" columnDataFromString=""DATETIME"" columnHeader=""Scan Datetime""/&gt;
  &lt;/excelExportTemplate&gt;
&lt;/fileExport&gt;


&lt;/#if&gt;

]]&gt;&lt;/scanTimeQuestionsTemplate&gt;
  &lt;excelExportTemplate writeHeader=""false""&gt;
    &lt;excelExportColumn columnDataFormatting=""d/M/yyyy HH:mm"" columnDataFromString=""DATETIME"" columnHeader=""Scan Time""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""RACK_BARCODE"" columnHeader=""Rack Barcode""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""ORIENTATION_BARCODE"" columnHeader=""Orientation Barcode""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""TUBE_ROW_TEXT"" columnHeader=""Tube Row""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""TUBE_COLUMN"" columnHeader=""Tube Column""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""TUBE_BARCODE"" columnHeader=""Tube Barcode""/&gt;
  &lt;/excelExportTemplate&gt;
&lt;/fileExport&gt;
</code></pre>

<p>However when I write this out on a windows computer the transformer is adding an extra \r into the outputted CDATA so I have:</p>

<pre><code>&lt;?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?&gt;
&lt;fileExport append=""false"" automaticExport=""false"" automaticExportWithErrors=""false"" exportDir=""C:\Users\benn\Desktop"" exportFileName="""" exportType=""text"" splitFiles=""false""&gt;
  &lt;exportTemplate&gt;&lt;![CDATA[Date,RackBarcode,Row,Col,tubeBarcode,OrientationBarcode

&lt;#list racks as rack&gt;

&lt;#list rack.containers as container&gt;

${scandate?datetime},${rack.barcode},${container.textRow},${container.col + 1},${container.barcode},${rack.orientationBarcode}

&lt;/#list&gt;

&lt;/#list&gt;

]]&gt;&lt;/exportTemplate&gt;
  &lt;scanTimeQuestionsTemplate&gt;&lt;![CDATA[&lt;#if scanTimeQuestionsEnabled&gt;

Scan Time Questions



&lt;#list scanTimeQuestions as question&gt;

${question.shortName} : ${question.answer}

&lt;/#list&gt;



&lt;/#if&gt;

]]&gt;&lt;/scanTimeQuestionsTemplate&gt;
  &lt;excelExportTemplate writeHeader=""false""&gt;
    &lt;excelExportColumn columnDataFormatting=""d/M/yyyy HH:mm"" columnDataFromString=""DATETIME"" columnHeader=""Scan Time""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""RACK_BARCODE"" columnHeader=""Rack Barcode""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""ORIENTATION_BARCODE"" columnHeader=""Orientation Barcode""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""TUBE_ROW_TEXT"" columnHeader=""Tube Row""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""TUBE_COLUMN"" columnHeader=""Tube Column""/&gt;
    &lt;excelExportColumn columnDataFormatting="""" columnDataFromString=""TUBE_BARCODE"" columnHeader=""Tube Barcode""/&gt;
  &lt;/excelExportTemplate&gt;
&lt;/fileExport&gt;
</code></pre>

<p>So basically \r\n is becoming \r\r\n - I've seen some messages about Xalan doing this but I'm using whatever JDK 7 provides as the back engine; has anybody seen this and is there a property I can set to stop transformer putting an extra line break in my CDATA></p>
","transformer-model"
"12846462","Mule ESB JAXB XML To Object Transformer Better Way?","2012-10-11 18:47:30","12846654","1","4450","<jaxb><esb><mule><transformer-model>","<p>Mule 3.3 can automatically unmarshall an XML string to an object using JAXB given that:<br>
    1. you first register your jaxb annotated classes with spring.
    2. there is a component that requires such type as input</p>

<p>So I have managed to do the transformation, but I had to create a ""DumbTransformer"" that does nothing.  It has a method that returns the same object it receives.  I need it in order to trigger the XML to Object conversion so that I can further process the message.</p>

<p>Flow Example:</p>

<pre><code>&lt;spring:beans&gt;
    &lt;spring:bean id=""dumbTransformer"" class=""foo.bar.DumbTransformer""/&gt;
&lt;/spring:beans&gt;

&lt;flow name=""main"" doc:name=""main""&gt;
    &lt;vm:inbound-endpoint path=""in"" doc:name=""VM"" /&gt;
        &lt;component doc:name=""Java""&gt;
            &lt;spring-object bean=""dumbTransformer""/&gt;
        &lt;/component&gt;
        &lt;splitter expression=""#[payload.items]"" doc:name=""Split Items""/&gt;
    &lt;logger message=""#[payload]"" level=""INFO"" doc:name=""Log Item""/&gt;
    &lt;vm:outbound-endpoint path=""out"" doc:name=""VM"" /&gt;
&lt;/flow&gt;
</code></pre>

<p>DumbTransformer.java</p>

<pre><code>package foo.bar;

@ContainsTransformerMethods
public class InvoiceUnmarshaller extends AbstractTransformer {

    @Transformer
    public MyJaxbAnnotatedClass foo(@Payload MyJaxbAnnotatedClass i) {
        return i;
    }

}
</code></pre>

<p>Is there a way to acomplish this without having to create such DumbTransformers?</p>

<p>Thanks.</p>
","transformer-model"
"12783991","Java XML Transformer : Empty elements in long notation instead of short","2012-10-08 14:40:06","12784204","0","1423","<java><xml><transformer-model>","<p>I have created a conversion tool to add some information to an existing xml file.
This is done by using DOM and the Transformer class.
The output file will be processed by third party software.
This TPS needs the empty tags from the input and outputfile in Long Notation.</p>

<p>Unfortunately, transformer class always change them to short notation.
Is there a way to prevent this from happenning?</p>

<p>I have been searching various sites, but haven't found a solution that really fits my needs.</p>

<p>Please help,
Thanks,
Kind regards,
Maarten</p>
","transformer-model"
"12555409","Transformer#transform(Source, Result) throws FileNotFoundException?","2012-09-23 19:13:56","","0","1226","<java><xml><filenotfoundexception><transformer-model>","<p>I am trying to output my XML Document using <code>Transformer.transform(Source, Result)</code>, but it does not export the Document, only throws a <code>FileNotFoundException</code>. I am trying to export it to <code>/WebContent/samples/users.xml</code> in my project with Apache Tomcat. After some work on Google, I found <a href=""https://bugs.java.com/bugdatabase/view_bug?bug_id=5077403"" rel=""nofollow noreferrer"">this</a> page on the supposedly &quot;bug&quot; on <a href=""http://bugs.sun.com"" rel=""nofollow noreferrer"">bugs.sun.com</a>.</p>
<p>Here is a selection of my code:</p>
<pre><code>Document doc = newDocument();
Element rootElement = doc.createElement(&quot;homeworkManager&quot;);

Element sampleUserElement = doc.createElement(&quot;user&quot;);
// (set username and password)

Element sampleHomeworkElement1 = doc.createElement(&quot;homework&quot;);
// (set homework attributes)

sampleUserElement.appendChild(sampleHomeworkElement1);
rootElement.appendChild(sampleUserElement);
doc.appendChild(rootElement);

// HomeworkManager.getRealPath(String) is the same as  HttpSession.getServletContext().getRealPath(String)
File output = new File(HomeworkManager.instance().getRealPath(&quot;/&quot;) + &quot;/samples/users.xml&quot;);

        
Transformer transformer = TransformerFactory.newInstance().newTransformer();
transformer.setOutputProperty(OutputKeys.INDENT, &quot;yes&quot;);
transformer.setOutputProperty(&quot;{http://xml.apache.org/xslt}indent-amount&quot;, &quot;2&quot;);

transformer.transform(new DOMSource(doc), new StreamResult(output));  // EXCEPTION THROWN HERE
</code></pre>
<br>
<br>
Here is the Stack Trace:
<pre><code>javax.xml.transform.TransformerException: java.io.FileNotFoundException: ...\workspace\.metadata\.plugins\org.eclipse.wst.server.core\tmp0\wtpwebapps\Homework Manager\samples\users.xml (The system cannot find the path specified)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.getOutputHandler(TransformerImpl.java:493)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:302)
at net.dean.homeworkmanager.UserManager.createSampleXMLFile(UserManager.java:215)
at org.apache.jsp.index_jsp._jspService(index_jsp.java:91)
at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:722)
at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:432)
at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:390)
at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:334)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:722)
at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)
at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)
at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)
at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)
at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:168)
at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)
at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:929)
at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)
at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)
at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1002)
at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:585)
at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:312)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:722)

Caused by: java.io.FileNotFoundException: (...)\workspace\.metadata\.plugins\org.eclipse.wst.server.core\tmp0\wtpwebapps\Homework Manager\samples\users.xml (The system cannot find the path specified)
at java.io.FileOutputStream.open(Native Method)
at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:212)
at java.io.FileOutputStream.&lt;init&gt;(FileOutputStream.java:104)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.getOutputHandler(TransformerImpl.java:466)
... 25 more
</code></pre>
<p>As you can tell, I am very confused. Any suggestions would be much appreciated.</p>
<p><strong>EDIT</strong>: This has only occurred on Web Projects. I can use the exact same code to export my Document on a desktop application.</p>
<p><strong>EDIT 2</strong>: Even when I have created the directory <em>and</em> <code>users.xml</code>, the file is not modified. It is instead created in <code>(...)\workspace\.metadata\.plugins\org.eclipse.wst.server.core\tmp0\wtpwebapps\Homework Manager\samples\users.xml</code></p>
","transformer-model"
"12041429","pass <xsl:param> to <a href>","2012-08-20 16:23:45","12041663","1","705","<java><xslt><transform><href><transformer-model>","<p>I try to pass parameter </p>

<pre><code>&lt;xsl:param name=""current_item"" /&gt;
</code></pre>

<p>Before that I sent parameter as <code>transformer.setParameter(""current_item"", ""Ball"");</code></p>

<p>But when I try to set this parameter as parameter </p>

<pre><code>&lt;a href=""controller?command=transform&amp;amp;current_item={$current_item}""&gt;&lt;/a&gt;
</code></pre>

<p>In this row I get error  Variable or parameter 'current_item' is undefined.'
How I should specify this variable?</p>
","transformer-model"
"11922748","Vertex painter encounter error when loading vertex and edges from database","2012-08-12 14:03:11","11922784","0","323","<java><render><jung><vertex><transformer-model>","<p>Im building simple family tree application and using Jung to draw the chart. So far I am able to draw and save the chart into embedded database using sqlitejdbc from zentus. I've yet create the function to clear the canvas though, so I have to restart the application to test the opening the database. But when I want to load them back, the application just froze and  I encountered this error.</p>

<blockquote>
<pre><code>  path:C:\Users\PaLi\Desktop\psm things\chart4.db
    Exception in thread ""AWT-EventQueue-0"" java.lang.NullPointerException
at FamTree.vertexPainter.transform(vertexPainter.java:20)
at FamTree.vertexPainter.transform(vertexPainter.java:1)
at edu.uci.ics.jung.visualization.renderers.BasicVertexRenderer.paintShapeForVertex(BasicVertexRenderer.java:98)
at edu.uci.ics.jung.visualization.renderers.BasicVertexRenderer.paintIconForVertex(BasicVertexRenderer.java:74)
at edu.uci.ics.jung.visualization.renderers.BasicVertexRenderer.paintVertex(BasicVertexRenderer.java:37)
at edu.uci.ics.jung.visualization.renderers.BasicRenderer.renderVertex(BasicRenderer.java:70)
at edu.uci.ics.jung.visualization.renderers.BasicRenderer.render(BasicRenderer.java:55)
at edu.uci.ics.jung.visualization.BasicVisualizationServer.renderGraph(BasicVisualizationServer.java:367)
 at edu.uci.ics.jung.visualization.BasicVisualizationServer.paintComponent(BasicVisualizationServer.java:321)
at javax.swing.JComponent.paint(JComponent.java:1054)
at javax.swing.JComponent.paintToOffscreen(JComponent.java:5221)
at javax.swing.RepaintManager$PaintManager.paintDoubleBuffered(RepaintManager.java:1482)
at javax.swing.RepaintManager$PaintManager.paint(RepaintManager.java:1413)
at javax.swing.RepaintManager.paint(RepaintManager.java:1206)
at javax.swing.JComponent._paintImmediately(JComponent.java:5169)
at javax.swing.JComponent.paintImmediately(JComponent.java:4980)
at javax.swing.RepaintManager.paintDirtyRegions(RepaintManager.java:770)
at javax.swing.RepaintManager.paintDirtyRegions(RepaintManager.java:728)
at javax.swing.RepaintManager.prePaintDirtyRegions(RepaintManager.java:677)
at javax.swing.RepaintManager.access$700(RepaintManager.java:59)
at javax.swing.RepaintManager$ProcessingRunnable.run(RepaintManager.java:1621)
at java.awt.event.InvocationEvent.dispatch(InvocationEvent.java:251)
at java.awt.EventQueue.dispatchEventImpl(EventQueue.java:705)
at java.awt.EventQueue.access$000(EventQueue.java:101)
at java.awt.EventQueue$3.run(EventQueue.java:666)
at java.awt.EventQueue$3.run(EventQueue.java:664)
at java.security.AccessController.doPrivileged(Native Method)
at java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)
at java.awt.EventQueue.dispatchEvent(EventQueue.java:675)
at    java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:211)
at java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:128)
at    java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:117)
at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:113)
at java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:105)
at java.awt.EventDispatchThread.run(EventDispatchThread.java:90)
</code></pre>
</blockquote>

<p>This is the vertex painter transformer's code. It works perfectly fine if its not loading the database part.</p>

<pre><code>package FamTree;

import org.apache.commons.collections15.Transformer;

import java.awt.*;
import java.util.Map;

class vertexPainter implements Transformer&lt;Integer, Paint&gt;
{
private Map&lt;Integer,Person&gt; tempV;

public vertexPainter(Map&lt;Integer,Person&gt; passV)
{
    tempV = passV;
}
public Paint transform(Integer v) 
{

    if (tempV.get(v).getpSex().equalsIgnoreCase(""male"")) 
        return (Color.blue); 
    else
    return (Color.red);

}
}
</code></pre>

<p>This is the initialization of graph and setting up the vv.</p>

<pre><code>     public TCanvas(){

     graph = new SparseMultigraph&lt;Integer,Number&gt;();

     layout2 = new StaticLayout&lt;Integer,Number&gt;(graph,new Dimension(2000,2000));

     vv =  new VisualizationViewer&lt;Integer,Number&gt;(layout2);
     vv.setBackground(Color.white);
     vv.setVertexToolTipTransformer(new ToStringLabeller&lt;Integer&gt;());
     vv.getRenderContext().setVertexFillPaintTransformer(new vertexPainter(vertex));
vv.getRenderer().getVertexLabelRenderer().setPosition(Renderer.VertexLabel.Position.S);
     vv.getRenderContext().setVertexLabelTransformer(new vertexLabel(vertex));
     vv.getRenderContext().setEdgeDrawPaintTransformer(new edgePainter());

     Transformer&lt;Number,String&gt; stringer = new Transformer&lt;Number,String&gt;(){
         public String transform(Number e) {
             return ""Edge:""+e+ ""-""+graph.getEndpoints(e).toString();
         }
     };

     vv.getRenderContext().setEdgeLabelTransformer(stringer);

     EdgeLabelRenderer edgeLabelRenderer= vv.getRenderContext().getEdgeLabelRenderer();
     edgeLabelRenderer.setRotateEdgeLabels(false);



     gm = new DefaultModalGraphMouse&lt;Integer, Number&gt;();
     vv.setGraphMouse(gm);
     gm.add(new PopupGraphMousePlugin());

     GraphZoomScrollPane scrollPane2 = new GraphZoomScrollPane(vv);

     JPanel vvPanel = new JPanel();
     vvPanel.setLayout(new BorderLayout());
     vvPanel.add(scrollPane2);

     addBottomControls(vvPanel);

     Container content = getContentPane();
     content.add(vvPanel);  
     }//TCanvas()
</code></pre>

<p>This is the part to load the database.</p>

<pre><code>     menu.add(new AbstractAction(""Open"") {
        public void actionPerformed(ActionEvent e) {

            JFileChooser open  = new JFileChooser();
            final File file ;

            int vat = open.showOpenDialog(rootPane);

            if(vat==JFileChooser.APPROVE_OPTION){

                file = open.getSelectedFile();

                String fPath = file.getAbsolutePath();

                System.out.println(""path:""+fPath);
                try{

                    Class.forName(""org.sqlite.JDBC"");
                    Connection conn = DriverManager.getConnection(""jdbc:sqlite:""+fPath);

                     Statement stat = conn.createStatement();

                     ResultSet rs = stat.executeQuery(""select * from Person;"");
                     while (rs.next()) {


                        Integer pid = rs.getInt(""pID"");
                        String pname = rs.getString(""pName"");
                        String psex = rs.getString(""pSex"");
                        Integer fatherid = (Integer)rs.getInt(""fatherID"");
                        Integer motherid = (Integer)rs.getInt(""motherID"");
                        Integer spouseid = (Integer)rs.getInt(""spouseID"");

                        Person personInfo = new Person();

                        personInfo.setpID(pid);
                        personInfo.setpName(pname);
                        personInfo.setpSex(psex);
                        personInfo.setFatherID(fatherid);
                        personInfo.setMotherID(motherid);
                        personInfo.setSpouseID(spouseid);

                        vertex.put(pid,personInfo);
                        graph.addVertex(pid);
                        vv.repaint();

                        ResultSet rs2 = stat.executeQuery(""select * from Location;"");
                         while (rs2.next()) {

                             if(rs2.getInt(""pID"")==pid){

                                 Point2D.Double loc = new Point2D.Double();

                                 Double x = rs2.getDouble(""pointX"");
                                 Double y = rs2.getDouble(""pointY"");
                                 loc.setLocation(x, y);

                                 layout2.setLocation(pid, vv.getRenderContext().getMultiLayerTransformer().inverseTransform(loc));
                                 vv.repaint();
                             }
                         }
                         rs2.close();


                     }
                     rs.close();

                    ResultSet rs3 = stat.executeQuery(""select * from Relation;"");

                    while (rs3.next()) {

                         Point2D.Double loc2 = new Point2D.Double();

                         Number edgeid = (Number)rs3.getInt(""edgeID"");
                         Integer v1 = rs3.getInt(""vertex1"");
                         Integer v2 = rs3.getInt(""vertex2"");
                         String type = rs3.getString(""RelationType"");

                         Relation relation = new Relation(edgeid, v1, v2, type);
                         edge.put(edgeid, relation);
                         if(type.equalsIgnoreCase(""DIRECTED"")){
                             graph.addEdge(edgeid, v1, v2, EdgeType.DIRECTED);
                         }
                         if(type.equalsIgnoreCase(""UNDIRECTED"")){
                             graph.addEdge(edgeid, v1, v2, EdgeType.UNDIRECTED);
                         }
                        vv.repaint();


                    }
                    rs3.close();

                    conn.close();
                    vv.repaint();

                }
                catch (Exception ex) {
                    // TODO: handle exception
                    ex.printStackTrace();
                }

            }

        }});
</code></pre>

<p>I really dont know whats happening here as Im pretty sure the opening and retrieving database part have no problem.</p>
","transformer-model"
"11819992","Running ipython script using a new extenction","2012-08-05 20:53:08","","1","68","<ipython><transformer-model>","<p>I am writing an IPython extension that defines a new shell transformer (see <a href=""http://ipython.org/ipython-doc/dev/api/generated/IPython.core.prefilter.html#IPython.core.prefilter.PrefilterManager"" rel=""nofollow"">http://ipython.org/ipython-doc/dev/api/generated/IPython.core.prefilter.html#IPython.core.prefilter.PrefilterManager</a>), and I need to use this extension (and syntax defined by the shell transformer) in a script.  However, a simple test reveals that the standard IPython <code>%run</code> commands does not work as expected: when the commands in the script are pasted directly in the IPython prompt, everything is fine; if instead I run the IPython script with </p>

<p><code>%run script.ipy</code></p>

<p>the new syntax is not recognized (i.e., it looks like the new shell transformer is not used).</p>
","transformer-model"
"11672673","MULE 3.2 - How to access Flow Session Property Values from a java component","2012-07-26 15:25:50","11676683","1","5075","<components><message><esb><mule><transformer-model>","<p>This Question relates to Mule ESB 3.2.</p>

<p>If I have read values from JMS, transform to JSON and store the values in my session like this:</p>

<pre><code>&lt;message-properties-transformer scope=""session"" doc:name=""save values to session""&gt;            
        &lt;add-message-property key=""id"" value=""#[json-node://id]""/&gt;
        &lt;add-message-property key=""name"" value=""#[json-node://name]""/&gt;
&lt;/message-properties-transformer&gt;   

&lt;component class=""org.mule.example.echo.Echo""/&gt;
</code></pre>

<p>How do I access these property values from a java component?</p>
","transformer-model"
"11649701","Mule 3.2 - Why am I getting this SAXParseException from XSLT transformation?","2012-07-25 12:27:47","11652929","0","5742","<xml><xslt><mule><transformer-model>","<p>Using Mule 3.2 I am trying to build XML from inside my mule flow but I get this error
I tried a few ideas online but I keep getting this message.</p>

<pre><code>   &lt;mulexml:xslt-transformer maxIdleTransformers=""2"" maxActiveTransformers=""5"" doc:name=""XSLT""&gt;
    &lt;mulexml:xslt-text&gt;
        &lt;xsl:stylesheet version=""2.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""&gt;
            &lt;xsl:output method=""xml"" omit-xml-declaration=""yes"" /&gt;
                &lt;xsl:param name=""numericId""/&gt;
                &lt;xsl:param name=""name""/&gt;
                &lt;xsl:param name=""url""/&gt;                             
                &lt;xsl:template match=""*""&gt;            
                &lt;Publisher&gt;
                &lt;numericId&gt;&lt;xsl:value-of select=""$numericId""/&gt;&lt;/numericId&gt;
                &lt;name&gt;&lt;xsl:value-of select=""$name""/&gt;&lt;/name&gt;
                &lt;url&gt;&lt;xsl:value-of select=""$url""/&gt;&lt;/url&gt;                    
                &lt;/Publisher&gt;                    
            &lt;/xsl:template&gt;
        &lt;/xsl:stylesheet&gt;
    &lt;/mulexml:xslt-text&gt;
    &lt;mulexml:context-property key=""numericId"" value=""#[header:SESSION:numericId]""/&gt;
    &lt;mulexml:context-property key=""name"" value=""#[header:SESSION:name]""/&gt;
    &lt;mulexml:context-property key=""url"" value=""#[header:SESSION:url]""/&gt;     
    &lt;/mulexml:xslt-transformer&gt;
</code></pre>

<p>ERROR:</p>

<pre><code>******************************************************************************** Message               : org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog. (net.sf.saxon.trans.DynamicError) Code                  : MULE_ERROR-64999
-------------------------------------------------------------------------------- Exception stack is:
1. Content is not allowed in prolog. (org.xml.sax.SAXParseException)   org.apache.xerces.util.ErrorHandlerWrapper:-1 (null)
2. org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog. (net.sf.saxon.trans.DynamicError)   net.sf.saxon.event.Sender:308 (null)
3. org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog. (net.sf.saxon.trans.DynamicError) (org.mule.api.transformer.TransformerException)   org.mule.module.xml.transformer.XsltTransformer:190 (http://www.mulesoft.org/docs/site/current3/apidocs/org/mule/api/transformer/TransformerException.html)
--------------------------------------------------------------------------------
Root Exception stack trace: org.xml.sax.SAXParseException; lineNumber: 1; columnNumber: 1;
 Content is not allowed in prolog.
at org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown Source)
at org.apache.xerces.util.ErrorHandlerWrapper.fatalError(Unknown Source)
at org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)
    + 3 more (set debug level logging or '-Dmule.verbose.exceptions=true' for everything)
********************************************************************************
</code></pre>
","transformer-model"
"11340779","HTML DOM Tree to String - Transformer NullPointerException","2012-07-05 08:46:05","11344725","0","892","<java><xml><dom><transform><transformer-model>","<p>I'm trying to convert the content of an org.w3c.dom.Document object into a string. I get the Document object of the current page displayed in the JBrowser component. The most common way to convert a document dom tree into a string seems to be using a javax.xml.transform.Transformer. So I implemented this:</p>

<pre><code>ByteArrayOutputStream baos = new ByteArrayOutputStream();

TransformerFactory.newInstance().newTransformer().transform(
            new DOMSource(aDocument), new StreamResult(baos));

return baos.toString();
</code></pre>

<p>This works for simple websites, but the more complex they get the higher the probability of me getting this exception:</p>

<pre><code>    ERROR:  ''
05.07.2012 10:17:09 com.de.test.Demonstrator$1 run
FATAL: null
javax.xml.transform.TransformerException: java.lang.NullPointerException
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:717)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:313)
at com.de.test.DocumentUtils.toHTML(DocumentUtils.java:47)
at com.de.test.Demonstrator$1.run(Demonstrator.java:172)
Caused by: java.lang.NullPointerException
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:178)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:132)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:94)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transformIdentity(TransformerImpl.java:662)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:708)
... 3 more
---------
java.lang.NullPointerException
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:178)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:226)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:132)
at com.sun.org.apache.xalan.internal.xsltc.trax.DOM2TO.parse(DOM2TO.java:94)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transformIdentity(TransformerImpl.java:662)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:708)
at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerImpl.transform(TransformerImpl.java:313)
at com.de.test.DocumentUtils.toHTML(DocumentUtils.java:47)
at com.de.test.Demonstrator$1.run(Demonstrator.java:172)
</code></pre>

<p>After some research found out about the hint, that some text elements might be null and that this causes the Transformer to crash. So I did just that:</p>

<pre><code>    public static final void traverseLevel(TreeWalker walker, Document aDocument, String indent)
{
    // describe current node:
    Node parent = walker.getCurrentNode();

    if (parent != null &amp;&amp; parent.getNodeValue() == null)
        parent.setNodeValue("" "");

    System.out.println(indent + ""- &lt;"" + ((Element) parent).getTagName() + ""&gt;"" + parent.getNodeValue());

    // traverse children:
    for (Node n = walker.firstChild(); n != null; n = walker.nextSibling())
    {
        if(n != null)
            traverseLevel(walker, aDocument, indent + '\t');
    }

    System.out.println(""&lt;/""+ ((Element) parent).getTagName() + ""&gt;"");

    // return position to the current (level up):
    walker.setCurrentNode(parent);
}
</code></pre>

<p>This is where I found out that ""parent.getNodeValue()"" always returns null. The funny thing is, that the problem also occurs on simple websites, but the transformer still outputs the tree's values. Any idea what's wrong with my replacement of null text nodes? Are there other potential problems that might cause this issue?</p>

<p>Thanks!</p>
","transformer-model"
"11227364","Mule custom transformer with","2012-06-27 13:29:42","11251993","0","5175","<java><mule><transformer-model>","<p>I'm having some trouble finding out how to create a custom transformer that can input and output a file in Mule 3.2. I have prototyped the code for the transformation and that works fine but I can't find any documentation on how to take in a file in a transformer.</p>

<p>Here is what I have so far as I'm but even this throws an error:</p>

<pre><code>@ContainsTransformerMethods
  public class xmlToJson {

  @Transformer
  public File xmlIn(File file) {
    // logic to go here
    return file;
  }
}
</code></pre>

<p>Here is the exception that is thrown:</p>

<pre><code>ERROR 2012-06-27 14:08:37,664 [main] org.mule.tooling.server.application.
ApplicationDeployer: null
java.lang.IllegalStateException: Cannot convert value of type [convert.xmlToJson]
to required type [org.mule.api.processor.MessageProcessor] for property 'messageProcessors[0]': no matching editors or conversion strategy found
</code></pre>

<p>I can't seem to find any documentation or tutorials that show how to structure a custom transformer to take in a file.</p>
","transformer-model"
"10915547","web.config transformer does nothing","2012-06-06 13:48:54","","0","520","<asp.net><web-config><transformer-model>","<p>I'm trying to use the web.config transformer, but it does nothing.
Here is my last part of web.config:</p>

<pre><code> &lt;system.serviceModel&gt;
&lt;bindings&gt;
  &lt;basicHttpBinding&gt;
    &lt;binding name=""BasicHttpBinding_IUserInterfaceService"" closeTimeout=""00:01:00"" openTimeout=""00:01:00"" receiveTimeout=""00:10:00"" sendTimeout=""00:10:00"" allowCookies=""false"" bypassProxyOnLocal=""false"" hostNameComparisonMode=""StrongWildcard"" maxBufferSize=""2147483647"" maxBufferPoolSize=""2147483647"" maxReceivedMessageSize=""2147483647"" messageEncoding=""Text"" textEncoding=""utf-8"" transferMode=""Buffered"" useDefaultWebProxy=""true""&gt;
      &lt;readerQuotas maxDepth=""2147483647"" maxStringContentLength=""2147483647"" maxArrayLength=""2147483647"" maxBytesPerRead=""2147483647"" maxNameTableCharCount=""2147483647"" /&gt;

      &lt;security mode=""None""&gt;
        &lt;transport clientCredentialType=""None"" proxyCredentialType=""None"" realm="""" /&gt;
        &lt;message clientCredentialType=""UserName"" algorithmSuite=""Default"" /&gt;
      &lt;/security&gt;
    &lt;/binding&gt;
  &lt;/basicHttpBinding&gt;

&lt;/bindings&gt;
&lt;client&gt;
  &lt;endpoint address=""http://[WhiteOPS User Interface]:[Port]/UIService/UserInterfaceService/""
    binding=""basicHttpBinding"" bindingConfiguration=""BasicHttpBinding_IUserInterfaceService""
    contract=""UserInterface.IUserInterfaceService"" name=""BasicHttpBinding_IUserInterfaceService"" /&gt;
&lt;/client&gt;
</code></pre>

<p></p>

<p>and here is my web.debug.config:</p>

<pre><code>&lt;?xml version=""1.0""?&gt;
&lt;configuration xmlns:xdt=""http://schemas.microsoft.com/XML-Document-Transform""&gt;
  &lt;endpoint name=""BasicHttpBinding_IUserInterfaceService"" address=""http://localhost:80/UIService/UserInterfaceService/""
   xdt:Transform=""SetAttributes"" xdt:Locator=""Match(name)"" /&gt;
</code></pre>

<p>
  </p>

<p>
</p>

<p>when i run (with or without debuging mode) in the debug configuration, i'm getting an error that this line cannot compiled:</p>

<pre><code>&lt;endpoint address=""http://[WhiteOPS User Interface]:[Port]/UIService/UserInterfaceService/""
binding=""basicHttpBinding"" bindingConfiguration=""BasicHttpBinding_IUserInterfaceService""
contract=""UserInterface.IUserInterfaceService"" name=""BasicHttpBinding_IUserInterfaceService"" /&gt;
</code></pre>

<p>so why the transformation not happening?</p>

<p>Thanks,</p>
","transformer-model"
"10663947","Asus transformer is not visible through adb","2012-05-19 09:23:30","","4","8003","<android><windows-7><adb><transformer-model>","<p>I have a problem with adb on Windows 7 and Asus Transformer (ISC, 4.0.3)</p>

<p>adb devices shows empty list.</p>

<p>I've installed Asus PC Suit, USB debugging is switched on. On Linux everything works fine.</p>
","transformer-model"
"10584670","Setting namespaces and prefixes in a Java DOM document","2012-05-14 13:51:47","10585180","14","70565","<java><xml><serialization><transformer-model>","<p>I'm trying to convert a ResultSet to an XML file.
I've first used this example for the serialization.</p>

<pre><code>import  org.w3c.dom.bootstrap.DOMImplementationRegistry;
import  org.w3c.dom.Document;
import  org.w3c.dom.ls.DOMImplementationLS;
import  org.w3c.dom.ls.LSSerializer;

...

DOMImplementationRegistry registry = DOMImplementationRegistry.newInstance();

DOMImplementationLS impl = 
    (DOMImplementationLS)registry.getDOMImplementation(""LS"");

...     

LSSerializer writer = impl.createLSSerializer();
String str = writer.writeToString(document);
</code></pre>

<p>After I made this work, I tried to validate my XML file, there were a couple of warnings.
One about not having a doctype. So I tried another way to implement this. I came across the Transformer class. This class lets me set the encoding, doctype, etc. </p>

<p>The previous implementation supports automatic namespace fix-up. The following does not.</p>

<pre><code>private static Document toDocument(ResultSet rs) throws Exception {   
    DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
    factory.setNamespaceAware(true);
    DocumentBuilder builder = factory.newDocumentBuilder();
    Document doc = builder.newDocument();

    URL namespaceURL = new URL(""http://www.w3.org/2001/XMLSchema-instance"");
    String namespace = ""xmlns:xsi=""+namespaceURL.toString();

    Element messages = doc.createElementNS(namespace, ""messages"");
    doc.appendChild(messages);

    ResultSetMetaData rsmd = rs.getMetaData();
    int colCount = rsmd.getColumnCount();

    String attributeValue = ""true"";
    String attribute = ""xsi:nil"";

    rs.beforeFirst();

    while(rs.next()) {
        amountOfRecords = 0;
        Element message = doc.createElement(""message"");
        messages.appendChild(message);

        for(int i = 1; i &lt;= colCount; i++) {

            Object value = rs.getObject(i);
            String columnName = rsmd.getColumnName(i);

            Element messageNode = doc.createElement(columnName);

            if(value != null) {
                messageNode.appendChild(doc.createTextNode(value.toString()));
            } else {
                messageNode.setAttribute(attribute, attributeValue);
            }
            message.appendChild(messageNode);
        }
        amountOfRecords++;
    }
    logger.info(""Amount of records archived: "" + amountOfRecords);

    TransformerFactory tff = TransformerFactory.newInstance();
    Transformer tf = tff.newTransformer();
    tf.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""yes"");
    tf.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");
    tf.setOutputProperty(OutputKeys.INDENT, ""yes"");

    BufferedWriter bf = createFile();
    StreamResult sr = new StreamResult(bf);
    DOMSource source = new DOMSource(doc);
    tf.transform(source, sr);

    return doc;
}
</code></pre>

<p>While I was testing the previous implementation I got an TransformationException: Namespace for prefix 'xsi' has not been declared. As you can see I've tried to add a namespace with the xsi prefix to the root element of my document. After testing this I still got the Exception. What is the correct way to set namespaces and their prefixes?</p>

<p>Edit: Another problem I have with the first implementation is that the last element in the XML document doesn't have the last three closing tags.</p>
","transformer-model"
"10356611","How to set org.apache.xalan.xsltc.trax.SmartTransformerFactoryImpl as transformer in JBoss AS 7.0.2 Managed server","2012-04-27 19:20:14","","0","2747","<xslt><jboss7.x><transformer-model>","<p>I'm having a problem setting the <em>org.apache.xalan.xsltc.trax.SmartTransformerFactoryImpl</em> as the <em>javax.xml.transform.TransformerFactory</em> when using the JBoss AS 7.0.2 in domain mode. If I put the following property in standalone.xml, everything works, but if I add the same in a server in host.xml (or in domain.xml) the server won't start.</p>

<pre><code>&lt;property name=""javax.xml.transform.TransformerFactory"" value=""org.apache.xalan.xsltc.trax.SmartTransformerFactoryImpl""/&gt;
</code></pre>

<p>I have in my deployment the xalan lib, but I even tried to put it as a global module. Nothing worked. I noticed the following error in console:</p>

<pre><code>[Server:server-one] Exception in thread ""main"" javax.xml.transform.TransformerFactoryConfigurationError: Provider org.apache.xalan.xsltc.trax.SmartTransformerFactoryImpl not found
[Server:server-one]     at javax.xml.transform.TransformerFactory.newInstance(TransformerFactory.java:108)
[Server:server-one]     at __redirected.__TransformerFactory.&lt;clinit&gt;(__TransformerFactory.java:66)
[Server:server-one]     at __redirected.__JAXPRedirected.initAll(__JAXPRedirected.java:82)
[Server:server-one]     at org.jboss.modules.Module$1.run(Module.java:88)
[Server:server-one]     at org.jboss.modules.Module$1.run(Module.java:75)
[Server:server-one]     at java.security.AccessController.doPrivileged(Native Method)
[Server:server-one]     at org.jboss.modules.Module.&lt;clinit&gt;(Module.java:75)
[Server:server-one]     at org.jboss.modules.Main.main(Main.java:257)
</code></pre>

<p>If I simply remove the property the server works fine.</p>

<p>What else can I do??</p>

<p>I'm using the JBoss AS 7.0.2 Final.</p>

<p><a href=""https://community.jboss.org/message/732756"" rel=""nofollow"">Link for the same question in JBoss community</a></p>
","transformer-model"
"9720201","How to convert JAVA Object to JSON Efficiently..?","2012-03-15 12:59:04","","3","9703","<java><json><mule><transformer-model>","<p>I am using <strong>Mule</strong>. I have a JAVA Object that is populated from my internal Class..It is actually a <code>HashMap&lt;String,Object&gt;</code>. <code>Object</code> can be anything..another <code>HashMap</code>, OR <code>List</code> etc ..Now i have to convert it into <code>JSON</code> (and removing all those keys that have value as NULL)..</p>

<p>When i use a given Mule Transformer , <code>ObjectToJSON</code>, it is converting into appropriate JSON..but not able to remove NULL value..And i could not find any properties to set in <strong>Custom-transformer</strong> that will remove NULL values..!!</p>

<hr>

<p>So then, i wrote a <strong>custom transformer</strong>, that uses the <code>net.sf.json-lib</code> library and i am able to remove NULL values. </p>

<p>But in one of my JAVA Object , i have a <code>HashMap&lt;Integer,String&gt;</code> and since in JSON Object , Integer cannot be keys, <code>net.sf.json</code> library is giving an Exception :</p>

<pre><code>Exception stack is:
1. JSON keys must be strings. (java.lang.ClassCastException)
  net.sf.json.JSONObject:1120 (null)
2. java.lang.ClassCastException: JSON keys must be strings. (net.sf.json.JSONException)
  net.sf.json.JSONObject:1160 (null)
3. java.lang.ClassCastException: JSON keys must be strings. (net.sf.json.JSONException). Message payload is of type: HashMap (org.mule.api.transformer.TransformerMessagingException)
</code></pre>

<p>and so it is unable to convert it into JSON..</p>

<hr>

<p>So what is most viable option..??</p>
","transformer-model"
"9717081","Writing entity references (quote,apostrophe) to xml file - java","2012-03-15 09:34:59","","1","1263","<java><xml><dom><transformer-model>","<p>I am writing a java program which writes an xml file as output. Now, there are certain node values in the xml which contain entity references like &lt;, >, ' and "". Current output is something like this:</p>

<pre><code>&lt;Parent&gt;
  &lt;Child&gt;&amp;lt;'""&lt;/Child&gt;
&lt;/Parent&gt;
</code></pre>

<p>The output i require:</p>

<pre><code>&lt;Parent&gt;
  &lt;Child&gt;&amp;lt;&amp;apos;&amp;quot;&lt;/Child&gt;
&lt;/Parent&gt;
</code></pre>

<p>I read in the following post that maybe its not possible to do what I want:
<a href=""https://stackoverflow.com/questions/5141413/stax-xml-parser-not-escaping-single-quote-apos"">StAX XML Parser not escaping single quote (&amp;apos;)</a></p>

<p>But the system which reads this xml file requires all quotes and apostrophes to be escaped. How can i achieve this?</p>

<p>Some code:</p>

<pre><code>DocumentBuilderFactory coDocFactory = DocumentBuilderFactory.newInstance();
DocumentBuilder coDocBuilder = coDocFactory.newDocumentBuilder();
coXMLDocument = coDocBuilder.newDocument();

Element coParent = coXMLDocument.createElement(""Parent"");
coXMLDocument.appendChild(coParent);

Element coChild = coXMLDocument.createElement(""Child"");
coParent.appendChild(coChild);
coChild.setTextContent(""&lt;&gt;/'/""""); //apostrophe and quotes have been escaped

TransformerFactory coTransFactory = TransformerFactory.newInstance();
Transformer transformer = coTransFactory.newTransformer();
transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
DOMSource coDomSource = new DOMSource(coXMLDocument);
StreamResult coResult = new StreamResult(new File(""C:\\a.xml""));
transformer.transform(coDomSource, coResult);
</code></pre>

<p>Help will be appreciated.</p>
","transformer-model"
"9598345","Omitting XML Declaration when invoking Transformer with StAXResult","2012-03-07 08:54:53","","2","2535","<xml-parsing><stax><transformer-model>","<p>I would like to copy multiple XML nodes from a source XML file to a target file. Both source and target files are very large, so I will use StAX. Typically the file I'm trying to process looks as follows:</p>

<pre><code>&lt;root&gt;
  &lt;header&gt;
    &lt;title&gt;A List of persons&lt;/title&gt;
  &lt;/header&gt;
  &lt;person&gt;
    &lt;name&gt;Joe&lt;/name&gt;
    &lt;surname&gt;Bloggs&lt;/surname&gt;
  &lt;/person&gt;  
  &lt;person&gt;
    &lt;name&gt;John&lt;/name&gt;
    &lt;surname&gt;Doe&lt;/surname&gt;
  &lt;/person&gt;  
  .
  .
  etc...
&lt;/root&gt;
</code></pre>

<p>The target files should be in the following format:</p>

<pre><code>&lt;root&gt;
  &lt;header&gt;
    &lt;title&gt;A List of persons&lt;/title&gt;
  &lt;/header&gt;
  &lt;person&gt;
    &lt;name&gt;Joe&lt;/name&gt;
    &lt;surname&gt;Bloggs&lt;/surname&gt;
  &lt;/person&gt;
&lt;/root&gt;
</code></pre>

<p>where each file should contain the <em>header</em> node, exactly one <em>person</em> node all enclosed within the <em>root</em> node.</p>

<p>Now my problem is the following: I'm trying to read in the source file through a XMLStreamReader, and writing it using a XMLStreamWriter, both of which are wired into a Transformer instance which copies <strong>fragments</strong> from the source file into the target file. The transformer is created as follows:</p>

<pre><code>TransformerFactory transformerFactory = TransformerFactory.newInstance();
Transformer transformer = transformerFactory.newTransformer();
transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""yes"");

StAXSource stAXSource = new StAXSource(reader);
StAXResult stAXResult = new StAXResult(writer);
</code></pre>

<p>I also have a custom made method which moves the cursor to the desired fragment in the XML input stream:</p>

<pre><code>// Moves XMLStreamReader cursor to the next fragment. 
moveCursorToNextFragment(XMLStreamReader reader, String fragmentNodeName)
</code></pre>

<p>So that in the end I end up with the following:</p>

<pre><code>// Open file as usual...

// Advance cursor to &lt;header&gt; node, and copy fragment till
// &lt;/header&gt; to the output writer. 
moveCursorToNextFragment(reader, ""header"");
transformer.transform(stAXSource, stAXResult);

// Advance cursor to &lt;person&gt; node, and copy fragment till
// &lt;/person&gt; to the output writer.
moveCursorToNextFragment(reader, ""person"");
transformer.transform(stAXSource, stAXResult);
</code></pre>

<p>The problem is that the resultant XML file contains 2 XML declaration sections, one for each invocation of </p>

<pre><code>transformer.transform(stAXSource, stAXResult);
</code></pre>

<p>I have tried using <strong>StreamResult</strong> to transform the output, as follows:</p>

<pre><code>transformer.transform(stAXSource, new StreamResult(myStream));
</code></pre>

<p>and the XML declaration is omitted, but when I reverted back to using <strong>StAXResult</strong>, the XML declaration is back again. I also noticed that OutputKeys.OMIT_XML_DECLARATION has no effect whether it is on or off (as are other settings such as OutputKeys.STANDALONE with a value of ""yes""). </p>

<p>So in short, it seems that these settings set globally on the Transformer are being disregarded when a StAXResult as a destination result. </p>

<p>My question is this: is there any way in which this can be achieved, so that the Transformer does not emit XML declarations upon each invocation of Transformer.transform() (i.e write fragments without the XML declaration)?</p>

<p>Your help is much appreciated and needed.</p>
","transformer-model"
"9298748","Spring Integration Get HTTP Outbound Gateway Response","2012-02-15 18:03:03","9300061","4","5738","<gateway><spring-integration><transformer-model><payload>","<p>I need to POST a REST service call and get the data it returns (all of this is with JSON).  I have an outbound-gateway with its reply-channel as a chain, and the chain has one transformer. </p>

<pre><code>&lt;int-http:outbound-gateway
    url=""#{appProperties['rootUrl']}#{appProperties['myMethod']}""
    request-channel=""myRequestChannel"" reply-channel=""myResponseChannel"" &gt;
&lt;/int-http:outbound-gateway&gt;

&lt;int:channel id=""myResponseChannel""/&gt;

&lt;int:chain input-channel=""myResponseChannel""&gt;
    &lt;int:transformer ref=""genericResponseTransformer""/&gt;
&lt;/int:chain&gt;
</code></pre>

<p>However when I debug through the transformer, the payload I get back is just an HttpStatus object.</p>

<p>Maybe I'm doing something wrong?  Any help would be greatly appreciated.  Thanks!</p>
","transformer-model"
"9008874","Android can't get result from standard camera intent","2012-01-25 19:22:01","","0","257","<android><camera><transformer-model>","<p>I'm trying to use the standard camera of the ASUS Transformer EE Tablet. I can open the camera and take a picture. But I'm not able to accept the taken picture. I can cancel or take a new picture. But the accept button doesn't work. Did anyone else had this problem?</p>

<p>On the device installed is Android 3.2.1:</p>

<p>Here's the code:</p>

<pre><code>Intent standard = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
standard.putExtra(MediaStore.EXTRA_OUTPUT, uri);
startActivityForResult(standard, 100);
</code></pre>

<p>and</p>

<pre><code>if (requestCode == CameraActivity.RESULT_OK &amp;&amp; requestCode == 100) {
    Toast.makeText(getActivity(), data.getData().toString(), Toast.LENGTH_LONG).show();
    report.getPhotoDefinitions().add(definition);
}
</code></pre>
","transformer-model"
"8889661","Use Asus Transformer Prime as USB Debugger","2012-01-17 04:30:40","9104506","0","1341","<android><debugging><transformer-model>","<p>I am thinking about buying the Asus Transformer Prime, to start developing on android. Currently I am borrowing a Motorola Xoom to develop, and that has a Micro USB port, that I can plug into, and use it to debug my apps. From what I can tell the Asus Transformer Prime does not have such a port. Can I even use this as my debugging device, or would I be making a $500 mistake?</p>

<p>Thank you in advance for all of your help!</p>
","transformer-model"
"8807340","prevent java transformer from replacing ","2012-01-10 17:07:18","9110879","0","918","<java><transformer-model>","<p>I have this XML file </p>

<pre><code>&lt;test&gt;&amp;#13; &amp;#xD; &amp;lt;&lt;/test&gt;
</code></pre>

<p>and i am tranforming it with the java code below, the xslt file just makes a copy of the xml</p>

<pre><code>public class XMLTransform {

    public static void main(String[] args) {

        try {
            StreamSource source = new StreamSource(new File(""file.xml""));
            StreamSource stylesource = new StreamSource(new File(""trans.xsl""));
            SAXTransformerFactory transformFactory = (SAXTransformerFactory) TransformerFactory.newInstance(); 
            Transformer transformer = transformFactory.newTransformer(stylesource);
            transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""no"");         
            transformer.setOutputProperty(OutputKeys.INDENT, ""yes""); 
            StreamResult result = new StreamResult(System.out);
            transformer.transform(source,result);
        }
        catch (Exception e) {
            e.printStackTrace();
        }

    }
}
</code></pre>

<p>my problem is that java replaces the canonical carriage return <strong>&amp;#xD</strong> with the <strong>&amp;#13</strong> which is the ascii character.</p>

<p>Any help on how to preserve the canonical name for the carriage return?</p>
","transformer-model"
"8369769","HornetQ: Divert + Transformer class usage","2011-12-03 18:07:42","","2","235","<hornetq><transformer-model>","<p>I was looking forward to using a divert on HornetQ to divert messages on a given address to many other addresses. What I would like to do is add some additional properties on the message ONCE it gets diverted. It seems like I can use a transformer class to do just that. My only question is that I would like to add custom properties based on the divert that the message came on. I am not sure how to get that info.</p>

<p>Example: 
address.order -> divert it to address.electronicorder, address.bookorder, address.clothorder.</p>

<p>Now once it gets diverted; I would like to add a property (like ""ORDER TYPE"") on the message in the transformer.. but I am not sure how I can get the divert that the message came on..</p>

<p>Any tips?</p>
","transformer-model"
"8058980","Java Transformer, TransformerFactory messes up multiline xml comments","2011-11-09 00:31:13","","2","1144","<java><xml><comments><wowza><transformer-model>","<p>I'm having a bit of trouble using Transformer/TransformerFactory to update an existing XML file (on a Wowza server).  It works great and updates correctly, but it makes a mess of multiline comments.</p>

<p>The comments start out looking like this:</p>

<pre><code>&lt;!--StorageDir path variables   
${com.wowza.wms.AppHome} - Application home directory   
${com.wowza.wms.ConfigHome} - Configuration home directory      
${com.wowza.wms.context.VHost} - Virtual host name  
${com.wowza.wms.context.VHostConfigHome} - Virtual host config directory        
${com.wowza.wms.context.Application} - Application name     
${com.wowza.wms.context.ApplicationInstance} - Application instance name        
--&gt;
</code></pre>

<p>After the transform, they end up all in one line with square characters where the return carriages used to be (won't post the result here because the squares don't show, and the line wrap makes the end result look pretty much like the above block).</p>

<p>This is the code I use to make the xml updates (just a snippet):</p>

<pre><code>TransformerFactory tFactory = TransformerFactory.newInstance();
tFactory.setAttribute(""indent-number"", new Integer(4));
Transformer transformer = tFactory.newTransformer();
transformer.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""yes"");
transformer.setOutputProperty(OutputKeys.METHOD, ""xml"");
transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");

DOMSource source = new DOMSource(document);
File outFile = new File(path.getPath() + ""/Application.xml"");
FileOutputStream outputStream = new FileOutputStream(outFile);

StreamResult result = new StreamResult(new OutputStreamWriter(outputStream));
transformer.transform(source, result);
</code></pre>

<p>I need to keep these multiline comments legible and correctly formatted because these xml files need to be edited by hand sometimes, and the comments are used as guides and reminders.</p>

<p>Any input?  How might I get the transformers to stop mangling my comments?  Thanks very much!  :)</p>

<p><strong>EDIT:</strong>  This is how the document is initially created (where appXML is the path of the Application.xml file that is going to be edited):</p>

<pre><code>Document document;
DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
DocumentBuilder builder = factory.newDocumentBuilder();
document = builder.parse(appXML);
</code></pre>

<p>After that, I make a lot of node-specific changes... too many in total to put them all here, but there are the formats all of the changes follow:</p>

<p><strong>Changing a node:</strong></p>

<pre><code>Node streamType = document.getElementsByTagName(""StreamType"").item(0);  
streamType.setTextContent(mountType);   
</code></pre>

<p><strong>Appending a child element</strong></p>

<pre><code>Node nameNode = document.createElement(""Name"");
nameNode.appendChild(document.createTextNode(""utilities""));

Node module = document.createElement(""Module"");
module.appendChild(nameNode);

Element modules = (Element)document.getElementsByTagName(""Modules"").item(0);
modules.appendChild(module);
</code></pre>

<p>Hope that info helps!</p>
","transformer-model"
"7718169","How to suppress the xml element added by Transformer of javax.xml.transform.Transformer","2011-10-10 20:17:34","7732298","1","2442","<java><xml-parsing><transformer-model>","<p>I've been using the Java APIs to parse an XML file so I could add, remove, or update elements/attributes. Everything works the way I want, except that the <code>Transformer</code> object I'm using adds <code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;</code> to the beginning of the XML file. I was wondering if there is a way to suppress this.</p>

<p>P.S. I also noticed that this top-voted <a href=""https://stackoverflow.com/questions/632043/how-do-i-extract-child-element-from-xml-to-a-string-in-java"">answer</a> mentioned that we might be able to supress it. </p>

<pre><code>DOMSource source = new DOMSource(document);
TransformerFactory tFactory = TransformerFactory.newInstance();
Transformer transformer = tFactory.newTransformer();
FileOutputStream fout = new FileOutputStream(new File(outputFile));            
StreamResult result = new StreamResult(fout);
transformer.transform(source, result);
fout.close();
</code></pre>

<p>The original document does not contain <code>&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;</code></p>
","transformer-model"
"7603382","How to unit test on Customer Transformer implementation of Solr","2011-09-29 21:04:06","7606603","0","244","<unit-testing><solr><transformer-model>","<p>How can I do unit test on Customer Transformer implementation of Solr ? Thanks</p>
","transformer-model"
"7163387","Remapping the Asus Transformer keyboard. How to insert acute accent?","2011-08-23 15:18:08","","2","734","<keyboard><character><dock><diacritics><transformer-model>","<p>I am trying to change the keyboard dock layout to that of Icelandic standard keyboard using this guide : <a href=""http://forum.xda-developers.com/showthread.php?t=1144204"" rel=""nofollow"">xda developers</a>, however im having difficulties finding the acute accent ´ that is used to type á,ó,é,í,ý,ú in Icelandic.</p>

<p>Is there anyone out there that can help me with this problem?</p>
","transformer-model"
"6941826","Java 1.6: javax.xml.transform.Transformer refuses to indent xml strings which contain newlines","2011-08-04 12:58:30","","2","5442","<java><xml><java-6><indentation><transformer-model>","<p>I need to be able to pretty print xml strings using Java APIs and have found multiple solutions for this both on the web and on this particular website. However despite multiple attempts to get this to work with javax.xml.transform.Transformer it's been a failure so far.
The code I provide below works only partially when the xml string in the argument does not contain any newlines between xml elements. This just wont do. I need to be able to pretty print anything, assuming it is well formed and valid xml, even previously pretty printed strings.</p>

<p>I got this (put together from code snippets I found, people claimed it worked for them):</p>

<pre><code>import java.io.*;
import javax.xml.transform.*;
import javax.xml.transform.stream.*;

public class XMLFormatter {

    public static String format(String xml, int indent, boolean omitXmlDeclaration)
            throws TransformerException {

        if (indent &lt; 0) {
            throw new IllegalArgumentException();
        }
        String ret = null;
        StringReader reader = new StringReader(xml);
        StringWriter writer = new StringWriter();
        try {
            TransformerFactory factory = TransformerFactory.newInstance();
            factory.setAttribute(""indent-number"", new Integer(indent));
            Transformer transformer = factory.newTransformer();
            if (omitXmlDeclaration) {
                transformer.setOutputProperty(
                        OutputKeys.OMIT_XML_DECLARATION, ""yes"");
            }
            transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
            transformer.setOutputProperty(
                    ""{http://xml.apache.org/xslt}indent-amount"",
                    String.valueOf(indent));
            transformer.setOutputProperty(OutputKeys.METHOD, ""xml"");
            transformer.transform(
                    new StreamSource(reader),
                    new StreamResult(writer));
            ret = writer.toString();
        } catch (TransformerException ex) {
            throw ex;
        } finally {
            if (reader != null) {
                reader.close();
            }
            try {
                if (writer != null) {
                    writer.close();
                }
            } catch (IOException ex) {}
        }

        return ret;
    }

    public static void main(String[] args) throws TransformerException {
        StringBuilder sb = new StringBuilder();
        sb.append(""&lt;rpc-reply&gt;&lt;data&gt;&lt;smth/&gt;&lt;/data&gt;&lt;/rpc-reply&gt;"");

        System.out.println(sb.toString());
        System.out.println();
        System.out.println(XMLFormatter.format(sb.toString(), 4, false));

        final String NEWLINE = System.getProperty(""line.separator"");
        sb.setLength(0);
        sb.append(""&lt;rpc-reply&gt;"");sb.append(NEWLINE);
        sb.append(""&lt;data&gt;"");sb.append(NEWLINE);
        sb.append(""&lt;smth/&gt;"");sb.append(NEWLINE);
        sb.append(""&lt;/data&gt;"");sb.append(NEWLINE);
        sb.append(""&lt;/rpc-reply&gt;"");

        System.out.println(sb.toString());
        System.out.println();
        System.out.println(XMLFormatter.format(sb.toString(), 4, false));
    }
}
</code></pre>

<p>This code should not be bothered by those newlines, should it? Is this a bug or am I missing something vital here? The output for the code snippet:</p>

<pre><code>&lt;rpc-reply&gt;&lt;data&gt;&lt;smth/&gt;&lt;/data&gt;&lt;/rpc-reply&gt;

&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;rpc-reply&gt;
    &lt;data&gt;
        &lt;smth/&gt;
    &lt;/data&gt;
&lt;/rpc-reply&gt;

&lt;rpc-reply&gt;
&lt;data&gt;
&lt;smth/&gt;
&lt;/data&gt;
&lt;/rpc-reply&gt;

&lt;?xml version=""1.0"" encoding=""UTF-8""?&gt;
&lt;rpc-reply&gt;
&lt;data&gt;
&lt;smth/&gt;
&lt;/data&gt;
&lt;/rpc-reply&gt;
</code></pre>

<p>As far as I can tell my code only differs from other examples in that I use StringWriter and StringReader for the transform(in, out) method. I've already tried converting the xml to a ByteArrayOutputStream and even parsing it with DOM and then feeding it to transformer but the result is the same.
I would really appreciate to know why this only works for single line strings.</p>

<p>I'm using jdk1.6_u24 combined with Netbeans 6.9.1.</p>

<p>This question is related to (and probably to a multitude of others) but not the same as:</p>

<p><a href=""https://stackoverflow.com/questions/139076/how-to-pretty-print-xml-from-java"">How to pretty print XML from Java?</a></p>

<p><a href=""https://stackoverflow.com/questions/1082886/indent-xml-text-with-transformer"">indent XML text with Transformer</a></p>

<p><a href=""https://stackoverflow.com/questions/5749217/indent-xml-made-with-transformer"">Indent XML made with Transformer</a></p>
","transformer-model"
"6883881","QCAR on ASUS Transformer (eee Pad)","2011-07-30 14:48:00","","1","432","<android><augmented-reality><transformer-model>","<p>I'm starting to develop an AR applcation and my development device is the one in the subject.
This target runs Android 3.1.</p>

<p>i covered all the steps needed to setup my environment and tried to run the Image Targets example that is supplied with the QCAR SDK but with no success.</p>

<p>Everything is complied and deployed successfully but when the application starts on the target i get an error:
""Network connection required to initialize camera settings.
Please check your connection and restart the application.
If you are still experiencing problems, then your device may not be currently supported."" 
While a network connection is present via Wi-fi to a local home network.</p>

<p>Anyone can suggest me what to do? I cannot replace my device and i must work with Quallcom's AR library.</p>

<p>Thank you very much</p>
","transformer-model"
"6820169","Formatting Generated XML in Java","2011-07-25 17:45:42","","0","176","<java><xml><dom><transformer-model>","<p>I have a generated xml using the Transformer to do the actual writing (to a StreamResult). I am trying to have it nicely generated that is more human-readable. I have inserted the code below and it does the indentation for me.</p>

<pre><code>transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
</code></pre>

<p>However, in such cases where I have attributes in a element like:</p>

<pre><code>&lt;ElementA name=""foo"" description=""bar"" value=""xyz""&gt;&lt;/ElementA&gt;
</code></pre>

<p>How can I make it to look like the one below and having the attribute listed in the order that I set them in the code using elementA.setAttribute(""name"", ""foo""); Currently it reorders itself according to alphabetical order:</p>

<pre><code>&lt;ElementA name=""foo""
          description=""bar""
          value=""xyz""&gt;

&lt;/ElementA&gt;
</code></pre>
","transformer-model"
"6739817","ADK or microbridge on ASUS Eee Pad Transformer","2011-07-18 21:43:13","","0","1158","<android><usb><arduino><transformer-model><adk>","<p>How can the <a href=""http://developer.android.com/guide/topics/usb/adk.html"" rel=""nofollow"">Android Open Accessory Development Kit</a> (ADK) or the microbridge project be made to work with the <a href=""http://en.wikipedia.org/wiki/ASUS_Eee_Pad_Transformer"" rel=""nofollow"">ASUS Eee Pad Transformer</a>?</p>

<p>I can compile and install apps on the Transformer which uses that, but when I connect the Transformer to the USB host <a href=""http://www.arduino.cc/en/Main/ArduinoShields"" rel=""nofollow"">shield</a> nothing happens, it doesn't receive any data from the Transformer. Sometimes the transformer enters in debug mode when connecting to the USB host shield.</p>

<hr>

<p>(Some context, from the first link: </p>

<blockquote>
  <p>The main hardware and software components of the ADK include: A USB micro-controller board that is based on the Arduino Mega2560 and Circuits@Home USB Host Shield designs (now referred to as the ADK board), which you will later implement as an Android USB accessory.</p>
</blockquote>

<p>)</p>
","transformer-model"
"6736942","Android Create XML on 3.1","2011-07-18 17:40:17","","1","399","<android><xml><transformer-model>","<p>I am getting this error using SDK 12.  </p>

<pre><code>Caused by: org.apache.xml.serializer.utils.WrappedRuntimeException: Could not load the propery file 'output_xml.properties' for output method 'xml' (check CLASSPATH)
</code></pre>

<p>Line 175 in my activity is <code>Transformer trans = transfac.newTransformer();</code>.</p>

<pre><code>07-18 10:32:41.695: ERROR/AndroidRuntime(12864): FATAL EXCEPTION: AsyncTask #1
07-18 10:32:41.695: ERROR/AndroidRuntime(12864): java.lang.RuntimeException: An error occured while executing doInBackground()
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at android.os.AsyncTask$3.done(AsyncTask.java:266)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at java.util.concurrent.FutureTask$Sync.innerSetException(FutureTask.java:273)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at java.util.concurrent.FutureTask.setException(FutureTask.java:124)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:307)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at java.util.concurrent.FutureTask.run(FutureTask.java:137)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1081)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:574)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at java.lang.Thread.run(Thread.java:1020)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864): Caused by: org.apache.xml.serializer.utils.WrappedRuntimeException: Could not load the propery file 'output_xml.properties' for output method 'xml' (check CLASSPATH)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at org.apache.xml.serializer.OutputPropertiesFactory.getDefaultMethodProperties(OutputPropertiesFactory.java:322)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at org.apache.xalan.templates.OutputProperties.&lt;init&gt;(OutputProperties.java:83)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at org.apache.xalan.transformer.TransformerIdentityImpl.&lt;init&gt;(TransformerIdentityImpl.java:88)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at org.apache.xalan.processor.TransformerFactoryImpl.newTransformer(TransformerFactoryImpl.java:823)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at jpl.test.AndroidserverActivity.generateXML(AndroidserverActivity.java:175)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at jpl.test.AndroidserverActivity$GetXML.doInBackground(AndroidserverActivity.java:63)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at jpl.test.AndroidserverActivity$GetXML.doInBackground(AndroidserverActivity.java:1)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at android.os.AsyncTask$2.call(AsyncTask.java:252)
07-18 10:32:41.695: ERROR/AndroidRuntime(12864):     at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:305)
</code></pre>

<p>Here is my code:</p>

<pre><code>public class AndroidserverActivity extends Activity {
    /** Called when the activity is first created. */
    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.main);

        GetXML xml = new GetXML();
        xml.execute();
    }

    private class GetXML extends AsyncTask&lt;String, Integer, String&gt; 
    {
        protected String doInBackground(String... conn) 
        {
            String UID = ""iPAD-CN5-DSS25-AC25"";
            try 
            {
                String xml = generateXML(""AC25"", ""/fa/gdscc/dss25-apc"", UID);
                if (send(""localhost"", xml))
                    getData(UID);
                else
                    System.out.println(""False"");
            } 
            catch (MalformedURLException e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            } catch (XPathExpressionException e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            } catch (IOException e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            } catch (ParserConfigurationException e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            } catch (SAXException e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            } catch (TransformerException e) {
                // TODO Auto-generated catch block
                e.printStackTrace();
            }

            return null;
        }
    }

    public static String generateXML(String conn, String funcAddr, String UID) throws ParserConfigurationException, SAXException, 
                                                                    IOException, XPathExpressionException, TransformerException 
    {   
        String url = getXML(""0"", conn);

        DocumentBuilderFactory domFactory = DocumentBuilderFactory.newInstance();
        domFactory.setNamespaceAware(true); // never forget this!
        DocumentBuilder builder = domFactory.newDocumentBuilder();
        Document doc = XMLfromString(url);

        XPath xpath = XPathFactory.newInstance().newXPath();
        XPathExpression expr = xpath.compile(""/SubscrProf/DataItem/DataItemName"");

        Object result = expr.evaluate(doc, XPathConstants.NODESET);
        NodeList nodes = (NodeList) result;

        //build xml
        Document output = builder.newDocument();

        //create root
        org.w3c.dom.Element root = output.createElement(""JMSMON2Req"");
        output.appendChild(root);

        //create subitem
        org.w3c.dom.Element subItemNode = output.createElement(""SubItem"");
        subItemNode.setAttribute(""UID"", UID);
        root.appendChild(subItemNode);

        //create funcAddr
        org.w3c.dom.Element funcAddrNode = output.createElement(""FuncAddr"");
        Text text = output.createTextNode(funcAddr);
        funcAddrNode.appendChild(text);
        subItemNode.appendChild(funcAddrNode);

        //create itemname
        for (int i = 0; i &lt; nodes.getLength(); i++) 
        {
            org.w3c.dom.Element itemNameNode = output.createElement(""ItemName"");
            text = output.createTextNode(nodes.item(i).getTextContent());
            itemNameNode.appendChild(text);
            subItemNode.appendChild(itemNameNode);
        }

        //create metadata uid
        org.w3c.dom.Element metaDataNode = output.createElement(""Metadata"");
        metaDataNode.setAttribute(""key"", ""UID"");
        text = output.createTextNode(UID);
        metaDataNode.appendChild(text);
        subItemNode.appendChild(metaDataNode);

        //create metadata conn
        org.w3c.dom.Element metaDataNode2 = output.createElement(""Metadata"");
        metaDataNode2.setAttribute(""key"", ""CONN"");
        text = output.createTextNode(""5"");
        metaDataNode2.appendChild(text);
        subItemNode.appendChild(metaDataNode2);

        /////////////////
        //Output the XML

        //set up a transformer
        TransformerFactory transfac = TransformerFactory.newInstance();
        Transformer trans = transfac.newTransformer();
        trans.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""yes"");
        trans.setOutputProperty(OutputKeys.INDENT, ""yes"");

        //create string from xml tree
        StringWriter sw = new StringWriter();
        StreamResult out = new StreamResult(sw);
        DOMSource source = new DOMSource(output);

        trans.transform(source, out);

        return URLEncoder.encode(""req"", ""UTF-8"") + ""="" + URLEncoder.encode(sw.toString(), ""UTF-8"");
    }
</code></pre>
","transformer-model"
"6601377","Is it possible to get the Android ADK working on an ASUS Eee Pad Transformer running 3.1?","2011-07-06 18:43:47","","1","608","<android><transformer-model><adk>","<p>I am running the <a href=""http://developer.android.com/guide/topics/usb/adk.html"" rel=""nofollow noreferrer"">ADK</a> demo app tweaked for Android 3.1, using the ASUS Eee Pad Transformer tablet and an ADK board, built as described in <em><a href=""http://www.googleadk.com/?page_id=17"" rel=""nofollow noreferrer"">Get Started With Android ADK (LINUX)</a></em>. </p>

<p>I seem to be having the same symptoms as described in Stack&nbsp;Overflow question <em><a href=""https://stackoverflow.com/q/6355031"">How can I get the ADK DemoKit example working on a Google I/O Galaxy Tab 10.1?</a></em>.  It could be the transformer does not support the ADK yet (as per the Galaxy tab), but is it possible to get the Transformer working somehow?</p>
","transformer-model"
"6530532","What is the Video Format for Asus Transformer?","2011-06-30 06:00:21","6530799","1","1449","<android><converters><movie><avi><transformer-model>","<p>I have an Asus Transformer and would like to put a lot of movies in it. However, it seems like it's very hard to find the proper format for it.</p>

<p>I've tried to convert the movies both in AVI and MP4 format where video codec is either MPEG4/H264/H263/XVID and audio codec is either MP3/AAC/AC3/PCM. However, despite many alternatives, none of them works and I am going insane right now.</p>

<p>At the moment, I have 2 720p movies that can be played and they are AVI with XVID codec. I tried to convert a movie with the same format container and video codec (AVI with XVID) but it does not work.</p>

<p>Do I need a special converter to make it work? Everything is just really weird.</p>

<p>Thank you so much for the help.</p>
","transformer-model"
"6380846","gcc ON arm/android","2011-06-17 02:45:37","6380914","14","18970","<android><gcc><arm><android-3.0-honeycomb><transformer-model>","<p>I just got a EEE pad transformer. Like any hardware I own I'd like to have a C compiler on it.   I know I can cross compile, but I'd like to do development ON the device itself. I've searched google and all I can seem to find are pages on how to build an arm toolchain for x86/64 Linux. Thoughts?</p>
","transformer-model"
"6291577","Premature end of file Error","2011-06-09 10:45:19","6291662","5","19228","<java><xslt><transformer-model>","<p>I am using XSL to configure my XML file into a smaller XML. My code fragments are so:</p>

<pre><code>public class MessageTransformer {

public static void main(String[] args) {
    try {
        TransformerFactory transformerFactory = TransformerFactory.newInstance();

        Transformer transformer = transformerFactory.newTransformer (new StreamSource(""sample.xsl""));
        transformer.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");
        transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
        transformer.setOutputProperty(OutputKeys.METHOD, ""xml"");
        transformer.setOutputProperty(""{http://xml.apache.org/xslt}indent-amount"", ""2"");
        transformer.transform(new StreamSource (""sample.xml""),  
            new StreamResult( new FileOutputStream(""sample.xml""))
        );
    }
    catch (Exception e) {
        e.printStackTrace( );
    }    
}   
}
</code></pre>

<p>I got this error</p>

<pre><code>ERROR:  'Premature end of file.'
ERROR:  'com.sun.org.apache.xml.internal.utils.WrappedRuntimeException: Premature end of file.'
</code></pre>

<p>When I use XSL file to transform XML manually I don' t have any problem. However with this JAVA file I cannot transform. </p>

<p>What would be the problem?</p>
","transformer-model"
"5946682","Providing parameters to XSLT program using Java API","2011-05-10 07:34:28","5948098","1","794","<java><xslt><saxon><transformer-model>","<p>What I want to do is: </p>

<pre><code>setParameter(String name, String value)
</code></pre>

<p>But the API is:</p>

<pre><code>void setParameter(QName name, XdmValue value)
</code></pre>

<p>I can't find any example to properly create XdmValue and QName, <a href=""https://stackoverflow.com/questions/3434644/saxon-9-2-java-xslt-setting-transformer-parameters-using-setparameters"">examples I found</a> are all using different versions of this function/api.</p>
","transformer-model"
"5749217","Indent XML made with Transformer","2011-04-21 20:03:45","5749451","4","4615","<java><xml><formatting><transformer-model>","<p>I am trying to create XML from Java and am having problems with indenting. In the following code you can see <code>OutputKeys.INDENT</code> set to <code>yes</code>...</p>

<pre><code>        //set up a transformer
        TransformerFactory transfac = TransformerFactory.newInstance();
        Transformer trans = transfac.newTransformer();
        trans.setOutputProperty(OutputKeys.OMIT_XML_DECLARATION, ""yes"");
        trans.setOutputProperty(OutputKeys.INDENT, ""yes"");

        //create string from xml tree
        StringWriter sw = new StringWriter();
        StreamResult result = new StreamResult(sw);
        DOMSource source = new DOMSource(doc);
        trans.transform(source, result);
        String xmlString = sw.toString();

        //print xml
        System.out.println(xmlString);
</code></pre>

<p>but it seems to have no affect, the output is:</p>

<pre><code>&lt;dataset id=""1""&gt;&lt;br&gt;
&lt;path&gt;&lt;/path&gt;&lt;br&gt;
&lt;session id=""1""&gt;&lt;br&gt;
&lt;method&gt;&lt;br&gt;
&lt;timestamp&gt;a timestamp&lt;/timestamp&gt;&lt;br&gt;
&lt;signiture&gt;&lt;br&gt;
&lt;classPath&gt;&lt;/classPath&gt;&lt;br&gt;
&lt;name&gt;methodName&lt;/name&gt;&lt;br&gt;
&lt;declarationType&gt;String&lt;/declarationType&gt;&lt;br&gt;
&lt;parameters&gt;&lt;br&gt;
&lt;parameter&gt;String&lt;/parameter&gt;&lt;br&gt;
&lt;parameter&gt;int&lt;/parameter&gt;&lt;br&gt;
&lt;/parameters&gt;&lt;br&gt;
&lt;/signiture&gt;&lt;br&gt;
&lt;arguments&gt;&lt;br&gt;
&lt;argument&gt;SomeValue&lt;/argument&gt;&lt;br&gt;
&lt;argument&gt;AnotherValue&lt;/argument&gt;&lt;br&gt;
&lt;/arguments&gt;&lt;br&gt;
&lt;return&gt;ReturnValue&lt;/return&gt;&lt;br&gt;
&lt;/method&gt;&lt;br&gt;
&lt;/session&gt;&lt;br&gt;
&lt;/dataset&gt;&lt;br&gt;
</code></pre>
","transformer-model"
"5620417","How to write contents of Document Object to String in NekoHTML?","2011-04-11 11:08:22","5842287","0","1098","<java><html-parsing><transformer-model><neko>","<p>I am using NekoHTML to parse contents of some HTML file..</p>

<p>Everything goes okay except for extracting the contents of the Document Object to some string.</p>

<p>I've tried uses</p>

<pre><code>TransformerFactory transformerFactory = TransformerFactory.newInstance();
Transformer transformer = transformerFactory.newTransformer();
DOMSource source = new DOMSource(doc);
StreamResult result = new StreamResult(writer);
transformer.transform(source, result);
</code></pre>

<p>But nothing appears returned.</p>
","transformer-model"
"4988114","java standard lib produce wrong xml 1.1","2011-02-14 01:24:06","","4","575","<java><xml><document><transformer-model><xml-1.1>","<p>I found this interesting problem last week. Run the program below. It's very simple, first create a dummy xml file, and read it with standard lib and write it back to a file.</p>

<p>Look through the generated gtest2.xml, you will see that it has some content that were come out of nowhere. </p>

<p>In my case, this is the sample of wrong section (the place vary on different machine). </p>

<pre><code>&lt;test&gt;1924&lt;/test&gt;
&lt;test&gt;1925&lt;/test&gt;
&lt;test&gt;t&amp;gt;24&lt;/test&gt;
&lt;test&gt;1927&lt;/test&gt;
&lt;test&gt;1928&lt;/test&gt;
&lt;test&gt;1929&lt;/test&gt;
</code></pre>

<p>This does not happen if I changed my xml version to 1.0. So something wrong with my code or jdk?</p>

<p>Here is the test code:</p>

<pre><code>import java.io.File;
import java.io.PrintWriter;

import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.transform.OutputKeys;
import javax.xml.transform.Transformer;
import javax.xml.transform.TransformerFactory;
import javax.xml.transform.dom.DOMSource;
import javax.xml.transform.stream.StreamResult;

import org.w3c.dom.Document;

public class DocumentBuilderCheck {

    public static void main(String[] args) throws Exception {
        String filename = ""/tmp/gtest.xml"";
        generateXmlFile(filename, 2500);
        Document doc = readXmlFile(filename);

        String filename2 = ""/tmp/gtest2.xml"";
        writeDocument(doc, filename2);
    }

    private static void writeDocument(Document document, String filename) throws Exception {
        StreamResult streamResult = new StreamResult(filename);
        TransformerFactory transformerFactory = TransformerFactory.newInstance();
        Transformer transformer = transformerFactory.newTransformer();
        transformer.setOutputProperty(OutputKeys.INDENT, ""yes"");
        transformer.setOutputProperty(OutputKeys.METHOD, ""xml"");
        transformer.transform(new DOMSource(document), streamResult);
    }

    private static Document readXmlFile(String filename) throws Exception {
        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();
        dbf.setNamespaceAware(true);
        DocumentBuilder db = dbf.newDocumentBuilder();
        Document doc = db.parse(new File(filename));
        return doc;
    }

    private static void generateXmlFile(String filename, int total)
            throws Exception {
        File f = new File(filename);

        PrintWriter pw = new PrintWriter(f);
        pw.write(""&lt;?xml version=\""1.1\"" encoding=\""UTF-8\""?&gt;"");
        pw.write(""&lt;main_tag&gt;"");
        for (int i = 0; i &lt; total; i++) {
            pw.write(""&lt;test&gt;"" + String.format(""%04d"", i) + ""&lt;/test&gt;"");
        }
        pw.write(""&lt;/main_tag&gt;"");
        pw.close();
    }
}
</code></pre>
","transformer-model"
"4714363","Java: Commons-Collections generics: How to get custom transformer to work","2011-01-17 14:40:11","4714771","3","4013","<java><generics><transformer-model><apache-commons-collection>","<p>Hi i am using commons collections generics 4.01. </p>

<p>I have a dto object. </p>

<pre><code>Class PricingDto {
   private Double tax;
   private Double price;
   private Double tip;

   // getters and setters
}
</code></pre>

<p>i have a list of <code>List&lt;PricingDto&gt; pricingDtos = this.pricingService.getAllPricings();</code></p>

<p>than i have a private static class. </p>

<pre><code>import org.apache.commons.collections15.Transformer;
import org.apache.commons.collections15.list.TransformedList;

class TotalServiceImpl implements TotalService {
    public static final PricingDtoTransformer PRICING_DTO_TRANSFORMER =
        new PricingDtoTransformer();
    private static class PricingDtoTransformer
        implements Transformer&lt;PricingDto, Double&gt; {
        public PricingDtoTransformer() {}

        @Override
        public Double transform(final PricingDto pricingDto) {
            return pricingDto.getTax()
                     + pricingDto.getPrice()
                     + pricingDto.getTips();
        }
    }

    @Override
    public List&lt;Double&gt; getListDouble(final List&lt;PricingDto&gt; pricingDtos) {
        final List&lt;Double&gt; totalList = 
            TransformedList.decorate(pricingDtos, PRICING_DTO_TRANSFORMER);
            for (Double d : totalList) {
                // print them. 
            }
        }
    }
}
</code></pre>

<p>My problem is i get class cast exception, because each item in totalList is a PricingDto and not Double.  </p>

<p>2.) What did i do wrong. Whats the correct way to implement custom transformer for generics commons-collections. </p>
","transformer-model"
"4552562","Java xml Transformer to escape &","2010-12-29 08:26:20","4552660","2","3630","<java><xml><ampersand><transformer-model>","<p>I am having a problem with <code>javax.xml.transform.Transformer</code>. </p>

<p>I am trying to create a XML document and one of the attributes is HTTP link which contains <code>&amp;</code> for query. After I've invoked the <code>transform()</code> method, all the <code>&amp;</code> characters become <code>&amp;amp;</code>. </p>

<p>So is there any way that I can configure Transformer not to convert <code>&amp;</code> to <code>&amp;amp;</code>? Thanks in advance.</p>

<p>Edit</p>

<hr>

<p>In the xml document that I am trying to create, one of the attributes is a http link with query string. So what I would like to have as a result is something like</p>

<pre><code>&lt;Notification url=""http://www.xyz.com/notify.jsp?param1=123&amp;param2=345/&gt;
</code></pre>

<p>But I am getting now is </p>

<pre><code>&lt;Notification url=""http://www.xyz.com/notify.jsp?param1=123&amp;amp;param2=345/&gt;
</code></pre>

<p>Because I have to transform the xml document into a String and send it over socket. So I do not want <code>&amp;</code> converted to <code>&amp;amp;</code> as final result. </p>
","transformer-model"
"4265526","java errorlistener use on a transformer instance(jaxp)","2010-11-24 10:02:53","4266804","2","927","<java><transform><jaxp><transformer-model>","<p>The signature for method transform of <code>Transformer</code> is <code>void  transform(Source xmlSource,Result outputTarget)throws TransformerException</code> i.e. specifies that it
throws a checked exception of type TransformerException.<br>
I can not understand how <code>ErrorListener</code> fits in.<br>
If an <code>ErrorListener</code> is set to <code>Transform</code> object does this mean that the exception will not be thrown?<br>
If it is thrown anyway how is an ErrorListener useful?</p>

<p>Thanks</p>
","transformer-model"
"3851715","How can you use sort descriptors and predicates with core data transformable fields?","2010-10-03 21:41:41","3887087","2","1341","<iphone><core-data><nspredicate><nssortdescriptor><transformer-model>","<p>I have an application which I wrote which used to work perfectly fine. There is new requirement that all data needs to be encrypted now, so I implemented encryption by making all my core data fields type transformable and writing custom transformers for each data type which encrypts/decrypts each element of data as it is written/read from the core data store.</p>

<p>The encryption works fine and I can see all the data. The problem is that sorting appears to be broken as does any even slightly complicated predicate (the ones that include subqueries). </p>

<p>I'm guessing that the sorting is being done on the values before they are sent through the transformer (namely, sorting is being done on the encrypted values). Is there a way I can get around this? I suppose I can try using a sort descriptor and specify my own selector to do the comparison and explicitly decrypt the values first. I'll post here if that works.</p>

<p>The predicate situation is a bigger problem though. It's strange that it seems to mostly work but then fails when I do subqueries (which are like joins across two objects in a relationship). Is there a known problem when using transformable values and predicates or maybe I have a bug in my transformers?</p>

<p>Here is an example of a predicate that no longer works:</p>

<pre><code>[NSPredicate predicateWithFormat:@""isdeleted == NO AND (SUBQUERY(appuserMessages, $am, $am.recAppUserID == %@ AND $am.isTrash == NO).@count &gt; 0)"", appuserid];
</code></pre>

<p>The predicate is performed on the Messages object which has a one-to-many relationship with AppuserMessages. This predicate is supposed to return all messages that are not isdeleted and have at least one appuserMessage where the recAppUserID is appuserid and isTrash is false. It used to work, but now returns nothing.</p>
","transformer-model"
"3751753","How to prevent xml transformer to transform empty tags into single tag","2010-09-20 13:08:22","","11","4699","<java><xml><transformer-model>","<p>I'm using <code>javax.xml.transform.Transformer</code> class to transform the DOM source into XML string. I have some empty elements in DOM tree, and these become one tag which I don't want. </p>

<p>How do I prevent <code>&lt;sampletag&gt;&lt;/sampletag&gt;</code> from becoming <code>&lt;sampletag/&gt;</code>?</p>
","transformer-model"
"3434644","Saxon 9.2 / Java / XSLT: setting transformer parameters using setParameters()","2010-08-08 14:20:09","3436311","0","4923","<java><xslt><saxon><transformer-model>","<p>I have the following XSLT 2.0 template:</p>

<pre><code>&lt;xsl:template name=""t1""&gt;
&lt;xsl:variable name=""totalpos"" as=""xsd:double"" select=""$currentTotal""/&gt;
..  
</code></pre>

<p>I am struggling to programmatticaly provide <code>currentTotal</code> as a parameter to the transformer, like this:</p>

<pre><code>transformer.setParameter(""currentTotal"", new Double(""100""))
</code></pre>

<p>.. but without any positive results:</p>

<blockquote>
  <p>Error at /xsl:transform/xsl:template[3]/xsl:variable[1]    XPST0008:
  XPath syntax error at char 13 on line -1 in {$currentTotal}:
      Variable $currentTotal has not been declared</p>
</blockquote>

<p>When calling <code>setParameter()</code>, the <code>currentTotal</code> variable will also get defined, right?
How should I invoke the <code>setParameter()</code> call so that the <code>currentTotal</code> defined in my application will be seen inside the style-sheet? </p>

<p>For clarification, I am instantiating the transformer like this:</p>

<pre><code>System.setProperty(""javax.xml.transform.TransformerFactory"", ""net.sf.saxon.TransformerFactoryImpl"");
transformerFactory = new TransformerFactoryImpl();
transformer = transformerFactory.newTransformer(inputNodes);
</code></pre>
","transformer-model"
"3029400","How to force javax xslt transformer to encode national characters using utf-8 and not html entities?","2010-06-12 16:51:40","3029456","0","6559","<java><xslt><utf-8><entities><transformer-model>","<p>I'm working on filter that should transform an output with some stylesheet. Important sections of code looks like this:</p>

<pre><code>PrintWriter out = response.getWriter();
...
StringReader sr = new StringReader(content);
Source xmlSource = new StreamSource(sr, requestSystemId);
transformer.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");
transformer.setParameter(""encoding"", ""UTF-8"");
//same result when using ByteArrayOutputStream xo = new java.io.ByteArrayOutputStream();
StringWriter xo = new StringWriter();
StreamResult result = new StreamResult(xo);
transformer.transform(xmlSource, result);
out.write(xo.toString());
</code></pre>

<p>The problem is that national characters are encoded as html entities and not by using UTF. Is there any way to force transformer to use UTF-8 instead of entities?</p>
","transformer-model"
"1059854","How Do You Prevent A javax Transformer From Escaping Whitespace?","2009-06-29 18:51:11","1066380","2","4686","<java><entity><escaping><transformer-model><xslt>","<p>I'm using the javax.xml.transform.Transformer class to perform some XSLT translations, like so:</p>

<pre><code>TransformerFactory factory = TransformerFactory.newInstance();
StreamSource source = new StreamSource(TRANSFORMER_PATH);
Transformer transformer = factory.newTransformer(source);
StringWriter extractionWriter = new StringWriter();
String xml = FileUtils.readFileToString(new File(sampleXmlPath));
transformer.transform(new StreamSource(new StringReader(xml)),
        new StreamResult(extractionWriter));
System.err.println(extractionWriter.toString());
</code></pre>

<p>However, no matter what I do I can't seem to avoid having the transformer convert any tabs that were in the source document in to their character entity equivalent (<code>&amp;#9;</code>).  I have tried both:</p>

<pre><code>transformer.setParameter(""encoding"", ""UTF-8"");
</code></pre>

<p>and:</p>

<pre><code>transformer.setOutputProperty(OutputKeys.ENCODING, ""UTF-8"");
</code></pre>

<p>but neither of those help.  Does anyone have any suggestions?  Because:</p>

<pre><code>&amp;#9;&amp;#9;&amp;#9;&amp;#9;&amp;#9;&lt;MyElement&gt;
</code></pre>

<p>looks really stupid (even if it does work).</p>
","transformer-model"
"369760","java append to file","2008-12-15 21:25:03","369807","3","4347","<java><xml><xslt><jaxp><transformer-model>","<p>I googled for this for a while but can't seem to find it and it should be easy. I want to append a CR to then end of an XML file that I am creating with a Transformer. Is there a way to do this></p>

<p>I tried the following but this resulted in a blank file?</p>

<pre><code>
Transformer xformer = TransformerFactory.newInstance().newTransformer();
xformer.setOutputProperty(OutputKeys.DOCTYPE_SYSTEM, ""file:///ReportWiz.dtd"");
xformer.transform(source, result);
OutputStream writer = new FileOutputStream(file);
Byte b = '\n';
writer.write(b);
writer.close();</code></pre>
","transformer-model"
"243912","GZip HttpResponse using XSL Transformer","2008-10-28 16:15:55","","-1","725","<java><xslt><gzip><servlets><transformer-model>","<p>I have the following code below in my Servlet, but when IE hits the page, it returns a blank html page.  If I use the response.getOutputStream() directly in the StreamResult constructor, the page loads fine.  What am I missing?</p>

<p><strong>response</strong> is an instance of HttpServletResponse and <strong>xsl</strong> is an instance of Transformer from XSLTC TransformerFactory</p>

<pre><code>response.setHeader(""Content-Encoding"", ""gzip"");
GZIPOutputStream gzipOut = new GZIPOutputStream(response.getOutputStream());
Result outputResult = new StreamResult(gzipOut);

xsl.transform(xmlSource, outputResult);
</code></pre>
","transformer-model"