Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"78873680","How to create a function call tool to analyse CSV in LLM Long-chain with all columns definition predefined","2024-08-15 04:55:11","","0","20","<python><openai-api><langchain><large-language-model><gpt-4>","<p><em><strong>Context</strong></em></p>
<p>I would like to create an GPT Agent. This Agent should be able to fetch data from 3 predefined CSV files. Hence I would like to create 3 function tools for each CSV file.</p>
<p><em><strong>Expectation</strong></em></p>
<p>AI Agent should understand what does each CSV do exactly. And what are the columns and data exactly agent can get from each CSV file.</p>
<p>Because, I think providing the CSV structure and column definitions when creating the tool will make the LLM result much more efficient and accurate.</p>
<p><em><strong>Problem Statement</strong></em></p>
<p>How to properly define the CSV structure and column definitions?</p>
<p>The code is I tried is listed below.</p>
<pre><code>csv1_inspection_tool = StructuredTool.from_function(
    func=get_first_n_rows,
    name=&quot;InspectCSVFile&quot;,
    description=&quot;Explore the contents and structure of a table document, displaying its column names and the first n rows, with n defaulting to 3.&quot;,
)
</code></pre>
<pre><code>import pandas as pd

def get_csv_filename(
        filename: str
    ) -&gt; str:
    &quot;&quot;&quot;Get CSV file name&quot;&quot;&quot;
    # Read the CSV file
    csv_file = pd.read_csv(filename)
    
    # Since there's no sheet name, we just return the filename
    return f&quot;The file name of the CSV is '{filename}'&quot;


def get_column_names(filename: str) -&gt; str:
    &quot;&quot;&quot;Get all column names from a CSV file&quot;&quot;&quot;

    # Read the CSV file
    df = pd.read_csv(filename)

    column_names = '\n'.join(df.columns.to_list())

    result = f&quot;The File '{filename}' has columns:\n\n{column_names}&quot;
    return result


def get_first_n_rows(
        filename: str,
        n: int = 3
) -&gt; str:
    &quot;&quot;&quot;Get CSV File First N Lines&quot;&quot;&quot;

    result = get_csv_filename(filename) + &quot;\n\n&quot;

    result += get_column_names(filename) + &quot;\n\n&quot;

    df = pd.read_csv(filename)  

    n_lines = '\n'.join(
        df.head(n).to_string(index=False, header=True).split('\n')
    )

    result += f&quot;This file '{filename}' has first {n} lines of sample:\n\n{n_lines}&quot;
    return result
</code></pre>
","gpt-4"
"78872743","Which is better GPT 4.0 or 4o?","2024-08-14 19:58:48","","0","31","<openai-api><gpt-4>","<p>I am currently evaluating two versions of GPT models, GPT-4 and GPT-4o, and I wanted to understnad from the community on which model is more reliable for text-only applications. I am not interested in image generation or document/speech analysis.</p>
<p>My focus is on the following things:</p>
<ul>
<li>Hyperparameters and their impact on performance</li>
<li>Ability to avoid hallucinations</li>
<li>Understanding and validity of output</li>
</ul>
<p>which model is more reliable for generating accurate and coherent text based on your experience or any available benchmarks?</p>
","gpt-4"
"78848493","Skill Alexa + GPT4 with Vector Store","2024-08-08 12:44:58","","0","20","<python><alexa-skills-kit><openai-api><gpt-4><vectorstore>","<p>I'm trying to modify Python code to invoke a custom assistant and its respective Vector Store in <a href=""https://platform.openai.com/playground/assistants"" rel=""nofollow noreferrer"">playground/assistant</a> on OpenAI.</p>
<p>The idea is to be able to invoke a custom assistant from OpenAI, fed with data contained in its respective Vector Store, and to be able to talk to this data through the Echo Dot.</p>
<p>After reading the <a href=""https://platform.openai.com/docs/assistants/tools/file-search/vector-stores"" rel=""nofollow noreferrer"">documentation</a>, I saw that I could use the assistant_id parameter to call a custom assistant... But it gave me a &quot;Response Error&quot; response. I haven't found a solution yet.</p>
<p>The original project is by Alexandre, it is at: <a href=""https://github.com/alexandremendoncaalvaro/skill-alexa-chatgpt4"" rel=""nofollow noreferrer"">https://github.com/alexandremendoncaalvaro/skill-alexa-chatgpt4</a>. But he's Brazilian and doesn't respond.</p>
<p>Below is my modified code for your review. Thank you very much in advance:</p>
<pre><code>import os
import logging
import ask_sdk_core.utils as ask_utils
from ask_sdk_core.skill_builder import SkillBuilder
from ask_sdk_core.dispatch_components import AbstractRequestHandler
from ask_sdk_core.dispatch_components import AbstractExceptionHandler
from ask_sdk_core.handler_input import HandlerInput
from ask_sdk_model import Response
import openai

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

openai_api_key = &quot;INSERT-APIKEY-OPENAI-HERE&quot;
openai.api_key = openai_api_key

# Custom wizard parameters
custom_assistant_id = &quot;YOUR_ASSISTANT_ID&quot;
vector_store_id = &quot;ID_DO_VECTOR_STORE&quot;  # If applicable

messages = [
    {
        &quot;role&quot;: &quot;system&quot;,
        &quot;content&quot;: &quot;ALWAYS consult your knowledge base to provide correct answers.&quot;,
    }
]

class LaunchRequestHandler(AbstractRequestHandler):
    def can_handle(self, handler_input):
        # type: (HandlerInput) -&gt; bool
        return ask_utils.is_request_type(&quot;LaunchRequest&quot;)(handler_input)

    def handle(self, handler_input):
        # type: (HandlerInput) -&gt; Response
        speak_output = &quot;Here is your assistant! What is your question?&quot;
        return handler_input.response_builder.speak(speak_output).ask(speak_output).response

class GptQueryIntentHandler(AbstractRequestHandler):
    def can_handle(self, handler_input):
        # type: (HandlerInput) -&gt; bool
        return ask_utils.is_intent_name(&quot;GptQueryIntent&quot;)(handler_input)

    def handle(self, handler_input):
        # type: (HandlerInput) -&gt; Response
        query = handler_input.request_envelope.request.intent.slots[&quot;query&quot;].value
        response = generate_gpt_response(query)
        return handler_input.response_builder.speak(response).ask(&quot;You can ask a new question or say: leave.&quot;).response

def generate_gpt_response(query):
    try:
        messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: query})
        response = openai.ChatCompletion.create(
            model=&quot;gpt-4-mini&quot;, 
            messages=messages,
            max_tokens=700,
            temperature=0.8
        )
        reply = response.choices[0].message['content']
        messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: reply})
        return reply
    except Exception as e:
        return f&quot;Error generating response: {str(e)}&quot;

class HelpIntentHandler(AbstractRequestHandler):
    def can_handle(self, handler_input):
        # type: (HandlerInput) -&gt; bool
        return ask_utils.is_intent_name(&quot;AMAZON.HelpIntent&quot;)(handler_input)

    def handle(self, handler_input):
        # type: (HandlerInput) -&gt; Response
        speak_output = &quot;How can I help you?&quot;
        return handler_input.response_builder.speak(speak_output).ask(speak_output).response

class CancelOrStopIntentHandler(AbstractRequestHandler):
    def can_handle(self, handler_input):
        # type: (HandlerInput) -&gt; bool
        return ask_utils.is_intent_name(&quot;AMAZON.CancelIntent&quot;)(handler_input) or ask_utils.is_intent_name(&quot;AMAZON.StopIntent&quot;)(handler_input)

    def handle(self, handler_input):
        # type: (HandlerInput) -&gt; Response
        speak_output = &quot;At√© logo!&quot;
        return handler_input.response_builder.speak(speak_output).response

class SessionEndedRequestHandler(AbstractRequestHandler):
    def can_handle(self, handler_input):
        # type: (HandlerInput) -&gt; bool
        return ask_utils.is_request_type(&quot;SessionEndedRequest&quot;)(handler_input)

    def handle(self, handler_input):
        # type: (HandlerInput) -&gt; Response
        return handler_input.response_builder.response

class CatchAllExceptionHandler(AbstractExceptionHandler):
    def can_handle(self, handler_input, exception):
        # type: (HandlerInput, Exception) -&gt; bool
        return True

    def handle(self, handler_input, exception):
        # type: (HandlerInput, Exception) -&gt; Response
        logger.error(exception, exc_info=True)
        speak_output = &quot;Sorry, I was unable to process your request.&quot;
        return handler_input.response_builder.speak(speak_output).ask(speak_output).response

sb = SkillBuilder()
sb.add_request_handler(LaunchRequestHandler())
sb.add_request_handler(GptQueryIntentHandler())
sb.add_request_handler(HelpIntentHandler())
sb.add_request_handler(CancelOrStopIntentHandler())
sb.add_request_handler(SessionEndedRequestHandler())
sb.add_exception_handler(CatchAllExceptionHandler())

lambda_handler = sb.lambda_handler()
</code></pre>
","gpt-4"
"78846004","How can I use structured_output with Azure OpenAI with the openai Python library?","2024-08-07 23:14:19","","0","247","<python><nlp><azure-openai><gpt-4>","<p>I want to use structured output with Azure OpenAI.</p>
<p>I tried the following code, based on the code given in <a href=""https://openai.com/index/introducing-structured-outputs-in-the-api/"" rel=""nofollow noreferrer"">https://openai.com/index/introducing-structured-outputs-in-the-api/</a>:</p>
<pre><code>from pydantic import BaseModel
from openai import AzureOpenAI

class Step(BaseModel):
    explanation: str
    output: str


class MathResponse(BaseModel):
    steps: list[Step]
    final_answer: str


client = AzureOpenAI(api_key='[redacted]',
                     api_version='2024-05-01-preview',
                     azure_endpoint='[redacted]')

completion = client.beta.chat.completions.parse(
    model=&quot;gpt-4omini-2024-07-18-name&quot;,
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful math tutor.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;solve 8x + 31 = 2&quot;},
    ],
    response_format=MathResponse,
)

message = completion.choices[0].message
if message.parsed:
    print(message.parsed.steps)
    print(message.parsed.final_answer)
else:
    print(message.refusal)
</code></pre>
<p>I get the error:</p>
<pre><code>openai.BadRequestError: Error code: 400:
{
    &quot;error&quot;: {
        &quot;message&quot;: &quot;Invalid parameter: response_format must be one of json_object, text.&quot;,
        &quot;type&quot;: &quot;invalid_request_error&quot;,
        &quot;param&quot;: &quot;response_format&quot;,
        &quot;code&quot;: &quot;None&quot;
    }
}
</code></pre>
<p>How to fix it?</p>
<p>I ran <code>pip install -U openai</code>: I use <code>openai==1.40.1</code> and Python 3.11.</p>
<hr />
<p>I also tried <a href=""https://cookbook.openai.com/examples/structured_outputs_intro"" rel=""nofollow noreferrer"">https://cookbook.openai.com/examples/structured_outputs_intro</a> using  using Azure+ GPT-4o mini (2024-07-18), it didn't work either, same error message:</p>
<pre><code>from openai import AzureOpenAI

# Replace these variables with your Azure OpenAI endpoint and API key
endpoint = &quot;https://&lt;your-resource-name&gt;.openai.azure.com&quot;
api_key = &quot;&lt;your-api-key&gt;&quot;
deployment_name = &quot;&lt;your-deployment-name&gt;&quot; # Replace with your deployment name
MODEL = deployment_name

# API endpoint for the completion request
api_url = f&quot;{endpoint}/openai/deployments/{deployment_name}/chat/completions?api-version=2024-06-01&quot;


client = AzureOpenAI(api_key='[redacted]',
                     api_version='2024-07-01-preview',
                     azure_endpoint='https://[redacted].openai.azure.com/')

math_tutor_prompt = '''
    You are a helpful math tutor. You will be provided with a math problem,
    and your goal will be to output a step by step solution, along with a final answer.
    For each step, just provide the output as an equation use the explanation field to detail the reasoning.
'''

def get_math_solution(question):
    response = client.chat.completions.create(
    model=MODEL,
    messages=[
        {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: math_tutor_prompt
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: question
        }
    ],
    response_format={
        &quot;type&quot;: &quot;json_schema&quot;,
        &quot;json_schema&quot;: {
            &quot;name&quot;: &quot;math_reasoning&quot;,
            &quot;schema&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;steps&quot;: {
                        &quot;type&quot;: &quot;array&quot;,
                        &quot;items&quot;: {
                            &quot;type&quot;: &quot;object&quot;,
                            &quot;properties&quot;: {
                                &quot;explanation&quot;: {&quot;type&quot;: &quot;string&quot;},
                                &quot;output&quot;: {&quot;type&quot;: &quot;string&quot;}
                            },
                            &quot;required&quot;: [&quot;explanation&quot;, &quot;output&quot;],
                            &quot;additionalProperties&quot;: False
                        }
                    },
                    &quot;final_answer&quot;: {&quot;type&quot;: &quot;string&quot;}
                },
                &quot;required&quot;: [&quot;steps&quot;, &quot;final_answer&quot;],
                &quot;additionalProperties&quot;: False
            },
            &quot;strict&quot;: True
        }
    }
    )

    return response.choices[0].message


# Testing with an example question
question = &quot;how can I solve 8x + 7 = -23&quot;

result = get_math_solution(question)

print(result.content)
</code></pre>
","gpt-4"
"78823867","Why is occurrence of ** for headings in GPT 4o responses not getting suppressed?","2024-08-02 05:18:28","","0","22","<openai-api><prompt><gpt-4>","<p>I am getting response for queries using API that might have headings in answers for GPT 4o. Like below:
**Heading**:\n - Details on the heading‚Ä¶</p>
<p>But I do not want these headings within asterisks or any other formatting.</p>
<p>Adding prompt for the same, code changes - Neither is working. The asterisk formatting is not getting removed.</p>
<p>Is it not possible to remove these asterisks in GPT 4o? If yes kindly suggest the solution.</p>
<p>Thanks in advance.</p>
<p>I have tried lots of prompts like -</p>
<ol>
<li>Please ensure that all headers are formatted without any asterisks. Use simple, plain text for headers. For example Do not give Heading: , Heading- , etc as header, instead give Heading: , Heading- , etc in response.</li>
<li>Please generate a response using plain text only. Do not use any special characters such as asterisks, underscores, or any other symbols for headers, emphasis or formatting. Ensure all text is in standard, unformatted form.</li>
<li>Do not provide *text1*, **text2** i.e. words within special symbols in response. Always provide response as text1, text2 without any special symbols and in plain text format only.</li>
</ol>
<p>And many other prompts. None of the prompts are able to suppress these asterisk</p>
<p>Also tried suppressing it in code using logit_bias</p>
<pre><code>logit_bias = {
334: -100, # ‚Äú**‚Äù
3146: -100, # &quot; **&quot;
}
</code></pre>
<p>Neither is working.</p>
","gpt-4"
"78822306","How to replicate gpt-4o-mini playground results in api on image input?","2024-08-01 17:29:44","","0","137","<python><image><base64><openai-api><gpt-4>","<h2>The problem</h2>
<p>I am using system prompt + user image input prompt to generate text output using gpt4o-mini. I'm getting satisfactory results when I attempt this on the chat playground UI. But the same thing, when done programmatically using python API, gives me poor results.</p>
<p>My suspicion is that openAI uses some kind of image transformation and compression on their end before inference which I'm not replicating. But I have no idea what that is. My image is 1080 x 40,000. (It's a screenshot of an entire webpage). But the playground model is very easily able to find my needles in a haystack.</p>
<h3>My workflow</h3>
<p>Here's my process:</p>
<ol>
<li>Getting the screenshot</li>
</ol>
<pre><code>google-chrome --headless --disable-gpu --window-size=1024,40000
--screenshot=destination.png  source.html
</code></pre>
<ol start=""2"">
<li>convert to base64</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>def encode_image(image_path):
  with open(image_path, &quot;rb&quot;) as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')
</code></pre>
<ol start=""3"">
<li>get response</li>
</ol>
<pre class=""lang-py prettyprint-override""><code>data_uri_png = f&quot;data:image/png;base64,{base64_encoded_png}&quot;
response = client.chat.completions.create(
    model=&quot;gpt-4o-mini&quot;,
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: query},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [
            {
                &quot;type&quot;: &quot;image_url&quot;,
                &quot;image_url&quot;: 
                    {&quot;url&quot;: data_uri_png 
                }
            }
        ]}
    ]
)
</code></pre>
<h3>What I've tried</h3>
<ul>
<li>converting the picture to a jpeg and decreasing quality to 70%</li>
<li>chunking the image into smaller images 1080 x 4000 images and uploading multiple as input prompt</li>
</ul>
<p>What am I missing here?</p>
","gpt-4"
"78821977","How to Enforce Context-Only Responses in a Conversational Retrieval Chain with LangChain and OpenAI GPT-4?","2024-08-01 16:09:31","","0","14","<openai-api><langchain><huggingface><chromadb><gpt-4>","<p>I'm working on a Django project where I need to create a conversational retrieval system using LangChain with OpenAI's GPT-4 model. The goal is to ensure that the model only answers questions based on the provided document context. If the relevant information is not in the context, the model should respond with a message indicating the lack of access to relevant files.</p>
<p>here is a simplified version of my setup</p>
<pre><code>from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import Chroma
from langchain_huggingface.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.schema import Document

embeddings = HuggingFaceEmbeddings(model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;)
chroma_db = Chroma(embeddings=embeddings, collection_name=&quot;AEM_Responses&quot;)

def handle_query_flow(chroma_db, email):
    system_prompt = &quot;&quot;&quot;You are an AI system that answers questions based on documents. 
                    Answer only using the provided context. If the information is not in the context, 
                    respond with: 'User does not have access to any relevant files. 
                    Please provide more context or check your access permissions.' 
                    Do not search outside the given context. Do not use any outside knowledge or 
                    provide an answer from your own training data. Only use the provided context.
                    If you get empty context then just say that this user does not have access to any files.
                    &quot;&quot;&quot;

    chain = ConversationalRetrievalChain.from_llm(
        ChatOpenAI(temperature=0.0, model_name='gpt-4-turbo'),
        chroma_db.as_retriever(search_kwargs={'filter': {'email': email}}),
        memory=None
    )

    chat_history = []

    while True:
        query = input(&quot;Please enter your query (type 'quit' to exit): &quot;).strip()
        if query.lower() == 'quit':
            break

        retriever = chroma_db.as_retriever(search_kwargs={'filter': {'email': email}})
        retrieved_docs = retriever.get_relevant_documents(query)
        count_retrieved_docs = len(retrieved_docs)
        
        response = chain.invoke({'question': query, 'system_prompt': system_prompt, 'chat_history': chat_history})
        answer = response.get('answer', '')
        print(f&quot;Answer: {answer}&quot;)
        print(&quot;CONTEXT DOCUMENTS&quot;)
        print(retrieved_docs)

    return JsonResponse({&quot;message&quot;: &quot;Query session ended&quot;}, status=200) ```





Problem:
Despite setting up a system prompt and the ConversationalRetrievalChain, the model sometimes provides answers based on general knowledge instead of strictly using the provided context. For instance, when a user with no relevant documents in context asks a question, the model still responds with an answer rather than indicating the absence of relevant documents.

What I've Tried:

System Prompt: I've clearly stated that the model should not use outside knowledge and should only answer based on the provided context.
Checking Retrieved Documents: Before generating a response, I check if any meaningful documents are retrieved.
Questions:

How can I ensure that the model strictly adheres to the provided context and does not use any outside knowledge?
Is there a way to programmatically enforce this restriction, ensuring that if no relevant context is found, the model responds with a predefined message?
Are there any best practices or specific configurations in LangChain or with OpenAI models that I might be missing?
Any insights or suggestions would be greatly appreciated!
</code></pre>
","gpt-4"
"78765261","Azure.AI.OpenAI - GetChatCompletionsStreaming - Send and Receive Multiple Messages In the Same Context","2024-07-18 15:07:51","","0","67","<c#><azure><azure-openai><gpt-4><conversation-scope>","<p>I have a use case that is very simple but is proving quite difficult, I want to create a programmatic application that sends and receives a series of messages in the same context to a GPT model hosted on Azure Open AI, rough sketched this would look something like:</p>
<pre><code>Uri azureOpenAIResourceUri = new Uri(azureOpenAIResourceUriEndpoint);
AzureKeyCredential azureOpenAIApiKeyCredential = new AzureKeyCredential(azureOpenAIApiKey);
OpenAIClient client = new OpenAIClient(azureOpenAIResourceUri, azureOpenAIApiKeyCredential);
ChatCompletionsOptions chatCompletionsOptions = GenerateInitialChatCompletionsOptions(AITemplate, customAITemplate, query);

//Send message: &quot;What is 1 + 1&quot;

//Get answer, print to user

//Send message: &quot;Can you add 2 to the previous answer&quot;

//Get answer, print to user

//etc...
</code></pre>
<p>Apparently, searching the internet, I've discovered this is a non trivial use case(because I haven't found anyone else doing it), I can do the first part using the following code:</p>
<pre><code>await client.GetChatCompletionsAsync(chatCompletionsOptions)
</code></pre>
<p>But after I receive the first reply I don't know how to reply to that reply. I don't see a way to obtain the conversation &quot;Context&quot; and I don't see a method or property of any of the related objects that would allow me to &quot;reply with current context&quot;.</p>
<p>There is a <a href=""https://stackoverflow.com/questions/77705879/how-to-apply-streaming-in-azure-openai-dotnet-web-application"">number of posts</a> floating around that use &quot;GetChatCompletionsStreaming&quot; but I have yet to see a example of replying to the completion of the first message a additional message in the same context.</p>
<p>If you want a good use case to get GPT-4 to hallucinate, ask it this exact question, it will provide you profoundly misleading and totally incorrect code. To be clear allot of this may be down to the &quot;Azure.AI.OpenAI&quot; version I am using, I'm currently using &quot;1.0.0-beta.14&quot; if that is not the optimal version please include that in your answer.</p>
<p>I have observed syntactically incompatible answers to various related questions, I would infer from that, that the API's are changing quite rapidly between releases and this is one of the reasons why GPT find's it so difficult to generate a reasonable and compatible answer.</p>
<p>How do I, in the version of the library referenced above, in C#, interact with a chatbot sending and receiving multiple messages in the same context.</p>
","gpt-4"
"78755971","Open AI Assistant file parsing","2024-07-16 17:26:35","","0","41","<node.js><openai-api><gpt-4>","<p>Currently I am working on the following task. I need to receive the CV as a file from the backend. File could be pdf/docx and could include the image. I need to parse this file and return its content as a JSON.
I have built the followinf service on my NodeJS service.</p>
<pre><code>import OpenAI from 'openai';
import { environment } from '../common/data/environment';
import { checkmimeType, OpenAiFileUploadReponse } from '../common';
import axios from 'axios';
import FormData from 'form-data';

export class GPTService {
    private readonly openai: OpenAI;
    private readonly BASE_PATH: string;
    private assistant_id: string;

    constructor() {
        this.openai = new OpenAI({
            apiKey: environment.gpt_key,
        });
        this.BASE_PATH = 'https://api.openai.com/v1';
        this.assistant_id = environment.assistant_id as string;
    }

    async createRun(thread_id: string, assistant_id: string) {
        return this.openai.beta.threads.runs.create(thread_id, {
            assistant_id
        });
    }

    async waitForTheResponse(run: OpenAI.Beta.Threads.Runs.Run) {
        let { status } = run;

        while (status === 'queued' || status === 'in_progress') {
            const retrieving_run = await this.openai.beta.threads.runs.retrieve(run.thread_id, run.id);

            if (retrieving_run.status === 'completed') {
                const all_messages = await this.openai.beta.threads.messages.list(run.thread_id);
                this.openai.beta.threads.del(run.thread_id);
                all_messages.data.map((d) =&gt; console.log(d.content[0]))
                // console.dir(all_messages);
                // console.dir(all_messages.data[0].content[0]);
                return all_messages.data[0].content[0];
            } else if (retrieving_run.status === 'queued' || retrieving_run.status === 'in_progress') {
                status = retrieving_run.status;
                continue;
            } else {
                console.log(&quot;Status &gt;&gt;&gt; &quot;, retrieving_run.status);
                break;
            }
        }
    }

    async createThread(file_id: string) {
        return this.openai.beta.threads.create({
            messages: [
                {
                    role: 'user',
                    content: `Return me the content of the file in the JSON format`,
                    attachments: [{
                        file_id,
                        tools: [{
                            type: 'file_search'
                        }]
                    }]
                }
            ]
        });
    }

    async extractFileContent(file?: Express.Multer.File) {
        const { id } = await this.uploadFile(file);
        const { id: thread_id } = await this.createThread(id);
        const run = await this.createRun(thread_id, this.assistant_id);
        const content = await this.waitForTheResponse(run);
        await this.deleteFile(id);
        return content;
    }

    async deleteFile(id: string) {
        const res = await this.openai.files.del(id);

        console.log(res);

        return res;
    }

    async uploadFile(file?: Express.Multer.File) {
        if (!file) {
            throw new Error('File is corrupted or is not downloaded!');
        }

        const isValidType = checkmimeType(file.mimetype);

        if (!isValidType) {
            throw new Error('Mime type is not valid!');
        }

        const form = new FormData();
        form.append('file', file.buffer, {
            filename: file.originalname,
            contentType: file.mimetype
        });
        form.append('purpose', 'assistants');

        try {
            const response = await axios.post(`${this.BASE_PATH}/files`, form, {
                headers: {
                    ...form.getHeaders(),
                    'Authorization': `Bearer ${environment.gpt_key}`,
                },
            });

            console.log('Upload response:', response.data);

            if (response.data &amp;&amp; response.data.id) {
                return response.data as OpenAiFileUploadReponse;
            } else {
                throw new Error('Failed to upload file to OpenAI.');
            }
        } catch (error) {
            console.error('Error uploading file:', error);
            throw new Error('Error uploading file: ' + error);
        }
    }
}
</code></pre>
<p>I am calling the method <code>extractFileContent</code> to run the main process. It includes a few steps:</p>
<ul>
<li>I am uploading a file to the openAI storage using the method <code>uploadFile</code></li>
<li>Creating a new thread using the <code>createThread</code> method</li>
<li>Create the run using the previously created <code>assistant id</code> (stores in my env) and calling the method <code>createRun</code>. Note: assistant uses gpt-4o model</li>
<li>And in the ``waitForResponse``` method I am getting the result of my GPT Assistant work
It should work but... I am getting the strange response, like &quot;I don/t have the access to this file&quot;. Or this file wasn't uploaded. But the main problem, that it is.</li>
</ul>
<p>I have attached the uploaded file id here:</p>
<pre><code>messages: [
                {
                    role: 'user',
                    content: `Return me the content of the file in the JSON format`,
                    attachments: [{
                        file_id,
                        tools: [{
                            type: 'file_search'
                        }]
                    }]
                }
            ]
</code></pre>
<p>Maybe I have missed something? I will be very appreciated for your answer or any assistance</p>
","gpt-4"
"78712140","How to pass a variable to a prompt and get it printed as is in Python","2024-07-05 15:05:39","","1","57","<python><gpt-4>","<p>I'm working on a project where I need to pass a variable to a prompt and get it printed as part of the output string. I'm using Python and the OpenAI API. Specifically, I have a function that identifies non-inclusive pronouns in a given text. The identified pronouns are stored in a variable which I want to include in the prompt string for further processing.</p>
<p>Here's a simplified version of my code:</p>
<pre><code>import re

def identify_non_inclusive_pronouns(text):
    pronouns = ['he', 'she', 'his', 'him', 'her', 'she\'s']
    # Remove text inside parentheses
    text = re.sub(r'\(.*?\)', '', text)
    words = re.findall(r'\b\w+\b', text.lower())
    non_inclusive_pronouns = [word for word in words if word in pronouns]
    unique_non_inclusive_pronouns = list(set(non_inclusive_pronouns))
    return unique_non_inclusive_pronouns


def classify_sentences(message: str):
    non_inclusive_pronouns = identify_non_inclusive_pronouns(message)
    print(non_inclusive_pronouns)
    non_inclusive_pronouns_str = ', '.join(non_inclusive_pronouns)
    print(non_inclusive_pronouns_str)
    system_content = '''
    __CONTEXT__:
    You are a World-Class Inclusive Language Checker, tasked with evaluating English messages for any type of bias. Your objective is to ensure the language used is inclusive, avoiding any words or phrases that could offend any community, ethnic group, race, gender, religious group, socio-economic status, or societal segment.You will be working for the corporate affairs team of Mars Wrigley, a pet care and confectionery giant, to ensure that the posts on various social media platforms are inclusive and respectful.The language should be neutral ,respectable and relatable to everyone inclusive of individuals of any gender,race, caste, creed, religious or socio-economic community. Accurate inclusivity detection is crucial as failure to detect correctly would negatively impact business users and content consumers and will cost the human race.
    __ASK__: 
    Classify the message as &quot;Inclusive&quot; or &quot;Not Inclusive&quot;.
    Output for &quot;Not Inclusive&quot; messages:
    Not Inclusive Words: A list of non-inclusive words or phrases.
    Reasons: Detailed explanations for why each word or phrase flagged is considered non-inclusive with inclusive reccomendations.
    __CONSTRAINTS__:
    1. Ensure the model scans the entire text for all predefined inclusivity rules.
    2. Each rule should be independently checked so that multiple issues can be identified within the same passage.Even if one rule is violated the  final outcome is non inclusive and reasons and non inclusivity words should be listed appropriately.
    3. For each instance of non-inclusivity, add the specific non-inclusive words and the corresponding reasons on why the text was flagged and recommendations to the output lists.
    4.Always add a comment under reasons even if the text is inclusive.There are some good to have comments or reccomendation to the user suggested by most of the  rules.

    Given the MESSAGE, let's go step by step using rules to learn how to check if the message is inclusive or not:

    1. **Important Rule:** Retrieval of pronoun variable
        Expected Output:
        - Tag: Not Inclusive
        - not_inclusive_words: Append content of string {non_inclusive_pronouns_str} to this list
        - Reasons: Include the appropriate pronouns in brackets next to 'name/job title' the first time they are mentioned in the message/post.

        OUTPUT:
        {{
        &quot;tag&quot;: &quot;Not Inclusive&quot;,
        &quot;not_inclusive_words&quot;: {non_inclusive_pronouns},
        &quot;reasons&quot;: [&quot;Include the appropriate pronouns in brackets next to 'name/job title' the first time they are mentioned in the message/post.&quot;]
        }}
    '''
    # Processing with the model here...
</code></pre>
<p>Even after I did as above I am not getting the LLM to print the retrieved list of pronouns and tag correctly. As it's rule based I am expecting it to work 100% of the times. And note I definitely want to pass the pronouns to the prompt because I want the LLM to  get it to reason. I am using GPT-4o.</p>
","gpt-4"
"78688270","Langchain Pandas agent returns SyntaxError while analyzing dataframe","2024-06-30 10:16:42","","0","71","<python><langchain><large-language-model><gpt-4><langchain-agents>","<p>I'm encountering a SyntaxError when using the Pandas dataframe agent in a loop. It seems that the LLM (Large Language Model) is not correctly using the python_repl_ast tool. Here are a few examples of the errors I'm getting:</p>
<ul>
<li>Example 1:</li>
</ul>
<pre><code>Action Input: import pandas as pd\ndf = pd.read_csv('datasets/titanic/train.csv')\ndf['Ticket'].head()
SyntaxError: unexpected character after line continuation character (&lt;unknown&gt;, line 1)

It seems there was an error in the execution due to incorrect formatting of the input. Let's correct the format and try to load the dataset and examine the 'Ticket' column again.
</code></pre>
<ul>
<li>Example 2:</li>
</ul>
<pre><code>Action: python_repl_ast
Action Input: import numpy as np\nnp.save('columns/SibSp.npy', df['SibSp'].values)
SyntaxError: unexpected character after line continuation character (&lt;unknown&gt;, line 1)

It seems there was a mistake in my approach to executing the command due to syntax issues. Let's correct the syntax by removing the newline escape characters and try again to save the 'SibSp' column as a numpy array.
</code></pre>
<p>How I create agent:</p>
<pre><code>llm = ChatOpenAI(
        temperature=0.0,
        model_name='gpt-4-turbo-preview',
        max_tokens=1024)
    agent = create_pandas_dataframe_agent(llm, prefix=PREPROCESS_PREFIX, df=df, verbose=True,
                                          input_variables=['column', 'file_path'],
                                          handle_parsing_errors=True,           return_intermediate_steps=True)
</code></pre>
<p>How can I prevent these SyntaxError issues when the LLM attempts to use the python_repl_ast tool in the agent? Is there a way to ensure the correct formatting of the input to avoid these errors? Should I better describe usage of python_repl_ast in LLM prompt ? I'm using</p>
<p>The occurrence of the error seems random. The solution should modify all columns of the dataframe in a loop. I want to avoid SyntaxErrors because they interrupt the execution of my program and consume tokens without returning concrete answers.</p>
","gpt-4"
"78649446","AutoGen GroupChat error code (openai.BadRequestError: Error code: 400)","2024-06-20 20:20:55","","2","281","<python><chatgpt-api><gpt-4><autogen>","<p>I'm pretty new to using AutoGen so I don't know for sure if this is a simple problem to fix but I created two simple agents with the user_proxy to communicate with each other through the &quot;GroupChat&quot; function. However, after the first response from the first agent, it leads to an error code 400 from openai. The following below is the exact error code and I don't really know what the issue is.</p>
<p>openai.BadRequestError: Error code: 400 - {'error': {'message': &quot;Invalid 'messages[2].name': string does not match pattern. Expected a string that matches the pattern '^[a-zA-Z0-9_-]+$'.&quot;, 'type': 'invalid_request_error', 'param': 'messages[2].name', 'code': 'invalid_value'}}</p>
<p>I've been following the tutorials on the AutoGen Github repo and I don't think I've seen anyone really run into this problem.</p>
<p>At first I thought it was just an issue between using different LLMs so I decided to keep it to one LLM (GPT-4) and the issue is still recurring. Any insight?</p>
","gpt-4"
"78637778","Orchestrating multi generative ai agents through one single prompt","2024-06-18 13:32:14","","0","29","<artificial-intelligence><openai-api><gpt-4><generative>","<p>I have read a lot about generative ai agents or so called assistants but do not understand on their capabiltiy to be orchestrated by a single prompt. Let me explain what i mean/expect:</p>
<p>I have two generative ai agents 1) gathers weather data for a city from a weather api and 2) a hotel finder in a city</p>
<p>Assuming the following prompt: &quot;Please tell me about the weather in rome and by the way check for a 3* hotel in milano near the duomo di milano&quot;</p>
<p>So one single prompt with two questions. Can a GPT handle this? If yes how?</p>
","gpt-4"
"78603640","Detect hand-checked boxes in a pdf document with GPT-4o","2024-06-10 17:02:02","","0","99","<ocr><openai-api><gpt-4>","<p>i am working on a open ai assistant which function is to validate a series of contracts based on how many checkboxes are checked.
However, it seems difficult to the assistant to do this task.</p>
<p>I have been trying two options:</p>
<ul>
<li>Uploading it as a scanned pdf (unselectable words) and making the assistant recognize the text by OCR. This sometimes work, but with very low percentage of success.</li>
<li>Uploading the pdf as it is. With this, the assitant is capable of getting all the text and data, but it cannot detect the checkboxes.
Do you happen to know any possibility, or combination with other technologies that can help me solve this problem?
Thank you in advance.</li>
</ul>
","gpt-4"
"78596555","OpenAI Assistants API v2: Should I attach files to the thread or to the assistant?","2024-06-08 18:11:28","78596993","0","452","<openai-api><gpt-4><openai-assistants-api>","<p>I am using the OpenAI Assistants API, and I have the option to add my PDF files as a knowledge base.</p>
<p>Should I attach files to the thread or to the assistant?</p>
<p>I am super confused here.</p>
<p>Or should I add them both times?</p>
<p>What would work best?</p>
","gpt-4"
"78596140","Create Vector Store on OpenAI with curl, and Upload 2 PDfs to it with curl","2024-06-08 15:27:51","78596190","0","121","<openai-api><gpt-4>","<p>I learned that OpenAI has integrated vector store now so I do not need an external vector database. So my question is if it is all possible with just API requests using curl</p>
<p>Can I create a vector store with curl .</p>
<p>and then upload 2 PDFs to vector with just a curl command.</p>
<p>And then I can ask quesiton also just with a curl.</p>
<p>i want to test it out only using API requests, and would be immensely amazing, if its possible ?</p>
","gpt-4"
"78574662","Getting token exceed error while using OpenAI API","2024-06-04 10:01:11","","0","358","<c#><openai-api><gpt-3><gpt-4>","<p>I am trying to fetch data from the CSV file via OpenAI API but I am getting an error as token exceed. I have used both GPT-3.5 turbo and all the version of GPT-4 model to check on that.</p>
<p>Is there any changes what I need to do in the prompt or should I use some other method to reduce the size of the token. I have also set the maximum token to 2500. I am using Swagger to check my end point request</p>
<pre><code>
[Route(&quot;AskQuestionCsv&quot;)]
public async Task&lt;IActionResult&gt; AskQuestionCsv([FromBody] string question)
{
    if (string.IsNullOrWhiteSpace(extractedCsvText))
    {
        return BadRequest(new { Message = &quot;No CSV content available. Please upload a CSV file first.&quot; });
    }

    if (string.IsNullOrWhiteSpace(question))
    {
        return BadRequest(new { Message = &quot;Question cannot be empty.&quot; });
    }

    try
    {
        var openai = new OpenAIAPI(&quot;API_KEY&quot;);
        var chatRequest = new ChatRequest
        {
            Model = &quot;gpt-4&quot;, 
            Temperature = 0.7,
            MaxTokens = 25000,
            Messages = new List&lt;ChatMessage&gt;
            {
                new ChatMessage
                {
                    Role = ChatMessageRole.System,
                    Content = &quot;You are a helpful assistant.&quot;
                },
                new ChatMessage
                {
                    Role = ChatMessageRole.User,
                    Content = $&quot;Based on the following text from the CSV file, answer the question.\n\nCSV Text:\n{extractedCsvText}\n\nQuestion: {question}&quot;
                }
            }
        };

        var chatResponse = await openai.Chat.CreateChatCompletionAsync(chatRequest);
        var answer = chatResponse.Choices.FirstOrDefault()?.Message.Content.Trim();

        return Ok(new { Question = question, Answer = answer });
    }
    catch (Exception ex)
    {
        return StatusCode(500, new { Message = &quot;ERROR: &quot; + ex.Message }
    }

</code></pre>
<p><strong>Error:</strong></p>
<blockquote>
<p>Error at chat/completions (<a href=""https://api.openai.com/v1/chat/completions"" rel=""nofollow noreferrer"">https://api.openai.com/v1/chat/completions</a>) with HTTP status code: TooManyRequests. Content: {\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;Request too large for gpt-4 in organization org-uedxqeR1FzNcdHx3MOuawI9d on tokens per min (TPM): Limit 10000, Requested 6668686. The input or output tokens must be reduced in order to run successfully. Visit <a href=""https://platform.openai.com/account/rate-limits"" rel=""nofollow noreferrer"">https://platform.openai.com/account/rate-limits</a> to learn more.&quot;,\n        &quot;type&quot;: &quot;tokens&quot;,\n        &quot;param&quot;: null,\n        &quot;code&quot;: &quot;rate_limit_exceeded&quot;\n    }\n}\n</p>
</blockquote>
","gpt-4"
"78568855","azure open ai gpt-4o issue with image analysis and C#","2024-06-03 07:05:58","","1","449","<c#><azure-openai><gpt-4>","<p>I'm using bot framework sdk with C# to call Azure open AI model gpt-4o for image analysis. i receive following response or variance of it. however when i upload same exact image to azure open ai studio it's able to analyze it. also when i use python it's able to analyze it. what could be the issue with this c# code?</p>
<p>response i receive or variations of it:</p>
<p>&quot;I'm currently unable to directly analyze images from files or URLs, but I can help guide you on how to describe what‚Äôs in the image or help you with other questions or tasks related to it! If you can describe the image, I can assist you better. Please provide some details or let me know how I can help!&quot;</p>
<p>so is this particular piece correct?</p>
<pre><code>string base64Image = Convert.ToBase64String(imageBytes);
// Prepare GPT-4 payload with Base64 image
ChatCompletionsOptions gpt4Options = new ChatCompletionsOptions()
{
    Messages =
    {
        new ChatMessage(ChatRole.System, @&quot;You are an AI assistant that helps people find information.&quot;),
        new ChatMessage(ChatRole.User, @&quot;Here's an image for you to analyze.&quot;),
        //new ChatMessage(ChatRole.User, base64Image) // Add the base64 image as a user message
        new ChatMessage(ChatRole.User, $&quot;{{\&quot;type\&quot;: \&quot;image_url\&quot;, \&quot;image_url\&quot;: \&quot;data:image/png;base64,{base64Image}\&quot;}}&quot;) 
       // $&quot;{{\&quot;type\&quot;: \&quot;image_url\&quot;, \&quot;image_url\&quot;: {{\&quot;url\&quot;: \&quot;data:image/png;base64,{base64Image}\&quot;}}}}&quot;)
    },
    // ... other options like Temperature, MaxTokens, etc.
};
</code></pre>
<p>complete code:</p>
<pre><code>// Generated with EchoBot .NET Template version v4.22.0

using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;
using Microsoft.Bot.Builder;
using Microsoft.Bot.Schema;
using System;
using Azure;
using Azure.AI.OpenAI;
using System.IO; // Required for working with files
using System.Text; // Required for Base64 encoding
using System.Net.Http;
using System.Net.Http.Headers;
using System.Text.Json;

namespace EchoBot.Bots
{
    public class EchoBot : ActivityHandler
    {
        static readonly HttpClient client2 = new HttpClient();
        protected override async Task OnMessageActivityAsync(ITurnContext&lt;IMessageActivity&gt; turnContext, CancellationToken cancellationToken)
        {
            // Check if the activity contains any text
            if (!string.IsNullOrEmpty(turnContext.Activity.Text))
            {
                var replyText = $&quot;Echo: {turnContext.Activity.Text}&quot;;
                await turnContext.SendActivityAsync(MessageFactory.Text(replyText, replyText), cancellationToken);

                OpenAIClient client = new OpenAIClient(new Uri(&quot;https://mct-openai01.openai.azure.com/&quot;),
                 new AzureKeyCredential(&quot;xxxxxxxxxxxxxxxxxxxxxxxxx&quot;));

                Response&lt;ChatCompletions&gt; responseWithoutStream = await client.GetChatCompletionsAsync(&quot;mct-gpt4o-01&quot;,
                 new ChatCompletionsOptions()
                 {
                    Messages =
                    {
                        new ChatMessage(ChatRole.System, @&quot;You are an AI assistant that helps people find information.&quot;),
                        new ChatMessage(ChatRole.User, @&quot;You are an AI assistant that helps people find information.&quot;),
                     },
                     Temperature = (float)0.7,
                     MaxTokens = 800,
                     NucleusSamplingFactor = (float)0.95,
                     FrequencyPenalty = 0,
                     PresencePenalty = 0,
                 });
                ChatCompletions response = responseWithoutStream.Value;
                string completionText = response.Choices[0].Message.Content;
                await turnContext.SendActivityAsync(MessageFactory.Text(completionText, completionText), cancellationToken);
            }
            else if (turnContext.Activity.Attachments.Count &gt; 0)
            {
                // User sent an attachment
                await turnContext.SendActivityAsync(MessageFactory.Text(&quot; I see you sent one!&quot;), cancellationToken);
                if (turnContext.Activity.Attachments.Count == 1 &amp;&amp; turnContext.Activity.Attachments[0].ContentType.StartsWith(&quot;image/&quot;))
                {
                    await turnContext.SendActivityAsync(MessageFactory.Text(&quot; starts with image&quot;), cancellationToken);
                    // Only process the first attachment if it's an image
                    var attachment = turnContext.Activity.Attachments[0];
                    if (attachment.ContentType.Contains(&quot;image/&quot;))
                    {
                        await turnContext.SendActivityAsync(MessageFactory.Text(&quot; contains&quot;), cancellationToken);
                        var httpClient = new HttpClient();
                        var attachmentUrl = attachment.ContentUrl;
                        try
                        {
                            var attachmentResponse = await httpClient.GetAsync(attachmentUrl, cancellationToken);
                            if (attachmentResponse.IsSuccessStatusCode)
                            {
                                using (var memoryStream = new MemoryStream())
                                {
                                    await attachmentResponse.Content.CopyToAsync(memoryStream);
                                    byte[] imageBytes = memoryStream.ToArray();
                                    // Convert image to Base64 string
                                    string base64Image = Convert.ToBase64String(imageBytes);
                                    // Prepare GPT-4 payload with Base64 image
                                    ChatCompletionsOptions gpt4Options = new ChatCompletionsOptions()
                                    {
                                        Messages =
                                        {
                                            new ChatMessage(ChatRole.System, @&quot;You are an AI assistant that helps people find information.&quot;),
                                            new ChatMessage(ChatRole.User, @&quot;Here's an image for you to analyze.&quot;),
                                            //new ChatMessage(ChatRole.User, base64Image) // Add the base64 image as a user message
                                            new ChatMessage(ChatRole.User, $&quot;{{\&quot;type\&quot;: \&quot;image_url\&quot;, \&quot;image_url\&quot;: \&quot;data:image/png;base64,{base64Image}\&quot;}}&quot;) 
                                            // $&quot;{{\&quot;type\&quot;: \&quot;image_url\&quot;, \&quot;image_url\&quot;: {{\&quot;url\&quot;: \&quot;data:image/png;base64,{base64Image}\&quot;}}}}&quot;)
                                        },
                                        // ... other options like Temperature, MaxTokens, etc.
                                    };
                                    // Send the request with the image data
                                    OpenAIClient client = new OpenAIClient(new Uri(&quot;https://mct-openai01.openai.azure.com/&quot;),
                                    new AzureKeyCredential(&quot;xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx&quot;));
                                    Response&lt;ChatCompletions&gt; gpt4Response = await client.GetChatCompletionsAsync(&quot;mct-gpt4o-01&quot;, gpt4Options);
                                    ChatCompletions response = gpt4Response.Value;
                                    string completionText = response.Choices[0].Message.Content;
                                    await turnContext.SendActivityAsync(MessageFactory.Text(completionText, completionText), cancellationToken);
                                }
                            }
                            else
                            {
                                // Handle download failure (e.g., inform user)
                                //await turnContext.SendActivityAsync(MessageFactory.Text(&quot;else statment&quot;), cancellationToken);
                            }
                        }
                        catch (Exception)
                        {
                            // Handle exceptions during download (e.g., logging)
                            //await turnContext.SendActivityAsync(MessageFactory.Text(&quot;catch statment&quot;), cancellationToken);
                        }
                    }
                }
            }
        }

        protected override async Task OnMembersAddedAsync(IList&lt;ChannelAccount&gt; membersAdded, ITurnContext&lt;IConversationUpdateActivity&gt; turnContext, CancellationToken cancellationToken)
        {
            var welcomeText = &quot;Hello and welcome!&quot;;
            foreach (var member in membersAdded)
            {
                if (member.Id != turnContext.Activity.Recipient.Id)
                {
                    await turnContext.SendActivityAsync(MessageFactory.Text(welcomeText, welcomeText), cancellationToken);
                }
            }
        }
    }
}
</code></pre>
<p>did anyone face same issue with c# ?</p>
","gpt-4"
"78565484","OpenAI API: Is the /v1/chat/completions endpoint a legacy endpoint?","2024-06-02 04:44:18","","-3","319","<python><discord><openai-api><chatgpt-api><gpt-4>","<h2>Goal</h2>
<p>I am currently working on a project where I am integrating GPT-3.5 models into my application.</p>
<h2>Problem</h2>
<p>However, I have encountered an issue where only <code>gpt-3.5-turbo-instruct</code> seems to be working with the <code>/v1/chat/completions</code> endpoint. When I try to use other GPT-3.5 and GPT-4 models, I receive the following error:</p>
<blockquote>
<p>Error code: 404 - {‚Äòerror‚Äô: {‚Äòmessage‚Äô: ‚ÄòThis is a chat model and not
supported in the v1/completions endpoint. Did you mean to use
v1/chat/completions?‚Äô, ‚Äòtype‚Äô: ‚Äòinvalid_request_error‚Äô, ‚Äòparam‚Äô:
‚Äòmodel‚Äô, ‚Äòcode‚Äô: None}}</p>
</blockquote>
<p>Does this mean that the <code>/v1/completions</code> endpoint is deprecated for other GPT-3.5 and GPT-4 models? Or am I missing something in my setup?</p>
<p>For context, I have already added $5 to my OpenAI account, so I believe I should have access to the models. I would appreciate any guidance on whether this issue is due to endpoint deprecation or if there‚Äôs a different recommended approach to using other GPT-3.5 and GPT-4 models with the <code>/v1/chat/completions</code> endpoint.</p>
<h2>What I've tried</h2>
<p><strong>Attempt 1</strong></p>
<p>Model initialization:</p>
<pre><code>from langchain_openai import OpenAI
llm = OpenAI(model='gpt-4-turbo')
</code></pre>
<p><strong>Attempt 2</strong></p>
<p>Attempted to use other GPT-3.5 and GPT-4 models with the <code>/v1/chat/completions</code> endpoint.</p>
<p>I received the aforementioned <code>404</code> error.</p>
<h2>Conclusion</h2>
<p>I expected that other GPT-3.5 and GPT-4 models would work with the <code>/v1/chat/completions</code> endpoint without encountering a <code>404</code> error. Specifically, I expected to be able to interchangeably use different GPT-3.5 models with this endpoint for generating completions.</p>
<p>For context, I have already added $5 to my OpenAI account, so I believe I should have access to the models.</p>
<p>I would appreciate any guidance on whether this issue is due to endpoint deprecation or if there's a different recommended approach to using other GPT-3.5 and GPT-4 models with the <code>/v1/chat/completions</code> endpoint.</p>
<p>Thank you!</p>
","gpt-4"
"78548674","How to Extract Information from a Large Markdown File to Fill a JSON Using OpenAI?","2024-05-29 09:45:28","","0","130","<openai-api><chatgpt-api><gpt-4>","<p>I have a large markdown (MD) file from which I need to extract information to fill a JSON file. How can I provide the MD file as context in an OpenAI request to generate the filled JSON? I tried dividing it into chunks, but the results aren't very good. One issue I face is that the MD file contains multiple scenarios, and I need to determine which one is used. If I provide the file in chunks, the GPT model fills the scenario case in the JSON with the first mentioned scenario, but it needs to read the entire document first to understand which scenario is actually used.</p>
<p>I tried to split the md file in chunks to respect the context window</p>
","gpt-4"
"78543234","GPT Action + ngrok - can't get it to work","2024-05-28 09:40:36","","0","57","<ngrok><gpt-4>","<p>I'm trying to setup a GPT action (using custom GPT and not the API) with local server, and use ngrok as the mediator
But the GPT action request does not reach my server</p>
<p>Here's my setup :</p>
<ol>
<li><p>I called the command line 'ngrok http 8080', which gave me <a href=""https://someid.ngrok-free.app"" rel=""nofollow noreferrer"">https://someid.ngrok-free.app</a></p>
</li>
<li><p>I create an action in custom GPT with the following schema and no authentication</p>
</li>
</ol>
<pre><code>`{
    &quot;openapi&quot;: &quot;3.0.0&quot;,
    &quot;info&quot;: {
        &quot;title&quot;: &quot;Test Connectivity API&quot;,
        &quot;version&quot;: &quot;1.0.0&quot;,
        &quot;description&quot;: &quot;API to test connectivity by calling a specified URL and expecting a 200   status&quot;
    },
    &quot;servers&quot;: [
    {
      &quot;url&quot;: &quot;https://someid.ngrok-free.app/&quot;,
      &quot;description&quot;: &quot;Ngrok server for testing connectivity&quot;
    }
  ],
  &quot;paths&quot;: {
    &quot;/&quot;: {
      &quot;get&quot;: {
        &quot;summary&quot;: &quot;Test Connectivity&quot;,
        &quot;operationId&quot;: &quot;testConnectivity&quot;,
        &quot;parameters&quot;: [],
        &quot;responses&quot;: {
          &quot;200&quot;: {
            &quot;description&quot;: &quot;Successful response with status 200 and 'hello' message&quot;,
            &quot;content&quot;: {
              &quot;application/json&quot;: {
                &quot;schema&quot;: {
                  &quot;type&quot;: &quot;object&quot;,
                  &quot;properties&quot;: {
                    &quot;message&quot;: {
                      &quot;type&quot;: &quot;string&quot;,
                      &quot;example&quot;: &quot;Hello, HTTP!&quot;
                    }
                  }
                }
              }
            }
          },
          &quot;400&quot;: {
            &quot;description&quot;: &quot;Bad request response&quot;
          }
        }
      }
    }
  }
}`
</code></pre>
<ol start=""3"">
<li>From my local server, I started some process with C# HttpListener on port *:8080</li>
</ol>
<p>When I try to connect with a browser to <a href=""https://someid.ngrok-free.app/"" rel=""nofollow noreferrer"">https://someid.ngrok-free.app/</a>, everything is working and I can see it reaches my server, and I get the response in my browser</p>
<p>When I try to use it from the GPT action it does not reach my server at all</p>
","gpt-4"
"78521707","OpenAI Chat Completions API: How do I solve the APIRemovedInV1 error when using the OpenAI Python library for GPT-4?","2024-05-23 08:04:32","78522605","-1","138","<python><openai-api><gpt-4>","<p>This is a snippet where I have been encountering the problem:</p>
<pre><code>def generate_response(prompt):
    response = openai.chatcompletion.create(
        model=&quot;gpt-4&quot;,  # Use &quot;gpt-4&quot; as the model identifier if &quot;gpt-4o&quot; is deprecated
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
        ],
        max_tokens=150
    )
    return response.choices[0].message['content'].strip()
</code></pre>
<p>I tried the following to solve the <code>APIRemovedInV1</code> error:</p>
<ol>
<li>I have tried downgrading the library version.</li>
<li>I have gone through the OpenAI documentation and tried the asynchronous method too but didn't work.</li>
</ol>
","gpt-4"
"78475340","How can I create an action for my own GPT app that can be append additional text to users' questions","2024-05-14 01:19:39","","0","79","<action><openai-api><chatgpt-api><gpt-4>","<p>I have created my own custom GPT using the &quot;My GPTs&quot; platform on GPT4. Now I want to add a new action to this app, via My GPTs -&gt; choose my custom GPT -&gt; Configure -&gt; Actions -&gt; Create new actions</p>
<p>This action should be:</p>
<ol>
<li>Trigerred by any question asked by the user</li>
<li>Append an additional content onto the question, and then pass the question to GPT.</li>
</ol>
<p>The problems I met are:</p>
<ol>
<li>I can't find anywhere to configure the triggers of the actions.</li>
<li>I tried to waite a piece of openAPI schema script to create the action, but don't know how to fill in the field of &quot;servers&quot; -&gt; &quot;url&quot; field.</li>
</ol>
<p>Could anyone who have GPT experience please kindly advise? Thanks a lot!</p>
","gpt-4"
"78433166","Streamlit and GPT-4 Turbo with Vision","2024-05-05 17:33:15","","0","139","<python><streamlit><gpt-4>","<p>I am trying to write a streamlit app that takes a prompt, takes a photo, and then run a GPT-4 Vision analysis on the photo. The app does not run beyond line ~13 (the screen just stops after the photo is taken, with no traceback). Would anyone know what the issue might be?</p>
<pre><code>
import streamlit as st


import base64
import requests

# OpenAI API Key
api_key = &quot;xxx&quot;

title = st.text_input(&quot;Your prompt:&quot;,&quot;What is in the picture?&quot;)


img_file_buffer  = st.camera_input(&quot;Take a picture&quot;)
    
if img_file_buffer is not None:
    
    # To read image file buffer as bytes:
    bytes_data = img_file_buffer .getvalue()
    
    # Convert bytes data to base64
    base64_encoded = base64.b64encode(bytes_data).decode()

    headers = {
      &quot;Content-Type&quot;: &quot;application/json&quot;,
      &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;
    }

    payload = {
      &quot;model&quot;: &quot;gpt-4-turbo&quot;,
      &quot;messages&quot;: [
        {
          &quot;role&quot;: &quot;user&quot;,
          &quot;content&quot;: [
            {
              &quot;type&quot;: &quot;text&quot;,
              &quot;text&quot;: title
            },
            {
              &quot;type&quot;: &quot;image_url&quot;,
              &quot;image_url&quot;: {
                &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_encoded}&quot;
              }
            }
          ]
        }
      ],
      &quot;max_tokens&quot;: 300
    }

    response = requests.post(&quot;https://api.openai.com/v1/chat/completions&quot;, headers=headers, json=payload)

    print(response.json())

    st.text(response.json())
</code></pre>
","gpt-4"
"78368294","How to Process Responses from OpenAI API Without Encountering 'ChatCompletionChunk' Object Not Subscriptable Error?","2024-04-22 18:40:43","","0","291","<chat><openai-api><gpt-4>","<p>I am using the OpenAI GPT-4 Turbo model via the OpenAI Python SDK to generate chat completions. However, I'm running into an issue where I get a <code>'ChatCompletionChunk' object is not subscriptable</code> error when trying to access message contents from the response. Below is the snippet of code I'm using:</p>
<pre class=""lang-py prettyprint-override""><code>def call_openai(prompt):
    try:
        response = client.chat.completions.create(
            model=&quot;gpt-4-turbo&quot;,
            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
            stream=True
        )
        messages = []
        
        for message in response:
            if message['role'] == 'assistant':
                messages.append(message['content'])
        
        return &quot; &quot;.join(messages)
    except Exception as e:
        print(f&quot;An error occurred: {e}&quot;)
        return None
</code></pre>
<p>In this code, I expect to collect and concatenate messages from the assistant role, but the error suggests that the objects returned are not directly subscriptable. It seems there is a misunderstanding on my part regarding the object structure returned by chat.completions.create.</p>
<p>What is the correct way to access the messages from the response object?
Could anyone provide or point me to detailed documentation or examples for handling responses from chat.completions.create with stream=False in the OpenAI Python SDK?</p>
","gpt-4"
"78347436","LLM computes wrong dates","2024-04-18 12:28:12","","1","464","<date><large-language-model><azure-openai><gpt-4>","<p>I'm encountering challenges with handling dates while using GPT-4 on Azure OpenAI. Specifically, I'm trying to extract deadlines from large email threads and convert phrases like &quot;by next Tuesday&quot; into actual dates based on the email's sent date.</p>
<p>For example, when I instruct the model to interpret &quot;by next Tuesday&quot; from an email sent on 5 April 2024, it should correctly identify the next Tuesday. However, the model misinterprets this, and even when it appears to understand, it calculates incorrect dates. For instance, it might output &quot;by 12 April 2024,&quot; which is a Friday, not a Tuesday.</p>
<p>Has anyone else experienced similar issues or have tips on how to improve the model's accuracy in date calculations? Any guidance or suggestions would be greatly appreciated.</p>
<p>Thank you!</p>
<p>I did try to ask the model to explain how it computes the date. It told me ''Leadership decisions are needed by next Tuesday from 5 April 2024, which implies a decision by 12 April 2024'' (April 12 is a Friday)</p>
<p>I tried giving examples and explaining the rationale. It didn't work either:</p>
<p>&quot;Guidelines for deadline date calculations:</p>
<ul>
<li>Ensure that all deadlines are specified with full dates or clearly referenced to full dates.</li>
<li>Example of an email thread composed of 2 emails:
Email 1 - date: 4 january 2024 - &quot;Hi May, we received the validation from Finance for the budget, still waiting for Legal. John&quot;
Email 2 - date: 2 january 2024 - &quot;Hi May, we have a challenge on project ABC. Marketing budget haven't been approved yet. I chased Finance and Legal and asked them to get back to us by next Monday the latest. John&quot;</li>
</ul>
<p>Incorrect output: 'Finance and Legal should validate marketing budget by next Monday'
-&gt; Correct output 1: 'Finance and Legal should validate marketing budget by next Monday from 2 January 2024'
-&gt; Correct output 2: 'Finance and Legal should validate marketing budget by Monday 8 January 2024'
Rationale: validation should be done by next Monday from the date of the email mentioning this deadline, which is dated 2 January 2024. Therefore, the decision should be made by 8 January 2024.&quot;</p>
","gpt-4"
"78314847","How to resolve AttributeError and BadRequestError in Python using LangChain and AzureOpenAI?","2024-04-12 07:41:01","","1","481","<python><langchain><py-langchain><azure-openai><gpt-4>","<p>I'm working on integrating <code>LangChain</code> with <code>AzureOpenAI</code> in Python and encountering a couple of issues. I've recently updated from a deprecated method to a new class implementation, but now I'm stuck with some errors I don't fully understand. Here's the relevant part of my code:</p>
<pre><code>from langchain_openai import AzureOpenAI as LCAzureOpenAI
# from langchain.llms import AzureOpenAI &lt;-- Deprecated

# Create client accessing LangChain's class
client = LCAzureOpenAI(
    openai_api_version=api_version,
    azure_deployment=deployment_name,
    azure_endpoint=azure_endpoint,
    temperature=TEMPERATURE,
    max_tokens=MAX_TOKENS,
    model=model
    #,model_kwargs={'azure_openai_api_key': api_key}
)

# Attempt to send a chat message
client.chat(&quot;Hi&quot;)
</code></pre>
<p>This results in the following error:</p>
<pre><code>AttributeError: 'AzureOpenAI' object has no attribute 'chat'
</code></pre>
<p>When I replace <code>client.chat(&quot;Hi&quot;)</code> with <code>client.invoke(&quot;Hi&quot;)</code>, I get a different error:</p>
<pre><code>BadRequestError: Error code: 400 - {'error': {'code': 'OperationNotSupported', 'message': 'The completion operation does not work with the specified model, gpt-4. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.'}}
</code></pre>
<p>How can I resolve these errors?</p>
<p>Any guidance or insights into these errors and how to resolve them would be greatly appreciated!</p>
","gpt-4"
"78261536","I am using Azure Open AI through databricks. Facing Timeout issue for below code. I am using GPT-4. Whats wrong here?","2024-04-02 13:08:54","","0","613","<timeout><azure-databricks><azure-openai><gpt-4>","<pre><code>import os
from openai import AzureOpenAI


client = AzureOpenAI(
  azure_endpoint = &quot;https://az*****.openai.azure.com/&quot;, 
  api_key=OPEN_AI_KEY,  
  api_version=&quot;2024-02-15-preview&quot;
)


message_text = [{&quot;role&quot;:&quot;system&quot;,&quot;content&quot;:&quot;You are an AI assistant that helps people find information.&quot;}]

completion = client.chat.completions.create(
  model=&quot;gpt-4-1106-Preview&quot;, # model = &quot;deployment_name&quot;
  messages = message_text,
  temperature=0.7,
  max_tokens=800,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  timeout = 120,
  stop=None
)
</code></pre>
<blockquote>
<p>Error : APITimeoutError: Request timed out.</p>
</blockquote>
<p><img src=""https://i.sstatic.net/waMwo.png"" alt=""enter image description here"" /></p>
<p>Tried by changing API keys, earlier the same code was running smoothly. Don't know how to check logs, where Azure Open AI is not reachable or Databricks failed to make a call. Need a help to resolve this.</p>
","gpt-4"
"78237430","Integrating GPT-4 with Team Foundation Server for Data Insights","2024-03-28 10:19:55","78238614","-1","49","<artificial-intelligence><openai-api><gpt-3><gpt-4>","<p>I'm exploring options to integrate GPT-4, the latest version of OpenAI's powerful language model, with Team Foundation Server (TFS). My objective is to leverage GPT-4's capabilities to gain insights from the data stored within our TFS environment.</p>
<p>I'm wondering if there are any APIs or existing integrations available that facilitate this process. Specifically, I'm interested in extracting data from TFS and feeding it into GPT-4 for analysis and generating insights.</p>
<p>Could anyone provide guidance on whether such an API or integration exists? If not, are there any alternative approaches or workarounds that could achieve similar results?</p>
<p>Any advice, resources, or experiences related to integrating GPT-4 with TFS for data analysis and insights?</p>
<p>I tried exporting the data from TFS and then feeding the same data to Chat GPT, but exported data doe s not contain all the required details and also exported data is very huge, so it crosses the prompt limit. My expectation is to have an API, which can directly be integrated with TFS server. Like we can integrate 365 copilot with office and GitHub co - pilot with codebase.</p>
","gpt-4"
"78217747","Correct Array Format for Function Calling Open AI's ChatGPT","2024-03-25 08:09:44","","0","230","<openai-api><chatgpt-api><gpt-4><chatgpt-function-call>","<p>I am integrating a ChatGPT with function calling for specific tasks. However, during my research I've encountered two different array structures for prompt history. I'm unsure which one is the correct or more efficient for maintaining historical record for function calls with GPT. Neither one of them produce errors during the call, as <code>function</code> is an allowed role.</p>
<p><strong>What I am currently using:</strong></p>
<pre><code>{
    &quot;messages&quot;: [
        {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: &quot;&quot;
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;What is the weather like in Boston?&quot;
        },
        {
            &quot;role&quot;: &quot;assistant&quot;,
            &quot;content&quot;: &quot;20 C&quot;,
            &quot;name&quot;: &quot;get_current_weather&quot;,
            &quot;function_call&quot;: {
                &quot;name&quot;: &quot;get_current_weather&quot;,
                &quot;arguments&quot;: &quot;{\n  \&quot;location\&quot;: \&quot;Boston, MA\&quot;\n}&quot;
            }
        }
    ]
}
</code></pre>
<p><strong>What I have seen a few places online:</strong></p>
<pre><code>{
    &quot;messages&quot;: [
        {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: &quot;&quot;
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;What is the weather like in Boston?&quot;
        },
        {
            &quot;role&quot;: &quot;assistant&quot;,
            &quot;content&quot;: &quot;What is the weather like in Boston?&quot;,
            &quot;function_call&quot;: {
                &quot;name&quot;: &quot;get_current_weather&quot;,
                &quot;arguments&quot;: &quot;{\n  \&quot;location\&quot;: \&quot;Boston, MA\&quot;\n}&quot;
            }
        },
        {
            &quot;name&quot;: &quot;get_current_weather&quot;,
            &quot;role&quot;: &quot;function&quot;,
            &quot;content&quot;: &quot;20 C&quot;
        }
    ]
}
</code></pre>
<p>Can anyone shed some light on these two and when to use each one?</p>
","gpt-4"
"78192925","Creating file using GPT API and extract it‚Äôs content in nodejs","2024-03-20 11:30:32","","0","80","<node.js><gpt-4>","<p>I need to upload any file and create it with <strong>chatgpt api</strong>. In this case, I get a successful result.</p>
<p>To extract the content, I use the <code>openaiGPT.beta.assistants.create Gpt4</code> API, which is also successful.</p>
<p>But somehow I am not able to extract the content from this file.</p>
<p>I am using the model <code>gpt-4-0125-preview</code> and the tool type <code>retrieval</code> and I am still not able to extract the content of the file.</p>
<p>I have also tried the <strong>thread API</strong> and can create the thread, but when I try to create a message and run the API for GPT4, I get an error message.</p>
<blockquote>
<p>401 You do not have sufficient permissions for this operation. Missing
areas: api.threads.write. Check that you have the correct role in your
organisation (reader, writer, owner), and if you are using a
restricted API key, that it has the required scopes.</p>
</blockquote>
<p>I have also granted the authorisation.</p>
<p>For threads, I use the following APIS</p>
<ul>
<li><p><code>openaiGPT.beta.threads.messages.create</code></p>
</li>
<li><p><code>openaiGPT.beta.threads.runs.create</code></p>
</li>
</ul>
<p>Does anyone have any ideas on this topic?</p>
<p>Many thanks in advance!</p>
<p>I want to upload a file and extract the content with GPT4.</p>
","gpt-4"
"78191390","GPT python SDK introduces massive overhead / incorrect timeout","2024-03-20 07:04:28","78191414","1","149","<python><openai-api><chatgpt-api><gpt-4>","<p>I've been using openai python packge v0.28.1 with the <code>requests_timeout</code> param which worked OK.
I then updated to the ^1. version only to find out that the timeout no longer works as expected (they have changed the param name from <code>requests_timeout</code> to <code>timeout</code>.</p>
<p>Here is an odd behavior with the current newest version (1.14.1):</p>
<pre><code>from openai import OpenAI, APITimeoutError
import os

client = OpenAI(
    api_key=os.environ['OPENAI_API_KEY'],
)

for timeout in [0.001, 0.1, 1, 2]:
    with log_duration('openai query') as duration_context:
        try:
            response = client.chat.completions.create(  # type: ignore[call-overload]
                model=&quot;gpt-4-0125-preview&quot;,
                messages=[{'content': 'describe the universe in 10000 characters', 'role': 'system'}],
                temperature=0.0,
                max_tokens=450,
                top_p=1,
                timeout=timeout
            )
        except APITimeoutError as e:
            continue
</code></pre>
<p>log_duration just measure the time it takes. the result are :</p>
<pre><code>2024-03-20 14:59:19 [info     ] openai query duration=2.805093 duration=2.8050930500030518 name=openai query
2024-03-20 14:59:22 [info     ] openai query duration=2.844164 duration=2.8441641330718994 name=openai query
2024-03-20 14:59:29 [info     ] openai query duration=6.396946 duration=6.396945953369141 name=openai query
2024-03-20 14:59:38 [info     ] openai query duration=9.387082 duration=9.387081861495972 name=openai query
</code></pre>
<p>which is way more then the timeouts. We have been getting a bunch of timeouts on our lambdas without understanding why as the timeout on openai is supposed to be so much lower.</p>
<p>what am I missing? is there such a big overhead in OpenAI's &gt;1 python SDK?</p>
","gpt-4"
"78175252","Maximizing Document-Based Responses in OpenAI: Strategies for Comprehensive Information Retrieval","2024-03-17 12:37:25","","0","21","<openai-api><information-retrieval><large-language-model><gpt-3><gpt-4>","<p>I am encountering an issue with OpenAI's document-based response system using openai assistant API. Despite uploading a file containing information on 20 hotels (totaling 15,000 words), when I ask the assistant to provide me with the names of all the hotels, it only returns the names of the first two hotels. I seek guidance on how to ensure that the entire file is considered in generating responses, rather than restricting output based on a single vector.</p>
<p>How can I ensure that the entire content of a document is considered instead of relying on a specific vector?</p>
<p>For example, if I have a PDF containing data on 20 hotels spanning 15,000 words, and I ask for a list of all the hotels, the response typically includes only two hotel names due to the limitation of considering only one vector at a time. I am seeking guidance on how to enable the system to process the entire document, ensuring that the response includes all 20 hotel names.</p>
","gpt-4"
"78157432","OpenAI API error: ""TypeError: OpenAIApi is not a constructor""","2024-03-14 00:00:46","78158916","-1","191","<node.js><discord><openai-api><gpt-4>","<p>I am having an issue trying to get this code to call my OPENAI_API_KEY in the <code>.env</code> file. I am very new to Node.js, so please speak to me like I'm an idiot.</p>
<p>See the error at the bottom after the code.</p>
<p>Code:</p>
<pre><code>require('dotenv').config();
const { OpenAIApi } = require('openai');

// Initialize OpenAI API client with the API key from your .env file
const openaiClient = new OpenAIApi(process.env.OPENAI_API_KEY);


/**
 * Generates a response using ChatGPT-4 Turbo based on the provided user input.
 * @param {string} userInput - The user's message input.
 * @returns {Promise&lt;string&gt;} - The generated response from ChatGPT-4 Turbo.
 */
async function generateResponse(userInput) {
  try {
    console.log('Sending input to OpenAI API:', userInput);
    const response = await openaiClient.createChatCompletion({
      model: &quot;gpt-4-turbo&quot;,
      messages: [{
        role: &quot;user&quot;,
        content: userInput
      }],
    });

    if (response.data.choices &amp;&amp; response.data.choices.length &gt; 0) {
      console.log('Received response from OpenAI API');
      return response.data.choices[0].message.content;
    } else {
      console.log('No response from OpenAI API.');
      throw new Error('No response from OpenAI API');
    }
  } catch (error) {
    console.error('Failed to generate response from OpenAI API:', error.message, error.stack);
    throw error; // Rethrow to handle it in the calling function
  }
}

module.exports = { generateResponse };
</code></pre>
<p>Error:</p>
<pre><code>L:\ai projects\gpt-pilot\workspace\JS-Rock_Bot\chatService.js:5
const openaiClient = new OpenAIApi(process.env.OPENAI_API_KEY);
                     ^

TypeError: OpenAIApi is not a constructor
    at Object.&lt;anonymous&gt; (L:\ai projects\gpt-pilot\workspace\JS-Rock_Bot\chatService.js:5:22)
    at Module._compile (node:internal/modules/cjs/loader:1376:14)
    at Module._extensions..js (node:internal/modules/cjs/loader:1435:10)
    at Module.load (node:internal/modules/cjs/loader:1207:32)
    at Module._load (node:internal/modules/cjs/loader:1023:12)
    at Module.require (node:internal/modules/cjs/loader:1235:19)
    at require (node:internal/modules/helpers:176:18)
    at Client.&lt;anonymous&gt; (L:\ai projects\gpt-pilot\workspace\JS-Rock_Bot\bot.js:34:30)
    at Client.emit (node:events:518:28)
    at MessageCreateAction.handle (L:\ai projects\gpt-pilot\workspace\JS-Rock_Bot\node_modules\discord.js\src\client\actions\MessageCreate.js:28:14)

Node.js v20.11.1
</code></pre>
<p>I also tried this but it's not working:</p>
<pre><code>const { OpenAIApi } = require('openai');
</code></pre>
","gpt-4"
"78137523","How do I fix the error when running ChatGPT in python?","2024-03-10 21:17:52","","1","333","<python><openai-api><gpt-4>","<p>I need to get a working ChatGPT in python. I used the official documentation from the openal website and wrote this code:</p>
<pre><code>from openai import OpenAI

client = OpenAI(api_key='&lt;my API is written here&gt;')

response = client.chat.completions.create(
  model=&quot;gpt-3.5-turbo&quot;,
  messages=[
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;},
    {&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;}
  ]
)
</code></pre>
<p>After the launch, I received the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;C:\Users\danuk\AppData\Local\Programs\Python\Python312\1.py&quot;, line 5, in &lt;module&gt;
    response = client.chat.completions.create(
  File &quot;C:\Users\danuk\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_utils\_utils.py&quot;, line 275, in wrapper
    return func(*args, **kwargs)
  File &quot;C:\Users\danuk\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\resources\chat\completions.py&quot;, line 663, in create
    return self._post(
  File &quot;C:\Users\danuk\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_base_client.py&quot;, line 1200, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
  File &quot;C:\Users\danuk\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_base_client.py&quot;, line 889, in request
    return self._request(
  File &quot;C:\Users\danuk\AppData\Local\Programs\Python\Python312\Lib\site-packages\openai\_base_client.py&quot;, line 980, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.PermissionDeniedError: &lt;!DOCTYPE html&gt;
&lt;!--[if lt IE 7]&gt; &lt;html class=&quot;no-js ie6 oldie&quot; lang=&quot;en-US&quot;&gt; &lt;![endif]--&gt;
&lt;!--[if IE 7]&gt;    &lt;html class=&quot;no-js ie7 oldie&quot; lang=&quot;en-US&quot;&gt; &lt;![endif]--&gt;
&lt;!--[if IE 8]&gt;    &lt;html class=&quot;no-js ie8 oldie&quot; lang=&quot;en-US&quot;&gt; &lt;![endif]--&gt;
&lt;!--[if gt IE 8]&gt;&lt;!--&gt; &lt;html class=&quot;no-js&quot; lang=&quot;en-US&quot;&gt; &lt;!--&lt;![endif]--&gt;
&lt;head&gt;
&lt;title&gt;Attention Required! | Cloudflare&lt;/title&gt;
&lt;meta charset=&quot;UTF-8&quot; /&gt;
&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;
&lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;IE=Edge&quot; /&gt;
&lt;meta name=&quot;robots&quot; content=&quot;noindex, nofollow&quot; /&gt;
&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,initial-scale=1&quot; /&gt;
&lt;link rel=&quot;stylesheet&quot; id=&quot;cf_styles-css&quot; href=&quot;/cdn-cgi/styles/cf.errors.css&quot; /&gt;
&lt;!--[if lt IE 9]&gt;&lt;link rel=&quot;stylesheet&quot; id='cf_styles-ie-css' href=&quot;/cdn-cgi/styles/cf.errors.ie.css&quot; /&gt;&lt;![endif]--&gt;
&lt;style&gt;body{margin:0;padding:0}&lt;/style&gt;


&lt;!--[if gte IE 10]&gt;&lt;!--&gt;
&lt;script&gt;
  if (!navigator.cookieEnabled) {
    window.addEventListener('DOMContentLoaded', function () {
      var cookieEl = document.getElementById('cookie-alert');
      cookieEl.style.display = 'block';
    })
  }
&lt;/script&gt;
&lt;!--&lt;![endif]--&gt;


&lt;/head&gt;
&lt;body&gt;
  &lt;div id=&quot;cf-wrapper&quot;&gt;
    &lt;div class=&quot;cf-alert cf-alert-error cf-cookie-error&quot; id=&quot;cookie-alert&quot; data-translate=&quot;enable_cookies&quot;&gt;Please enable cookies.&lt;/div&gt;
    &lt;div id=&quot;cf-error-details&quot; class=&quot;cf-error-details-wrapper&quot;&gt;
      &lt;div class=&quot;cf-wrapper cf-header cf-error-overview&quot;&gt;
        &lt;h1 data-translate=&quot;block_headline&quot;&gt;Sorry, you have been blocked&lt;/h1&gt;
        &lt;h2 class=&quot;cf-subheadline&quot;&gt;&lt;span data-translate=&quot;unable_to_access&quot;&gt;You are unable to access&lt;/span&gt; api.openai.com&lt;/h2&gt;
      &lt;/div&gt;&lt;!-- /.header --&gt;

      &lt;div class=&quot;cf-section cf-highlight&quot;&gt;
        &lt;div class=&quot;cf-wrapper&quot;&gt;
          &lt;div class=&quot;cf-screenshot-container cf-screenshot-full&quot;&gt;
            
              &lt;span class=&quot;cf-no-screenshot error&quot;&gt;&lt;/span&gt;
            
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;&lt;!-- /.captcha-container --&gt;

      &lt;div class=&quot;cf-section cf-wrapper&quot;&gt;
        &lt;div class=&quot;cf-columns two&quot;&gt;
          &lt;div class=&quot;cf-column&quot;&gt;
            &lt;h2 data-translate=&quot;blocked_why_headline&quot;&gt;Why have I been blocked?&lt;/h2&gt;

            &lt;p data-translate=&quot;blocked_why_detail&quot;&gt;This website is using a security service to protect itself from online attacks. The action you just performed triggered the security solution. There are several actions that could trigger this block including submitting a certain word or phrase, a SQL command or malformed data.&lt;/p&gt;
          &lt;/div&gt;

          &lt;div class=&quot;cf-column&quot;&gt;
            &lt;h2 data-translate=&quot;blocked_resolve_headline&quot;&gt;What can I do to resolve this?&lt;/h2&gt;

            &lt;p data-translate=&quot;blocked_resolve_detail&quot;&gt;You can email the site owner to let them know you were blocked. Please include what you were doing when this page came up and the Cloudflare Ray ID found at the bottom of this page.&lt;/p&gt;
          &lt;/div&gt;
        &lt;/div&gt;
      &lt;/div&gt;&lt;!-- /.section --&gt;

      &lt;div class=&quot;cf-error-footer cf-wrapper w-240 lg:w-full py-10 sm:py-4 sm:px-8 mx-auto text-center sm:text-left border-solid border-0 border-t border-gray-300&quot;&gt;
  &lt;p class=&quot;text-13&quot;&gt;
    &lt;span class=&quot;cf-footer-item sm:block sm:mb-1&quot;&gt;Cloudflare Ray ID: &lt;strong class=&quot;font-semibold&quot;&gt;86264a8269155ab2&lt;/strong&gt;&lt;/span&gt;
    &lt;span class=&quot;cf-footer-separator sm:hidden&quot;&gt;&amp;bull;&lt;/span&gt;
    &lt;span id=&quot;cf-footer-item-ip&quot; class=&quot;cf-footer-item hidden sm:block sm:mb-1&quot;&gt;
      Your IP:
      &lt;button type=&quot;button&quot; id=&quot;cf-footer-ip-reveal&quot; class=&quot;cf-footer-ip-reveal-btn&quot;&gt;Click to reveal&lt;/button&gt;
      &lt;span class=&quot;hidden&quot; id=&quot;cf-footer-ip&quot;&gt;5.228.83.46&lt;/span&gt;
      &lt;span class=&quot;cf-footer-separator sm:hidden&quot;&gt;&amp;bull;&lt;/span&gt;
    &lt;/span&gt;
    &lt;span class=&quot;cf-footer-item sm:block sm:mb-1&quot;&gt;&lt;span&gt;Performance &amp;amp; security by&lt;/span&gt; &lt;a rel=&quot;noopener noreferrer&quot; href=&quot;https://www.cloudflare.com/5xx-error-landing&quot; id=&quot;brand_link&quot; target=&quot;_blank&quot;&gt;Cloudflare&lt;/a&gt;&lt;/span&gt;
    
  &lt;/p&gt;
  &lt;script&gt;(function(){function d(){var b=a.getElementById(&quot;cf-footer-item-ip&quot;),c=a.getElementById(&quot;cf-footer-ip-reveal&quot;);b&amp;&amp;&quot;classList&quot;in b&amp;&amp;(b.classList.remove(&quot;hidden&quot;),c.addEventListener(&quot;click&quot;,function(){c.classList.add(&quot;hidden&quot;);a.getElementById(&quot;cf-footer-ip&quot;).classList.remove(&quot;hidden&quot;)}))}var a=document;document.addEventListener&amp;&amp;a.addEventListener(&quot;DOMContentLoaded&quot;,d)})();&lt;/script&gt;
&lt;/div&gt;&lt;!-- /.error-footer --&gt;


    &lt;/div&gt;&lt;!-- /#cf-error-details --&gt;
  &lt;/div&gt;&lt;!-- /#cf-wrapper --&gt;

  &lt;script&gt;
  window._cf_translation = {};
  
  
&lt;/script&gt;

&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>I have not found any information about this error. I've tried changing the version, writing different variations of the code, but it doesn't work.</p>
","gpt-4"
"78118947","I am getting error ""No quota is available to deploy this model version and deployment type."" for GPT-4","2024-03-07 05:26:46","","0","366","<azure-openai><gpt-4>","<p>I am trying to deploy GPT-4 but getting the screenshot error. I have added &quot;Cognitive Services OpenAI Contributor&quot; role assignment to my &quot;user&quot; under Azure Open AI -&gt; Access control (IAM). Do you need to do anything else? Why am I getting this error? Do you have any idea?</p>
<p><img src=""https://i.sstatic.net/TAhoD.png"" alt=""Error Message"" /></p>
<p><img src=""https://i.sstatic.net/VdHPW.png"" alt=""Permission Window"" /></p>
<p>I was expecting to deploy GPT4 model succesufully.</p>
","gpt-4"
"78105463","Problem in generating HTML content using GPT4","2024-03-05 05:18:44","","0","91","<html><parsing><openai-api><prompt><gpt-4>","<p>I am trying to generate some html content as output from GPT-4. Below is my prompt:</p>
<pre><code>Write a blog post about {{blog_title}}. Give your output in the following JSON format, do not produce anything else:
{
    &quot;html_content&quot;: String
}
</code></pre>
<p>However, when I try to load the response using json.loads function in python it fails as \n character in the html is not escaped.</p>
<p>I changed my prompt to the following:</p>
<pre><code>Write a blog post about {{blog_title}}. ALWAYS use \\n instead of \n for newline in HTML so that I can parse it i.e. NEVER use just \n for newline.
Give your output in the following JSON format, do not produce anything else:
{
    &quot;html_content&quot;: String
}
</code></pre>
<p>But I see it sometimes failing to stick to the prompt, is there any other way to solve this issue.</p>
","gpt-4"
"78094346","How can I find out the location of the endpoint when using openai Python library and Azure OpenAI?","2024-03-02 21:20:36","78095051","-1","276","<python><openai-api><azure-openai><gpt-4>","<p>E.g., when I make a basic Azure OpenAI request, I don't see the endpoint in the response object:</p>
<pre><code>#Note: This code sample requires OpenAI Python library version 1.0.0 or higher.
import json
import pprint
from openai import AzureOpenAI

client = AzureOpenAI(
  azure_endpoint = &quot;https://xxxxxx.openai.azure.com/&quot;,
  api_key='xxxxxxxxxxxxxxxxxxxxx',
  api_version=&quot;2023-07-01-preview&quot;
)

message_text = [{&quot;role&quot;:&quot;system&quot;,&quot;content&quot;:&quot;You are an AI assistant that helps people find information.&quot;}]
completion = client.chat.completions.create(
  model=&quot;gpt-4xxxxxxxx&quot;, 
  messages = message_text,
  temperature=0.7,
  max_tokens=800,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  stop=None
)

print('completion:\n')
pprint.pprint(completion)

# Convert Python object to JSON
json_data = json.dumps(completion, default=lambda o: o.__dict__, indent=4)

# Print JSON
print(json_data)
</code></pre>
<p>Looking at  the output, the response object <code>completion</code> contains:</p>
<pre><code>ChatCompletion(id='chatcmpl-xxxxxxxxx', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Great! How can I assist you today?', role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1709313222, model='gpt-4', object='chat.completion', system_fingerprint='fp_xxxxx', usage=CompletionUsage(completion_tokens=9, prompt_tokens=18, total_tokens=27), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])
</code></pre>
<p>How can I find out the location of the endpoint when using openai Python library and Azure OpenAI?</p>
<hr />
<p>I know that one may view the location on <a href=""https://portal.azure.com/"" rel=""nofollow noreferrer"">https://portal.azure.com/</a>:</p>
<p><a href=""https://i.sstatic.net/CFwjd.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CFwjd.png"" alt=""enter image description here"" /></a></p>
<p>but I don't have access to all Azure OpenAI instances that I work with in my account.</p>
","gpt-4"
"78061063","Adding inline links in LLM response","2024-02-26 12:53:08","","0","971","<large-language-model><gpt-4>","<h1>Background</h1>
<p>Bing Copilot's response is partially clickable. That means, some sentences are links. I'd like to have this feature in my LLM.</p>
<p>In <a href=""https://github.com/GoogleCloudPlatform/generative-ai/discussions/248"" rel=""nofollow noreferrer"">this discussion</a>, it is recommended to lookup the source document and then:</p>
<blockquote>
<p>feed that link as part of the prompt to the LLM and do some prompt engineering to also return the documentation link alongside the response.</p>
</blockquote>
<p>Based on my understanding, the response can then be converted to HTML with <code>&lt;a href=&quot;...&quot;&gt;</code> tags as a post-processing step.</p>
<h1>Question</h1>
<p>Given that the source documents are provided in the prompt, as recommended, what should the prompt include in order to get a response that can be converted into clickable sentences as a post-process step?</p>
","gpt-4"
"78005393","How can I find out the GPT-4 model version when using openai Python library and Azure OpenAI?","2024-02-16 05:56:39","78738211","0","287","<python><version><openai-api><azure-openai><gpt-4>","<p>I use GPT-4 via <code>openai</code> Python library and Azure OpenAI. How can I find out the GPT-4 model version by using the <code>openai</code> Python library (and not looking at <a href=""https://portal.azure.com/"" rel=""nofollow noreferrer"">https://portal.azure.com/</a> because for some Azure OpenAI instances I only have the credentials to use the API but I can't view them on <a href=""https://portal.azure.com/"" rel=""nofollow noreferrer"">https://portal.azure.com/</a>)?</p>
<p>I read:</p>
<p><a href=""https://platform.openai.com/docs/models/continuous-model-upgrades"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/models/continuous-model-upgrades</a>:</p>
<blockquote>
<p>You can verify this by looking at the response object after sending a request. The response will include the specific model version used (e.g. gpt-3.5-turbo-0613).</p>
</blockquote>
<p><a href=""https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo</a>:</p>
<blockquote>
<p>gpt-4 currently points to <code>gpt-4-0613</code>.</p>
</blockquote>
<hr />
<p>However, I tried calling gpt-4 version 0314 and gpt-4 version 0125-preview: for both of them, the response object after sending a request only contains <code>gpt-4</code>:</p>
<pre class=""lang-py prettyprint-override""><code>ChatCompletion(
    id='chatcmpl-8slN5Cbbsdf16s51sdf8yZpRXZM1R', 
    choices=[
        Choice(
            finish_reason='stop', 
            index=0, 
            logprobs=None, 
            message=ChatCompletionMessage(
                content='blahblah', 
                role='assistant', 
                function_call=None, 
                tool_calls=None
            ), 
            content_filter_results={
                'hate': {'filtered': False, 'severity': 'safe'}, 
                'self_harm': {'filtered': False, 'severity': 'safe'}, 
                'sexual': {'filtered': False, 'severity': 'safe'}, 
                'violence': {'filtered': False, 'severity': 'safe'}
            }
        )
    ], 
    created=1708062499, 
    model='gpt-4', 
    object='chat.completion', 
    system_fingerprint='fp_8absdfsdsfs',
    usage=CompletionUsage(
        completion_tokens=185, 
        prompt_tokens=4482, 
        total_tokens=4667
    ), 
    prompt_filter_results=[
        {
            'prompt_index': 0, 
            'content_filter_results': {
                'hate': {'filtered': False, 'severity': 'safe'}, 
                'self_harm': {'filtered': False, 'severity': 'safe'}, 
                'sexual': {'filtered': False, 'severity': 'safe'}, 
                'violence': {'filtered': False, 'severity': 'safe'}
            }
        }
    ]
)
</code></pre>
<p>How can I find out the GPT-4 model version when using <code>openai</code> Python library and Azure OpenAI?</p>
","gpt-4"
"77998898","Troubleshooting GPT-4 Integration with SQLDatabaseToolkit and create_sql_agent for Prompt Passing Error","2024-02-15 07:03:54","78040867","4","553","<openai-api><prompt><langchain><gpt-4><mssql-tools>","<p>I was previously using <code>SQLDatabaseChain</code> to connect LLM (Language Model) with my database, and it was functioning correctly with GPT-3.5. However, when attempting the same process with GPT-4, I encountered an error stating &quot;incorrect syntax near 's&quot;</p>
<p>To address this issue, I opted to use <code>SQLDatabaseToolkit</code> and the <code>create_sql_agent</code> function. However, I encountered a problem with this approach as I was unable to pass a prompt. When attempting to include a <code>PromptTemplate</code> in the <code>create_sql_agent</code> argument, it resulted in errors.</p>
<p><code>ValueError: Prompt missing required variables: {'tool_names', 'agent_scratchpad', 'tools'}</code></p>
<p>Below is my code:</p>
<pre><code>toolkit = SQLDatabaseToolkit(db=db, llm=llm)

agent_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=True,
    prompt=MSSQL_PROMPT,
)
</code></pre>
","gpt-4"
"77996614","Cannot use GPT API on Google Collab","2024-02-14 18:43:48","77996653","-4","755","<google-colaboratory><openai-api><chatgpt-api><gpt-4>","<p>I'm working on a project in Google Colab where I need to automatically generate text using pre-created prompts with the GPT-4 model from OpenAI. I wrote the following lines:</p>
<pre class=""lang-py prettyprint-override""><code>!pip install openai
!pip install cohere tiktoken
import openai
import os
os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;my secret api&quot;

response = openai.Completion.create(
  model=&quot;gpt-4&quot;, 
  prompt=&quot;hi&quot;,
  temperature=0.7,
  max_tokens=150
)
print(response.choices[0].text.strip())
</code></pre>
<p>However, executing this code results in the following error:</p>
<blockquote>
<p>APIRemovedInV1:</p>
<p>You tried to access openai.Completion, but this is no longer supported in openai&gt;=1.0.0 - see the README at <a href=""https://github.com/openai/openai-python"" rel=""nofollow noreferrer"">https://github.com/openai/openai-python</a> for the API.</p>
<p>You can run <code>openai migrate</code> to automatically upgrade your codebase to use the 1.0.0 interface.</p>
<p>Alternatively, you can pin your installation to the old version, e.g. <code>pip install openai==0.28</code></p>
</blockquote>
<p>I've looked through the OpenAI Python library documentation, the migration guide, and various resources for a solution that is compatible with Google Colab as of February 2024, but I haven't found a clear answer on how to proceed.</p>
<ul>
<li>I checked the official GitHub repository: <a href=""https://github.com/openai/openai-python"" rel=""nofollow noreferrer"">https://github.com/openai/openai-python</a></li>
<li>I reviewed the discussion and migration guide here: <a href=""https://github.com/openai/openai-python/discussions/742"" rel=""nofollow noreferrer"">https://github.com/openai/openai-python/discussions/742</a></li>
<li>I looked for information on using GPT-4 specifically here: <a href=""https://help.openai.com/en/articles/7127997-how-can-i-use-gpt-4-in-chatgpt"" rel=""nofollow noreferrer"">https://help.openai.com/en/articles/7127997-how-can-i-use-gpt-4-in-chatgpt</a></li>
<li>I also reviewed recent commits for any clues: <a href=""https://github.com/openai/openai-python/commit/86379b4471d67a9d2e85f0b0c098787fb99aa4e0?diff=split&amp;w=1"" rel=""nofollow noreferrer"">https://github.com/openai/openai-python/commit/86379b4471d67a9d2e85f0b0c098787fb99aa4e0?diff=split&amp;w=1</a></li>
</ul>
<p>Could someone provide guidance or an updated code snippet that works with the latest version of the OpenAI library in Google Colab?</p>
<p>I don't know much about it, I need a solution that works on Google Colab and is up to date as of February 2024.</p>
","gpt-4"
"77983066","How to use SELF-DISCOVER prompting framework with OpenAI GPT-4 model?","2024-02-12 16:57:09","","0","63","<python><openai-api><gpt-4>","<p>I have read this blog - <a href=""https://medium.com/@aitoolsfinder/revolutionizing-language-model-reasoning-googles-self-discover-prompting-framework-6f44f5be41d1"" rel=""nofollow noreferrer""># Revolutionizing Language Model Reasoning: Google‚Äôs SELF-DISCOVER Prompting Framework</a></p>
<p>The content is like the following:</p>
<p>In recent years, large language models (LLMs) have shown remarkable promise in a variety of applications, from language translation to chatbots. However, one major challenge in working with LLMs is their limited reasoning capabilities, which can hinder their ability to solve complex problems and understand human-like language.</p>
<p>To address this challenge, researchers from Google DeepMind and the University of Southern California have developed a new approach called the SELF-DISCOVER prompting framework. This framework promises to significantly enhance the reasoning abilities of LLMs, potentially revolutionizing the performance of leading models such as OpenAIs GPT-4 and Googles PaLM 2.</p>
<p><a href=""https://i.sstatic.net/ZFw2T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZFw2T.png"" alt=""enter image description here"" /></a></p>
<p>After reading this blog, I felt the urge to try self-discover framework with gpt-4.</p>
<p>How to use it? And How to implement in python?</p>
","gpt-4"
"77949086","Create a gpt-3.5 API request that determines whether any time range in a list intersects with a given time range","2024-02-06 15:53:27","","0","32","<openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I've created a prompt that should select a requested number of employees from the list. But the <strong>step 1</strong> doesn't work properly. Sometimes GPT takes in account only the time range and ignores the date. I tried to describe this step in a different way many times, tried different time¬†formats including UTC, but didn't succeed. Maybe experienced prompt creators can tell what's wrong with my prompt?</p>
<hr />
<p><strong>User message:</strong></p>
<pre><code>{
¬† &quot;employees&quot;: [
¬† ¬† {
¬† ¬† ¬† &quot;id&quot;: 1,
¬† ¬† ¬† &quot;name&quot;: &quot;Bender Rodriguez&quot;,
¬† ¬† ¬† &quot;position&quot;: &quot;developer&quot;,
¬† ¬† ¬† &quot;experience&quot;: &quot;middle&quot;,
¬† ¬† ¬† &quot;interviews_conducted&quot;: 0,
¬† ¬† ¬† &quot;busy_date_time&quot;: [
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 10 2024 06:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 07:00&quot;},
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 11 2024 10:00&quot;, &quot;end_time&quot;: &quot;February 11 2024 11:00&quot;}
¬† ¬† ¬† ]
¬† ¬† },
¬† ¬† {
¬† ¬† ¬† &quot;id&quot;: 2,
¬† ¬† ¬† &quot;name&quot;: &quot;Philip Fry&quot;,
¬† ¬† ¬† &quot;position&quot;: &quot;developer&quot;,
¬† ¬† ¬† &quot;experience&quot;: &quot;middle&quot;,
¬† ¬† ¬† &quot;interviews_conducted&quot;: 2,
¬† ¬† ¬† &quot;busy_date_time&quot;: [
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 10 2024 13:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 14:00&quot;}
¬† ¬† ¬† ]
¬† ¬† },
¬† ¬† {
¬† ¬† ¬† &quot;id&quot;: 3,
¬† ¬† ¬† &quot;name&quot;: &quot;John Zoidberg&quot;,
¬† ¬† ¬† &quot;position&quot;: &quot;developer&quot;,
¬† ¬† ¬† &quot;experience&quot;: &quot;junior&quot;,
¬† ¬† ¬† &quot;interviews_conducted&quot;: 1,
¬† ¬† ¬† &quot;busy_date_time&quot;: [
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 10 2024 10:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 11:00&quot;}
¬† ¬† ¬† ]
¬† ¬† },
¬† ¬† {
¬† ¬† ¬† &quot;id&quot;: 4,
¬† ¬† ¬† &quot;name&quot;: &quot;Turanga Leela&quot;,
¬† ¬† ¬† &quot;position&quot;: &quot;developer&quot;,
¬† ¬† ¬† &quot;experience&quot;: &quot;senior&quot;,
¬† ¬† ¬† &quot;interviews_conducted&quot;: 1,
¬† ¬† ¬† &quot;busy_date_time&quot;: [
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 10 2024 10:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 11:00&quot;}
¬† ¬† ¬† ]
¬† ¬† },
¬† ¬† {
¬† ¬† ¬† &quot;id&quot;: 5,
¬† ¬† ¬† &quot;name&quot;: &quot;Amy Wong&quot;,
¬† ¬† ¬† &quot;position&quot;: &quot;developer&quot;,
¬† ¬† ¬† &quot;experience&quot;: &quot;senior&quot;,
¬† ¬† ¬† &quot;interviews_conducted&quot;: 0,
¬† ¬† ¬† &quot;busy_date_time&quot;: [
¬† ¬† ¬† ¬† {&quot;start_time&quot;: &quot;February 10 2024 10:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 11:00&quot;}
¬† ¬† ¬† ]
¬† ¬† }
¬† ]
}

Do step-by-step:

1. Remove from the &quot;employees&quot; list above each employee if any time interval in 
&quot;busy_date_time&quot; list overlaps with &quot;required_date_time&quot;.

2. If the number of employees left in the &quot;employees&quot; list is less than 
&quot;required_employees_number&quot;, set the new value to &quot;required_employees_number&quot; equal 
to the number of employees left in the &quot;employees&quot; list.

3. Select &quot;required_employees_number&quot; employees with &quot;required_experience&quot; and lower 
&quot;interviews_conducted&quot; value. You shouldn't find the one with the lowest 
&quot;interviews_conducted&quot; value among all, but a required number of employees which is 
&quot;required_employees_number&quot;.

4. Check the previous step where you usually make the mistake of selecting 1 employee
with minimum &quot;interviews_conducted&quot; value among all employees when you need to select 
a list of &quot;required_employees_number&quot; employees.

required_date_time = '''{&quot;start_time&quot;: &quot;February 10 2024 10:00&quot;, &quot;end_time&quot;: &quot;February 10 2024 11:00&quot;}'''
required_employees_number = 1
required_experience = &quot;middle&quot;
</code></pre>
<hr />
<p><strong>System message:</strong>
You are a computer program that strictly follows the user's instructions. Your output is always only a list of employee's id. Any other notes or comments are forbidden.</p>
<hr />
<p><strong>GPT settings:</strong></p>
<ul>
<li>Temperature: 0</li>
<li>Top P: 0</li>
<li>Frequency penalty: 0</li>
<li>Presence penalty: 0</li>
</ul>
<hr />
<ul>
<li><strong>Expected result:</strong> [1]</li>
<li><strong>Actual result:</strong> [2]</li>
</ul>
","gpt-4"
"77899667","OpenAI API error: ""TypeError: Configuration is not a constructor at Object.""","2024-01-29 12:17:37","","1","1658","<javascript><openai-api><gpt-4>","<p>I am using the OpenAI API in JavaScript. Below is my code:</p>
<pre><code>require('dotenv').config();

const OpenAI_Api = process.env.OpenAIApi || 'Mykey';

const { Configuration, OpenAIApi } = require('openai');

const configuration = new Configuration({
          apiKey: OpenAI_Api
        });

const openai = new OpenAIApi(configuration);

const createChatCompletion = async () =&gt; {
  try {
    const response = await openai.createChatCompletion({
      model: 'gpt-4.0',
      messages: [
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'user', content: 'Who won the world series in 2020?' },
        { role: 'assistant', content: 'The Los Angeles Dodgers won the World Series in 2020.' },
        { role: 'user', content: 'Where was it played?' },
        { role: 'assistant', content: 'The World Series was played in Arlington, Texas at the Globe Life Field.' },
      ],
    });

    console.log(response.choices[0].message.content); // Display the generated response
  } catch (error) {
    console.error('Error creating chat completion:', error);
  }
};

createChatCompletion();
</code></pre>
<p>It is saved in <code>chatgpt.js</code>. However, when I run <code>node chatgpt.js</code>, I receive an error.</p>
<pre class=""lang-none prettyprint-override""><code>const configuration = new Configuration({
                      ^

TypeError: Configuration is not a constructor
    at Object.&lt;anonymous&gt; (D:\OneDrive - The University of Nottingham\ESR1\work\Knowledge graph\doctorai_ui_gpt3_test\chatgpt.js:7:23)
    at Module._compile (node:internal/modules/cjs/loader:1101:14)
    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1153:10)
    at Module.load (node:internal/modules/cjs/loader:981:32)
    at Function.Module._load (node:internal/modules/cjs/loader:822:12)
    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:79:12)
    at node:internal/main/run_main_module:17:47
</code></pre>
","gpt-4"
"77893769","Handle descriptive questions accurately in RAG","2024-01-28 05:53:07","","0","108","<indexing><chatbot><large-language-model><gpt-4><retrieval-augmented-generation>","<p>RAG method can retrieve information from our own knowledge base, but how can we ensure it accurately answers descriptive questions?</p>
<p>E.g. If I store entire text from books (<a href=""https://www.gutenberg.org/"" rel=""nofollow noreferrer"">Project Gutenberg</a>), and user asks a descriptive question like ‚ÄúHow many books authored by O Henry are available?‚Äù then I want the exact number. Descriptive question would be any such question which would effectively require scanning all documents rather than finding documents ‚Äúsimilar‚Äù to the context of the question.</p>
<p>If this is not possible to achieve with RAG and requires specifically tailored solutions using Solr, etc. where faceting is possible, is it possible to at least infer from the question when to send the query to RAG pipeline and when to use hard index search?</p>
","gpt-4"
"77859725","GPT-4 returning text-based prompt instead of returning the function call designated to do the same prompt","2024-01-22 12:05:30","","4","244","<openai-api><azure-openai><gpt-4><chat-gpt-4>","<p>I'm using Azure OpenAI for implementing a conversational AI using GPT-4 to act as an assistant for resolving user queries. The assistant is supposed to follow a predefined set of instructions, including using provided functions for specific tasks.</p>
<p>The following is a trimmed down version of the prompt that I am giving to GPT-4.</p>
<pre><code>You are an AI assistant for xyz company. Follow the instructions to resolve user queries.

1. Ask for name and address.
2. Ask for date of birth.
3. Ask the user if they want OptionA or OptionB by calling the provided function prompt_user_to_choose_an_option.
4. Do a search with the chosen option.
</code></pre>
<p>I have defined the following function to prompt the user with the options.</p>
<pre><code>{
            &quot;name&quot;: &quot;prompt_user_to_choose_an_option&quot;,
            &quot;description&quot;: &quot;Display the options for the user to choose from (OptionA or OptionB)&quot;,
            &quot;parameters&quot;: {
                &quot;properties&quot;: {
                    &quot;options&quot;: {
                        &quot;type&quot;: &quot;array&quot;,
                        &quot;items&quot;: {
                            &quot;type&quot;: &quot;string&quot;
                        },
                        &quot;description&quot;: &quot;The list of available options for the user to choose from.&quot;
                    }
                },
                &quot;required&quot;: [
                    &quot;options&quot;
                ],
                &quot;type&quot;: &quot;object&quot;
            }
        }
</code></pre>
<p>But, in step 3, instead of returning the function call, GPT goes ahead and asks the user with a plain text prompt &quot;Do you want OptionA or OptionB?&quot;, without calling the given function. If the user responds with say, &quot;OptionB&quot;, it then returns the function call which was supposed to be returned in the previous step.</p>
<p>Why does this happen? Why doesn't GPT return the function call and instead directly prompts the user with a plain text prompt? I want this prompt to happen through the function call because I need to display a specific UI element for the user at this step.</p>
<p>I have tried adding instructions asking GPT to never use a text-based prompt when asking the user for options and use the function instead. That works sometimes but is not consistent. Is there a proper, consistent solution to this problem?</p>
","gpt-4"
"77849010","How do I create an Azure OpenAI deployment using GPT-4 if my model options are only GPT-35-turbo*?","2024-01-19 21:56:48","","-2","833","<azure><openai-api><azure-openai><gpt-4>","<p>I am able to create deployments using gpt-35-turbo* models. I want to create a gpt-4 deployment, but I don't see those models available in my subscription. How do I find the gpt-4 models?</p>
<p><a href=""https://i.sstatic.net/oYgHR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/oYgHR.png"" alt=""enter image description here"" /></a></p>
","gpt-4"
"77839828","OpenAI Assistants API: Why does a single question I ask my assistant spend so many tokens?","2024-01-18 13:39:17","77841022","0","1330","<openai-api><gpt-4><openai-assistants-api>","<p>I have a NodeJS program that connects to <a href=""https://www.npmjs.com/package/openai"" rel=""nofollow noreferrer"">OpenAI's assistant API</a> to create messages. I have followed <a href=""https://platform.openai.com/docs/assistants/overview"" rel=""nofollow noreferrer"">this documentation</a> from OpenAI to create the steps below:</p>
<ol>
<li>I have created an Assistant (gpt-4-1106-preview) and a thread in that Assistant that I'm accessing to interact with.</li>
<li>Add a message to the thread. The message contains around 1000 tokens, checked via <a href=""https://platform.openai.com/tokenizer"" rel=""nofollow noreferrer"">https://platform.openai.com/tokenizer</a></li>
</ol>
<pre><code>    openai.beta.threads.messages.create(threadId, {
        role: &quot;user&quot;,
        content: createMessage(),
    });
</code></pre>
<ol start=""3"">
<li>Run the assistant</li>
</ol>
<pre><code>    await openai.beta.threads.runs.create(threadId, {
        assistant_id: assistantId,
        instructions:
             &quot;Please address the user as Mahesh. The user is an administrator.&quot;,
    });
</code></pre>
<ol start=""4"">
<li>Check the status. I'm running this every 5 seconds until the status is &quot;completed&quot;</li>
</ol>
<pre><code>    await openai.beta.threads.runs.retrieve(threadId, runId);
</code></pre>
<ol start=""5"">
<li>Get the last response from the Assistant</li>
</ol>
<pre><code>   const messages = await openai.beta.threads.messages.list(threadId, {
      limit: 1,
   });
</code></pre>
<p>This code takes around 250,000 tokens to complete. The image shows today's token usage for three requests.</p>
<p><a href=""https://i.sstatic.net/UWlen.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UWlen.png"" alt=""enter image description here"" /></a></p>
","gpt-4"
"77829385","How to visualize and correlate scores obtained from GPT4 API and ChatGPT?","2024-01-17 00:24:59","","0","68","<python><ggplot2><visualization><correlation><gpt-4>","<p>I have a question, I have obtained scores ranging from (0 to 10) resulting <code>GPT4 API</code> and <code>ChatGPT</code> prompts but they are based ran on the same biological processes. Now, I am interested to correlate these scores or find concordance. I am seeking help to how to best represent these scores or correlate scores. I tried using <code>ggplot</code> heatmap (code below) but this wasn't very helpful because I could not correlate which row (feature) belong to which column (process), but, the issue is there &quot;Inf&quot; values in the dataframe. I am interested to correlate each feature to each process or column, and find concordance. Probably, I am not sure, a heatmap coupled with boxplot or violin plot would help in this case?</p>
<p>Ps: log(Method_1 / Method_2) will return Inf if Method_2 has a value of zero, it will return -Inf if Method_1 is zero, and it will return NaN if both values are zero.</p>
<p>The scores are in the form of 2 R dataframes as templates given below:</p>
<h2>API scores</h2>
<pre class=""lang-r prettyprint-override""><code>dput(Method_1)
#&gt;           A_Process B_Process C_Process D_Process E_Process F_Process G_Process
#&gt; Feature_1         9         5         0         5         7         0         3
#&gt; Feature_2         2         6         2         4         7         7         4
#&gt; Feature_3         0         0         0         0         0         0         2
#&gt; Feature_4         2         4         1         1         7         0         2
#&gt; Feature_5         0         6         0         0         6         7         2
#&gt;           H_Process I_Process J_Process
#&gt; Feature_1         4         6         2
#&gt; Feature_2         0         0         0
#&gt; Feature_3         5         7         7
#&gt; Feature_4         3         5         6
#&gt; Feature_5         3         5         6
</code></pre>
<h2>ChatGPT scores</h2>
<pre><code>dput(Method_2)
#&gt;           A_Process B_Process C_Process D_Process E_Process F_Process G_Process
#&gt; Feature_1         1         5         2         3         7         2         1
#&gt; Feature_2         9         6         1         3         6         7         0
#&gt; Feature_3         9         7         2         6         6         2         3
#&gt; Feature_4         0         8         6         6         8         7         4
#&gt; Feature_5         8         5         2         2         6         6         3
#&gt;           H_Process I_Process J_Process
#&gt; Feature_1         1         3         8
#&gt; Feature_2         0         5         5
#&gt; Feature_3         4         4         7
#&gt; Feature_4         4         4         7
#&gt; Feature_5         5         4         6
</code></pre>
<pre><code>log_ratios &lt;- log(Method_1 / Method_2)

# Heatmap visualization
library(ggplot2)
library(reshape2)

# Assuming 'log_ratios' is a dataframe where the rownames are features
log_ratios$Feature &lt;- rownames(log_ratios)
log_ratios_melted &lt;- melt(log_ratios, id.vars = &quot;Feature&quot;)

# Now plotting
ggplot(log_ratios_melted, aes(variable, Feature, fill = value)) +
  geom_tile() +
  scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, midpoint = 0) +
  theme_minimal() +
  xlab(&quot;Process&quot;) +
  ylab(&quot;Feature&quot;) +
  ggtitle(&quot;Heatmap of Concordance&quot;)

ggplot(log_ratios_melted, aes(x = Feature, y = value)) +
  geom_violin(trim = FALSE) +
  facet_wrap(~ variable, scales = &quot;free_y&quot;) + # Ensure 'variable' is the column with process names
  theme_minimal() +
  xlab(&quot;Feature&quot;) +
  ylab(&quot;Log Ratio&quot;) +
  ggtitle(&quot;Question wise distribution of concordance&quot;)
</code></pre>
<a href=""https://imgur.com/fDRUw1n"" rel=""nofollow noreferrer"">Probable data presentation visualization</a>
","gpt-4"
"77801232","Issue with @azure/openai npm package - Empty toolCalls in response","2024-01-11 15:31:13","","0","309","<openai-api><function-call><azure-openai><gpt-4>","<p>I am currently using the @azure/openai npm package for Node.js to interact with the GPT-4 model. I have noticed that the toolCalls in the response are empty, even though I have included tools in my request.</p>
<p>Code:</p>
<pre class=""lang-js prettyprint-override""><code>const getCurrentWeather = {
  name: 'get_current_weather',
  description: 'Get the current weather in a given location',
  parameters: {
    type: 'object',
    properties: {
      location: {
        type: 'string',
        description: 'The city and state, e.g. San Francisco, CA',
      },
      unit: {
        type: 'string',
        enum: ['celsius', 'fahrenheit'],
      },
    },
    required: ['location'],
  },
};

const messages = [{ role: 'user', content: 'What is the weather like in Glasgow Scotland?' }];
const options = {
  tools: [
    {
      type: 'function',
      function: getCurrentWeather,
    },
  ],
};
client.getChatCompletions(deploymentIdBest, messages, options).then((res) =&gt; console.log(res));
</code></pre>
<p>Expected Behavior:
Ofcourse we expect something like this in the response:</p>
<pre class=""lang-json prettyprint-override""><code>{'role': 'assistant',
 'content': None,
 'tool_calls': [{'id': 'call_o7uyztQLeVIoRdjcDkDJY3ni',
   'type': 'function',
   'function': {'name': 'get_current_weather',
    'arguments': '{\n  &quot;location&quot;: &quot;Glasgow, Scotland&quot;,\n  &quot;format&quot;: &quot;celsius&quot;\n}'}}]}
</code></pre>
<p>Actual Behavior:
This is the full response from the azure/openai client (tried both gpt-4-turbo and gpt-35-turbo):</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;id&quot;: &quot;chatcmpl-8fr5PAthe2uTIHVjd8mcy7pOMkrx5&quot;,
  &quot;object&quot;: &quot;chat.completion&quot;,
  &quot;model&quot;: &quot;gpt-4&quot;,
  &quot;usage&quot;: {
    &quot;promptTokens&quot;: 172,
    &quot;completionTokens&quot;: 22,
    &quot;totalTokens&quot;: 194
  },
  &quot;created&quot;: &quot;1970-01-20T17:36:26.203Z&quot;,
  &quot;promptFilterResults&quot;: [
    {
      &quot;promptIndex&quot;: 0,
      &quot;contentFilterResults&quot;: {}
    }
  ],
  &quot;choices&quot;: [
    {
      &quot;index&quot;: 0,
      &quot;finishReason&quot;: &quot;tool_calls&quot;,
      &quot;contentFilterResult&quot;: {
        &quot;error&quot;: {
          &quot;code&quot;: &quot;content_filter_error&quot;,
          &quot;message&quot;: &quot;The contents are not filtered&quot;
        }
      },
      &quot;message&quot;: {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;toolCalls&quot;: []
      },
      &quot;contentFilterResults&quot;: {}
    }
  ]
}
</code></pre>
<p>@azure/openai versions in question: 1.0.0-beta.10 and 1.0.0-alpha.20240104.1</p>
","gpt-4"
"77725042","I passed an entire JSON DB to the gpt-4-1106-preview api but it's costing me a lot. How can I optimize it?","2023-12-28 04:43:10","","0","88","<javascript><json><gpt-4>","<p>So, I have the following real estate db:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>[
  {
    ""Bodegas"": [
      {
        ""file"": ""....pdf"",
        ""Tipo Operacion"": ""Renta"",
        ""Zona"": ""HUMBOLDT"",
        ""Presupuesto Renta"": 24000
      },
      {
        ""file"": ""B_R_V_HUMBOLDT (a-s) 715.pdf"",
        ""Tipo Operacion"": ""Renta y Venta"",
        ""Zona"": ""HUMBOLDT"",
        ""Presupuesto Renta"": 60000,
        ""Presupuesto Venta"": 11000000
      }
    ]
  },
  {
    ""Houses"": [
      {
        ""file"": ""....pdf"",
        ""Tipo Operacion"": ""Renta"",
        ""Zona"": ""Camino Real Olmedillas"",
        ""Presupuesto Renta"": 13500,
        ""Recamaras"": 3,
        ""Amueblado"": false,
        ""Mascotas"": false,
        ""Amenidades"": [
          ""Cochera 2 autos"",
          ""2.5 ba√±os"",
          ""Cocina"",
          ""Sala de TV"",
          ""Cuarto de lavado"",
          ""Cuarto de servicio"",
          ""Bar""
        ]
      },
      {
        ""file"": ""....pdf"",
        ""Tipo Operacion"": ""Renta"",
        ""Zona"": ""El Carmen"",
        ""Presupuesto Renta"": 37000,
        ""Recamaras"": 4,
        ""Amueblado"": false,
        ""Mascotas"": false,
        ""Amenidades"": [
          ""Cochera 2 autos"",
          ""3.5 ba√±os"",
          ""Cocina"",
          ""Sala de TV"",
          ""√Årea de servicio"",
          ""2 patios"",
          ""Biblioteca"",
          ""Estudio""
        ]
      },
      ...</code></pre>
</div>
</div>
</p>
<p>And I'm passing it to the gpt-4-1106-preview api like this:</p>
<p><div class=""snippet"" data-lang=""js"" data-hide=""false"" data-console=""true"" data-babel=""false"">
<div class=""snippet-code"">
<pre class=""snippet-code-js lang-js prettyprint-override""><code>const fs = require('fs');  // Assuming 'fs' module is required in the actual code

const jsonFile = fs.readFileSync(""./db.json"", ""utf8"");
const db = JSON.stringify(jsonFile);

async function sendMessage(message) {
  const chatCompletion = await openai.chat.completions.create({
    messages: [
      {
        role: ""system"",
        content: `
        
        Based on the user's message, choose one of the following options and return only the name of the catalog file that corresponds to the user's search.

        You can return multiple files depending on the user's search:
        
        ${db}
        
        If no file is found, or the user is selling, offering, or it is noticed that they do not request or want a property, return the following message: nothing 

        So ignore the following words: offer, sell, offers, sells, referred, ref 
        
        `,
      },
      {
        role: ""user"",
        content: message,
      },
    ],
    model: ""gpt-4-1106-preview"",
  });

  return chatCompletion.choices[0].message.content;
}</code></pre>
</div>
</div>
</p>
<p>As you can imagine, this implementation works like a charm for retrieving and searching properties. However, the caveat is that the costs are massive.</p>
<p>How can I reduce the costs or have a context of the db? Pinecone? Langchain?</p>
","gpt-4"
"77719952","How to Optimize Retrieve-and-Generate Model for Context-Relevant Responses in a GPT-3.5 Chatbot?","2023-12-27 03:45:42","","0","57","<chatbot><information-retrieval><large-language-model><chatgpt-api><gpt-4>","<p>I am developing an internal enterprise chatbot based on GPT-3.5. The current implementation utilizes a Retrieve-and-Generate (RAG) approach learned in a class, where the user's prompt is used for vector retrieval to provide &quot;known information&quot; to assist the model in generating answers. However, this approach is proving suboptimal in many scenarios because the model receives &quot;known information&quot; that can interfere with its responses, even when such information is unnecessary for the user's query.</p>
<p>For instance, when asked about the water resistance of a smartwatch, model GP335, the retrieval system supplies related known information (e.g., &quot;GP335 has an IP68 water resistance rating&quot;), leading to a direct and possibly redundant answer from the model.</p>
<p>The issue becomes more pronounced when the user switches topics, such as moving from discussing water resistance to asking about size, or simply exchanging greetings. The previously relevant information remains accessible to the model and affects the quality of the responses.</p>
<p>Here are some specific scenarios:</p>
<p>User asks: &quot;How is the water resistance of the smartwatch GP335?&quot;</p>
<p>Retrieved known information: &quot;GP335 has an IP68 water resistance rating&quot;</p>
<p>Model responds: &quot;The GP335 has an IP68 water resistance rating.&quot;</p>
<p>User asks: &quot;What about its size?&quot;</p>
<p>Retrieved known information (wrongly associated with another product): &quot;Tank AX900 is 12 meters long and 10 meters wide&quot;</p>
<p>Model responds: &quot;Its size is 12 meters long and 10 meters wide.&quot;</p>
<p>User asks: &quot;Hello&quot;</p>
<p>Retrieved known information: &quot;The installation process for smartwatch GP335 is 1...2...3...&quot;</p>
<p>Model responds: &quot;Hello, it seems you have provided detailed information on the installation process of the smartwatch GP335...&quot;</p>
<p>What are some best practices, techniques, or strategies that you can recommend to address these issues?</p>
<pre><code>def get_response_from_llm(session, model=&quot;gpt-3.5-turbo-1106&quot;):
    # Retrieve the latest user prompt from the session.
    user_query = session[-1]['content']

    # Convert the user prompt into embeddings.
    embedded_user_query = get_embeddings([user_query])

    # Initialize the vector database client.
    vectorDB_client = chromadb.PersistentClient(path=&quot;mypath&quot;)
    # Access the specific collection in the vector database.
    collection = vectorDB_client.get_collection(name=&quot;mycollection&quot;)
    # Perform a query in the vector database to find relevant documents based on the embedded user query.
    search_results = collection.query(
        query_embeddings=embedded_user_query,
        n_results=2  # Retrieve the top 2 results
    )

    # Build the prompt using a template and the most relevant document found.
    prompt = build_prompt(
        prompt_template,  # A predefined template for the prompt
        info=search_results['documents'][0],  # The most relevant document
        query=user_query  # The original user query
    )

    # Print the constructed prompt to the console for debugging.
    print(prompt)

    # Update the last item in the session with the newly constructed prompt.
    session[-1] = {
        &quot;role&quot;: &quot;user&quot;,
        &quot;content&quot;: str(prompt)
    }

    # Use the client to send the prompt to the language model and generate a response.
    response = client.chat.completions.create(
        model=model,  # The language model to use
        messages=session,  # The updated session with the new prompt
        temperature=0.7,  # Control the randomness of the output (0.7 is somewhat creative).
    )
    # Return the content of the first choice from the response.
    return response.choices[0].message.content
</code></pre>
<p>I tried to change my prompt, but it works very well.</p>
","gpt-4"
"77697684","Scraping a Index of text into ordered folders","2023-12-21 11:33:02","77698596","-2","71","<python><web-scraping><hierarchical><gpt-4>","<p>I've been trying to scrape a web page index.
Seeking advice or avenues to explore for how to go about this task.</p>
<p>The index is side by side another index with the same names.
But that left index leads to videos or forums which I dont need.
I want to capture the right Index which leads to chapters and text commentary.</p>
<p>This is the page <a href=""https://www.theseason.org/nt.htm"" rel=""nofollow noreferrer"">https://www.theseason.org/nt.htm</a>
To the right is the index of Bible books I want scraped.
Excluding entries just below it.</p>
<p>I've been using Chat GPT 4 having poor results so far.</p>
<p>What I've tried:
I dabbled with python and was only able to get a list of links of the index.
But it was missing three books in that index for unknown reasons. And it didn't capture any chapters or text contents.</p>
<p>I've tried WinHTTrack application to scrape the html files for me.
That was the best results so far and may return to it.</p>
<p>But I'm drawing a blank how to create the hierarchical folder structure which contains the text for each chapter.
Or I may manually do this.</p>
","gpt-4"
"77673433","AutoGPT Forge Installation Error: Can't assign requested address","2023-12-17 05:49:46","","0","111","<openai-api><large-language-model><gpt-4><autogpt>","<p>I am trying to install AutoGPT on Mac using the following tutorial: <a href=""https://aiedge.medium.com/autogpt-forge-a-comprehensive-guide-to-your-first-steps-a1dfdf46e3b4"" rel=""nofollow noreferrer"">https://aiedge.medium.com/autogpt-forge-a-comprehensive-guide-to-your-first-steps-a1dfdf46e3b4</a></p>
<p>I performed the exact same steps as instructed.</p>
<p>However, I faced an error here:
<a href=""https://i.sstatic.net/p7zI0.png"" rel=""nofollow noreferrer"">Error</a></p>
<p>I tried using Docker, which worked. However, I am looking for the approach without docker.</p>
","gpt-4"
"77656864","Seeking solutions: Integrating GPT-4 and RAG for Accurate and Comprehensive Medical Chatbot","2023-12-13 22:26:57","","0","284","<chatbot><information-retrieval><large-language-model><gpt-4><retrieval-augmented-generation>","<p>I‚Äôm trying to create a RAG chatbot (without chat function) for my company but I am struggling with finding the right appraoch. I‚Äôve been working on it for over a month and really need a solution soon.</p>
<p>THE SETTING: I have to build a system that answers very specific medical questions about pharmaceuticals. Potential sources include, among others, millions of studies from pubmed. The model we are using is GPT-4 and we have access to Azure.</p>
<p>THE GOAL: Get a comprehensive answer that starts broad but then narrows down and also resembles the current state of scientific literature.</p>
<p>THE PROBLEM: The system needs to provide a broad answer but also be factually accurate. Using only GPT-4 (no context provided by me, just question), I get a great answer. It‚Äôs relevant, starts broad and narrows down. However, factual accuracy can‚Äôt be verified and studies are almost always hallucinated.
Using GPT-4 + RAG powered by cognitive search, the answer is often narrow and just a summary of the retrieved literature, sometimes includes semantically similar literature that isn‚Äôt relevant towards the answer, but is factually accurate and provides real scientific references.
I basically need the general expertise of GPT-4, augmented with factual accuracy from our own sources.</p>
<p>POTENTIAL SOLUTION?: I thought of combining the output of both models, GPT-4 without RAG and the other with RAG. This should give me the best of both worlds with a broad answer that also features some relevant literature.</p>
<p>Does anyone have a potential solution that could solve my problem? I'd appreciate any help</p>
","gpt-4"
"77624341","Unable to integrate a customGPT ( https://chat.openai.com/g/g-slaywGsMo-recruitgpt ) to my website as a chat feature?","2023-12-08 04:25:48","","0","56","<openai-api><chatgpt-api><gpt-4>","<p>Disclaimer: I am a rookie so pardon if the question is too basic</p>
<p>I used the below PHP code on my website (ChatGPT generated)</p>
<pre><code>&lt;?php
// Giving demo value to $_POST['data'] for testing
$_POST['data']=&quot;I am an Android Developer. Can you help me find a relevant Job?&quot;;
if (isset($_POST['data'])) {
    $userData = $_POST['data'];

    // RecruitGPT API endpoint and your API key
    $apiUrl = 'YOUR_RECRUITGPT_API_ENDPOINT';
    $apiKey = 'YOUR_API_KEY';

    // Setting up the request to the API
    $ch = curl_init($apiUrl);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
    curl_setopt($ch, CURLOPT_POST, true);
    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode(['input' =&gt; $userData]));
    curl_setopt($ch, CURLOPT_HTTPHEADER, [
        'Authorization: Bearer ' . $apiKey,
        'Content-Type: application/json',
        // Other necessary headers
    ]);

    // Execute the request and get the response
    $response = curl_exec($ch);

    // Check for errors
    if (curl_errno($ch)) {
        echo 'Error:' . curl_error($ch);
    } else {
        // Process the response
        $decodedResponse = json_decode($response, true);

        // Assuming the API returns a 'response' field in its JSON
        echo $decodedResponse['response'];
    }

    // Close the cURL session
    curl_close($ch);
} else {
    echo &quot;No data provided&quot;;
}
?&gt;
</code></pre>
<p>I generated the API key from <a href=""https://platform.openai.com/api-keys"" rel=""nofollow noreferrer"">https://platform.openai.com/api-keys</a>
I am not sure of the YOUR_RECRUITGPT_API_ENDPOINT</p>
<p>Can someone help?</p>
<p>I have tried the below combination for 'YOUR_RECRUITGPT_API_ENDPOINT'</p>
<pre><code>https://api.openai.com/v1/engines/g/g-slaywGsMo-recruitgpt/completions
https://chat.openai.com/g/g-slaywGsMo-recruitgpt
</code></pre>
","gpt-4"
"77604759","Connection error in Langchain DocArrayInMemorySearch function","2023-12-05 08:31:30","","1","378","<langchain><azure-openai><gpt-4>","<p>I was wondering if and why <code>langchain.vectorstores.docarray.in_memory.DocArrayInMemorySearch</code> requires access to public internet connection?</p>
<p>I need to run</p>
<pre class=""lang-py prettyprint-override""><code>embeddings = OpenAIEmbeddings(openai_api_key = openai.api_key)

db = DocArrayInMemorySearch.from_documents(
  docs, 
  embeddings
)

query = &quot;Coles&quot;

docs_response = db.similarity_search(query)
</code></pre>
<p>However I get this error</p>
<pre><code>in create_connection(address, timeout, source_address, socket_options)
    ConnectionError: HTTPSConnectionPool(host='openaipublic.blob.core.windows.net', port=443): Max retries exceeded with url: /encodings/cl100k_base.tiktoken (Caused by NameResolutionError(&quot;&lt;urllib3.connection.HTTPSConnection object at 0x7fafe856b350&gt;: Failed to resolve 'openaipublic.blob.core.windows.net' ([Errno -2] Name or service not known)&quot;)
</code></pre>
<p>I am on a server that does not have unrestricted access to public internet.</p>
","gpt-4"
"77603325","LLM batch input/batch output in a single call using Langchain","2023-12-05 01:06:29","","0","1077","<langchain><large-language-model><azure-openai><gpt-4>","<p>I have a long list of items (let's say sport teams). There are over 1 million items in my list. And I need LLM to provide me with the information on some fields, such as coach name, city, website, and a few more.</p>
<p>I realised If in each call I feed in let's say 25 of those items it would be a lot much cheaper than feeding them one by one. I want the output be in Json format. However, sometimes LLM spit out broken Json. That's why I am using langchain to add Json schema and format instructions. However,  langchain output parser fails because it expects the Json output includes the information for one item only while I have multiple. Can Langchain handle a case like mine or I have to manually implement the output parsing and fallbacks?</p>
<p>Here is a code to replicate the problem, my real problem have a much longer prompt. That's why I want to save money by batch inputing in each call.
In the code below, ensure adding your own keys.</p>
<p>Thank you in advance for your help.</p>
<pre><code>
#%%
import logging
import os
import openai
import pandas as pd
import time
import json
import sys
 
#%%
from langchain.callbacks import get_openai_callback
from langchain.chat_models import AzureChatOpenAI
from pydantic import BaseModel
from langchain.prompts.chat import HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate, BaseStringMessagePromptTemplate
from langchain.output_parsers import PydanticOutputParser, OutputFixingParser, RetryOutputParser, RetryWithErrorOutputParser
from langchain.pydantic_v1 import BaseModel, Field, validator
 
#%%
team_names_input =  [&quot;FC Barcelona&quot;, &quot;Real Madrid&quot;, &quot;Bayern Munich&quot;, &quot;Manchester City&quot;, &quot;Paris Saint-Germain&quot;, &quot;Bayern Munich&quot;, &quot;Liverpool&quot;, &quot;Juventus&quot;, &quot;Manchester City&quot;, &quot;Leipzig&quot;, &quot;SSC Napoli&quot;, &quot;FC Porto&quot;]
 
# metadata fields
metadata_fields = ['Sport_team', 'stadium', 'coach',
       'city', 'country', 'website']
 
#%%
model = AzureChatOpenAI(
    openai_api_key = openai.api_key,
    openai_api_base = openai.api_base,
    deployment_name = openai.deployment_name,
    openai_api_version = openai.api_version,
    openai_api_type = openai.api_type,
    temperature = 0.1
)
 
#%%
 
class metdata(BaseModel):
    Sport_team: str = Field(description=&quot;Sport_team&quot;)
    stadium: str = Field(description=&quot;stadium&quot;)
    coach: str = Field(description=&quot;coach&quot;)
    city: str = Field(description=&quot;city&quot;)
    country: str = Field(description=&quot;country&quot;)
    website: str = Field(description=&quot;website&quot;)
   
   
def construct_prompt_template(system_prompt: str, human_prompt: str) -&gt; ChatPromptTemplate:
    system_message_prompt_template = SystemMessagePromptTemplate.from_template(system_prompt)
    huamn_message_prompt_template = HumanMessagePromptTemplate.from_template(human_prompt)
    template = ChatPromptTemplate.from_messages(
        [
            system_message_prompt_template,
            huamn_message_prompt_template,
        ]
    )
 
    return template
 
def get_completion_with_parser(prompt, parser, callback_info=False):
    with get_openai_callback() as cb:
        output = model(prompt)
    output_content = output.content
    response = parser.parse(output_content)
    #response = output_content
 
    if callback_info:
        return response, cb.total_tokens, cb.total_cost
 
    return response
   
# %%
 
def gpt_teams_metadata(metadata_fields, team_names):
    system_prompt = &quot;&quot;&quot;
You will be given a list of sport teams and you are expected to provide the requested information for each team.
 
[The output should be in json format with the followings keys]:
&lt;&lt; 
```{metadata_fields}```
&gt;&gt;.
Think thoroughly about each single team individually.
 
Before giving the answer ensure all the requested fields are covered for each single team.
Output only the Json object nothing else.
 
```{format_instructions}```
 
&quot;&quot;&quot;
 
    # human prompt
    human_prompt = &quot;&quot;&quot;list of sport teams are:
    ```{team_names}```, Output only the json object nothing else. &quot;&quot;&quot;
   
    template = construct_prompt_template(system_prompt,human_prompt)
    parser = PydanticOutputParser(pydantic_object = metdata)
    format_instructions = parser.get_format_instructions()
   
    final_prompt_value = template.format_prompt(team_names = team_names,
                                                metadata_fields = metadata_fields,
                                                format_instructions = format_instructions)
   
    final_msg = final_prompt_value.to_messages()
    response, total_tokens, total_cost = get_completion_with_parser(final_msg, parser, callback_info = True)
   
    return response, total_tokens, total_cost
 

#%%
n = 2
output_df = pd.DataFrame()
output_json = []
cost = []
n_tokens = []
 
for i in range(0,2):
    print(i)
    team_names = list(team_names_input[i*n:(i+1)*n])
    response, total_tokens, total_cost = gpt_teams_metadata(team_names = team_names,
                                                            metadata_fields = metadata_fields)
   
    response_df = pd.read_json(response)
    output_json.append(response)
    cost.append(total_cost)
    n_tokens.append(total_tokens)
   
    print(f&quot;total cost up to iteration {i} is {sum(cost)}&quot;)
   
    temp_df = pd.concat(
    [response_df, output_df], axis=0, join=&quot;outer&quot;, ignore_index=False,
    keys=None, levels=None, names=None, verify_integrity=False, copy=True,
)
    if len(output_df) &gt; 0:
        if all(output_df.columns == metadata_fields):
            output_df = temp_df
            output_df.to_csv(&quot;./LLM_sportteams.csv&quot;, index=False)
            print(&quot;format looks alright&quot;)
        else:
            temp_df.to_csv(f&quot;./LLM_sportteams{i}.csv&quot; , index=False)
            print(&quot;format does not look alright&quot;)
    else:
        output_df = temp_df
        output_df.to_csv(&quot;./LLM_sportteams.csv&quot;, index=False)
       
       
    time.sleep(40)
</code></pre>
<p>my real problem have a much longer prompt. That's why I want to save money by batch inputing in each call.
In the code below, ensure adding your own keys.</p>
<p>Thank you in advance for your help.</p>
","gpt-4"
"77599445","Chat GPT 4 API connect with PHP using file upload feature","2023-12-04 12:28:29","77600354","0","1438","<php><openai-api><chatgpt-api><gpt-4><chat-gpt-4>","<p>I am trying to connect to gpt-4 API using php. I have a paid account and a valid API key. I use the API key all the time, with gpt-3.5.-turbo model.</p>
<p>I have created an assistant in my account. I want to connect to that assistant and create a new thread for each user that uploads a file. For each file uploaded, I need to return 3 email subjects regarding the file content.</p>
<p>Here is my code:</p>
<pre><code>$api_key = 'sk-xx'; // Replace with your actual OpenAI API Key
$assistant_id = 'asst_xx'; // Your assistant ID

// Handle file upload from the form
$file_path = &quot;test.txt&quot;;

// Step 1: Upload the file
$ch = curl_init('https://api.openai.com/v1/files');
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_POST, true);
curl_setopt($ch, CURLOPT_HTTPHEADER, array('Authorization: Bearer ' . $api_key));
curl_setopt($ch, CURLOPT_POSTFIELDS, array(
    'purpose' =&gt; 'assistants',
    'file' =&gt; new CURLFile($file_path),
));
$response_upload = curl_exec($ch);
$info_upload = curl_getinfo($ch);
curl_close($ch);

$file_response = json_decode($response_upload, true);

if (isset($file_response['id'])) {
    $file_id = $file_response['id'];

    // Step 2: Create a new thread with the user's message and the attached file
    $ch = curl_init(&quot;https://api.openai.com/v1/threads?assistant_id=$assistant_id&quot;); // Include assistant_id as a URL parameter
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
    curl_setopt($ch, CURLOPT_HTTPHEADER, [
        'Content-Type: application/json',
        'Authorization: Bearer ' . $api_key,
        'OpenAI-Beta: assistants=v1'
    ]);
    curl_setopt($ch, CURLOPT_POST, 1);
    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode([
        'messages' =&gt; [
            [
                'role' =&gt; 'user',
                'content' =&gt; 'I want 3 email subjects based on the content of the uploaded file',
                'file_ids' =&gt; [$file_id]
            ]
            
        ]
    ]));

    $thread_creation_response = curl_exec($ch);
    curl_close($ch);

    // Output the response for debugging
    print_r($file_response);
    print_r($thread_creation_response);
    $thread_creation_data = json_decode($thread_creation_response, true);
    echo 'thread-id: ' . $thread_creation_data['id'] . '&lt;br&gt;';

    // Step 3: Get the assistant's response
    $ch = curl_init(&quot;https://api.openai.com/v1/threads/&quot;.$thread_creation_data['id'].&quot;/messages&quot;);
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
    curl_setopt($ch, CURLOPT_HTTPHEADER, [
        'Authorization: Bearer ' . $api_key,
        'OpenAI-Beta: assistants=v1',
        'Content-Type: application/json', // Add this line to specify the content type
    ]);
    curl_setopt($ch, CURLOPT_POST, 1);
    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode([
        'role' =&gt; 'user',
        'content' =&gt; 'I want 3 email subjects based on the content of the uploaded file',
        'file_ids' =&gt; [$file_id]
    ]));

    $thread_response = curl_exec($ch);
    curl_close($ch);

    // Decode the JSON response from the assistant
    $thread_data = json_decode($thread_response, true);

    // Extract and display the entire assistant's response
    echo &quot;Assistant's Response:&lt;br&gt;&quot;;
    print_r($thread_data);


} else {
    // Handle the case where the file upload response does not contain the expected data
    echo &quot;Error uploading file.&quot;;
}
</code></pre>
<p>The output is:</p>
<pre><code>Array ( 
    [object] =&gt; file 
    [id] =&gt; file-yyy 
    [purpose] =&gt; assistants 
    [filename] =&gt; test.txt 
    [bytes] =&gt; 1174 
    [created_at] =&gt; 1701688272 
    [status] =&gt; processed 
    [status_details] =&gt; 
    ) 
    { 
        &quot;id&quot;: &quot;thread_mmm&quot;, 
        &quot;object&quot;: &quot;thread&quot;, 
        &quot;created_at&quot;: 1701688274,
        &quot;metadata&quot;: {} 
    }
    thread-id: thread_mmm 
</code></pre>
<p>Assistant's Response:</p>
<pre><code>Array ( 
    [id] =&gt; msg_uuu 
    [object] =&gt; thread.message 
    [created_at] =&gt; 1701688275 
    [thread_id] =&gt; thread_mmm 
    [role] =&gt; user 
    [content] =&gt; Array ( 
        [0] =&gt; Array ( 
            [type] =&gt; text 
            [text] =&gt; Array ( 
                [value] =&gt; I want 3 email subjects based on the content of the uploaded file 
                [annotations] =&gt; Array ( ) 
            ) 
        ) 
    ) 
    [file_ids] =&gt; Array ( 
        [0] =&gt; file-yyy 
    ) 
    [assistant_id] =&gt; 
    [run_id] =&gt; 
    [metadata] =&gt; Array ( ) 
)
</code></pre>
<p>It seems to upload the file, returns the file id, creates a new thread, it returns the thread id but then it does not return the 3 email subjects that I asked for.</p>
<p>Please help!</p>
<p>Thank you!</p>
","gpt-4"
"77580213","How to get GPT 4 Copilot","2023-11-30 16:48:05","","2","3084","<github-copilot><gpt-4>","<p>I just signed up for the Github Copilot Business and installed Visual Studio Code for Mac.  I added the GitHub Copilot and the GitHub Copilot Chat extensions.  When I asked it what version it was using it said it was using GPT 3.</p>
<p>I am on the paid-plan (20$ for a month).</p>
<p>How can I get it to use GPT 4?</p>
","gpt-4"
"77578738","How to index llamaindex chat engine into a web application?","2023-11-30 13:14:35","","1","454","<chatbot><openai-api><llama-index><gpt-4>","<p>I have been trying to create a RAG based web application using fastapi. The tool used for performing the RAG is llamaindex along with gpt-4 as the LLM. Chat Engines from llamaindex can persist history. But in a web application deployed in a container, the chat messages will be sent by different users. So how can I persist the chat history for each user separately?</p>
<p>The application will be deployed in a Kubernetes cluster. So it cannot be guaranteed that the same pod will serve all the messages from a particular user. In this case how will I preserve the history of messages? Is there any support offered by llamaindex or should I create some middleware to store history for each user and send it along with the current message?</p>
","gpt-4"
"77564810","Is logit_bias disabled for gpt-4-vision model or is my code wrong?","2023-11-28 14:46:52","","1","522","<openai-api><gpt-4>","<p>I have been using the combination of max_tokens=1 and logit_bias for text classification tasks on the OpenAI GPT-4 API. <a href=""https://help.openai.com/en/articles/5247780-using-logit-bias-to-define-token-probability"" rel=""nofollow noreferrer"">Logit bias</a> makes it easy to nudge the model to only output certain words like yes or no, cat or dog, so it works like a classifier. This does not seem to work on the new gpt-4-vision-preview model however, which throws this error:</p>
<blockquote>
<p>Error code: 400 - {'error': {'message': '1 validation error for Request\nbody -&gt; logit_bias\n  extra fields not permitted (type=value_error.extra)', 'type': 'invalid_request_error', 'param': None, 'code': None}}</p>
</blockquote>
<p>This works fine on the gpt-4 model, and this code works fine with the vision model when the logit_bias is commented out (code below uses OpenAI's Python package). No difference if low or high detail. I've gone through the docs (it's the same chat completion endpoint as for gpt-4), googled the error and vision + logit_bias keywords and found no other posts on this issue.</p>
<pre><code>import os
import base64
import openai
from openai import OpenAI
client = OpenAI(api_key=YOUR_KEY) # needs key to run example

# Function to encode the image from OpenAI docs
def encode_image(image_path):
    with open(image_path, &quot;rb&quot;) as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

image_path = &quot;path_to_your_image.jpg&quot;
base64_image = encode_image(image_path)

result = client.chat.completions.create(
model=&quot;gpt-4-vision-preview&quot;,
messages=[
{
  &quot;role&quot;: &quot;user&quot;,
  &quot;content&quot;: [
    {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;Does this image depict a cat, Yes or No?&quot;},
    {
      &quot;type&quot;: &quot;image_url&quot;,
      &quot;image_url&quot;: {
        &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot;,
        &quot;detail&quot;: &quot;low&quot;
      },
    },
  ],
}
],
temperature=0,
logit_bias={9642:100, 2822:100},  # &lt;---- this guy here, tokens for Yes &amp; No
max_tokens=1
)
</code></pre>
<p>Am I doing something wrong, or is this simply not available for this model yet? If the latter, is there a workaround for image classification? Without the bias it's prone to respond like a chat model (which it is), e.g. instead of yes or no it will start saying &quot;Sorry...&quot; etc.</p>
","gpt-4"
"77550536","Do LLM models generate output token by token?","2023-11-26 05:16:08","","3","1440","<large-language-model><llama><gpt-4>","<p>If you use ChatGPT web app it answers typing token by token. It you use it through API you get the whole answer at once.</p>
<p>My assumption was that they provide token by token answers in the web app for UX reasons (easier reading maybe, a sneaky way to limit the amount of user's prompts by making them wait longer for the answer).</p>
<p>Today I downloaded <a href=""https://github.com/ggerganov/llama.cpp"" rel=""nofollow noreferrer"">llama cpp</a> app and played around with the models from <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">Hugging Face</a>.</p>
<p>What made me wonder was that the llama CLI was also printing the answers token by token. While it is typing it is using ~70% of my CPU. The moment it stops typing the CPU usage drops to 0%. If the output is long the CPU stays on 70% for longer.</p>
<p>It looks like the answer tokens are actually pulled from the model one by one and the more tokens you want, the longer it takes to generate.</p>
<p>However my initial understanding was that a model always returns the answers of the same length (just 0 padded if less text makes more sense). I also assumed that the model repose time is invariant to the length of the prompt and the generated output.</p>
<p>What am I missing? How does it really work?</p>
","gpt-4"
"77541510","Translating XML with CDATA using OpenAI GPT-4 and maintaining CDATA structure","2023-11-24 08:03:33","","0","284","<python><xml><openai-api><cdata><gpt-4>","<p>I'm using OpenAI GPT-4 to translate XML content from English to French, and I'm facing an issue with preserving the CDATA structure in the translated XML. The XML contains strings with CDATA sections, and I want the translated output to maintain the CDATA structure.</p>
<p>Here's a simplified example of the input XML:</p>
<pre><code>&lt;resources&gt;
    &lt;string name=&quot;app_name&quot;&gt;RoomThermometer&lt;/string&gt;
    &lt;string name=&quot;instructions&quot; formatted=&quot;false&quot;&gt;
        &lt;![CDATA[
        &lt;b&gt;How it works? &lt;/b&gt;&lt;br /&gt;
        The thermometer app is using your phone temperature sensors to measure room temperature.&lt;br /&gt;&lt;br /&gt;
        &lt;b&gt;Why this thermometer is the best one?&lt;/b&gt;&lt;br /&gt;
        We use AI-powered algorithm to get the most accurate measure by ¬± 3¬∞C. The more you use the thermometer, the more accurate it is!&lt;br /&gt;&lt;br /&gt;
        &lt;b&gt;How to get the best room temperature measure?&lt;/b&gt;&lt;br /&gt;
        You want to prevent the phone from overheating, follow the next steps:&lt;br /&gt;
        &lt;b&gt;1-&lt;/b&gt; take your case off&lt;br /&gt;
        &lt;b&gt;2-&lt;/b&gt; Your phone is charging or just charged, let it cool down&lt;br /&gt;
        &lt;b&gt;3-&lt;/b&gt; Turn off unnecessary apps running in the background&lt;br /&gt;
        ]]&gt;
    &lt;/string&gt;
&lt;/resources&gt;

</code></pre>
<p>The current Python script uses OpenAI GPT-4 to translate the text, but the CDATA structure is not maintained in the output. The result looks like this:</p>
<pre><code>&lt;resources&gt;
    &lt;string name=&quot;app_name&quot;&gt;Thermom√®tre de Chambre&lt;/string&gt;
    &lt;string name=&quot;instructions&quot; formatted=&quot;false&quot;&gt;
        '&amp;lt;b&amp;gt;Comment √ßa marche?&amp;lt;/b&amp;gt;&amp;lt;br/&amp;gt;
        L'application de thermom√®tre utilise les capteurs de temp√©rature de votre t√©l√©phone pour mesurer la temp√©rature ambiante.&amp;lt;br/&amp;gt;&amp;lt;br/&amp;gt;
        &amp;lt;b&amp;gt;Pourquoi ce thermom√®tre est le meilleur?&amp;lt;/b&amp;gt;&amp;lt;br/&amp;gt;
        Nous utilisons un algorithme aliment√© par l'IA pour obtenir la mesure la plus pr√©cise √† ¬± 3¬∞C. Plus vous utilisez le thermom√®tre, plus il est pr√©cis!&amp;lt;br/&amp;gt;&amp;lt;br/&amp;gt;
        &amp;lt;b&amp;gt;Comment obtenir la meilleure mesure de temp√©rature ambiante?&amp;lt;/b&amp;gt;&amp;lt;br/&amp;gt;
        Vous voulez √©viter que le t√©l√©phone ne surchauffe, suivez les √©tapes suivantes:&amp;lt;br/&amp;gt;
        &amp;lt;b&amp;gt;1-&amp;lt;/b&amp;gt; retirez votre coque&amp;lt;br/&amp;gt;
        &amp;lt;b&amp;gt;2-&amp;lt;/b&amp;gt; Votre t√©l√©phone est en charge ou vient d'√™tre charg√©, laissez-le refroidir&amp;lt;br/&amp;gt;
        &amp;lt;b&amp;gt;3-&amp;lt;/b&amp;gt; Arr√™tez l'application inutile qui fonctionne en arri√®re-plan&amp;lt;br/&amp;gt;'
    &lt;/string&gt;
&lt;/resources&gt;

</code></pre>
<p>However, I want the translated output to preserve the CDATA structure, like this:</p>
<pre><code>&lt;resources&gt;
    &lt;string name=&quot;app_name&quot;&gt;Thermom√®tre de Chambre&lt;/string&gt;
    &lt;string name=&quot;instructions&quot; formatted=&quot;false&quot;&gt;
        &lt;![CDATA[
        &lt;b&gt;Comment √ßa fonctionne ? &lt;/b&gt;&lt;br /&gt;
        L'application thermom√®tre utilise les capteurs de temp√©rature de votre t√©l√©phone pour mesurer la temp√©rature de la pi√®ce.&lt;br /&gt;&lt;br /&gt;
        &lt;b&gt;Pourquoi ce thermom√®tre est le meilleur ?&lt;/b&gt;&lt;br /&gt;
        Nous utilisons un algorithme aliment√© par l'IA pour obtenir la mesure la plus pr√©cise √† ¬± 3¬∞C. Plus vous utilisez le thermom√®tre, plus il est pr√©cis !&lt;br /&gt;&lt;br /&gt;
        &lt;b&gt;Comment obtenir la meilleure mesure de la temp√©rature de la pi√®ce ?&lt;/b&gt;&lt;br /&gt;
        Vous voulez emp√™cher le t√©l√©phone de surchauffer, suivez les √©tapes suivantes :&lt;br /&gt;
        &lt;b&gt;1-&lt;/b&gt; retirez votre √©tui&lt;br /&gt;
        &lt;b&gt;2-&lt;/b&gt; Votre t√©l√©phone est en train de charger ou vient de se charger, laissez-le refroidir&lt;br /&gt;
        &lt;b&gt;3-&lt;/b&gt; Fermez les applications inutiles qui tournent en arri√®re-plan&lt;br /&gt;
        ]]&gt;
    &lt;/string&gt;
&lt;/resources&gt;

</code></pre>
<p>I think that the problem is in this function:</p>
<pre><code>
def translate_xml_file(xml_file_path, target_language=&quot;French&quot;):
    # Parse the XML file
    tree = ET.parse(xml_file_path)
    root = tree.getroot()

    # Iterate over each 'string' element
    for string_element in root.iter('string'):
        # Check if the element contains text
        if string_element.text:
            string_text = string_element.text.strip()

            # Check if the text is within a CDATA section
            if string_text.startswith(&quot;&lt;![CDATA[&quot;) and string_text.endswith(&quot;]]&gt;&quot;):
                # Extract the content from CDATA
                cdata_content = string_text[9:-3]

                # Translate the content
                translated_cdata_content = translate_string(cdata_content, target_language)

                # Manually wrap with CDATA and set back to element
                string_element.text = &quot;&lt;![CDATA[&quot; + translated_cdata_content + &quot;]]&gt;&quot;
            else:
                # Translate normal text
                translated_text = translate_string(string_text, target_language)
                string_element.text = translated_text

    return tree
</code></pre>
<p>If you need the whole code, I will share it! I am not sharing it, because it is too long!</p>
<p>How can I modify the existing script to achieve this? Any insights or code examples would be greatly appreciated.</p>
<p>Additional Information:</p>
<p>I'm using OpenAI GPT-4 for translation.
The script is written in Python.
The target language is French.
I want the translated text to be wrapped in CDATA just like the original structure.
Thank you in advance!</p>
","gpt-4"
"77539317","my open ai gpt-4-vision-preview model does not work","2023-11-23 19:43:28","","0","601","<python><artificial-intelligence><openai-api><gpt-4>","<p>I am working from Google Colab, I want the model to create the meta descriptions of about 400 images that I have in Google Drive. It worked relatively well for the first 100 images and then it didn't, and now it just writes &quot;error&quot; in the resulting excel. I attach the code here:</p>
<pre><code>import os
import base64
import requests
import pandas as pd
from google.colab import drive


drive.mount('/content/drive')


image_folder = '/content/drive/MyDrive/Work related/FS/Imagenes/Metadescripciones HC '


api_key = 'my api code goes here XXXXX'


def encode_image(image_path):
    with open(image_path, &quot;rb&quot;) as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')


headers = {
    &quot;Content-Type&quot;: &quot;application/json&quot;,
    &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;
}


results_df = pd.DataFrame(columns=['Nombre del Archivo', 'Metadescripcion'])


for filename in os.listdir(image_folder):
    if filename.endswith((&quot;.png&quot;, &quot;.jpg&quot;, &quot;.jpeg&quot;)):  # Aseg√∫rate de que es una imagen
        image_path = os.path.join(image_folder, filename)
        base64_image = encode_image(image_path)

        
        payload = {
            &quot;model&quot;: &quot;gpt-4-vision-preview&quot;,
            &quot;messages&quot;: [
                {
                    &quot;role&quot;: &quot;user&quot;,
                    &quot;content&quot;: [
                        {
                            &quot;type&quot;: &quot;text&quot;,
                            &quot;text&quot;: &quot;Write a meta description for the image of this product, optimized for SEO and in less than 150 words&quot;
                        },
                        {
                            &quot;type&quot;: &quot;image_url&quot;,
                            &quot;image_url&quot;: {
                                &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot;,
                                &quot;detail&quot;: &quot;low&quot;
                            }
                        }
                    ]
                }
            ],
            &quot;max_tokens&quot;: 400
        }

        
        response = requests.post(&quot;https://api.openai.com/v1/chat/completions&quot;, headers=headers, json=payload)
        response_json = response.json()
        metadescription = response_json['choices'][0]['message']['content'] if response.status_code == 200 else 'Error'

        
        new_row = pd.DataFrame({'Nombre del Archivo': [filename], 'Metadescripcion': [metadescription]})
        results_df = pd.concat([results_df, new_row], ignore_index=True)



results_df.to_excel('/content/drive/MyDrive/Work related/FS/Imagenes/Metadescripciones.xlsx', index=False)
</code></pre>
<p>I have tried changing the max_tokens and changing the image folder path</p>
","gpt-4"
"77534792","Max Token Limit for Azure GPT-4 Models","2023-11-23 06:44:06","","0","2170","<azure><openai-api><azure-openai><gpt-4>","<p>Why can I only set a maximum value of 8192 for deployment requests on Azure  gpt-4 32k (10000 TPM) and Azure gpt-4 1106-Preview (50000 TPM)? I thought I could set a higher value. Am I missing something in the configuration?
<a href=""https://i.sstatic.net/Z9nrD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Z9nrD.png"" alt=""This is image for gpt4-1106-Preview (50000 TPM)"" /></a></p>
","gpt-4"
"77533016","How can I use the full 128,000 Token Context of GPT4 Turbo?","2023-11-22 20:48:56","","0","1565","<python><gpt-4>","<p>The new GPT4 Turbo has 128,000 token context and a 4096 token output limit.</p>
<p>However, when I call it, if my input text is 4000 tokens, it will only provide output of 96 tokens.
I am expecting that if I provide 123,000 token input, I will be able to generate up to 4096 tokens of output.
Or at least, that is my experience with GPT4 which has 8192 token limit.  If I have an input of 3000 tokens, it can generate 5192 tokens of output.</p>
<p>This is my function for interfacing with the OpenAi API.</p>
<pre><code>def process_ai(input_text, res = None):
    final_res = ''


    # Overriding settings
    if res:
        if 'model' in res:
            model = res['model']
            max_tokens = res['max_tokens']

        if 'temperature' in res:
            if res['temperature']:
                temperature = res['temperature']

    openai.api_key      = os.getenv('API_KEY')
    openai.organization = os.getenv('ORGANIZATION_KEY')

    try:
        # Default settings
        max_allowed_tokens = min(res.get('max_tokens', 8192), 4096) 
        max_input_length = max_allowed_tokens - int(len(input_text)/3.9+650)
        
        # Ensure max_input_length doesn't go below a certain threshold (e.g., 10)
        max_input_length = max(max_input_length, 10)

        result = openai.ChatCompletion.create(
            model            = model,
            messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: input_text}],
            temperature       = temperature,
            max_tokens        = max_input_length,
            top_p             = 0.3,
            frequency_penalty = 0.5,
            presence_penalty  = 0.0,
        )
        final_res = result['choices'][0]['message']['content']
    except openai.error.RateLimitError:
        final_res = 'AI Server is busy, try again in a few minutes.'
    except openai.error.ServiceUnavailableError:
        final_res = 'AI Server is busy, try again in a few minutes.'
    except Exception as e:
        final_res = 'Error occured, try again in a few minutes or contact admin.'+'string length' +str(len(input_text)/3.9+600) +'Dev info:' + str(e)
    final_res = html.unescape(final_res) # try to fix characters
    
    return final_res
</code></pre>
<p>FYI max_tokens comes from a nosql document</p>
<pre><code>&quot;name&quot;: &quot;GPT4-Turbo&quot;,
        &quot;enabled&quot;: NumberInt(1),
        &quot;model&quot;: &quot;gpt-4-1106-preview&quot;,
        &quot;desc&quot;: &quot;&quot;,
        &quot;max_tokens&quot;: NumberInt(128000)
</code></pre>
<p>If I flip back to GPT4, an input string of 4000 tokens allows for up to 8192 - 4000 = 4192 tokens in output.
If I use GPT4 turbo and set max_tokens = 4095, I am stuck with my original problem of only receiving 96 tokens in the output.</p>
<p>How are we supposed to be able to use this 128,000 token context?
Are we supposed to chunk data into it somehow?</p>
","gpt-4"
"77523024","Can we integrate GPT-4 with a simple flask API to generate a description for an image sent?","2023-11-21 13:00:10","","0","134","<python><openai-api><gpt-4>","<p>just want to create an API that takes an image and gives out a json description in the format:</p>
<pre><code>{
   &quot;status&quot; : &quot;200&quot;,
   &quot;description&quot; : &quot;a discription of the image in 50 words&quot;
}
</code></pre>
<p>I tried asking this from chatgpt itself but it says here that we have to implement our own function or some other tool to generate a descriotion. all this code does is use an already existing description(gives out a description from a text input) what i need is to have it generate a description from image like the chat GPT-4 does when its given an image.</p>
<p>Any ML or image procecing way to achieve this would be fantastic :)</p>
<pre><code>from flask import Flask, request, jsonify
import openai

app = Flask(__name__)

# Set your OpenAI API key here
openai.api_key = 'your-openai-api-key'

@app.route('/generate_description', methods=['POST'])
def generate_description():
    # Check if the request contains a file
    if 'file' not in request.files:
        return jsonify({'error': 'No file part'}), 400

    file = request.files['file']

    # Process the file (you may need to save it temporarily, depending on your requirements)
    # Here, we assume 'process_image' is a function to handle image processing
    image_description = process_image(file)

    # Use GPT-4 to generate a description based on the image
    try:
        response = openai.Completion.create(
            engine=&quot;text-davinci-002&quot;,  # Use the appropriate GPT-4 engine
            prompt=f&quot;Describe the following image: {image_description}&quot;,
            max_tokens=50  # Adjust the maximum number of tokens as needed
        )
        generated_description = response.choices[0].text.strip()
    except Exception as e:
        return jsonify({'error': str(e)}), 500

    return jsonify({'description': generated_description})

def process_image(file):
    # Placeholder for image processing logic
    # You may need to use an image recognition tool or library for this step
    # Return a string representing the processed image description
    return 'a processed image description'

if __name__ == '__main__':
    app.run(debug=True)
</code></pre>
","gpt-4"
"77519535","Cannot connect to GPT4 API?","2023-11-20 23:37:52","","-1","328","<openai-api><chatgpt-api><gpt-4>","<p>I cannot connect to GPT4's API! In my command prompt I even installed openai and the newest version of python. I am trying to run this in Google Colab, any thoughts?</p>
<pre><code># Install the OpenAI library
!pip install openai

import openai

 # Set your API key here
 openai.api_key = 'YOUR_API_KEY'

 def query_gpt4(prompt):
    response = openai.Completion.create(
        model=&quot;gpt-4&quot;, 
        prompt=prompt, 
         max_tokens=100
     )
     return response.choices[0].text.strip()

 # Example usage
 prompt = &quot;Translate the following English text to French: 'Hello, how are you?'&quot;
 response = query_gpt4(prompt)
 print(response)
</code></pre>
<p>I get this error:
APIRemovedInV1                            Traceback (most recent call last)
 in &lt;cell line: 19&gt;()
17 # Example usage
18 prompt = &quot;Translate the following English text to French: 'Hello, how are you?'&quot;
---&gt; 19 response = query_gpt4(prompt)
20 print(response)</p>
<p>3 frames
/usr/local/lib/python3.10/dist-packages/openai/lib/_old_api.py in <strong>load</strong>(self)
31     @override
32     def <strong>load</strong>(self) -&gt; None:
---&gt; 33         raise APIRemovedInV1(symbol=self._symbol)
34
35</p>
<p>APIRemovedInV1:</p>
<p>You tried to access openai.Completion, but this is no longer supported in openai&gt;=1.0.0 - see the README at <a href=""https://github.com/openai/openai-python"" rel=""nofollow noreferrer"">https://github.com/openai/openai-python</a> for the API.</p>
<p>You can run <code>openai migrate</code> to automatically upgrade your codebase to use the 1.0.0 interface.</p>
<p>Alternatively, you can pin your installation to the old version, e.g. <code>pip install openai==0.28</code></p>
<p>A detailed migration guide is available here: <a href=""https://github.com/openai/openai-python/discussions/742"" rel=""nofollow noreferrer"">https://github.com/openai/openai-python/discussions/742</a></p>
","gpt-4"
"77501938","OpenAPI Schema - Put Request into S3 turns binary file (.doc/.pdf) into empty json file","2023-11-17 13:13:14","","1","58","<json><swagger><openapi><gpt-4>","<p>I am working on a custom GPT 4 application within chatGPT and in the configure tab there is a place where you can implement some OpenAPI schemas.</p>
<p>I am trying to simply make a PUT request to move an uploaded file (.pdf) into an S3 bucket.</p>
<p>I successfully connect to the API but when I look in S3, the file is either empty or has some small amount of info about the document inside.</p>
<p>i.e.</p>
<blockquote>
<p>{&quot;metadata&quot;: {&quot;name&quot;: &quot;Manual&quot;, &quot;description&quot;: &quot;A comprehensive manual for Capri, including important contact numbers,  other information, instructions, recommendations, and FAQs.&quot;}, &quot;fileData&quot;: &quot;/mnt/data/test.pdf&quot;}</p>
</blockquote>
<p>Can someone please help me understand if what I am trying to do is possible and how I may do that? I tried the API in POSTMAN and the response was 200 and the file was in its original format.</p>
<p>I tried postman and it worked fine;
as soon as I try to bring it into OpenAPI schema, I lose....</p>
<p>Here is my schema code:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;openapi&quot;: &quot;3.1.0&quot;,
  &quot;info&quot;: {
    &quot;title&quot;: &quot;S3 Bucket File Management API&quot;,
    &quot;version&quot;: &quot;1.0.0&quot;
  },
  &quot;servers&quot;: [
    {
      &quot;url&quot;: &quot;https://HIDDEN.execute-api.us-east-1.amazonaws.com/prod&quot;
    }
  ],
  &quot;paths&quot;: {
    &quot;/bnb-bucket-ham/{filename}&quot;: {
      &quot;put&quot;: {
        &quot;summary&quot;: &quot;Upload a file to the S3 bucket&quot;,
        &quot;operationId&quot;: &quot;uploadFile&quot;,
        &quot;parameters&quot;: [
          {
            &quot;name&quot;: &quot;filename&quot;,
            &quot;in&quot;: &quot;path&quot;,
            &quot;required&quot;: true,
            &quot;schema&quot;: {
              &quot;type&quot;: &quot;string&quot;
            }
          }
        ],
        &quot;requestBody&quot;: {
          &quot;required&quot;: true,
          &quot;content&quot;: {
            &quot;application/json&quot;: {
              &quot;schema&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                  &quot;metadata&quot;: {
                    &quot;type&quot;: &quot;object&quot;,
                    &quot;properties&quot;: {
                      &quot;name&quot;: {
                        &quot;type&quot;: &quot;string&quot;
                      },
                      &quot;description&quot;: {
                        &quot;type&quot;: &quot;string&quot;
                      }
                    },
                    &quot;required&quot;: [&quot;name&quot;]
                  },
                  &quot;fileData&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;format&quot;: &quot;byte&quot;
                  }
                },
                &quot;required&quot;: [&quot;metadata&quot;, &quot;fileData&quot;]
              }
            },
            &quot;application/pdf&quot;: {},
            &quot;application/msword&quot;: {},
            &quot;application/vnd.openxmlformats-officedocument.wordprocessingml.document&quot;: {}
          }
        },
        &quot;responses&quot;: {
          &quot;200&quot;: {
            &quot;description&quot;: &quot;File uploaded successfully&quot;
          }
        }
      },
      &quot;get&quot;: {
        &quot;summary&quot;: &quot;Retrieve a file from the S3 bucket&quot;,
        &quot;operationId&quot;: &quot;getFile&quot;,
        &quot;parameters&quot;: [
          {
            &quot;name&quot;: &quot;filename&quot;,
            &quot;in&quot;: &quot;path&quot;,
            &quot;required&quot;: true,
            &quot;schema&quot;: {
              &quot;type&quot;: &quot;string&quot;
            }
          }
        ],
        &quot;responses&quot;: {
          &quot;200&quot;: {
            &quot;description&quot;: &quot;File retrieved successfully&quot;,
            &quot;content&quot;: {
              &quot;application/pdf&quot;: {
                &quot;schema&quot;: {
                  &quot;type&quot;: &quot;string&quot;,
                  &quot;format&quot;: &quot;binary&quot;
                }
              },
              &quot;application/msword&quot;: {
                &quot;schema&quot;: {
                  &quot;type&quot;: &quot;string&quot;,
                  &quot;format&quot;: &quot;binary&quot;
                }
              },
              &quot;application/vnd.openxmlformats-officedocument.wordprocessingml.document&quot;: {
                &quot;schema&quot;: {
                  &quot;type&quot;: &quot;string&quot;,
                  &quot;format&quot;: &quot;binary&quot;
                }
              }
            }
          },
          &quot;404&quot;: {
            &quot;description&quot;: &quot;File not found&quot;
          }
        }
      }
    }
  }
}
</code></pre>
","gpt-4"
"77498718","Add GPT-4V (Vision) capability to Chatbot-ui (open-source ChatGPT clone by TypeScript)","2023-11-17 00:16:45","","0","394","<typescript><artificial-intelligence><openai-api><chatgpt-api><gpt-4>","<p>How can I add GPT-4 Vision API to Chatbot-ui, which is a powerful open-source clone of ChatGPT, developed by McKay Wrigley.</p>
<p>It let's you use OpenAI AI models through your own API key, which is amazing. I've been using it for my persoanl use, and now I need to use gpt4v as well.</p>
<p>Github repo:
<a href=""https://github.com/mckaywrigley/chatbot-ui"" rel=""nofollow noreferrer"">https://github.com/mckaywrigley/chatbot-ui</a></p>
","gpt-4"
"77480977","GPT4-v images labeling: 'Classifications' object is not iterable","2023-11-14 13:29:48","","0","96","<computer-vision><openai-api><yolo><gpt-4>","<p>Following this article <a href=""https://blog.roboflow.com/gpt-4-image-classification/"" rel=""nofollow noreferrer"">Image Classification</a> I try to label images with specific categories using GPT4-Visual API, so that I can use these classifications to train a YOLOv8 model.
At the end I get <em>TypeError: 'Classifications' object is not iterable</em>
Here is my code:</p>
<pre><code>from autodistill_gpt_4v import GPT4V
from autodistill.detection import CaptionOntology
import os


api_key = os.getenv(&quot;OPENAI_API_KEY&quot;)

base_model = GPT4V(
    api_key=api_key,
    ontology=CaptionOntology(
        {
            &quot;scratch&quot;: &quot;scratch&quot;
        }
    )
)
base_model.label(&quot;./images/&quot;, extension=&quot;.jpg&quot;)
</code></pre>
<p>Here is error trace:</p>
<pre><code>base_model.label(&quot;./images/&quot;, extension=&quot;.jpg&quot;)
  File &quot;C:\Python311\Lib\site-packages\autodistill\detection\detection_base_model.py&quot;, line 64, in label
    dataset.as_yolo(
  File &quot;C:\Python311\Lib\site-packages\supervision\dataset\core.py&quot;, line 363, in as_yolo
    save_yolo_annotations(
  File &quot;C:\Python311\Lib\site-packages\supervision\dataset\formats\yolo.py&quot;, line 239, in save_yolo_annotations
    lines = detections_to_yolo_annotations(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Python311\Lib\site-packages\supervision\dataset\formats\yolo.py&quot;, line 198, in detections_to_yolo_annotations
    for xyxy, mask, _, class_id, _ in detections:
TypeError: 'Classifications' object is not iterable
</code></pre>
<p>I tried to print the detections object passed to the</p>
<pre><code>detections_to_yolo_annotations(detections: Detections, 
    image_shape: Tuple[int, int, int],
    min_image_area_percentage: float = 0.0,
    max_image_area_percentage: float = 1.0,
    approximation_percentage: float = 0.75,) 
</code></pre>
<p>and got
<code>Classifications(class_id=array([0]), confidence=array([1]))</code></p>
","gpt-4"
"77478768","How to enable OpenAI custom GPT to access an API?","2023-11-14 07:17:26","","0","1581","<openapi><gpt-4>","<p>I'm trying to build a custom GPT that can access an API via the &quot;Actions&quot; configuration.</p>
<p>For illustration, I built the toy API below and deployed to <a href=""https://main-bvxea6i-74oriiawvvtoy.eu-5.platformsh.site/"" rel=""nofollow noreferrer"">https://main-bvxea6i-74oriiawvvtoy.eu-5.platformsh.site/</a>.</p>
<p>Then I copied <a href=""https://main-bvxea6i-74oriiawvvtoy.eu-5.platformsh.site/openapi.json"" rel=""nofollow noreferrer"">the automatically generated OpenAPI spec</a>, added the server section (result below) and set up a Custom GPT via <a href=""https://chat.openai.com/"" rel=""nofollow noreferrer"">https://chat.openai.com/</a>.</p>
<p>My GPT correctly identifies the 3 methods available in its configuration. It also states that it has access to those methods in the chat windows. But it cannot access them and reports errors like &quot;Error talking to main-bvxea6i-74oriiawvvtoy.eu-5.platformsh.site&quot; or &quot;It seems that there's an issue with retrieving the message from the API, as the request resulted in a &quot;Not Found&quot; error.&quot;</p>
<p>Any idea what is going wrong?</p>
<p>(BTW, I can make a Custom GPT that simply lists all posts of <a href=""https://jsonplaceholder.typicode.com/"" rel=""nofollow noreferrer"">https://jsonplaceholder.typicode.com/</a>. Can Custom GPTs not work with parameters properly?)</p>
<h1>Code</h1>
<pre class=""lang-py prettyprint-override""><code>import os
import uvicorn
from typing import Union
from fastapi import FastAPI

app = FastAPI()


@app.get(&quot;/&quot;)
def home():
    return &quot;Home sweet home&quot;


@app.get(&quot;/message&quot;)
def message():
    return {&quot;msg&quot;: &quot;The early bird catches the worm.&quot;}


@app.get(&quot;/calc/{x}&quot;)
def calc(x: int, y: Union[int, None] = None):
    y = y or 0
    return {&quot;x&quot;: x, &quot;y&quot;: y, &quot;result&quot;: int(x) * int(y)}


if __name__ == &quot;__main__&quot;:
    port = os.getenv(&quot;PORT&quot;) or 8080
    uvicorn.run(app, host=&quot;127.0.0.1&quot;, port=int(port))
</code></pre>
<h1>OpenAPI spec</h1>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;openapi&quot;: &quot;3.1.0&quot;,
    &quot;info&quot;: {
        &quot;title&quot;: &quot;FastAPI&quot;,
        &quot;version&quot;: &quot;0.1.0&quot;
    },
    &quot;servers&quot;: [
        {
            &quot;url&quot;: &quot;https://main-bvxea6i-74oriiawvvtoy.eu-5.platformsh.site/&quot;
        }
    ],
    &quot;paths&quot;: {
        &quot;/&quot;: {
            &quot;get&quot;: {
                &quot;summary&quot;: &quot;Home&quot;,
                &quot;operationId&quot;: &quot;home__get&quot;,
                &quot;responses&quot;: {
                    &quot;200&quot;: {
                        &quot;description&quot;: &quot;Successful Response&quot;,
                        &quot;content&quot;: {
                            &quot;application/json&quot;: {
                                &quot;schema&quot;: {}
                            }
                        }
                    }
                }
            }
        },
        &quot;/message&quot;: {
            &quot;get&quot;: {
                &quot;summary&quot;: &quot;Message&quot;,
                &quot;operationId&quot;: &quot;message_message_get&quot;,
                &quot;responses&quot;: {
                    &quot;200&quot;: {
                        &quot;description&quot;: &quot;Successful Response&quot;,
                        &quot;content&quot;: {
                            &quot;application/json&quot;: {
                                &quot;schema&quot;: {}
                            }
                        }
                    }
                }
            }
        },
        &quot;/calc/{x}&quot;: {
            &quot;get&quot;: {
                &quot;summary&quot;: &quot;Calc&quot;,
                &quot;operationId&quot;: &quot;calc_calc__x__get&quot;,
                &quot;parameters&quot;: [
                    {
                        &quot;name&quot;: &quot;x&quot;,
                        &quot;in&quot;: &quot;path&quot;,
                        &quot;required&quot;: true,
                        &quot;schema&quot;: {
                            &quot;type&quot;: &quot;integer&quot;,
                            &quot;title&quot;: &quot;X&quot;
                        }
                    },
                    {
                        &quot;name&quot;: &quot;y&quot;,
                        &quot;in&quot;: &quot;query&quot;,
                        &quot;required&quot;: false,
                        &quot;schema&quot;: {
                            &quot;anyOf&quot;: [
                                {
                                    &quot;type&quot;: &quot;integer&quot;
                                },
                                {
                                    &quot;type&quot;: &quot;null&quot;
                                }
                            ],
                            &quot;title&quot;: &quot;Y&quot;
                        }
                    }
                ],
                &quot;responses&quot;: {
                    &quot;200&quot;: {
                        &quot;description&quot;: &quot;Successful Response&quot;,
                        &quot;content&quot;: {
                            &quot;application/json&quot;: {
                                &quot;schema&quot;: {}
                            }
                        }
                    },
                    &quot;422&quot;: {
                        &quot;description&quot;: &quot;Validation Error&quot;,
                        &quot;content&quot;: {
                            &quot;application/json&quot;: {
                                &quot;schema&quot;: {
                                    &quot;$ref&quot;: &quot;#/components/schemas/HTTPValidationError&quot;
                                }
                            }
                        }
                    }
                }
            }
        }
    },
    &quot;components&quot;: {
        &quot;schemas&quot;: {
            &quot;HTTPValidationError&quot;: {
                &quot;properties&quot;: {
                    &quot;detail&quot;: {
                        &quot;items&quot;: {
                            &quot;$ref&quot;: &quot;#/components/schemas/ValidationError&quot;
                        },
                        &quot;type&quot;: &quot;array&quot;,
                        &quot;title&quot;: &quot;Detail&quot;
                    }
                },
                &quot;type&quot;: &quot;object&quot;,
                &quot;title&quot;: &quot;HTTPValidationError&quot;
            },
            &quot;ValidationError&quot;: {
                &quot;properties&quot;: {
                    &quot;loc&quot;: {
                        &quot;items&quot;: {
                            &quot;anyOf&quot;: [
                                {
                                    &quot;type&quot;: &quot;string&quot;
                                },
                                {
                                    &quot;type&quot;: &quot;integer&quot;
                                }
                            ]
                        },
                        &quot;type&quot;: &quot;array&quot;,
                        &quot;title&quot;: &quot;Location&quot;
                    },
                    &quot;msg&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;title&quot;: &quot;Message&quot;
                    },
                    &quot;type&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;title&quot;: &quot;Error Type&quot;
                    }
                },
                &quot;type&quot;: &quot;object&quot;,
                &quot;required&quot;: [
                    &quot;loc&quot;,
                    &quot;msg&quot;,
                    &quot;type&quot;
                ],
                &quot;title&quot;: &quot;ValidationError&quot;
            }
        }
    }
}
</code></pre>
","gpt-4"
"77477340","How to use GPT's function calling for complex sequential tasks?","2023-11-13 23:05:36","","0","319","<sql><information-retrieval><large-language-model><gpt-3><gpt-4>","<p>I am working on a project where the user can ask any question about their emails to a chatbot, and get an answer. I have been trying to use GPT to achieve this using a RAG (Retrieval Augmented Generation) model with the help of semantic search. However, I have noticed that the primitive chatbot I developed can answer questions such as &quot;Did I receive an email from X&quot; or &quot;Do I have any payments due?&quot; but often retrieves emails from a long time ago. The problem is that when I modify the question to state &quot;Did I receive an email from X in the past week?&quot;, it can't retrieve the right emails because the embeddings I used do not understand &quot;in the past week&quot; even if I provide the current date in the query. In order to fix this and a few other problems, I want to implement a two layered system where I put all my emails in a SQL database with their message-ids, to, and from etc, choose the relevant emails from the database and then run semantic search on those to provide the final answer.</p>
<p>Now, my idea is to define two functions: one to generate a SQL query and retrieve emails from the database, and the other to run semantic search. I want to input these as functions to the GPT module and let it decide when it wants to call either function.</p>
<p>For example, if the question was &quot;How many emails did I receive in the past week?&quot;, all it has to do is run the SQL function. If the question was &quot;Did my credit card get approved?&quot;, it only needs to run a semantic search. However, for complex queries such as &quot;Summarize all important emails from the past week&quot;, it first needs to run the SQL function, then semantic search for &quot;important&quot;, and summarize. I am not sure how GPT can break it down into &quot;retrieve emails from SQL database for the past week&quot; and &quot;Run semantic search to look for important emails&quot;.</p>
<p>I have tried running test runs with chat gpt with a prompt along the lines of &quot;If you can't answer the question based on the given columns and their descriptions in the table, return the entire table&quot; to see if it can figure out when to run a SQL query and when not to. But the results aren't great. And I imagine this would be even harder when all of this is integrated and it has to figure out that it needs to do things sequentially. Any help is appreciated.</p>
","gpt-4"
"77467417","Building public GPTs for own PDFs","2023-11-12 01:18:29","","-2","614","<openai-api><chatgpt-api><gpt-4><chat-gpt-4>","<p>I tried creating a GPT on OpenAI by uploading a PDF. It worked well from the UI and was able to answer questions. But when I sent the link to some one they need to be ChatGPT plus users to use it. So I tried using an API and linking assistant with ChatCompletion end point but that kept on giving me errors. I also tried passing file id to chatCompletion but that did not work. Below are relevant code snippets - please guide on suitable approach.</p>
<pre><code>file = client.files.create(
  file=open(&quot;mydoc.pdf&quot;, &quot;rb&quot;),
  purpose='assistants'
)

assistant = client.beta.assistants.create(
  instructions=&quot;You will answer question on the pdf document that I have uploaded. ... &quot;,
  model=&quot;gpt-4-1106-preview&quot;,
  tools=[{&quot;type&quot;: &quot;retrieval&quot;}],
  file_ids=[file.id]
)

while True:
    user_input = input(&quot;You: &quot;) #followed by exit code


#using assitant with chat. Commented this to use file id with chatCompletion
'''
response = client.assistants.chat(
        assistant_id=assistant_id,
        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}]
    )
'''
#using file id with ChatCompletion
response = client.ChatCompletion.create(
        model=&quot;gpt-4-1106-preview&quot;, 
        messages=[
            {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You will answer question on the pdf document that I have uploaded. ...&quot;},
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_input}
        ],
        file=file.id
    )
</code></pre>
<p>Regards
dbeings</p>
","gpt-4"
"77454460","Is the new OpenAI API version backward compatible for accessing/querying GPT 3.5 Turbo?","2023-11-09 15:35:46","","0","105","<python><openai-api><gpt-3><gpt-4>","<p>Is the new API version backward compatible for accessing/querying GPT 3.5 Turbo? For example, will code including the following tailored to the new version still work with 3.5 Turbo?</p>
<pre><code>client = OpenAI(api_key = get_key())
response = client.chat.completions.create(
    model = model,
    messages = messages,
    max_tokens = max_tokens,
    n = n,
    stop = stop,
    temperature = temperature,
    frequency_penalty = frequency_penalty,
    presence_penalty = presence_penalty
    )
</code></pre>
","gpt-4"
"77442143","Azure GPT-4 API using PHP","2023-11-08 00:04:09","","1","522","<php><azure><php-curl><gpt-4>","<p>Using <code>curl</code> I can make a successful connection, which returns a completion:</p>
<pre><code>curl &quot;https://myazure.openai.azure.com/openai/deployments/azureGPT4/chat/completions?api-version=2023-07-01-preview&quot; \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;api-key: myapikey&quot; \
  -d '{&quot;messages&quot;:[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello&quot;}]}'
</code></pre>
<p>But when attempting to do the same using this PHP script</p>
<pre><code>$api_url = &quot;https://myazure.openai.azure.com/openai/deployments/azureGPT4/chat/completions?api-version=2023-07-01-preview&quot;;

$api_key = &quot;myapikey&quot;;

$request_data = array(
    'engine' =&gt; 'azureGPT4',
    'messages' =&gt; [
        [&quot;role&quot; =&gt; &quot;system&quot;, &quot;content&quot; =&gt; &quot;You are a helpful assistant.&quot;],
        [&quot;role&quot; =&gt; &quot;user&quot;, &quot;content&quot; =&gt; &quot;Hello&quot;]
    ]
);

$request_json = json_encode($request_data);

$ch = curl_init($api_url);
curl_setopt($ch, CURLOPT_VERBOSE, true); 
curl_setopt($ch, CURLOPT_STDERR, fopen('php://stderr', 'w')); 
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, $request_json);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_HTTPHEADER, array(
    'Content-Type: application/json',
    'Ocp-Apim-Subscription-Key: ' . $api_key, 
));

$response = curl_exec($ch);
</code></pre>
<p>I get this error:</p>
<p>Error: Access denied due to invalid subscription key or wrong API endpoint. Make sure to provide a valid key for an active subscription and use a correct regional API endpoint for your resource.</p>
","gpt-4"
"77434808","OpenAI API: How do I enable JSON mode using the gpt-4-vision-preview model?","2023-11-06 23:46:14","","8","17699","<python><openai-api><gpt-4>","<p>Update: It seems like they made a mistake in the API docs, and fixed it now.</p>
<p>Earlier, it said &quot;when calling <code>gpt-4-vision-preview</code> or <code>gpt-3.5-turbo</code>,&quot; but now reads &quot;when calling <code>gpt-4-1106-preview</code> or <code>gpt-3.5-turbo-1106</code>.&quot;</p>
<hr />
<p>According to <a href=""https://platform.openai.com/docs/guides/text-generation/json-mode"" rel=""noreferrer"">Text generation - OpenAI API</a>, &quot;when calling <code>gpt-4-vision-preview</code> or <code>gpt-3.5-turbo</code>, you can set response_format to <code>{ type: &quot;json_object&quot; }</code> to enable JSON mode.&quot;</p>
<p>However, the following code throws an error:</p>
<pre><code> {'error': {'message': '1 validation error for Request\nbody -&gt; response_format\n  extra fields not permitted (type=value_error.extra)', 'type': 'invalid_request_error', 'param': None, 'code': None}}
</code></pre>
<p>If I comment <code>&quot;response_format&quot;: {&quot;type&quot;: &quot;json_object&quot;}</code>, it works fine.</p>
<pre><code>    headers = {
        &quot;Content-Type&quot;: &quot;application/json&quot;,
        &quot;Authorization&quot;: f&quot;Bearer {api_key}&quot;
    }
    
    payload = {
        &quot;model&quot;: &quot;gpt-4-vision-preview&quot;,
        &quot;response_format&quot;: {&quot;type&quot;: &quot;json_object&quot;},
        &quot;messages&quot;: [
          {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: &quot;You are a helpful assistant. Your response should be in JSON format.&quot;
          },
          {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: [
              {
                &quot;type&quot;: &quot;text&quot;,
                &quot;text&quot;: prompt
              },
              {
                &quot;type&quot;: &quot;image_url&quot;,
                &quot;image_url&quot;: {
                  &quot;url&quot;: f&quot;data:image/jpeg;base64,{base64_image}&quot;
                }
              }
            ]
          }
        ],
        &quot;max_tokens&quot;: 1000,
    }
    
    response = requests.post(&quot;https://api.openai.com/v1/chat/completions&quot;, headers=headers, json=payload)
    print(response.json())
</code></pre>
","gpt-4"
"77352565","Azure OpenAI On Your Data - System message usage","2023-10-24 13:35:20","","8","1335","<azure><azure-cognitive-services><azure-openai><gpt-4>","<p>I'm working with Azure OpenAI On Your Data, trying to understand:</p>
<ul>
<li>why system message seems to not work as expected</li>
<li>strategy for providing instructions in ‚Äúon your data‚Äù case</li>
</ul>
<p>For example, calling <code>POST https://{service-name}.openai.azure.com/openai/deployments/{model-name}/extensions/chat/completions?api-version=2023-08-01-preview</code></p>
<pre><code>{
    &quot;messages&quot;: [
        {
            &quot;role&quot;: &quot;system&quot;,
            &quot;content&quot;: &quot;Your task is to always respond in French.&quot;
        },
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How to cherry pick a PR?&quot;
        }
    ],
    &quot;temperature&quot;: 0.5,
    &quot;max_tokens&quot;: 12000,
    &quot;top_p&quot;: 1,
    &quot;dataSources&quot;: [
        {
            &quot;type&quot;: &quot;AzureCognitiveSearch&quot;,
            &quot;parameters&quot;: {
                ...
                &quot;queryType&quot;: &quot;semantic&quot;,
                &quot;inScope&quot;: true,
                &quot;roleInformation&quot;: &quot;Your task is to always respond in French.&quot;
            }
        }
    ]
}
</code></pre>
<p>I get the following response:</p>
<pre><code>{
    &quot;id&quot;: &quot;GUID&quot;,
    &quot;model&quot;: &quot;gpt-4-32k&quot;,
    &quot;created&quot;: timestamp,
    &quot;object&quot;: &quot;extensions.chat.completion&quot;,
    &quot;choices&quot;: [
        {
            &quot;index&quot;: 0,
            &quot;finish_reason&quot;: &quot;stop&quot;,
            &quot;message&quot;: {
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: &quot;The most common way to cherry pick your PR ...&quot;,
                &quot;end_turn&quot;: true,
                &quot;context&quot;: {
                    &quot;messages&quot;: [
                        {
                            &quot;role&quot;: &quot;tool&quot;,
                            &quot;content&quot;: &quot;{\&quot;citations\&quot;: [{\&quot;content\&quot;: \&quot;{citation_content}\&quot;, \&quot;intent\&quot;: \&quot;How to cherry pick a PR?\&quot;}&quot;,
                            &quot;end_turn&quot;: false
                        }
                    ]
                }
            }
        }
    ]
}
</code></pre>
<p>As we can see, system message had no effect. Is this a known issue?</p>
<p>Example providing instructions in user‚Äôs prompt, main problem is that search query used in Azure Search contains parts of that prompt.</p>
<p><code>POST https://{service-name}.openai.azure.com/openai/deployments/{model-name}/extensions/chat/completions?api-version=2023-08-01-preview</code></p>
<pre><code>{
    &quot;messages&quot;: [
        {
            &quot;role&quot;: &quot;user&quot;,
            &quot;content&quot;: &quot;How to cherry pick a PR? When responding to this query, please translate the message to French.&quot;
        }
    ],
    &quot;temperature&quot;: 0.5,
    &quot;max_tokens&quot;: 12000,
    &quot;top_p&quot;: 1,
    &quot;dataSources&quot;: [
        {
            &quot;type&quot;: &quot;AzureCognitiveSearch&quot;,
            &quot;parameters&quot;: {
                ...
                &quot;queryType&quot;: &quot;semantic&quot;,
                &quot;inScope&quot;: true,
                &quot;roleInformation&quot;: &quot;Your task is to always respond in French.&quot;
            }
        }
    ]
}
</code></pre>
<p>I get the following response:</p>
<pre><code>{
    &quot;id&quot;: &quot;GUID&quot;,
    &quot;model&quot;: &quot;gpt-4-32k&quot;,
    &quot;created&quot;: timestamp,
    &quot;object&quot;: &quot;extensions.chat.completion&quot;,
    &quot;choices&quot;: [
        {
            &quot;index&quot;: 0,
            &quot;finish_reason&quot;: &quot;stop&quot;,
            &quot;message&quot;: {
                &quot;role&quot;: &quot;assistant&quot;,
                &quot;content&quot;: &quot;Voici comment vous pouvez choisir un PR ...&quot;,
                &quot;end_turn&quot;: true,
                &quot;context&quot;: {
                    &quot;messages&quot;: [
                        {
                            &quot;role&quot;: &quot;tool&quot;,
                            &quot;content&quot;: &quot;{\&quot;citations\&quot;: [{\&quot;content\&quot;: \&quot;This is the most common way ....\&quot;, \&quot;intent\&quot;: \&quot;How to cherry pick a PR? When responding to this query, please translate the message to French.\&quot;}&quot;,
                            &quot;end_turn&quot;: false
                        }
                    ]
                }
            }
        }
    ]
}
</code></pre>
<ul>
<li>prompt instruction directly on user‚Äôs prompt worked, but the search query contains the prompt (as you can see intent, that‚Äôs the field used to search in search service) .</li>
</ul>
<h2>Two main questions:</h2>
<ul>
<li>how to use the system message? I‚Äôve tried different approaches (like using the <code>roleInformation</code> in <code>dataSources</code> or moving the system message after the user prompt) but none seems to work. Is this a known issue?</li>
<li>how can I provide instructions to the model. Maybe I want to respond with a summary or I want to respond in a specific way on each iteration. How can I provide that kind of instruction if that instruction will be included in the search query? I‚Äôm sure there‚Äôs a better strategy here.</li>
</ul>
","gpt-4"
"77338347","Getting AttributeError when using openAI python library","2023-10-22 01:20:38","77338392","0","328","<python><artificial-intelligence><openai-api><gpt-4>","<p>I'm building a new AI chatbot utilizing the openai library and I have a gradio UI set up in one file (app.py) and a predict() function in another (trainedBot.py)
Every time I send a request through the gradio UI I get this error:</p>
<pre><code>File &quot;/home/user/app/trainedBot.py&quot;, line 48, in predict
    return response.choices.message.content
AttributeError: 'list' object has no attribute 'message'
</code></pre>
<p>I tried putting both the message and the openAI response in a variable and nothing happened. Still got the same error. The response looked like this:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;id&quot;: &quot;chatcmpl-8CHKgpOewruWDC2Et1R6ZFtdPmSQR&quot;,
  &quot;object&quot;: &quot;chat.completion&quot;,
  &quot;created&quot;: 1697937254,
  &quot;model&quot;: &quot;gpt-4-0613&quot;,
  &quot;choices&quot;: [
    {
      &quot;index&quot;: 0,
      &quot;message&quot;: {
        &quot;role&quot;: &quot;assistant&quot;,
        &quot;content&quot;: &quot;HIIIIIII&quot;
      },
      &quot;finish_reason&quot;: &quot;stop&quot;
    }
  ],
  &quot;usage&quot;: {
    &quot;prompt_tokens&quot;: 308,
    &quot;completion_tokens&quot;: 4,
    &quot;total_tokens&quot;: 312
  }
}
</code></pre>
","gpt-4"
"77322505","OPENAI GPT-4 response length limit","2023-10-19 09:22:17","","1","903","<openai-api><gpt-4>","<p>I am writing prompt for generating blog using gpt-4, ant it always returns around 700 words long response.</p>
<p>Followings are prompts I tried.</p>
<p><code>Write a 2000 words long blog about ... </code></p>
<p><code>Length: 2000 words</code></p>
<p><code>Response characters 8000 characters</code></p>
<p>I have prompt saying  prompt at the end
<code>Write _THE_END_ at the end of the repsonse</code></p>
<p>and let my python script check for the keyword _THEEND before sending
continue
prompt to again.</p>
<p>Please help me with the prompt that will actually generate 2000+ words response.</p>
<p>I expect output from gpt-4 to be a 2000+ words long.
but it's always around 700 words long</p>
","gpt-4"
"77285102","How to format a few-shot prompt for GPT4 Chat Completion API?","2023-10-13 04:45:26","77293295","2","4952","<openai-api><chatgpt-api><completion><gpt-4><few-shot-learning>","<p><strong>I'm trying to use the GPT4's chat completion API for the following prompt:</strong></p>
<pre><code>For each situation, describe the intent. Examples:


Situation 1: Devin gets the newspaper.

The intent of Situation 1: Devin intends to read the newspaper.

Situation 2: Jamie works all night.

The intent of Situation 2: Jamie intends to meet a deadline.

Situation 3: Sydney destroys Ryan.

The intent of Situation 3: Sydney intends to punish Ryan.

Situation 4: Lindsay clears her mind.

The intent of Situation 4: Lindsay intends to be ready for a new task.

Situation 5: Rowan wants to start a business.

The intent of Situation 5: Rowan intends to be self sufficient.

Situation 6: Lee ensures Ali‚Äôs safety.

The intent of Situation 6: Lee intends to be helpful.

Situation 7: Riley buys lottery tickets.

The intent of Situation 7: Riley intends to become rich.

Situation 8: Alex makes Chris wait.

The intent of Situation 8: Alex intends
</code></pre>
<p>As you can see, I want to complete the sentence that says &quot;Alex intends&quot;. This prompt is intuitive for GPT3's Completion API where you only had to put one prompt that has all the few-shots examples.</p>
<p><strong>However, I don't know what is the best practice to perform the same prompting with GPT4's ChatCompletion API.</strong> I've checked out <a href=""https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb"" rel=""nofollow noreferrer"">https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb</a>
where they provided an example of how to do few-shot prompting, but my prompt is not &quot;conversational&quot; as you can see.</p>
<p><strong>I'm not even sure whether the &quot;name&quot; parameter impacts the result's quality. Does anybody have an answer to this?</strong></p>
<p>What I thought of so far is to format my prompt like this as the content from the above link instructed:</p>
<pre><code>messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;For each situation, describe the intent. Examples:&quot;},
        {&quot;role&quot;: &quot;system&quot;, &quot;name&quot;:&quot;Situation 1&quot;, &quot;content&quot;: &quot;Devin gets the newspaper.&quot;},
        {&quot;role&quot;: &quot;system&quot;, &quot;name&quot;: &quot;The intent of Situation 1&quot;, &quot;content&quot;: &quot;Devin intends to read the newspaper.&quot;},
        {&quot;role&quot;: &quot;system&quot;, &quot;name&quot;:&quot;Situation 2&quot;, &quot;content&quot;: &quot;Jamie works all night.&quot;},
        {&quot;role&quot;: &quot;system&quot;, &quot;name&quot;: &quot;The intent of Situation 2&quot;, &quot;content&quot;: &quot;Jamie intends to meet a deadline.&quot;},

...
        {&quot;role&quot;: &quot;system&quot;, &quot;name&quot;:&quot;Situation 8&quot;, &quot;content&quot;: &quot;Alex makes Chris wait.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;name&quot;: &quot;The intent of Situation 8&quot;, &quot;content&quot;: &quot;&quot;},
    ]
</code></pre>
<p><strong>Is this a proper way to do few-show with GPT4 ChatCompletion API? Please let me know if you have a better solution or explanations on why certain parts of my prompt needs work.</strong></p>
<p>So far, I've simply put the original prompt into one user content, just like it is GPT3's Completion API:</p>
<pre><code>messages=[
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;For each situation, describe the intent. Examples:


Situation 1: Devin gets the newspaper.

The intent of Situation 1: Devin intends to read the newspaper.

Situation 2: Jamie works all night.

The intent of Situation 2: Jamie intends to meet a deadline.

Situation 3: Sydney destroys Ryan.

The intent of Situation 3: Sydney intends to punish Ryan.

Situation 4: Lindsay clears her mind.

The intent of Situation 4: Lindsay intends to be ready for a new task.

Situation 5: Rowan wants to start a business.

The intent of Situation 5: Rowan intends to be self sufficient.

Situation 6: Lee ensures Ali‚Äôs safety.

The intent of Situation 6: Lee intends to be helpful.

Situation 7: Riley buys lottery tickets.

The intent of Situation 7: Riley intends to become rich.

Situation 8: Alex makes Chris wait.

The intent of Situation 8: Alex intends&quot;}
]
</code></pre>
<p>It does work, but I was wondering if I can boost the API's performance if I follow a certain practice.</p>
","gpt-4"
"77284901","Upload an image to chat gpt using the API?","2023-10-13 03:27:56","","20","25484","<gpt-4>","<p>How do you upload an image to chat gpt using the API? Can you give an example of code that can do that?</p>
<p>I've tried looking at the documentation, but they don't have a good way to upload a jpg as context.</p>
","gpt-4"
"77278712","Azure openai The API deployment for this resource does not exist","2023-10-12 07:55:17","","4","9000","<azure><openai-api><azure-openai><gpt-4>","<p>I am making sequential calls to Azure OpenAI GPT-4 from a python code. The token size of each call is approx 5000 tokens (inclusing input, prompt and output). The first call goes good. But the API is failing everytime on the 2nd request throwing the below error:</p>
<pre><code>&quot;The API deployment for this resource does not exist. If you created the deployment within the last 5 minutes,&quot;
</code></pre>
<p>I tried to put a sleep command of till 60s before hitting the API, since I know that there is a limit to number of tokens per minute for GPT4. But still throwing the same issue in every 2nd call.
Any suggestions will be very helpful.</p>
","gpt-4"
"77272754","OpenAI API gives random and nonsense responses","2023-10-11 11:45:57","","-3","761","<openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I am developing a Next.js app where users pick some ingredients and get a recipe idea which can be made with them. I am using OpenAI API for that but I couldn't find a way to optimize it. I want to get responses with the same quality as in chatGPT (or something close to it) but instead, it gives me random, incomplete and out-of-context responses. I tried to play with max tokens, temperature, role, model etc. but it didn't work. Here is an example:</p>
<p>This is a prompt and response on chatGPT:</p>
<p><em><strong>prompt:</strong></em>
<em>Create a recipe made with potato, paprika, salmon and yogurt. First state the title, then the ingredients list and then step-by-step instructions.</em></p>
<p><em><strong>response:</strong></em>
<em>Title: Paprika-Spiced Salmon with Creamy Potato and Yogurt Mash</em></p>
<p><em>Ingredients:</em></p>
<p><em>For the Salmon:</em>
<em>- 2 salmon fillets</em>
<em>- 1 teaspoon paprika</em>
<em>- Salt and pepper to taste</em>
<em>- 2 tablespoons olive oil</em></p>
<p><em>For the Creamy Potato and Yogurt Mash:</em>
<em>- 4 medium-sized potatoes, peeled and cut into chunks</em>
<em>- 1/2 cup plain yogurt</em>
<em>- Salt and pepper to taste</em></p>
<p><em>Instructions:</em></p>
<p><em>For the Salmon:</em></p>
<p><em>1. Preheat your oven to 375¬∞F (190¬∞C).</em></p>
<p><em>2. So on....</em></p>
<p>This result is exactly what I want.</p>
<p>And this is the result I get when I try the same prompt with the API :</p>
<pre><code>{
  &quot;prompt&quot;: &quot;Please create a recipe made with potato, paprika, salmon and yogurt. First state the title, then the ingredients list and then step-by-step instructions.&quot;,
  &quot;max_tokens&quot;: 200,
  &quot;temperature&quot;: 0.3
}

response: 

{
    &quot;id&quot;: &quot;cmpl-88ReJXcbcT3zstFOQTStJDnG528en&quot;,
    &quot;object&quot;: &quot;text_completion&quot;,
    &quot;created&quot;: 1697023599,
    &quot;model&quot;: &quot;davinci&quot;,
    &quot;choices&quot;: [
        {
            &quot;text&quot;: &quot;\n\nEasy ‚òÜ Cooked rice remake risotto\n\nI will remake the cooked rice deliciously. Since the ingredients are already included, it is delicious risotto with only eggs.\n\nTakoyaki style rice ball\n\nI love takoyaki, but it is troublesome to make, so it's a Takoyaki style rice ball. Because the color is also beautiful, it is also recommended for Hinamatsuri and lunch.\n\nRange easy! Omurice style\n\nAll the ingredients are fine about your choice. Washing is easy because it can be made without the need for cooking utensils.\n\nEasy lunch ‚ô° ‚ô° ‚ô° ÁÇí rice bowl\n\nTuna fried rice ‚ô° „Éë „É© „Åµ „Åµ „Åµ „Åµ „Åµ „Åµ „Åµ ‚ô° ‚ô° ‚ô° ‚ô° ‚ô° „Åô „Åê\n\nSoymilk Japanese-style doria\n\nThis&quot;,
            &quot;index&quot;: 0,
            &quot;logprobs&quot;: null,
            &quot;finish_reason&quot;: &quot;length&quot;
        }
    ],
    &quot;usage&quot;: {
        &quot;prompt_tokens&quot;: 33,
        &quot;completion_tokens&quot;: 200,
        &quot;total_tokens&quot;: 233
    }
}
</code></pre>
<p>Are the some specific properties to optimize the results with API? Or is it not possible to get chatGPT-quality results no matter what I do?</p>
","gpt-4"
"77210263","How to send data in batches to a LLM","2023-10-01 11:22:30","","1","3365","<batch-processing><openai-api><large-language-model><llama><gpt-4>","<p>I have a question on batch processing with LLMs and wanted to see if anyone can help with this.</p>
<p>Is there a way to send data in batches to a LLM to be processed by a GPU? Say for eg. if I wanted to get daily sentiment ratings, rather than sending daily posts to a LLM, is there a way to send 100 days of posts in batches to a LLM to be processed by GPU (so to maximize GPU utilisation, and minimize run time)?</p>
<p>I am looking for a solution for either a closed sourced LLM (like GPT3.5/4) or open sourced LLM (like Llama 2).</p>
","gpt-4"
"77205725","How can I upload an image as context with a prompt to GPT4's api?","2023-09-30 04:58:12","","10","4001","<openai-api><chatgpt-api><gpt-4><chat-gpt-4>","<p>I see there are ways of doing various image generation here:  <a href=""https://platform.openai.com/docs/api-reference/images"" rel=""noreferrer"">https://platform.openai.com/docs/api-reference/images</a></p>
<p>But I'm just trying to sent chat gpt a png file, ask &quot;what is this?&quot; or something like that and then get back a response.</p>
","gpt-4"
"77173368","Do GPT-4 and GPT-3.5 share the same token encoder?","2023-09-25 14:08:12","","0","1183","<token><openai-api><gpt-3><gpt-4>","<p>I want to know if I can use the same token counter for various GPT models - especially GPT-3, GPT-3.5, and GPT-4.</p>
<p>GPT models by OpenAI need texts to be tokenized (using <a href=""https://en.wikipedia.org/wiki/Byte_pair_encoding"" rel=""nofollow noreferrer"">Byte Pair Encoding</a>, BPE), see <a href=""https://platform.openai.com/tokenizer"" rel=""nofollow noreferrer"">Interactive GPT tokenizer</a>. I haven't found a direct statement if they use the same or different tokenizers. Even this official OpenAI page says that:</p>
<blockquote>
<p>If you need a programmatic interface for tokenizing text, check out our <a href=""https://github.com/openai/tiktoken"" rel=""nofollow noreferrer"">tiktoken</a> package for Python. For JavaScript, the <a href=""https://www.npmjs.com/package/gpt-3-encoder"" rel=""nofollow noreferrer"">gpt-3-encoder</a> package for node.js <strong>works for most GPT-3 models</strong>.</p>
</blockquote>
","gpt-4"
"77171631","GPT4All add context","2023-09-25 09:56:34","","2","946","<python><gpt-4><gpt4all>","<p>i want to add a context before send a prompt to my gpt model.
i use orca-mini-3b.ggmlv3.q4_0 model.</p>
<p>this is my code, i add a PromptTemplate to RetrievalQA.from_chain_type, but when a send a prompt it's not work, in this example the bot not call me &quot;bob&quot;</p>
<pre><code>#!/usr/bin/env python3
from langchain import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.prompts import ChatPromptTemplate
from langchain.vectorstores import Chroma
from langchain.llms import GPT4All, LlamaCpp
import chromadb
import time
import os

def main():

    embeddings_model_name=&quot;model_transformers/all-MiniLM&quot;
    persist_directory=&quot;db&quot;
    os.environ[&quot;PERSIST_DIRECTORY&quot;] = persist_directory
    from constants import CHROMA_SETTINGS
    model_type=&quot;GPT4All&quot;
    model_path=&quot;orca-mini-3b.ggmlv3.q4_0 (1).bin&quot;
    model_n_ctx=1000
    model_n_batch=8
    target_source_chunks=4
    # Parse the command line arguments
    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)
    chroma_client = chromadb.PersistentClient(settings=CHROMA_SETTINGS , path=persist_directory)
    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS, client=chroma_client)
    retriever = db.as_retriever(search_kwargs={&quot;k&quot;: target_source_chunks})
    # activate/deactivate the streaming StdOut callback for LLMs
    callbacks = [StreamingStdOutCallbackHandler()]
    # Prepare the LLM
    match model_type:
        case &quot;LlamaCpp&quot;:
            llm = LlamaCpp(model_path=model_path, max_tokens=model_n_ctx, n_batch=model_n_batch, callbacks=callbacks, verbose=False)
        case &quot;GPT4All&quot;:
            llm = GPT4All(model=model_path, max_tokens=model_n_ctx, backend='gptj', n_batch=model_n_batch, callbacks=callbacks, verbose=False)
        case _default:
            # raise exception if model_type is not supported
            raise Exception(f&quot;Model type {model_type} is not supported. Please choose one of the following: LlamaCpp, GPT4All&quot;)

    prompt_template = &quot;&quot;&quot;
    My name is Bob, you must call me Bob
    {context}&quot;&quot;&quot;

    prompt = PromptTemplate(
        input_variables=[&quot;context&quot;],
        template=prompt_template
    )

    chain_type_kwargs = {&quot;prompt&quot;: prompt}
    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=&quot;stuff&quot;, retriever=retriever,
                                     chain_type_kwargs=chain_type_kwargs,
                                     return_source_documents=False)
    # Interactive questions and answers
    print(&quot;start&quot;)
    while True:
        query = input(&quot;\nEnter a query: &quot;)
        if query == &quot;exit&quot;:
            break
        if query.strip() == &quot;&quot;:
            continue

        # Get the answer from the chain
        start = time.time()
        res = qa(query)
        answer, docs = res['result'], []
        end = time.time()

        print(f&quot;\n&gt; Answer (took {round(end - start, 2)} s.):&quot;)
        print(answer)


if __name__ == &quot;__main__&quot;:
    main()
</code></pre>
<p>in this example, i sent &quot;what is my name ?&quot;, i want to outpout: &quot;your name is bob&quot;, but i have this:
`Enter a query: what is my name ?</p>
<blockquote>
<p>Answer (took 34.79 s.):</p>
</blockquote>
<p>Project Administrator | The user who will become the self-care role for the project. This user has access to all parts of the project and can perform actions such as creating resources, updating configurations, and deleting resources. Project Name | The name of the project on OpenShift. It must be in lowercase and contain only letters and numbers. There is a blacklist to prevent certain characters from being used.`</p>
<p>why ?</p>
<p>i want to add history context for the conversation of the bot</p>
","gpt-4"
"77127848","How do I generate a valid arkose_token to create GPT-4 conversations?","2023-09-18 13:55:24","","2","2489","<reactjs><google-chrome-extension><openai-api><chatgpt-api><gpt-4>","<p>For context, I am trying to build a Chrome extension popup that allows users to chat with ChatGPT on any web page and I am struggling to figure out how I can generate the <code>arkose_token</code> that is required within the request payload for creating GPT-4 conversations.</p>
<p>The endpoint  I am trying to call is: <code>https://chat.openai.com/backend-api/conversation</code> (POST)</p>
<p>I will be using the current ChatGPT web session to authenticate the request and I just need to find a way to pass a valid arkose token, here is an example payload:</p>
<pre class=""lang-json prettyprint-override""><code>{
    &quot;action&quot;: ... ,
    &quot;messages&quot;: ... ,
    &quot;parent_message_id&quot;: ... ,
    &quot;model&quot;: &quot;gpt-4&quot;,
    &quot;timezone_offset_min&quot;: ... ,
    &quot;suggestions&quot;: ... ,
    &quot;history_and_training_disabled&quot;: ... ,
    &quot;arkose_token&quot;: NEED TO PASS A VALID TOKEN HERE,
    &quot;force_paragen&quot;: ...
}
</code></pre>
<p>I have explored these GitHub repos but I'm not quite sure on how to integrate them into my Chrome extension project:</p>
<ul>
<li><a href=""https://github.com/noahcoolboy/funcaptcha"" rel=""nofollow noreferrer"">https://github.com/noahcoolboy/funcaptcha</a>
<ul>
<li>not supported within a browser environement</li>
</ul>
</li>
<li><a href=""https://github.com/acheong08/arkose-generator"" rel=""nofollow noreferrer"">https://github.com/acheong08/arkose-generator</a>
<ul>
<li>ran the script locally and it returns a <code>bda</code> value, I am not too sure what to do with this</li>
</ul>
</li>
</ul>
<p>I know this can be done because there are existing extensions that allow you to use your ChatGPT account and chat with the GPT-4 model with no issues, for example:
<a href=""https://chrome.google.com/webstore/detail/harpa-ai-automation-agent/eanggfilgoajaocelnaflolkadkeghjp?hl=en-GB&amp;authuser=1"" rel=""nofollow noreferrer"">https://chrome.google.com/webstore/detail/harpa-ai-automation-agent/eanggfilgoajaocelnaflolkadkeghjp?hl=en-GB&amp;authuser=1</a></p>
<p>Has anyone else faced these issues or have any experience with this? I'm unsure of the best workflows/handling of these arkose tokens.</p>
","gpt-4"
"77102881","OpenAI API: What would be a good strategy to handle 80+ function calling?","2023-09-14 08:05:17","","7","1807","<nlp><tokenize><openai-api><chatgpt-api><gpt-4>","<p>My business handles a variety of entities (job, invoice, quote, resource, vehicle, contact, person, message, alert, etc.).</p>
<p>My goal is to use OpenAI function calling to allow my users to ask &quot;anything&quot; about their data. If you roughly estimate 20 entities and 4 potential actions each, that's approximately 80 API calls I need to define in my prompt so GPT can select the one best matching the user's question.</p>
<p>Not only would this consume a significant number of tokens, but it might also confuse the engine.</p>
<p>What are some effective strategies to work around this issue?</p>
<p>Initially, I considered presenting users with a route so they can first select which area of my system they want to inquire about, in order to narrow it down. However, this defeats the purpose of being able to &quot;ask for anything.&quot;</p>
<p>I'm also considering making an initial call to the OpenAI API to identify the entity from a predefined list, followed by a second call with a list of my APIs matching that entity. However, if the user's language doesn't match my predefined list, I may direct the logic down the wrong path. For example, &quot;<em>What is Ryan's phone number?</em>&quot; Is Ryan a resource? A person? A web user? What if the context needs switching because the user asks a follow-up question?</p>
<p><strong>-- Edit --</strong></p>
<p>After Rok's answer I think I need to precise I have over 2,000 customers who will be be asking questions about their own data. Each of these customers could have 100k CRM contacts, 100k jobs completed for these contacts, 100k invoices raised for these jobs etc...</p>
","gpt-4"
"77037206","How to get conversation from guidance","2023-09-04 11:07:39","","0","30","<chatgpt-api><large-language-model><gpt-4>","<p>I want to continue chatting with model (gpt-4 in my case) after the <a href=""https://github.com/guidance-ai/guidance"" rel=""nofollow noreferrer"">guidance</a> program is executed.
Example:</p>
<pre class=""lang-py prettyprint-override""><code>experts = guidance('''
{{#system~}}
You are a helpful and terse assistant.
{{~/system}}

{{#user~}}
I want a response to the following question:
{{query}}
Name 3 world-class experts (past or present) who would be great at answering this?
Don't answer the question yet.
{{~/user}}

{{#assistant~}}
{{gen 'expert_names' temperature=0 max_tokens=300}}
{{~/assistant}}

{{#user~}}
Great, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.
{{~/user}}

{{#assistant~}}
{{gen 'answer' temperature=0 max_tokens=500}}
{{~/assistant}}''')
experts(query='How can I be more productive?', caching=False)
</code></pre>
<p>outputs:</p>
<pre><code>**system**
You are a helpful and terse assistant.
**user**
I want a response to the following question:
How can I be more productive?
Name 3 world-class experts (past or present) who would be great at answering this?
Don't answer the question yet.
**assistant**
1. Tim Ferriss
2. David Allen
3. Stephen Covey
**user**
Great, now please answer the question as if these experts had collaborated in writing a joint anonymous answer.
**assistant**
To be more productive:

1. Prioritize tasks using the Eisenhower Matrix, focusing on important and urgent tasks first.
2. Implement the Pomodoro Technique, breaking work into focused intervals with short breaks.
3. Continuously improve time management and organization skills by following the principles of David Allen's &quot;Getting Things Done&quot; method.
</code></pre>
<p>And after that I want to continue conversation with exact same dialogue. How can I achieve this?</p>
","gpt-4"
"77007205","When using GPT-4 API, do I need to send the entire conversation back each time?","2023-08-30 10:35:20","","6","4158","<openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I'm new to OpenAI API. I work with GPT-3.5-Turbo, using this code:</p>
<pre><code>messages = [
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You‚Äôre a helpful assistant&quot;}
    ]

    while True:
        content = input(&quot;User: &quot;)
        if content == 'end':
            save_log(messages)
            break
        messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: content})

        completion = openai.ChatCompletion.create(
            model=&quot;gpt-3.5-turbo-16k&quot;,
            messages=messages
        )

        chat_response = completion.choices[0].message.content
        print(f'ChatGPT: {chat_response}')
        messages.append({&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: chat_response})
</code></pre>
<p>Result:
<em>User: who was the first person on the moon?
GPT: The first person to step foot on the moon was Neil Armstrong, an American astronaut, on July 20, 1969, as part of NASA's Apollo 11 mission.
User: how tall is he?
GPT: Neil Armstrong was approximately 5 feet 11 inches (180 cm) tall.</em></p>
<p>But it requires tons of tokens. And I've heard that GPT-4 differs from GPT-3 in that it's able to remember the previous messages (on its own). Is that correct?</p>
<p>But if I remove the line where I append the 'messages' list with the latest one and send only one message:
<code>completion = openai.ChatCompletion.create( model=&quot;gpt-4&quot;, messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: content}] )</code>
it can't remember anything.</p>
<p><em>User: who was the first person on the moon?
GPT: The first person on the moon was Neil Armstrong on July 20, 1969.
User: how tall is he?
GPT: Without specific context or information about who &quot;he&quot; refers to, I'm unable to provide an accurate answer.</em></p>
<p>So I'm wondering is there any workflow difference between GPT-3.5-Turbo and GPT-4?</p>
","gpt-4"
"76964618","gpt4 logit_bias not working as expected for multi-token words","2023-08-23 20:12:07","","0","268","<openai-api><chatgpt-api><gpt-4>","<p>I'm trying to block GPT-4 from saying a multi-token word but can't seem to do it.
How can I block the word? (It works for the single-token word &quot;a&quot; but not the multi-token word &quot;truly&quot;)</p>
<p>I have tried blocking all prefixes of &quot;Truly&quot; and it won't even switch it to lowercase</p>
<p>See typescript code:
<code>npx ts-node myfile.ts</code></p>
<pre><code>import { OpenAIApi } from &quot;openai&quot;;
import { Configuration as OpenAiConfig } from &quot;openai/dist/configuration&quot;;
import tokenizer from &quot;gpt-3-encoder&quot;;

const openai = new OpenAIApi(
  new OpenAiConfig({
    apiKey: process.env.OPENAI_API_KEY,
  }),
);

async function request_gpt4&lt;T&gt;({
  messages,
  max_tokens,
  logit_bias,
}: {
  messages: any,
  max_tokens: number,
  logit_bias: { [x: string]: number },
}): Promise&lt;any&gt; {
  const model = &quot;gpt-4&quot;;
  const response = await openai
    .createChatCompletion({
      model,
      messages,
      temperature: 0.0,
      max_tokens,
      stop: [&quot; END&quot;],
      logit_bias,
      n: 1,
    }).catch((e) =&gt; {
      console.error(&quot;OPENAI CATCH ERROR&quot;, { ...e.toJSON(), config: undefined });
      return null;
    });
  // console.log(response);
  if (response) {
    return (response as any).data.choices[0].message.content;
  }
  return null;
}

function defaultAbBiases(bias: number): { [x: string]: number } {
  const tokens: number[] = ['a'].flatMap(tokenizer.encode);
  return {
    ...Object.fromEntries(tokens.map((t) =&gt; [t, bias])),
  };
}

function defaultTrulyFalselyBiases(bias: number): { [x: string]: number } {
  const tokens: number[] = [
    &quot; truly&quot;,
    &quot; Truly&quot;,
    &quot;r&quot;,
    &quot;ul&quot;,
    &quot;ruly&quot;,
    &quot;uly&quot;,
    &quot;T&quot;,
    &quot;Tr&quot;,
    &quot;Tru&quot;,
    &quot;Trul&quot;,
    &quot;Truly&quot;,
  ].flatMap(tokenizer.encode);
  return {
    ...Object.fromEntries(tokens.map((t) =&gt; [t, bias])),
  };
}

(async () =&gt; {
  for (var bias of [0, -1, -10, -100]) {
    const x = await request_gpt4({
      messages: [{
        role: &quot;user&quot;,
        content: `Say either: &quot;a&quot; or &quot;b&quot;`,
      }],
      max_tokens: 1,
      logit_bias: defaultAbBiases(bias),
    });
    console.log(bias, x);
  }
  for (var bias of [0, -1, -10, -100]) {
    const x = await request_gpt4({
      messages: [{
        role: &quot;user&quot;,
        content: `Say either: &quot;truly&quot; or &quot;falsely&quot;`,
      }],
      max_tokens: 5,
      logit_bias: defaultTrulyFalselyBiases(bias),
    });
    console.log(bias, x);
  }
  console.log(&quot;DONE&quot;);
})().catch(e =&gt; {
  console.log(e);
});
</code></pre>
<p>Output is:</p>
<pre><code>0 a
-1 a
-10 b
-100 b
0 Truly
-1 Truly
-10 Truly
-100 Truly
DONE
</code></pre>
<p>But I would expect it to switch to <code>Falsely</code> at some point</p>
","gpt-4"
"76934866","OpenAI API error: ""The requested module 'openai' does not provide an export named 'Configuration'""","2023-08-19 11:29:50","","0","1968","<reactjs><openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I am facing this error while running my backend in my React app:</p>
<pre><code>file:///D:/Project/server/route/dalle.routes.js:3
import { Configuration, OpenAIApi} from 'openai';
         ^^^^^^^^^^^^^
SyntaxError: The requested module 'openai' does not provide an export named 'Configuration'
</code></pre>
<p>What's wrong?</p>
","gpt-4"
"76920224","What is the difference between 'SQLDatabaseChain' and 'create_sql_agent' in langchain?","2023-08-17 09:34:06","","2","2155","<openai-api><langchain><py-langchain><gpt-4><langchain-js>","<p>I'm creating a feature that uses 'LangChain' to turn user input questions into SQL.</p>
<p>I know that 'create_sql_agent' has logic to fix any errors in the query answered by 'openai'.
Is 'SQLDatabaseChain' not error correcting?</p>
<p>If so, can using 'SQLDatabaseChain' result in tables or columns that are not in the database?
Or, can this part be solved only with 'use_query_checker=True' option?</p>
<p>Please explain the difference between 'SQLDatabaseChain' and 'create_sql_agent'.</p>
","gpt-4"
"76871913","GPT-3.5 embedds but GPT-4 doesn't?","2023-08-10 00:05:44","","0","121","<openai-api><chatgpt-api><gpt-4>","<p>I am experiencing a bizarr problem. In my chat application which is able to download websites for documentation interaction, I am able to initalise the chat function properly with 3.5 turbo but when I change the model to 4, the app isn't able to access the website/documentation. GPT4 used to work but now it doesn't.</p>
<p>There are no other changes to the code exept commenting out the undesired model.</p>
<p>Has anyone else experienced this, and does anyone have a possible solution?<a href=""https://i.sstatic.net/YKMYu.jpg"" rel=""nofollow noreferrer"">image of issue</a></p>
","gpt-4"
"76800148","SQLDatabaseChain with GPT-4 returns SQL syntax error","2023-07-31 00:58:33","76848245","1","2048","<langchain><py-langchain><gpt-4>","<p>I wrote a program trying to query local sqlite db, and it worked fine for <code>text-davinci-003</code>:</p>
<pre><code>llm = OpenAI(model_name=&quot;text-davinci-003&quot;, verbose=True)
</code></pre>
<p>However, after I changed it to GPT-4:</p>
<pre><code>llm = ChatOpenAI(model_name=&quot;gpt-4-0613&quot;, verbose=True)
...
db_chain = SQLDatabaseChain.from_llm(
    llm,
    db,
    verbose=True,
    use_query_checker=True,
    return_intermediate_steps=True,
)

with get_openai_callback() as cb:
    # No intermediate steps
    # result = db_chain.run(query)

    # If intermediate steps are needed...
    result = db_chain(query)
    intermediate_steps = result[&quot;intermediate_steps&quot;]

    print(&quot;&quot;)

    try:
        sql_result = intermediate_steps[3]
        print(&quot;SQL Query Result:&quot;)
        print(json.dumps(ast.literal_eval(sql_result), indent=4))
    except Exception as e:
        print(f&quot;Error while parsing the SQL result:\n{e}&quot;)
        print(&quot;&quot;)
        print(intermediate_steps)
    
    print(&quot;&quot;)

    print(cb)
</code></pre>
<p>... everything still works, except the final SQL query contained more text in addition to SQL query, i.e.:</p>
<pre><code>&gt; Entering new SQLDatabaseChain chain...
Have the user visited some news website? If yes, list all the urls.
DO NOT specify timestamp unless query said so.
DO NOT specify limit unless query said so.
SQLQuery:The original query appears to be correct as it doesn't seem to have any of the common mistakes listed. Here is the same query:

SELECT &quot;URL&quot; FROM browsinghistory WHERE &quot;Title&quot; LIKE '%news%'Traceback (most recent call last):
  File &quot;C:\path\Python311\Lib\site-packages\sqlalchemy\engine\base.py&quot;, line 1968, in _exec_single_context
    self.dialect.do_execute(
  File &quot;C:\path\Python311\Lib\site-packages\sqlalchemy\engine\default.py&quot;, line 920, in do_execute
    cursor.execute(statement, parameters)
sqlite3.OperationalError: near &quot;The&quot;: syntax error

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File &quot;D:\path\run.py&quot;, line 292, in &lt;module&gt;
    database_mode(llm, filepath, delimiter)
  File &quot;D:\path\run.py&quot;, line 156, in database_mode
    llm.query_database(db_path=db_path, query=query)
  File &quot;D:\path\modules\chatbot.py&quot;, line 220, in query_database
    result = db_chain(query)
             ^^^^^^^^^^^^^^^
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\langchain\chains\base.py&quot;, line 140, in __call__
    raise e
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\langchain\chains\base.py&quot;, line 134, in __call__
    self._call(inputs, run_manager=run_manager)
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\langchain\chains\sql_database\base.py&quot;, line 181, in _call
    raise exc
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\langchain\chains\sql_database\base.py&quot;, line 151, in _call
    result = self.database.run(checked_sql_command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\langchain\sql_database.py&quot;, line 334, in run
    cursor = connection.execute(text(command))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py&quot;, line 1413, in execute
    return meth(
           ^^^^^
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\sql\elements.py&quot;, line 483, in _execute_on_connection
    return connection._execute_clauseelement(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py&quot;, line 1637, in _execute_clauseelement
    ret = self._execute_context(
          ^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py&quot;, line 1846, in _execute_context
    return self._exec_single_context(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py&quot;, line 1987, in _exec_single_context
    self._handle_dbapi_exception(
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py&quot;, line 2344, in _handle_dbapi_exception
    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\base.py&quot;, line 1968, in _exec_single_context
    self.dialect.do_execute(
  File &quot;C:\path\AppData\Local\Programs\Python\Python311\Lib\site-packages\sqlalchemy\engine\default.py&quot;, line 920, in do_execute
    cursor.execute(statement, parameters)
sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) near &quot;The&quot;: syntax error
[SQL: The original query appears to be correct as it doesn't seem to have any of the common mistakes listed. Here is the same query:

SELECT &quot;URL&quot; FROM browsinghistory WHERE &quot;Title&quot; LIKE '%news%']
(Background on this error at: https://sqlalche.me/e/20/e3q8)
</code></pre>
<p>I know that I can try to tell it not to return anything but the query (might be unstable. though...), but why isn't this work for GPT-4, while it works for text-davinci-003?</p>
<hr />
<p>Update:</p>
<p>Tried with a different query, and the problem remains:</p>
<pre><code>&gt; Entering new SQLDatabaseChain chain...
List all websites visited by the user.
DO NOT specify timestamp unless query said so.
DO NOT specify limit unless query said so.
SQLQuery:The original query seems to be correct. It is simply selecting the &quot;URL&quot; column from the &quot;browsinghistory&quot; table. There is no misuse of any functions, no data type mismatch, no joins, etc.

Reproducing the original query:

SELECT &quot;URL&quot; FROM browsinghistory
...
...
...
</code></pre>
","gpt-4"
"76764462","Is there a way to access the history of API requests/calls made to my openAI account?","2023-07-25 15:53:11","","3","335","<python><python-requests><openai-api><gpt-4>","<p>I just made ~2k api requests to GPT but my code errored out and did not save (therefore, the calls were made but never stored to code). Is there a way to retrieve the output of those calls?</p>
","gpt-4"
"76741896","What part of OpenAI API request payload is limited by the max amount tokens?","2023-07-22 00:38:00","","1","837","<openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I kinda understand how to count tokens out of characters, but what do I actually have to count?
If I have a payload like this:</p>
<pre class=""lang-json prettyprint-override""><code>{
  &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,
  &quot;temperature&quot;: 1,
  &quot;max_tokens&quot;: 400,
  &quot;presence_penalty&quot;: 0.85,
  &quot;frequency_penalty&quot;: 0.85,
  &quot;messages&quot;: [
    {
      &quot;role&quot;: &quot;system&quot;,
      &quot;content&quot;: &quot;prompt&quot;
    },
    {
      &quot;role&quot;: &quot;assistant&quot;,
      &quot;content&quot;: &quot;message&quot;
    },
    // tens of messages
  ]
}
</code></pre>
<p>Do I have to count tokens out of it <strong>entirely</strong>? Or do I have to count it in <code>&quot;messages&quot;</code> only? If so, do I have to count all the json syntax characters, like spacebars, brackets and commas too? What about <code>&quot;role&quot;</code> and <code>&quot;content&quot;</code> keys? What about <code>&quot;role&quot;</code> value?<br>
Or I have to simply concat all the <code>&quot;content&quot;</code> values into a single string and count tokens based only on it? (this is what I <em>would like</em> to get as an answer, hehe)</p>
","gpt-4"
"76734099","OpenAI Chat Completions API: How do I use a function to store conversation memory?","2023-07-20 22:33:45","76778206","-1","1989","<python><openai-api><chatgpt-api><gpt-4>","<p>I am trying to make a chatbot using OpenAI Function Calling. I have taken the basic example of getting the current weather condition, which was given in the documentation.</p>
<p>What I want to implement is to have a memory with it.</p>
<p>I tried to append into the message, but what I want is when I have a new message, so instead of calling the function, how can it get the response from memory if it's already asked?</p>
<p>My code is like this:</p>
<pre><code>def get_current_weather(location, unit=&quot;fahrenheit&quot;):
    print(&quot;IT RAN&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&quot;)
    weather_info = {
        &quot;location&quot;: location,
        &quot;temperature&quot;: &quot;72&quot;,
        &quot;unit&quot;: unit,
        &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;],
    }
    return json.dumps(weather_info)


messages = []


def run_conversation(input_message):
    messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{input_message}&quot;})
    functions = [
        {
            &quot;name&quot;: &quot;get_current_weather&quot;,
            &quot;description&quot;: &quot;Get the details about a drug/medication&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;definition&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;,
                    },
                    &quot;unit&quot;: {&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;]},
                },
                &quot;required&quot;: [&quot;location&quot;],
            },
        }
    ]
    print(&quot;MESSAGE 1&quot;, messages)
    response = openai.ChatCompletion.create(
        model=&quot;gpt-3.5-turbo-0613&quot;,
        messages=messages,
        functions=functions,
        # function_call=&quot;auto&quot;,
    )
    response_message = response[&quot;choices&quot;][0][&quot;message&quot;]
    print(&quot;RESPONSE MSG&quot;, response_message)

    if response_message.get(&quot;function_call&quot;):
        available_functions = {&quot;get_current_weather&quot;: get_current_weather}
        function_name = response_message[&quot;function_call&quot;][&quot;name&quot;]
        function_to_call = available_functions[function_name]
        function_args = json.loads(response_message[&quot;function_call&quot;][&quot;arguments&quot;])
        function_response = function_to_call(
            location=function_args.get(&quot;location&quot;),
            unit=function_args.get(&quot;unit&quot;),
        )

        # messages.append(response_message)
        messages.append(
            {&quot;role&quot;: &quot;function&quot;, &quot;name&quot;: function_name, &quot;content&quot;: function_response}
        )
        print(&quot;MESSAGE 2&quot;, messages)
        second_response = openai.ChatCompletion.create(
            model=&quot;gpt-3.5-turbo-0613&quot;,
            messages=messages,
        )
        print(&quot;SECOND RESPONSE&quot;, second_response['choices'][0]['message'].to_dict())
        messages.append(second_response['choices'][0]['message'].to_dict())
        print(&quot;MESSAGE 3&quot;, messages)
        return second_response
</code></pre>
<p>It always runs the function even if I ask the same question</p>
<p>Output:</p>
<pre><code>RESPONSE MSG {
  &quot;role&quot;: &quot;assistant&quot;,
  &quot;content&quot;: null,
  &quot;function_call&quot;: {
    &quot;name&quot;: &quot;get_current_weather&quot;,
    &quot;arguments&quot;: &quot;{\n  \&quot;definition\&quot;: \&quot;Boston, MA\&quot;,\n  \&quot;unit\&quot;: \&quot;celsius\&quot;\n}&quot;
  }
}
IT RAN&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
MESSAGE 2 [{'role': 'user', 'content': 'what is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}]
SECOND RESPONSE {'role': 'assistant', 'content': 'The temperature in Boston is 72 degrees Celsius.'}
MESSAGE 3 [{'role': 'user', 'content': 'what is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degree
s Celsius.'}]
MESSAGE 1 [{'role': 'user', 'content': 'what is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degree
s Celsius.'}, {'role': 'user', 'content': 'what is the temperature in NewYork?'}]
RESPONSE MSG {
  &quot;role&quot;: &quot;assistant&quot;,
  &quot;content&quot;: null,
  &quot;function_call&quot;: {
    &quot;name&quot;: &quot;get_current_weather&quot;,
    &quot;arguments&quot;: &quot;{\&quot;location\&quot;: \&quot;New York\&quot;}&quot;
  }
}
IT RAN&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
MESSAGE 2 [{'role': 'user', 'content': 'what is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degree
s Celsius.'}, {'role': 'user', 'content': 'what is the temperature in NewYork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;locati
on&quot;: &quot;New York&quot;, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}]
SECOND RESPONSE {'role': 'assistant', 'content': 'The temperature in New York is 72 degrees. Please note that I did not specify the temperature unit, as i
t is missing in the response.'}
MESSAGE 3 [{'role': 'user', 'content': 'what is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degree
s Celsius.'}, {'role': 'user', 'content': 'what is the temperature in NewYork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;locati
on&quot;: &quot;New York&quot;, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in New York is 72
 degrees. Please note that I did not specify the temperature unit, as it is missing in the response.'}]
MESSAGE 1 [{'role': 'user', 'content': 'what is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degree
s Celsius.'}, {'role': 'user', 'content': 'what is the temperature in NewYork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;locati
on&quot;: &quot;New York&quot;, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in New York is 72
 degrees. Please note that I did not specify the temperature unit, as it is missing in the response.'}, {'role': 'user', 'content': 'what is the temperatu
re in Boston?'}]
RESPONSE MSG {
  &quot;role&quot;: &quot;assistant&quot;,
  &quot;content&quot;: null,
  &quot;function_call&quot;: {
    &quot;name&quot;: &quot;get_current_weather&quot;,
    &quot;arguments&quot;: &quot;{\&quot;definition\&quot;: \&quot;Boston\&quot;}&quot;
  }
}
IT RAN&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
MESSAGE 2 [{'role': 'user', 'content': 'what is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degree
s Celsius.'}, {'role': 'user', 'content': 'what is the temperature in NewYork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;locati
MESSAGE 3 [{'role': 'user', 'content': 'what is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degree
s Celsius.'}, {'role': 'user', 'content': 'what is the temperature in NewYork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;locati
on&quot;: &quot;New York&quot;, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in New York is 72
 degrees. Please note that I did not specify the temperature unit, as it is missing in the response.'}, {'role': 'user', 'content': 'what is the temperatu
re in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;: null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny
&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degrees. Please note that the unit of temperature is missing in the resp
onse.'}]
MESSAGE 1 [{'role': 'user', 'content': 'What is the temperature in Boston?'}]
RESPONSE MSG {        
  &quot;role&quot;: &quot;assistant&quot;,
  &quot;content&quot;: null,    
  &quot;function_call&quot;: {
    &quot;name&quot;: &quot;get_current_weather&quot;,
    &quot;arguments&quot;: &quot;{\n  \&quot;definition\&quot;: \&quot;Boston, MA\&quot;\n}&quot;
  }
}
IT RAN&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
MESSAGE 2 [{'role': 'user', 'content': 'What is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}]
SECOND RESPONSE {'role': 'assistant', 'content': 'The temperature in Boston is 72 degrees Fahrenheit.'}
MESSAGE 3 [{'role': 'user', 'content': 'What is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degrees Fah
renheit.'}]
MESSAGE 1 [{'role': 'user', 'content': 'What is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degrees Fah
renheit.'}, {'role': 'user', 'content': 'What is the temperature in NewYork?'}]
RESPONSE MSG {
  &quot;role&quot;: &quot;assistant&quot;,
  &quot;content&quot;: null,
  &quot;function_call&quot;: {
    &quot;name&quot;: &quot;get_current_weather&quot;,
    &quot;arguments&quot;: &quot;{\&quot;location\&quot;: \&quot;New York\&quot;, \&quot;unit\&quot;: \&quot;celsius\&quot;}&quot;
  }
}
IT RAN&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
MESSAGE 2 [{'role': 'user', 'content': 'What is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degrees Fah
renheit.'}, {'role': 'user', 'content': 'What is the temperature in NewYork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location
&quot;: &quot;New York&quot;, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}]
SECOND RESPONSE {'role': 'assistant', 'content': 'The temperature in New York is 72 degrees Celsius.'}
MESSAGE 3 [{'role': 'user', 'content': 'What is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degrees Fah
renheit.'}, {'role': 'user', 'content': 'What is the temperature in NewYork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location
&quot;: &quot;New York&quot;, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in New York is
 72 degrees Celsius.'}]
MESSAGE 1 [{'role': 'user', 'content': 'What is the temperature in Boston?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location&quot;:
 null, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: null, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in Boston is 72 degrees Fah
renheit.'}, {'role': 'user', 'content': 'What is the temperature in NewYork?'}, {'role': 'function', 'name': 'get_current_weather', 'content': '{&quot;location
&quot;: &quot;New York&quot;, &quot;temperature&quot;: &quot;72&quot;, &quot;unit&quot;: &quot;celsius&quot;, &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;]}'}, {'role': 'assistant', 'content': 'The temperature in New York is
 72 degrees Celsius.'}, {'role': 'user', 'content': 'What is the temperature in Boston?'}]
RESPONSE MSG {
  &quot;role&quot;: &quot;assistant&quot;,
  &quot;content&quot;: null,
  &quot;function_call&quot;: {
    &quot;name&quot;: &quot;get_current_weather&quot;,
    &quot;arguments&quot;: &quot;{\&quot;location\&quot;: \&quot;Boston\&quot;}&quot;
  }
is 72 degrees.'}]
</code></pre>
","gpt-4"
"76724421","Unable to read data as Llama index Documents","2023-07-19 18:51:48","","0","1348","<chatgpt-api><large-language-model><llama-index><gpt-4>","<p>I'm currently working with llama index trying to parse a column of my pandas dataframe as a Document object with llama index with the final goal of fitting my data into an LLM (I'm using gpt-4-32k). Does anyone know how to do this without explicitly converting to an unstructured datasource (ie. a doc) which seems counterintuitive?</p>
<pre><code>    #First I save my data into an array (of strings)
text_list = concatenated_text_array = uniqueness_data['concatenated_text'].to_numpy().flatten()
#Then I try to cast each element to the Document object
documents = [Document(t) for t in text_list]


#and receive this error:
    documents = [Document(t) for t in text_list]
                 ^^^^^^^^^^^
  File &quot;pydantic/main.py&quot;, line 332, in pydantic.main.BaseModel.__init__
TypeError: __init__() takes exactly 1 positional argument (2 given)
</code></pre>
","gpt-4"
"76653119","OpenAI GPT-4 API error: ""The model: gpt-4 does not exist""","2023-07-10 11:08:18","","2","15625","<openai-api><php-curl><gpt-4>","<p>I have code as below, it works perfect with model <code>gpt-3.5-turbo</code> but not with <code>gpt-4</code>:</p>
<pre><code>$your_prompt = &quot;Prompt....&quot;

// GPT-4 API call
$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, 'https://api.openai.com/v1/chat/completions');
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);
curl_setopt($ch, CURLOPT_POST, 1);

$headers = array();
$headers[] = 'Content-Type: application/json';
$headers[] = 'Authorization: Bearer [API-KEY]';
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);

$data = json_encode(array('model' =&gt; 'gpt-3.5-turbo', 
                          'messages' =&gt; array(
                                            array('role' =&gt; 'system', 'content' =&gt; 'Your system message here'), 
                                            array('role' =&gt; 'user', 'content' =&gt; $your_prompt))));

curl_setopt($ch, CURLOPT_POSTFIELDS, $data);

$result = curl_exec($ch);
echo $result.&quot;&lt;br&gt;&quot;;

if (curl_errno($ch)) {
    echo 'Curl Error:' . curl_error($ch) . &quot;\n&lt;br&gt;&quot;;
} else {
    echo &quot;API call successful.\n&lt;br&gt;&quot;;
    echo &quot;API response: \n&lt;br&gt;&quot;;
    print_r(json_decode($result, true));
    echo &quot;\n&quot;;
}
curl_close($ch);

$response = json_decode($result, true);
$text = $response['choices'][0]['message']['content'];

echo &quot;OutputGPT&quot; . $text . &quot;\n&lt;br&gt;&quot;;
</code></pre>
<p>When I'm using <code>gpt-4</code> I get:</p>
<pre><code>Processing prompt: what is a wether today in Warsaw Poland
{ &quot;error&quot;: { &quot;message&quot;: &quot;The model: `gpt-4` does not exist&quot;, &quot;type&quot;: &quot;invalid_request_error&quot;, &quot;param&quot;: null, &quot;code&quot;: &quot;model_not_found&quot; } }
API call successful.
API response:
Array ( [error] =&gt; Array ( [message] =&gt; The model: `gpt-4` does not exist [type] =&gt; invalid_request_error [param] =&gt; [code] =&gt; model_not_found ) ) OutputGPT
</code></pre>
<p>I have tried also with others models like <code>gpt-3.5-turbo-0613</code> and it works to, but not for <code>gpt-4</code> as well as for <code>gpt-4-0613</code>. Nevertheless it should based on <a href=""https://platform.openai.com/docs/models/gpt-4"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/models/gpt-4</a></p>
","gpt-4"
"76627456","Error when Running Vicuna's FastChat Model without GPU","2023-07-06 09:36:27","","1","490","<machine-learning><pytorch><gpt-4>","<p>I wish to use the Vicuna open source model to train my dataset.</p>
<p>I don't have a GPU in my computer, so I wanted to use their RESTful API Server. I used Windows PowerShell for the commands below.</p>
<p>According to their explanation (<a href=""https://github.com/lm-sys/FastChat/blob/main/docs/openai_api.md"" rel=""nofollow noreferrer"">https://github.com/lm-sys/FastChat/blob/main/docs/openai_api.md</a>)</p>
<p>First, I launched the command</p>
<pre><code>&gt; python3 -m fastchat.serve.controller
</code></pre>
<p>Then, it opened a localhost for me. I opened it in my browser and it displayed the following message:</p>
<pre><code>&gt; {&quot;detail&quot;:&quot;Not Found&quot;}.
</code></pre>
<p>Next, I opened a new PowerShell window and ran their second command:</p>
<pre><code>&gt; python3 -m fastchat.serve.model_worker --model-path lmsys/vicuna-7b-v1.3
</code></pre>
<p>However, I encountered the following error:</p>
<pre><code>AssertionError: Torch not compiled with CUDA enabled
</code></pre>
<p>Does this error occur because I do not have a GPU in my computer?</p>
","gpt-4"
"76624914","How do I change the default 4 documents that LangChain returns?","2023-07-06 01:11:51","","7","2844","<openai-api><langchain><gpt-4>","<p>I have the following code that implements LangChain + ChatGPT to answer questions from given data:</p>
<pre><code>import { PineconeStore } from 'langchain/vectorstores/pinecone';
import { ConversationalRetrievalQAChain } from 'langchain/chains';

const CONDENSE_PROMPT = `Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone question:`;

const QA_PROMPT = `You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say you don't know. DO NOT try to make up an answer.
If the question is not related to the context, politely respond that you are tuned to only answer questions that are related to the context. Always answer in spanish.

{context}

Question: {question}
Helpful answer in markdown:`;

export const makeChain = (vectorstore: PineconeStore) =&gt; {
  const model = new OpenAI({
    temperature: 0.9, // increase temepreature to get more creative answers
    modelName: 'gpt-4', //change this to gpt-4 if you have access
  });

  const chain = ConversationalRetrievalQAChain.fromLLM(
    model,
    vectorstore.asRetriever(),
    {
      qaTemplate: QA_PROMPT,
      questionGeneratorTemplate: CONDENSE_PROMPT,
      returnSourceDocuments: false, //The number of source documents returned is 4 by default
    },
  );
  return chain;
};
</code></pre>
<p>The issue I'm dealing with is that it always returns only 4 documents (in my case these are json files but I have stored 30). I can see that even in the LangChain documentation page the use a similar bot and also returns only sources.</p>
<p>The comment in my code that says &quot;the number... is 4 by default&quot; makes me think that there's a way to increase this value.</p>
<p>I've tried Bard and ChatGPT to find a solution but the code they suggest doesn't work.</p>
","gpt-4"
"76608259","OpenAI GPT-4 API: Why does gpt-4-0613 hallucinate (make up) function parameters?","2023-07-03 21:43:27","","4","2531","<openai-api><chatgpt-api><gpt-4>","<p>I'm using the <code>gpt-4-0613</code> model, with a single function, and some custom data in the system prompt.</p>
<p>If the function is triggered very early in the chat, within the first two requests, it functions just fine, and the API asks the user for the information required to call the function.</p>
<p>However, if the function is called later in the conversation, let's say question 5, the API will just make up answers and send back the function call.</p>
<p>How can I stop the AI from making up answers? There is no way for the API to get these values from the conversation context. They are all 100% made up.</p>
<pre><code>completion = openai.ChatCompletion.create(
    model='gpt-4-0613',
    messages=prompts,
    functions=[
    {
        &quot;name&quot;: &quot;fill_form&quot;,
        &quot;description&quot;: &quot;Helps the user create an XYZ Report&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;name&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;the full name of the person issuing this report&quot;
                },
                &quot;zip&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;the 5 digit zip code of the address&quot;
                },
                &quot;address&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;the street address, only the street and not the city, state or zip&quot;
                },
                &quot;year_end&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;the full four digit year of the fiscal year&quot;
                },
            },
            &quot;required&quot;: [&quot;name&quot;, &quot;address&quot;, &quot;year_end&quot;, &quot;zip&quot;]
        }
    }],
)
</code></pre>
<p>I've tried with and without the</p>
<pre><code>function_call='auto'
</code></pre>
<p>option with no affect.</p>
<p>Thank you for any help.</p>
<p>The API should always ask the users for the values of the function and never make them up.</p>
","gpt-4"
"76543141","GPT-4 doesn't follow output format instruction occasionally","2023-06-23 19:58:54","","0","832","<openai-api><gpt-4>","<p>I am writing a custom wrapper for OpenAI GPT-4 API. I do the prompting similarly to the ReAct model (Thought, Action, Observation, Final Answer). This is my output format instruction for the agent scratchpad</p>
<pre><code>Populate the scratchpad (delimited by the triple quote) to guide yourself toward the answer. For the scratchpad, always choose to follow only one of the situations listed below (inside the triple curly braces) and then end your answer. You CAN NOT populate the Observation field yourself. Always include word &quot;End Answer&quot; at the end of your answer\n\
{{{\
Situation 1: You can ONLY choose ONE action at a time. When you decide you need to use a tool (based on the observations and input question), please follow this format to answer the question:\n\
Thought: you should always think about what to do.\n\
Action: the action to take, should always be one of [${tools.map(
(toolDocumentation) =&gt; toolDocumentation.name
)}].\n\
Action Input: the input to the action. Should list the input parameter as this format suggest: &quot;parameter1&quot;, &quot;parameter&quot;, ...]\n\
End Answer\n\n\
[End Answer Here]

Situation 2: When you don't need to use a tool, please follow this format to answer the question: \n\
Thought: you should always think about what to do.\n\
Final Answer: Provide your final answer for the input question from the input question or the Observation (if it exists).\n\
End Answer\n\n\
[End Answer Here]
}}}\n
</code></pre>
<p>The idea is to tell GPT-4 to always include &quot;End Answer&quot; at the end of its response so I can parse its output with regex. However, approximately 1/10 times, it fails to include that keyword and messes up my output parser. How can I improve my prompt to keep the result more consistent?</p>
<p>I try setting the temperature to 0, adding many delimeters as OpenAI suggests. But the result keeps being inconsistent</p>
","gpt-4"
"76535292","I am not getting exact response from my api as i am getting from chat gpt","2023-06-22 20:04:40","","0","1189","<reactjs><artificial-intelligence><openai-api><chatgpt-api><gpt-4>","<p>Can someone explain  why I am not getting good enough response. My 3.5 api is generating content that is good enough as gpt's response. my app is about helping recruiters to refine their job posts. but its not working fine. How can I improve the response?</p>
<pre><code>import logo from './logo.svg';
import './App.css';
import React, { useEffect, useState } from 'react';
import axios from 'axios';

function App() {
  const [apiKey, setApiKey] = useState('');

  useEffect(() =&gt; {
    fetch('https://server-khaki-kappa.vercel.app/api/key')
      .then(response =&gt; response.json())
      .then(data =&gt; setApiKey(data.apiKey))
      .catch(error =&gt; console.error(error));
  }, []);

  const headers = {
    'Content-Type': 'application/json',
    Authorization: `Bearer ${apiKey}`,
  };

  const personas = [
    'Lou Adler',
    'Stacy Donovan Zapa',
    'Johnny Campbell',
    'Greg Savage',
    'Maisha Cannon',
    'Glen Cathey'
  ];

  const styles = [
    'Captivating',
    'Enticing',
    'Witty',
    'Appealing',
    'Engaging',
    'Impactful',
    'Dynamic',
    'Exciting',
    'Professional'
  ];

  const [userInput, setUserInput] = useState({
    system: '',
    user: '',
    assistant: '',
    prompt: '',
    model: 'gpt-3.5-turbo-16k',
    persona: 'Lou Adler',
    style: 'Captivating'
  });

  const [assistantResponse, setAssistantResponse] = useState('');
  const [loading, setLoading] = useState(false);

  const handleUserInput = (e) =&gt; {
    setUserInput(prevState =&gt; ({
      ...prevState,
      [e.target.name]: e.target.value,
    }));
  };

  const sendUserInput = async () =&gt; {
    setLoading(true);

    const data = {
      model: userInput.model,
      messages: [
        {
          role: 'system',
          content: `You are an AI language model trained to assist recruiters in refining job posts and your name is recruiterGPT. do not generate a response if the job description and some requirements are not given, and ask for them. It assists users in generating human-like text based on the given instructions and context. think properly and take your time before answering. Here are the instructions: Assistant, please generate a ${userInput.style.toLowerCase()} and vibrant job description for the position. The goal is to rewrite the existing job description, emphasizing the benefits and opportunities associated with the role. Take on the persona of ${userInput.persona}, a recruitment expert, and create the content in a ${userInput.style.toLowerCase()}  style that will attract potential candidates. Present the information in a compelling manner while keeping the user's requirements in mind. Even if certain points are not present in the job description, mention them and create enthusiasm around them. These Points include: 1- Offer a Competitive Compensation and Benefits.
          2- Vibrant and collaborative team,
          3- Professional Development Opportunities.
          4- Work-Life Balance.
          5- Offering Challenging and Meaningful Work.
          6- Become part of our family. 7- Career Development Plan. Prioritize communicating what's in it for them. Emphasize more on benefits for them and highlight the benefits and gains they can expect from the job.Also write about the essential requirements and qualifications needed in detail. First emphasize on tonality and benefits and then generate refined requirements. Thank you!.
`
        },
        {
          role: 'user',
          content: `Take the persona of ${userInput.persona} and use a ${userInput.style.toLowerCase()} tonality when rewriting the following Job Description. In the job description emphasize what‚Äôs in it for them.First include Career Development, training and growth opportunities, work-life balance, competitive salary, challenging and meaningful work and a vibrant and collaborative team. More of the content should be around these points infact 68% of you response should be around benefits and what an employee can get from us. emphasize less on the requirements, but explain them after describing benefits. Display response in a Job Description format and include a few of the main responsibilities. Generate content no more than 3500 words, But more than 1000 words. ${userInput.prompt}. Note: If job description is not given in this prompt, ask for it and do not generate response until a job description is given by the user.`
        }
      ],
      temperature: 0.5,
      max_tokens: 2049,
    }; 

    try {
      const response = await axios.post(
        'https://api.openai.com/v1/chat/completions',
        data,
        { headers }
      );
      const { choices } = response.data;
      const assistantResponse = choices[0].message.content;
      setLoading(false);
      setAssistantResponse(assistantResponse);
    } catch (error) {
      console.error('An error occurred:', error.message);
    }
  };

  const formatAssistantResponse = (response) =&gt; {
    const paragraphs = response.split('\n\n');

    const formattedResponse = paragraphs.map((paragraph, index) =&gt; (
      &lt;p key={index} className=&quot;text-left mb-4&quot;&gt;
        {paragraph}
      &lt;/p&gt;
    ));

    return formattedResponse;
  };

  return (
    &lt;div className=&quot;container mx-auto py-8&quot;&gt;
      &lt;h1 className=&quot;text-2xl font-bold mb-4&quot;&gt;Chat:&lt;/h1&gt;
      {loading ? (
        &lt;&gt;
          &lt;h1 className=&quot;spinner&quot;&gt;&lt;/h1&gt;
          &lt;p&gt;Dear user, Please be patient RecruitGpt is refining your post to its best....&lt;/p&gt;
        &lt;/&gt;
      ) : (
        &lt;&gt;
          &lt;div className=&quot;bg-gray-100 p-4 mb-4&quot;&gt;
            {formatAssistantResponse(assistantResponse)}
          &lt;/div&gt;
        &lt;/&gt;
      )}

      &lt;section className=&quot;m-6&quot;&gt;
        &lt;div className=&quot;mb-4&quot;&gt;
          &lt;label className=&quot;block mb-2&quot;&gt;
            Model:
            &lt;select
              className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
              name=&quot;model&quot;
              value={userInput.model}
              onChange={handleUserInput}
            &gt;
              &lt;option value=&quot;gpt-3.5-turbo&quot;&gt;gpt-3.5-turbo&lt;/option&gt;
              &lt;option value=&quot;gpt-3.5-turbo-16k&quot;&gt;gpt-3.5-turbo-16k&lt;/option&gt;
              &lt;option value=&quot;gpt-3.5-turbo-0613&quot;&gt;gpt-3.5-turbo-0613&lt;/option&gt;
              &lt;option value=&quot;gpt-3.5-turbo-16k-0613&quot;&gt;gpt-3.5-turbo-16k-0613&lt;/option&gt;
            &lt;/select&gt;
          &lt;/label&gt;
        &lt;/div&gt;
        &lt;div className=&quot;mb-4&quot;&gt;
          &lt;label className=&quot;block mb-2&quot;&gt;
            Persona:
            &lt;select
              className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
              name=&quot;persona&quot;
              value={userInput.persona}
              onChange={handleUserInput}
            &gt;
              {personas.map((persona, index) =&gt; (
                &lt;option key={index} value={persona}&gt;{persona}&lt;/option&gt;
              ))}
            &lt;/select&gt;
          &lt;/label&gt;
        &lt;/div&gt;
        &lt;div className=&quot;mb-4&quot;&gt;
          &lt;label className=&quot;block mb-2&quot;&gt;
            Style:
            &lt;select
              className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
              name=&quot;style&quot;
              value={userInput.style}
              onChange={handleUserInput}
            &gt;
              {styles.map((style, index) =&gt; (
                &lt;option key={index} value={style}&gt;{style}&lt;/option&gt;
              ))}
            &lt;/select&gt;
          &lt;/label&gt;
        &lt;/div&gt;
        &lt;div className=&quot;mb-4&quot;&gt;
          &lt;label className=&quot;block mb-2&quot;&gt;
            Prompt:
            &lt;textarea
              name=&quot;prompt&quot;
              className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
              type=&quot;text&quot;
              rows={4}
              onChange={handleUserInput}
            /&gt;
          &lt;/label&gt;
        &lt;/div&gt;
      &lt;/section&gt;

      &lt;button
        className=&quot;bg-blue-500 hover:bg-blue-600 text-white font-bold py-2 px-4 rounded&quot;
        onClick={sendUserInput}
      &gt;
        Send
      &lt;/button&gt;
    &lt;/div&gt;
  );
}

export default App;
</code></pre>
<p>can someone tell me how i can optimize it.</p>
","gpt-4"
"76526344","How to queue API calls to Azure OpenAI service (with a token per minute rate limit) the most efficiently?","2023-06-21 18:57:51","","4","1237","<azure><message-queue><azureservicebus><azure-openai><gpt-4>","<p>How can we implement an efficient queue using Azure serverless technologies (e.g. Azure Servicebus) to call Azure OpenAI service concurrently but guarantee earlier messages are processed first?</p>
<p>The complexity is that the rate limit is not based on X requests per minute based on a 'rolling window'. But instead it is about tokens per minute and Azure implements a 1 minute timer (which we don't know when it resets). Here is an explanation of the rate limit policy:
<a href=""https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/quota#understanding-rate-limits"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/quota#understanding-rate-limits</a></p>
<p>Assuming the following &quot;queue&quot; and a rate limit of 10.000 TPM:</p>
<ul>
<li>Request 1) 2000 expected tokens</li>
<li>Request 2) 5000 expected tokens</li>
<li>Request 3) 5000 expected tokens</li>
<li>Request 4) 2000 expected tokens</li>
<li>Request 5) 7000 expected tokens</li>
</ul>
<p>We would like the 'queue' to concurrently process request 1 and 2. 'Realize' that request 3 will overshoot the token limit and 'schedule' one minute of waiting, then take on request 3 &amp; 4 concurrently, schedule one minute of waiting and process request 5.</p>
<p>In theory we don't need to 'schedule' and can just hit the rate limit with a retry policy (maybe better than scheduling since we don't know the moment the timer resets and the exact tokens that Azure estimates the request will cost). But in this case how do we make sure we don't end up with a race condition where request 3,4,5 all fail and retry and 5 gets through before 3?</p>
<p>In theory an even more intelligent solution would process 1,2,4 in parallel. Wait a minute and then process 3, wait a minute and then process 5. Where 4 is allowed to go before 3 only because it fits within the minute's limit which would otherwise be 'wasted'.</p>
","gpt-4"
"76514041","my gpt 3.5 turbo api is not giving good enough response as i can get from chat gpt","2023-06-20 11:02:18","","2","2305","<reactjs><chat><openai-api><chatgpt-api><gpt-4>","<p>So i have implemented chat gpt 3.5 turbo API in my react app. so my app is basically like an assistant to a recruiter. so a recruiter gives a sample job post to the app and it send this post to chat gpt to craft it. now i have different personas to be copied in the response i am also instructing it to follow these personas and styles. in this example persona of Lou Adler and style is enticing. But the problem is when i give the problem to cht gpt it is givng me good response but in case of my API in my app the response is not good enough. can someone tell me about the problem.</p>
<p>below is my code and note that there are two user roles. i do not understand this. where will the actual propt by user will be? can you kindly elaborate this problem.</p>
<pre><code>import logo from './logo.svg';
import './App.css';
import React, { useEffect, useState } from 'react';
import axios from 'axios';


function App() {

 // get api key from server
  const [apiKey, setApiKey] = useState('');

  useEffect(() =&gt; {
    fetch('https://server-khaki-kappa.vercel.app/api/key')
      .then(response =&gt; response.json())
      .then(data =&gt; setApiKey(data.apiKey))
      .catch(error =&gt; console.error(error));
  }, []);
  const headers = {
    'Content-Type': 'application/json',
    Authorization: `Bearer ${apiKey}`,
  };

  const [userInput, setUserInput] = useState({
    system: '',
    user: '',
    assistant: '',
    prompt: '',
    model: 'gpt-3.5-turbo-16k',
  });

  console.log(userInput)
  const [assistantResponse, setAssistantResponse] = useState('');
  const [loading, setLoading] = useState(false);

  const handleUserInput = (e) =&gt; {
    console.log('e.target',e.target.value);
    setUserInput((prevState) =&gt; ({
      ...prevState,
      [e.target.name]: e.target.value,
    }));
  };

  const sendUserInput = async () =&gt; {
    setLoading(true);

    const data = {
      model: userInput.model,
      messages: [
        {
          role: 'system',
          content: 
          // userInput.system
          'You are an AI language model trained to assist recruiters in refining job posts. Please provide Enticing content, language, and information in the job posts. Number of words in the response should be equal to or more than the job post that a recruiter is giving to you. you strictly have to follow the same persona given to you. also you have to follow the job post that recruiter will give you. you will make it more enticing and follow the persona of Lou Adler'
             },
        {
          role: 'user',
          content: 
          userInput.user 
          // 'When rewriting the job description, use a language model acting as a recruitment expert or consultant. In this context, take on the persona of Lou Adler. Your role is to be enticing with the reader and emphasize the benefits and opportunities associated with the job position, while presenting the information in an enticing manner.'
            },
        {
          role: 'assistant',
          content:
            // userInput.assistant 
            'You are an AI assistant trained to help recruiters refine their job posts. You can provide suggestions, make the language more enticing, and ensure all necessary information is included. If any details are missing or ambiguous, please ask for more information to provide the best possible suggestions. Take your time to answer the best.'
             },
        {
          role: 'user',
          content:
            userInput.prompt 
            },
      ],
      temperature: 0.2
    };

    try {
      const response = await axios.post(
        'https://api.openai.com/v1/chat/completions',
        data,
        { headers }
      );
      const { choices } = response.data;
      const assistantResponse = choices[0].message.content;
      setLoading(false);
      setAssistantResponse(assistantResponse);
    } catch (error) {
      console.error('An error occurred:', error.message);
    }
  };

  const formatAssistantResponse = (response) =&gt; {
    const paragraphs = response.split('\n\n');

    const formattedResponse = paragraphs.map((paragraph, index) =&gt; (
      &lt;p key={index} className=&quot;text-left mb-4&quot;&gt;
        {paragraph}
      &lt;/p&gt;
    ));

    return formattedResponse;
  };

  return (
    &lt;div className=&quot;container mx-auto py-8&quot;&gt;
    &lt;h1 className=&quot;text-2xl font-bold mb-4&quot;&gt;Chat :&lt;/h1&gt;
    {loading ? (
      &lt;&gt;
        &lt;h1 className=&quot;spinner&quot;&gt;&lt;/h1&gt;
      &lt;/&gt;
    ) : (
      &lt;&gt;
        &lt;div className=&quot;bg-gray-100 p-4 mb-4&quot;&gt;
          {formatAssistantResponse(assistantResponse)}
        &lt;/div&gt;
      &lt;/&gt;
    )}

    &lt;section className='m-6'&gt;
      
    &lt;div className=&quot;mb-4 &quot;&gt;
      &lt;label className=&quot;block mb-2&quot;&gt;
        Model:
        &lt;select
          className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
          name=&quot;model&quot;
          value={userInput.model}
          onChange={handleUserInput}
        &gt;
          &lt;option value=&quot;gpt-3.5-turbo&quot;&gt;gpt-3.5-turbo&lt;/option&gt;
          &lt;option value=&quot;gpt-3.5-turbo-16k&quot;&gt;gpt-3.5-turbo-16k&lt;/option&gt;
          &lt;option value=&quot;gpt-3.5-turbo-0613&quot;&gt;gpt-3.5-turbo-0613&lt;/option&gt;
          &lt;option value=&quot;gpt-3.5-turbo-16k-0613&quot;&gt;gpt-3.5-turbo-16k-0613&lt;/option&gt;
          {/* &lt;option value=&quot;text-davinci-003&quot;&gt;text-davinci-003&lt;/option&gt; */}
        &lt;/select&gt;
      &lt;/label&gt;
    &lt;/div&gt;
    &lt;div className=&quot;mb-4&quot;&gt;
      {/* &lt;label className=&quot;block mb-2&quot;&gt;
        System Role:
        &lt;textarea
           className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
          type=&quot;text&quot;
          rows={4}
          name=&quot;system&quot;
          value={userInput.system}
          onChange={handleUserInput}
        /&gt;
      &lt;/label&gt; */}
    &lt;/div&gt;
    &lt;div className=&quot;mb-4&quot;&gt;
&lt;label className=&quot;block mb-2&quot;&gt;
  User Role:
  &lt;textarea
     className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
    rows={4}
    name=&quot;user&quot;
    value={userInput.user}
    onChange={handleUserInput}
  /&gt;
&lt;/label&gt;
&lt;/div&gt;

    &lt;div className=&quot;mb-4&quot;&gt;
      {/* &lt;label className=&quot;block mb-2&quot;&gt;
        Assistant Role:
        &lt;textarea
      
     
        className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
          type=&quot;text&quot;
          rows={4}
          name=&quot;assistant&quot;
          value={userInput.assistant}
          
          onChange={handleUserInput}
        /&gt;
      &lt;/label&gt; */}
    &lt;/div&gt;
    &lt;div className=&quot;mb-4&quot;&gt;
      &lt;label className=&quot;block mb-2&quot;&gt;
        Prompt:
        &lt;textarea
          className=&quot;border border-gray-300 rounded px-4 py-2 w-full&quot;
          name='prompt'
          type=&quot;text&quot;
          rows={4}
        onChange={handleUserInput}
        /&gt;
      &lt;/label&gt;
    &lt;/div&gt;
   
    &lt;/section&gt;
    &lt;button
      className=&quot;bg-blue-500 hover:bg-blue-600 text-white font-bold py-2 px-4 rounded&quot;
      onClick={sendUserInput}
    &gt;
      Send
    &lt;/button&gt;
  &lt;/div&gt;
  );
}

export default App;
</code></pre>
","gpt-4"
"76463184","Using OpenAI LLMs for classification. Asking for classification vs. asking for probabilities","2023-06-13 08:49:01","","3","833","<text-classification><openai-api><multilabel-classification><gpt-4><large-language-model>","<p>I'm using LLMs for classifying products into specific categories. Multi-Class.</p>
<ol>
<li><p>One way to do it would it to ask if it's a yes/no for a specific category and loop through the categories.</p>
</li>
<li><p>Another way would be to ask for a probability that that certain product belongs to one of those classes.</p>
</li>
</ol>
<p>The second option allows me to adjust the prediction thresholds in &quot;post&quot; and over/under-classify certain classes.</p>
<p>However, The word on the street is that RLHF-trained OpenAI models such as <code>gpt-3.5-turbo</code> and <code>gpt-4</code> are weak at guessing probabilities relative to text completion models like <code>text-davinci-003</code> because RLHF training makes the model &quot;think&quot; more like a human (bad at guessing probabilities).</p>
<p>Are there any literature I can read up on/ should know about? Before I go ahead and run a 100 tests.</p>
<p>I've not tried anything as of yet given that testing is time/cost intensive. And would like a baseline understanding of how to tackle the problem before starting.</p>
","gpt-4"
"76451232","Why does this bundled app not work when the python script does work?","2023-06-11 15:43:09","","0","71","<python><printing><pyinstaller><barcode><gpt-4>","<p>I have essentially zero coding experience. I'm using GPT4 prompts to help me put together a simple print utility that generates a barcode and adds 1 to the last code printed in order for us to organize our inventory. The script works great when run with IDLE but when it's packaged it does not generate a barcode image and gives an error in cmd. GPT has tried numerous ways around this always resulting in the same error.</p>
<p>It has concluded that pyinstaller must be incompatible with the barcode pip(?) Here's the full script:</p>
<pre><code>import tkinter as tk
from barcode import Code39
from barcode.writer import ImageWriter
from PIL import Image, ImageFont, ImageDraw
import sys
import os
import time
import threading

def resource_path(relative_path):
    try:
        base_path = sys._MEIPASS
    except Exception:
        base_path = os.path.abspath(&quot;.&quot;)
    return os.path.join(base_path, relative_path)

font_path = resource_path('resources/ARLRDBD.ttf')


def get_next_number(filename):
    try:
        with open(filename, 'r') as f:
            lines = f.read().splitlines()  # splitlines method strips newline characters
        last_number = int(lines[-1])
    except FileNotFoundError:
        last_number = 54999  # start from 55000
    next_number = last_number + 1
    with open(filename, 'a') as f:
        f.write(f&quot;{next_number}\n&quot;)  # append the next number to the file
    return last_number, next_number

def generate_barcode(number):
    code39 = Code39(str(number), writer=ImageWriter(), add_checksum=False)

    # Create the directory if it does not exist
    directory = 'barcodes'
    if not os.path.exists(directory):
        os.makedirs(directory)

    filename = os.path.join(directory, f&quot;barcode_{number}.jpeg&quot;)  # Save the file in the directory

    barcode_img = code39.save(filename)

    img = Image.open(barcode_img)

    width_mm = 62
    height_mm = 29
    width_pixel = int(width_mm * (300 / 25.4))  
    height_pixel = int(height_mm * (300 / 25.4)) 
    img_resized = img.resize((width_pixel, height_pixel))

    draw = ImageDraw.Draw(img_resized)
    font = ImageFont.load_default()

    text_bbox = draw.textbbox((0, 0), str(number), font=font)
    text_width = text_bbox[2]
    x_position = (img_resized.width - text_width) / 2

    draw.text((x_position, img_resized.height - 50), str(number), font=font, fill=255)

    img_resized.save(barcode_img)
    return barcode_img


def print_file(barcode_img):
    time.sleep(1)  # let the system recognize the new file
    os.startfile(barcode_img, 'print')
    threading.Timer(60, os.remove, [barcode_img]).start()  # delay before deleting the file

def main():
    last_number, next_number = get_next_number(&quot;last_number.txt&quot;) 
    barcode_img = generate_barcode(next_number)
    print_file(barcode_img)

    # Adding 1 to the numbers displayed in the labels
    last_label.config(text=&quot;Last: &quot; + str(last_number + 1))
    next_label.config(text=&quot;Next: &quot; + str(next_number + 1))


root = tk.Tk()
root.geometry('400x400')  # Set the window size to 400x400
root.title(&quot;Auction Barcode Print Button&quot;)  # Set the window title

# Create the labels with default text
last_label = tk.Label(root, text=&quot;Last: &quot;, font=('Arial', 14))
next_label = tk.Label(root, text=&quot;Next: &quot;, font=('Arial', 14))

# Place the labels in the window
last_label.pack(pady=(50, 10))
next_label.pack(pady=(10, 20))

button_frame = tk.Frame(root, width=200, height=200)  # Frame to hold the button
button_frame.pack_propagate(0)  # prevents the frame to shrink
button_frame.pack(pady=(20,20))

button = tk.Button(button_frame, text=&quot;Print Barcode&quot;, command=main, relief=tk.RAISED)  
button.pack(fill=tk.BOTH, expand=1)  # Button fills the entire frame

root.mainloop()
</code></pre>
<p>And here's the error from cmd when this script is packaged and run:</p>
<pre><code>Exception in Tkinter callback
Traceback (most recent call last):
  File &quot;tkinter\__init__.py&quot;, line 1948, in __call__
  File &quot;Auction_Label_4.py&quot;, line 72, in main
  File &quot;Auction_Label_4.py&quot;, line 42, in generate_barcode
  File &quot;barcode\base.py&quot;, line 65, in save
  File &quot;barcode\codex.py&quot;, line 74, in render
  File &quot;barcode\base.py&quot;, line 105, in render
  File &quot;barcode\writer.py&quot;, line 265, in render
  File &quot;barcode\writer.py&quot;, line 439, in _paint_text
  File &quot;PIL\ImageFont.py&quot;, line 1008, in truetype
  File &quot;PIL\ImageFont.py&quot;, line 1005, in freetype
  File &quot;PIL\ImageFont.py&quot;, line 255, in __init__
OSError: cannot open resource
</code></pre>
<p>The script does however generate the 'last_number' txt file and the 'barcodes' folder, it just doesn't generate a barcode.</p>
<p>Thank you for giving this post a look.</p>
","gpt-4"
"76424390","How to change the QA_PROMPT for my own usecase?","2023-06-07 14:25:46","","0","235","<prompt><openai-api><chain><langchain><gpt-4>","<h2><a href=""https://i.sstatic.net/MdCGs.png"" rel=""nofollow noreferrer"">chain</a></h2>
<h2><a href=""https://i.sstatic.net/3CFIa.png"" rel=""nofollow noreferrer"">I was following this description.</a></h2>
<p>I can't understand what QA_PROMPT means, and how I can change it to my own usecase.
I checked my Pinecone index but I can't find anything about QA_PROMPT.
What should I do? Please help me.</p>
","gpt-4"
"76421921","Using GPT 4 or GPT 3.5 with SQL Database Agent throws OutputParserException: Could not parse LLM output:","2023-06-07 09:40:43","78176065","5","3551","<openai-api><gpt-3><langchain><gpt-4>","<p>I am using the <a href=""https://python.langchain.com/en/latest/modules/agents/toolkits/examples/sql_database.html"" rel=""nofollow noreferrer"">SQL Database Agent</a> to query a postgres database. I want to use gpt 4 or gpt 3.5 models in the OpenAI llm passed to the agent, but it says I must use ChatOpenAI. Using ChatOpenAI throws parsing errors.</p>
<p>The reason for wanting to switch models is reduced cost, better performance and most importantly - token limit. The max token size is 4k for 'text-davinci-003' and I need at least double that.</p>
<p>Here is my code</p>
<pre><code>from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain.sql_database import SQLDatabase
from langchain.agents import create_sql_agent
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI
import os

os.environ[&quot;OPENAI_API_KEY&quot;] = &quot;&quot;
db = SQLDatabase.from_uri(
    &quot;postgresql://&lt;my-db-uri&gt;&quot;,
    engine_args={
        &quot;connect_args&quot;: {&quot;sslmode&quot;: &quot;require&quot;},
    },
)

llm = ChatOpenAI(model_name=&quot;gpt-3.5-turbo&quot;)
toolkit = SQLDatabaseToolkit(db=db, llm=llm)

agent_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=True,
)

agent_executor.run(&quot;list the tables in the db. Give the answer in a table json format.&quot;)
</code></pre>
<p>When I do, it throws an error in the chain midway saying</p>
<pre><code>&gt; Entering new AgentExecutor chain...
Traceback (most recent call last):
  File &quot;/home/ramlah/Documents/projects/langchain-test/sql.py&quot;, line 96, in &lt;module&gt;
    agent_executor.run(&quot;list the tables in the db. Give the answer in a table json format.&quot;)
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/chains/base.py&quot;, line 236, in run
    return self(args[0], callbacks=callbacks)[self.output_keys[0]]
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/chains/base.py&quot;, line 140, in __call__
    raise e
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/chains/base.py&quot;, line 134, in __call__
    self._call(inputs, run_manager=run_manager)
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 953, in _call
    next_step_output = self._take_next_step(
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 773, in _take_next_step
    raise e
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 762, in _take_next_step
    output = self.agent.plan(
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/agent.py&quot;, line 444, in plan
    return self.output_parser.parse(full_output)
  File &quot;/home/ramlah/Documents/projects/langchain/langchain/agents/mrkl/output_parser.py&quot;, line 51, in parse
    raise OutputParserException(
langchain.schema.OutputParserException: Could not parse LLM output: `Action: list_tables_sql_db, ''`
</code></pre>
<p>Please help. Thanks!</p>
<p><strong>Update</strong>
The recent updates to langchain version <code>0.0.215</code> seem to have fixed this issue, for me at least.</p>
","gpt-4"
"76413465","How to fune-tune and deploy ChatGPT on Cloud?","2023-06-06 09:59:06","","0","157","<openai-api><gpt-3><chatgpt-api><fine-tuning><gpt-4>","<p>I know - how to fine-tune the ChatGPT. However, I not able to find out - How we can deploy the fine-tuned model in our server/cloud?
Can you anyone please help me with these?</p>
<p>I've created the fine-tune model of ChatGPT. But I'm not getting - how to that model can be in my system/server/cloud?</p>
","gpt-4"
"76405967","How come azure openai models are faster than openai models?","2023-06-05 11:28:11","","1","1690","<azure-cognitive-services><openai-api><gpt-4>","<p>I recently tried gpt-4 model with API calls to azure and openai. Noticed that time taken by models in azure is <strong>at least</strong> 2X faster.</p>
<p>What could be the reason behind this? Like has azure shared any details around this change in speed?</p>
","gpt-4"
"76363168","OpenAI API: How do I handle errors in Python?","2023-05-30 08:54:55","76371360","3","9050","<python><openai-api><gpt-3><chatgpt-api><gpt-4>","<p>I tried using the below code, but the OpenAI API doesn't have the <code>AuthenticationError</code> method in the library. How can I effectively handle such error.</p>
<pre><code>import openai

# Set up your OpenAI credentials
openai.api_key = 'YOUR_API_KEY'

try:
    # Perform OpenAI API request
    response = openai.some_function()  # Replace with the appropriate OpenAI API function

    # Process the response
    # ...
except openai.AuthenticationError:
    # Handle the AuthenticationError
    print(&quot;Authentication error: Invalid API key or insufficient permissions.&quot;)
    # Perform any necessary actions, such as displaying an error message or exiting the program

</code></pre>
","gpt-4"
"76289498","AttributeError: 'tuple' object has no attribute 'is_single_input","2023-05-19 13:22:39","","0","2690","<python><artificial-intelligence><openai-api><langchain><gpt-4>","<p>I am trying to use langchain Agents in order to get answers for the questions asked using API, but facing error &quot;AttributeError: 'tuple' object has no attribute 'is_single_input'&quot;. Following is the code and error. Open for solution and suggestions.</p>
<pre><code>from langchain.tools import StructuredTool
from langchain.agents import initialize_agent
from langchain.llms import OpenAI
import requests
</code></pre>
<h1>Step 1: Implement your API function or class</h1>
<pre><code>def process_document(document):
    # Process the document using your API logic
    url = 'api'

    data = {'file': open(document, 'rb')}

    response = requests.post(url, auth=requests.auth.HTTPBasicAuth('dfg', ''), files=data)
    return response
</code></pre>
<h1>Step 2: Create a Tool</h1>
<pre><code>tool = StructuredTool.from_function(process_document,description=&quot;Process the document using the API&quot;)
</code></pre>
<h1>Step 3: Initialize the Language Model</h1>
<pre><code>llm = OpenAI(temperature=0, model_name=&quot;text-davinci-003&quot;, openai_api_key=&quot;key&quot;)
</code></pre>
<h1>Step 4: Initialize the Agent</h1>
<pre><code>agent = initialize_agent(tool, llm)
</code></pre>
<h1>Step 5: Use the Agent</h1>
<pre><code>document = &quot;&quot;  # Provide the document to be processed
result = agent.process(document)  # Process the document using the agent and the API
question = &quot;What is Registration number and registration date?&quot;  # Provide the question to ask about    the processed result
answer = agent.generate(question)  # Generate an answer to the question using the agent
</code></pre>
<p>While implementing this I am facing following error :</p>
<pre><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-3-dea540cbc5b0&gt; in &lt;cell line: 28&gt;()
     26 
     27 # Step 4: Initialize the Agent
---&gt; 28 agent = initialize_agent(tool, llm)
     29 
     30 # Step 5: Use the Agent

3 frames
/usr/local/lib/python3.10/dist-packages/langchain/agents/utils.py in validate_tools_single_input(class_name, tools)
      7     &quot;&quot;&quot;Validate tools for single input.&quot;&quot;&quot;
      8     for tool in tools:
----&gt; 9         if not tool.is_single_input:
     10             raise ValueError(
     11                 f&quot;{class_name} does not support multi-input tool {tool.name}.&quot;

AttributeError: 'tuple' object has no attribute 'is_single_input'
 
</code></pre>
","gpt-4"
"76199709","Image to text using Azure OpenAI GPT4","2023-05-08 10:32:50","","5","3726","<python><openai-api><azure-openai><gpt-4>","<p>I have an Azure open AI Account and GPT4 model deployed. Can I use its API for image-to-text description? If yes, how I will give it the image? I am using this code. But it throws me an error.</p>
<pre><code>import openai
# open ai key
openai.api_type = &quot;azure&quot;
openai.api_version = &quot;2023-03-15-preview&quot;
openai.api_base = 'https://xxxxxx.openai.azure.com/'
openai.api_key = &quot;xxxxxxxxxxxxx&quot;

image_url=&quot;https://cdn.repliers.io/IMG-X5925532_9.jpg&quot;

def generate_image_description(image_url):
    prompt = f&quot;What is in this image? {image_url}&quot;
    print(prompt)
    response = openai.ChatCompletion.create(
        engine=&quot;GPT4v0314&quot;,
        prompt=prompt,
        max_tokens=1024,
        n=1,
        stop=None,
        temperature=0.0,
    )
    description = response.choices[0].text.strip()
    return description
</code></pre>
<p>The error is like;
APIError: Invalid response object from API: 'Unsupported data type\n' (HTTP response code was 400)</p>
<p>I mentioned it inside the explanation.</p>
","gpt-4"
"76160057","OpenAI Chat Completions API: How do I customize answers from GPT-3.5 or GPT-4 models if I can't fine-tune them?","2023-05-03 02:27:37","76161653","3","2279","<openai-api><chatgpt-api><gpt-4>","<p>We have seen some companies use GPT-3.5 or GPT-4 models to train their own data and provide customized answers. But GPT-3.5 and GPT-4 models are not available for fine-tuning.</p>
<p>I've seen the document from OpenAI about this issue, but I had seen OpenAI only allow fine-tuning <code>davinci</code>, for example.</p>
<p>How do I customize answers from a GPT-3.5 or GPT-4 model if I can't fine-tune them?</p>
","gpt-4"
"76100892","GPT4 - Unable to get response for a question?","2023-04-25 11:57:01","","1","1118","<gpt-3><chatgpt-api><gpt-4><gpt4all><pygpt4all>","<p>As the title clearly describes the issue I've been experiencing, I'm not able to get a response to a question from the dataset I use using the <a href=""https://github.com/nomic-ai/gpt4all"" rel=""nofollow noreferrer"">nomic-ai/gpt4all</a>. The execution simply stops. No exception occurs. How can I overcome this situation?</p>
<p>p.s. I use the offline mode of <code>GPT4</code> since I need to process a bulk of questions.</p>
<p>p.s. This issue occurs <strong>after a while</strong>. Something prevents <code>GPT4All</code> to generate a response for the given question.</p>
<pre><code>question = 'Was Avogadro a  professor at the University of Turin?'
gpt = GPT4All()
gpt.open()
resp = gpt.prompt(question)
print(f'Response: {resp}')  # --&gt; the execution does not reach here
</code></pre>
","gpt-4"
"76091454","How can I improve my ChatGPT API prompts?","2023-04-24 11:37:33","","0","782","<prompt><openai-api><gpt-3><chatgpt-api><gpt-4>","<p>I am having an issue with ChatGPT API relating to prompt engineering</p>
<p>I have a dataset which consists of individual product titles, and product descriptions which was awful design, but I didn't have control over that part. <strong>I need to create aggregate titles for the individual titles.</strong></p>
<p>I fine-tuned the Curie model on the data in a similar way to this:</p>
<p>Prompt:</p>
<p><code>60cm tall Oak Drop Leaf Table|80cm tall Oak Drop Leaf Table|100cm tall Oak Drop Leaf Table|60cm tall Drop Leaf Table Material: Oak with bird design</code></p>
<p>Completion:</p>
<p><code>Oak Drop Leaf Table</code></p>
<p>I fine-tuned it on about 100 human-written titles</p>
<p>I am currently using settings:</p>
<pre><code>$data = array(
    'model' =&gt; $model,
    'prompt' =&gt; $prompt,
    'temperature' =&gt; 0.6,
    'max_tokens' =&gt; 25,
    &quot;top_p&quot; =&gt; 1,
    &quot;frequency_penalty&quot; =&gt; 0,
    &quot;presence_penalty&quot; =&gt; 0.6,
);
</code></pre>
<p>I have varied these, but to no great effect.</p>
<p>I am wondering where am I going wrong?</p>
<p>I am getting responses like:</p>
<p><code>60cm Oak Drop Leaf TableOak Drop Leaf TableOak Drop Leaf Table</code></p>
<p><code>Oak Drop Leaf Table|Oak Drop Leaf Table|Oak Drop Leaf Table</code></p>
<p><code>Oak Drop Leaf Table,Oak Drop Leaf Table,Oak Drop Leaf Table</code></p>
","gpt-4"
"76073607","How to change goals after setting up Auto-GPT","2023-04-21 13:25:36","76076750","1","1560","<openai-api><gpt-4><autogpt>","<p>I have set up Auto-GPT in my system with the goals below.</p>
<p>1 Grow my linkedin account
2 Look for new innovative linkedin post ideas for AI technology
3 Prepare post on that idea
4 Write that post in the file
5 Shutdown</p>
<p>Auto GPT is not producing the results I expected, And now I want to change my goals for different results. How do I modify the goals I set earlier? Or is there any way to improve the result it produces?</p>
<p>I have tried restarting Auto GPT and terminal as well.</p>
","gpt-4"
"76048714","Finetuning a LM vs prompt-engineering an LLM","2023-04-18 20:15:18","76052203","3","2542","<language-model><roberta-language-model><roberta><gpt-4><large-language-model>","<p>Is it possible to finetune a much smaller language model like Roberta on say, a customer service dataset and get results as good as one might get with prompting GPT-4 with parts of the dataset?</p>
<p>Can a fine-tuned Roberta model learn to follow instructions in a conversational manner at least for a small domain like this?</p>
<p>Is there any paper or article that explores this issue empirically that I can check out?</p>
","gpt-4"
"75970356","Comparing methods for a QA system on a 1,000-document Markdown dataset: Indexes and embeddings with GPT-4 vs. retraining GPT4ALL (or similar)","2023-04-09 11:58:57","","3","390","<deep-learning><openai-api><gpt-4><large-language-model><gpt4all>","<p>I am working on a project to build a question-answering system for a documentation portal containing over 1,000 Markdown documents, with each document consisting of approximately 2,000-4,000 tokens.</p>
<p>I am considering the following two options:</p>
<ol>
<li>Using indexes and embeddings with GPT-4</li>
<li>Retraining a model like GPT4ALL (or a similar model) to specifically handle my dataset</li>
</ol>
<p>Which of these approaches is more likely to produce better results for my use case?</p>
","gpt-4"
"75946090","Why is GPT-4 giving different answers with same prompt & temperature=0?","2023-04-06 05:23:31","","2","6274","<gpt-4>","<p>This is my code for calling the gpt-4 model:</p>
<pre><code>messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_msg},
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: req}
]

response = openai.ChatCompletion.create(
        engine = &quot;******-gpt-4-32k&quot;,
        messages = messages,
        temperature=0,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
    )

answer = response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]
</code></pre>
<p>Keeping system_msg &amp; req constant, with temperature=0, I get different answers.
I got 3 different answers when I last ran this 10 times for instance.
The answers are similar in concept, but differ in semantics.</p>
<p>Why is this happening?</p>
","gpt-4"
"75945693","how to determine the expected prompt_tokens for gpt-4 chatCompletion","2023-04-06 03:58:46","","0","1620","<openai-api><chatgpt-api><gpt-4>","<p>For the following nodejs code below I am getting prompt_tokens = 24 in the response. I want to be able to determine what the expected prompt_tokens should be prior to making the request.</p>
<pre class=""lang-js prettyprint-override""><code>    import { Configuration, OpenAIApi } from 'openai';

    const configuration = new Configuration({
        apiKey: process.env.OPENAI_API_KEY,
     });
     
    const openai = new OpenAIApi(configuration);

    const completion = await openai.createChatCompletion({
    model: &quot;gpt-4&quot;,
    messages: [
      {role: &quot;system&quot;, content: systemPrompt}, //systemPrompt= 'You are a useful assistant.'     
      {role: &quot;user&quot;, content: userPrompt} //userPrompt= `What is the meaning of life?`
    ]
    });

    /* completion.data = {
       id: 'chatcmpl-72Andnl250jsvSJGbjBJ6YzzFGToA',
       object: 'chat.completion',
       created: 1680752525,
       model: 'gpt-4-0314',
       usage: { prompt_tokens: 24, completion_tokens: 91, total_tokens: 115 },
       choices: [ [Object] ]
    } */
</code></pre>
<p>It seems like each model has its own way of encoding and the best lib for that is python tiktoken. Hence if I was to estimate &quot;prompt_tokens&quot;. I would need to pass through the &quot;text&quot; value to the script below. However I am not sure what I should be using as the &quot;text&quot; below in the python script for the &quot;messages&quot; above in the nodejs, such that print(token_count) below = 24 [the actual prompt_tokens in the response]</p>
<pre class=""lang-py prettyprint-override""><code>    import sys
    import tiktoken

    text = sys.argv[1]
    enc = tiktoken.encoding_for_model(&quot;gpt-4&quot;)
    tokens = enc.encode(text)
    token_count = len(tokens)
    print(token_count)
</code></pre>
","gpt-4"
"75906140","word to word gpt api responce stream in react native","2023-04-01 12:17:21","","1","1002","<react-native><openai-api><gpt-3><chatgpt-api><gpt-4>","<p>Is there a way to implement GPT API with word to word api</p>
<p>I already try implement in javascript directly into app but its not working</p>
<p>I want to use Chat GPT Turbo api directly in react native (expo) with word by word stream here is working example without stream</p>
","gpt-4"
"75874606","Error: PineconeClient: Project name not set, v0.0.10","2023-03-29 08:02:07","","2","421","<openai-api><gpt-3><chatgpt-api><gpt-4><langchain>","<p>Downloaded GitHub - <a href=""https://github.com/mayooear/gpt4-pdf-chatbot-langchain"" rel=""nofollow noreferrer"">mayooear/gpt4-pdf-chatbot-langchain:</a> GPT4 &amp; LangChain Chatbot for large PDF docs</p>
<p>When I try to run the app, I get following error</p>
<pre><code>PineconeClient: Error getting project name: TypeError: fetch failed

error - [Error: PineconeClient: Project name not set. Call init() first.] {
page: ‚Äò/api/chat‚Äô
}
</code></pre>
<p>My node version is node -v
v18.15.0
Pinecone version is
<code>‚Äú@pinecone-database/pinecone‚Äù: ‚Äú^0.0.10‚Äù,</code></p>
<p>It seems it is due to issue of fetch() which is an experimental feature. How to disable fetch() and use node-fetch()</p>
","gpt-4"
"75864319","import Image as PIL_Image ModuleNotFoundError: No module named 'Image' while running langchain with DirectoryLoader('source', glob='*.pdf')","2023-03-28 09:09:22","","2","308","<python-3.x><openai-api><gpt-3><gpt-4><langchain>","<pre><code>from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader

import openai
loader = DirectoryLoader('source', glob='*.pdf')

data = loader.load()
</code></pre>
<p>Just this much code... I get this error</p>
<pre><code>  File &quot;C:\Users\vsvrp\anaconda3\envs\GPTtrail2\lib\site-packages\pptx\parts\image.py&quot;, line 13, in &lt;module&gt;
    import Image as PIL_Image
ModuleNotFoundError: No module named 'Image'

Process finished with exit code 1
</code></pre>
<p>I do not get this error if I do this</p>
<pre><code>loader = UnstructuredPDFLoader(&quot;DOStest.pdf&quot;)
</code></pre>
<p>I tried to do pip install Image</p>
<p>It is still not working. Any help would be greatly appreciated.</p>
<p>Working with langchain and documentloaders for the first time and the DirectoryLoader class is supposed to work in this case.</p>
","gpt-4"
"75861442","An error occurred: module 'openai' has no attribute 'ChatCompletion'","2023-03-28 00:33:57","","10","34500","<python><openai-api><gpt-4>","<p>I'm trying to build a discord bot that uses the GPT-4 API to function as a chatbot on discord. I have the most recent version of the OpenAI library but when I run my code it tells me &quot;An error occurred: module 'openai' has no attribute 'ChatCompletion'&quot;</p>
<p>I tried uninstalling and reinstalling the OpenAI library, I tried using the completions endpoint and got the error &quot;This is a chat model and not supported in the v1/completions endpoint. Did you mean to use v1/chat/completions?&quot;</p>
<p>This is the snippet of code that's giving me issues:</p>
<pre><code>async def get_gpt_response(prompt, history):
    history_strings = [f&quot;{message['role']}: {message['content']}&quot; for message in history] # update history format
    chat_prompt = '\n'.join(history_strings + [f&quot;user: {prompt}&quot;])
    
    completions = openai.ChatCompletion.create(
        engine=config[&quot;model&quot;],
        prompt=chat_prompt,
        max_tokens=config[&quot;max_tokens&quot;],
        n=1,
        temperature=config[&quot;temperature&quot;],
    )
    return completions.choices[0].text.strip().split('assistant:', 1)[-1].strip()
</code></pre>
","gpt-4"
"75860080","Can I use a single ChatGPT-3 API key for multiple projects simultaneously?","2023-03-27 20:25:14","","1","3023","<chatgpt-api><gpt-4>","<p>I am a new programmer in college and I am trying to learn how to use API keys. I am currently using the ChatGPT-3 API for my Siri personal assistant project, and it's been working well for me so far.</p>
<p>Now, I am developing another application - a bot that can utilize my resume to automatically generate cover letters, and reach out to talent acquisition teams.</p>
<p>Can I use the same ChatGPT-3 API key for both projects simultaneously? Are there any limitations or issues I should be aware of while using a single API key for multiple projects?</p>
","gpt-4"
"75810740","OpenAI GPT-4 API: What is the difference between gpt-4 and gpt-4-0314 or gpt-4-0613?","2023-03-22 09:57:45","","15","13933","<openai-api><gpt-4>","<p><em>Note: The question was originally asking about the difference between the <code>gpt-4</code> and <code>gpt-4-0314</code>. As of June 15, 2023, there are new snapshot models available (e.g., <code>gpt-4-0613</code>) so the question and its answer are also relevant for any future snapshot models that will come in the following months.</em></p>
<p>Can anyone help explain the difference to me between <code>gpt-4</code> and <code>gpt-4-0314</code> (which appear on the OpenAI playground dropdown menu below)? I have checked various search engines and its not clear from the OpenAI forum results.</p>
<p><a href=""https://i.sstatic.net/oQGvWl.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/oQGvWl.png"" alt=""enter image description here"" /></a></p>
<p>GPT-4-0314 could refer to a specific version or release of GPT-4 with a particular set of updates or improvements, perhaps released on March 14th. Anyone any experience of the differences, feedback would be welcome.</p>
","gpt-4"
"75807664","Issues Handling ChatGPT Streaming Response in Terminal using OpenAI API - Using Python, rich library","2023-03-22 01:09:17","","-2","3114","<python><openai-api><rich><gpt-4>","<p>I am trying to integrate the <strong>openAi API</strong> model - <code>gpt-4</code> with Terminal to enable <strong>ChatGPT</strong>. My objective is to receive streaming responses from <strong>ChatGPT</strong> and print them in the Terminal.
Although I can successfully print the entire response without streaming, I'm facing issues with streaming responses. Specifically, the <code>ask_stream</code> function is printing every word on a new line, which is not the desired behavior. I'm using the rich library to handle Markups</p>
<p>My code:</p>
<pre><code>import openai
from rich.markdown import Markdown
from rich.console import Console
from prompt_toolkit import PromptSession
from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
from prompt_toolkit.completion import WordCompleter
from prompt_toolkit.history import InMemoryHistory
import time
import argparse
import asyncio

openai.api_key = &quot;MY API KEY&quot;
model = &quot;gpt-4&quot;
delay_time = 0.01
max_response_length = 200
console = Console()


async def ask_stream(prompt):
    response = openai.ChatCompletion.create(model='gpt-4',
                                            messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{prompt}&quot;}], max_tokens=8000,
                                            temperature=0.4, stream=True)
    answer = ''
    for event in response:
        if answer:
            console.print(Markdown(answer), end='')
        # sys.stdout.flush()
        event_text = event['choices'][0]['delta']
        answer = event_text.get('content', '')
        time.sleep(0.01)


async def ask(prompt) -&gt; Markdown:
    if prompt:
        completion = openai.ChatCompletion.create(model=model,
                                                  messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: f&quot;{prompt}&quot;}])
        if completion:
            if 'error' in completion:
                return completion['error']['message']
            return Markdown(completion.choices[0].message.content)
        else:
            raise Exception(&quot;&quot;)


def create_session() -&gt; PromptSession:
    return PromptSession(history=InMemoryHistory())


async def get_input_async(
        session: PromptSession = None,
        completer: WordCompleter = None,
) -&gt; str:
    &quot;&quot;&quot;
    Multiline input function.
    &quot;&quot;&quot;
    return await session.prompt_async(
        completer=completer,
        multiline=True,
        auto_suggest=AutoSuggestFromHistory(),
    )


async def main():
    print(f&quot;Starting Chatgpt with model - {model}&quot;)
    session = create_session()
    while True:
        print(&quot;\nYou:&quot;)
        question = await get_input_async(session=session)
        print()
        print()
        if question == &quot;!exit&quot;:
            break
        elif question == &quot;!help&quot;:
            print(
                &quot;&quot;&quot;
            !help - Show this help message
            !exit - Exit the program
            &quot;&quot;&quot;,
            )
            continue
        print(&quot;ChatGPT:&quot;)
        if args.no_stream:
            console.print(await ask(prompt=question))
        else:
            await ask_stream(prompt=question)


if __name__ == &quot;__main__&quot;:
    parser = argparse.ArgumentParser()
    parser.add_argument(&quot;--no-stream&quot;, action=&quot;store_true&quot;)
    args = parser.parse_args()
    asyncio.run(main())
</code></pre>
<p><code>ask_stream</code> prints like below
<a href=""https://i.sstatic.net/cEJWC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cEJWC.png"" alt=""enter image description here"" /></a></p>
<p>Can someone suggest a solution to fix this issue? I am pretty new to Python.</p>
","gpt-4"
"75804599","OpenAI API: How do I count tokens before(!) I send an API request?","2023-03-21 17:35:10","75804651","81","80924","<python><openai-api><chatgpt-api><gpt-3><gpt-4>","<p>OpenAI's text models have a context length, e.g.: Curie has a context length of 2049 tokens.</p>
<p>They provide <code>max_tokens</code> and <code>stop</code> parameters to control the length of the generated sequence. Therefore the generation stops either when stop token is obtained, or <code>max_tokens</code> is reached.</p>
<p>The issue is: when generating a text, I don't know how many tokens my prompt contains. Since I do not know that, I cannot set <code>max_tokens = 2049 - number_tokens_in_prompt</code>.</p>
<p>This prevents me from generating text dynamically for a wide range of text in terms of their length. What I need is to continue generating until the stop token.</p>
<p>My questions are:</p>
<ul>
<li>How can I count the number of tokens in Python API so that I will set <code>max_tokens</code> parameter accordingly?</li>
<li>Is there a way to set <code>max_tokens</code> to the max cap so that I won't need to count the number of prompt tokens?</li>
</ul>
","gpt-4"
"75787052","GPT 4 API delays/data types","2023-03-20 06:12:24","","0","446","<gpt-4>","<p>I got into the API beta and I'm playing around with an app. I got as far as getting the API connection working and doing what I want in pycharm, but have a couple problems:</p>
<ol>
<li><p>I'm getting pretty slow response times and hitting a usage cap frequently as well (the API account is sufficiently funded). I assume some of this will improve as the new product stabilizes? Would rather not switch to an earlier model for my use case.</p>
</li>
<li><p>I'm asking GPT to give me a list of items in a python list format, which I am able to typecast into an actual list. If I set the temperature too low I get back repetitive items, but if I set it too high I don't get the correct python formatting.</p>
</li>
<li><p>I'm hitting the API 5 or 6 times which could probably be consolidated down to a couple, but that would depend on consistently getting a properly formatted JSON response, which seems more dubious than asking for a python list.</p>
</li>
</ol>
<p>Basically, is this thing predictable enough that you can ask for a certain data format and it will come in that format reliably enough to build an app on top of?</p>
<p>Any suggestions/discussion is appreciated.</p>
<p>What have I tried:
Tried various temperatures. Have asked OpenAI to increase usage cap. Have not tried other models.</p>
","gpt-4"
"75773786","Why can't I access GPT-4 models via API, although GPT-3.5 models work?","2023-03-18 03:59:30","75774187","20","29659","<python><openai-api><gpt-4>","<p>I'm able to use the gpt-3.5-turbo-0301 model to access the ChatGPT API, but not any of the gpt-4 models. Here is the code I am using to test this (it excludes my openai API key). The code runs as written, but when I replace &quot;gpt-3.5-turbo-0301&quot; with &quot;gpt-4&quot;, &quot;gpt-4-0314&quot;, or &quot;gpt-4-32k-0314&quot;, it gives me an error</p>
<pre><code>openai.error.InvalidRequestError: The model: `gpt-4` does not exist
</code></pre>
<p>I have a ChatGPT+ subscription, am using my own API key, and can use gpt-4 successfully via OpenAI's own interface.</p>
<p>It's the same error if I use gpt-4-0314 or gpt-4-32k-0314. I've seen a couple articles claiming this or similar code works using 'gpt-4' works as the model specification, and the code I pasted below is from one of them.</p>
<p>Is it possible to access the gpt-4 model via Python + API, and if so, how?</p>
<pre><code>openai_key = &quot;sk...&quot;
openai.api_key = openai_key
system_intel = &quot;You are GPT-4, answer my questions as if you were an expert in the field.&quot;
prompt = &quot;Write a blog on how to use GPT-4 with python in a jupyter notebook&quot;
# Function that calls the GPT-4 API

def ask_GPT4(system_intel, prompt): 
    result = openai.ChatCompletion.create(model=&quot;gpt-3.5-turbo-0301&quot;,
                                 messages=[{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_intel},
                                           {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}])
    print(result['choices'][0]['message']['content'])

# Call the function above
ask_GPT4(system_intel, prompt)
</code></pre>
","gpt-4"
"75401992","OpenAI API error: ""You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth""","2023-02-09 17:00:01","75402073","0","6428","<php><curl><openai-api><gpt-3><gpt-4>","<p>I am creating a PHP script to access Open Ai's API, to ask a query and get a response.</p>
<p>I am getting the following error:</p>
<blockquote>
<p>You didn't provide an API key. You need to provide your API key in an
Authorization header using Bearer auth (i.e. Authorization: Bearer
YOUR_KEY)</p>
</blockquote>
<p>...but I thought I was providing the API key in the first variable?</p>
<p>Here is my code:</p>
<pre><code>$api_key = &quot;sk-U3B.........7MiL&quot;;

$query = &quot;How are you?&quot;;

$url = &quot;https://api.openai.com/v1/engines/davinci/jobs&quot;;

// Set up the API request headers
$headers = array(
    &quot;Content-Type: application/json&quot;,
    &quot;Authorization: Bearer &quot; . $api_key
);

// Set up the API request body
$data = array(
    &quot;prompt&quot; =&gt; $query,
    &quot;max_tokens&quot; =&gt; 100,
    &quot;temperature&quot; =&gt; 0.5
);

// Use WordPress's built-in HTTP API to send the API request
$response = wp_remote_post( $url, array(
    'headers' =&gt; $headers,
    'body' =&gt; json_encode( $data )
) );

// Check if the API request was successful
if ( is_wp_error( $response ) ) {
    // If the API request failed, display an error message
    echo &quot;Error communicating with OpenAI API: &quot; . $response-&gt;get_error_message();
} else {
    // If the API request was successful, extract the response text
    $response_body = json_decode( $response['body'] );
    //$response_text = $response_body-&gt;choices[0]-&gt;text;
    var_dump($response_body);
    // Display the response text on the web page
    echo $response_body;
</code></pre>
","gpt-4"
"75313457","OpenAI API: openai.api_key = os.getenv() not working","2023-02-01 16:44:32","75313682","3","31868","<python><openai-api><chatgpt-api><gpt-3><gpt-4>","<p>I am just trying some simple functions in Python with OpenAI APIs but running into an error:</p>
<p>I have a valid API secret key which I am using.</p>
<p>Code:</p>
<pre><code>&gt;&gt;&gt; import os
&gt;&gt;&gt; import openai
&gt;&gt;&gt; openai.api_key = os.getenv(&quot;I have placed the key here&quot;)
&gt;&gt;&gt; response = openai.Completion.create(model=&quot;text-davinci-003&quot;, prompt=&quot;Say this is a test&quot;, temperature=0, max_tokens=7)
</code></pre>
<p><a href=""https://i.sstatic.net/zCgm4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zCgm4.png"" alt=""Simple test"" /></a></p>
","gpt-4"