Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"78872601","Sample weights for loss computing by huggingface transformer model","2024-08-14 19:14:23","","1","20","<python><deep-learning><pytorch><huggingface-transformers><language-model>","<p>I'm training a <code>GPT2LMHeadModel</code> in Python using huggingface's <code>transformers</code> library. The task is next token prediction. If I understand correctly, if this object is provided a <code>labels</code> argument, it should automatically compute loss for each pair of tokens, i.e. if I have input <code>[1,2,3]</code>, then the object should compute the loss for each of the (input, next_token) combinations: <code>([1], [2])</code> and <code>([1,2], [3])</code></p>
<p>My dataset includes weights for each sample. Is there a way for me to include these weights in the training process? Note that these are not class weights, but weights for each individual record in my train set. In a more &quot;vanilla&quot; training scenario with PyTroch, I would just set <code>reduction='none'</code> on my criterion, then multiple the result by the batch's weights. I'm not sure how I would implement this efficiently, accounting for all the prediction tasks that go into a single sequence.</p>
","language-model"
"78817584","When I run the file, it starts processing and then stops, giving the following error","2024-07-31 17:06:53","","0","26","<deep-learning><pre-trained-model><language-model>","<ol>
<li>I'm new to deep learning and I need help. The more detailed it is, the better. I have the following problem: In an attempt to train a model with three different languages, with Portuguese (as the fourth language) as the reference language, I'm getting the following error:</li>
</ol>
<pre><code>2024-07-31 17:34:52.770154: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-07-31 17:34:53.956581: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
C:\Users\myuser\PycharmProjects\.venv\Lib\site-packages\keras\src\layers\core\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.
  warnings.warn(
2024-07-31 17:35:09.997360: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
Epoch 1/5
Traceback (most recent call last):
  File &quot;C:\Users\myuser\PycharmProjects\.venv\Lib\site-packages\fasttext\align_sentences_ANN.py&quot;, line 112, in &lt;module&gt;
    main()
  File &quot;C:\Users\myuser\PycharmProjects\.venv\Lib\site-packages\fasttext\align_sentences_ANN.py&quot;, line 99, in main
    trained_model = train_lstm_model(lstm_model, sentences, embeddings, tokenizer)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File &quot;C:\Users\myuser\PycharmProjects\.venv\Lib\site-packages\fasttext\align_sentences_ANN.py&quot;, line 43, in train_lstm_model
    model.fit(padded_sequences, np.array(embeddings), epochs=epochs)
  File &quot;C:\Users\myuser\PycharmProjects\.venv\Lib\site-packages\keras\src\utils\traceback_utils.py&quot;, line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File &quot;C:\Users\myuser\PycharmProjects\.venv\Lib\site-packages\keras\src\losses\losses.py&quot;, line 1286, in mean_squared_error
    return ops.mean(ops.square(y_true - y_pred), axis=-1)
                               ~~~~~~~^~~~~~~~
ValueError: Dimensions must be equal, but are 300 and 100 for '{{node compile_loss/mean_squared_error/sub}} = Sub[T=DT_FLOAT](data_1, sequential_1/dense_1/Add)' with input shapes: [?,300], [?,100]. 
</code></pre>
<p>The code being executed is as follows:</p>
<pre><code>import os
import nltk
import numpy as np
import fasttext.util
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM
from sklearn.metrics.pairwise import cosine_similarity

#nltk.download('punkt')

# Functions for processing text and generating embeddings

def segment_sentences(file_path):
    &quot;&quot;&quot;Segment text into sentences using NLTK.&quot;&quot;&quot;
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
    sentences = nltk.sent_tokenize(text)
    return sentences

def get_sentence_embedding(sentence, model):
    &quot;&quot;&quot;Generate sentence embeddings using a FastText model.&quot;&quot;&quot;
    words = sentence.split()
    embeddings = [model.get_word_vector(word) for word in words]
    return np.mean(embeddings, axis=0)

# Functions for neural network model creation and training

def create_lstm_model(vocab_size, embedding_dim=100, input_length=50):
    &quot;&quot;&quot;Create and compile an LSTM neural network model.&quot;&quot;&quot;
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length),
        LSTM(units=128, return_sequences=False),
        Dense(units=embedding_dim)
    ])
    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

def train_lstm_model(model, sentences, embeddings, tokenizer, epochs=5):
    &quot;&quot;&quot;Train the LSTM model on the given sentences and embeddings.&quot;&quot;&quot;
    sequences = tokenizer.texts_to_sequences(sentences)
    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=50)
    model.fit(padded_sequences, np.array(embeddings), epochs=epochs)
    return model

# Functions for sentence alignment and output

def align_sentences(embeddings1, embeddings2):
    &quot;&quot;&quot;Align sentences based on cosine similarity of embeddings.&quot;&quot;&quot;
    similarity_matrix = cosine_similarity(embeddings1, embeddings2)
    alignment = []
    for i in range(len(embeddings1)):
        j = similarity_matrix[i].argmax()
        alignment.append((i, j))
    return alignment

def combine_alignments(alignment12, alignment13, alignment14):
    &quot;&quot;&quot;Combine pairwise alignments into a single alignment.&quot;&quot;&quot;
    combined_alignment = []
    for i, j in alignment12:
        k = next(k for k, l in alignment13 if k == i)
        l = next(l for m, l in alignment14 if m == i)
        combined_alignment.append((i, j, k, l))
    return combined_alignment

def output_aligned_sentences(alignment, sentences1, sentences2, sentences3, sentences4):
    &quot;&quot;&quot;Print aligned sentences.&quot;&quot;&quot;
    for i, j, k, l in alignment:
        print(f&quot;text1: {sentences1[i]}&quot;)
        print(f&quot;text2: {sentences2[j]}&quot;)
        print(f&quot;text3: {sentences3[k]}&quot;)# this is a pre-processed text in Portuguese.
        print(f&quot;text4: {sentences4[l]}&quot;)
        print('-' * 80)

# Main function to execute the alignment process

def main():
    directory = 'C:/Users/myuser/PycharmProjects/.venv/Lib/site-packages/fasttext'
    files = ['text1.txt', 'text2.txt', 'text3.txt', 'text4.txt']
    file_paths = [os.path.join(directory, file) for file in files]
    
    sentences_list = [segment_sentences(file_path) for file_path in file_paths]
    
    # Load pre-trained FastText model for embedding generation
    model = fasttext.load_model('cc.pt.300.bin') 
    
    embeddings_list = [[get_sentence_embedding(sentence, model) for sentence in sentences] for sentences in sentences_list]
    
    # Prepare tokenizer for each text
    tokenizers = [tf.keras.preprocessing.text.Tokenizer() for _ in sentences_list]
    for tokenizer, sentences in zip(tokenizers, sentences_list):
        tokenizer.fit_on_texts(sentences)
    
    # Create and train an LSTM model for each text
    lstm_models = []
    for tokenizer, sentences, embeddings, file_name in zip(tokenizers, sentences_list, embeddings_list, files):
        vocab_size = len(tokenizer.word_index) + 1
        lstm_model = create_lstm_model(vocab_size)
        trained_model = train_lstm_model(lstm_model, sentences, embeddings, tokenizer)
        lstm_models.append(trained_model)
        trained_model.save(os.path.join(directory, f'lstm_model_{file_name.split(&quot;.&quot;)[0]}.h5'))
    
    alignment12 = align_sentences(embeddings_list[0], embeddings_list[1])
    alignment13 = align_sentences(embeddings_list[0], embeddings_list[2])
    alignment14 = align_sentences(embeddings_list[0], embeddings_list[3])
    
    final_alignment = combine_alignments(alignment12, alignment13, alignment14)
    
    output_aligned_sentences(final_alignment, *sentences_list)

if __name__ == &quot;__main__&quot;:
    main()

Can anyone help me overcome this error?
</code></pre>
","language-model"
"78743278","DSPy: How to get the number of tokens available for the input fields?","2024-07-13 08:25:59","","0","88","<python><large-language-model><language-model><retrieval-augmented-generation><dspy>","<p>This is a cross-post from <a href=""https://github.com/stanfordnlp/dspy/issues/1245"" rel=""nofollow noreferrer"">Issue #1245 of DSPy GitHub Repo</a>. There were no responses in the past week, am I am working on a project with a tight schedule.</p>
<p>When running a DSPy module with a given signature, I am interested in getting the token count of the &quot;prompt template&quot; that it currently passes to the language model (LM), by which I meant the number of input tokens passed to the LM minus the token counts of the input fields. This would thus count the length of the signature description, field descriptions, and the few-shot examples. Then, by subtracting the context window of the LM with the token count of the prompt template, I would get the maximum number of tokens that I can squeeze into the input fields.</p>
<p>I am interested in this as I am currently building a RAG pipeline that retrieves texts from a database to synthesize the final response. However, the total length of the texts retrieved from the database might exceed the context window size of the LM I am using. Thus, a iterative or recursive summarization process is need to compress the prompt before synthesizing the final response. While I acknowledge that you can simply summarize each chunk of text one-by-one to be extra cautious to not exceed the context window, I think this might not be the most effective way to do this.</p>
<p>I originally built an RAG pipeline entirely using LlamaIndex where the response would be generated by <a href=""https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/"" rel=""nofollow noreferrer"">response synthesizers</a>. Note that the <code>compact</code> mode of response synthesizers would try to pack as many tokens from the retrieved contexts into a single LM call as possible to reduce the number of calls. This is achived via <a href=""https://docs.llamaindex.ai/en/v0.10.19/api_reference/service_context/prompt_helper.html"" rel=""nofollow noreferrer"">PromptHelper</a> that squeezes as many tokens into the fields of the prompt template as possible so that the length of the fields altogether does not excess <code>context_window - prompt_template_length</code>.</p>
<p>Now, as I am switching all the prompting to DSPy for more flexibility, I wonder what would be the best way for me to implement something alike <code>PromptHelper</code>?  I also checked how the LlamaIndex integration for DSPy does this: <a href=""https://github.com/stanfordnlp/dspy/blob/55510eec1b83fa77f368e191a363c150df8c5b02/dspy/predict/llamaindex.py#L22-L36"" rel=""nofollow noreferrer"">https://github.com/stanfordnlp/dspy/blob/55510eec1b83fa77f368e191a363c150df8c5b02/dspy/predict/llamaindex.py#L22-L36</a></p>
<p>It appears that it converts the signature to a legacy format first? Therefore, would this be a good approach to this problem or are there better alternatives?</p>
","language-model"
"78587300","What are the key quality metrics for large language model releases?","2024-06-06 14:27:30","","0","22","<machine-learning><large-language-model><pre-trained-model><language-model>","<p>I am a first year PhD student working on improving the release practices of Machine Learning Models, especially pre-trained large language models. I want to understand the above concept for a preliminary learning and opinion from the experts.</p>
<p>I have been reading different articles, but still need experts' opinions.</p>
","language-model"
"78485460","'SymbolicTensor' object cannot be interpreted as an integer","2024-05-15 17:01:32","","0","114","<python><tensorflow><nlp><lstm><language-model>","<p>I have been trying to implement Peephole LSTM using Tensorflow, and I am getting the error below
<a href=""https://i.sstatic.net/19mX2G53.png"" rel=""nofollow noreferrer"">Error</a>
below is my model and I am not sure why I cant get the input layer in my model summary
<a href=""https://i.sstatic.net/f5jxXei6.png"" rel=""nofollow noreferrer"">Model</a>
and below is my Call method for PeepholeLSTM
<a href=""https://i.sstatic.net/fzcyUxp6.png"" rel=""nofollow noreferrer"">Peephole LSTM class</a></p>
<p>It looks like I cannot pass symbolic Tensorflow object to range method but not sure how to avoid this. Appreciate your help</p>
","language-model"
"78484313","Using Language Model Phi-3-Mini quantized version in Jupyter Notebook","2024-05-15 13:31:18","","1","194","<python><language-model>","<p>I am trying to use a small language model in my jupyter notebook and am not able to find a working solution. I want to use the quantized version of Phi-3-mini as that is small enough to fit on my GPU and runs faster.
Loading the normal version of phi-3-mini works just fine. But when loading the quantized version I always get a ValueError saying that: &quot;Unrecognized configuration class &lt;class 'transformers_modules.microsoft.Phi-3-mini-128k-instruct-onnx.791e509f326110e83437e537c2c4182815a6819a.configuration_phi3.Phi3Config'&gt; to build an AutoTokenizer.&quot;</p>
<p>From the documentation on HuggingFace: <a href=""https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-onnx</a> it says that only the onnx version is quantized so I am using that version.</p>
<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM

# This works just fine (normal version but too big for my GPU)
tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/Phi-3-mini-128k-instruct&quot;,trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(&quot;microsoft/Phi-3-mini-128k-instruct&quot;,trust_remote_code=True)

# But this throws an error (quantized version)
tokenizer = AutoTokenizer.from_pretrained(&quot;microsoft/Phi-3-mini-128k-instruct-onnx&quot;, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(&quot;microsoft/Phi-3-mini-128k-instruct-onnx&quot;, trust_remote_code=True)
</code></pre>
","language-model"
"78445793","ValueError: The model did not return a loss from the inputs: During Further Pretraining ARBERT Model","2024-05-08 02:11:55","","0","21","<nlp><huggingface-transformers><arabic><language-model>","<p>I am applying further pretraining for the ARBERT model for both the Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) tasks. However, I am receiving the below error. I know this error arises if I don't name my dataset columns as &quot;text&quot; and &quot;labels&quot;. I am already naming my dataset column as &quot;text&quot;. I don't have a &quot;labels&quot; one since it is an unsupervised task. Also, as far as I know, the &quot;labels&quot; here should be the Tokens of MLM and NSP ([MASK], [CLS], [NSP]) which should be handled automatically by the DataCollator function. So, can anyone advice me on this?</p>
<p><strong>ValueError: The model did not return a loss from the inputs, only the following keys: prediction_logits,seq_relationship_logits. For reference, the inputs it received are input_ids,attention_mask,labels</strong>.</p>
<p><em><strong>Note:</strong></em> I receive this error upon initializing the &quot;Trainer&quot;:<br />
<code>trainer_mlm = Trainer(model=ARBERT_model, args=training_args, data_collator=data_collator_mlm, train_dataset=train_dataset_for_mlm) </code></p>
","language-model"
"78261781","Issues with Generating Text from Fine-Tuned Mistral 7B Model on Georgian Dataset","2024-04-02 13:46:29","","0","235","<nlp><huggingface><language-model><fine-tuning><text-generation>","<p>I've fine-tuned the Mistral 7B model using a Georgian dataset with approximately 100,000 articles, including custom tokenizer fine-tuning. The fine-tuning process took about 9 hours. However, when I try to generate text, the output is not as expected; it consistently returns the input as the output, regardless of the input provided.</p>
<p>Here's the code I used for fine-tuning:</p>
<pre class=""lang-py prettyprint-override""><code>import time
import json
import torch
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments

# Load dataset, preprocess, and fine-tuning details...

training_args = TrainingArguments(
    output_dir=&quot;mistral_georgian_news_finetuning&quot;,
    max_steps=3125,
    per_device_train_batch_size=32,
    learning_rate=3e-4,
    # Other arguments...
)

# Fine-tuning setup...

# Start fine-tuning
trainer.train()
</code></pre>
<p>For testing the fine-tuned model, I used the following code:</p>
<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_path = &quot;/path/to/fine-tuned-model&quot;
tokenizer_path = &quot;/path/to/tokenizer&quot;

tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
model = AutoModelForCausalLM.from_pretrained(model_path)

def generate_text(prompt_text, max_length=500):
    input_ids = tokenizer(prompt_text, return_tensors=&quot;pt&quot;).input_ids
    output = model.generate(input_ids, max_length=max_length)
    return tokenizer.decode(output[0], skip_special_tokens=True)

prompt = &quot;რამდენიმე დღეში შესრულდება ...&quot;
generated_text = generate_text(prompt)
print(generated_text)
</code></pre>
<p>During testing, the model just echoes the prompt without generating new text. Below are the logs observed:</p>
<pre class=""lang-none prettyprint-override""><code>config.json: 0%| | 0.00/571 [00:00&lt;?, ?B/s]
model.safetensors.index.json: 0%| | 0.00/25.1k [00:00&lt;?, ?B/s]
Downloading shards: 0%| | 0/2 [00:00&lt;?, ?it/s]
model-00001-of-00002.safetensors: 0%| | 0.00/9.94G [00:00&lt;?, ?B/s]
model-00002-of-00002.safetensors: 0%| | 0.00/4.54G [00:00&lt;?, ?B/s]
Loading checkpoint shards: 0%| | 0/2 [00:00&lt;?, ?it/s]
generation_config.json: 0%| | 0.00/116 [00:00&lt;?, ?B/s]
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
</code></pre>
<p>I am unsure if the issue lies in how I am loading and testing the model or if it is related to the fine-tuning process. The model should generate text based on the input prompt, but it returns the input as the output.</p>
<p>Has anyone experienced similar issues, or can someone spot what might be wrong with my approach?</p>
","language-model"
"77757228","What are the differences between 'fairseq' and 'fairseq2'?","2024-01-04 09:46:32","","0","253","<deep-learning><frameworks><open-source><language-model><fairseq>","<p>What are the differences between <a href=""https://github.com/facebookresearch/fairseq"" rel=""nofollow noreferrer"">fairseq</a> and <a href=""https://github.com/facebookresearch/fairseq2"" rel=""nofollow noreferrer"">fairseq2</a>?</p>
<p>Quotes from the github pages are not very clear</p>
<blockquote>
<p>Fairseq(-py) is a sequence modeling toolkit that allows researchers
and developers to train custom models for translation, summarization,
language modeling and other text generation tasks.</p>
</blockquote>
<p>and</p>
<blockquote>
<p>fairseq2 is a sequence modeling toolkit that allows researchers and
developers to train custom models for translation, summarization,
language modeling, and other content generation tasks. It is also the
successor of fairseq.</p>
</blockquote>
<p>So far I realized there're <code>API</code> changes (for example, model loading from checkpoint), and the addition of new multimodal such as <code>Seamless communication</code>,
but why abandon a well-established framework and start from scratch?</p>
","language-model"
"77645239","Adding Conversation Memory to Xenova/LaMini-T5-61M Browser-based Model in JS","2023-12-12 10:13:00","","0","40","<huggingface-transformers><langchain><onnx><large-language-model><language-model>","<p>I'm currently working with the browser-based model in JavaScript, specifically 'text2text-generation' by Xenova/LaMini-T5-61M. My goal is to implement conversation memory functionality using Langchain. Could someone provide guidance or code examples on how to integrate Langchain for conversation memory in this context?</p>
","language-model"
"77614620","specify task_type for embeddings in Vertex AI","2023-12-06 16:01:14","","2","463","<google-cloud-platform><nlp><google-cloud-vertex-ai><language-model><openaiembeddings>","<p>Has someone tried the last update of GCP TextEmbeddingInput that allows to specify the task_type of your application? Theoretically it should allows you to use different fine tuned models to generate embeddings but I cannot see any difference between embeddings generated for different task_type.</p>
<pre><code>from vertexai.language_models import TextEmbeddingModel
from vertexai.language_models import TextEmbeddingInput

def text_embedding():
    &quot;&quot;&quot;Text embedding with a Large Language Model.&quot;&quot;&quot;
    model = TextEmbeddingModel.from_pretrained(&quot;textembedding-gecko-multilingual@001&quot;)
    embeddings = model.get_embeddings([&quot;La vita è un viaggio, ma non è solo un percorso da seguire. È un'esperienza che ci forma e ci cambia. È un'opportunità per imparare e crescere, per affrontare le sfide e superare gli ostacoli. È un'occasione per conoscere noi stessi e gli altri, per creare legami e costruire relazioni.&quot;])
    return embeddings

def text_embedding2():
    &quot;&quot;&quot;Text embedding with a Large Language Model.&quot;&quot;&quot;
    model = TextEmbeddingModel.from_pretrained(&quot;textembedding-gecko-multilingual@001&quot;)
    text = TextEmbeddingInput(text=&quot;La vita è un viaggio, ma non è solo un percorso da seguire. È un'esperienza che ci forma e ci cambia. È un'opportunità per imparare e crescere, per affrontare le sfide e superare gli ostacoli. È un'occasione per conoscere noi stessi e gli altri, per creare legami e costruire relazioni.&quot;, task_type='CLUSTERING')
    embeddings = model.get_embeddings([text])
    return embeddings

text_embedding2()[0].values == text_embedding()[0].values
</code></pre>
","language-model"
"76964298","Why do unmasked tokens of a sequence change when passed through a language model?","2023-08-23 19:13:20","","0","39","<machine-learning><language-model>","<p>Why passing a sequence of tokens, say <code>[&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;]</code> through a masked language model without any masking does not result in the same sequence being output when you select the highest probability tokens from the output model logits, i.e., <code>tokenizer.decode(softmax(logits).argmax()) ≠ [&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;]</code>?</p>
<p>Is this just a byproduct of the fact that the model may not have converged yet? I understand that when a model has just been initialized the embeddings are random and have nothing to do with the semantics of the vocab but after a few epochs I would expect the model to be able to retrieve the unmasked input sequence perfectly.</p>
","language-model"
"76882664","Why do we add |V| in the denominator in the Add-One smoothing for n-gram language models?","2023-08-11 10:21:45","76889108","1","410","<nlp><smoothing><language-model>","<p>In NLP when we use Laplace(Add-one) smoothing technique we assume that the every word is seen one more time than the actual count and the formula is like this</p>
<p><a href=""https://i.sstatic.net/1tyTn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/1tyTn.png"" alt=""formula for add one smoothing"" /></a></p>
<p>where V is the size of the vocabulary. My question is why do we add V when we are only considering the count of the previous word.</p>
<p>I only have a rough idea that every word is incremented by one so we have to normalize it by V time but I still don't understand it properly. As I said we are only considering the count of previous word right so why don't just add 1 to it.</p>
<p>I also saw that if we add V then the addition of all bigrams will be 1 which is what it should be. But is there any other explanation of why V?</p>
","language-model"
"76827550","How to vectorize text data in Pandas.DataFrame and then one_hot encoode it ""inside"" the model","2023-08-03 11:15:02","","0","240","<tensorflow><nlp><one-hot-encoding><language-model>","<p>I try to implement sequence model (trained to predict next word) built on one-hot encoded vector sequences. My custom one-hot encoder works well. But just as exercise I want to do all things with tensorflow (inspired by Deep Learning with Python, chapter 11, which I can reproduce, but not with my data. One difference: in book tensorflow.dataset is used, not DataFrame). Let we have input data 'df':</p>
<pre><code>df = pd.DataFrame(data=[['I', 'have', 'an', 'idea','xyz'],
                         ['This', 'idea', 'is', 'awesome', 'asd']],
                   columns=['cell_0_0','cell_0_1','cell_1_0','cell_1_1','next_word'])
</code></pre>
<p>First of all:</p>
<pre><code>import tensorflow as tf
from tensorflow import keras 
from tensorflow.keras.layers import Input, Dense, LSTM, TextVectorization
from tensorflow.keras.optimizers import RMSprop
import numpy as np
import pandas as pd
</code></pre>
<p>Let's prepare data for the model.</p>
<ol>
<li>Get text corpus and vocabulary:</li>
</ol>
<pre><code>Corpus = df.to_numpy().flatten().tolist()
Vocab  = sorted(list(set(Corpus)))
Vocab.insert(0, '[UNK]')
Vocab.insert(0, '')
print(Vocab)
</code></pre>
<p>Output: ['', '[UNK]', 'I', 'This', 'an', 'asd', 'awesome', 'have', 'idea', 'is', 'xyz']</p>
<ol start=""2"">
<li>Split 'df' into X and Y. As I want to use tensorflow.TextVoctorizer, X should be a vector of text, i.e. the dimension of X should be 2 (or 1?):</li>
</ol>
<pre><code>X = [[' '.join(Row[:-1])] for Row in df.to_numpy()]
Y = [[Row[-1]] for Row in df.to_numpy()]
</code></pre>
<ol start=""3"">
<li>Then vectorize input data:</li>
</ol>
<pre><code>X_vectorizer = TextVectorization(max_tokens=len(Vocab), output_mode=&quot;int&quot;, output_sequence_length=df.shape[1]-1, vocabulary=Vocab, standardize=None)
Y_vectorizer = TextVectorization(max_tokens=len(Vocab), output_mode=&quot;int&quot;, output_sequence_length=1, vocabulary=Vocab, standardize=None)

X_vectorized = X_text_vectorization(X)
Y_vectorized = Y_text_vectorization(Y)
print(X_vectorized.shape, Y_vectorized.shape)
</code></pre>
<p>Output: (2, 4) (2, 1)</p>
<pre><code>print(X_vectorized[0], Y_vectorized[0])
</code></pre>
<p>Output: tf.Tensor([2 7 4 8], shape=(4,), dtype=int64) tf.Tensor([10], shape=(1,), dtype=int64)</p>
<p>So data is correctly vectorized.</p>
<p>Then I build the model:</p>
<pre><code>inputs = keras.Input(shape=(None,), dtype=&quot;int64&quot;, name=&quot;input_layer&quot;)
embedded = tf.one_hot(inputs, depth=len(Vocab))
layer1 = LSTM(32)(embedded)
outputs = Dense(len(Vocab), activation='softmax')(layer1)

model = keras.Model(inputs, outputs)
model.compile(loss='categorical_crossentropy', optimizer='RMSprop')
model.fit(x=X_vectorized.numpy()
         ,y=Y_vectorized
         ,batch_size=256
         ,epochs=55
         )
</code></pre>
<p>Please, correct the built model or/and parameters in fit method. As a current version generates ValueError: Shapes (None, 1) and (None, 11) are incompatible.
Is such 'dataflow' is correct or is it a wrong approach? Should data be vectorized and the one_hot encoded right in a model?</p>
","language-model"
"76762585","With a HuggingFace trainer, how do I show the training loss versus the eval data set?","2023-07-25 12:10:59","","0","375","<language-model><huggingface-trainer>","<p>I'm running:</p>
<pre><code>#original training script

trainer = transformers.Trainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=test_dataset, #turn on the eval dataset for comparisons
    args=transformers.TrainingArguments(
        num_train_epochs=2,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=1,
        warmup_ratio=0.05,
        max_steps=20,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=1,
        output_dir=&quot;outputs&quot;,
        optim=&quot;paged_adamw_8bit&quot;,
        lr_scheduler_type='cosine',
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
</code></pre>
<p>I'm not 100% clear, but I think the loss shown is versus the training dataset versus the eval dataset... <a href=""https://i.sstatic.net/kqVQM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kqVQM.png"" alt=""training steps and losses"" /></a></p>
<p>How do I show losses versus eval (and training set too, ideally)?</p>
<p>I would have expected adding eval_dataset was enough...</p>
","language-model"
"76708250","GPT4All Metal Library Conflict during Embedding on M1 Mac","2023-07-17 21:42:08","","2","527","<metal><langchain><language-model><gpt4all>","<p>I am trying to run GPT4All's embedding model on my M1 Macbook with the following code:</p>
<pre><code>import json
import numpy as np
from gpt4all import GPT4All, Embed4All

# Load the cleaned JSON data
with open('coursesclean.json') as file:
    data = json.load(file)

# Create an index and embeddings array
index = {}
embeddings = []

# Iterate over each course
for course_code, course_info in data.items():
    course_name = course_info['course_name']
    course_desc = course_info['course_desc']

    text = f&quot;{course_name} {course_desc}&quot;

    embedder = Embed4All()
    embeddings_response = embedder.embed(text)
    course_embeddings = np.array(embeddings_response)

    # Store the embeddings in the array
    embeddings.append(course_embeddings)

    # Index the embeddings
    index[course_code] = len(embeddings) - 1

# Convert the embeddings array to a NumPy array
embeddings_array = np.stack(embeddings)

save the index and embeddings array to NumPy files
np.save('embeddings.npy', embeddings_array)
with open('index.json', 'w') as file:
    json.dump(index, file, indent=2)
</code></pre>
<p>However, I seem to be running into a Metal library conflict that puts me in an infinite loop of the Found model file statement.</p>
<pre><code>Found model file at  /Users/MY_USERNAME/.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin
objc[42338]: Class GGMLMetalClass is implemented in both /opt/homebrew/lib/python3.11/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libreplit-mainline-metal.dylib (0x11eb0c208) and /opt/homebrew/lib/python3.11/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/libllamamodel-mainline-metal.dylib (0x11ef38208). One of the two will be used. Which one is undefined.
Found model file at  /Users/MY_USERNAME//.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin
Found model file at  /Users/MY_USERNAME//.cache/gpt4all/ggml-all-MiniLM-L6-v2-f16.bin
...
</code></pre>
<p>I've tried reinstalling GPT4All, cleared the model from cache and tried through Langchain but I'm having the same issue.</p>
","language-model"
"76677150","Python-based way to extract text from scientific/academic paper for a language model","2023-07-13 07:58:41","","1","436","<python><pdf><text-extraction><language-model>","<p>I am looking for a method to extract only the core text of a scientific paper. The paper is structured in paragraphs and I only want to cover the text without any mail-adress, websites, tables or pictures. My purpose is to create a clean txt file for a language model.</p>
<p>Which methods are available to filter data (i.e. font size, searching for keywords or inlcude Spacy etc.)?</p>
<p>Thank you in advance!</p>
<pre><code>`from langchain.document_loaders import PyPDFLoader # for loading the pdf
import glob
import os

# Path to pdf folder
folder_path = &quot;C:/Users/faenkaya/Desktop/Language Models/documents/Scientific Data eng&quot;

# Path to output
output_file = &quot;C:/Users/faenkaya/Desktop/Language Models/documents/Scientific Data eng/Full_text.txt&quot;

# Write the txt file
with open(output_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as file:
    # Loop for each pdf
    for file_path in glob.glob(os.path.join(folder_path, &quot;*.pdf&quot;)):

        # Open PDF in read-mode
        with open(file_path, &quot;rb&quot;) as pdf_file:
            loader = PyPDFLoader(file_path)
            pages = loader.load_and_split()
            text = &quot;&quot;
            for i in range(len(pages)):
                text += pages[i].page_content
                text += &quot;\n&quot;
            print(file_path)
            # Write text to txt file
            file.write(text)
            file.write(&quot;\n&quot;)`
</code></pre>
","language-model"
"76671494","How to get the embedding of any vocabulary token in GPT?","2023-07-12 14:09:00","76672105","0","965","<machine-learning><pytorch><nlp><huggingface-transformers><language-model>","<p>I have a GPT model</p>
<pre><code>model = BioGptForCausalLM.from_pretrained(&quot;microsoft/biogpt&quot;).to(device)
</code></pre>
<p>When I send my batch to it I can get the logits and the hidden states:</p>
<pre><code>out = model(batch[&quot;input_ids&quot;].to(device), output_hidden_states=True, return_dict=True)
print(out.keys())
&gt;&gt;&gt; odict_keys(['logits', 'past_key_values', 'hidden_states'])
</code></pre>
<p>The logits have shape of</p>
<pre><code>torch.Size([2, 1024, 42386]) # batch of size 2, sequence length = 1024, vocab size = 42386
</code></pre>
<p>Corresponding to <code>(batch, seq_length, vocab_length)</code>. If I understand correctly, for each token in the sequence, the logits is a vector of size <code>vocab_length</code> which points the model to which token from the vocabulary to use, after passing it to softmax. I believe that each of these tokens should have an embedding.</p>
<p>From my <a href=""https://stackoverflow.com/questions/76655508/how-to-get-the-vector-embedding-of-a-token-in-gpt"">previous question</a> I found how to get the embeddings of each <strong>sequence token</strong> (shape <code>[2,1024,1024]</code> in my setting). But, how can I get the embeddings of each token in <strong>the vocabulary</strong> of the model? This should be of size <code>[2, 1024, 42386, 1024]</code> (BioGPT has a hidden size of length <code>1024</code>).</p>
<p>I'm mainly interested in just a few special tokens (e.g., indices 1,2,6,112 out of the 42386).</p>
","language-model"
"76655508","How to get the vector embedding of a token in GPT?","2023-07-10 16:06:41","76661403","0","537","<machine-learning><pytorch><huggingface-transformers><language-model>","<p>I have a GPT model</p>
<pre><code>model = BioGptForCausalLM.from_pretrained(&quot;microsoft/biogpt&quot;).to(device)
</code></pre>
<p>When I send my batch to it I can get the logits and the hidden states:</p>
<pre><code>out = model(batch[&quot;input_ids&quot;].to(device), output_hidden_states=True, return_dict=True)
print(out.keys())
&gt;&gt;&gt; odict_keys(['logits', 'past_key_values', 'hidden_states'])
</code></pre>
<p>The logits have shape of</p>
<pre><code>torch.Size([2, 1024, 42386])
</code></pre>
<p>Corresponding to <code>(batch, seq_length, vocab_length)</code></p>
<p>How can I get the vector embedding of the first (i.e., <code>dim=0</code>) token in the last layer (i.e., after the fully connected layer)? I believe it should be of size <code>[2, 1024, 1024]</code></p>
<p>From <a href=""https://huggingface.co/transformers/v3.1.0/model_doc/gpt2.html"" rel=""nofollow noreferrer"">here</a> it seems like it should be under <code>last_hidden_state</code>, but I can't seem to generate it. <code>out.hidden_states</code> seems to be a tuple of length <code>25</code>, where each is of dimension <code>[2, 1024, 1024]</code>. I'm wondering if the last one is the one I'm looking for, but I'm not sure.</p>
","language-model"
"76595914","How to use a biomedical model from Huggingface to get text embeddings?","2023-07-01 17:51:03","","0","463","<machine-learning><pytorch><word-embedding><huggingface><language-model>","<p>I have biomedical text that I'm trying to get the embeddings for using a biomedical transformer:</p>
<pre><code>my_text = [&quot;Chocolate has a history of human consumption tracing back to 400 AD and is rich in polyphenols such as catechins, anthocyanidins, and pro anthocyanidins. As chocolate and cocoa product consumption, along with interest in them as functional foods, increases worldwide, there is a need to systematically and critically appraise the available clinical evidence on their health effects. A systematic search was conducted on electronic databases such as MEDLINE, EMBASE, and Cochrane Central Register of Controlled Trials (CENTRAL) using a search strategy and keywords. Among the many health effects assessed on several outcomes (including skin, cardiovascular, anthropometric, cognitive, and quality of life), we found that compared to controls, chocolate or cocoa product consumption significantly improved lipid profiles (triglycerides), while the effects of chocolate on all other outcome parameters were not significantly different. In conclusion, low-to-moderate-quality evidence with short duration of research (majority 4-6 weeks) showed no significant difference between the effects of chocolate and control groups on parameters related to skin, blood pressure, lipid profile, cognitive function, anthropometry, blood glucose, and quality of life regardless of form, dose, and duration among healthy individuals. It was generally well accepted by study subjects, with gastrointestinal disturbances and unpalatability being the most reported concerns.&quot;]
</code></pre>
<p>I found that I can use <a href=""https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"" rel=""nofollow noreferrer"">sentence-transformers</a> to get embeddings for text pretty easily (I assume I can just average the sentence embeddings over all sentences). I found this <a href=""https://stackoverflow.com/questions/65494850/how-to-convert-text-to-word-embeddings-using-berts-pretrained-model-faster"">SO answer</a> that use the same framework and seems like applicable with any (unless I'm wrong) biomedical model (e.g., <a href=""https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"" rel=""nofollow noreferrer"">this</a>):</p>
<pre><code>from sentence_transformers import SentenceTransformer
sbert_model = SentenceTransformer('microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext')
document_embeddings = sbert_model.encode(pd.Series(['hello', 'cell type', 'protein']))
document_embeddings 
</code></pre>
<p>But when I run the code I get</p>
<pre><code>No sentence-transformers model found with name /home/user/.cache/torch/sentence_transformers/microsoft_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext. Creating a new one with MEAN pooling.
Some weights of the model checkpoint at /home/user/.cache/torch/sentence_transformers/microsoft_BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
</code></pre>
<p>If I understand correctly this means that some of the weights from the model are either not used or randomly initialized, which means that I can't trust these generated embeddings.</p>
<p>What is the correct way to do this, if say, I want to use that <code>PubMedBERT</code> model, or another one like <a href=""https://huggingface.co/dmis-lab/biobert-v1.1"" rel=""nofollow noreferrer"">BioBERT</a>?</p>
","language-model"
"76552786","Error while installing lmql[hf] using pip: ""No matching distribution found for lmql[hf]","2023-06-25 23:25:19","","0","271","<python><pip><bert-language-model><language-model><large-language-model>","<p>I am trying to install lmql[hf] using the pip package manager in order to set up a local LMQL playground. Following the <a href=""https://docs.lmql.ai/en/stable/language/hf.html"" rel=""nofollow noreferrer"">documentation</a>, I ran the command <code>pip install lmql[hf]</code>.</p>
<p>However, I encountered the following error:</p>
<pre><code>ERROR: Ignored the following versions that require a different python version: 0.0.2 Requires-Python &gt;=3.9; 0.0.2.1 Requires-Python &gt;=3.9; 0.0.3.0 Requires-Python &gt;=3.10; 0.0.3.1 Requires-Python &gt;=3.10; 0.0.4 Requires-Python &gt;=3.10; 0.0.4.1 Requires-Python &gt;=3.10; 0.0.4.2 Requires-Python &gt;=3.10; 0.0.5 Requires-Python &gt;=3.10; 0.0.5.1 Requires-Python &gt;=3.10; 0.0.6 Requires-Python &gt;=3.10; 0.0.6.1 Requires-Python &gt;=3.10; 0.0.6.2 Requires-Python &gt;=3.10; 0.0.6.3 Requires-Python &gt;=3.10; 0.0.6.4 Requires-Python &gt;=3.10
ERROR: Could not find a version that satisfies the requirement lmql[hf] (from versions: none)
ERROR: No matching distribution found for lmql[hf]
</code></pre>
<p>I have ensured that my pip package manager is up to date by running <code>pip install --upgrade pip</code>, but the error persists.</p>
<p>Python 3.11</p>
<p>pip 23.1.2</p>
<p>Any guidance or assistance in resolving this installation issue would be greatly appreciated. Thank you!</p>
<p>I expect to run lmql with a local language model e.g. <code>bert-base-uncased</code> loacally</p>
","language-model"
"76546004","OpenAI Fine-tuning API: Why would I use LlamaIndex or LangChain instead of fine-tuning a model?","2023-06-24 12:12:32","76558875","3","4350","<openai-api><langchain><chatgpt-api><language-model><llama-index>","<p>I'm just getting started with working with LLMs, particularly OpenAIs and other OSS models. There are a lot of guides on using LlamaIndex to create a store of all your documents and then query on them. I tried it out with a few sample documents, but discovered that each query gets super expensive quickly. I think I used a 50-page PDF document, and a summarization query cost me around 1.5USD per query. I see there's a lot of tokens being sent across, so I'm assuming it's sending the entire document for every query. Given that someone might want to use thousands of millions of records, I can't see how something like LlamaIndex can really be that useful in a cost-effective manner.</p>
<p>On the other hand, I see OpenAI allows you to train a ChatGPT model. Wouldn't that, or using other custom trained LLMs, be much cheaper and more effective to query over your own data? Why would I ever want to set up LlamaIndex?</p>
","language-model"
"76509562","ArrowInvalid: Column 4 named input_ids expected length 1000 but got length 328","2023-06-19 19:27:58","","3","2635","<python><machine-learning><language-model>","<pre><code># Formatting
block_size = 128  # or any number suitable to your context


def group_texts(examples):
    # Concatenate all 'input_ids'
    concatenated_examples = sum(examples[&quot;input_ids&quot;], [])
    total_length = len(concatenated_examples)
    # Organize into sequences of fixed length
    sequences = [
        concatenated_examples[i : i + block_size]
        for i in range(0, total_length, block_size)
    ]
    result = {
        &quot;input_ids&quot;: sequences,
        # Shift the labels for CLM
        &quot;labels&quot;: [sequence[1:] + [tokenizer.eos_token_id] for sequence in sequences],
    }
    return result


tokenized_dataset = tokenized_dataset.map(
    group_texts,
    batched=True,
    batch_size=1000,  # or any number suitable to your context
</code></pre>
<p>I am not getting what the block_size and the batch_size refers to?</p>
","language-model"
"76451205","Difference between Instruction Tuning vs Non Instruction Tuning Large Language Models","2023-06-11 15:37:08","","23","21200","<language-model><fine-tuning><large-language-model>","<p>What is the difference between instruction tuning and normal fine-tuning for large language models?</p>
<p>Also the instruction-tuning I'm referring to isn't the in-context/prompt one.</p>
<p>All the recent papers about fine-tuning seem to be about instruction tuning.</p>
<p>I have looked at a couple of papers about fine-tuning/instruction tuning (e.g. FLAN) and none really describe the difference between instruction tuning and the alternatives (whatever the alternatives are).</p>
<p>I understand instruction-tuning is a form of fine-tuning but with an instruction dataset. But are all datasets not instruction datasets? What other kinds are there?</p>
","language-model"
"76416680","How to structure data for question-answering task to fine-tune a model with Huggingface run_qa.py example?","2023-06-06 16:30:59","76425973","2","981","<python><nlp><huggingface-transformers><language-model><nlp-question-answering>","
<pre class=""lang-python prettyprint-override""><code>import sagemaker
import boto3
from sagemaker.huggingface import HuggingFace

try:
    role = sagemaker.get_execution_role()
except ValueError:
    iam = boto3.client('iam')
    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']
        
hyperparameters = {
    'model_name_or_path':'t5-base',
    'output_dir':'/opt/ml/model'
    # add your remaining hyperparameters
    # more info here https://github.com/huggingface/transformers/tree/v4.26.0/examples/pytorch/question-answering
}

# git configuration to download our fine-tuning script
git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.26.0'}

# creates Hugging Face estimator
huggingface_estimator = HuggingFace(
    entry_point='run_qa.py',
    source_dir='./examples/pytorch/question-answering',
    instance_type='ml.p3.2xlarge',
    instance_count=1,
    role=role,
    git_config=git_config,
    transformers_version='4.26.0',
    pytorch_version='1.13.1',
    py_version='py39',
    hyperparameters = hyperparameters
)

# starting the train job
huggingface_estimator.fit()
</code></pre>
<p>Given the above script (<code>launch_training.py</code>) which can be found here: <a href=""https://huggingface.co/t5-base"" rel=""nofollow noreferrer"">https://huggingface.co/t5-base</a>, how should my data be structured for a generative question-answering task?</p>
<p><strong>For context</strong>: I am training T5 on some synthetic company text data so that I can then prompt it with questions such as &quot;How can CompanyX improve sales?&quot; or &quot;How can CompanyX reduce the turnover rate?&quot;</p>
<p>I have tried formatting my data as question-answer pairs, e.g. {&quot;question&quot;: &quot;How can CompanyX improve the performance of their marketing campaigns?&quot;, &quot;answer&quot;: &quot;The recent marketing campaign of CompanyX attracted a 20% increase in new customers. It suggests that if CompanyX focuses on customer-centric strategies and amplifies their digital marketing efforts, they might achieve even better results.&quot;} but this gives <code>ValueError: Need either a dataset name or a training/validation file</code></p>
<p>I am passing an S3 URI to <code>huggingface_estimator.fit()</code>, namely <code>huggingface_estimator.fit({&quot;train_data_uri&quot;: &quot;s3://fine-tuning/q-a_pairs.json&quot;})</code></p>
","language-model"
"76384301","Starcoder finetuning - How to select the GPU and how to estimate the time it will take to finetune","2023-06-01 17:22:14","","7","664","<deep-learning><pytorch><huggingface><language-model><large-language-model>","<p>I'd like to finetune Starcoder (<a href=""https://huggingface.co/bigcode/starcoder"" rel=""noreferrer"">https://huggingface.co/bigcode/starcoder</a>) on my dataset and on a GCP VM instance.</p>
<p>It's says in the documentation that for training the model, they used 512 Tesla A100 GPUs and it took 24 days.</p>
<p>I also saw the model (.bin) files in files section of huggingFace (<a href=""https://huggingface.co/bigcode/starcoder/tree/main"" rel=""noreferrer"">https://huggingface.co/bigcode/starcoder/tree/main</a>)</p>
<p>The total size of the model is ~64GB</p>
<p>Based on all this information,</p>
<ol>
<li>How do I decide which GPU is best for finetuning on my dataset ?</li>
<li>How to estimate the time it will take finetune ? (based on assumptions on parameters like epoch=1, for instance)</li>
<li>Are there any other factors that are considered to choose hardware / calculate time ?</li>
</ol>
","language-model"
"76373220","Fine-tuning a pre-trained LLM for question-answering","2023-05-31 11:55:10","","3","7609","<huggingface-transformers><huggingface><language-model><fine-tuning><text-generation>","<h3>Objective</h3>
<p>My goal is to fine-tune a pre-trained LLM on a dataset about Manchester United's (MU's) 2021/22 season (they had a poor season). I want to be able to prompt the fine-tuned model with questions such as &quot;How can MU improve?&quot;, or &quot;What are MU's biggest weaknesses?&quot;. The ideal responses would be insightful/logical and +100 words</p>
<h3>Data</h3>
<ul>
<li>I will simply use text from the relevant wiki page as my data: <a href=""https://en.wikipedia.org/wiki/2021%E2%80%9322_Manchester_United_F.C._season"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/2021%E2%80%9322_Manchester_United_F.C._season</a></li>
<li>How should I structure my data? Should it be a list dictionaries where the keys are the questions and the values are the answers (i.e. a list of question-answer pairs), or a long string containing all the text data (for context), or a combination of both?</li>
</ul>
<h3>Notes</h3>
<ul>
<li>I have mainly been experimenting with variations of Google's T5 (e.g.: <a href=""https://huggingface.co/t5-base"" rel=""nofollow noreferrer"">https://huggingface.co/t5-base</a>) which I have imported from the Hugging Face Transformers library</li>
<li>So far I have only fine-tuned the model on a list of 30 dictionaries (question-answer pairs), e.g.: {&quot;question&quot;: &quot;How could Manchester United improve their consistency in the Premier League next season?&quot;, &quot;answer&quot;: &quot; To improve consistency, Manchester United could focus on strengthening their squad depth to cope with injuries and fatigue throughout the season. Tactical adjustments could also be explored to deal with teams of different strengths and styles.&quot;}</li>
<li>Use of this small dataset (list of 30 dictionaries) has given poor results</li>
</ul>
<h3>Further Questions and Notes</h3>
<ul>
<li>Other than increasing the size of my dataset, is my approach sound?</li>
<li>What would you recommend as a minimum number of dictionaries to train/fine-tune the model on?</li>
<li>I am also aware that I can tune the hyperparameters to improve performance, but for now I am more concerned about my general approach being logical</li>
</ul>
","language-model"
"76285124","How can I speed up a QA Langchain using load_qa_with_sources_chain?","2023-05-18 23:37:54","","1","8075","<python><python-3.x><language-model><langchain><py-langchain>","<p>I am currently running a QA model using <code>load_qa_with_sources_chain()</code>. However, when I run it with three chunks of each up to 10,000 tokens, it takes about 35s to return an answer. I would like to speed this up.</p>
<p>Can somebody explain what influences the speed of the function and if there is any way to reduce the time to output. If that's not possible, what other changes could you undertake to increase the speed of QA with sources?</p>
<p>I tried changing the size of the text chunks but that did not have a significant effect. I am using the map_reduce chain. I am using Python3.10.</p>
","language-model"
"76186890","Why is perplexity calculation giving different results for the same input?","2023-05-06 02:41:33","76433107","1","621","<pytorch><nlp><huggingface-transformers><language-model><perplexity>","<p>I'm following <a href=""https://huggingface.co/docs/transformers/perplexity"" rel=""nofollow noreferrer"">Huggingface</a> doc on calculating the perplexity of fixed-length models. I'm trying to verify that the formula works for various strings and I'm getting odd behavior. In particular, they mention</p>
<blockquote>
<p>We don’t want the log-likelihood for the tokens we’re just treating as context to be included in our loss, so we can set these targets to -100 so that they are ignored</p>
</blockquote>
<p>So given 2 different contexts but the same remaining tokens, the formula should return the same perplexity. However, it does not:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch
tokenizer = T5Tokenizer.from_pretrained(&quot;t5-small&quot;)
model = T5ForConditionalGeneration.from_pretrained(&quot;t5-small&quot;)

context_1 = 'here is some context_1 and some more stuff'
context_2 = 'here is some context and some more stuff and more stuff aspodkaspd'
answer_1 = 'this is not the answer'

input_ids_wrong = tokenizer(context_1 + answer_1, return_tensors=&quot;pt&quot;).input_ids
input_ids_correct = tokenizer(context_2 + answer_1, return_tensors=&quot;pt&quot;).input_ids
context_1_tokens_length = len(tokenizer(context_1, return_tensors=&quot;pt&quot;).input_ids[0])
context_2_tokens_length = len(tokenizer(context_2, return_tensors=&quot;pt&quot;).input_ids[0])

target_ids_wrong = input_ids_wrong.clone()
target_ids_correct = input_ids_correct.clone()

target_ids_wrong[:, :context_1_tokens_length] = -100 
target_ids_correct[:, :context_2_tokens_length] = -100 

print('target_ids_wrong', target_ids_wrong)
print('target_ids_correct', target_ids_correct)

with torch.no_grad():
    outputs_wrong = model(input_ids_wrong, labels=target_ids_wrong)
    outputs_correct = model(input_ids_correct, labels=target_ids_correct)
    
    neg_log_likelihood_wrong = outputs_wrong.loss
    neg_log_likelihood_correct = outputs_correct.loss

    ppl_wrong = torch.exp(neg_log_likelihood_wrong)
    ppl_correct = torch.exp(neg_log_likelihood_correct)
    print('ppl_wrong', ppl_wrong)
    print('ppl_correct', ppl_correct)
</code></pre>
<p>Output:</p>
<pre><code>    target_ids_wrong tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,   19,
               59,    8, 1525,    1]])
    target_ids_correct tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,
             -100, -100, -100, -100, -100, -100,   19,   59,    8, 1525,    1]])
    ppl_wrong tensor(9.0377)
    ppl_correct tensor(21.1208)
</code></pre>
<p>I tried this with other models as well (e.g., <code>gpt2</code> and <code>sshleifer/tiny-gpt2</code>) and got the same odd behavior. From the <a href=""https://huggingface.co/docs/transformers/model_doc/t5"" rel=""nofollow noreferrer"">T5 doc</a> they wrote</p>
<blockquote>
<p>we must make sure that padding token id’s of the labels are not taken into account by the loss function. In PyTorch and Tensorflow, this can be done by replacing them with -100, which is the ignore_index of the CrossEntropyLoss.
So I don't understand why it takes the pad token into account. They also wrote in the same link
which for T5 is equal to 0 (i.e. the id of the pad token)</p>
</blockquote>
<p>So I tried replacing the <code>-100</code> with <code>0</code> and actually a got different perplexity score (still different than each other, but different than the <code>-100</code>). Which makes me think they don't actually ignore the <code>-100</code> token for some reason.</p>
<p>Am I missing something?</p>
","language-model"
"76186015","How to denoise text using T5?","2023-05-05 21:24:02","","0","218","<pytorch><nlp><huggingface-transformers><huggingface><language-model>","<p>I'm trying to denoise text using a T5 model following <a href=""https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/t5#transformers.T5ForConditionalGeneration"" rel=""nofollow noreferrer"">the Huggingface doc:</a></p>
<pre><code>from transformers import T5Tokenizer, T5ForConditionalGeneration
tokenizer = T5Tokenizer.from_pretrained(&quot;t5-small&quot;)
model = T5ForConditionalGeneration.from_pretrained(&quot;t5-small&quot;)

input_ids = tokenizer(&quot;The &lt;extra_id_0&gt; walks in &lt;extra_id_1&gt; park&quot;, return_tensors=&quot;pt&quot;).input_ids
labels = tokenizer(&quot;&lt;extra_id_0&gt; cute dog &lt;extra_id_1&gt; the &lt;extra_id_2&gt;&quot;, return_tensors=&quot;pt&quot;).input_ids

# the forward function automatically creates the correct decoder_input_ids
loss = model(input_ids=input_ids, labels=labels).loss
loss.item()
</code></pre>
<p>But I can't figure out how to get the actual text that corresponds to the masked input. They only show how to get the loss and mention</p>
<blockquote>
<p>the forward function automatically creates the correct
decoder_input_ids</p>
</blockquote>
<p>I tried the following:</p>
<pre><code>from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained(&quot;t5-small&quot;)
model = T5ForConditionalGeneration.from_pretrained(&quot;t5-small&quot;)

input_ids = tokenizer(&quot;The &lt;extra_id_0&gt; walks in &lt;extra_id_1&gt; park&quot;, return_tensors=&quot;pt&quot;).input_ids
labels = tokenizer(&quot;&lt;extra_id_0&gt; cute dog &lt;extra_id_1&gt; the &lt;extra_id_2&gt;&quot;, return_tensors=&quot;pt&quot;).input_ids
outputs = model(input_ids=input_ids, labels=labels)
loss = outputs.loss
logits = outputs.logits
tokenizer.batch_decode(logits.argmax(-1))
</code></pre>
<p>But the output doesn't make sense:</p>
<pre><code>['&lt;extra_id_0&gt; park park&lt;extra_id_1&gt; the&lt;extra_id_2&gt; park']
</code></pre>
<p><strong>I don't care for the loss, nor do I have labels in my setting. I just have text with masked tokens that I need to fill:</strong></p>
<pre><code>my_masked_text = [
&quot;The kid went to the [MASK]&quot;,
&quot;The dog likes [MASK] and also [MASK]&quot;
]
</code></pre>
","language-model"
"76177216","How is scaled_dot_product_attention meant to be used with cached keys/values in causal LM?","2023-05-04 20:39:21","","5","1758","<pytorch><language-model><self-attention>","<p>I'm implementing a transformer and I have everything working, including attention using the new <code>scaled_dot_product_attention</code> from PyTorch 2.0. I'll only be doing causal attention, however, so it seems like it makes sense to use the <code>is_causal=True</code> flag for efficiency. This also works as I'd expect as long as the k, v and q tensors have the same size.</p>
<p>But I'm not sure how to pass along past (cached) keys/values to the function in this mode. If the k, v tensors are wider than q, I need a rectangular mask as wide as k/v and as tall as q, with the upper <em>right</em> triangle masked out. All is well if I construct such a mask myself and pass it to the function. I get behavior similar to typical causal attention, where past tokens are attended to fully and new tokens (for which there are queries) are attended causally.</p>
<p>According to <a href=""https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html"" rel=""noreferrer"">the documentation</a>, though, <code>is_causal=True</code> is equivalent to using a mask built with:</p>
<pre><code>attn_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)
</code></pre>
<p>Where L and S are the query and key/value lengths, respectively. This has all but the lower <em>left</em> triangular portion masked out, which attends partially to past tokens and not at all to new tokens. Is this causal mode just not suitable for my use case, or am I missing something?</p>
<p>Suppose I have the following tensors:</p>
<pre><code>q = torch.rand((1, n_heads, 3, head_dim))
k = torch.rand((1, n_heads, 6, head_dim))
v = torch.rand((1, n_heads, 6, head_dim))

attn_output = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, is_causal=True)
</code></pre>
<p>Where k and v are wider since they're concatenated onto the cached results of a previous inference pass. <code>scaled_dot_product_attention</code> applies the following mask:</p>
<pre><code>[[0, -inf, -inf, -inf, -inf, -inf]
 [0,    0, -inf, -inf, -inf, -inf]
 [0,    0,    0, -inf, -inf, -inf]]
</code></pre>
<p>But I would (maybe naively?) expect the attention operation to use a mask like this:</p>
<pre><code>[[0, 0, 0, 0, -inf, -inf]
 [0, 0, 0, 0,    0, -inf]
 [0, 0, 0, 0,    0,    0]]
</code></pre>
<p>Can I achieve this somehow with <code>scaled_dot_product_attention</code>, or am I maybe going about this all wrong? How am I meant to use the function with a cache of past keys/values?</p>
","language-model"
"76075666","Endless loop in a text generation script","2023-04-21 17:57:19","","0","99","<python><machine-learning><nltk><markov-chains><language-model>","<p>I am trying to make a simple text generator using the Bulgarian language but my code is stuck in an endless loop. Here is the code:</p>
<pre><code>from tokenization import tokenize_bulgarian_text
from nltk import bigrams, trigrams
from collections import Counter, defaultdict
import random

with open('IvanVazov1.txt', 'r', encoding='utf-8') as f:
    data = f.read()

# Tokenize text
tokenized_sentences = tokenize_bulgarian_text(data)
print(tokenized_sentences)

# Create a placeholder for model
model = defaultdict(lambda: defaultdict(lambda: 0))

# Count frequency of co-occurance  
for sentence in tokenized_sentences:
    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):
        print(&quot;Trigram:&quot;, (w1, w2, w3))
        model[(w1, w2)][w3] += 1
 
# Transform the counts to probabilities
for w1_w2 in model:
    total_count = float(sum(model[w1_w2].values()))
    for w3 in model[w1_w2]:
        model[w1_w2][w3] /= total_count
print(model)


# starting words
text = [&quot;беше&quot;]
sentence_finished = False
 
print(&quot;Starting words:&quot;, text)

while not sentence_finished:
    # select a random probability threshold  
    r = random.random()
    accumulator = .0
    for word in model[tuple(text[-2:])].keys():
        **# print statement never executes**
        print(f&quot;word: {word}, probability: {model[tuple(text[-2:])][word]}, accumulator: {accumulator}&quot;)
        accumulator += model[tuple(text[-2:])][word]
        # select words that are above the probability threshold
        if accumulator &gt;= r:
            text.append(word)
            break

    if text[-2:] == [None, None]:
        print(&quot;End of sentence.&quot;)
        sentence_finished = True
 
print (' '.join([t for t in text if t]))
</code></pre>
<p>Here is my tokenization.py file that I import:</p>
<pre><code>import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
import string
from spacy.lang.bg.stop_words import STOP_WORDS as bg_stopwords


def tokenize_bulgarian_text(text):
    extra_stopwords = {&quot;—&quot;, &quot;“&quot;, &quot;„&quot;, &quot;не&quot;, &quot;та&quot;, &quot;па&quot;}
    bg_stopwords.update(extra_stopwords)
    # Remove punctuation and lowercase all letters
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = text.lower()
    
    # Split the text into individual sentences
    sentences = sent_tokenize(text)

    # Tokenize and filter each sentence
    filtered_tokens = []
    for sentence in sentences:
        # Split the sentence into individual words or tokens
        tokens = word_tokenize(sentence)
        # Remove any stop words from the tokenized text
        filtered_sentence = [word for word in tokens if word not in bg_stopwords]
        filtered_tokens.append(filtered_sentence)

    return filtered_tokens
</code></pre>
<p>Also the text file I use for data:</p>
<pre><code>Нощта беше влажна и мрачна и браилските улици пустееха. Студената декемврийска мъгла, която обикновено пада покрай бреговете на Дунава, се беше напластила в една от главните улици на града и задушаваше с отровния си дъх последните минувачи, които бързаха да се приберат у дома си. 
</code></pre>
<p>From what I have gathered I think the tokenization is fine and the triagrams are correct. I tried using print sentences to debug the code. For some reason the print statement in the nested <strong>for loop</strong> never executes. New words are never appended to the text list. I copy pasted the code from a website and it works fine with <code>reuters.sents()</code> instead of my <code>tokenized_sentences</code>.</p>
","language-model"
"76070068","Not able to resolve TypeError: Transformer.forward() got an unexpected keyword argument 'labels'","2023-04-21 05:10:20","","1","255","<python><machine-learning><deep-learning><nlp><language-model>","<p>I am trying to implement the chapter 10 of NLP with transformers by lewis tunstall book. I am facing an error in this particular cell :</p>
<pre><code>    from transformers.optimization import get_scheduler 

    from accelerate import Accelerator

    set_seed(args.seed)

   # Accelerator

    accelerator = Accelerator()
    samples_per_step = accelerator.state.num_processes * args.train_batch_size

   # Logging

    #logger, tb_writer, run_name = setup_logging(project_name.split(&quot;/&quot;)[1])
    #logger.info(accelerator.state)

    # Load model and tokenizer

    #if accelerator.is_main_process:
    #hf_repo = Repository(&quot;./&quot;, clone_from=project_name, revision=run_name)
    #model = AutoModelForCausalLM.from_pretrained(&quot;./&quot;, gradient_checkpointing=True)
    #tokenizer = AutoTokenizer.from_pretrained(&quot;./&quot;)

   # Load dataset and dataloader

    dataset_name = 'transformersbook/codeparrot'
    train_dataloader, eval_dataloader = create_dataloaders(dataset_name)

   # Prepare the optimizer and learning rate scheduler

    optimizer = torch.optim.AdamW(get_grouped_params(model), lr=args.learning_rate)
    lr_scheduler = get_scheduler(name=args.lr_scheduler_type, optimizer=optimizer,
                             num_warmup_steps=args.num_warmup_steps,
                             num_training_steps=args.max_train_steps,)
    def get_lr():
        return optimizer.param_groups[0]['lr']

# Prepare everything with our `accelerator` (order of args is not important)

    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader)

# Train model

    model.train()
    completed_steps = 0
    for step, batch in enumerate(train_dataloader, start=1):
        loss = model(batch, labels=batch).loss
        log_metrics(step, {'lr': get_lr(), 'samples': step*samples_per_step,
                           'steps': completed_steps, 'loss/train': loss.item()})
        loss = loss / args.gradient_accumulation_steps
        accelerator.backward(loss)
        if step % args.gradient_accumulation_steps == 0:
            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            completed_steps += 1
        if step % args.save_checkpoint_steps == 0:
            logging.info('Evaluating and saving model checkpoint')
            eval_loss, perplexity = evaluate()
            log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})
            accelerator.wait_for_everyone()
            unwrapped_model = accelerator.unwrap_model(model)
        
            model.train()
        if completed_steps &gt;= args.max_train_steps:
            break

# Evaluate and save the last checkpoint

    logging.info('Evaluating and saving model after training')
    eval_loss, perplexity = evaluate()
    log_metrics(step, {'loss/eval': eval_loss, 'perplexity': perplexity})
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)

</code></pre>
<p>Details of error :</p>
<pre><code>---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Cell In[81], line 40
     38 completed_steps = 0
     39 for step, batch in enumerate(train_dataloader, start=1):
---&gt; 40     loss = model(batch, labels=batch).loss
     41     log_metrics(step, {'lr': get_lr(), 'samples': step*samples_per_step,
     42                        'steps': completed_steps, 'loss/train': loss.item()})
     43     loss = loss / args.gradient_accumulation_steps

File d:\Crimson Energy Experts\env\Lib\site-packages\torch\nn\modules\module.py:1501, in Module._call_impl(self, *args, **kwargs)
   1496 # If we don't have any hooks, we want to skip the rest of the logic in
   1497 # this function, and just call forward.
   1498 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks
   1499         or _global_backward_pre_hooks or _global_backward_hooks
   1500         or _global_forward_hooks or _global_forward_pre_hooks):
-&gt; 1501     return forward_call(*args, **kwargs)
   1502 # Do not call functions when jit is used
   1503 full_backward_hooks, non_full_backward_hooks = [], []


TypeError: Transformer.forward() got an unexpected keyword argument 'labels'
</code></pre>
","language-model"
"76048714","Finetuning a LM vs prompt-engineering an LLM","2023-04-18 20:15:18","76052203","3","2542","<language-model><roberta-language-model><roberta><gpt-4><large-language-model>","<p>Is it possible to finetune a much smaller language model like Roberta on say, a customer service dataset and get results as good as one might get with prompting GPT-4 with parts of the dataset?</p>
<p>Can a fine-tuned Roberta model learn to follow instructions in a conversational manner at least for a small domain like this?</p>
<p>Is there any paper or article that explores this issue empirically that I can check out?</p>
","language-model"
"75897180","Langchain Chatbot with Memory + Vector Database","2023-03-31 09:59:05","","3","1622","<nlp><chatbot><language-model><langchain>","<p>In Langchain, what is the suggested way to build a chatbot with memory and retrieval from a vector embedding database at the same time?</p>
<p>The examples in the <a href=""https://python.langchain.com/en/latest/modules/memory/how_to_guides.html"" rel=""nofollow noreferrer"">docs</a> add memory modules to chains that do not have a vector database. Related <a href=""https://github.com/hwchase17/langchain/issues/2185"" rel=""nofollow noreferrer"">issue</a>.</p>
","language-model"
"75791645","Cannot allocate memory Failed to allocate when using KenLM build_binary","2023-03-20 14:35:56","75792240","1","400","<c++><nlp><n-gram><language-model><kenlm>","<p>I have a <code>arpa</code> file which I created by the following command:</p>
<pre><code> ./lmplz -o 4 -S 1G &lt;tmp_100M.txt &gt;100m.arpa
</code></pre>
<p>Now I want to convert this <code>arpa</code> file to binary file:</p>
<pre><code>./build_binary 100m.arpa 100m.bin
</code></pre>
<p>And I'm getting error:</p>
<pre><code>mmap.cc:225 in void util::HugeMalloc(std::size_t, bool, util::scoped_memory&amp;) threw ErrnoException because `!to.get()'.
Cannot allocate memory Failed to allocate 106122412848 bytes Byte: 80
ERROR
</code></pre>
<p>I tried to add <code>-S</code> parameter:</p>
<pre><code>./build_binary -S 1G 100m.arpa 100m.bin
</code></pre>
<p>and I got the same error.</p>
<ol>
<li><p>How can I convert to binary file ?</p>
</li>
<li><p>Why I'm getting this error ?</p>
</li>
</ol>
","language-model"
"75740212","Inferring a large language model on a GPU with not enough video RAM","2023-03-15 03:14:53","","0","962","<machine-learning><memory><deep-learning><gpu><language-model>","<p>I'm trying some experiments running downloaded language models on a desktop machine. Specifically so far Bloom 3B and 7B on a machine with 32GB RAM, a 2-core CPU and no GPU.</p>
<p>(Throughout this question, I will be talking only about inferring – actually running – pretrained models. Training workload for large language models is conveniently measured in petaflop-days; doing that on a desktop is obviously out of the question.)</p>
<p>As might be expected, running these models on a weak CPU is somewhat slow, minutes to tens of minutes per run, and it would be nice to have hardware that would do it faster. But the limiting factor actually seems to be memory.</p>
<p>Bloom-3B downloads as 6GB, takes about 12GB RAM when running. Bloom-7B downloads as 14GB, takes about 28GB RAM when running. (I'm guessing this is because they download as fp16, but the CPU doesn't understand that format, so they need to be expanded to fp32 for running?) That means, with Windows and other stuff in the background, Bloom-7B struggles to run on this machine, and anything bigger would not work.</p>
<p>It is usually said that neural networks prefer to run on a GPU. I looked at the list of graphics cards on offer from one supplier, and the top-end Nvidia card, costing about $2000 (several times the value of this entire computer!), only has 24GB of video memory. On the face of it, that means if I had that card, it would be amazingly fast at running Bloom-3B, but Bloom-7B (which is obviously much more interesting), either wouldn't run at all, or would have to spend most of its time swapping data from main memory, so the speed of the GPU would be wasted. (Which of those is the case? Would it run at all? Would there be a memory-saving benefit from being able to keep the parameters as fp16, or am I misunderstanding the issue with that?)</p>
<p>If this analysis is correct, then when you are inferring (rather than training) a large language model, the usual wisdom about GPU's is inapplicable, and what you actually need is a fast CPU and lots of RAM.</p>
<p>Is that correct, or am I missing something?</p>
","language-model"
"75710007","forward() got an unexpected keyword argument 'labels'","2023-03-11 22:37:28","","2","1747","<nlp><data-science><huggingface-transformers><language-model>","<p>I am trying to use fine-tune TransformerXL for language modeling.</p>
<pre><code>from transformers import TransfoXLTokenizer, TransfoXLModel

tokenizer = TransfoXLTokenizer.from_pretrained(&quot;transfo-xl-wt103&quot;)
model = TransfoXLModel.from_pretrained(&quot;transfo-xl-wt103&quot;)
</code></pre>
<p>I have a .csv file with only one column ('text') that contains paragraphs.</p>
<pre><code>from sklearn.model_selection import train_test_split

df_train, df_test = train_test_split(df, test_size=0.2)
df_train.to_csv('train.csv',index=False)
df_test.to_csv('test.csv',index=False)
</code></pre>
<pre><code>from datasets import load_dataset
dataset = load_dataset(&quot;csv&quot;, data_files={&quot;train&quot;: &quot;train.csv&quot;, &quot;test&quot;: &quot;test.csv&quot;})
</code></pre>
<pre><code>encoded_dataset = dataset.map(lambda t: tokenizer(t['text'],  truncation=True, padding='max_length'), batched=True, load_from_cache_file=False)
</code></pre>
<pre><code>from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False, mlm_probability=0.15
)
</code></pre>
<pre><code>training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=500,
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset[&quot;train&quot;],
    eval_dataset=encoded_dataset[&quot;test&quot;],
    data_collator=data_collator,
)
trainer.train()
</code></pre>
<p>The last line fails with error:</p>
<pre><code>forward() got an unexpected keyword argument 'labels'
</code></pre>
<p>I have no idea where the argument 'labels' comes from. The encoded dataset has the following shape:</p>
<pre><code>DatasetDict({
    train: Dataset({
        features: ['text', 'input_ids'],
        num_rows: 18
    })
    test: Dataset({
        features: ['text', 'input_ids'],
        num_rows: 5
    })
})
</code></pre>
","language-model"
"75664012","I want to make an AI text classifier using OpenAI API, based on GPT2 but i cannot find the API documentation for the GPT2","2023-03-07 15:33:17","75768183","-1","569","<machine-learning><artificial-intelligence><openai-api><language-model><gpt-2>","<p>I wanted to create an AI text classifier project for my college, I wanted to use GPT2 API for the same as it is more reliable to catch the content generated by GPT 3.5, so how can I use GPT2 documentation? also any useful resources for the same are welcome</p>
<p>I tried going through model section of the documentation but couldn't find for GPT2, there's only for GPT 3.5</p>
","language-model"
"75548271","Supervised fine tuning in pre-trained language model","2023-02-23 17:26:03","","0","153","<nlp><transformer-model><language-model><fine-tuning>","<p>Supervised find turning adds a extra output layer to the pre-trained model.</p>
<p>Does this extra layer alter the probability of words that are not related to the fine tune data?</p>
","language-model"
"75528808","How to use language model for speech recognition","2023-02-22 06:05:33","","0","344","<python-3.x><deep-learning><recurrent-neural-network><speech-to-text><language-model>","<p>I am working with a end to emd speech recognition system. i have language model for a language in .lm extension a and other inference and pronunciation models.I want it to make prediction from that models can any one suggest me how to do it in python. I can get mfcc's from the audio file and i have language model how to connect these two to make predictions.Thank you in advance.</p>
<p>I am looking for how to use and what library is to be used in python.</p>
","language-model"
"74869109","When using OPT-2.7B or any other natural language model, is there a way to trick it into having a conversation/ give it a pre prompt in the code","2022-12-20 21:30:17","","1","279","<neural-network><huggingface-transformers><language-model><huggingface><gpt-2>","<p>Using this code, or a variant of, is there anything that can be added to &quot;trick&quot; opt into conversing as another user in a style more similar to a chatbot. As of now it will either start something more similar to an article or have a conversation with itself for a few lines as seen below.</p>
<pre><code>    val = input(&quot;Enter your value: &quot;)
    input_ids = tokenizer((val), return_tensors='pt').input_ids

    output = model.generate(input_ids, min_length=1, max_length=1024, penalty_alpha=0.6, top_k=6)
    print(&quot;Output:\n&quot; + 100 * '-')
    print(tokenizer.decode(output[0], skip_special_tokens=True))
    print(&quot;&quot; + 100 * '-')
    with open(&quot;OutputText.txt&quot;, &quot;w&quot;) as f:
        f.write(tokenizer.decode(output[0], skip_special_tokens=True))


</code></pre>
<p>Here's an example of the current output:</p>
<p>*User Input:
Hello There.</p>
<p>Model Output:
Hello there. I have an egg that matches your TSV. Would you mind hatching it for me?
Sure, I'll add you now. Let me know when you're online.
Sorry for the late reply. I'm online now for the next few hours. Just send me a trade request whenever you're ready.
No probs, I'm in the middle of a battle at the moment, but I'll get you as soon as I'm done.
Thank you very much for the hatch. Have a nice day :D
*</p>
<p>I've attempted to add a prompt to the start and it hasn't made a difference.</p>
","language-model"
"73914453","Forcing transformer models to generate only some tokens from a vocab","2022-09-30 22:21:33","","3","2244","<python><deep-learning><nlp><huggingface-transformers><language-model>","<p>I trained a language model (encoder-decoder) to generate text. I want to restrict the generation vocab of this model to a specific vocab. How can I do that?</p>
<p>I found in <code>generate</code> (<a href=""https://huggingface.co/docs/transformers/v4.22.2/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"" rel=""nofollow noreferrer"">model.generate</a>) function that I can pass a parameter called <code>force_words_ids</code> where the model will be forced to generate &quot;all&quot; the tokens in this list. I am looking for something similar, but instead, to generate some of the list's tokens.</p>
","language-model"
"73891087","How bert [cls] can collect the relevant information from the rest of the hidden states","2022-09-29 06:22:12","","0","91","<deep-learning><nlp><huggingface-transformers><bert-language-model><language-model>","<p>How bert [cls] can collect the relevant information from the rest of the hidden states.??. Does [cls] has mlm information?  If i train my bert using only mlm, in this case cls works?</p>
","language-model"
"73637681","Clustering Lists of Words (Python)","2022-09-07 15:00:15","","-1","198","<python><machine-learning><cluster-analysis><language-model>","<p>I have 54 lists consisting of words of varying lengths. For example:<br />
1 = [&quot;fly&quot;, &quot;robot&quot;, &quot;ketchup&quot;].<br />
2 = [&quot;rain&quot;, &quot;fly&quot;, &quot;top&quot;, &quot;jacket&quot;].<br />
....</p>
<p>I would like to cluster similar lists into groups based on the words in each list. The order of the words in the list does matter slightly but isn't the only criteria for a match. Any ideas? I was thinking of using BERT and then K-means clustering.</p>
<p>I want the lists to remain intact, just grouped/clustered.</p>
","language-model"
"73524907","How to understand the bias term in language model head (when we tie the word embeddings)?","2022-08-29 06:33:02","","1","345","<nlp><huggingface-transformers><bert-language-model><language-model>","<p>I was learning the masked language modeling codebase in Huggingface Transformers. Just a question to understand the language model head.</p>
<p>Here at the final linear layer where we project hidden size to vocab size (<a href=""https://github.com/huggingface/transformers/blob/f2fbe4475386bfcfb3b83d0a3223ba216a3c3a91/src/transformers/models/bert/modeling_bert.py#L685-L702"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/f2fbe4475386bfcfb3b83d0a3223ba216a3c3a91/src/transformers/models/bert/modeling_bert.py#L685-L702</a>).</p>
<pre class=""lang-py prettyprint-override""><code>self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
self.bias = nn.Parameter(torch.zeros(config.vocab_size))
self.decoder.bias = self.bias
</code></pre>
<p>We set the <code>bias</code> term to zero at the moment. And later when we initialize the weight, we tie the weight of the linear layer and the word embedding.</p>
<p>But we don't do such a thing for the bias term. I wonder how we can understand that and why we want to initialize the bias term as a zero vector.</p>
<p><a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1060-L1079"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1060-L1079</a></p>
<pre class=""lang-py prettyprint-override""><code>    def tie_weights(self):
        &quot;&quot;&quot;
        Tie the weights between the input embeddings and the output embeddings.
        If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the
        weights instead.
        &quot;&quot;&quot;
        if getattr(self.config, &quot;tie_word_embeddings&quot;, True):
            output_embeddings = self.get_output_embeddings()
            if output_embeddings is not None:
                self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())

        if getattr(self.config, &quot;is_encoder_decoder&quot;, False) and getattr(self.config, &quot;tie_encoder_decoder&quot;, False):
            if hasattr(self, self.base_model_prefix):
                self = getattr(self, self.base_model_prefix)
            self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)

        for module in self.modules():
            if hasattr(module, &quot;_tie_weights&quot;):
                module._tie_weights()
</code></pre>
<p>My understanding:</p>
<ol>
<li>Because the final linear weight accepts the hidden representations that have been transformed by several feed-forward layers. We might not be able to match them exactly, we need the bias term to somehow regularize them.</li>
</ol>
<p>As I'm not sure my understanding is accurate, I would like to seek your opinions.</p>
","language-model"
"73335404","NAN values appears when including a new padding token in my tokenizer","2022-08-12 14:05:34","","1","322","<python><deep-learning><huggingface-transformers><language-model><gpt-2>","<p>I'm trying to fine-tune a DialoGPT model on a new dataset. I already processed my data correctly and adding a new padding token in the tokenizer didn't seem to make any issue :</p>
<pre class=""lang-py prettyprint-override""><code>#my dataset : 
print(dataset)
print(dataset[0]['text'])
</code></pre>
<blockquote>
<h3>output</h3>
<p>Dataset({
features: ['text'],
num_rows: 48423
})</p>
<p>[speaker 1]: Great that you wish to hear the voices of the guitarists. Here are your booking details of the tickets. You wish to purchase 4 tickets for the event The Original Wailers that is going to take place on March 8th in Berkeley, right?
[speaker 2]: Yup, you're right. Please May I know where is the event conducted and I need the complete address?
[speaker 1]: Please note down the complete address of the event happening. It's at Cornerstone Craft Beer &amp; Live Music, 2367 Shattuck Avenue. Your reservation is successful and have a great time there!
[speaker 2]: Thanks much for the information you've given. Please can you help me to find some intermediate priced restaurant that provides Ethiopian kind of food.
[speaker 1]: Yup! There is an Ethiopian Restaurant named Addis Restaurant providing excellent and authentic traditional Ethiopian cuisine located in Berkeley. Do you wish to reserve a table here?
[speaker 2]:</p>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>#tokenizing and adding labels
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;],  padding='max_length', add_special_tokens =True, max_length=246) #truncation=True, max_length=13)

tokenized_datasets = ds.map(
    tokenize_function, batched=True, num_proc=4, remove_columns=[&quot;text&quot;]
)

tokenized_datasets = tokenized_datasets.add_column(&quot;labels&quot;, tokenized_datasets[:]['input_ids']) 

train_set = model.prepare_tf_dataset(
    tokenized_datasets,
    shuffle=True,
    batch_size=1,
)
sample = train_set.as_numpy_iterator()
sample = sample.next()

print(tokenized_datasets)
print(train_set)
print(sample)
</code></pre>
<blockquote>
<h3>output</h3>
<p>Dataset({
features: ['input_ids', 'attention_mask', 'labels'],
num_rows: 48423
})</p>
<p>&lt;PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(1, 246), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(1, 246), dtype=tf.int64, name=None)}, TensorSpec(shape=(1, 246), dtype=tf.int64, name=None))&gt;</p>
<p>({'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0]]),
'input_ids': array([[   58,  4125,  3110,   352,  5974,   314,   765,   284,   711,
440,  9190,   440, 14918,   440,  3825,   319,   616,  3359,
13,   198,    58,  4125,  3110,   362,  5974,   921,   765,
284,  3350,   262,  3496,   440,  9190,   440, 14918,   440,
3825,  4291,   262,  3195,    11,   826,    30,   198,    58,
4125,  3110,   352,  5974,  1320,   318,   826,    13,  1867,
2099,   286,  3496,   318,   340,    30,   198,    58,  4125,
3110,   362,  5974,   632,   318,  5610,   739,   262, 12136,
6536,   290,   534,  3496,   468,  2067,    13,   198,    58,
4125,  3110,   352,  5974, 20558,   617,  1637,   329,   502,
13,   198,    58,  4125,  3110,   362,  5974,   220, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257]])},
array([[   58,  4125,  3110,   352,  5974,   314,   765,   284,   711,
440,  9190,   440, 14918,   440,  3825,   319,   616,  3359,
13,   198,    58,  4125,  3110,   362,  5974,   921,   765,
284,  3350,   262,  3496,   440,  9190,   440, 14918,   440,
3825,  4291,   262,  3195,    11,   826,    30,   198,    58,
4125,  3110,   352,  5974,  1320,   318,   826,    13,  1867,
2099,   286,  3496,   318,   340,    30,   198,    58,  4125,
3110,   362,  5974,   632,   318,  5610,   739,   262, 12136,
6536,   290,   534,  3496,   468,  2067,    13,   198,    58,
4125,  3110,   352,  5974, 20558,   617,  1637,   329,   502,
13,   198,    58,  4125,  3110,   362,  5974,   220, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,
50257, 50257, 50257]]))</p>
</blockquote>
<p>The ouputs so far seem pretty clean for me. But when I try to make a prediction with my model or train it I have nan values as output :</p>
<pre class=""lang-py prettyprint-override""><code>#Instatiation of model 
from transformers import TFAutoModelForCausalLM
model = TFAutoModelForCausalLM.from_pretrained(&quot;microsoft/DialoGPT-medium&quot;)

optimizer = AdamWeightDecay(learning_rate=1e-9, weight_decay_rate=0.01)
model.compile(optimizer=optimizer, jit_compile=True)

</code></pre>
<pre class=""lang-py prettyprint-override""><code>#model inference
loss = model(sample[0], labels=sample[1])
print(loss)
</code></pre>
<blockquote>
<h3>output</h3>
<p>TFCausalLMOutputWithCrossAttentions([('loss',
&lt;tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)&gt;),
('logits',
&lt;tf.Tensor: shape=(1, 246, 50258), dtype=float32, numpy=
array([[[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
...,
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan]]], dtype=float32)&gt;),
('past_key_values',
(&lt;tf.Tensor: shape=(2, 1, 16, 246, 64), dtype=float32, numpy=
array([[[[[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
...,
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan],
[nan, nan, nan, ..., nan, nan, nan]],</p>
<pre><code>                                            [[nan, nan, nan, ..., nan, nan, nan],
                                             [nan, nan, nan, ..., nan, nan, nan],
                                             [nan, nan, nan, ..., nan, nan, nan],
                                             ...,
                                             [nan, nan, nan, ..., nan, nan, nan],
                                             [nan, nan, nan, ..., nan, nan, nan],
                                             [nan, nan, nan, ..., nan, nan, nan]],
                                             .............
</code></pre>
</blockquote>
<pre class=""lang-py prettyprint-override""><code>#model training
model.fit(train_set, epochs=1)
</code></pre>
<blockquote>
<h3>output</h3>
<p>56/48423 [..............................] - ETA: 2:27:49 - loss: nan</p>
</blockquote>
<p>This NAN value is certainly caused by the new token '[PAD]' added but I don't know how to deal with it.
Can someone help me please ?</p>
","language-model"
"72986749","How to get token or code embedding using Codex API?","2022-07-14 21:16:14","73096314","-1","620","<python><transformer-model><openai-api><language-model>","<p>For a given code snippet, how to get embedding using the Codex API?</p>
<pre><code>import os
import openai
import config


openai.api_key = config.OPENAI_API_KEY

def runSomeCode():
    response = openai.Completion.create(
      engine=&quot;code-davinci-001&quot;,
      prompt=&quot;\&quot;\&quot;\&quot;\n1. Get a reputable free news api\n2. Make a request to the api for the latest news stories\n\&quot;\&quot;\&quot;&quot;,
      temperature=0,
      max_tokens=1500,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0)

    if 'choices' in response:
        x = response['choices']
        if len(x) &gt; 0:
            return x[0]['text']
        else:
            return ''
    else:
        return ''



answer = runSomeCode()
print(answer)
</code></pre>
<p>But I want to figure out given a python code block like the following, can I get the embedding from codex?</p>
<p>Input:</p>
<pre><code>import Random
a = random.randint(1,12)
b = random.randint(1,12)
for i in range(10):
    question = &quot;What is &quot;+a+&quot; x &quot;+b+&quot;? &quot;
    answer = input(question)
    if answer = a*b
        print (Well done!)
    else:
        print(&quot;No.&quot;)
</code></pre>
<p>Output:</p>
<ul>
<li>Embedding of the input code</li>
</ul>
","language-model"
"72851812","Arguments of OpenIE for extracting fewer event triples","2022-07-04 04:36:11","","1","86","<nlp><stanford-nlp><information-retrieval><information-extraction><language-model>","<p>I'm new to NLP and I'm trying to using OpenIE to extract event triples from texts.</p>
<p>I looked into its documents but quite don't understand its arguments. For example, <code>max_entailments_per_clause</code> controls the maximum number of entailments to produce for each clause extracted in the sentence. What is entailment here? And <code>ignore_affinity</code> controls whether to ignore the affinity model for prepositional attachments. What is affinity here?</p>
<p>My question is, I try to use OpenIE to extract event triples from news headlines, but the triples might overlap. For example, the headline is &quot;U.S. stock index futures points to higher start&quot;, the event triples extracted from OpenIE are <code>[('U.S. stock index futures', 'points to', 'start'), ('U.S. stock index futures', 'points to', 'higher start')]</code>. But they are quite overlapping and I only want the one without modifier (<code>('U.S. stock index futures', 'points to', 'start')</code> in this case). What should I do? What arguments should I use?</p>
","language-model"
"72640964","How does BERT loss function works?","2022-06-16 05:36:33","","0","4354","<deep-learning><neural-network><huggingface-transformers><bert-language-model><language-model>","<p>I'm confused about how cross-entropy works in bert LM. To calculate loss function we need the truth labels of masks. But we don't have the vector representation of the truth labels and the predictions are vector representations. So how to calculate loss ?</p>
","language-model"
"72562921","Pre-trained Language Models: Parameters, data, method?","2022-06-09 15:24:54","","0","183","<nlp><huggingface-transformers><bert-language-model><language-model><roberta-language-model>","<p>I am doing a research on pre-trained LMs, specifically the following LMs:</p>
<ul>
<li>BERT</li>
<li>ALBERT</li>
<li>RoBERTa</li>
<li>XLNet</li>
<li>DistilBERT</li>
<li>BigBird</li>
<li>ConvBERT</li>
</ul>
<p>I am looking for information to compare these LMs like: number of parameters, layers, data on which they were pre-trained...</p>
<p>In other words, I want to extend the following table to the other LMs:</p>
<p><a href=""https://i.sstatic.net/aiLuo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/aiLuo.png"" alt=""enter image description here"" /></a></p>
<p>But I can't find information online! Can you please help me?</p>
<p>Thanks.</p>
","language-model"
"72479175","How to force GPT2 to generate specific tokens in each sentence?","2022-06-02 16:07:42","","1","497","<machine-learning><pytorch><huggingface-transformers><language-model><gpt-2>","<p>My input is a string and the outputs are vector representations (corresponding to the generated tokens). I'm trying to force the outputs to have specific tokens (e.g., 4 commas/2 of the word &quot;to&quot;, etc). That is, <strong>each generated sentence</strong> must have those.</p>
<p>Is there a potential loss component that can force GPT2 to generate specific tokens? Another approach that will be easier and more robust (but I'm not sure is possible), is similar to the masking of tokens in BERT. That is, instead of forcing GPT2 to generate sentences with unique tokens, to have the predefined tokens in the sentence beforehand:</p>
<pre><code>[MASK][MASK][specific_token][MASK][MASK][specific_token]
</code></pre>
<p>However, an issue with this approach is that there isn't a predefined number of tokens that should be generated/masked before or after the <code>[specific_token]</code>, nor there is a predefined number of sentences to generate for each given input (else I would have used BERT).</p>
<p><strong>Code:</strong></p>
<pre><code>from transformers import logging
from transformers import GPT2Tokenizer, GPT2Model
import torch 

checkpoint = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(checkpoint)
model = GPT2Model.from_pretrained(checkpoint)

num_added_tokens = tokenizer.add_special_tokens({'pad_token': '[CLS]'})
embedding_layer = model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size

input_string = 'Architecturally, the school has a Catholic character.'
token_ids = tokenizer(input_string, truncation = True, padding=True)
output = model(torch.tensor(token_ids['input_ids']))
</code></pre>
","language-model"
"72467610","OOM while fine-tuning medium sized model with DialoGPT on colab","2022-06-01 20:20:08","73437914","0","394","<google-colaboratory><huggingface-transformers><language-model><gpt-2>","<p>I am trying to finetune DialoGPT with a medium-sized model, I am getting Cuda error while the training phase, I reduced the batch size from 4, but still, the error persists. My parameters are</p>
<pre><code>        #self.output_dir = 'output-small'
        self.output_dir = 'output-medium'
        self.model_type = 'gpt2'
        #self.model_name_or_path = 'microsoft/DialoGPT-small'
        self.model_name_or_path = 'microsoft/DialoGPT-medium'
        #self.config_name = 'microsoft/DialoGPT-small'
        self.config_name = 'microsoft/DialoGPT-medium'
        #self.tokenizer_name = 'microsoft/DialoGPT-small'
        self.tokenizer_name = 'microsoft/DialoGPT-medium'
        self.cache_dir = 'cached'
        self.block_size = 512
        self.do_train = True
        self.do_eval = True
        self.evaluate_during_training = False
        self.per_gpu_train_batch_size = 2
        self.per_gpu_eval_batch_size = 2
        self.gradient_accumulation_steps = 1
        self.learning_rate = 5e-5
        self.weight_decay = 0.0
        self.adam_epsilon = 1e-8
        self.max_grad_norm = 1.0
        self.num_train_epochs = 5
        self.max_steps = -1
        self.warmup_steps = 0
        self.logging_steps = 1000
        self.save_steps = 3500
        self.save_total_limit = None
        self.eval_all_checkpoints = False
        self.no_cuda = False
        self.overwrite_output_dir = True
        self.overwrite_cache = True
        self.should_continue = False
        self.seed = 42
        self.local_rank = -1
        self.fp16 = False
        self.fp16_opt_level = 'O1'
</code></pre>
<p>The GPU allocated is Tesla P100-PCIE with 16GB memory.
Please kindly let me know how to resolve this issue. Any suggestion is appreciated.</p>
","language-model"
"72451171","Word embeddings with Google's T5?","2022-05-31 16:19:16","","2","3426","<nlp><lm><huggingface-transformers><word-embedding><language-model>","<p>Is it possible to generate word embeddings with Google's T5?</p>
<p>I'm assuming that this is possible. However, I cannot find the code I would need to be able to generate word embeddings on the relevant Github (<a href=""https://github.com/google-research/text-to-text-transfer-transformer"" rel=""nofollow noreferrer"">https://github.com/google-research/text-to-text-transfer-transformer</a>) or HuggingFace (<a href=""https://huggingface.co/docs/transformers/model_doc/t5"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/t5</a>) pages.</p>
","language-model"
"72368294","How to not break differentiability with a model's output?","2022-05-24 19:12:39","","1","59","<python><machine-learning><split><pytorch><language-model>","<p>I have an autoregressive language model in Pytorch that generates text, which is a collection of sentences, given one input:</p>
<pre><code>output_text = [&quot;sentence_1. sentence_2. sentence_3. sentence_4.&quot;]
</code></pre>
<p>Note that the output of the language model is in the form of logits (probability over the vocabulary), which can be converted to token IDS or strings.</p>
<p>Some of these sentences need to go into another model to get a loss that should affect only those sentences:</p>
<pre><code>loss1 = model2(&quot;sentence_2&quot;)
loss2 = model2(&quot;sentence_4&quot;)
loss_total = loss1+loss2
</code></pre>
<p>What is the correct way to break/split the generated text from the first model without breaking differentiability? That is, so the corresponding text (from above) will look like a pytorch tensor of tensors (in order to then use some of them in the next model):</p>
<pre><code>&quot;[[&quot;sentence_1.&quot;]
[&quot;sentence_2.&quot;] 
[&quot;sentence_3.&quot;]
[&quot;sentence_4.&quot;]]
</code></pre>
<p>For example, Python's <code>split(&quot;.&quot;)</code> method will most likely break differentiability, but will allow me to take each individual sentence and insert it into the second model to get a loss.</p>
","language-model"
"72050166","Using German GPT-2 in Rasa","2022-04-28 20:59:24","","0","305","<rasa><rasa-nlu><language-model>","<p>Does the Rasa Framework also allow the use of the German GPT-2 model (<a href=""https://huggingface.co/dbmdz/german-gpt2"" rel=""nofollow noreferrer"">https://huggingface.co/dbmdz/german-gpt2</a>) with the LanguageModelFeaturizer? I had also tried two older Rasa 2. x versions. There it did not work as well. Does anyone know more?
Thanks in advance.
I tried it and get, among other things, the error message (in Rasa 3.x)</p>
<pre><code>TypeError: expected str, bytes or os.PathLike object, not NoneType
</code></pre>
<p>and</p>
<pre><code>  File &quot;C:\ProgramData\Anaconda3\envs\tests\lib\site-packages\rasa\engine\graph.py&quot;, line 403, in _load_component
    raise GraphComponentException(
rasa.engine.exceptions.GraphComponentException: Error initializing graph component for node run_LanguageModelFeaturizer1.```
</code></pre>
","language-model"
"72032858","Training a FF Neural Language Model","2022-04-27 17:15:41","72042038","0","106","<python><tensorflow><keras><neural-network><language-model>","<p>Consider 3-grams of the sentence &quot;The cat is upstairs&quot; where each word is separated by the rest with @ and ~ symbols.</p>
<pre><code>trigrams = ['@th', 'the', 'he~', '@ca', 'cat', 'at~', '@is', 'is~', 
             '@up', 'ups', 'pst', 'sta', 'tai', 'air', 'irs', 'rs~']

</code></pre>
<p>I want to train a character based Feed Forward Neural Language model using this sentence, but I am having trouble fitting the X and y parameters correctly.</p>
<p>My code is as follows :</p>
<pre><code># trigrams encoded
d = dict([(y,x+1) for x,y in enumerate(sorted(set(trigrams)))])
trigrams_encoded = [d[x] for x in trigrams]
# trigrams_encoded = [3, 15, 8, 1, 7, 6, 2, 10, 4, 16, 11, 13, 14, 5, 9, 12]

# x_train
x_train = [] # list of lists, each list contains 3 encoded trigrams
for i in range(len(trigrams_encoded)-3) :
    lst = trigrams_encoded[i:i+3]
    x_train.append(lst)
x_train = np.array(x_train)            # x_train shape is (13,3)

# y_train
y_train = trigrams_encoded[3:]
data = np.array(y_train)
y_onehot = to_categorical(data)        # y_onehot shape is (13,17)
y_onehot = np.delete(y_onehot, 0, 1)   # now shape is (13,16)

# define model
model = Sequential()
model.add(Embedding(len(d), 10, input_length=3)) #len(d) = 16
model.add(Flatten())
model.add(Dense(10, activation='relu'))
model.add(Dense(len(d), activation='softmax'))

# compile the model
# i have set sparse_categorical_crossentropy here, but not sure if this is correct. feel free to change it
model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer='adam', metrics=['accuracy'])

# train the model
model.fit(x_train, y_onehot, epochs=1, verbose=0)
</code></pre>
<p>My initial attempt was to say that since input_length=3, the model will take as input triplets of the listed n-grams which should be labelled as the next n-gram in the list. But this seems to fail. (should it fail ?)</p>
<p>The above code raises the following error which I do not know how to solve :</p>
<pre><code>&quot;InvalidArgumentError: Graph execution error:

Detected at node 'sequential/embedding/embedding_lookup' defined at (most recent call last):

(... many lines...)

Node: 'sequential/embedding/embedding_lookup'
indices[5,1] = 16 is not in [0, 16)&quot;
</code></pre>
<p>Could you please assist on the correct choices of X and y here ?</p>
","language-model"
"71515500","What is the correct return of BertForMaskedLM?","2022-03-17 16:01:33","","0","609","<nlp><huggingface-transformers><bert-language-model><transformer-model><language-model>","<p>I'm using huggingface BertForMaskedLM.
For a sentence, I'm getting a 3-dimensional return from BertForMaskedLM.
For example (P,N,V), Here I understand the N is the length of the sentence and V is the vocab size in Bert. But I'm confused about the P. What is exactly the first return of BertForMaskedLM?</p>
","language-model"
"71487474","How bert is a bidirectional?","2022-03-15 18:46:02","","3","2767","<nlp><lstm><bert-language-model><language-model><bilstm>","<p>Bert encoder takes the input and goes for the multi-head attention model. But how do they maintain sequence? Since current words don't take sequence of previous words. Besides, why is it bidirectional? Does it maintain forward and backward sequence like LSTM?</p>
","language-model"
"70174901","How to load spacy language model from local machine?","2021-11-30 19:20:43","","0","1827","<spacy><tokenize><language-model>","<p>Since the server I use is not connected to the Internet, I would need to load model from the local disk.</p>
<p>For example, I should be able to run the following code,</p>
<pre><code>from torchtext.data.utils import get_tokenizer
my_language = get_tokenizer('spacy', language='en_core_web_md')
</code></pre>
<p>I have downloaded en_core_web_md-3.1.0.tar.gz and extracted into the local folder. Then tried adding path_to_folder. It didn't work and says,</p>
<pre><code>OSError: [E053] Could not read config.cfg from Sig_Data/en_core_web_md-3.1.0/config.cfg
</code></pre>
<p>Which makes sense as the folder does NOT have this file.</p>
<p>Then, I have installed en_core_web_md using pip install. I can import it as</p>
<pre><code>import en_core_web_md
</code></pre>
<p>But not sure how can I use it in the context of</p>
<pre><code>from torchtext.data.utils import get_tokenizer
my_language = get_tokenizer('spacy', language='en_core_web_md')
</code></pre>
<p>Thanks in advance for any help.</p>
","language-model"
"70043586","Subprocess call error while calling generate_lm.py of DeepSpeech","2021-11-20 05:55:22","70240579","0","205","<python><speech-to-text><language-model><mozilla-deepspeech>","<p>I am trying to build customised scorer (language model) for speech-to-text using DeepSpeech in colab. While calling generate_lm.py getting this error:</p>
<pre><code>    main()
  File &quot;generate_lm.py&quot;, line 201, in main
    build_lm(args, data_lower, vocab_str)
  File &quot;generate_lm.py&quot;, line 126, in build_lm
    binary_path,
  File &quot;/usr/lib/python3.7/subprocess.py&quot;, line 363, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command '['/content/DeepSpeech/native_client/kenlm/build/bin/build_binary', '-a', '255', '-q', '8', '-v', 'trie', '/content/DeepSpeech/data/lm/lm_filtered.arpa', '/content/DeepSpeech/data/lm/lm.binary']' died with &lt;Signals.SIGSEGV: 11&gt;.```

Calling the script generate_lm.py like this :

```! python3 generate_lm.py --input_txt hindi_tokens.txt --output_dir /content/DeepSpeech/data/lm --top_k 500000 --kenlm_bins /content/DeepSpeech/native_client/kenlm/build/bin/ --arpa_order 5 --max_arpa_memory &quot;85%&quot; --arpa_prune &quot;0|0|1&quot; --binary_a_bits 255 --binary_q_bits 8 --binary_type trie```
</code></pre>
","language-model"
"69955550","Keras model with fasttext word embedding","2021-11-13 15:12:23","69956188","1","2745","<python><tensorflow><keras><fasttext><language-model>","<p>I am trying to learn a language model to predict the last word of a sentence given all the previous words using keras. I would like to embed my inputs using a learned fasttext embedding model.</p>
<p>I managed to preprocess my text data and embed the using fasttext. My training data is comprised of sentences of 40 tokens each. I created 2 np arrays, X and y as inputs, with y what I want to predict.</p>
<p>X is of shape (44317, 39, 300) with 44317 the number of example sentences, 39 the number of tokens in each sentence, and 300 the dimension of the word embedding.</p>
<p>y is of shape (44317, 300) is for each example the embedding of the last token of the sentence.</p>
<p>My code for the keras model goes as follow (inspired by <a href=""https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/?fbclid=IwAR3NyptI5uPN_8uOP69QWKRfpTfaqG-Y1XzUB2ciN0aTr-vQDxUhnfY4Ets"" rel=""nofollow noreferrer"">this</a>)</p>
<pre><code>#importing all the needed tensorflow.keras components
model = Sequential()  
model.add(InputLayer((None, 300)))
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(100, activation='relu'))
model.add(Dense(300, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, y, batch_size=128, epochs=20)
model.save('model.h5')
</code></pre>
<p>However, the accuracy I get while training on this model is extremely low (around 1.5%). I think there is some component of the keras model that I misundertood, as if I don't embed my inputs and add an extra embedding layer instead of the InputLayer I get an accuracy of about 60 percents.</p>
<p>My main doubt is the value of &quot;300&quot; on my second Dense layer, as I read that this should correspond the vocabulary size of my word embedding model (which is 48000), however if I put anything else than 300 I get a dimension error. So I understand that I'm doing something wrong, but I can't find how to fix it.</p>
<p>PS :
I have also tried <code>y = to_categorical(y, num_classes=vocab_size)</code> with vocab_size the vocabulary size of my word embedding, and by changing 300 by this same value in the second Dense, however then it tries to create an array of shape(13295100, 48120) instead of what I expect : (44317, 48120).</p>
","language-model"
"69839105","Add custom punctuation to spacy model","2021-11-04 12:15:11","","0","788","<spacy><punctuation><language-model><spacy-transformers>","<p>How do you add custom punctuation (e.g. asterisk) to the infix list in a Tokenizer and have that recognized by <code>nlp.explain</code> as punctuation? I would like to be able to add characters that are not currently recognized as punctuation to the punctuation list from the list of set infixes so that the Matcher can use them when matching <code>{'IS_PUNCT': True}</code> .</p>
<p>An answer to a similar issue was provided here
<a href=""https://stackoverflow.com/questions/56019093/how-can-i-add-custom-signs-to-spacys-punctuation-functionality"">How can I add custom signs to spaCy&#39;s punctuation functionality?</a></p>
<p>The only problem is I am unable to package the newly recognized punctuation with the model. A side note: the tokenizer already recognizes infixes with the desired punctuation, so all that is left is propagating this to the Matcher.</p>
","language-model"
"69489265","N-gram Language Model returns nothing","2021-10-08 00:44:48","69545012","1","402","<python><dictionary><prediction><n-gram><language-model>","<p>I am following the tutorial here: <a href=""https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/#h2_5"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-language-model-nlp-python-code/#h2_5</a> to create a Language model. I am following the bit about the N-gram Language model.</p>
<p>This is the completed code:</p>
<pre class=""lang-py prettyprint-override""><code>from nltk.corpus import reuters
from nltk import bigrams, trigrams
from collections import Counter, defaultdict

# Create a placeholder for model
model = defaultdict(lambda: defaultdict(lambda: 0))

# Count frequency of co-occurance
for sentence in reuters.sents():
    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):
        model[(w1, w2)][w3] += 1

# Let's transform the counts to probabilities
for w1_w2 in model:
    total_count = float(sum(model[w1_w2].values()))
    for w3 in model[w1_w2]:
        model[w1_w2][w3] /= total_count

input = input(&quot;Hi there! Please enter an incomplete sentence and I can help you\
 finish it!\n&quot;).lower().split()

print(model[tuple(input)])
</code></pre>
<p>To get output from the model, the website does this: <code>print(dict(model[&quot;the&quot;, &quot;price&quot;]))</code> but I want to generate output from a user inputted sentence. When I write <code>print(model[tuple(input)])</code>, it gives me an empty defaultdict.</p>
<p>Disregard this (keeping for history):</p>
<blockquote>
<p>How do I give it the list I create from the input? <code>model</code> is a
dictionary and I've read that using a list as a key isn't a good idea
but that's exactly what they're doing? And I'm assuming mine doesn't
work because I'm listing a list? Would I have to iterate through the
words to get results?</p>
<p>As a side note, is this model considering the sentence as a whole to
predict the next word, or just the <em>last</em> word?</p>
</blockquote>
","language-model"
"69380237","Why is my Transformer implementation losing to a BiLSTM?","2021-09-29 16:28:44","69386710","1","591","<deep-learning><pytorch><lstm><transformer-model><language-model>","<p>I am dealing with a sequence tagging problem and I am using a single Transformer Encoder to obtain logits from each element of the sequence. Having experimented both with Transformer and BiLSTM it looks like in my case BiLSTM is working better, so I was wondering if maybe it is because my Transformer implementation has some problem... Below is my implementation of the Transformer Encoder and related functions for creating padding mask and positional embeddings:</p>
<pre><code>def create_mask(src, lengths):
    &quot;&quot;&quot;Create a mask hiding future tokens
    Parameters:
        src (tensor): the source tensor having shape [batch_size, number_of_steps, features_dimensions]
        length (list): a list of integers representing the length (i.e. number_of_steps) of each sample in the batch.&quot;&quot;&quot;
    mask = []
    max_len = src.shape[1]
    for index, i in enumerate(src):
        # The mask consists in tensors having false at the step number that doesn't need to be hidden and true otherwise
        mask.append([False if (i+1)&gt;lengths[index] else True for i in range(max_len)])
    return torch.tensor(mask)

class PositionalEncoding(nn.Module):

    def __init__(self, d_model, dropout=0.1, max_len=5000, device = 'cpu'):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.device = device
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(1, max_len, d_model)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :].to(self.device)
        return self.dropout(x)

class Transformer(nn.Module):
    &quot;&quot;&quot;Class implementing transformer ecnoder, partially based on
    https://pytorch.org/tutorials/beginner/transformer_tutorial.html&quot;&quot;&quot;
    def __init__(self, in_dim, h_dim, n_heads, n_layers, dropout=0.2, drop_out = 0.0, batch_first = True, device = 'cpu', positional_encoding = True):
        super(Transformer, self).__init__()
        self.model_type = 'Transformer'
        self.pos_encoder = PositionalEncoding(in_dim, dropout, device = device)
        encoder_layers = nn.TransformerEncoderLayer(in_dim, n_heads, h_dim, dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers, norm=nn.LayerNorm(in_dim))
        self.in_dim = in_dim
        self.drop_out = drop_out
        self.positional_encoding = positional_encoding
    
        
    def forward(self, src, mask = None, line_len=None):
        src = src * math.sqrt(self.in_dim)
        if self.positional_encoding:
            src = self.pos_encoder(src)
        if line_len is not None and mask is None:
            mask = create_mask(src, line_len)
        else:
            mask = None
        output = self.transformer_encoder(src, src_key_padding_mask = mask)
        if self.drop_out:
            output = F.dropout(output, p = self.drop_out)
        return src, output
</code></pre>
<p>As it can be seen, the above network outputs the hidden states and then I pass them into an additional linear layer and train with a CrossEntropy loss over two classes and Adam optimizer. I have tried multiple combinations of hyperparameters but the BiLSTM still performs better. Can anyone spot anything off in my Transformer or suggest why I experience such a counterintuitive result?</p>
","language-model"
"69217404","Spacy Model load error from local directory","2021-09-17 03:25:00","69219179","0","2673","<nlp><spacy><language-model><spacy-3>","<p>I am trying to find a way to load the downloaded <code>en_core_web_lg ==2.3.1</code> for <code>Spacy == 2.3.2</code>.</p>
<p>Steps:</p>
<ol>
<li>Downloaded the <code>tar</code> file</li>
<li>extracted it to <code>path</code></li>
</ol>
<p>Code:</p>
<pre><code>import spacy
nlp=spacy.load(&quot;path/en_core_web_lg&quot;)
</code></pre>
<p>Error:</p>
<pre><code>OSERROR: [E053] Could not read meta.json from en_core_web_lg/meta.json
</code></pre>
<p>Any suggestions will be helpful</p>
","language-model"
"69216523","Spacy download en_core_web_lg manually","2021-09-17 00:32:48","","2","13570","<nlp><spacy><language-model><spacy-3>","<p>I am trying to find a way to download the  model <code>en_core_web_lg ==2.3.1</code> for <code>Spacy == 2.3.2</code>.</p>
<p>Currently using</p>
<pre><code>python -m spacy download en_core_web_lg
import spacy
nlp = spacy.load (&quot;en_core_web_lg&quot;)
</code></pre>
<p>Is it possible to download the <code>model file or directory</code> directly and <code>load the model</code> from that downloaded folder.</p>
","language-model"
"68907519","Bert with Padding and Masked Token Predicton","2021-08-24 12:23:04","69064907","2","1550","<tensorflow><keras><bert-language-model><huggingface-transformers><language-model>","<p>I am Playing around with Bert Pretrained Models (bert-large-uncased-whole-word-masking)
I used Huggingface to try it I first Used this Piece of Code</p>
<pre><code>m = TFBertLMHeadModel.from_pretrained(&quot;bert-large-cased-whole-word-masking&quot;)
logits = m(tokenizer(&quot;hello world [MASK] like it&quot;,return_tensors=&quot;tf&quot;)[&quot;input_ids&quot;]).logits
</code></pre>
<p>I then used Argmax to get max probabilities after applying softmax,
Things works fine Until now.</p>
<p>When I used padding with max_length = 100 The model started making false prediction and not working well and all predicted tokens were the same i.e 119-Token ID</p>
<p>Code I used for Argmax</p>
<pre><code>tf.argmax(tf.keras.activations.softmax(m(tokenizer(&quot;hello world [MASK] like it&quot;,return_tensors=&quot;tf&quot;,max_length=,padding=&quot;max_length&quot;)[&quot;input_ids&quot;]).logits)[0],axis=-1)
</code></pre>
<p>Output Before using padding</p>
<pre><code>&lt;tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 9800, 19082,  1362,   146,  1176,  1122,   119])&gt;
</code></pre>
<p>Output After using padding with max_length of 100</p>
<pre><code>&lt;tf.Tensor: shape=(100,), dtype=int64, numpy=
array([119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119, 119,
       119, 119, 119, 119, 119, 119, 119, 119, 119])&gt;
</code></pre>
<p>I wonder if this problem prevail even training a new model as It is mandatory to set Input shape for training new model I Padded and tokenized the data but, now I want to know if this problem continues with it too.</p>
","language-model"
"68732271","RuntimeError: CUDA error: device-side assert triggered - BART model","2021-08-10 19:01:33","68744915","1","4162","<pytorch><huggingface-transformers><language-model>","<p>I am trying to run BART language model for a text generation task.</p>
<p>My code was working fine when I used for another encoder-decoder model (T5), but with bart I am getting this error:</p>
<pre><code>File &quot;train_bart.py&quot;, line 89, in train
    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)                                                     cs-lab-host1&quot; 12:39 10-Aug-21
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 1308, in forward
    return_dict=return_dict,
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 1196, in forward
    return_dict=return_dict,
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 985, in forward
    attention_mask, input_shape, inputs_embeds, past_key_values_length
  File &quot;.../venv/tf_23/lib/python3.6/site-packages/transformers/models/bart/modeling_bart.py&quot;, line 866, in _prepare_decoder_attent
ion_mask
    ).to(self.device)
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
</code></pre>
<p>And this is where error happens:</p>
<pre><code>for _, data in tqdm(enumerate(loader, 0), total=len(loader), desc='Processing batches..'):
    y = data['target_ids'].to(device, dtype = torch.long)
    y_ids = y[:, :-1].contiguous()
    lm_labels = y[:, 1:].clone().detach()
    lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100
    ids = data['source_ids'].to(device, dtype = torch.long)
    mask = data['source_mask'].to(device, dtype = torch.long)

    outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)
    loss = outputs[0]
</code></pre>
<p><code>loader</code> is the tokenized and processed data.</p>
","language-model"
"68729645","How to get the language modeling loss by passing 'labels' while using ONNX inference session?","2021-08-10 15:30:00","","0","589","<pytorch><huggingface-transformers><language-model><onnxruntime><gpt-2>","<p>When using GPT2 we can simply pass on the 'labels' parameter to get the loss as follows:</p>
<pre><code>import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2', return_dict=True)

inputs = tokenizer(&quot;Hello, my dog is cute&quot;, return_tensors=&quot;pt&quot;)
outputs = model(**inputs, labels=inputs[&quot;input_ids&quot;])
loss = outputs.loss
</code></pre>
<p>But, not able to find out how to get the same loss in an ONNX inference session. I am using the below code which only returns the 'last_hidden_state':</p>
<pre><code>import onnxruntime as ort

from transformers import GPT2TokenizerFast
#tokenizer = GPT2TokenizerFast.from_pretrained(&quot;gpt2&quot;)

ort_session = ort.InferenceSession(&quot;onnx/gpt2/model.onnx&quot;)

inputs = tokenizer(&quot;Using BERT in ONNX!&quot;, return_tensors=&quot;np&quot;)
outputs = ort_session.run([&quot;last_hidden_state&quot;], dict(inputs))
</code></pre>
","language-model"
"68363587","Tensorflow hub-NNLM word embedding using sentiment140 data gives input shape error","2021-07-13 13:41:57","68380306","0","368","<keras><sentiment-analysis><word-embedding><tensorflow-hub><language-model>","<p>I am using tensorflow hub &quot;https://tfhub.dev/google/nnlm-en-dim128/2&quot; word embedding for the sentiment analysis of Kaggle &quot;sentiment140&quot; dataset.</p>
<p>Data set : Kaggle(&quot;sentiment140&quot;) <a href=""https://www.kaggle.com/kazanova/sentiment140"" rel=""nofollow noreferrer"">https://www.kaggle.com/kazanova/sentiment140</a>
Tensorflow-Hub : <a href=""https://tfhub.dev/google/nnlm-en-dim128/2"" rel=""nofollow noreferrer"">https://tfhub.dev/google/nnlm-en-dim128/2</a></p>
<p>Here i am using keras sequential layer when i fit the model it gives value error</p>
<pre><code>ValueError: Python inputs incompatible with input_signature:
      inputs: (
        Tensor(&quot;IteratorGetNext:0&quot;, shape=(None, 128), dtype=float32))
      input_signature: (
        TensorSpec(shape=(None,), dtype=tf.string, name=None))
</code></pre>
<p>My code:</p>
<pre><code>    import pandas as pd
import tensorflow as tf
from sklearn.model_selection import  train_test_split
import seaborn as sns
import tensorflow_hub as hub
from tensorflow.keras import Sequential
import keras

tweet_df = pd.read_csv(&quot;training.1600000.processed.noemoticon.csv&quot;, names=['polarity', 'id', 'date', 'query', 'user', 'text'],encoding='latin-1')

tweet_df.info()

tweet_df.head()

&quot;&quot;&quot;#### 2.) Data Visualization&quot;&quot;&quot;

tweet_df['polarity'] = tweet_df['polarity'].replace(to_replace=4,value=1)

### Print two movies reviews from each class

print(&quot;Movie Review Polarity Negative class 0 :\n&quot;, tweet_df[tweet_df['polarity']==0]['text'].head(2) )

print(&quot;\n\nMovie Review Polarity Positive class 1 :\n&quot;, tweet_df['text'][tweet_df['polarity']==1].head(2) )

class_dist = tweet_df['polarity'].value_counts().rename_axis('Class Label').reset_index(name='Tweets')
#class_dist = class_dist['Class Label'].replace({0:'Negative',1:'Positve'})
class_dist

## Bar graph of Distribution of Classes
class_dist['class'] = ['Positive','Negative']
sns.set_theme(style='whitegrid')
sns.barplot(x='Class Label', y='Tweets', hue='class', data= class_dist)

### Train and test split 
X = tweet_df.iloc[:,5]
y = tweet_df.iloc[:,0]
X_train, X_test,y_train, y_test = train_test_split(X,y,random_state=5, test_size=0.2)

print(&quot;Training shape of X and y : &quot;, X_train.shape ,y_train.shape)
print(&quot;Testing shape of X and y : &quot;, X_test.shape ,y_test.shape)

&quot;&quot;&quot;#### 3.) Data Pre-processing&quot;&quot;&quot;

embed = hub.load(&quot;https://tfhub.dev/google/nnlm-en-dim128/2&quot;)
X_train_embed = embed(X_train)

y_train = tf.keras.utils.to_categorical(y_train,2)

X_train_embed.shape


X_sample = X_train_embed[:1000]
y_sample = y_train[:1000]
y_sample = tf.keras.utils.to_categorical(y_sample,2)


&quot;&quot;&quot;#### 4.) Model Building&quot;&quot;&quot;

hub_layer = hub.KerasLayer('https://tfhub.dev/google/nnlm-en-dim128/2',input_shape=[],dtype=tf.string,trainable=False)

model = Sequential()
model.add(hub_layer)
model.add(keras.layers.Dense(128, 'relu', name ='layer_1'))
model.add(keras.layers.Dense(64, 'relu', name = 'layer_2'))
model.add(keras.layers.Dense(2, activation='sigmoid', name='output'))

model.compile(optimizer='adam',loss= 'BinaryCrossentropy',  #'categorical_crossentropy' ,
              metrics=['accuracy'] )

NN_model = model.fit(X_sample, y_sample, epochs=20, validation_split=0.1, verbose=1)
</code></pre>
<p>Input shape:</p>
<pre><code>X_sample.shape
</code></pre>
<p>TensorShape([1000, 128])</p>
<pre><code>y_sample.shape
</code></pre>
<p>(1000, 2, 2)</p>
<pre><code>X_sample

&lt;tf.Tensor: shape=(1000, 128), dtype=float32, numpy=
array([[ 0.10381411,  0.07044576, -0.0282673 , ...,  0.08205549,
0.15822364, -0.10019408],
[-0.03332436, -0.00529242,  0.20348714, ..., -0.14174528,
0.05178985, -0.12599435],
[ 0.2461916 , -0.03084931,  0.05861813, ...,  0.07956063,
-0.03579932,  0.07493019],
[ 0.4102695 ,  0.15445013,  0.19045362, ...,  0.12681636,
0.12362286, -0.03969387],
[-0.0144283 , -0.05236297,  0.04851832, ...,  0.05562773,
0.01529189,  0.12605236],
[ 0.29280087,  0.05795274, -0.11779188, ..., -0.01890504,
0.02824693, -0.13629636]], dtype=float32)&gt;
</code></pre>
","language-model"
"67788151","Does adding a list of Word2Vec embeddings give a meaningful represenation?","2021-06-01 11:38:23","67793126","0","390","<nlp><word2vec><embedding><language-model>","<p>I'm using a pre-trained word2vec model (word2vec-google-news-300) to get the embeddings for a given list of words. Please note that this is NOT a list of words that we get after tokenizing a  sentence, it is just a list of words that describe a given image.</p>
<p>Now I'd like to get a single vector representation for the entire list. Does adding all the individual word embeddings make sense? Or should I consider averaging?
Also, I would like the vector to be of a constant size so concatenating the embeddings is not an option.</p>
<p>It would be really helpful if someone can explain the intuition behind considering either one of the above approaches.</p>
","language-model"
"67362300","fill-mask usage from transformers pipeline","2021-05-03 00:36:31","","1","746","<nlp><pytorch><artificial-intelligence><language-model><gpt-2>","<p>I fine-tune a gpt2 language model and I am generation the text according to my model by using following lines of code:</p>
<p>generator = pipeline('text-generation', tokenizer='gpt2', model='data/out')
print(generator('Once upon a time', max_length=40)[0]['generated_text'])</p>
<p>Now I want to do the prediction of only next word with the probabilities. I know we can do it by using 'fill-mask' but I don't know how to do it. When I put 'fill-mask' inplace of 'text-generation', I am getting this error:</p>
<p>&quot;Unrecognized configuration class &lt;class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'&gt; for this kind of AutoModel: AutoModelForMaskedLM.
Model type should be one of BigBirdConfig, Wav2Vec2Config, ConvBertConfig, LayoutLMConfig, DistilBertConfig, AlbertConfig, BartConfig, MBartConfig, CamembertConfig, XLMRobertaConfig, LongformerConfig, RobertaConfig, SqueezeBertConfig, BertConfig, MobileBertConfig, FlaubertConfig, XLMConfig, ElectraConfig, ReformerConfig, FunnelConfig, MPNetConfig, TapasConfig, DebertaConfig, DebertaV2Config, IBertConfig.&quot;.</p>
<p>generator = pipeline('fill-mask', tokenizer='gpt2', model='data/out') // this line is giving me the above mentioned error.</p>
<p>Please let me know how can I fix this issue. Any kind of help would be greatly appreciated.
Thanks in advance.</p>
<p>The whole code for better understanding.</p>
<p>from transformers import (
GPT2Tokenizer,
DataCollatorForLanguageModeling,
TextDataset,
GPT2LMHeadModel,
TrainingArguments,
Trainer,
pipeline)</p>
<p>train_path = 'parsed_data.txt'
test_path = 'parsed_data.txt'</p>
<p>tokenizer = GPT2Tokenizer.from_pretrained(&quot;gpt2&quot;)</p>
<p>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)</p>
<p>train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=128)
test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=128)</p>
<p>model = GPT2LMHeadModel.from_pretrained('gpt2')</p>
<p>training_args = TrainingArguments(output_dir = 'data/out', overwrite_output_dir = True, per_device_train_batch_size = 32, per_device_eval_batch_size = 32, learning_rate = 5e-5, num_train_epochs = 3,)</p>
<p>trainer = Trainer(model = model, args = training_args, data_collator=data_collator, train_dataset = train_dataset, eval_dataset = test_dataset)</p>
<p>trainer.train()</p>
<p>trainer.save_model()
generator = pipeline('fill-mask', tokenizer='gpt2', model='data/out')</p>
","language-model"
"67097467","About BertForMaskedLM","2021-04-14 18:48:10","67097860","3","5932","<nlp><bert-language-model><huggingface-transformers><language-model>","<p>I have recently read about Bert and want to use BertForMaskedLM for fill_mask task. I know about Bert architecture. Also, as far as I know, BertForMaskedLM  is built from Bert with a language modeling head on top, but I have no idea about what <em>language modeling head</em> means here. Can anyone give me a brief explanation.</p>
","language-model"
"66990399","pip install a spacy language model in a particular folder","2021-04-07 16:39:43","","2","1185","<python><installation><spacy><language-model>","<p>I would like to pip install several language models in a particular folder different than the default one.</p>
<p>How to proceed?</p>
<p>The following does not seem to work:</p>
<pre><code>pip install /shared/public/spacy/en_core_web_lg-3.0.0-py3-none-any.whl
</code></pre>
<p>see:
<a href=""https://github.com/explosion/spacy-models/releases//tag/en_core_web_lg-3.0.0"" rel=""nofollow noreferrer"">https://github.com/explosion/spacy-models/releases//tag/en_core_web_lg-3.0.0</a></p>
<p>With the typical installation procedure:
python -m spacy download en_core_web_lg
I can not control the folder.</p>
<p>the use case here is install the model in a folder available to everyone in a server in order to avoid multiple downloads.</p>
<p>NOTE: I already have Spacy installed in my particular virtual environment, i.e. I dont need to create a new one. Actually in my particular environment I do have the small language model of spacy for English. The question relates ONLY to installing yet another language model, the large one, in a particular folder, and being able to load that model from that folder.</p>
<p>thanks</p>
","language-model"
"66956460","Huggingface GPT transformers layers output","2021-04-05 16:40:57","","1","679","<tensorflow><nlp><huggingface-transformers><language-model><gpt-2>","<p>I'm trying to use a GPT language model and get the weights it assigns to each word in the last state of text generation. My model is a GPT2 from the transformers library. Below is how I call the pretrained model:</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained(
&quot;HooshvareLab/gpt2-fa-poetry&quot;
) 

model = AutoModelForCausalLM.from_pretrained(
    &quot;HooshvareLab/gpt2-fa-poetry&quot;
)
device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

model = model.to(device)
</code></pre>
<p>My goal is to use this information from the last layer of this model (a matrix with the length of vocabulary after the softmax activation) and use it in combination with another model.</p>
<p>I'm trying to do this in TensorFlowPlease, but share your comments if you think there are easier and more convenient ways of doing this in PyTorch.</p>
","language-model"
"66821321","BERT: Weights of input embeddings as part of the Masked Language Model","2021-03-26 17:03:32","67065742","1","1401","<nlp><pytorch><bert-language-model><transformer-model><language-model>","<p>I looked through different implementations of BERT's Masked Language Model.
For pre-training there are <strong>two</strong> common versions:</p>
<ol>
<li>Decoder would simply take the final embedding of the [MASK]ed token and pass it throught a linear layer (without any modifications):</li>
</ol>
<pre><code>    class LMPrediction(nn.Module):
        def __init__(self, hidden_size, vocab_size):
            super().__init__()
            self.decoder = nn.Linear(hidden_size, vocab_size, bias = False)
            self.bias = nn.Parameter(torch.zeros(vocab_size))
            self.decoder.bias = self.bias
        def forward(self, x):
             return self.decoder(x)
</code></pre>
<ol start=""2"">
<li>Some implementations would use the weights of the input embeddings as weights of the decoder-linear-layer:</li>
</ol>
<pre><code>    class LMPrediction(nn.Module):
        def __init__(self, hidden_size, vocab_size, embeddings):
            super().__init__()
            self.decoder = nn.Linear(hidden_size, vocab_size, bias = False)
            self.bias = nn.Parameter(torch.zeros(vocab_size))
            self.decoder.weight = embeddings.weight ## &lt;- THIS LINE
            self.decoder.bias = self.bias
        def forward(self, x):
             return self.decoder(x)
</code></pre>
<p>Which one is correct? Mostly, I see the first implementation. However, the second one makes sense as well - but I cannot find it mentioned in any papers (I would like to see if the second version is somehow superior to the first one)</p>
","language-model"
"66678314","How to incoporate mask into negative likelihood loss (torch.nn.functional.nll_loss)","2021-03-17 17:32:48","","0","655","<python><deep-learning><pytorch><torch><language-model>","<p>Hello I am implementing a lstm for language modelling
for homework and I am at the loss implementation phase. Our instructor told us to use F.nll_loss but the sequences are padded and we have to take into account a mask that is given which tells us when the sequences stop.</p>
<p>input:</p>
<ul>
<li>log_probas (batch_size, sequence_length(padded), vocabulary size)</li>
<li>targets (batch_size, sequence_length(padded))</li>
<li>mask (batch_size, sequence_length(padded)</li>
</ul>
<p>naive implementation which works without taking into account the mask:</p>
<pre class=""lang-py prettyprint-override""><code>import torch.nn.functional as F
loss = F.nll_loss(log_probas.transpose(1, 2), targets)
</code></pre>
<p>I've been crawling the internet and banging my head but can't seem to find an answer on how to incorporate a mask into the averaging scheme of the loss.</p>
","language-model"
"66561475","loading a FastText model in MATLAB","2021-03-10 09:03:17","","0","93","<python><matlab><gensim><fasttext><language-model>","<p>I have trained a FastText model in Python and saved the files into a folder. These are the contents of the folder:</p>
<pre><code>fasttext.model
fasttext.model.trainables.syn1neg.npy
fasttext.model.trainables.vectors_ngrams_lockf.npy
fasttext.model.trainables.vectors_vocab_lockf.npy
fasttext.model.wv.vectors.npy
fasttext.model.wv.vectors_ngrams.npy
fasttext.model.wv.vectors_vocab.npy
</code></pre>
<p>How can I load the model in MATLAB and extract the word embeddings of certain words?
This is what we do in Python:</p>
<pre><code>from gensim.models.fasttext import FastText
model = FastText.load(fasttext.model)
vector = model.wv[word]
</code></pre>
<p>Is there a similar thing in MATLAB? How can I get the word embeddings generated by a FastText model in Python in MATLAB and work with them?</p>
","language-model"
"66518375","How is transformers loss calculated for blank token predictions?","2021-03-07 15:51:16","66526823","3","1580","<machine-learning><nlp><transformer-model><language-model>","<p>I'm currently trying to implement a transformer and have trouble understanding its loss calculation.</p>
<p>My encoders input looks for batch_size=1  and max_sentence_length=8 like:</p>
<pre><code>[[Das, Wetter, ist, gut, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>My decoders input looks like (german to english):</p>
<pre><code>[[&lt;start&gt;, The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>Let's say my transformer predicted those class probabilities (only showing the word for the class with the highest class probability):</p>
<pre><code>[[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
</code></pre>
<p>Now I calculate the loss using:</p>
<pre><code>loss = categorical_crossentropy(
   [[The, good, is, weather, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]],
   [[The, weather, is, good, &lt;end&gt;, &lt;blank&gt;, &lt;blank&gt;, &lt;blank&gt;]]
)
</code></pre>
<p>Is this the correct way to calculate the loss? My transformer always predicts the blank token for the next word and I thought that's because I have a mistake in my loss calculation and have to do something with the blank tokens before calculating the loss.</p>
","language-model"
"66509682","Running into troubles with data path (os.path.isdir(path) returning false when it exists) using FB XLM","2021-03-06 19:12:59","","1","102","<directory><path><bert-language-model><language-model>","<p>I'm trying to run an evaluation on FB's Transcoder (<a href=""https://github.com/facebookresearch/TransCoder"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/TransCoder</a>) which implements FB's XLM (cross language model): <a href=""https://github.com/facebookresearch/XLM#iii-applications-supervised--unsupervised-mt"" rel=""nofollow noreferrer"">https://github.com/facebookresearch/XLM#iii-applications-supervised--unsupervised-mt</a></p>
<p>I have everything configured and follow the instructions as in the github page where I run the validation/test set unzip them and place them into a specific directory: <code>/home/username/TransCoder/data/bpe.cpp-java-python.with_comments/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test</code>.
When I run the following command that is in the <strong>run an evaluation</strong> section:</p>
<pre><code>python XLM/train.py 
--n_heads 8 
--bt_steps 'python_sa-cpp_sa-python_sa,cpp_sa-python_sa-cpp_sa,java_sa-cpp_sa-java_sa,cpp_sa-java_sa-cpp_sa,python_sa-java_sa-python_sa,java_sa-python_sa-java_sa' # The evaluator will use this parameter to infer the languages to test on 
--max_vocab '-1'  
--word_blank '0.1' 
--n_layers 6  
--generate_hypothesis true 
--max_len 512 
--bptt 256  
--fp16 true 
--share_inout_emb true 
--tokens_per_batch 6000 
--has_sentences_ids true 
--eval_bleu true  
--split_data false  
--data_path '/home/salzubi/TransCoder/data/bpe.cpp-java-python.with_comments/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test'  
--eval_computation true 
--batch_size 32 
--reload_model 'model_1.pth,model_1.pth'  
--amp 2  
--max_batch_size 128 
--ae_steps 'cpp_sa,python_sa,java_sa' 
--emb_dim 1024 
--eval_only True 
--beam_size 10 
--retry_mistmatching_types 1 
--dump_path '/tmp/' 
--exp_name='eval_final_model_wc_30' 
--lgs 'cpp_sa-java_sa-python_sa' 
--encoder_only=False
</code></pre>
<p>I get the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;XLM/train.py&quot;, line 337, in &lt;module&gt;
    check_data_params(params)
  File &quot;/home/username/TransCoder/XLM/src/data/loader.py&quot;, line 246, in check_data_params
    assert os.path.isdir(params.data_path), params.data_path
AssertionError
</code></pre>
<p>I'm not sure why this is the case?</p>
<pre><code>$ python
import os
print(os.path.isdir(&quot;/home/username/TransCoder/data/bpe.cpp-java-python.with_comments/XLM-cpp-java-python-with-comments-functions-sa-cl-split-test&quot;))

output:
True
</code></pre>
<p>Any ideas on how to fix this?</p>
","language-model"
"66400723","Set up kenlm for Windows","2021-02-27 16:08:13","66400724","0","2382","<language-model><kenlm><make-scorer>","<p>The official <a href=""https://kheafield.com/code/kenlm/"" rel=""nofollow noreferrer"">website</a> makes it pretty clear that there is no support for <code>kenlm</code> in Windows. There is a Windows tag at the <a href=""https://github.com/kpu/kenlm/releases/tag/windows"" rel=""nofollow noreferrer"">github repository</a> but it seems to be maintained by few random contributors then and there.</p>
<p>How to set up kenlm for Windows then?</p>
","language-model"
"66348221","Solve Speed Difference in ktrain Predictor vs. Learner prediction?","2021-02-24 09:38:52","","1","118","<python><language-model><ktrain>","<p>I am using ktrain huggingface library to build a language model. When implementing it for production, I noticed, there is a huge difference in speed for a &quot;learner prediction&quot; vs. a &quot;predictor prediction&quot;.
How come and is there any way to speed up the predictor prediction?</p>
<pre><code>%timeit test = learner.predict(val) # takes 10s
%timeit test = predictor.predict(x_val,return_proba = True) # takes 25s
</code></pre>
","language-model"
"66276186","HuggingFace - GPT2 Tokenizer configuration in config.json","2021-02-19 10:53:15","66278433","2","4221","<pytorch><huggingface-transformers><language-model><huggingface-tokenizers><gpt-2>","<p>The GPT2 finetuned model is uploaded in <a href=""https://huggingface.co/models"" rel=""nofollow noreferrer"">huggingface-models</a> for the inferencing</p>
<p>Below error is observed during the inference,</p>
<p><strong>Can't load tokenizer using from_pretrained, please update its configuration: Can't load tokenizer for 'bala1802/model_1_test'. Make sure that: - 'bala1802/model_1_test' is a correct model identifier listed on 'https://huggingface.co/models' - or 'bala1802/model_1_test' is the correct path to a directory containing relevant tokenizer files</strong></p>
<p>Below is the configuration - config.json file for the Finetuned huggingface model,</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;gpt2&quot;,
  &quot;activation_function&quot;: &quot;gelu_new&quot;,
  &quot;architectures&quot;: [
    &quot;GPT2LMHeadModel&quot;
  ],
  &quot;attn_pdrop&quot;: 0.1,
  &quot;bos_token_id&quot;: 50256,
  &quot;embd_pdrop&quot;: 0.1,
  &quot;eos_token_id&quot;: 50256,
  &quot;gradient_checkpointing&quot;: false,
  &quot;initializer_range&quot;: 0.02,
  &quot;layer_norm_epsilon&quot;: 1e-05,
  &quot;model_type&quot;: &quot;gpt2&quot;,
  &quot;n_ctx&quot;: 1024,
  &quot;n_embd&quot;: 768,
  &quot;n_head&quot;: 12,
  &quot;n_inner&quot;: null,
  &quot;n_layer&quot;: 12,
  &quot;n_positions&quot;: 1024,
  &quot;resid_pdrop&quot;: 0.1,
  &quot;summary_activation&quot;: null,
  &quot;summary_first_dropout&quot;: 0.1,
  &quot;summary_proj_to_labels&quot;: true,
  &quot;summary_type&quot;: &quot;cls_index&quot;,
  &quot;summary_use_proj&quot;: true,
  &quot;task_specific_params&quot;: {
    &quot;text-generation&quot;: {
      &quot;do_sample&quot;: true,
      &quot;max_length&quot;: 50
    }
  },
  &quot;transformers_version&quot;: &quot;4.3.2&quot;,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 50257
}
</code></pre>
<p>Should I configure the GPT2 Tokenizer just like the <code>&quot;model_type&quot;: &quot;gpt2&quot;</code> in the config.json file</p>
","language-model"
"65987683","Modifying the Learning Rate in the middle of the Model Training in Deep Learning","2021-02-01 05:42:01","65991030","2","1878","<deep-learning><pytorch><huggingface-transformers><language-model><gpt-2>","<p>Below is the code to configure <a href=""https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments"" rel=""nofollow noreferrer"">TrainingArguments</a> consumed from the <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">HuggingFace transformers</a> library to finetune the <a href=""https://huggingface.co/transformers/model_doc/gpt2.html"" rel=""nofollow noreferrer"">GPT2</a> language model.</p>
<pre><code>training_args = TrainingArguments(
        output_dir=&quot;./gpt2-language-model&quot;, #The output directory
        num_train_epochs=100, # number of training epochs
        per_device_train_batch_size=8, # batch size for training #32, 10
        per_device_eval_batch_size=8,  # batch size for evaluation #64, 10
        save_steps=100, # after # steps model is saved
        warmup_steps=500,# number of warmup steps for learning rate scheduler
        prediction_loss_only=True,
        metric_for_best_model = &quot;eval_loss&quot;,
        load_best_model_at_end = True,
        evaluation_strategy=&quot;epoch&quot;,
        learning_rate=0.00004, # learning rate
    )

early_stop_callback = EarlyStoppingCallback(early_stopping_patience  = 3)
    
trainer = Trainer(
        model=gpt2_model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        callbacks = [early_stop_callback],
 )
</code></pre>
<p>The <strong>number of epochs</strong> as <strong>100</strong> and <strong>learning_rate</strong> as <strong>0.00004</strong> and also the <strong>early_stopping</strong> is configured with the patience value as <strong>3</strong>.</p>
<p>The model ran for <strong>5/100</strong> epochs and noticed that the difference in loss_value is negligible. The latest checkpoint is saved as <code>checkpoint-latest</code>.</p>
<p>Now Can I modify the <code>learning_rate</code> may be to <code>0.01</code> from <code>0.00004</code> and resume the training from the latest saved checkpoint - <code>checkpoint-latest</code>? Doing that will be efficient?</p>
<p>Or to train with the new <code>learning_rate</code> value should I start the <strong>training</strong> from the beginning?</p>
","language-model"
"65925640","Assigning weights during testing the bert model","2021-01-27 18:58:04","66193869","0","1587","<bert-language-model><huggingface-transformers><transformer-model><language-model>","<p>I have a basic conceptual doubt. When i train a bert model on sentence say:</p>
<pre><code>Train: &quot;went to get loan from bank&quot; 
Test :&quot;received education loan from bank&quot;
</code></pre>
<p>How does the test sentence assigns the weights for each token because i however dont pass exact sentence for testing and there is a slight addition  of words like &quot;education&quot; which change the context slightly</p>
<p>Assuming such context is not trained in my model how the weights are assigned for each token in my bert before i fine tune further</p>
<p>If i confuse with my question, simply put i am trying to understand how the weights get assigned during testing if a slight variation in context occurs that was not trained on.</p>
","language-model"
"65529156","Huggingface Transformer - GPT2 resume training from saved checkpoint","2021-01-01 11:07:28","65563077","3","3362","<python><pytorch><huggingface-transformers><language-model><gpt-2>","<p>Resuming the <code>GPT2</code> finetuning, implemented from <code>run_clm.py</code></p>
<p>Does GPT2 <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface</a>  has a parameter to resume the training from the saved checkpoint, instead training again from the beginning? Suppose the python notebook crashes while training, the checkpoints will be saved, but when I train the model again still it starts the training from the beginning.</p>
<p>Source: <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">here</a></p>
<p>finetuning code:</p>
<pre><code>!python3 run_clm.py \
    --train_file source.txt \
    --do_train \
    --output_dir gpt-finetuned \
    --overwrite_output_dir \
    --per_device_train_batch_size 2 \
    --model_name_or_path=gpt2 \
    --save_steps 100 \
    --num_train_epochs=1 \
    --block_size=200 \
    --tokenizer_name=gpt2
</code></pre>
<p>From the above code, <code>run_clm.py</code> is a script provided by <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_clm.py"" rel=""nofollow noreferrer"">huggingface</a> to finetune gpt2 to train with the customized dataset</p>
","language-model"
"65508857","How to get text from XML via Python?","2020-12-30 14:50:19","65509763","0","128","<python><xml><data-processing><txt><language-model>","<p>I'm training language model. My input are subtitles in XML format. I need to get just plain text from it and save to to a text file so I can work with it.</p>
<h2>Input</h2>
<pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; &lt;document&gt; &lt;s id=&quot;1&quot;&gt; &lt;time id=&quot;T1S&quot; value=&quot;00:00:14,660&quot; /&gt; &lt;w id=&quot;1.1&quot;&gt;-&lt;/w&gt; &lt;w id=&quot;1.2&quot;&gt;Všetko&lt;/w&gt; &lt;w id=&quot;1.3&quot;&gt;v&lt;/w&gt; &lt;w id=&quot;1.4&quot;&gt;poriadku&lt;/w&gt; &lt;w id=&quot;1.5&quot;&gt;.&lt;/w&gt; &lt;/s&gt;&lt;/document&gt;
</code></pre>
<h2>Output</h2>
<pre><code>- Všetko v poriadku . 
</code></pre>
","language-model"
"65107718","Best approach for semantic similarity in large documents using BERT or LSTM models","2020-12-02 12:02:44","","1","1369","<python><bert-language-model><language-model>","<p>I am trying to build a search application for resumes which are in .pdf format. For a given search query like &quot;who is proficient in Java and worked in an MNC&quot;, the output should be the CV which is most similar. My plan is to read pdf text and find the cosine similarity between the text and the query.</p>
<p>However, BERT has a problem with long documents. It supports a sequence length of only 512 but all my CVs have more than 1000 words. I am really stuck here. Methods like truncating the documents don't suit the purpose.</p>
<p>Is there any other model that can do this?</p>
<p>I could not find the right approach with models like Longformer and XLNet for this task.</p>
<pre><code>module_url = &quot;https://tfhub.dev/google/universal-sentence-encoder/4&quot; 
model = hub.load(module_url)
print (&quot;module %s loaded&quot; % module_url)

corpus = list(documents.values())
sentence_embeddings = model(corpus)
query = &quot;who is profiecient in C++ and has Rust&quot;
query_vec = model([query.lower()])[0]

doc_names = list(documents.keys())

results = []
for i,sent in enumerate(corpus):
  sim = cosine(query_vec, model([sent])[0])
  results.append((i,sim))
  #print(&quot;Document = &quot;, doc_name[i], &quot;; similarity = &quot;, sim)

print(results)
results= sorted(results, key=lambda x: x[1], reverse=True)
print(results)

for idx, distance in results[:5]:
  print(doc_names[idx].strip(), &quot;(Cosine Score: %.4f)&quot; % (distance))
</code></pre>
","language-model"
"64986092","How to train a keras tokenizer on a large corpus that doesn't fit in memory?","2020-11-24 12:02:35","","2","259","<python><machine-learning><keras><nlp><language-model>","<p>I am trying to train a language model that based on a 2-word input tries to predict a 1-word output. This is the model definition (all the layers are imported from <code>keras.layers</code>):</p>
<pre class=""lang-py prettyprint-override""><code>model = Sequential()
model.add(Embedding(vocab_size, 2, input_length=seq_length))
model.add(LSTM(100, return_sequences=True))
model.add(LSTM(100))
model.add(Dense(100, activation='relu'))
model.add(Dense(vocab_size, activation='softmax'))
print(model.summary())
</code></pre>
<p>The problem is that my dataset has 87 million lines of 3-word data (2 for input, 1 for output) and it does not fit into my memory. I heard that <code>keras.preprocessing.text.Tokenizer</code> creates tokens based on their frequency in text. I am training my tokenizer like this:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = Tokenizer(oov_token=1)
tokenizer.fit_on_texts(lines)
sequences = tokenizer.texts_to_sequences(lines)
</code></pre>
<p>How am I supposed to fit my tokenizer on all texts if they don't fit into memory?</p>
","language-model"
"64750834","Keras Lstm predicting next item, taking whole sequences or sliding window. Will sliding window need stateful LSTM?","2020-11-09 11:32:19","","4","870","<python><keras><lstm><language-model><lstm-stateful>","<p>I have a sequence prediction problem in which, given the last <code>n</code> items in a sequence I need to predict next item.</p>
<p>I have more than 2 million  sequences  each with different <code>timesteps</code> (<code>length of sequence</code>), like some are just 5 and some are 50/60/100/200 upto 500.</p>
<pre><code>    seq_inputs = [
    [&quot;AA1&quot;, &quot;BB3&quot;, &quot;CC4&quot;,…,&quot;DD5&quot;], #length/timeteps 5
    [&quot;FF1&quot;, &quot;DD3&quot;, &quot;FF6&quot;,&quot;KK8&quot;,&quot;AA5&quot;, &quot;CC8&quot;,…, &quot;AA2&quot;]   #length/timeteps 50
   [&quot;AA2&quot;, &quot;CC8&quot;, &quot;CC11&quot;,&quot;DD3&quot;, &quot;FF6&quot;,&quot;AA1&quot;, &quot;BB3&quot;,……,”DD11”]#length/timesteps 200
    ..
    ..
    ] # there are 2million + of them 
</code></pre>
<p>For prediction next item in sequence, I <code>trim</code> sequences to 60 maximum length with <code>post/pre padding</code> and just take last element of all sequences</p>
<p>for e.g, X’s will be</p>
<pre><code>[[0,0,0,….,'AA1', 'BB3', 'CC4'],#lenght 60
 [0,0,0,….,'FF1', 'DD3', 'FF6', 'KK8', 'AA5', 'CC8'],#lenght 60
 [0,0,0,….,'AA2', 'CC8', 'CC11', 'DD3', 'FF6', 'AA1', 'BB3']#lenght 60
 ....
 ]
</code></pre>
<p>and y is last element</p>
<pre><code>['DD5', 'AA2', 'DD11',...]
</code></pre>
<p>First I tokenise them and convert them in numeric form using <code>keras tokenizer.text_to_sequence()</code>and reshape them to 60 time steps and one feature for every sequence:**</p>
<pre><code>X = [
    [[0],[0],[0],[0],[1], ..., [10], [200], [5], [3], [90] ],
    [[0],[0],[0],[0],[95],...,  [15], [4],[11],[78], [43]]
    ..
    ..
    ] 
y = [40,3, ... , ... ]
</code></pre>
<p>I am using LSTMs with embedding like below</p>
<pre><code>model = Sequential()
model.add(Embedding(vocabulary_size, 32, input_length=seq_len)) #seq_length
model.add(LSTM(80,return_sequences=True))
..
..
model.fit(train_inputs,train_targets,epochs=50,verbose=1,batch_size=32)
</code></pre>
<p>For my problem of predicting next item in sequence, should  this approach (of trimming sequences to 60 max length with post/pre padding and just taking last item as target) good?
As target will be different timestep for each like 5th,50th,200th and so on , in my example.</p>
<p>Should I make every sequence n-gram/sliding window? For example for this first sequence of of my dataset</p>
<pre><code>   [&quot;AA1&quot;, &quot;BB3&quot;, &quot;CC4&quot;,…,&quot;DD5&quot;]
</code></pre>
<p>Sliding window of 5 , first example will be will be converted as</p>
<pre><code>seq_inputs = [
   [0,0,0,0,&quot;AA1&quot;]
   [0,0,0,&quot;AA1&quot;, &quot;BB3&quot;]
   [0,0,&quot;AA1&quot;, &quot;BB3&quot;,&quot;CC4&quot;],
…,
... 
] 
</code></pre>
<p>And similarly others will also be converted to sliding windows.</p>
<p><strong>To summarise the problem and questions again:</strong></p>
<p>With current approach, taking last element as <code>y</code>, I am struck at 30 validation accuracy, but my concern is not performance, my concern is if I am doing it right. So, Need guidance on following</p>
<ol>
<li>Since I need to predict next item in sequence, is taking last item  as output for each sequence right approach?</li>
<li>Since my input length varies (from 5 to 500) and I am restricting it to 60 timesteps, should I increase or decrease it?</li>
<li>Instead of taking whole sequence should I take sliding window approach like I shared?</li>
<li>Will I need to have stateful LSTM in case of sliding windows?</li>
</ol>
","language-model"
"64543626","How to work with n-grams for classification tasks?","2020-10-26 19:17:19","","0","556","<python><nlp><classification><n-gram><language-model>","<p>I'm going to train a classifier on a sample dataset using <code>n-gram</code>. I searched for related content and wrote the code below. As I'm a beginner in python, I have <strong>two questions</strong>.</p>
<p>1- Why should the dictionary have this 'True' structure (marked with comment)? Is this related to Naive Bayes Classifier input?</p>
<p>2- Which classifier do you recommend to do this task?</p>
<p>Any other suggestion to shorten the code are welcome :).</p>
<pre><code>from nltk.corpus import movie_reviews
from nltk.corpus import stopwords
from nltk import ngrams
from nltk.classify import NaiveBayesClassifier
import nltk.classify.util


stoplist = set(stopwords.words(&quot;english&quot;))


def stopword_removal(words):
    useful_words = [word for word in words if word not in stoplist]
    return useful_words


def create_ngram_features(words, n):
    ngram_vocab = ngrams(words, n)
    my_dict = dict([(ng, True) for ng in ngram_vocab])  # HERE
    return my_dict


for n in [1,2]:
    positive_data = []
    for fileid in movie_reviews.fileids('pos'):
        words = stopword_removal(movie_reviews.words(fileid))
        positive_data.append((create_ngram_features(words, n), &quot;positive&quot;))
    print('\n\n---------- Positive Data Sample----------\n', positive_data[0])

    negative_data = []
    for fileid in movie_reviews.fileids('neg'):
        words = stopword_removal(movie_reviews.words(fileid))
        negative_data.append((create_ngram_features(words, n), &quot;negative&quot;))
    print('\n\n---------- Negative Data Sample ----------\n', negative_data[0])

    train_set = positive_data[:100] + negative_data[:100]
    test_set = positive_data[100:] + negative_data[100:]

    classifier = NaiveBayesClassifier.train(train_set)

    accuracy = nltk.classify.util.accuracy(classifier, test_set)
    print('\n', str(n)+'-gram accuracy:', accuracy)
</code></pre>
","language-model"
"64480794","Feed Forward Neural Network Language Model","2020-10-22 10:57:37","","2","382","<python><tensorflow><neural-network><language-model>","<p>I am currently in the process of trying to develop a feed-forward neural network n-gram language model using TensorFlow 2.0. Just to be clear, I do not want this to be implemented via a recurrent neural network, I simply want to use a few Dense layers and a Softmax layer to accomplish this.
This is the reference that I have used; the architecture of the model has also been outlined,
<a href=""https://www.researchgate.net/publication/301875194_Authorship_Attribution_Using_a_Neural_Network_Language_Model"" rel=""nofollow noreferrer"">https://www.researchgate.net/publication/301875194_Authorship_Attribution_Using_a_Neural_Network_Language_Model</a></p>
<p>However, when I tried to do this, I kept getting an error. Given below is my model,</p>
<pre><code>tf.keras.optimizers.Adam(learning_rate=0.01)
model = tf.keras.Sequential([
                             tf.keras.layers.Embedding(total_words, 300, weights = [embeddings_matrix], input_length=inputs.shape[1], trainable = False),
                             tf.keras.layers.Dense(100, activation = 'relu'),
                             tf.keras.layers.Dense(total_words, activation = 'softmax')
])

model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
</code></pre>
<p>When this code is run, the error I get is as follows,</p>
<p><code>ValueError: Shapes (None, 7493) and (None, 116, 7493) are incompatible</code></p>
<p>Can someone please tell me how to resolve this? I am slightly confused.</p>
","language-model"
"64145666","Fine tuning of Bert word embeddings","2020-09-30 20:58:50","","1","8071","<python><pytorch><word-embedding><bert-language-model><language-model>","<p>I would like to load a pre-trained Bert model and to fine-tune it and particularly the word embeddings of the model using a custom dataset.
The task is to use the word embeddings of chosen words for further analysis.
It is important to mention that the dataset consists of tweets and there are no labels.
Therefore, I used the BertForMaskedLM model.</p>
<p>Is it OK for this task to use the input ids (the tokenized tweets) as the labels?
I have no labels. There are just tweets in randomized order.</p>
<p>From this point, I present the code I wrote:</p>
<p>First, I cleaned the dataset from emojis, non-ASCII characters, etc as described in the following link (2.3 Section):
<a href=""https://www.kaggle.com/jaskaransingh/bert-fine-tuning-with-pytorch"" rel=""nofollow noreferrer"">https://www.kaggle.com/jaskaransingh/bert-fine-tuning-with-pytorch</a></p>
<p>Second, the code of the fine tuning process:</p>
<pre><code>import torch

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

model = BertForMaskedLM.from_pretrained('bert-base-uncased')

model.to(device)
model.train()

lr = 1e-2

optimizer = AdamW(model.parameters(), lr=lr, correct_bias=False)
max_len = 82
chunk_size = 20
epochs = 20

for epoch in range(epochs):
    epoch_losses = []
    for j, batch in enumerate(pd.read_csv(path + file_name, chunksize=chunk_size)):
        tweets = batch['content_cleaned'].tolist()
    
        encoded_dict = tokenizer.batch_encode_plus(
                            tweets,                      # Sentence to encode.
                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                            max_length = max_len,           # Pad &amp; truncate all sentences.
                            pad_to_max_length = True,
                            truncation=True,
                            return_attention_mask = True,   # Construct attn. masks.
                            return_tensors = 'pt',     # Return pytorch tensors.
                       )
        input_ids = encoded_dict['input_ids'].to(device)
        
        # Is it correct? or should I train it in another way?
        loss, _ = model(input_ids, labels=input_ids)
        loss_score = loss.item()

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
        optimizer.zero_grad()
model.save_pretrained(path + &quot;Fine_Tuned_BertForMaskedLM&quot;)
</code></pre>
<p>The loss starts from 50 and reduced until 2.3.</p>
","language-model"
"64132343","How can i use kenlm to check word alignment in a sentence?","2020-09-30 06:55:12","","0","372","<spell-checking><language-model><kenlm>","<p>I have seen many blogs saying language models can be used for numerous tasks but I cannot find any good implementations other than just the text generation.</p>
<p>My query is,
How can i use a language model like kenlm to correct my sentences for alignments and spelling mistakes. I know kenlm already does this for deepspeech but i want to understand how it actually does and how can i implement the same.</p>
","language-model"
"63858339","Obtaining the Probability of a Sentence using a Language Model","2020-09-12 08:21:22","","1","365","<python><tensorflow><deep-learning><nlp><language-model>","<p>I have trained a language model using the following architecture,</p>
<pre><code>model = tf.keras.Sequential([

tf.keras.layers.Embedding(total_words, 300, weights=[embeddings_matrix], input_length=inputs.shape[1], trainable=False),

tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(150)),

tf.keras.layers.Dense(total_words, activation = 'softmax')

])
</code></pre>
<p>Note that I have used a pre-trained layer of word embeddings for this purpose.</p>
<p>What I would like to do now is, given an entirely new sentence, to generate the probability of that sequence based on this language model.</p>
<p>This is not about predicting the next word in the sequence, but about generating the probability of the entire sentence provided.</p>
<p>How do I do this in TensorFlow 2?
I have seen similar questions posted based on TensorFlow 1, but not with relation to the new version.</p>
","language-model"
"63606049","fine tune causal language model using transformers and pytorch","2020-08-26 21:35:17","","0","3158","<python-3.x><pytorch><huggingface-transformers><language-model>","<p>I have some questions about fine-tuning causal language model using transformers and PyTorch.</p>
<p>My main goal is to fine-tune XLNet. However, I found the most of posts online was targeting at text classification, like this <a href=""https://mccormickml.com/2019/09/19/XLNet-fine-tuning/"" rel=""nofollow noreferrer"">post</a>. I was wondering, is there any way to fine-tune the model, without using the <a href=""https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py"" rel=""nofollow noreferrer""><code>run_language_model.py</code></a> from transformers' GitHub?</p>
<p>Here is a piece of my code trying to fine-tune XLNet:</p>
<pre class=""lang-py prettyprint-override""><code>model = XLNetLMHeadModel.from_pretrained(&quot;xlnet-base-cased&quot;)
tokenizer = XLNetTokenizer.from_pretrained(&quot;xlnet-base-cased&quot;, do_lower_case=True)
LOSS = torch.nn.CrossEntrypoLoss()
batch_texts = [&quot;this is sentence 1&quot;, &quot;i have another sentence like this&quot;, &quot;the final sentence&quot;]
encodings = tokenizer.encode_plus(batch_texts, add_special_tokens=True,
                                  return_tensors=True, return_attention_mask=True)
outputs = model(encodings[&quot;input_ids&quot;], encodings[&quot;attention_mask&quot;])
loss = LOSS(outputs[0], target_ids)
loss.backward()
# ignoring the rest of codes...
</code></pre>
<p>I got stuck at the last two lines. At first, when using this LM model, it seems I don't have any <code>labels</code> as the supervised learning usually do; Second, as the language model which is to minimize the loss (cross-entropy here), I need a <code>target_ids</code> to compute the loss and perplexity with <code>input_ids</code>.</p>
<p>Here are my follow-up questions:</p>
<ol>
<li>How should I deal with this <code>labels</code> during the model fitting?</li>
<li>Should I set something like <code>target_ids=encodings[&quot;input_ids&quot;].copy()</code> to compute cross-entropy loss and perplexity?</li>
<li>If not, how should set this <code>target_ids</code>?</li>
<li>From the perplexity page from transformers' <a href=""https://huggingface.co/transformers/perplexity.html"" rel=""nofollow noreferrer"">documentation</a>, how should I adapt its method for non-fixed length of input text?</li>
<li>I saw another <a href=""https://huggingface.co/transformers/task_summary.html#text-generation"" rel=""nofollow noreferrer"">post</a> from the documentation saying that it requires padding text for causal language modeling. However, from the link in 3), there is no such sign for padding text. Which one should I follow?</li>
</ol>
<p>Any suggestions and advice will be appreciated!</p>
","language-model"
"63316759","LM in elastic search","2020-08-08 14:53:56","","0","206","<elasticsearch><search><indexing><stemming><language-model>","<p>how can I improve recall for this condition ?any suggestion?
I want to create an index with 39 million passages each one containing at least four sentences in English. My queries are short and interrogative sentences. I know that a language model with Dirichlet smoothing, stop word removal and stemmer is best for this condition. how can I index with these conditions (I've indexed with this configs but there is no difference in results with default bm25)</p>
<p><strong>My index:</strong></p>
<pre><code>{
&quot;settings&quot;: {
&quot;index&quot;:{
            &quot;similarity&quot; : {
          &quot;my_similarity&quot; : {
            &quot;type&quot; : &quot;LMDirichlet&quot;,
            &quot;mu&quot; : 2000
          }
        },
  &quot;analysis&quot;:{
    &quot;filter&quot;:{
      &quot;english_stop&quot;:{
        &quot;type&quot;:&quot;stop&quot;,
        &quot;stopwords&quot;:&quot;_english_&quot;
      },
      &quot;my_stemmer&quot;:{
        &quot;type&quot;:&quot;stemmer&quot;,
        &quot;name&quot;:&quot;english&quot;
      }
    },
    &quot;analyzer&quot;:{
      &quot;my_custom_analyzer&quot;:{
        &quot;type&quot;:&quot;custom&quot;,
        &quot;tokenizer&quot;:&quot;standard&quot;,
        &quot;filter&quot;:[
          &quot;lowercase&quot;,
          &quot;english_stop&quot;,
          &quot;my_stemmer&quot;
          ]
      }
    }
  }
},
    &quot;number_of_shards&quot;: 1
},
&quot;mappings&quot;: {
    &quot;properties&quot;: {
        &quot;content&quot;: {
        &quot;similarity&quot; : &quot;my_similarity&quot; ,
        &quot;analyzer&quot;: &quot;my_custom_analyzer&quot;,
            &quot;type&quot;: &quot;text&quot;
        }
    }
}
}
</code></pre>
<p><strong>and for searching my python code is:</strong></p>
<pre><code>query = &quot; (&quot; + prevTurn + &quot;)^1 (&quot; + currentTurn + &quot;)^2&quot;

search_param={
&quot;query&quot;: {
&quot;query_string&quot;: {
&quot;query&quot;:query,
&quot;analyzer&quot;: &quot;my_stop_analyzer&quot;,
&quot;default_field&quot;:&quot;doc.content&quot;
}
}
}
</code></pre>
<p><strong>one sample turn:</strong></p>
<pre><code>Title: The Neolithic Revolution
Description: The neolithic revolution and technology used within it and when it emerged in the british isles.  Also, the transition to the bronze age and its significance.
1   What was the neolithic revolution?
2   When did it start and end?
3   Why did it start?
4   What did the neolithic invent?
5   What tools were used?
6   When was it brought to the british isles?
</code></pre>
","language-model"
"63244440","Explicit likelihood of WordPiece used for pre-processing of BERT","2020-08-04 09:58:08","","2","178","<nlp><tokenize><bert-language-model><language-model><log-likelihood>","<p>At each iteration the WordPiece algorithm for subword tokenization merges the two symbols which increase the likelihood the most. Now, in the literature it is only mentioned that this likelihood is the likelihood of the language model  (e.g., the same likelihood used during decoding, in case of NMT). Does anyone know which likelihood was used for pre-processing of BERT?</p>
","language-model"
"63201518","RNN Language Model in PyTorch predicting the same three words repeatedly","2020-08-01 03:40:55","","1","676","<machine-learning><nlp><pytorch><recurrent-neural-network><language-model>","<p>I am attempting to create a word-level language model using an RNN in PyTorch. Whenever I am training the loss stays about the same for the whole training set and when I try to sample a new sentence the same three words are predicted in the same order. For example in my most recent attempt the RNN predicted 'the' then 'same' then 'of' and that sequence just kept repeating. I have tried changing the how I've set up the RNN including using LSTM's, GRU's, and different embeddings but so far nothing has worked.</p>
<p>The way that I am training the RNN is by taking a sentence of 50 words, and selecting a progressively larger part of the sentence with the next word being the target. At the end of the sentence, I have an EOS tag. I am using text from <em>Republic</em> by Plato as my training set and embedding it using a pytorch embedding layer. I am then feeding it into an LSTM and then a linear layer to get the right shape. I am not sure if the problem is in the RNN, the data, the training or what so any help would be greatly appreciated.</p>
<p>If anyone has any experience in nlp or in language modeling I would greatly appreciate any help you could offer for this fixing this problem. My end goal is simply to just be able to generate a sentence. Thanks in advance!</p>
<p>Here is my RNN</p>
<pre><code>class LanguageModel(nn.Module):
  &quot;&quot;&quot;
    Class that defines the reccurent neural network.

    Methods
    -------
    forward(input, h, c)
      Forward propogation through the RNN.
    initHidden()
      Initializes the hidden and cell states.
  &quot;&quot;&quot;
  def __init__(self, vocabSize, seqLen = 51, embeddingDim = 30, hiddenSize = 32, numLayers = 1, bid = False):
    &quot;&quot;&quot;
      Initializes the class

      Parameters
      ----------
      seqLen : int, optional
        The length of the input sequence.
      embeddingDim : int, optional
        The dimension that the embedding dimension for the encoder should be.
      vocabSize : int
        The length of the vocab dictionary.
      hiddenSize : int, optional
        The size that the hidden state should be.
      numLayers : int, optional
        The number of LSTM Layers.
      bid : bool, optional
        Whether the RNN should be bidirctional or not.
    &quot;&quot;&quot;

    super(LanguageModel, self).__init__()
    self.hiddenSize = hiddenSize
    self.numLayers = numLayers

    # Set value of numDirections based on whether or not the RNN is bidirectional.
    if bid == True:
      self.numDirections = 2
    else:
      self.numDirections = 1

    self.encoder = nn.Embedding(vocabSize, embeddingDim)
    self.LSTM = nn.LSTM(input_size = embeddingDim, hidden_size = hiddenSize, num_layers = numLayers, bidirectional = bid)
    self.decoder = nn.Linear(seqLen * self.numDirections * hiddenSize, vocabSize)

  def forward(self, input, h, c):
    &quot;&quot;&quot;
      Forward propogates through the RNN

      Parameters
      ----------
      input : torch.Tensor
        Input to RNN. Should be formatter using makeInput() and padSeq().
      h : torch.Tensor
        Hidden state.
      c : torch.Tensor
        Cell state.

      Returns
      -------
      torch.Tensor
        Log probabilities for the predicted word from the RNN.
    &quot;&quot;&quot;

    emb = self.encoder(input)
    emb.unsqueeze_(1) # Add in the batch dimension so the shape is right for the LSTM

    out, (h, c) = self.LSTM(emb, (h, c))
    out = out.view(1, -1) # Reshaping to fit into the loss function.

    out = self.decoder(out)

    logProbs = F.log_softmax(out, dim = 1)

    return logProbs

  def initHidden(self):
    &quot;&quot;&quot;
      Initializes the hidden and cell states.

      Returns
      -------
      torch.Tensor
        Tensor containing the initial hidden state.
      torch.Tensor
        Tensor containing the intial cell state.
    &quot;&quot;&quot;
    h = torch.zeros(self.numLayers * self.numDirections, 1, self.hiddenSize)
    c = torch.zeros(self.numLayers * self.numDirections, 1, self.hiddenSize)
    
    return h, c
</code></pre>
<p>here is how I create my input and targets</p>
<pre><code>def makeInput(sentence):
  &quot;&quot;&quot;
    Prepares a sentence for input to the RNN.

    Parameters
    ----------
    sentence : list
      The sentence to be converted into input. Should be of form: [str] 

    Returns
    -------
    torch.Tensor
      Tensor of the indices for each word in the input sentence.
  &quot;&quot;&quot;

  sen = sentence[0].split() # Split the list into individual words
  sen.insert(0, 'START')

  input = [word2Idx[word] for word in sen] # Iterate over the words in sentence and convert to indices

  return torch.tensor(input)

def makeTarget(sentence):
  &quot;&quot;&quot;
    Prepares a sentence to be a target.

    Parameters
    ----------
    sentence : str
      The sentence to be made into a target. Should be of form: [str]

    Returns
    -------
    torch.Tensor
      Tensor of the indices for the target phrase including the &lt;EOS&gt; tag.
  &quot;&quot;&quot;

  sen = sentence[0].split() # Split the list into individual words
  sen.append('EOS')
  
  target = [word2Idx[word] for word in sen]
  target = torch.tensor(target, dtype = torch.long)

  return target.unsqueeze_(-1) # Removing dimension for loss function 

def padSeq(seq, refSeq):
  &quot;&quot;&quot;
    Pads a sequence to be the same shape as another sequence.

    Parameters
    ----------
    seq : torch.Tensor
      The sequence to pad.
    refSeq : torch.Tensor
      The reference sequence. seq will be padded to be the same shape as refSeq.

    Returns
    -------
    torch.Tensor
      Tensor containing the padded sequence.
  &quot;&quot;&quot;

  padded = pad_sequence([refSeq, seq])
  tmp = torch.t(padded) # Transpose the padded sequence for easier indexing on return

  return tmp[1] # Return only the padded seq not both sequences
</code></pre>
<p>and here is my training loop</p>
<pre><code>def train():
  &quot;&quot;&quot;
    Trains the model.
  &quot;&quot;&quot;

  start = time.time()
  for i, data in enumerate(trainLoader):
    inputTensor = makeInput(data)
    targetTensor = makeTarget(data)

    targetTensor = targetTensor.to(device)

    h, c = model.initHidden()
    h = h.to(device)
    c = c.to(device)
    
    optimizer.zero_grad()
    loss = 0

    for x in range(inputTensor.size(0)): # Iterate over all of the words in the input sentence
      &quot;&quot;&quot; Preparing input for the rnn &quot;&quot;&quot;
      input = inputTensor[: x + 1] # We only want part of the input so the RNN can learn on predicting the next words
      input = padSeq(input, inputTensor)
      input = input.to(device)

      out = model(input, h, c)
      l = criterion(out, targetTensor[x])
      loss += l

    loss.backward()
    optimizer.step()
  
    if i % 250 == 0: # Print updates to the models loss every 10 iters.
      print('[{}] Epoch: {} -&gt; {}'.format(timeSince(start), i, loss / inputTensor.size(0)))
</code></pre>
","language-model"
"63096908","BERT + custom layer training performance going down with epochs","2020-07-26 06:36:45","","2","3847","<tensorflow><machine-learning><nlp><language-model>","<p>I'm training a classification model with custom layers on top of BERT. During this, the training performance of this model is going down with increasing epochs ( after the first epoch ) .. I'm not sure what to fix here - is it the model or the data?</p>
<p>( for the data it's binary labels, and balanced in the number of data points for each label).</p>
<p>Any quick pointers on what the problem could be? Has anyone come across this before?</p>
<p>Edit: Turns out there was a mismatch in the transformers library and tf version I was using. Once I fixed that, the training performance was fine!</p>
<p>Thanks!</p>
","language-model"
"62963127","When using padding in sequence models, is Keras validation accuracy valid/ reliable?","2020-07-17 23:47:20","","5","1535","<tensorflow><machine-learning><keras><deep-learning><language-model>","<p>I have a group of non zero sequences with different lengths and I am using Keras LSTM to model these sequences. I use Keras Tokenizer to tokenize (tokens start from 1). In order to make sequences have the same lengths, I use <strong>padding</strong>.</p>
<p>An example of padding:</p>
<pre><code># [0,0,0,0,0,10,3]
# [0,0,0,0,10,3,4]
# [0,0,0,10,3,4,5]
# [10,3,4,5,6,9,8]
</code></pre>
<p>In order to evaluate if the model is able to generalize, I use a validation set with 70/30 ratio. In the end of each epoch Keras shows the training and validation accuracy.</p>
<p><strong>My big doubt is</strong> whether Keras validation accuracy is reliable <strong>when using padding</strong>. Because the validation set can simply be sequences of 0's --&gt; [0,0,0]. Since there are a lot of sequences of 0's (<strong>because of padding</strong>), the model can easily learn and predict the sequences of 0's correctly, and as a result, create a fake high validation accuracy. In other words the model may learn sequences of zeros and not learn the real sequences.</p>
<p>So, does padding influences the validation accuracy in Keras?</p>
","language-model"
"62830783","Scripts missing for GPT-2 fine tune, and inference in Hugging-face GitHub?","2020-07-10 08:59:58","62830856","1","424","<python><huggingface-transformers><language-model><gpt-2>","<p>I am following the <a href=""https://huggingface.co/transformers/v2.0.0/examples.html"" rel=""nofollow noreferrer"">documentation</a> on the hugging face website, in there they say that to fine-tune GPT-2 I should use the script
<a href=""https://github.com/huggingface/transformers/blob/master/examples/run_lm_finetuning.py"" rel=""nofollow noreferrer"">run_lm_finetuning.py</a> for fine-tuning, and the script <a href=""https://github.com/huggingface/transformers/blob/master/examples/run_generation.py"" rel=""nofollow noreferrer"">run_generation.py</a>
for inference.
However, both scripts don't actually exist on GitHub anymore.</p>
<p>Does anybody know whether the documentation is outdated? or where to find those two scripts?</p>
<p>Thanks</p>
","language-model"
"62798180","How to tell Alexa to punctuate user responses properly. Please see the use case","2020-07-08 15:21:25","","0","42","<alexa-skills-kit><language-model>","<p>I am sorry if this looks like a stupid question!</p>
<p>My skill is recording user responses in the database.  This part is working fine. But my concern is Alexa is not punctuating the response at all. Here is an example:</p>
<p>User: <strong>The loading speed of the website is very slow</strong> (a few milliseconds of pause) <strong>can't we make it faster</strong> <em>(vocal tone was used in such a way so that Alexa can understand that this part is a question)</em></p>
<p>Recorded: <strong>the loading speed of the website is very slow can't we make it faster</strong></p>
<p>Expected: <strong>the loading speed of the website is very slow. can't we make it faster?</strong></p>
<p>Is there any way to accomplish this? Because it is very important to have correctly punctuated responses to be stored in the database. as this skill will be used for project management purpose.</p>
","language-model"
"62772819","HuggingFace Trainer Segmentation Fault","2020-07-07 10:06:23","","3","2099","<python><machine-learning><huggingface-transformers><language-model>","<p>Huggingface Trainer keeps giving <code>Segmentation Fault</code> with this setup code.
The dataset is around 600MB, and the server has 2*32GB Nvidia V100. Can anyone help find the issue?</p>
<pre><code>from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, LineByLineTextDataset
from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast

tokenizer = GPT2TokenizerFast.from_pretrained(&quot;./data/TOKEN&quot;)

config = GPT2Config.from_pretrained('gpt2-large')
model = GPT2LMHeadModel(config=config)
tokenizer = GPT2TokenizerFast.from_pretrained(&quot;./data/TOKEN&quot;, model_max_length=1024)

print('loading dataset...')
dataset = LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=&quot;./data/kowiki.txt&quot;,
    block_size=128,
)

training_args = TrainingArguments(
    output_dir='./m',          # output directory
    num_train_epochs=1,              # total # of training epochs
    per_device_train_batch_size=1,  # batch size per device during training - the higher the better, but may OOM
    per_device_eval_batch_size=1,   # batch size for evaluation
    logging_dir='./logs',            # directory for storing logs
    save_steps=10000,
    do_train=True
)

trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=dataset,         # training dataset
)

trainer.train()
</code></pre>
<p>Error message :</p>
<pre><code>loading dataset...
Epoch:   0%|                                              | 0/1 [00:00&lt;?, ?it/s]
Fatal Python error: Segmentation fault                | 0/99996 [00:00&lt;?, ?it/s]

Thread 0x00007f872dfff700 (most recent call first):
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 299 in wait
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 551 in wait
  File &quot;/opt/conda/lib/python3.6/site-packages/tqdm/_monitor.py&quot;, line 69 in run
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 916 in _bootstrap_inner
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 884 in _bootstrap

Thread 0x00007f8736bb5700 (most recent call first):
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 299 in wait
  File &quot;/opt/conda/lib/python3.6/queue.py&quot;, line 173 in get
  File &quot;/opt/conda/lib/python3.6/site-packages/tensorboard/summary/writer/event_file_writer.py&quot;, line 205 in run
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 916 in _bootstrap_inner
  File &quot;/opt/conda/lib/python3.6/threading.py&quot;, line 884 in _bootstrap

Current thread 0x00007f88273e7740 (most recent call first):
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/cuda/comm.py&quot;, line 39 in broadcast_coalesced
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py&quot;, line 21 in forward
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/replicate.py&quot;, line 71 in _broadcast_coalesced_reshape
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/replicate.py&quot;, line 88 in replicate
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 159 in replicate
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py&quot;, line 154 in forward
  File &quot;/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 577 in __call__
  File &quot;/opt/conda/lib/python3.6/site-packages/transformers/trainer.py&quot;, line 622 in _training_step
  File &quot;/opt/conda/lib/python3.6/site-packages/transformers/trainer.py&quot;, line 499 in train
  File &quot;trainer.py&quot;, line 34 in &lt;module&gt;
Segmentation fault (core dumped)

</code></pre>
<p>Python version is 3.7.7 with Pytorch 1.5.1+cu101
with a 'recently installed' HF transformer &amp; tokenizer(0.8.0).</p>
<p>First time here :D sorry if I didn't keep trivial stuff</p>
<p>EDIT : May be a bug - <a href=""https://github.com/huggingface/transformers/issues/5590"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/5590</a></p>
<p>EDIT 2: Also segfaults on transformer 3.0.2 and tokenizer 0.7.0, 0.8.1-rc1</p>
","language-model"
"62767124","How to use HuggingFace nlp library's GLUE for CoLA","2020-07-07 02:12:25","","1","644","<deep-learning><nlp><language-model><bert-language-model><huggingface-tokenizers>","<p>I've been trying to use the HuggingFace nlp library's GLUE metric to check whether a given sentence is a grammatical English sentence. But I'm getting an error and is stuck without being able to proceed.</p>
<p>What I've tried so far;</p>
<p>reference and prediction are 2 text sentences</p>
<pre><code>!pip install transformers
</code></pre>
<pre><code>from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')
</code></pre>
<pre><code>reference=&quot;Security has been beefed across the country as a 2 day nation wide curfew came into effect.&quot;
prediction=&quot;Security has been tightened across the country as a 2-day nationwide curfew came into effect.&quot;
</code></pre>
<pre><code>import nlp
glue_metric = nlp.load_metric('glue',name=&quot;cola&quot;)

#Using BertTokenizer
encoded_reference=tokenizer.encode(reference, add_special_tokens=False)
encoded_prediction=tokenizer.encode(prediction, add_special_tokens=False)

glue_score = glue_metric.compute(encoded_prediction, encoded_reference)

</code></pre>
<p>Error I'm getting;</p>
<pre><code>
ValueError                                Traceback (most recent call last)
&lt;ipython-input-9-4c3a3ce7b583&gt; in &lt;module&gt;()
----&gt; 1 glue_score = glue_metric.compute(encoded_prediction, encoded_reference)

6 frames
/usr/local/lib/python3.6/dist-packages/nlp/metric.py in compute(self, predictions, references, timeout, **metrics_kwargs)
    198         predictions = self.data[&quot;predictions&quot;]
    199         references = self.data[&quot;references&quot;]
--&gt; 200         output = self._compute(predictions=predictions, references=references, **metrics_kwargs)
    201         return output
    202 

/usr/local/lib/python3.6/dist-packages/nlp/metrics/glue/27b1bc63e520833054bd0d7a8d0bc7f6aab84cc9eed1b576e98c806f9466d302/glue.py in _compute(self, predictions, references)
    101             return pearson_and_spearman(predictions, references)
    102         elif self.config_name in [&quot;mrpc&quot;, &quot;qqp&quot;]:
--&gt; 103             return acc_and_f1(predictions, references)
    104         elif self.config_name in [&quot;sst2&quot;, &quot;mnli&quot;, &quot;mnli_mismatched&quot;, &quot;mnli_matched&quot;, &quot;qnli&quot;, &quot;rte&quot;, &quot;wnli&quot;, &quot;hans&quot;]:
    105             return {&quot;accuracy&quot;: simple_accuracy(predictions, references)}

/usr/local/lib/python3.6/dist-packages/nlp/metrics/glue/27b1bc63e520833054bd0d7a8d0bc7f6aab84cc9eed1b576e98c806f9466d302/glue.py in acc_and_f1(preds, labels)
     60 def acc_and_f1(preds, labels):
     61     acc = simple_accuracy(preds, labels)
---&gt; 62     f1 = f1_score(y_true=labels, y_pred=preds)
     63     return {
     64         &quot;accuracy&quot;: acc,

/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py in f1_score(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)
   1097                        pos_label=pos_label, average=average,
   1098                        sample_weight=sample_weight,
-&gt; 1099                        zero_division=zero_division)
   1100 
   1101 

/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py in fbeta_score(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)
   1224                                                  warn_for=('f-score',),
   1225                                                  sample_weight=sample_weight,
-&gt; 1226                                                  zero_division=zero_division)
   1227     return f
   1228 

/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py in precision_recall_fscore_support(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)
   1482         raise ValueError(&quot;beta should be &gt;=0 in the F-beta score&quot;)
   1483     labels = _check_set_wise_labels(y_true, y_pred, average, labels,
-&gt; 1484                                     pos_label)
   1485 
   1486     # Calculate tp_sum, pred_sum, true_sum ###

/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py in _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
   1314             raise ValueError(&quot;Target is %s but average='binary'. Please &quot;
   1315                              &quot;choose another average setting, one of %r.&quot;
-&gt; 1316                              % (y_type, average_options))
   1317     elif pos_label not in (None, 1):
   1318         warnings.warn(&quot;Note that pos_label (set to %r) is ignored when &quot;

ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].
</code></pre>
<p>However, I'm able to get results (pearson and spearmanr) for 'stsb' with the same workaround as given above.
Some help and a workaround for(cola) this is really appreciated. Thank you.</p>
","language-model"
"62526917","Is there a particular range for good perplexity value in NLP?","2020-06-23 03:36:00","","5","614","<deep-learning><neural-network><nlp><language-model><perplexity>","<p>I'm fine-tuning a language model and am calculating training and validation losses along with the training and validation perplexities. It s calculated by taking the exponential of the loss, in my program. I'm aware that lower perplexities represent better language models and is wondering what the range of values are for a good model. Any help is appreciated. Thank you.</p>
","language-model"
"62458671","BERT:Question-Answering - Total number of permissible words/tokens for training","2020-06-18 20:16:21","62476507","0","176","<pytorch><recurrent-neural-network><language-model><bert-language-model>","<p>Let's say I want to train BERT with 2 sentences (query-answer) pair against a certain binary label (1,0) for the correctness of the answer,  will BERT let me use 512 words/tokens each for the query and the answer or together(query+answer combined) they should be 512? [510 upon ignoring the [start] and [sep] token]</p>

<p>Thanks in advance!</p>
","language-model"
"62328660","Spacy Dutch noun_phrases returns empty list using nl_core_news_sm","2020-06-11 16:09:53","","2","285","<spacy><chunks><language-model>","<p>I want to extract the noun_phrases of a Dutch text using the model nl_core_news_sm by spacy.
It returns an empty list
On the other hand the equivalent English model en_core_web_sm provides indeed the list of noun_chunks (noun_phrases)</p>

<p>Is this normal behaiviour? i.e. the Dutch language moedel does not include the noun_phrases separator and the english model does? Or am I doing something wrong?</p>

<pre><code>string='''In een wereld waarin je wordt overspoeld met informatie, is het prettig om een nieuwsbron te hebben met heldere stukken, die de ruimte laten om je eigen mening te vormen.'''
nlp = spacy.load('nl_core_news_sm')
print(dir(doc))
print(doc.noun_chunks)
list_chunks=[chunk for chunk in doc.noun_chunks]
for chunk in doc.noun_chunks:
    print(chunk.text)
</code></pre>

<p>The result here is that list_chunks is []
And of course nothing is printed in the loop</p>

<p>I used dir(doc) to compare the methods available in order to compare to the English model. They are the same.</p>

<pre><code>nlp_en = spacy.load('en_core_web_sm')
string='''They normally organises a wide range of activities for kids in the summer holidays. Due to the virus, these have all been cancelled'''
doc2=nlp_en(string)
print(dir(doc2))
print(doc2.noun_chunks)
for chunk in doc2.noun_chunks:
    print(chunk.text)
</code></pre>

<p>In English it works.</p>

<p>Some idea?</p>

<p>EDIT NOTE:
Here I compared three language models:
<a href=""https://i.sstatic.net/zaeLz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zaeLz.png"" alt=""enter image description here""></a></p>
","language-model"
"62288026","Why word-level language model should help in beam search decoding in ASR?","2020-06-09 17:02:28","","2","638","<deep-learning><speech-recognition><language-model><ctc><beam-search>","<p>I was experimenting with <strong>beam search decoding</strong> of an acoustic model trained with <strong>CTC</strong> loss trained on an automatic speech recognition task. The version I was using was based on <a href=""https://arxiv.org/pdf/1408.2873.pdf"" rel=""nofollow noreferrer"">this</a> paper.
However, even though many sources describe integration of similar word level language model as beneficial to word error rate performance, in my case, the integration of LM worsened the results. </p>

<p>It actually does not surprise me too much, because the language model scores only prefixes with finished words at the end, and scoring means multiplying the probability of the prefix by the LM probability, which decreases the probability of the whole prefix. This way, probability of prefixes that end with a word from a vocabulary is systematically decreased by the language model, while the prefixes that do not end with a complete word yet are not scored by the LM at all. At each time step, the prefixes ending with complete words seem to be discarted due to the lowered score, while the incomplete prefixes survive in the beam.</p>

<p>My question is, why should word level LM integration work, if it decreases the probability of valid prefixes? I would understand that some character-level LM that scores everything at every step or some look-ahead word-level LM could help. For example <a href=""http://proceedings.mlr.press/v32/graves14.pdf"" rel=""nofollow noreferrer"">Graves</a> describes the integration of a word-level language model by using sums of probabilities of all posible word with given prefix and by applying the LM update at each time step, which seems reasonable even though the computational cost could be much larger.</p>
","language-model"
"62270525","In PyTorch, what's the difference between training an RNN to predict the last word given a sequence, vs predicting the entire sequence shifted?","2020-06-08 20:17:34","62271354","0","581","<python><machine-learning><pytorch><recurrent-neural-network><language-model>","<p>Let's say I'm trying to train an RNN language model in PyTorch. Suppose I iterate over batches of word sequences, and that each training batch tensor has the following shape:</p>

<pre><code>data.shape = [batch_size, sequence_length, vocab_dim]
</code></pre>

<p>My question is, what's the difference between using only the last word in each sequence as the target label:</p>

<pre><code>X = data[:,:-1]
y = data[:,-1]
</code></pre>

<p>and training to minimize loss using a softmax prediction of the last word,</p>

<p>vs setting the target to be the entire sequence shifted right:</p>

<pre><code>X = data[:,:-1]
y = data[:,1:]
</code></pre>

<p>and training to minimize the <em>sum of losses</em> of each predicted word in the shifted sequence?</p>

<p>What's the correct approach here? I feel like i've seen both examples online. Does this also have to do with loop unrolling vs BPTT?</p>
","language-model"
"62069350","Transformer-XL: Input and labels for Language Modeling","2020-05-28 16:08:25","62141748","0","597","<huggingface-transformers><language-model>","<p>I'm trying to finetune the pretrained Transformer-XL model <code>transfo-xl-wt103</code> for a language modeling task. Therfore, I use the model class <code>TransfoXLLMHeadModel</code>.</p>

<p>To iterate over my dataset I use the <code>LMOrderedIterator</code> from the file <a href=""https://github.com/huggingface/transformers/blob/5e737018e1fcb22c8b76052058279552a8d6c806/src/transformers/tokenization_transfo_xl.py#L467"" rel=""nofollow noreferrer"">tokenization_transfo_xl.py</a> which yields a tensor with the <code>data</code> and its <code>target</code> for each batch (and the sequence length).</p>

<p>Let's assume the following data with <code>batch_size = 1</code> and <code>bptt = 8</code>:</p>

<pre><code>data = tensor([[1,2,3,4,5,6,7,8]])
target = tensor([[2,3,4,5,6,7,8,9]])
mems # from the previous output
</code></pre>

<p><strong>My question is:</strong> I currently pass this data into the model like this:</p>

<pre><code>output = model(input_ids=data, labels=target, mems=mems)
</code></pre>

<p>Is this correct?</p>

<p>I am wondering because the documentation says for the <code>labels</code> parameter:</p>

<blockquote>
  <p>labels (:obj:<code>torch.LongTensor</code> of shape :obj:<code>(batch_size, sequence_length)</code>, <code>optional</code>, defaults to :obj:<code>None</code>):
              Labels for language modeling.
              Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set <code>lm_labels = input_ids</code></p>
</blockquote>

<p>So what is it about the parameter <code>lm_labels</code>? I only see <code>labels</code> defined in the <code>forward</code> method.</p>

<p>And when the labels ""are shifted"" inside the model, does this mean I have to pass <code>data</code> twice (additionally instead of <code>targets</code>) because its shifted inside? But how does the model then know the next token to predict?</p>

<p>I also read through <a href=""https://github.com/huggingface/transformers/issues/3711"" rel=""nofollow noreferrer"">this bug</a> and the fix in <a href=""https://github.com/huggingface/transformers/pull/3716"" rel=""nofollow noreferrer"">this pull request</a> but I don't quite understand how to treat the model now (before vs. after fix)</p>

<p>Thanks in advance for some help!</p>

<p><strong>Edit</strong>: <a href=""https://github.com/huggingface/transformers/issues/4698"" rel=""nofollow noreferrer"">Link</a> to issue on Github</p>
","language-model"
"61897882","What are some techniques to improve contextual accuracy of semantic search engine using BERT?","2020-05-19 18:10:29","","3","159","<semantic-web><word-embedding><language-model><bert-language-model>","<p>I am implementing a semantic search engine using BERT (using cosine distance) To a certain extend the method is able to find out sentences in a high level context. However when it comes narrowed down context of the sentence, it gives several issues.</p>

<p>Example:
If my search term is ""Wrong Product"", the search engine might match with sentences like ""I bought a washing machine but it was defective"".</p>

<p>I understand that the fixes to these is always finding some equilibrium and live with minor errors.</p>

<p>However if there are any rules, techniques which has improved your semantic search implementation accuracy please do share. </p>
","language-model"
"61829973","Condition FOR/WHILE in M function - Paginate API in Power BI","2020-05-15 23:34:19","","0","454","<powerbi><powerquery><m><language-model>","<p>I created this API paginated below. It roughly works, but in the OFFSET property, I need to stipulate instead of the number of the next sequence number of the record, for example, for the second page, the number 251, the next record of the second page, and so on.</p>
<ul>
<li><p>My record limit per page is 250</p>
</li>
<li><p>The field <code>totalItems</code> returned the total of records, for example: 4500</p>
</li>
<li><p>I divide the total number of records by the total number of records per page, to get to know how many pages my API has: <code>pageRange = {0..Number.RoundUp(totalItems / 250)}</code></p>
</li>
<li><p>When going to the second page, what happens in the API below, is that the records of the second page are coming repeated, because I should instead use the number 1 (referring to the second page), pass the number 251, and then, when doing the loop again, pass the number 501, until finishing the whole sequence (this parameter in the API is: <code>offset=</code>).</p>
</li>
</ul>
<p>I need alter this line to include the FOR/WHILE for the item &quot;ufnCallAPI(_)&quot; of <em><code>pages = List.Transform(pageRange, each ufnCallAPI(_)),</code></em></p>
<p>For example, the item above:</p>
<pre><code>List.Transform(pageRange, each ufnCallAPI(_)),
List.Transform(pageRange, each ufnCallAPI(250)),
List.Transform(pageRange, each ufnCallAPI(500)),
List.Transform(pageRange, each ufnCallAPI(750)),
</code></pre>
<p>up to the total number <em><code>totalItems</code></em></p>
<p>and include a FOR/WHILE to modified my API to not pass the number of the next page, but the number of the beginning of the list of the next item start (offset).</p>
<p>Thanks very much!</p>
<p>My code:</p>
<pre><code>     let
        ufnCallAPI = (offSet) =&gt;
            let
                query = Web.Contents(&quot;https://api.vhsys.com/v2/pedidos?offset=&quot; &amp; Number.ToText(offSet)  &amp;  &quot;&amp;limit=250&quot;, 
                [Headers=[#&quot;access-token&quot;=&quot;OCKNYbAMaDgLBZBSQPCOGPWOXGSbdO&quot;, #&quot;secret-access-token&quot;=&quot;XXXXXXXXXXXXXX&quot;]]),
                result = Json.Document(query)
            in
                result,
    
            tmpResult = ufnCallAPI(1),
    
            auxTotal1 = Record.ToTable(tmpResult),
            Value = auxTotal1{2}[Value],
            auxTotal2 = Value[total],
            totalItems = auxTotal2 -1,
            pageRange = {0..Number.RoundUp(totalItems / 250)},
    
            pages =List.Transform(pageRange, each ufnCallAPI(_)),
            pages2 = Table.FromList(pages, Splitter.SplitByNothing(), null, null, ExtraValues.Error),
            pages3 = Table.ExpandRecordColumn(pages2, &quot;Column1&quot;, {&quot;code&quot;, &quot;status&quot;, &quot;paging&quot;, &quot;data&quot;}, {&quot;Column1.code&quot;, &quot;Column1.status&quot;, &quot;Column1.paging&quot;, &quot;Column1.data&quot;}),
            pages4 = Table.ExpandListColumn(pages3, &quot;Column1.data&quot;),
            pages5 = Table.RemoveColumns(pages4,{&quot;Column1.code&quot;, &quot;Column1.status&quot;, &quot;Column1.paging&quot;}),
            data = Table.ExpandRecordColumn(pages5, &quot;Column1.data&quot;, {&quot;id_ped&quot;, &quot;id_pedido&quot;, &quot;id_cliente&quot;, &quot;nome_cliente&quot;, &quot;id_local_retirada&quot;, &quot;id_local_cobranca&quot;, &quot;vendedor_pedido&quot;, &quot;vendedor_pedido_id&quot;, &quot;listapreco_produtos&quot;, &quot;valor_total_produtos&quot;, &quot;desconto_pedido&quot;, &quot;desconto_pedido_porc&quot;, &quot;peso_total_nota&quot;, &quot;peso_total_nota_liq&quot;, &quot;frete_pedido&quot;, &quot;valor_total_nota&quot;, &quot;valor_baseICMS&quot;, &quot;valor_ICMS&quot;, &quot;valor_baseST&quot;, &quot;valor_ST&quot;, &quot;valor_IPI&quot;, &quot;condicao_pagamento_id&quot;, &quot;condicao_pagamento&quot;, &quot;frete_por_pedido&quot;, &quot;transportadora_pedido&quot;, &quot;id_transportadora&quot;, &quot;data_pedido&quot;, &quot;prazo_entrega&quot;, &quot;referencia_pedido&quot;, &quot;obs_pedido&quot;, &quot;obs_interno_pedido&quot;, &quot;status_pedido&quot;, &quot;contas_pedido&quot;, &quot;comissao_pedido&quot;, &quot;estoque_pedido&quot;, &quot;ordemc_emitido&quot;, &quot;data_cad_pedido&quot;, &quot;data_mod_pedido&quot;, &quot;id_aplicativo&quot;, &quot;id_pedido_aplicativo&quot;, &quot;lixeira&quot;}, {&quot;id_ped&quot;, &quot;id_pedido&quot;, &quot;id_cliente&quot;, &quot;nome_cliente&quot;, &quot;id_local_retirada&quot;, &quot;id_local_cobranca&quot;, &quot;vendedor_pedido&quot;, &quot;vendedor_pedido_id&quot;, &quot;listapreco_produtos&quot;, &quot;valor_total_produtos&quot;, &quot;desconto_pedido&quot;, &quot;desconto_pedido_porc&quot;, &quot;peso_total_nota&quot;, &quot;peso_total_nota_liq&quot;, &quot;frete_pedido&quot;, &quot;valor_total_nota&quot;, &quot;valor_baseICMS&quot;, &quot;valor_ICMS&quot;, &quot;valor_baseST&quot;, &quot;valor_ST&quot;, &quot;valor_IPI&quot;, &quot;condicao_pagamento_id&quot;, &quot;condicao_pagamento&quot;, &quot;frete_por_pedido&quot;, &quot;transportadora_pedido&quot;, &quot;id_transportadora&quot;, &quot;data_pedido&quot;, &quot;prazo_entrega&quot;, &quot;referencia_pedido&quot;, &quot;obs_pedido&quot;, &quot;obs_interno_pedido&quot;, &quot;status_pedido&quot;, &quot;contas_pedido&quot;, &quot;comissao_pedido&quot;, &quot;estoque_pedido&quot;, &quot;ordemc_emitido&quot;, &quot;data_cad_pedido&quot;, &quot;data_mod_pedido&quot;, &quot;id_aplicativo&quot;, &quot;id_pedido_aplicativo&quot;, &quot;lixeira&quot;}),
        #&quot;Tipo Alterado&quot; = Table.TransformColumnTypes(data,{{&quot;id_ped&quot;, type text}, {&quot;id_pedido&quot;, Int64.Type}, {&quot;nome_cliente&quot;, type text}, {&quot;valor_total_produtos&quot;, type text}}),
        #&quot;Valor Substituído&quot; = Table.ReplaceValue(#&quot;Tipo Alterado&quot;,&quot;.&quot;,&quot;,&quot;,Replacer.ReplaceText,{&quot;valor_total_produtos&quot;}),
        #&quot;Tipo Alterado1&quot; = Table.TransformColumnTypes(#&quot;Valor Substituído&quot;,{{&quot;valor_total_produtos&quot;, Currency.Type}}),
        #&quot;Valor Substituído1&quot; = Table.ReplaceValue(#&quot;Tipo Alterado1&quot;,&quot;.&quot;,&quot;,&quot;,Replacer.ReplaceValue,{&quot;desconto_pedido&quot;, &quot;desconto_pedido_porc&quot;, &quot;peso_total_nota&quot;, &quot;peso_total_nota_liq&quot;, &quot;frete_pedido&quot;, &quot;valor_total_nota&quot;, &quot;valor_baseICMS&quot;, &quot;valor_ICMS&quot;, &quot;valor_baseST&quot;, &quot;valor_ST&quot;, &quot;valor_IPI&quot;}),
        #&quot;Tipo Alterado2&quot; = Table.TransformColumnTypes(#&quot;Valor Substituído1&quot;,{{&quot;desconto_pedido&quot;, Currency.Type}, {&quot;desconto_pedido_porc&quot;, Currency.Type}, {&quot;peso_total_nota&quot;, Currency.Type}, {&quot;peso_total_nota_liq&quot;, Currency.Type}, {&quot;frete_pedido&quot;, Currency.Type}, {&quot;valor_total_nota&quot;, type text}, {&quot;valor_baseICMS&quot;, Currency.Type}, {&quot;valor_ICMS&quot;, Currency.Type}, {&quot;valor_baseST&quot;, Currency.Type}, {&quot;valor_ST&quot;, Currency.Type}, {&quot;valor_IPI&quot;, Currency.Type}, {&quot;prazo_entrega&quot;, type text}, {&quot;data_pedido&quot;, type date}}),
        #&quot;Colunas Removidas&quot; = Table.RemoveColumns(#&quot;Tipo Alterado2&quot;,{&quot;id_aplicativo&quot;, &quot;id_pedido_aplicativo&quot;, &quot;lixeira&quot;}),
        #&quot;Tipo Alterado3&quot; = Table.TransformColumnTypes(#&quot;Colunas Removidas&quot;,{{&quot;valor_total_nota&quot;, type text}}),
        #&quot;Valor Substituído2&quot; = Table.ReplaceValue(#&quot;Tipo Alterado3&quot;,&quot;.&quot;,&quot;,&quot;,Replacer.ReplaceText,{&quot;valor_total_nota&quot;}),
    #&quot;Tipo Alterado4&quot; = Table.TransformColumnTypes(#&quot;Valor Substituído2&quot;,{{&quot;valor_total_nota&quot;, Currency.Type}})
    
    in
    #&quot;Tipo Alterado4&quot;
</code></pre>
","language-model"
"61700506","What does 'theta' mean in a language model?","2020-05-09 16:58:19","61709975","0","1681","<nlp><stanford-nlp><information-retrieval><n-gram><language-model>","<p>I know that if X denotes a text , p(X) denotes the language model of the text. And most often , we use maximum likelihood estimation to estimate the language model. 
But in many cases , I find a parameter $\theta$ used to represent a language model. I don't understand the meaning of this $\theta$ . 
For Example , for a document d in a collection what purpose does $\theta$ serve in ' p(d|$\theta$) ' ? </p>

<p>Does $\theta$ represent a maximum likelihood estimator or a language model ? </p>

<p>Can someone please explain this difference between a language model and $\theta$ in depth ? </p>

<p>Thanks in advance ! </p>
","language-model"
"61598029","Size of input and output layers in Keras implementation of an RNN Language Model","2020-05-04 17:23:55","61598434","0","274","<tensorflow><keras><neural-network><word-embedding><language-model>","<p>As part of my thesis, I am trying to build a recurrent Neural Network Language Model. </p>

<p>From theory, I know that the input layer should be a one-hot vector layer with a number of neurons equal to the number of words of our Vocabulary, followed by an Embedding layer, which, in Keras, it apparently translates to a single Embedding layer in a Sequential model. I also know that the output layer should also be the size of our vocabulary so that each output value maps 1-1 to each vocabulary word.</p>

<p>However, in both the Keras documentation for the Embedding layer (<a href=""https://keras.io/layers/embeddings/"" rel=""nofollow noreferrer"">https://keras.io/layers/embeddings/</a>) and in this article (<a href=""https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/#comment-533252"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/#comment-533252</a>), the vocabulary size is arbitrarily augmented by one for both the input and the output layers! Jason gives an explenation that this is due to the implementation of the Embedding layer in Keras but that doesn't explain why we would also use +1 neuron in the output layer. I am at the point of wanting to order the possible next words based on their probabilities and I have one probability too many that I do not know to which word to map it too.</p>

<p>Does anyone know what is the correct way of acheiving the desired result? Did Jason just forget to subtrack one from the output layer and the Embedding layer just needs a +1 for implementation reasons (I mean it's stated in the official API)?</p>

<p>Any help on the subject would be appreciated (why is Keras API documentation so laconic?).</p>

<p><strong>Edit:</strong></p>

<p>This post <a href=""https://stackoverflow.com/questions/43227938/keras-embedding-layer-masking-why-does-input-dim-need-to-be-vocabulary-2?rq=1"">Keras embedding layer masking. Why does input_dim need to be |vocabulary| + 2?</a> made me think that Jason does in fact have it wrong and that the size of the Vocabulary should not be incremented by one when our word indices are: <code>0, 1, ..., n-1</code>.</p>

<p>However, when using Keras's Tokenizer our word indices are: <code>1, 2, ..., n</code>. In this case, the correct approach is to:</p>

<ol>
<li><p>Set <code>mask_zero=True</code>, to treat 0 differently, as there is never a
0 (integer) index input in the Embedding layer and keep the
vocabulary size the same as the number of vocabulary words (<code>n</code>)?</p></li>
<li><p>Set <code>mask_zero=True</code> but augment the vocabulary size by one?</p></li>
<li><p>Not set <code>mask_zero=True</code> and keep the vocabulary size the same as the
number of vocabulary words?</p></li>
</ol>
","language-model"
"61482810","Fine tuning a pretrained language model with Simple Transformers","2020-04-28 14:35:17","61487842","1","2094","<python-3.x><huggingface-transformers><language-model><simpletransformers>","<p>In his article 'Language Model Fine-Tuning For Pre-Trained Transformers' Thilina Rajapakse (<a href=""https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee"" rel=""nofollow noreferrer"">https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee</a>)
provides the following code snippet for fine-tuning a pre-trained model using the library <code>simpletransformers</code>:</p>

<pre><code>from simpletransformers.language_modeling import LanguageModelingModel
import logging


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger(""transformers"")
transformers_logger.setLevel(logging.WARNING)

train_args = {
    ""reprocess_input_data"": True,
    ""overwrite_output_dir"": True,
}

model = LanguageModelingModel('bert', 'bert-base-cased', args=train_args)

model.train_model(""data/train.txt"", eval_file=""data/text.txt"")

model.eval_model(""data/test.txt"")
</code></pre>

<p>He then adds:</p>

<blockquote>
  <p>We assume that you have combined all the text in your dataset into two
  text files train.txt and test.txt which can be found in the data/
  directory.</p>
</blockquote>

<p>I have 2 questions:</p>

<p><strong>Question 1</strong></p>

<p>Does the highlighted sentence above implies that the entire corpus will be merged into one text file?  So assuming that the Training Corpus is comprised of 1,000,000 text files, are we supposed to merge them all in one text file with code like this?</p>

<pre><code>import fileinput
with open(outfilename, 'w') as fout, fileinput.input(filenames) as fin:
    for line in fin:
        fout.write(line)
</code></pre>

<p><strong>Question 2</strong></p>

<p>I presume that I can use the pretrained model: <code>bert-base-multilingual-cased</code>.  Correct?</p>
","language-model"
"61470768","How does masked_lm_labels argument work in BertForMaskedLM?","2020-04-28 00:34:51","","1","1929","<nlp><language-model><bert-language-model><mlmodel>","<pre><code>from transformers import BertTokenizer, BertForMaskedLM
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')

input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"", add_special_tokens=True)).unsqueeze(0) # Batch size 1
outputs = model(input_ids, masked_lm_labels=input_ids)

loss, prediction_scores = outputs[:2] 
</code></pre>

<p>This code is from huggingface transformers page. <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm</a></p>

<p>I cannot understand the <code>masked_lm_labels=input_ids</code> argument in <code>model</code>. 
How does it work? Does it means that it will automatically mask some of the text when <code>input_ids</code> is passed?</p>
","language-model"
"61440281","Is positional encoding necessary for transformer in language modeling?","2020-04-26 11:54:02","63948329","6","6236","<transformer-model><language-model>","<p>I am developing a language model like <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""noreferrer"">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a>.</p>

<p>It is not clear for me - whether positional encoding is neccessary here ?
As far as I understand - it is necessary for language translation task because the decoder should be able to position  the word from the previous output within the sequence from encoder.
But is it necessary in language modeling without the decoder ?</p>

<p>Is it possible that the words in the encoder output are shuffled ?</p>

<p><strong>Edit:</strong> </p>

<p>there are no explanations in the original paper. And I didn't find explanations in tutorials (like here <a href=""https://kazemnejad.com/blog/transformer_architecture_positional_encoding/"" rel=""noreferrer"">https://kazemnejad.com/blog/transformer_architecture_positional_encoding/</a>).</p>

<p>I don't understand this: </p>

<p>""As each word in a sentence simultaneously flows through the Transformer’s encoder/decoder stack, The model itself doesn’t have any sense of position/order for each word.""</p>

<p>From my point of view - transformer encoder has info about the order because its input is an ordered sequence (similar to RNN).</p>

<p>I tried to remove positional encoding from the model. It works, but with a worse performance.</p>

<p>Is it useful to add such positional encoding to RNN ? Could it improve its performance ?</p>
","language-model"
"61416197","Pretraining a language model on a small custom corpus","2020-04-24 19:38:46","","7","4188","<deep-learning><transfer-learning><huggingface-transformers><language-model><bert-language-model>","<p>I was curious if it is possible to use transfer learning in text generation, and re-train/pre-train it on a specific kind of text.   </p>

<p>For example, having a pre-trained BERT model and a small corpus of medical (or any ""type"") text, make a language model that is able to generate medical text. The assumption is that you do not have a huge amount of ""medical texts"" and that is why you have to use transfer learning.</p>

<p>Putting it as a pipeline, I would describe this as:  </p>

<ol>
<li>Using a pre-trained BERT tokenizer.  </li>
<li>Obtaining new tokens from my new text and adding them to the existing pre-trained language model (i.e., vanilla BERT).  </li>
<li>Re-training the pre-trained BERT model on the custom corpus with the combined tokenizer.  </li>
<li>Generating text that resembles the text within the small custom corpus.</li>
</ol>

<p>Does this sound familiar? Is it possible with hugging-face?</p>
","language-model"
"60914449","Define and Use new smoothing method in nltk language models","2020-03-29 12:43:43","60934055","1","486","<nlp><nltk><smoothing><language-model>","<p>I'm trying to provide and test new smoothing method for language models. I'm using nltk tools and don't want to redefine everything from scratch. So is there any way to define and use my own smoothing method in nltk models?</p>

<p>Edit:
I'm trying to do something like this :</p>

<pre class=""lang-py prettyprint-override""><code>def my_smoothing_method(model) :
    # some code using model (MLE) count

model = nltk.lm.MLE(n, smoothing_method=my_smoothing_method)
model.fit(train)
</code></pre>
","language-model"
"60579343","padding and attention mask does not work as intended in batch input in GPT language model","2020-03-07 15:48:08","","2","5068","<python><pytorch><language-model><huggingface-transformers>","<p>The following code is without batch:</p>

<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.eval()
context=torch.tensor([tokenizer.encode(""This is"")])
output, past = model(context)
token = torch.argmax(output[..., -1, :])
print(tokenizer.decode(token.item()))

output: ' a'
</code></pre>

<p>This is working fine. Now, I extended this to batch setting:</p>

<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.eval()

context=[torch.tensor(tokenizer.encode(""This is "")),torch.tensor(tokenizer.encode(""Hello How are ""))]
context=pad_sequence(context,batch_first=True)

mask=torch.tensor([[1,1,0],[1,1,1]])
output, past = model(context,attention_mask=mask)
token = torch.argmax(output[..., -1, :],dim=1)
tokenizer.decode(token)

output: '\n you'
</code></pre>

<p>Here <code>\n</code> is next token for the first context and <code>you</code> is next token for second context of the batch.
But The expected next token for the first context is <code>a</code>, since all the settings are same. Furthermore, if you reduce the second context to 2 token you will get <code>a</code> in this batch setting. So clearly, model can not understand the padding.
Also, the attention mask does not work. Because,
after padding the next token of sequence <code>this is</code> is 0 (zero). And according to the attention mask (<code>[1,1,0]</code>), this zero should be avoided and only the tokens <code>this</code> and <code>is</code> should be attended. The proofs that this attention masking is not working are:</p>

<ul>
<li><p>Use attention mask [1,1,1], that means attend even on the padding zero, you get the same output
which is <code>\n</code>.</p></li>
<li><p>Use the the string <code>this is!</code>. Here <code>!</code> has the zero index in the vocabulary matrix. Again you get the same output which is <code>\n</code>.</p></li>
</ul>

<p>Only time, it is possible to get desirable output is without the batch settings and attention mask ( now it seems, it does not matter because it has no effect anyway)</p>

<p>Then I found <a href=""https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.pad_token"" rel=""nofollow noreferrer"">this</a>, which suggested to use pad_token. So I used like following:</p>

<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
from torch.nn.utils.rnn import pad_sequence  

tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"",pad_token=""&lt;PAD&gt;"")
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.eval()

context=[torch.tensor(tokenizer.encode(""This is &lt;PAD&gt; "")),torch.tensor(tokenizer.encode(""Hello How are""))]
context=torch.stack(context)
print(context)
mask=torch.tensor([[1,1,0],[1,1,1]])

output, past = model(context,attention_mask=mask)
token = torch.argmax(output[..., -1, :],dim=1)
tokenizer.decode(token)

output: 'The you'
</code></pre>

<p>Here <code>The</code> is next token for the first context and <code>you</code> is next token for second context of the batch. This is also not working. Because <code>The</code> is not expected for the first context.</p>

<p>How do I use variable length sequence in batch setting in gpt/gpt2 model?</p>
","language-model"
"60327007","Trainable USE-lite-based classifier with SentencePiece input","2020-02-20 19:11:09","","1","186","<tensorflow><language-model>","<p>I have heard that it is possible to use the pretrained Universal Sentence Encoder (USE) (neural language model) from TF-hub as part of a trainable model, e.g. a sentence classifier.  Some versions of USE rely on SentencePiece sub-word tokenizer, which I also need.  There are minimal instructions online for how to do this.  </p>

<p>Here is how to use USE-lite with SentencePiece:<br>
-  <a href=""https://tfhub.dev/google/universal-sentence-encoder-lite/2"" rel=""nofollow noreferrer"">https://tfhub.dev/google/universal-sentence-encoder-lite/2</a></p>

<p>Here is how to train a classifier based on a pretrained USE model:<br>
-  <a href=""http://hunterheidenreich.com/blog/google-universal-sentence-encoder-in-keras/"" rel=""nofollow noreferrer"">http://hunterheidenreich.com/blog/google-universal-sentence-encoder-in-keras/</a>
-  <a href=""https://www.youtube.com/watch?v=gnz1CUzb5qo"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=gnz1CUzb5qo</a></p>

<p>And here is how to measure sentence similarity using both USE-lite and SentencePiece:<br>
-  <a href=""https://github.com/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb"" rel=""nofollow noreferrer"">https://github.com/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder_lite.ipynb</a></p>

<p>I have successfully reproduced the above pieces separately.  I have then tried to combine the above ideas into a single POC that will build a classifier model based on USE-lite and SentencePiece, but I cannot see how to do it.  I am currently stuck on the part where I modify the trainable classifier's first layer(s).  I have tried to make it accept either (1) SentencePiece token IDs (in which I tokenize the text outide of the Tensorflow graph) or (2) raw text (using SentencePiece as an Op inside the Tensorflow graph).  After that point, it should feed tokenized text forward into the USE-lite model, either in a lambda or in some other way.  Finally, the output of USE-lite should be fed into a dense layer (or two?) ending in softmax for computing class probabilities.  </p>

<p>I am relatively new to Tensorflow.  I imagine that the above sources would be sufficient for a more experienced Tensorflow developer to merge and make work for my use-case.  Let me know if you can provide any pointers.  Thanks.  </p>
","language-model"
"60187634","GPT-2 language model: multiplying decoder-transformer output with token embedding or another weight matrix","2020-02-12 11:46:54","","1","287","<python><nlp><pytorch><language-model><huggingface-transformers>","<p>I was reading the code of GPT2 language model. The transformation of hidden states to the probability distribution over the vocabulary has done in the following line:</p>

<pre><code>lm_logits = self.lm_head(hidden_states)
</code></pre>

<p>Here,</p>

<pre><code>self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
</code></pre>

<p>However, 
In the <a href=""https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf"" rel=""nofollow noreferrer"">original paper</a>, they suggested multiplying hidden states with the token embedding matrix whereas <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface</a>  implementation used another matrix.</p>

<p>Is there any advantage of this? Am I missing something?  </p>
","language-model"
"60173639","Size of the training data of GPT2-XL pre-trained model","2020-02-11 16:41:17","60175519","2","939","<pytorch><language-model><huggingface-transformers>","<p>In <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">huggingface transformer</a>, it is possible to use the pre-trained GPT2-XL language model. But I don't find, on which dataset it is trained? Is it the same trained model which OpenAI used for their <a href=""https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"" rel=""nofollow noreferrer"">paper</a> (trained on 40GB dataset called <code>webtext</code>) ?</p>
","language-model"
"60137162","Pre-training BERT/RoBERTa language model using domain text, how long it gonna take estimately? which is faster?","2020-02-09 13:33:21","60146856","2","1178","<language-model><bert-language-model><huggingface-transformers>","<p>I want to pre-train BERT and RoBERTa MLM using domain corpus (sentiment-related text). How long it gonna take for using 50k~100k words. Since RoBERTa is not trained on predicting the next sentence objective, one training objective less than BERT and with larger mini-batches and learning rates, I assume RoBERTa will be much faster?</p>
","language-model"
"60121768","while running huggingface gpt2-xl model embedding index getting out of range","2020-02-07 22:08:48","60173172","1","1570","<python-3.x><language-model><huggingface-transformers>","<p>I am trying to run <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#tfgpt2lmheadmodel"" rel=""nofollow noreferrer"">hugginface</a> gpt2-xl model. I ran code from the <a href=""https://huggingface.co/transformers/quickstart.html"" rel=""nofollow noreferrer"">quickstart</a> page that load the small gpt2 model and generate text by the following code: </p>

<pre><code>from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
model = GPT2LMHeadModel.from_pretrained('gpt2')

generated = tokenizer.encode(""The Manhattan bridge"")
context = torch.tensor([generated])
past = None

for i in range(100):
    print(i)
    output, past = model(context, past=past)
    token = torch.argmax(output[0, :])

    generated += [token.tolist()]
    context = token.unsqueeze(0)

sequence = tokenizer.decode(generated)

print(sequence)
</code></pre>

<p>This is running perfectly. Then I try to run <code>gpt2-xl</code> model. 
I changed <code>tokenizer</code> and <code>model</code> loading code like following:
  tokenizer = GPT2Tokenizer.from_pretrained(""gpt2-xl"")
  model = GPT2LMHeadModel.from_pretrained('gpt2-xl')</p>

<p>The <code>tokenizer</code> and <code>model</code> loaded perfectly. But I a getting error on the following line:</p>

<pre><code>output, past = model(context, past=past)
</code></pre>

<p>The error is:</p>

<pre><code>RuntimeError: index out of range: Tried to access index 204483 out of table with 50256 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:418
</code></pre>

<p>Looking at error it seems that the embedding size is not correct. So I write the following line to specifically fetch the config file of <code>gpt2-xl</code>:</p>

<pre><code>config = GPT2Config.from_pretrained(""gpt2-xl"")
</code></pre>

<p>But, here <code>vocab_size:50257</code>
So I changed explicitly the value by:</p>

<pre><code>config.vocab_size=204483
</code></pre>

<p>Then after printing the <code>config</code>, I can see that the previous line took effect in the configuration. But still, I am getting the same error. </p>
","language-model"
"60088400","spaCy can't load model ONLY when calling ""rasa train""","2020-02-06 05:45:33","","2","1927","<spacy><language-model><rasa>","<p>I'm training a rasa model via command line but spaCy seems to be unable to load my language model <code>pt_core_news_sm</code> only when I try to train via terminal.</p>

<p>Everything is done inside my venv and executed as admin;</p>

<p>A may load the model when calling spaCy from python:</p>

<pre><code>import spacy
spacy.load(""pt_core_news_sm"")
</code></pre>

<p>Training via <code>rasa_nlu</code> in a python script works as well.</p>

<p>But when I call <code>rasa train</code> I get this traceback:</p>

<pre><code>2020-02-06 02:03:19 INFO     rasa.nlu.utils.spacy_utils  - Trying to load spacy model with name 'pt_core_news_sm'
Traceback (most recent call last):
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\nlu\utils\spacy_utils.py"", line 51, in load_model
    return spacy.load(spacy_model_name, disable=[""parser""])
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\spacy\__init__.py"", line 30, in load
    return util.load_model(name, **overrides)
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\spacy\util.py"", line 169, in load_model
    raise IOError(Errors.E050.format(name=name))
OSError: [E050] Can't find model 'pt_core_news_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\aliss\AppData\Local\Programs\Python\Python36\Scripts\rasa.exe\__main__.py"", line 9, in &lt;module&gt;
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\__main__.py"", line 76, in main
    cmdline_arguments.func(cmdline_arguments)
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\cli\train.py"", line 76, in train
    additional_arguments=extract_additional_arguments(args),
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\train.py"", line 50, in train
    additional_arguments=additional_arguments,
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\asyncio\base_events.py"", line 473, in run_until_complete
    return future.result()
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\train.py"", line 101, in train_async
    additional_arguments,
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\train.py"", line 188, in _train_async_internal
    additional_arguments=additional_arguments,
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\train.py"", line 245, in _do_training
    persist_nlu_training_data=persist_nlu_training_data,
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\train.py"", line 474, in _train_nlu_with_validated_data
    persist_nlu_training_data=persist_nlu_training_data,
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\nlu\train.py"", line 74, in train
    trainer = Trainer(nlu_config, component_builder)
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\nlu\model.py"", line 147, in __init__
    self.pipeline = self._build_pipeline(cfg, component_builder)
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\nlu\model.py"", line 159, in _build_pipeline
    component = component_builder.create_component(component_cfg, cfg)
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\nlu\components.py"", line 515, in create_component
    component = registry.create_component_by_config(component_config, cfg)
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\nlu\registry.py"", line 228, in create_component_by_config
    return component_class.create(component_config, config)
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\nlu\utils\spacy_utils.py"", line 81, in create
    nlp = cls.load_model(spacy_model_name)
  File ""c:\users\aliss\appdata\local\programs\python\python36\lib\site-packages\rasa\nlu\utils\spacy_utils.py"", line 58, in load_model
    ""en_core_web_md en"".format(spacy_model_name)
rasa.nlu.model.InvalidModelError: Model 'pt_core_news_sm' is not a linked spaCy model.  Please download and/or link a spaCy model, e.g. by running:
python -m spacy download en_core_web_md
python -m spacy link en_core_web_md en

</code></pre>

<p>I've tried the following download strategies with python and python3. The first seemed to be the answer for <a href=""https://stackoverflow.com/questions/49964028/spacy-oserror-cant-find-model-en"">this question</a>, but didn't work as well:</p>

<pre><code>pip install https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz

python -m spacy download pt

python -m spacy download pt_core_news_sm
</code></pre>

<p>This is my configuration file:</p>

<pre><code>language: ""pt_core_news_sm""

pipeline:
- name: ""SpacyNLP""                 
- name: ""tokenizer_spacy""          
- name: ""ner_crf""                
- name: ""intent_featurizer_spacy""     
- name: ""intent_classifier_sklearn""  
- name: ""ner_synonyms""               

policies:
  - name: ""FormPolicy""
</code></pre>

<p>I'm using...</p>

<pre><code>spacy==2.2.3

rasa==1.7.0
rasa-core==0.14.5
rasa-core-sdk==0.14.0
rasa-nlu==0.15.0
rasa-sdk==1.7.0
</code></pre>
","language-model"
"59721819","Word2vec: what does it mean that the projection layer is shared?","2020-01-13 18:04:36","","1","162","<neural-network><word2vec><language-model>","<p>The slides of my professor compare the ""Neural Net Language Model"" (Bengio et al., 2003) with Google's word2vec (Mikolov et al., 2013). It says that, differently from the Bengio's model, in word2vec ""the projection layer is shared (not just the weight matrix)""</p>

<p>What does it mean? Shared across what?</p>

<p>The other differences are that there is no hidden layer in Mikolov's model, and that the context contains words from both the past and the future (while only words from the past are accounted for in Bengio's model).</p>

<p>I understood these latter differences, but I have difficulty in understading the ""shared layer"" concept.</p>
","language-model"
"59315138","How to get words from output of XLNet using Transformers library","2019-12-13 01:58:57","60255556","2","1201","<nlp><masking><transformer-model><language-model><huggingface-transformers>","<p>I am using Hugging Face's Transformer library to work with different NLP models. Following code does masking with XLNet. It outputs a tensor with numbers. How do I convert the output to words again?  </p>

<pre><code>import torch
from transformers import XLNetModel,  XLNetTokenizer, XLNetLMHeadModel

tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')
model = XLNetLMHeadModel.from_pretrained('xlnet-base-cased')

# We show how to setup inputs to predict a next token using a bi-directional context.
input_ids = torch.tensor(tokenizer.encode(""I went to &lt;mask&gt; York and saw the &lt;mask&gt; &lt;mask&gt; building."")).unsqueeze(0)  # We will predict the masked token
print(input_ids)

perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)
perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token

target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  # Shape [1, 1, seq_length] =&gt; let's predict one token
target_mapping[0, 0, -1] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)

outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)
next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]
</code></pre>

<p>The current output I get is: </p>

<p>tensor([[[ -5.1466, -17.3758, -17.3392,  ..., -12.2839, -12.6421, -12.4505]]],
       grad_fn=AddBackward0)</p>
","language-model"
"59222579","squad2.0 training error: THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected","2019-12-07 02:51:21","","4","1712","<python><tensorflow><transformer-model><language-model>","<pre><code>!python -m torch.distributed.launch --nproc_per_node=8 /root/examples/run_squad.py \
    --model_type bert \
    --model_name_or_path bert-large-uncased-whole-word-masking \
    --do_train \
    --do_eval \
    --do_lower_case \
    --train_file /root/DATA/train-v2.0.json \
    --predict_file /root/DATA/dev-v2.0.json \
    --learning_rate 3e-5 \
    --num_train_epochs 2 \
    --max_seq_length 384 \
    --doc_stride 128 \
    --output_dir ../root/result/ \
    --per_gpu_eval_batch_size=3   \
    --per_gpu_train_batch_size=3   \
</code></pre>

<p>I'm using google colab and I want to training my A&amp;Q dataset which downloaded from SQuad website.
But when I run the code above it return me an error.</p>

<p>Can some one help me fix this problem?The full error msg as following and I'll appreciate any suggestions:</p>

<p>this is error msg:
    [THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
        main()
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
      File ""/root/examples/run_squad.py"", line 469, in main
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch.cuda.set_device(args.local_rank)
        torch._C._cuda_setDevice(device)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/root/examples/run_squad.py"", line 575, in 
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
      File ""/root/examples/run_squad.py"", line 469, in main
        torch._C._cuda_init()
        main()
        torch.cuda.set_device(args.local_rank)
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
    THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=100 : no CUDA-capable device is detected
    Traceback (most recent call last):
      File ""/root/examples/run_squad.py"", line 575, in 
        main()
      File ""/root/examples/run_squad.py"", line 469, in main
        torch.cuda.set_device(args.local_rank)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 300, in set_device
        torch._C._cuda_setDevice(device)
      File ""/usr/local/lib/python3.6/dist-packages/torch/cuda/<strong>init</strong>.py"", line 193, in _lazy_init
        torch._C._cuda_init()
    RuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50
    Traceback (most recent call last):
      File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
        ""<strong>main</strong>"", mod_spec)
      File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
        exec(code, run_globals)
      File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py"", line 253, in 
        main()
      File ""/usr/local/lib/python3.6/dist-packages/torch/distributed/launch.py"", line 249, in main
        cmd=cmd)
    subprocess.CalledProcessError: Command '['/usr/bin/python3', '-u', '/root/examples/run_squad.py', '--local_rank=7', '--model_type', 'bert', '--model_name_or_path', 'bert-large-uncased-whole-word-masking', '--do_train', '--do_eval', '--do_lower_case', '--train_file', '/root/DATA/train-v2.0.json', '--predict_file', '/root/DATA/dev-v2.0.json', '--learning_rate', '3e-5', '--num_train_epochs', '2', '--max_seq_length', '384', '--doc_stride', '128', '--output_dir', '../root/result/', '--per_gpu_eval_batch_size=3', '--per_gpu_train_batch_size=3']' returned non-zero exit status 1.]</p>

<p><a href=""https://i.sstatic.net/DJJrX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DJJrX.png"" alt=""enter image description here""></a></p>
","language-model"
"59209086","calculate perplexity in pytorch","2019-12-06 07:58:13","59219379","8","18145","<python><nlp><pytorch><language-model>","<p>I've just trained an LSTM language model using pytorch. The main body of the class is this:  </p>

<pre><code>class LM(nn.Module):
    def __init__(self, n_vocab, 
                       seq_size, 
                       embedding_size, 
                       lstm_size, 
                       pretrained_embed):

        super(LM, self).__init__()
        self.seq_size = seq_size
        self.lstm_size = lstm_size
        self.embedding = nn.Embedding.from_pretrained(pretrained_embed, freeze = True)
        self.lstm = nn.LSTM(embedding_size,
                            lstm_size,
                            batch_first=True)
        self.fc = nn.Linear(lstm_size, n_vocab)

    def forward(self, x, prev_state):
        embed = self.embedding(x)
        output, state = self.lstm(embed, prev_state)
        logits = self.fc(output)

        return logits, state
</code></pre>

<p>Now I want to write a function which calculates how good a sentence is, based on the trained language model <em>(some score like perplexity, etc.)</em>. </p>

<p>I'm a bit confused and I don't know how should I calculate this. <br>A similar sample would be of greate use.</p>
","language-model"
"58891892","Next word Prediction RNN","2019-11-16 14:49:23","","1","833","<python><keras><lstm><language-model>","<p>this is my second post. I am really sorry If I sound awkward. I am new to Machine Learning. Reporting the question or giving a negative point will help me. Again I am sorry for unable to clear my question. </p>

<p>Now coming to my question, I am working on an assignment on the next word Prediction. I am trying to create a model that can be used to generate the next words based on the input like the swift keyboard. I have created a model that is able to generate the next word. But I want to generate more than one word for one input word. For example, I ate hot ___. For the blank space, I want Predictions like dog, pizza, chocolate. However, my current model is able to generate the next word which is having the maximum probability. But I want 3 outputs.  I am using LSTM and Keras as a framework. I am using Keras's sequential model. There are three main parts of the code: dataset preparation, model training, and generating the prediction. </p>

<pre><code>def dataset_preparation():

    # basic cleanup
    corpus = text.split(""\n\n"")

    # tokenization  
    tokenizer.fit_on_texts(str(corpus).split(""##""))
    total_words = len(tokenizer.word_index) + 1

    # create input sequences using list of tokens
    input_sequences = []
    for line in str(corpus).split(""\n\n""):
        #print(line)
        token_list = tokenizer.texts_to_sequences([line])[0]
        #print(""printing token_list"",token_list)
        for i in range(1, len(token_list)):
            n_gram_sequence = token_list[:i+1]
            #print(""printing n_gram_sequence"",n_gram_sequence)
            #print(""printing n_gram_sequence length"",len(n_gram_sequence))
            input_sequences.append(n_gram_sequence)
    #print(""Printing Input Sequence:"",input_sequences)
    # pad sequences 
    max_sequence_len = 378
    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

    # create predictors and label
    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]
    label = ku.to_categorical(label, num_classes=total_words)

    return predictors, label, max_sequence_len, total_words

def create_model(predictors, label, max_sequence_len, total_words):

    model = Sequential()
    model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))
    model.add(LSTM(150, return_sequences = True))
    # model.add(Dropout(0.2))
    model.add(LSTM(100))
    model.add(Dense(total_words, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    earlystop = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')
    model.fit(predictors, label, epochs=1, verbose=1, callbacks=[earlystop])
    print (model.summary())
    return model

def generate_text(seed_text, next_words, max_sequence_len):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]
        print(""Printing token list:"",token_list)
        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
        predicted = model.predict_classes(token_list, verbose=0)
        output_word = """"
        for word, index in tokenizer.word_index.items():
            if index == predicted:
                output_word = word
                break
        seed_text += "" "" + output_word
    return seed_text
</code></pre>

<p>I am following the mentioned article from <a href=""https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275"" rel=""nofollow noreferrer"">medium</a>
Thanking you in advance.</p>
","language-model"
"58572733","How to deal with large vocab_size when training a Language Model in Keras?","2019-10-26 16:44:47","","1","2388","<keras><deep-learning><nlp><out-of-memory><language-model>","<p>I want to train a language model in Keras, by this tutorial:
<a href=""https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/"" rel=""nofollow noreferrer"">https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/</a></p>

<p>My input is composed of:
lines num: 4823744
maximum line: 20
Vocabulary Size: 790609
Total Sequences: 2172328
Max Sequence Length: 11</p>

<p>As you can see by this lines: </p>

<pre><code>num_words = 50
tokenizer = Tokenizer(num_words=num_words, lower=True)
tokenizer.fit_on_texts([data])
# determine the vocabulary size
vocab_size = len(tokenizer.word_index) + 1
</code></pre>

<p>I'm using the tokenizer with num_words=50. 
The vocab_size is taken from the tokenizer, but it's still the bigger size (790K).</p>

<p>Therefore this line:</p>

<pre><code>y = to_categorical(y, num_classes=vocab_size)
</code></pre>

<p>Causes a memory error. </p>

<p>This is the model definition: </p>

<pre><code>model = Sequential()
model.add(Embedding(vocab_size, 10, input_length=max_length-1))
model.add(LSTM(50))
model.add(Dense(vocab_size, activation='softmax'))
</code></pre>

<p>How can I deal with it?</p>

<p>I do want to have word-level model and not char-level. 
And I do want to take at least 10K of the most common words. </p>

<p>I thought about filtering words before hand, but it may cause the language model to learn false sequences. </p>

<p>How can I solve it? </p>

<p>Thanks</p>
","language-model"
"58309837","How does Ulmfit's language model work when applied on a text classification problem?","2019-10-09 18:01:27","58312614","1","120","<nlp><lstm><text-classification><language-model><fast-ai>","<p>I have been playing around with Ulmfit a lot lately and still cannot wrap my head around how the language model’s ability to make sound predictions about the next word affects the classification of texts. I guess my real problem is that I do not understand what is happening at the low level of the network. So correct me if I am wrong but the procedure is like this right (?):</p>

<p>The language model gets pre-trained and then fine-tuned. This part seems clear to me: Based on the current and preceding words you form probabilities about the next words.
Then the model gets stripped from the softmax layer designed to create the probability distribution.
You add the decoder consisting of a reLU-Layer (what is this layer actually doing?) and another softmax layer that outputs the probability of class membership of a given text document. So here are a lot of things I do not understand: How is the text document taken in and processed? Word for word I assume? So how do you end up with the prediction at the end? Is it averaged over all words?
Hmm you can see I am very confused. I hope you can help me understand Ulmfit better! Thanks in advance!</p>
","language-model"
"57884850","Difference between spaCy models sm, md, lg","2019-09-11 08:12:24","57885943","4","2901","<spacy><language-model>","<p>I can see that in <a href=""https://spacy.io/models/en"" rel=""nofollow noreferrer"">the English spaCy models</a> the medium model performs better than the small one, and the large model outperforms the medium one - but only marginally. However, in the description of the models, it is written that they have all been trained on OntoNotes. The exception being the vectors of md and lg, which have been trained on CommonCrawl. So if all models were trained on the same dataset (OntoNotes), and the only difference is the vectors, why then is there a performance difference for the tasks that don't require vectors? I would love to find out more about each model and the settings they were trained with and so on, but it appears that this information is not readily available.</p>
","language-model"
"57310742","How to use Deespeech (v0.5.1) effectively and use of language models during training and inference?","2019-08-01 13:57:07","","0","1096","<deep-learning><speech-to-text><language-model><mozilla-deepspeech>","<p>I am trying to train and use a model using Deepspeech v0.5.1 for English. My aim to train two models, one with and without a language model. Request your help on several fronts please. Sorry this is long but trying be as detailed as possible; and also, being new to Linux and data-science I may be stating some very obvious things.
Thank you in advance for your help. Since SO said the original form was spam, I am posting and answering this question with further background information.
Regards,
Rohit</p>

<p>B) My questions:</p>

<p>B1) When using a language model either for training or inference, do I HAVE to specify the lm_binary parameter AND the corresponding trie file? Can using only the trie work?</p>

<p>B2) Irrespective of whether a language model was used while training the model (binaryFile and trie together), later when the model is used for inference, can I choose to use OR not use a language model? Can a different language model be used later or only the one used for training? Are there things to note while choosing an alternative model? E.g. training using a 3-gram model but using a 4-gram model during inference? Is there anything else like this you can think of?</p>

<p>B3) Suppose my model is already built by training on a vocabulary file, arpa, trie and lm_binary built from only 10k data points. Say I create a new vocabulary called BigVocabulary.file from a larger corpus than the one used for training. E.g. the entire 629731 data points in validated.tsv file; use bigger vocabulary to create the .arpa, lmBinary and trie files. I ensure that the valid characters are exactly the same by comparing the alphabet files. Then on the model trained with smaller vocabulary, can I use BigVocabulary.binary.file and BigVocabulary.trie while doing inference using the command?</p>

<p>I already created a model with only first 1000 files and inference is poor but works.
Command:</p>

<blockquote>
  <p>deepspeech \
    --model /home/rohit/dpspTraining/models/v051/model8-validFirst1k-yesLM-4gram/savedModel/output_graph.pb \
    --alphabet /home/rohit/dpspTraining/data/wavFiles/commVoiceSet5-1kTotal/alphabetDir/alphabet-Set5First1050.txt \
    --lm /home/rohit/dpspTraining/data/wavFiles/commVoiceSet5-1kTotal/lm/lm4gram/vocabulary-Set5First1050_4gram.klm \
    --trie /home/rohit/dpspTraining/data/wavFiles/commVoiceSet5-1kTotal/trie/trie4gram/Set5First1050_4gram.trie \
    --audio /home/rohit/dpspTraining/data/wavFiles/wav33/test/File28.</p>
</blockquote>

<p>Console output:</p>

<blockquote>
  <p>(dpsp5v051basic) rohit@DE-W-0246802:~/dpspCODE/v051/DeepSpeech$ deepspeech \
    --model /home/rohit/dpspTraining/models/v051/model8-validFirst1k-yesLM-4gram/savedModel/output_graph.pb \
    --alphabet /home/rohit/dpspTraining/data/wavFiles/commVoiceSet5-1kTotal/alphabetDir/alphabet-Set5First1050.txt \
    --lm /home/rohit/dpspTraining/data/wavFiles/commVoiceSet5-1kTotal/lm/lm4gram/vocabulary-Set5First1050_4gram.klm \
    --trie /home/rohit/dpspTraining/data/wavFiles/commVoiceSet5-1kTotal/trie/trie4gram/Set5First1050_4gram.trie \
    --audio /home/rohit/dpspTraining/data/wavFiles/wav33/test/File28.wav
  Loading model from file /home/rohit/dpspTraining/models/v051/model8-validFirst1k-yesLM-4gram/savedModel/output_graph.pb
  TensorFlow: v1.13.1-10-g3e0cc53
  DeepSpeech: v0.5.1-0-g4b29b78
  Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.
  2019-08-01 16:11:02.155443: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
  2019-08-01 16:11:02.179690: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""CPU""') for unknown op: UnwrapDatasetVariant
  2019-08-01 16:11:02.179740: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: WrapDatasetVariant
  2019-08-01 16:11:02.179756: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""CPU""') for unknown op: WrapDatasetVariant
  2019-08-01 16:11:02.179891: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: UnwrapDatasetVariant
  Loaded model in 0.0283s.
  Loading language model from files /home/rohit/dpspTraining/data/wavFiles/commVoiceSet5-1kTotal/lm/lm4gram/vocabulary-Set5First1050_4gram.klm /home/rohit/dpspTraining/data/wavFiles/commVoiceSet5-1kTotal/trie/trie4gram/Set5First1050_4gram.trie
  Loaded language model in 0.068s.
  Running inference.
  a on a in a is the
  Inference took 0.449s for 3.041s audio file.</p>
</blockquote>

<p>But if i use the BigVocabulary.trie and lmBinary files then I get an error saying the trie file version mismatches and to update the trie file.</p>

<p>But it still seems to load the language model. So did Deepspeech actually pick it up and apply it correctly? How do I fix this error?</p>

<p>Command:</p>

<blockquote>
  <p>deepspeech \
    --model /home/rohit/dpspTraining/models/v051/model8-validFirst1k-yesLM-4gram/savedModel/output_graph.pb \
    --alphabet /home/rohit/dpspTraining/data/wavFiles/commVoiceSet5-1kTotal/alphabetDir/alphabet-Set5First1050.txt \
    --lm /home/rohit/dpspTraining/data/wavFiles/testVocabAllValidated/lm/lm4gram/vocabulary-allValidated_o4gram.klm \
    --trie /home/rohit/dpspTraining/data/wavFiles/testVocabAllValidated/trie/trie4gram/allValidated_o4gram.trie \
    --audio /home/rohit/dpspTraining/data/wavFiles/wav33/test/File28.wav</p>
</blockquote>

<p>Console output:</p>

<p>(dpsp5v051basic) rohit@DE-W-0246802:~/dpspCODE/v051/DeepSpeech$ deepspeech \</p>

<blockquote>
  <p>--model /home/rohit/dpspTraining/models/v051/model8-validFirst1k-yesLM-4gram/savedModel/output_graph.pb \
    --alphabet /home/rohit/dpspTraining/data/wavFiles/commVoiceSet5-1kTotal/alphabetDir/alphabet-Set5First1050.txt \
    --lm /home/rohit/dpspTraining/data/wavFiles/testVocabAllValidated/lm/lm4gram/vocabulary-allValidated_o4gram.klm \
    --trie /home/rohit/dpspTraining/data/wavFiles/testVocabAllValidated/trie/trie4gram/allValidated_o4gram.trie \
    --audio /home/rohit/dpspTraining/data/wavFiles/wav33/test/File28.wav
  Loading model from file /home/rohit/dpspTraining/models/v051/model8-validFirst1k-yesLM-4gram/savedModel/output_graph.pb
  TensorFlow: v1.13.1-10-g3e0cc53
  DeepSpeech: v0.5.1-0-g4b29b78
  Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.
  2019-08-01 16:11:58.305524: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
  2019-08-01 16:11:58.322902: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""CPU""') for unknown op: UnwrapDatasetVariant
  2019-08-01 16:11:58.322945: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: WrapDatasetVariant
  2019-08-01 16:11:58.322956: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""CPU""') for unknown op: WrapDatasetVariant
  2019-08-01 16:11:58.323063: E tensorflow/core/framework/op_kernel.cc:1325] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: UnwrapDatasetVariant
  Loaded model in 0.0199s.
  Loading language model from files /home/rohit/dpspTraining/data/wavFiles/testVocabAllValidated/lm/lm4gram/vocabulary-allValidated_o4gram.klm /home/rohit/dpspTraining/data/wavFiles/testVocabAllValidated/trie/trie4gram/allValidated_o4gram.trie
  Error: Trie file version mismatch (4 instead of expected 3). Update your trie file.
  Loaded language model in 0.00368s.
  Running inference.
   an on o tn o as te tee
  Inference took 1.893s for 3.041s audio file.</p>
</blockquote>

<p>Thank you for your time.</p>
","language-model"
"57273962","Ngrams from Tensorflow TextLineDataset","2019-07-30 14:23:46","57321351","2","223","<python><tensorflow><tensorflow-datasets><language-model>","<p>I have a text file containing one sentence per line</p>

<p>When I create a TextLineDataset and iterate on it with an iterator it returns the file line by line</p>

<p>I want to iterate through my file two tokens at a time, here's my current code:</p>

<pre><code>sentences = tf.data.TextLineDataset(""data/train.src"")
iterator = sentences.make_initializable_iterator()
next_element = iterator.get_next()

sess = tf.Session()

sess.run(tf.tables_initializer())
sess.run(iterator.initializer)

elem = sess.run(next_element)
print(elem)
</code></pre>

<p>Is it possible to do so using a TextLineDataset ?</p>

<p>EDIT : By ""tokens"" I mean ""words""</p>
","language-model"
"57248098","using huggingface's pytorch- transformers GPT-2 for classifcation tasks","2019-07-29 06:16:27","60574502","1","1264","<python><nlp><pytorch><language-model><huggingface-transformers>","<p>I want to use GPT-2 to make a text classifier model. I am not really sure what head should I add after I extracted features through the GPT-2. for eample I have a sequence.</p>

<pre><code>import pytorch_transformers as pt 
import torch
text=test.iloc[1,1]
text
'If a fire wanted fanning, it could readily be fanned with a newspaper, and as the government grew weaker, I have no doubt that leather and iron acquired durability in proportion, for, in a very short time, there was not a pair of bellows in all Rotterdam that ever stood in need of a stitch or required the assistance of a hammer.'
len(text)

74
tokenizer = pt.GPT2Tokenizer.from_pretrained('gpt2')
model = pt.GPT2Model.from_pretrained('gpt2')
zz = tokenizer.tokenize(text)
z1=torch.tensor([tokenizer.convert_tokens_to_ids(zz)])
z1
tensor([[ 1532,   257,  2046,  2227,  4336,   768,    11,   340,   714, 14704,
           307,   277,  3577,   351,   257,  7533,    11,   290,   355,   262,
          1230,  6348, 17642,    11,   314,   423,   645,  4719,   326, 11620,
           290,  6953,  9477, 26578,   287,  9823,    11,   329,    11,   287,
           257,   845,  1790,   640,    11,   612,   373,   407,   257,  5166,
           286,  8966,  1666,   287,   477, 18481,   353, 11043,   326,  1683,
          6204,   287,   761,   286,   257, 24695,   393,  2672,   262,  6829,
           286,   257, 15554,    13]])
output,hidden=model(z1)
ouput.shape
torch.Size([1, 74, 768])
</code></pre>

<p>the output of GPT2 is n x m x 768 for me, which n is the batch size,m is the number of tokens in the seqence(for example I can pad/truncate to 128.), so I can not do what as the paper said for a classification task just add a fully connected layer in the tail.And I searched on google, few GPT-2 classification task is mensioned.
I am not sure what is correct. Should I do flatten/max pooling/average pooling before the  fully connected layer or something else?</p>
","language-model"
"56298584","Access spaCy Masked Language Model","2019-05-24 19:29:14","56406000","1","433","<python><nlp><spacy><language-model>","<p>As of v2.1, spaCy has a BERT-style language model (LM). It predicts word-vectors instead of words, so I am going to use ""words"" and ""word vectors"" interchangeably here.</p>

<p>I need to take a sentence with a word masked, and a list of words, and rank the words by how likely they are to appear in the masked slot. Currently I am using BERT for this (similar to <a href=""https://github.com/yoavg/bert-syntax/blob/master/eval_bert.py#L14"" rel=""nofollow noreferrer"">bert-syntax</a>). I would like to see if spaCy's performance on this task is acceptable. Between <a href=""https://github.com/explosion/spaCy/blob/master/spacy/_ml.py"" rel=""nofollow noreferrer"">this file</a> and <a href=""https://github.com/explosion/spaCy/blob/master/spacy/cli/pretrain.py"" rel=""nofollow noreferrer"">this one</a> I'm pretty sure it's possible to build something. However, it feels like reaching deeper into the internals of the library than I'd like.</p>

<p>Is there a straightforward way to interact with spaCy's masked language model? </p>
","language-model"
"55973414","How to fine tune BERT on its own tasks?","2019-05-03 15:58:36","","0","3206","<python><keras><nlp><pytorch><language-model>","<p>I wanted to pre-train BERT with the data from my own language since multilingual (which includes my language) model of BERT is not successful. Since whole pre-training costs a lot, I decided to fine tune it on its own 2 tasks: masked language model and next sentence prediction. There are previous implementation on different tasks (NER, sentiment analysis etc.), but I couldn't find any fine tuning on its own tasks. Is there an implementation that I couldn't see? If not, where should I start? I need some initial help.</p>
","language-model"
"55935805","How to properly initialize the hidden state at first time step of a LSTM decoder in Keras","2019-05-01 11:49:48","","2","823","<keras><lstm><language-model>","<p>I am currently implementing the attr2seq model as described in this <a href=""https://aclweb.org/anthology/E17-1059"" rel=""nofollow noreferrer"">paper</a> by Dong et al. (2018) in Keras and I got completely stuck at initializing the hidden vectors at first time step of the LSTM decoder using the encoded attribute vectors $a$ (last paragraph in section ""3.2 Sequence Decoder""). Therefore, I want to find a solution to how to use the encoded attribute vectors to properly initialize the hidden vectors at the vary first time step of the sequence LSTM decoder.</p>

<p>I have been testing out a first approach as suggested <a href=""https://stackoverflow.com/questions/42415909/initializing-lstm-hidden-state-tensorflow-keras"">here</a> where one makes the LSTM layer to become stateful and then specify the hidden state after the model has been successfully compiled (and make sure that <code>shuffle = False</code> when fitting the model as well). However, one problem with this approach is that you have to specify a specific batch size and if you have a number of data samples that is not divisible by that batch size, the program will exit with an error (i.e. <code>Exception: In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size.</code>. Also, when fitting the model, then only the LSTM layers are trained whereas the attribute encoders are not considered during training, leading to un-trained attribute vectors. Maybe I have completely misunderstood the usage of them, so please correct me here if I am saying anything wrong.</p>

<p>Another approach I have tested out was to symbolically set a initial hidden state by assigning values to <code>initial_state</code> that is included in <code>call(…)</code> methods across the recurrent layers in Keras. However, I realized from the source code <a href=""https://github.com/keras-team/keras/blob/master/keras/layers/recurrent.py"" rel=""nofollow noreferrer"">here</a> that one has to provide weights matrices to provide some custom initial state of a LSTM layer and I am unfortunately clueless on how to do this, even though I think this kind of initialization is wanted but for the hidden vectors at first time step (i.e. <code>t=0</code>) rather than weights at the first time step. Maybe one could do this, but I don't since I am not used to Keras that much (or deep learning for that matter).</p>

<p>The codes illustrate roughly how the model should look like more or less. When running the codes in my program, I am getting the error message <code>ValueError: An initial_state was passed that is not compatible with cell.state_size. Received state_spec=[InputSpec(shape=(None, 514), ndim=2)]; however cell.state_size is (514, 514)</code>, which made me realize that <code>initial_state</code> required weights matrix rather than arbitrary number of states for each single data point.</p>

<pre class=""lang-py prettyprint-override""><code>x_drug = Input(shape=(onehotted_drugs.shape[1],))
x_cond = Input(shape=(onehotted_conds.shape[1],))
x_rating = Input(shape=(onehotted_ratings.shape[1],))

g_drug = Dense(self.attr_size)(x_drug)
g_cond = Dense(self.attr_size)(x_cond)
g_rating = Dense(self.attr_size)(x_rating)
g_concatenated = Concatenate()([g_drug, g_cond, g_rating])
a = Dense(self.num_layers * self.hidden_size, activation = ""tanh"")(g_concatenated)
hidden_vectors = [Lambda(lambda l: slice(l, (0, self.hidden_size*i), 
                         (-1, self.hidden_size*(i+1))))(a) for i in range(self.num_layers)]

x_prev_words = Input(shape = (self.num_tokens,))
ground_zero = Dense(self.hidden_size, use_bias = False)(x_prev_words)
lstm_layers = [LSTM(self.hidden_size, input_shape = 
                    (self.max_sequence_length - 1, self.hidden_size), return_sequences = True)(
               RepeatVector(self.max_sequence_length - 1)(ground_zero), initial_state = hidden_vectors[0])]
for i in range(1, self.num_layers):
   lstm_layers.append(LSTM(self.hidden_size, return_sequences = False if i==self.num_layers-1 else True)(
   lstm_layers[-1], initial_state = hidden_vectors[i]))

next_word_dist = Dense(self.num_tokens, activation = ""softmax"")(lstm_layers[-1])    
self.dong = Model(inputs=[x_drug, x_cond, x_rating, x_prev_words], outputs=[next_word_dist])
self.dong.compile(loss = ""categorical_crossentropy"", optimizer = ""rmsprop"")
</code></pre>

<p>If you would require any further information or more codes for inspection, then feel free to tell me. Thank you all for all help I can get with this matter!</p>
","language-model"
"55604680","How to predict whether the given sentence is grammatically correct or not?","2019-04-10 04:18:27","","0","478","<python><nlp><prediction><word2vec><language-model>","<p>I am trying to create a predictive model where the model tells whether the give sentence is correct or not by checking the order of the words in the sentence. The model checks weather the particular sequence of words as already occurred in a huge corpus and makes sense or no. </p>

<p>I tried doing this with the word2vec model and removed the cosine similarity or WMD distance of the two sentences but that only gives the similarity based on the word vector similarity and not the sequence of the words.</p>

<p>So if we give the input as 2 sentences:</p>

<p>Sentence 1- ""I am going to the shop""</p>

<p>Sentence 2- ""going I am the shop to""</p>

<p>output should indicate that the sentence is invalid or with a similarity of 20% or less</p>

<p>Whereas the word2vec model shows 100% similarity as the words entered are same irrespective of the order. So i guess it cannot be used for comparing the word order. Any other suggestions could also be very helpful.</p>
","language-model"
"55575681","Use BERT for feature extraction of a unique word","2019-04-08 14:25:44","55576031","1","1577","<python><tensorflow><nlp><language-model>","<p>I am using BERT for feature extraction of a word given the text where it appears, but it seems current implementation in bert's official github (<a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a>) can only compute the features of all the words in text, which makes it consume too much resources. Is it possible to adapt it for this purporse? Thanks!!</p>
","language-model"
"54890488","TextLMDataBunch Memory issue Language Model Fastai","2019-02-26 16:54:03","54980878","0","642","<nlp><out-of-memory><pytorch><language-model><fast-ai>","<p>I have a dataset with 45 million rows of data. I have three 6gb ram gpu. I am trying to train a language model on the data.</p>

<p>For that, I am trying to load the data as the fastai data bunch. But this part always fails because of the memory issue. </p>

<pre><code>data_lm = TextLMDataBunch.from_df('./', train_df=df_trn, 
valid_df=df_val, bs=10)
</code></pre>

<p>How do I handle this issue?</p>
","language-model"
"53928957","How to create window/chunk for list of sentences?","2018-12-26 07:39:25","53929129","1","365","<python><nlp><nltk><n-gram><language-model>","<p>I have list of sentence and I want to create skipgram <code>(window size = 3)</code> but I <strong>DONT</strong> want the counter to span across sentences since they are all unrelated.</p>

<p>So, if I have the sentences: </p>

<pre><code>[[""my name is John""] , [""This PC is black""]]
</code></pre>

<p>the triplets will be:</p>

<pre><code>[my name is]
[name is john]
[this PC is]
[PC is black]
</code></pre>

<p>What is the best way to do it?</p>
","language-model"
"53765598","Calculate perplexity of word2vec model","2018-12-13 15:54:49","","2","1633","<python><nlp><gensim><word2vec><language-model>","<p>I trained Gensim W2V model on 500K sentences (around 60K) words and I want to calculate the perplexity.</p>

<ol>
<li>What will be the best way to do so?</li>
<li>for 60K words, how can I check what will be a proper amount of data?</li>
</ol>

<p>Thanks</p>
","language-model"
"53521000","How to train ngram model on my own corpus","2018-11-28 13:51:54","","0","604","<nlp><nltk><n-gram><language-model><trigram>","<p>I have a corpus of list of strings:</p>

<pre><code>corpus = [""Hello I am Sam"", ""This is a white desk"",""I ate cereals"", ...]
</code></pre>

<p>I want to build a language model (preferably using nltk) on this corpus, to get the probability of a word in a sentence. 
So, my later usage will be to get</p>

<blockquote>
  <p>P(""Sam""| ""I am"")</p>
</blockquote>

<p>in this corpus.
I couldn't find - what is the best way to do so? How to train an ngram model and later get such probabilities?</p>

<p>Thanks!</p>
","language-model"
"53515547","Check perplexity of a Language Model","2018-11-28 08:56:22","","2","2146","<keras><nlp><lstm><language-model><perplexity>","<p>I created a language model with Keras LSTM and now I want to assess wether it's good so I want to calculate perplexity.</p>

<p>What is the best way to calc perplexity of a model in Python?</p>
","language-model"
"53289208","One hot encoding of 1 million category","2018-11-13 20:44:00","","1","196","<python><keras><neural-network><one-hot-encoding><language-model>","<p>For a language model, I have to predict a word for a given sequence of words. My vocabulary contains 1 million words. I'm trying to predict the words from it. I tried to use one hot encoding using keras (to_categorical) for predicted words. But for such a large vocabulary, I'm getting memory error in python. Is there any way to overcome this or my approach is wrong?</p>
","language-model"
"53195062","Why ""add one smoothing"" in language model does not count the </s> in denominator","2018-11-07 17:54:15","","1","884","<nlp><language-model>","<p>English is not my native language , Sorry for any grammatical mistakes.</p>

<p>I saw many documents for add one smoothing in language model, and I still very confused about the variable V in the formula:</p>

<pre><code>P (wi |w_i-1 ) = c(w_i-1 ,wi )+1  / c(w_i-1 )+V
</code></pre>

<p>as for this example corpus and I use bigram</p>

<pre><code>&lt;s&gt; John read Moby Dick &lt;/s&gt;
&lt;s&gt; Mary read a different book &lt;/s&gt;
&lt;s&gt; She read a book by Cher &lt;/s&gt;
</code></pre>

<p>if i want to calculate any P(wi | w_i-1) . The V will be 11
because the count of combination of [ w_i-1 , w ] is 11
. But I found it does not include the case   [w_i-1 , ""&lt;""/s"">""]  (or the V will be 12)
Why we do not need to include this case ? Isn't it the case that w_i-1 is in the end of an article or sentence ?</p>
","language-model"
"52804418","Code to create a reliable Language model from my own corpus","2018-10-14 15:49:23","","0","556","<python><nlp><lstm><data-science><language-model>","<p>I have a corpus of sentences in a specific domain.
I am looking for an open-source code/package, that I can give the data and it will generate a good, reliable language model. (Meaning, given a context, know the probability for each word).</p>

<p>Is there such a code/project?</p>

<p>I saw this github repo: <a href=""https://github.com/rafaljozefowicz/lm"" rel=""nofollow noreferrer"">https://github.com/rafaljozefowicz/lm</a>, but it didn't work.</p>
","language-model"
"52776653","Running out of memory when building a language model in Fast.ai 1.0","2018-10-12 09:37:44","","0","418","<machine-learning><nlp><out-of-memory><language-model><fast-ai>","<p>I'm dealing with a rather large text dataset (5.4 million short texts) and I'm trying to perform sentiment analysis con them on 16GB of ram.</p>

<p>I keep running out of memory whenever I try to build the language model:</p>

<pre><code>data_lm = text_data_from_csv(DATASET_PATH, data_func=lm_data, chunksize=4000)
# Out of memory
data_clas = text_data_from_csv(DATASET_PATH, data_func=classifier_data, vocab=data_lm.train_ds.vocab, chunksize=500)
</code></pre>

<p>I've played around with the chunksize but the memory usage seems to keep rising over time and eventually results in a memory error.</p>

<p>Is there any way to work around this?</p>
","language-model"
"52444202","get next word from bigram model on max probability","2018-09-21 12:50:56","52444927","2","1384","<python><nltk><defaultdict><language-model>","<p>I want to generate sonnets using nltk with bigrams. I have generated bigrams and computed probability of each bigram and stored in default dict like that.</p>
<pre><code>[('&quot;Let', defaultdict(&lt;function &lt;lambda&gt;.&lt;locals&gt;.&lt;lambda&gt; at0x1a17f98bf8&gt;, 
{'the': 0.2857142857142857, 'dainty': 
0.14285714285714285, 'it': 0.14285714285714285, 'those': 
0.14285714285714285, 'me': 0.14285714285714285, 'us': 
0.14285714285714285}))]
</code></pre>
<p>Probability of each word appearing after let is given. Like that I have bigram model for my corpus. Now I want to generate 4 lines sonnet with 15 words in each line. I have tried this code but it is not working.</p>
<pre><code>def generate_sonnet(word):
lines = 4
words= 15
for i in range(lines):
    line = ()
    for j in range(words):
   #I am selecting max probability but not that word. How I can select that word which has max probability of occurring with word?
        nword = float(max(model[word].values()))
        word += nword
        
word = random.choice(poetrylist)
generate_sonnet(word)
</code></pre>
<p>I select a random word and pass it to my function. where I want to join 15 words using bigrams and when 1 line completes the next 3 should be done.</p>
","language-model"
"52226905","In the context of recurrent neural networks, what is the meaning of 'conditioned on something'?","2018-09-07 17:07:06","52230892","2","767","<deep-learning><recurrent-neural-network><language-model>","<p>In recurrent neural networks (RNN), for example in the paper: <a href=""https://arxiv.org/pdf/1409.3215.pdf"" rel=""nofollow noreferrer"">Sequence to Sequence Learning with Neural Networks</a>, it says that RNN language model is conditioned on the input sequence on line 7 in paragraph 3 in the Introduction. </p>

<p>So, what is the concept of conditioning in RNN?</p>
","language-model"
"51638889","How word association mining is generalization of n-gram language model","2018-08-01 17:14:51","51640411","0","134","<data-mining><text-mining><language-model>","<p>I am working on text mining (reading book...) author said <strong>word association mining is actually the generalization of n-gram language model</strong> Can you please tell how word association mining is generalization of n-gram language model.
For Me <strong>word association mining</strong> is finding symptomatic relation (finding co-occurring) words and <strong>n-gram language model</strong> is compare all n-words in query to suggest or return relevant documents. </p>
","language-model"
"51170775","fastai: ValueError: __len__() should return >= 0","2018-07-04 09:50:18","51227853","3","3235","<deep-learning><language-model>","<p>While running the following program - <a href=""https://rawgit.com/sizhky/eef1482e63387df8e9e045ac1e5a0ce8/raw/bdbebafaab21739a27f6bf32e83da1557919b44b/lm.html"" rel=""nofollow noreferrer"">https://rawgit.com/sizhky/eef1482e63387df8e9e045ac1e5a0ce8/raw/bdbebafaab21739a27f6bf32e83da1557919b44b/lm.html</a></p>

<p>I'm unable to call <code>learner.fit</code> as it throws the above error.</p>

<p>Specifically,<br>
I'm trying to train a language model taking a text file and converting it to a <code>LanguageModelData</code> and feeding it to an RNN via <code>get_model</code></p>

<p><code>
    md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)<br>
    learner = md.get_model(opt_fn, em_sz, nh, nl,
                   dropouti=0.05, dropout=0.05, wdrop=0.1, dropoute=0.02, dropouth=0.05)
    learner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)
    learner.clip=0.3
    learner.fit(3e-3, 4)
</code></p>

<hr>

<pre><code>ValueErrorTraceback (most recent call last)
&lt;ipython-input-7-579772ee6693&gt; in &lt;module&gt;()
----&gt; 1 learner.fit(3e-3, 4)

/nfsroot/data/home/yeshwanth/misc/fastai/fastai/courses/dl1/practice/fastai/learner.py in fit(self, lrs, n_cycle, wds, **kwargs)
    285         self.sched = None
    286         layer_opt = self.get_layer_opt(lrs, wds)
--&gt; 287         return self.fit_gen(self.model, self.data, layer_opt, n_cycle, **kwargs)
    288 
    289     def warm_up(self, lr, wds=None):

/nfsroot/data/home/yeshwanth/misc/fastai/fastai/courses/dl1/practice/fastai/learner.py in fit_gen(self, model, data, layer_opt, n_cycle, cycle_len, cycle_mult, cycle_save_name, best_save_name, use_clr, use_clr_beta, metrics, callbacks, use_wd_sched, norm_wds, wds_sched_mult, use_swa, swa_start, swa_eval_freq, **kwargs)
    232             metrics=metrics, callbacks=callbacks, reg_fn=self.reg_fn, clip=self.clip, fp16=self.fp16,
    233             swa_model=self.swa_model if use_swa else None, swa_start=swa_start,
--&gt; 234             swa_eval_freq=swa_eval_freq, **kwargs)
    235 
    236     def get_layer_groups(self): return self.models.get_layer_groups()

/nfsroot/data/home/yeshwanth/misc/fastai/fastai/courses/dl1/practice/fastai/model.py in fit(model, data, n_epochs, opt, crit, metrics, callbacks, stepper, swa_model, swa_start, swa_eval_freq, **kwargs)
    159 
    160         if not all_val:
--&gt; 161             vals = validate(model_stepper, cur_data.val_dl, metrics, seq_first=seq_first)
    162             stop=False
    163             for cb in callbacks: stop = stop or cb.on_epoch_end(vals)

/nfsroot/data/home/yeshwanth/misc/fastai/fastai/courses/dl1/practice/fastai/model.py in validate(stepper, dl, metrics, seq_first)
    220     stepper.reset(False)
    221     with no_grad_context():
--&gt; 222         for (*x,y) in iter(dl):
    223             y = VV(y)
    224             preds, l = stepper.evaluate(VV(x), y)

/nfsroot/data/home/yeshwanth/misc/fastai/fastai/courses/dl1/practice/fastai/nlp.py in __next__(self)
    135 
    136     def __next__(self):
--&gt; 137         if self.i &gt;= self.n-1 or self.iter&gt;=len(self): raise StopIteration
    138         bptt = self.bptt if np.random.random() &lt; 0.95 else self.bptt / 2.
    139         seq_len = max(5, int(np.random.normal(bptt, 5)))

ValueError: __len__() should return &gt;= 0
</code></pre>
","language-model"
"51093636","Keras shape error when checking input","2018-06-29 02:57:20","","3","87","<neural-network><keras><language-model>","<p>I am trying to train a simple MLP model that maps input questions (using a 300D word embedding) and image features extracted using a pretrained VGG16 model to a feature vector of fixed length. However, I can't figure out how to fix the error mentioned below. Here is the code I'm trying to run at the moment:</p>



<pre class=""lang-py prettyprint-override""><code>parser = argparse.ArgumentParser()
parser.add_argument('-num_hidden_units', type=int, default=1024)
parser.add_argument('-num_hidden_layers', type=int, default=3)
parser.add_argument('-dropout', type=float, default=0.5)
parser.add_argument('-activation', type=str, default='tanh')
parser.add_argument('-language_only', type=bool, default= False)
parser.add_argument('-num_epochs', type=int, default=10) #default=100
parser.add_argument('-model_save_interval', type=int, default=10)
parser.add_argument('-batch_size', type=int, default=128)
args = parser.parse_args()

questions_train = open('data/qa/preprocess/questions_train2014.txt', 'r').read().splitlines()
answers_train = open('data/qa/preprocess/answers_train2014_modal.txt', 'r').read().splitlines()
images_train = open('data/qa/preprocess/images_train2014.txt', 'r').read().splitlines()
vgg_model_path = 'data/coco/vgg_feats.mat'
maxAnswers = 1000
questions_train, answers_train, images_train = selectFrequentAnswers(questions_train,answers_train,images_train, maxAnswers)

#encode the remaining answers
labelencoder = preprocessing.LabelEncoder()
labelencoder.fit(answers_train)
nb_classes = len(list(labelencoder.classes_))
joblib.dump(labelencoder,'models/labelencoder.pkl')

features_struct = scipy.io.loadmat(vgg_model_path)
VGGfeatures = features_struct['feats']
print ('loaded vgg features')
image_ids = open('data/coco/coco_vgg_IDMap.txt').read().splitlines()
id_map = {}
for ids in image_ids:
    id_split = ids.split()
    id_map[id_split[0]] = int(id_split[1])

nlp = English()
print ('loaded word2vec features...')
img_dim = 4096
word_vec_dim = 300

model = Sequential()
if args.language_only:
    model.add(Dense(args.num_hidden_units, input_dim=word_vec_dim, init='uniform'))
else:
    model.add(Dense(args.num_hidden_units, input_dim=img_dim+word_vec_dim, init='uniform'))
model.add(Activation(args.activation))
if args.dropout&gt;0:
    model.add(Dropout(args.dropout))
for i in range(args.num_hidden_layers-1):
    model.add(Dense(args.num_hidden_units, init='uniform'))
    model.add(Activation(args.activation))
    if args.dropout&gt;0:
        model.add(Dropout(args.dropout))
model.add(Dense(nb_classes, init='uniform'))
model.add(Activation('softmax'))

json_string = model.to_json()
if args.language_only:
    model_file_name = 'models/mlp_language_only_num_hidden_units_' + str(args.num_hidden_units) + '_num_hidden_layers_' + str(args.num_hidden_layers)
else:
    model_file_name = 'models/mlp_num_hidden_units_' + str(args.num_hidden_units) + '_num_hidden_layers_' + str(args.num_hidden_layers)     
open(model_file_name  + '.json', 'w').write(json_string)

print ('Compiling model...')
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
print ('Compilation done...')

print ('Training started...')
for k in range(args.num_epochs):
    #shuffle the data points before going through them
    index_shuf = list(range(len(questions_train)))
    shuffle(index_shuf)
    questions_train = [questions_train[i] for i in index_shuf]
    answers_train = [answers_train[i] for i in index_shuf]
    images_train = [images_train[i] for i in index_shuf]
    progbar = generic_utils.Progbar(len(questions_train))
    for qu_batch,an_batch,im_batch in zip(grouper(questions_train, args.batch_size, fillvalue=questions_train[-1]), 
                                        grouper(answers_train, args.batch_size, fillvalue=answers_train[-1]), 
                                        grouper(images_train, args.batch_size, fillvalue=images_train[-1])):
        X_q_batch = get_questions_matrix_sum(qu_batch, nlp)
        if args.language_only:
            X_batch = X_q_batch
        else:
            X_i_batch = get_images_matrix(im_batch, id_map, VGGfeatures)
            X_batch = np.hstack((X_q_batch, X_i_batch))
        Y_batch = get_answers_matrix(an_batch, labelencoder)
        loss = model.train_on_batch(X_batch, Y_batch)            
        progbar.add(args.batch_size, values=[(""train loss"", loss)])
    #print type(loss)
    if k%args.model_save_interval == 0:
        model.save_weights(model_file_name + '_epoch_{:02d}.hdf5'.format(k))

model.save_weights(model_file_name + '_epoch_{:02d}.hdf5'.format(k))
</code></pre>

<p>And here is the error I get:</p>

<blockquote>
  <p>Keras: Error when checking input: expected dense_9_input to have shape
  (4396,) but got array with shape (4096,)</p>
</blockquote>
","language-model"
"51019655","any way to combine 2 ngram language model into 1?","2018-06-25 08:57:25","","2","663","<n-gram><language-model>","<p>I has 2 ngram language model (<code>model_A</code> and <code>model_B</code>) now.</p>

<ul>
<li>they are trained based on differenct corpus, so the vocabulary is different</li>
<li>they are smoothed with backoff, stored in <code>ARPA</code> format, so I have 2 <code>ARPA</code> files, <code>ARPA_A</code> and <code>ARPA_B</code>.</li>
</ul>

<p>Now if I want to interpolate them, i.e. given any phrase ABC.</p>

<ul>
<li><code>model_C(ABC) = 0.5 * model_A(ABC) + 0.5 * model_B(ABC)</code></li>
</ul>

<p>How can I merge <code>ARPA_A</code> and <code>ARPA_B</code> into one (<code>ARPA_C</code>)?</p>
","language-model"
"50623129","Alternative to one-hot encoding for output to a model when vocabulary size is very large","2018-05-31 11:20:47","","4","865","<nlp><keras><language-model>","<p>I was following <a href=""https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/"" rel=""nofollow noreferrer"">this blog</a>. In it he talks about how to build a language model in keras. He shows how to build a simple model in keras.</p>
<blockquote>
<p>After separating, we need to one hot encode the output word. This means converting it from an integer to a vector of 0 values, one for each word in the vocabulary, with a 1 to indicate the specific word at the index of the words integer value.</p>
<p>This is so that the model learns to predict the probability distribution for the next word and the ground truth from which to learn from is 0 for all words except the actual word that comes next.</p>
<p>Keras provides the to_categorical() that can be used to one hot encode the output words for each input-output sequence pair.</p>
</blockquote>
<p>He uses the following:</p>
<p><code>y = to_categorical(y, num_classes=vocab_size)</code></p>
<p>In his case, the vocabulary size is manageable. I am working with vocabulary having size &gt; 100 million. I guess I should not use a one-hot encoding for the output <code>y</code> as done by him. Is there any alternative?</p>
","language-model"
"50400481","Positional Encodings leads to worse convergence, language modeling","2018-05-17 21:05:22","","0","456","<python><encoding><keras><position><language-model>","<p>This is a tough question, but I might as well try. I'm implementing the architecture from this paper <a href=""https://arxiv.org/pdf/1503.08895.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1503.08895.pdf</a> for language modeling. See page 2 for a diagram, and the top of page 5 for the section on positional or ""temporal"" encoding. More on positional encoding can be found here, <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1706.03762.pdf</a> at the bottom of page 5/top of page 6. (I was directed to that second paper by the authors of the first.)</p>

<p>So here's my keras implementation in a nutshell:</p>

<pre><code>word_seq = Input(shape = (SEQ_LEN,), dtype = ""int32"", name = ""word_seq"")

query = Input(shape = (EMBED_DIM, ), dtype = ""float32"", name = ""q_input"")
#the query for lang. modeling is a constant vector filled with 0.1, as described at the bottom of page 7 in the first linked paper

T_A = Added_Weights(input_dim = (SEQ_LEN, EMBED_DIM))
#Added_Weights is a custom layer I wrote, which I'll post below
#These are the ""positional encoding"" components

T_C = Added_Weights(input_dim = (SEQ_LEN, EMBED_DIM))

Emb_A = Embedding(output_dim = EMBED_DIM, input_dim = VOCAB_SIZE, input_length = SEQ_LEN, name = ""Emb_A"")

Emb_C = Embedding(output_dim = EMBED_DIM, input_dim = VOCAB_SIZE, input_length = SEQ_LEN, name = ""Emb_C"")

int_state_weights = Dense(units = EMBED_DIM, activation = 'linear',
           kernel_initializer=RandomNormal(mean=0., stddev = 0.05, seed = None))

layer_output = query
#the loop uses the output from the previous layer as the query, but the first layer's query is just that constant vector

for i in range(0, NUM_LAYERS - 1):
    memories = Emb_A(word_seq) #these all re-use the weights instantiated earlier.

    memories = T_A(memories)

    memories = Dropout(DROPOUT_R)(memories)

    content = Emb_C(word_seq)

    content = T_C(content)

    mem_relevance = Dot(axes=[1, 2])([layer_output, memories])

    weighted_internal_state = int_state_weights(mem_relevance)

    mem_relevance = Softmax()(mem_relevance)

    content_relevance = Dot(axes=1)([mem_relevance,
                                content])  # weight each piece of content by it's probability of being relevant

    layer_output = Add()([content_relevance, weighted_internal_state])

    layer_output = Dropout(DROPOUT_R)(layer_output)

final_output = Dense(units = VOCAB_SIZE, activation ='relu',
                 kernel_initializer=RandomNormal(mean=0., stddev = 0.05, seed = None))(layer_output)

model = Model(inputs = [word_seq, query], outputs = prediction)
model.compile(optimizer = SGD(lr = 0.01, clipnorm = 50.), loss = 'categorical_crossentropy', metrics = ['accuracy'])
model.fit(x = [td_seqs, td_query], y = [td_labels],
      batch_size = BATCH_SIZE, callbacks = [lr_adjust, lr_termination, for_csv], epochs=200, verbose = 1)
</code></pre>

<p>BATCH_SIZE is currently 128. This went well on ~35,000 training samples BEFORE I added the T_A and T_C parts, ending at 96% accuracy. As soon as I implemented T_A and T_C (the positional encoding), training ended at around 10% accuracy and 5.2-ish training loss. I increased the training data by a factor of 10 and didn't see any real improvement. Here's my Added_Weights class:</p>

<pre><code>class Added_Weights(Layer):

    def __init__(self, input_dim, **kwargs):
        super(Added_Weights, self).__init__(**kwargs)
        self.input_dim = input_dim

    def build(self, input_shape):
        # Create a trainable weight variable for this layer.
        self.kernel = self.add_weight(name='kernel',
                                  shape=(self.input_dim[0], self.input_dim[1]),
                                  initializer=RandomNormal(mean=0., stddev=0.05, seed=None),
                                  trainable=True)


        super(Added_Weights, self).build(input_shape)  


    def call(self, x, **kwargs):
        return x + self.kernel

    def compute_output_shape(self, input_shape):
        return input_shape
</code></pre>

<p>I am agonizing over why this won't work, after reading both of these awesome papers explicitly stating that it SHOULD work. If anyone can manage to help with this, that would be amazing.</p>
","language-model"
"50006582","How to learn pocketsphinx for bi-lingual system?","2018-04-24 16:23:34","","0","146","<machine-learning><cmusphinx><pocketsphinx><language-model><phonetics>","<p>I did create a dictionary with 2 languages(English/Persian) at the one file like this:</p>

<pre><code>بگو        B E G U
خزنده        KH A Z A N D E
قدت        GH A D E T
چنده        CH A N D E
قد         GH A D
من        M A N
شب        SH A B
hi        H AA Y
hello        H E L L O
how        H O V
are        AA R
you        Y U
what         V AA T
is         I Z
your         Y O R
name        N E Y M
old        O L D
where         V E R
from        F E R AA M
</code></pre>

<p>And used <a href=""http://www.speech.cs.cmu.edu/tools/lmtool-new.html"" rel=""nofollow noreferrer"">http://www.speech.cs.cmu.edu/tools/lmtool-new.html</a> to build the language model. Then I tried to learn an acoustic model with that language model and test it.</p>

<p>It works good for Persian voices but doesn't work for English words. After some try&amp;error I found that the problem is about my phoneset. I used my own phoneset as you can see above, but it seems pocketsphinx doesn't accept this phoneset for English words and it only accepts it's own phoneset for English!</p>

<p>So I want to know did I found the problem true? Should I use the pocketsphinx phoneset for my Persian words as well? Where should I find it's complete phoneset and a guide to learn how to use it for Persian words?</p>
","language-model"
"49828024","How to build deep learning model that picks words from serval distinct bags and forms a meaningful sentence","2018-04-14 04:45:06","50005527","-2","389","<deep-learning><keras><nltk><language-model><google-natural-language>","<p><a href=""https://i.sstatic.net/dRZ88.png"" rel=""nofollow noreferrer"">Image of Bags and how to choose from them</a></p>

<p>Imagine I have 10 bags,Ordered one after other.ie Bag 1 , Bag 2 ......... Bag n.</p>

<p>Each bag has distinct set of words.</p>

<p>In order to understand what a bag is,
Consider we have a vocabulary of 10,000 words.
The first bag contains words Hello , India , Manager.</p>

<p>ie Bag 1 will have 1's at the words index present in the bag.
ex:Bag 1 will be of size 10000*1 
if Hello's index was 1 India's index was 2 and Manager's was 4
It will be
[0 , 1, 1, 0 , 1 ,0,0,0,0.........]</p>

<p>*I dont have a model yet.
*I'm thinking to use story books,But its still kind of abstract for me.</p>

<p>A word has to chosen from each bag and assigned a number word 1(word from bag 1)
word 2(word from bag 2) and they must form a MEANINGFULL sentence in their numerical order.!</p>
","language-model"
"49738313","How to predict word using trained CBOW","2018-04-09 17:18:34","49739761","2","1472","<neural-network><nlp><deep-learning><word2vec><language-model>","<p>I have a question about CBOW prediction. Suppose my job is to use 3 surrounding words w(t-3), w(t-2), w(t-1)as input to predict one target word w(t). Once the model is trained and I want to predict a missing word after a sentence. Does this model only work for a sentence with four words which the first three are known and the last is unknown? If I have a sentence in 10 words. The first nine words are known, can I use 9 words as input to predict the last missing word in that sentence?</p>
","language-model"
"49605248","How to relate the language model score of a whole sentence to those of the sentence's constituents","2018-04-02 04:17:39","","4","1128","<python><nlp><language-model><kenlm>","<p>I trained a KENLM language model on around 5000 English sentences/paragraphs. I want to query this ARPA model with two or more segments and see if they can be concatenated to form a longer sentence, hopefully more ""grammatical."" Here as follows is the Python code that I have used to get the logarithmic scores - and the ten-based power value - of the segments and the ""sentence."" I have given two examples. Obviously, the sentence in the first example is more grammatical than the one in the second example. However, my question is not about this, but about how to relate the language model score of a whole sentence to those of the sentence's constituents. That is, if the sentence is grammatically better than its constituents.</p>

<pre><code>import math
import kenlm as kl
model = kl.LanguageModel(r'D:\seg.arpa.bin')
print ('************')
sentence = 'Mr . Yamada was elected Chairperson of'
print(sentence)
p1=model.score(sentence)
p2=math.pow(10,p1)
print(p1)
print(p2)
sentence = 'the Drafting Committee by acclamation .'
print(sentence)
p3=model.score(sentence)
p4=math.pow(10,p3)
print(p3)
print(p4)
sentence = 'Mr . Yamada was elected Chairperson of the Drafting Committee by acclamation .'
print(sentence)
p5=model.score(sentence)
p6=math.pow(10,p5)
print(p5)
print(p6)
print ('-------------')
sentence = 'Cases cited in the present volume ix'
print(sentence)
p1=model.score(sentence)
p2=math.pow(10,p1)
print(p1)
print(p2)
sentence = 'Multilateral instruments cited in the present volume xiii'
print(sentence)
p3=model.score(sentence)
p4=math.pow(10,p3)
print(p3)
print(p4)
sentence = 'Cases cited in the present volume ix Multilateral instruments cited in the present volume xiii'
print(sentence)
p5=model.score(sentence)
p6=math.pow(10,p5)
print(p5)
print(p6)
</code></pre>

<ul>
<li>************ Mr . Yamada was elected Chairperson of
-34.0706558228
8.49853715087e-35 the Drafting Committee by acclamation .
-28.3745193481
4.22163470933e-29 Mr . Yamada was elected Chairperson of the Drafting Committee by acclamation .
-55.5128440857
3.07012398337e-56
------------- Cases cited in the present volume ix
-27.7353248596
1.83939558773e-28 Multilateral instruments cited in the present volume xiii
-34.4523620605
3.52888852435e-35 Cases cited in the present volume ix Multilateral instruments cited in the present volume xiii
-60.7075233459
1.9609957573e-61</li>
</ul>
","language-model"
"49124333","NLP - What to do when unigram is not present in corpus while doing stupid backoff smoothing","2018-03-06 06:01:42","","1","623","<nlp><stanford-nlp><smoothing><language-model>","<p>In stupid backoff for smoothing for trigrams, if trigram is not found then we backoff to bigram , if bigram is also not found we backoff to unigram. But what if unigram is not present in the corpus. In the <a href=""http://www.aclweb.org/anthology/D07-1090.pdf"" rel=""nofollow noreferrer"">paper</a> under stupid backoff section it is mentioned that </p>

<blockquote>
  <p>The
  recursion ends at unigrams</p>
</blockquote>

<p>So what probability should be assigned to a completely new unigram, which is not present in training dataset.</p>
","language-model"
"49077647","How to normalize probabilities of words in varying length sentences?","2018-03-02 21:24:43","","2","2023","<machine-learning><nlp><deep-learning><recurrent-neural-network><language-model>","<p>Let's say we have an RNN model that outputs the probability of a word given context (or no context) trained on a corpus. 
We can chain the probability of each word in a sequence to get the overall probability of the sentence itself. But, because we are chaining, the probability (or likelihood) of the sentence goes down as it's length increases. This is the same case even if we are using log probabilities. </p>

<p>Is there anyway we could normalize these probabilities? This is an interesting subproblem that I am facing while building a language model. I have a corpus of 9 million sentences of whose lengths vary from 2-30. But all of the sentences are valid ones and I am using these as the corpus to train the LM.</p>

<p>Now, I am taking a subset of data and making changes to it like shuffling or cutting the sentence into half, prepending or appending a random word and so on. This is to create a ""fake sentence"" that need not be valid. What I would like to do is get a threshold of some sort over the likelihood of all the valid sentences and then when I use the RNN to compute probability of the fake sentence, it should be fairly smaller or different form the calculated threshold.</p>

<p>tldr;
sentences like </p>

<pre><code>""the cat sat on the red mat""
""the cat sat on a mat""
""a cat sat on the red mat with brown coffee stains""
</code></pre>

<p>should all have a comparable probability/score/metric
while sentences like</p>

<pre><code>""cat cat mat on the brown red sat is""
""not mat in door on cat""
</code></pre>

<p>have a lower score.</p>
","language-model"
"48868660","TensorFlow - predicting next word - loss function logit na target shape","2018-02-19 14:51:24","48931530","0","121","<tensorflow><neural-network><recurrent-neural-network><seq><language-model>","<p>I'm trying to create a language model. I have <code>logit</code> and target of size: <code>[32, 312, 512]</code></p>

<p>Where: </p>

<ul>
<li><code>.shape[0]</code> is <code>batch_size</code></li>
<li><code>.shape[1]</code> is <code>sequence_max_len</code></li>
<li><code>.shape[2]</code> is <code>vocabulary size</code></li>
</ul>

<p>The question is - when I pass <code>logit</code> and <code>target</code> to the loss function as follows:</p>

<pre><code>self.loss = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(
                                          logits=self.logit, labels=self.y))
</code></pre>

<p>Does it compute appropriate loss for the current batch? Or should I reshape <code>logit</code> and <code>target</code> to express the following shape: <code>[32, 312*512]</code>?</p>

<p>Thanks in advance for your help!</p>
","language-model"
"48779574","Differences between en_vectors_web_lg and Glove vectors (spaCy)","2018-02-14 04:15:07","","5","3270","<python><spacy><language-model>","<p><a href=""https://spacy.io/models/en#en_vectors_web_lg"" rel=""noreferrer"">https://spacy.io/models/en#en_vectors_web_lg</a>
stated that the model contains 1.1m keys, but 
<a href=""https://nlp.stanford.edu/projects/glove/"" rel=""noreferrer"">https://nlp.stanford.edu/projects/glove/</a>
stated that the Glove vectors contain 2.2M vocabs</p>

<p>May I know what vocabs are missing?</p>

<p>Thank you very much.</p>
","language-model"
"47558634","Tensorflow num_classes parameter of nce_loss()","2017-11-29 17:40:28","","2","674","<tensorflow><nlp><word2vec><word-embedding><language-model>","<p>My understanding of noise contrastive estimation is that we sample some vectors from our word embeddings (the negative sample), and then calculate the log-likelihood of each. Then we want to maximize the difference between the probability of the target word and the log-likelihood of each of the negative sample words (So if I'm correct about this, we want to optimize the loss function so that it gets as close to 1 as possible).</p>

<p>My question is this:</p>

<p>What is the purpose of the <code>num_classes</code> parameters to the <code>nce_loss</code> function? My best guess is that the number of classes is passed in so that Tensorflow knows the size of the distribution from which the negative samples our drawn, but this might not make sense, since we could just infer the size of the distribution from the variable itself. Otherwise, I can't think of a reason for why we would need to know the total possible number of classes, especially if the language model is only outputting k + 1 predictions (negative sample size + 1 for the target word). </p>
","language-model"
"47360704","Extract word/sentence probabilities from lm_1b trained model","2017-11-17 23:21:41","","4","496","<python><tensorflow><nlp><lstm><language-model>","<p>I have successfully downloaded the 1B word language model trained using a CNN-LSTM (<a href=""https://github.com/tensorflow/models/tree/master/research/lm_1b"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/tree/master/research/lm_1b</a>), and I would like to be able to input sentences or partial sentences to get the probability of each subsequent word in the sentence.</p>

<p>For example, if I have a sentence like, ""An animal that says "", I'd like to know the probability of the next word being ""woof"" vs. ""meow"".</p>

<p>I understand that running the following produces the LSTM embeddings:</p>

<pre><code>bazel-bin/lm_1b/lm_1b_eval --mode dump_lstm_emb \
                           --pbtxt data/graph-2016-09-10.pbtxt \
                           --vocab_file data/vocab-2016-09-10.txt \
                           --ckpt 'data/ckpt-*' \
                           --sentence ""An animal that says woof"" \                             
                           --save_dir output
</code></pre>

<p>That will produce files <code>lstm_emb_step_*.npy</code> where each file is the LSTM embedding for each word in the sentence. How can I transform these into probabilities over the trained model to be able to compare <code>P(woof|An animal that says)</code> vs. <code>P(meow|An animal that says)</code>?</p>

<p>Thanks in advance.</p>
","language-model"
"47305633","language modeling in tensorflow - how to tie embedding and softmax weights","2017-11-15 10:58:22","47573828","1","1149","<tensorflow><nlp><language-model>","<p>As suggested by recent language modeling papers, I want to use weight tying in my RNN language model. That is, I want to share the weights between the embedding and softmax layer. However, I am not sure how this can be done in TensorFlow.</p>

<p>My network receives inputs of shape <code>(batch_size, sequence_length)</code>. The embedding matrix has shape <code>(vocab_size, embedding_size)</code> and is created as follows (I am using pre-trained word2vec embeddings):</p>

<pre><code>        with tf.variable_scope('embedding'):
            self.embedding_matrix = tf.Variable(tf.constant(0.0, shape=[self.vocab_size, self.embd_size]), trainable=False, name='embedding')
            self.embed_placeholder = tf.placeholder(tf.float32, [self.vocab_size, self.embd_size])
            self.embed_init = self.embedding_matrix.assign(self.embed_placeholder)
</code></pre>

<p>The logits are computed as follows:</p>

<pre><code>            output, self.final_state = tf.nn.dynamic_rnn(
                cell,
                inputs=self.inputs,
                initial_state=self.init_state)

            self.output_flat = tf.reshape(output, [-1, cell.output_size])
            softmax_w = tf.get_variable(""softmax_w"", [self.n_hidden, self.vocab_size], dtype=tf.float32)

            softmax_b = tf.get_variable(""softmax_b"", [self.vocab_size], dtype=tf.float32)
            logits = tf.nn.xw_plus_b(self.output_flat, softmax_w, softmax_b)
            # Reshape logits to be a 3-D tensor
            self.logits = tf.reshape(logits, [self.batch_size, self.seq_length, self.vocab_size])
</code></pre>

<p>My questions are:</p>

<ol>
<li>The matrix that has to be changed to using the embeddings weights is <code>softmax_w</code>, correct?</li>
<li><code>softmax_w</code> has shape <code>(n_hidden, vocab_size)</code>. How does that fit the size of the embedding matrix? Or do I have to ensure that n_hidden = embedding_size?</li>
<li>How can I reuse the embedding weights in TensorFlow? I know that I have to use <code>reuse=True</code> in the variable_scope.</li>
</ol>
","language-model"
"47297321","how to learn language model?","2017-11-15 00:05:14","47298406","0","95","<machine-learning><nlp><lstm><language-model><penn-treebank>","<ol>
<li><p>I'm trying to train a language model with LSTM based on Penn Treebank (PTB) corpus.</p>

<p>I was thinking that I should simply train with every bigram in the corpus so that it could predict the next word given previous words, but then it wouldn't be able to predict next word based on multiple preceding words.    </p>

<p>So what exactly is it to train a language model?    </p></li>
<li><p>In my current implementation, I have batch size=20 and the vocabulary size is 10000, so I have 20 resulting matrices of 10k entries (parameters?) and the loss is calculated by making comparison to 20 ground-truth matrices of 10k entries, where only the index for actual next word is 1 and other entries are zero. Is this a right implementation? I'm getting perplexity of around 2 that hardly changes over iterations, which is definitely not in a right range of what it usually is, say around 100.  </p></li>
</ol>
","language-model"
"47163477","language modeling - model loss and accuracy not improving, model is underfitting","2017-11-07 16:59:29","","0","990","<python><tensorflow><nlp><language-model>","<p>I am trying to build a word-level language model in TensorFlow. My inputs are batches with word id's of shape <code>(batch_size, seq_length)</code>, my targets are the inputs shifted one time step to the left (so for each word, the target is the next word in the sequence).</p>

<p>The model receives word embeddings as an input (word embeddings were pre-trained using gensim word2vec). I manually checked that the word embeddings are read in correctly and that they correspond to the right word id's.</p>

<p>Although I have tried out a lot of things, my model is not improving. Even when training for 100 epochs over the full training set, the accuracy remains the same.</p>

<p>What I have tried (without any success):</p>

<ul>
<li>Removing dropout. My first goal is to get rid of underfitting</li>
<li>Different vocabulary size (100, 1000, 10000)</li>
<li>Using gradient clipping/ not using gradient clipping</li>
<li>Changing the initialization of the weights </li>
<li>Data shuffling</li>
<li>different optimizer (RSMProp, Adam and Gradient Descent)</li>
<li>larger/smaller model (2-4 hidden layers with 128-256 hidden units)</li>
<li>different batch size (10, 20, 128)</li>
<li>different learning rate (0.01, 0.001, 0.1)</li>
<li>different loss function (sparse_softmax_cross_entropy_with_logits or tf.contrib.seq2seq.sequence_loss)</li>
<li>refeeding/not refeeding the final state of the LSTM during training*</li>
</ul>

<p>In the beginning, both loss and accuracy are improving. Also, the model is adapting its predictions. But then, after some epochs over the full training set, loss and accuracy stay constant. Also, the model predictions aren't changing anymore and it gets stuck.
Here is an example that shows the development of loss and accuracy for the same input sequence. After epoch 30, nothing is changing anymore:</p>

<pre><code>2017-11-08 06:59:24,298 - DEBUG - Targets: [  91    4    9  116  237 1953  240    3    2    1    0    2    1    9  144 351   29  299   24  453]
 2017-11-08 06:59:24,299 - DEBUG - Predicted sequence: [0 0 0 0 0 0 0 0 2 1 0 0 1 0 0 0 0 0 0 0]
 2017-11-08 06:59:24,299 - INFO - Current epoch: 1
 2017-11-08 06:59:24,299 - INFO - Current training step: 2000
 2017-11-08 06:59:24,299 - INFO - Current loss: 107.67147064208984
 2017-11-08 06:59:24,299 - INFO - Current accuracy: 0.1599999964237213


 2017-11-08 07:04:09,559 - DEBUG - Targets: [  91    4    9  116  237 1953  240    3    2    1    0    2    1    9  144 351   29  299   24  453]
 2017-11-08 07:04:09,560 - DEBUG - Predicted sequence: [ 4  4  6  6 16  0  0  3  2  1  9  2  1  0  0  4  0  0  4  8]
 2017-11-08 07:04:09,560 - INFO - Current epoch: 5
 2017-11-08 07:04:09,560 - INFO - Current training step: 2000
 2017-11-08 07:04:09,560 - INFO - Current loss: 97.8116455078125
 2017-11-08 07:04:09,560 - INFO - Current accuracy: 0.2150000035762787


2017-11-08 07:43:03,875 - DEBUG - Targets: [  91    4    9  116  237 1953  240    3    2    1    0    2    1    9  144 351   29  299   24  453]
 2017-11-08 07:43:03,875 - DEBUG - Predicted sequence: [ 6  4  9 55 47  0  5  3  2  1  9  2  1  0 55 24  0  0  3  6]
 2017-11-08 07:43:03,876 - INFO - Current epoch: 30
 2017-11-08 07:43:03,876 - INFO - Current training step: 2000
 2017-11-08 07:43:03,876 - INFO - Current loss: 84.75357055664062
 2017-11-08 07:43:03,876 - INFO - Current accuracy: 0.2549999952316284
</code></pre>

<p>I have been working on this for a week already and I don't know what I can try out anymore. I would be super grateful for any tips or ideas.</p>

<p>The important parts of the code are here:</p>

<pre><code>    def build_graph(self, graph):
    with graph.as_default():
        tf.set_random_seed(self.random_seed)

        with tf.variable_scope('embedding'):
            embedding_matrix = tf.get_variable(name='embedding_matrix', shape=self.embds.shape, initializer=tf.constant_initializer(self.embds), trainable=False)

        with tf.name_scope('input'):
            self.input_batch = tf.placeholder(tf.int64, shape=(None, self.seq_length))
            self.inputs = tf.nn.embedding_lookup(embedding_matrix, self.input_batch)
            self.label_batch = tf.placeholder(tf.int64, shape=(None, self.seq_length))

        with tf.name_scope('rnn'):
            # Set up the RNN architecture
            cells = []

            for i in range(self.n_layers):
                cell = tf.contrib.rnn.LSTMCell(self.n_hidden, initializer=tf.contrib.layers.xavier_initializer())#use_peepholes=True,

                # Add dropout (only used during training)
                # cell = tf.contrib.rnn.DropoutWrapper(
                #     cell,
                #     output_keep_prob=(1.0 if not self.config['train'] else
                #                       self.dropout_keep_prob))
                cells.append(cell)


            cell = tf.contrib.rnn.MultiRNNCell(
                cells, state_is_tuple=True)

            # Create a zero-filled state tensor as an initial state
            self.init_state = cell.zero_state(self.batch_size, tf.float32)

            # Create a recurrent neural network
            output, self.final_state = tf.nn.dynamic_rnn(
                cell,
                inputs=self.inputs,
                initial_state=self.init_state)

            # OLD VERSION
            # self.logits = tf.contrib.layers.fully_connected(outputs, self.vocab_size, activation_fn=None)

            # NEW VERSION
            # Try out part of tensorflow tutorial

            self.output_flat = tf.reshape(output, [-1, cell.output_size])
            softmax_w = tf.get_variable(""softmax_w"", [self.n_hidden, self.vocab_size], dtype=tf.float32)

            softmax_b = tf.get_variable(""softmax_b"", [self.vocab_size], dtype=tf.float32)
            logits = tf.nn.xw_plus_b(self.output_flat, softmax_w, softmax_b)
            # Reshape logits to be a 3-D tensor for sequence loss
            self.logits = tf.reshape(logits, [self.batch_size, self.seq_length, self.vocab_size])

            # Use the contrib sequence loss and average over the batches
            loss = tf.contrib.seq2seq.sequence_loss(
                self.logits,
                self.label_batch,
                tf.ones([self.batch_size, self.seq_length], dtype=tf.float32),
                average_across_timesteps=False, average_across_batch=True)

            self.loss = tf.reduce_sum(loss)


        with tf.name_scope('prediction'):

            # Compute real-valued predictions of the network
            self.predictions = tf.argmax(self.logits, axis=2)

            # Compute the softmax                
            # softmax_ce = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.label_batch, logits=self.logits)

        #with tf.name_scope(""loss""):
            # Compute the loss (cross-entropy)
            # self.loss = tf.reduce_mean(softmax_ce)

        with tf.name_scope(""metrics""):
            # Compute accuracy and perplexity for evaluation

            correct_predictions = tf.to_float(tf.equal(self.label_batch, self.predictions))

            self.perplexity = tf.reduce_mean(tf.exp(softmax_ce))
            self.accuracy = tf.reduce_mean(correct_predictions)

        with tf.name_scope('train'):
            # Create a global step variable
            self.global_step = tf.Variable(
                0,
                trainable=False,
                name=""global_step"",
                collections=[ tf.GraphKeys.GLOBAL_STEP, tf.GraphKeys.GLOBAL_VARIABLES ])

            # Get all variables created with trainable=True
            parameters = tf.trainable_variables()
            # Compute the gradient of the loss w.r.t to the params
            gradients = tf.gradients(self.loss, parameters)
            # Clip the gradients. How this works: Given a tensor t, and a maximum
            # clip value clip_norm the op normalizes t so that its L2-norm is less
            # than or equal to clip_norm
            clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.clip_norm)

            self.optimizer =  tf.train.AdamOptimizer(learning_rate=self.lr, epsilon=0.1)
            # Apply the optimizer              
            self.train_step = self.optimizer.apply_gradients(zip(clipped_gradients, parameters), global_step=self.global_step)

            # If not clipping the gradients, minimize the loss directly
            # self.train_step = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)
            # self.train_step = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss, global_step=self.global_step)

        self._create_summaries()

    return graph


def train(self, save_every=20):
    with self.graph.as_default():

        # Initialize the state of the network
        feed2 = np.zeros((self.batch_size, self.n_hidden))
        t = tuple((feed2, feed2))
        _current_state = np.array([t, t])
        training_step = 0

        for epoch_id in range(0, self.n_epochs):     
            m, n = self.x_train.shape
            self.n_batches = int(m//self.batch_size)

            for batch_number in range(0, self.n_batches):
                training_step += 1
                from_index = batch_number*self.batch_size
                to_index = (batch_number+1)*self.batch_size
                _inputs = self.x_train[from_index:to_index,:]
                _labels = self.y_train[from_index:to_index,:]

                # Run training step
                # The final state of the net is fed back into the net 
                _logits, _predictions, _train_step, _current_state, _loss, _acc, summary = self.sess.run(
                        [self.logits,
                        self.predictions,
                        self.train_step,
                        self.final_state,
                        self.loss,
                        self.accuracy,
                        #self.perplexity,
                        self.merged],
                        feed_dict={
                            self.input_batch: _inputs,
                            self.label_batch: _labels,
                            self.init_state[0][0]: _current_state[0][0],
                            self.init_state[0][1]: _current_state[0][1],
                            self.init_state[1][0]: _current_state[1][0],
                            self.init_state[1][1]: _current_state[1][1],
                           })

                pred = _predictions[0]

                if batch_number % 2000 == 0:
                    self.sw.add_summary(summary, training_step)
                    tf.logging.debug(""Targets: {}"".format(_labels[0]))
                    tf.logging.debug(""Predicted sequence: {}"".format(pred))
                    tf.logging.info(""Current epoch: {}"".format(epoch_id))
                    tf.logging.info(""Current training step: {}"".format(batch_number))
                    tf.logging.info(""Current loss: {}"".format(_loss))
                    tf.logging.info(""Current accuracy: {}"".format(_acc))
                    tf.logging.info(""Current perplexity: {}"".format(_perpl))

            self.save(epoch_id)
</code></pre>
","language-model"
"46889727","word2vec - what is best? add, concatenate or average word vectors?","2017-10-23 12:44:40","48320687","21","19385","<python><word2vec><gensim><word-embedding><language-model>","<p>I am working on a recurrent language model. To learn word embeddings that can be used to initialize my language model, I am using gensim's word2vec model. 
After training, the word2vec model holds two vectors for each word in the vocabulary: the word embedding (rows of input/hidden matrix) and the context embedding (columns of hidden/output matrix).</p>

<p>As outlined in <a href=""https://stackoverflow.com/questions/36731784/wordvectors-how-to-concatenate-word-vectors-to-form-sentence-vector"">this post</a> there are at least three common ways to combine these two embedding vectors:</p>

<ol>
<li>summing the context and word vector for each word</li>
<li>summing &amp; averaging</li>
<li>concatenating the context and word vector</li>
</ol>

<p>However, I couldn't find proper papers or reports on the best strategy. So my questions are:</p>

<ol>
<li>Is there a common solution whether to sum, average or concatenate the vectors?</li>
<li>Or does the best way depend entirely on the task in question? If so, what strategy is best for a word-level language model?</li>
<li>Why combine the vectors at all? Why not use the ""original"" word embeddings for each word, i.e. those contained in the weight matrix between input and hidden neurons.</li>
</ol>

<p>Related (but unanswered) questions: </p>

<ul>
<li><a href=""https://stackoverflow.com/questions/42119824/word2vec-summing-concatenate-inside-and-outside-vector?rq=1"">word2vec: Summing/concatenate inside and outside vector</a> </li>
<li><a href=""https://stackoverflow.com/questions/46065773/why-we-use-input-hidden-weight-matrix-to-be-the-word-vectors-instead-of-hidden-o?rq=1"">why we use input-hidden weight matrix to be the word vectors instead of hidden-output weight matrix?</a></li>
</ul>
","language-model"
"46712938","How does language model evaluation work with unknown words?","2017-10-12 15:03:12","","0","1027","<language-model><perplexity>","<p>So for building language models, less frequent words ranked beyond vocabulary size are replaced as 'UNK'. </p>

<p>My question is, how to evaluate such language models that evaluates probabilities based on 'UNK'? Say we want to evaluate the perplexity of such a language model on a test set, for words unknown to the model, the probability we get is evaluated based on a 'bag' of unknown words. </p>

<p>This seems problematic because if we set the vocabulary size as 1, i.e. all words are unknown, then the perplexity of this can-do-nothing language model is going to be 1.</p>
","language-model"
"46474426","Assertion `THIndexTensor_(size)(target, 0) == batch_size' failed","2017-09-28 16:49:44","","0","697","<python><machine-learning><torch><pytorch><language-model>","<p>It occured when I writing Python code with PyTorch. I am trying to construct a simple language model using CrossEntropyLoss, but this error came up. Actually, I wrote this piece of code according to <a href=""http://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html"" rel=""nofollow noreferrer"">this</a>. Here is my code.</p>

<pre><code>import numpy as np
import torch
from torch.autograd import Variable
import torch.nn as nn

data = '...'
words = list(set(data))
word2ind = {word: i for i, word in enumerate(words)}
ind2word = {i: word for i, word in enumerate(words)}

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.in2h = nn.Linear(input_size-1+hidden_size, hidden_size)
        self.in2o = nn.Linear(input_size-1+hidden_size, output_size)
        self.o2o = nn.Linear(hidden_size+output_size, output_size)
        self.softmax = nn.LogSoftmax()

    def forward(self, inputs, hidden):
        input_combined = torch.cat((inputs.float(), hidden.float()), 1)
        print(type(input_combined.data))
        hidden = self.in2h(input_combined)
        output = self.in2o(input_combined)
        output_combined = torch.cat((hidden, output), 1)
        output = self.o2o(output_combined)
        output = self.softmax(output)
        print(output)
        return output, hidden

    def init_hidden(self):
        return Variable(torch.from_numpy(np.zeros((1, self.hidden_size))).type(torch.LongTensor))

def form_onehot(sent):
    one_hot = np.zeros((len(data), len(words)), dtype=np.int64)
    for i, word in enumerate(sent):
        one_hot[i, word2ind[word]] = 1
    return torch.LongTensor(one_hot)

def random_choice(vec):
    return np.random.choice(range(len(words)), p=vec)

def train(rnn, learning_rate, optimizer, criterion, input_tensor, target_tensor):
    hidden = rnn.init_hidden()
    optimizer.zero_grad()
    for i in range(input_tensor.size(1)):
        output, hidden = rnn(input_tensor[i, :].unsqueeze(0), hidden)
        loss = criterion(output, target_tensor[i])
        loss.backward()
        optimizer.step()
    return output, loss.data[0] / input_tensor.size()[0]

onehot_data = form_onehot(data)
rnn = RNN(len(words), 10, len(words))
learning_rate = 0.1
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)
input_tensor = Variable(onehot_data[:, :-1].type(torch.FloatTensor))
print(type(input_tensor.data))
target_tensor = Variable(onehot_data[:, 1:])
int_target_tensor = Variable(onehot_data[1:, :].type(torch.LongTensor))
output, loss = train(rnn, learning_rate, optimizer, criterion, input_tensor, int_target_tensor)
</code></pre>

<p>Here is the error:</p>

<pre><code>    ---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
&lt;ipython-input-449-8abc91b616c7&gt; in &lt;module&gt;()
----&gt; 1 output, loss = train(rnn, learning_rate, optimizer, criterion, input_tensor, int_target_tensor)

&lt;ipython-input-445-72363097fc21&gt; in train(rnn, learning_rate, optimizer, criterion, input_tensor, target_tensor)
     52         output, hidden = rnn(input_tensor[i, :].unsqueeze(0), hidden)
     53         print(output.size(), target_tensor[i].size())
---&gt; 54         loss = criterion(output, target_tensor[i])
     55         print('aaaaaaaaaaa')
     56         loss.backward()

D:\Anaconda3\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    204 
    205     def __call__(self, *input, **kwargs):
--&gt; 206         result = self.forward(*input, **kwargs)
    207         for hook in self._forward_hooks.values():
    208             hook_result = hook(self, input, result)

D:\Anaconda3\lib\site-packages\torch\nn\modules\loss.py in forward(self, input, target)
    319         _assert_no_grad(target)
    320         return F.cross_entropy(input, target,
--&gt; 321                                self.weight, self.size_average)
    322 
    323 

D:\Anaconda3\lib\site-packages\torch\nn\functional.py in cross_entropy(input, target, weight, size_average)
    535                 for each minibatch.
    536     """"""
--&gt; 537     return nll_loss(log_softmax(input), target, weight, size_average)
    538 
    539 

D:\Anaconda3\lib\site-packages\torch\nn\functional.py in nll_loss(input, target, weight, size_average)
    503     else:
    504         raise ValueError('Expected 2 or 4 dimensions (got {})'.format(dim))
--&gt; 505     return f(input, target)
    506 
    507 

D:\Anaconda3\lib\site-packages\torch\nn\_functions\thnn\auto.py in forward(self, input, target)
     39         output = input.new(1)
     40         getattr(self._backend, update_output.name)(self._backend.library_state, input, target,
---&gt; 41                                                    output, *self.additional_args)
     42         return output
     43 

RuntimeError: Assertion `THIndexTensor_(size)(target, 0) == batch_size' failed.  at d:\downloads\pytorch-master-1\torch\lib\thnn\generic/ClassNLLCriterion.c:50
</code></pre>

<p>I am really stuck here now, so any reply will be appreciated.</p>
","language-model"
"46328518","Correct way to calculate probabilities using ARPA LM data","2017-09-20 17:38:58","47548406","4","795","<nlp><n-gram><language-model>","<p>I am writing a small library for calculating ngram probabilities.</p>

<p>I have a LM described by arpa file (its a quite simple format: probability ngram backoff_weight):</p>

<pre><code>...
-5.1090264  Hello   -0.05108307
-5.1090264  Bob -0.05108307
-3.748848   we -0.38330063
...
-2.5558481  Hello Bob   -0.012590006
...
-1.953679   Hello Bob how   -0.0022290824
...
-0.58411354 Hello Bob how are   -0.0007929117
...
-1.4516809  Hello Bob how are you
...
</code></pre>

<p>But how do I calculate <code>P(we|Hello Bob how are)</code> here correctly?</p>

<pre><code>P(we|Hello Bob how are) = P(we) * BWt(Hello Bob how are) ?
</code></pre>

<p>or is this the right way:</p>

<pre><code>P(we|Hello Bob how are) = P(are we) * BWt(Hello Bob how) ?
</code></pre>

<p>what if I don't have backoff weight for the 4-gram <code>(Hello Bob how are)</code> ?</p>

<p>Please point me to some universal formula for calculating the probabilities or where can I read it, I really can't find anything good somehow...</p>
","language-model"
"46316068","What is the input to an RNN language model (TensorFlow)?","2017-09-20 07:41:36","","1","332","<python><tensorflow><sequence><recurrent-neural-network><language-model>","<p>I want to build a recurrent neural network (RNN) in TensorFlow that predicts the next word in a sequence of words. I have looked at several tutorials, e.g. the one of <a href=""https://www.tensorflow.org/tutorials/recurrent"" rel=""nofollow noreferrer"">TensorFlow</a>. I know that each word in the training text(s) is mapped to an integer index. However there are still a few things about the input that I don't get:</p>

<ol>
<li><p>Networks are trained with batches, e.g. with 128 examples at the same time. Let's say we have 10.000 words in our vocabulary. Is the input to the network a matrix of size (128, sequence_length) or a one-hot encoded tensor (128, sequence_length, 10.000)?</p></li>
<li><p>How large is the second dimension, i.e. the sequence length? Do I use one sentence in each row of the batch, padding the sentences that are shorter than others with zeros?</p></li>
<li><p>Or can a row correspond to multiple sentences? E.g. can a row stand for ""This is a test sentence. How are""? If so, where does the second sentence continue? In the next row of the same batch? Or in the same row in the next batch? How do I guarantee that TensorFlow continues the sentence correctly?</p></li>
</ol>

<p>I wasn't able to find answers to these questions even if they are quite simple. I hope someone can help!</p>
","language-model"
"46065514","Getting probability of the text given word embedding model in gensim word2vec model","2017-09-06 01:35:34","","1","948","<python><nlp><gensim><word2vec><language-model>","<p>I am trying to get most probable sequence of word using gensim word2vec model. I have found a pretrained model which provides these files:</p>

<pre><code>word2vec.bin
word2vec.bin.syn0.npy
word2vec.bin.syn1neg.npy
</code></pre>

<p>This is my code trying to get the probability of the sentence with this model:</p>

<pre><code>model = model.wv.load(word_embedding_model_path)
model.hs = 1
model.negative = 0
print model.score(sentence.split("" ""))
</code></pre>

<p>While running this code I am getting this error:</p>

<pre><code>AttributeError: 'Word2Vec' object has no attribute 'syn1'
</code></pre>

<p>Can anyone help me figure out how to solve the problem. In general, I want to use some pretrained model to get the probability of sequence of word appearing together.</p>
","language-model"
"45612636","character level bidirectional language model in tensorflow","2017-08-10 11:35:44","49400525","0","607","<tensorflow><language-model>","<p>Inspired from Andrej Karpathy Char-RNN, There is a Tensorflow implementation of char-rnn <a href=""https://github.com/sherjilozair/char-rnn-tensorflow"" rel=""nofollow noreferrer"">sherjilozair/char-rnn-tensorflow: Multi-layer Recurrent Neural Networks (LSTM, RNN) for character-level language models in Python using Tensorflow</a>. I want to implement bidirectional character level language model from this code. I change the <a href=""https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/model.py"" rel=""nofollow noreferrer"">model.py</a> and wrote a simple code:</p>

<pre><code>class Model:
def __init__(self, input_data, targets, seq_length=Config.max_seq_length, training=True):
    if Config.model == 'rnn':
        cell_fn = rnn.BasicRNNCell
    elif Config.model == 'gru':
        cell_fn = rnn.GRUCell
    elif Config.model == 'lstm':
        cell_fn = rnn.BasicLSTMCell
    elif Config.model == 'nas':
        cell_fn = rnn.NASCell
    else:
        raise Exception(""model type not supported: {}"".format(Config.model))

    fw_cells = []
    bw_cells = []
    for _ in range(Config.num_layers):
        fw_cell = cell_fn(Config.rnn_size)
        bw_cell = cell_fn(Config.rnn_size)
        fw_cells.append(fw_cell)
        bw_cells.append(bw_cell)

    self.fw_cell = rnn.MultiRNNCell(fw_cells, state_is_tuple=True)
    self.bw_cell = rnn.MultiRNNCell(bw_cells, state_is_tuple=True)

    self.input_data, self.targets = input_data, targets

    with tf.variable_scope('rnnlm'):
        softmax_w = tf.get_variable(""softmax_w"", [Config.rnn_size*2, Config.vocab_size])
        softmax_b = tf.get_variable(""softmax_b"", [Config.vocab_size])

    embedding = tf.get_variable(""embedding"", [Config.vocab_size, Config.rnn_size])
    inputs = tf.nn.embedding_lookup(embedding, self.input_data)

    inputs = tf.unstack(inputs, num=seq_length, axis=1)

    outputs, _, _ = tf.nn.static_bidirectional_rnn(self.fw_cell, self.bw_cell, inputs,
                                                   dtype=tf.float32, scope='rnnlm')
    output = tf.reshape(tf.concat(outputs, 1), [-1, Config.rnn_size*2])

    self.logits = tf.matmul(output, softmax_w) + softmax_b
    self.probs = tf.nn.softmax(self.logits)

    self.lr = tf.Variable(0.0, trainable=False)

    if training:
        loss = legacy_seq2seq.sequence_loss_by_example(
                [self.logits],
                [tf.reshape(self.targets, [-1])],
                [tf.sign(tf.cast(tf.reshape(self.targets, [-1]), dtype=tf.float32))])
        with tf.name_scope('cost'):
            self.cost = tf.reduce_mean(loss)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), Config.grad_clip)

        with tf.name_scope('optimizer'):
            optimizer = tf.train.AdamOptimizer(self.lr)
        self.train_op = optimizer.apply_gradients(zip(grads, tvars))
</code></pre>

<p>In training phase, I see a fast converge. After near 3000 iteration, the loss reach 0.003. In test phase, the probability of all character is 1.0. I think there is a mistake.
I will so glad to get some help to find my mistake.</p>
","language-model"
"45086510","Perplexity calculation for Language Model on 1 Billion Word Language Model Benchmark","2017-07-13 16:36:26","","2","731","<neural-network><deep-learning><lstm><recurrent-neural-network><language-model>","<p>Recently, I have been trying to implement RNNLM based on this <a href=""https://arxiv.org/pdf/1602.02410.pdf"" rel=""nofollow noreferrer"">article</a>. 
There is an <a href=""https://github.com/okuchaiev/f-lm"" rel=""nofollow noreferrer"">implementation</a> with some LSTM factorization tricks, but similar to the original implementation by the author.</p>

<h1><em>Preambula</em></h1>

<p>1) The dataset is split into files and then lines of the file are being shuffled at the train time and being feed sequentially at test time. (<a href=""https://github.com/okuchaiev/f-lm/blob/master/data_utils.py#L79"" rel=""nofollow noreferrer"">link</a>):
</p>

<pre><code># deterministic at test time, non deterministic at train time
if not self._deterministic:
    random.shuffle(lines)
</code></pre>

<p>2) The batches is formed continuously</p>

<p><img src=""https://i.sstatic.net/ep87H.png"" alt=""""></p>

<p>The * symbol represents start \ ending of the sentence. Each matrix represents one bach. <a href=""https://github.com/okuchaiev/f-lm/blob/master/data_utils.py#L102"" rel=""nofollow noreferrer"">The code link</a>. So:</p>

<ol>
<li><p>If sentence is longer then num_steps, it continues on the next batch at the same line.</p></li>
<li><p>If sentence is shorter, the batch line is being filled with another sentences. </p></li>
</ol>

<p>3) They calculate a batch mean loss. num_steps - the memory of the LSTM. <a href=""https://github.com/okuchaiev/f-lm/blob/master/language_model.py#L1490"" rel=""nofollow noreferrer"">Code</a>.
</p>

<pre><code># loss_shape = [batch_size * num_steps]
# 1D tensor, reshaped 2d tensor with dims of [batch_size, num_steps]
loss = tf.reduce_mean(loss)
</code></pre>

<p>4) The LSTM cell is being update after each training iteration and being zeroed out at evaluation time. </p>

<p>They declare it as local variables <a href=""https://github.com/okuchaiev/f-lm/blob/master/language_model.py#L71"" rel=""nofollow noreferrer"">declaration</a>.</p>

<p>And then it's being <a href=""https://github.com/okuchaiev/f-lm/blob/master/language_model.py#L126"" rel=""nofollow noreferrer"">updated</a> at the train time. And <a href=""https://github.com/okuchaiev/f-lm/blob/master/run_utils.py#L130"" rel=""nofollow noreferrer"">zeroed out</a> at eval time.</p>

<p>5) On the eval time the authors calculate the perplexity this way (<a href=""https://github.com/okuchaiev/f-lm/blob/master/run_utils.py#L139"" rel=""nofollow noreferrer"">the link</a>):
</p>

<pre><code>for i, (x, y) in enumerate(data_iterator):
    # get a batch
    loss = sess.run(model.loss, {model.x: x, model.y: y})
    loss_nom += loss
    loss_den += 1
    loss = loss_nom / loss_den
    sys.stdout.write(""%d: %.3f (%.3f) ... "" % (i, loss, np.exp(loss)))
    sys.stdout.flush()
    sys.stdout.write(""\n"")
</code></pre>

<p>It means they measure batch-average perplexity. </p>

<p>That being said, I have 2 main questions.</p>

<h1>Questions</h1>

<ol>
<li>Considering preamble 1), 2) and 4). </li>
</ol>

<h3>Why batches are being formed that way?</h3>

<p>The LSTM cell is not being zeroed out after each sentence, so it keeps memory of the previous sentence.</p>

<p>In the example at the top when neural net is processing batch №1 the line №2 for the word ""Half"" it remembers the context of word Music and start\end tokens. 
It could make sense if the sentences were not shuffled and it was the real text, but they <em>are</em> shuffled and not connected to each other.</p>

<p>I implemented both methods and infinite batches gave <em>much</em> better performance. </p>

<ol start=""2"">
<li>Considering preamble 3) and 5).</li>
</ol>

<h3>Why do we estimate batch-average perplexity?</h3>

<p>Taking in consideration first question, it's not clear to me that when we measure perplexity this way, we can really estimate how good our model is.
But sentence-average perplexity seems more efficient.</p>

<p>If there is a flaw in my logic, I'd be grateful if you point that out.</p>
","language-model"
"44946739","Use Tensorflow LSTM PTB example for scoring sentences","2017-07-06 10:44:31","45153343","0","395","<tensorflow><lstm><scoring><language-model>","<p>I try to use an example LSTM, trained according to <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py"" rel=""nofollow noreferrer"">Tensorflow LSTM example</a>. This example allows to get perplexity on whole test set. But I need to use the trained model to score (get loglikes) of each sentence separately (to score hypotheses of STT decoder output). I modified <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/reader.py"" rel=""nofollow noreferrer"">reader</a> a bit and used code:</p>

<pre><code>mtests=list()
with tf.name_scope(""Test""):        
    for test_data_item in test_data:
      test_input.append(PTBInput(config=eval_config, data=test_data_item, name=""TestInput""))   
    with tf.variable_scope(""Model"", reuse=True, initializer=initializer):
      for test_input_item in test_input:
        mtests.append(PTBModel(is_training=False, config=eval_config,
                     input_=test_input_item))
sv = tf.train.Supervisor(logdir=FLAGS.model_dir)

with sv.managed_session() as session:
  checkpoint=tf.train.latest_checkpoint(FLAGS.model_dir)      
  sv.saver.restore(session, checkpoint)
  sys.stderr.write(""model restored\n"") 

  for mtest in mtests:      
    score, test_perplexity = run_epoch_test(session, mtest)
    print(score)
</code></pre>

<p>So, using that code, I get score of each sentence independently. If I pass 5 sentences, it works ok. But if I pass 1k sentences to this code, it works extremely slow and uses a lot of memory, because I create 1k models mtest. So, could you tell me another way to reach my goal? Thank you.</p>
","language-model"
"44873156","How can the perplexity of a language model be between 0 and 1?","2017-07-02 16:56:18","44875134","0","617","<python><tensorflow><language-model><sequence-to-sequence><perplexity>","<p>In Tensorflow, I'm getting outputs like 0.602129 or 0.663941. It appears that values closer to 0 imply a better model, but it seems like perplexity is supposed to be calculated as 2^loss, which implies that loss is negative. This doesn't make any sense.</p>
","language-model"
"44586333","Understanding Character Level Embedding in Keras LSTM","2017-06-16 09:57:48","44601817","5","4917","<python><keras><lstm><embedding><language-model>","<p>I am a newbie in implementation of language models in Keras RNN structures. I have a dataset of discrete words (not from a single paragraph) that have the following statistics,</p>

<ol>
<li>Total word samples: 1953</li>
<li>Total number of Distinct Characters: 33 (including START,END and *)</li>
<li>Maximum length (number of characters) in a word is 10</li>
</ol>

<p>Now, I want to build a model that will accept a character and predict the next character in the word. I have padded all the words so that they have same length. So my input is Word_input with shape <strong>1953 x 9</strong> and target is <strong>1953 x 9 x 33</strong>. I also want to use Embedding layer. So my network architecture is,</p>

<pre><code>    self.wordmodel=Sequential()
    self.wordmodel.add(Embedding(33,embedding_size,input_length=9))
    self.wordmodel.add(LSTM(128, return_sequences=True))
    self.wordmodel.add(TimeDistributed(Dense(33)))
    self.wordmodel.compile(loss='mse',optimizer='rmsprop',metrics=['accuracy'])
</code></pre>

<p>As an example a word ""CAT"" with padding represents</p>

<p>Input to Network -- START C A T END * * * * (9 Characters)</p>

<p>Target of the same --- C A T END * * * * *(9 Characters)</p>

<p>So with the <code>TimeDistributed</code> output I am measuring the difference of network prediction and target. I have also set the <code>batch_size</code> to 1. So that after reading every sample word the network reset its state. </p>

<p>My question is am I doing it conceptually right? Whenever I am running my training the accuracy is stuck about 56%.</p>

<p>Kindly enlighten me. Thanks.</p>
","language-model"
"44417390","How to use n-grams for multi-label classification?","2017-06-07 15:48:58","","0","988","<nlp><classification><n-gram><language-model>","<p>I am working on a project to identify the nationality of the person based on the manner in which they communicate in English. I have 10 nationalities &amp; 1000 files, 100 for each nationality. I am using n-grams as features &amp; looking to validate my approach. My data structure, would have different n-grams (character n-grams, bi-grams, as columns) as features &amp; nationality as labels (as rows) My Steps are : </p>

<ol>
<li>Pick up all files for a given nationality (FINE). 
1.1. Combine them together to form a text corpus (e.g Combine 100 files into 1 big file). I initially thought for taking one file at a time &amp; updating the count of n-grams based on their occurance. But then for each new n-gram, i would have to do a look up to see if it has already occured &amp; then update its frequency, for a given label. Would this be a better approach ?</li>
<li>Extract bi-grams / tri-grams &amp; get frequency of each gram. (NLTK has FreqDist which gives count for each)</li>
<li>Store this information, so I would use it to classify my test set. (HOW DO I STORE THIS INFORMATION. MORE INFORMATION BELOW) </li>
</ol>

<p>Question is should I store the n-gram &amp; frequency information in a matrix (a single matrix having all n-grams &amp; labels OR separate matrix for each label) or should I store it as a map (a map for each label, having n-gram &amp; its frequency count). I would like to have a data structure (for storing the extracted information) that would be easy for classifiers to take as input &amp; process them. I cannot foresee, which data structure would be a better option. </p>
","language-model"
"44274199","Character-Word Embeddings from lm_1b in Keras","2017-05-31 01:19:49","","18","1697","<machine-learning><nlp><keras><language-model><word-embedding>","<p>I would like to use some pre-trained word embeddings in a Keras NN model, which have been published by Google in a <a href=""https://arxiv.org/pdf/1602.02410.pdf"" rel=""noreferrer"">very well known article</a>.   They have provided the code to train a new model, as well as the embeddings <a href=""https://github.com/tensorflow/models/tree/master/lm_1b"" rel=""noreferrer"">here</a>.</p>

<p>However, it is not clear from the documentation how to retrieve an embedding vector from a given string of characters (word) from a simple python function call.  Much of the documentation seems to center on dumping vectors to a <em>file</em> for an entire sentence presumably for sentimental analysis.  </p>

<p>So far, I have seen that you can feed in pretrained embeddings with the following syntax:</p>

<pre><code>embedding_layer = Embedding(number_of_words??,
                            out_dim=128??,
                            weights=[pre_trained_matrix_here],
                            input_length=60??,
                            trainable=False)
</code></pre>

<p>However, converting the different files and their structures to <code>pre_trained_matrix_here</code> is not quite clear to me.</p>

<p>They have several softmax outputs, so I am uncertain which one would belong - and furthermore how to align the words in my input to the dictionary of words for which they have.</p>

<p>Is there a simple manner to use these word/char embeddings in keras and/or to construct the character/word embedding portion of the model in keras such that further layers may be added for other NLP tasks?</p>
","language-model"
"44211206","Keep a language model loaded through multiple program runs","2017-05-26 23:03:46","","0","62","<python><bash><language-model>","<p>I am loaded a large <code>word2vec</code> language model in Python. Each time I run the program, I need to load the model into memory.</p>

<p>I'm running the same program with different command line arguments from a shell script, e.g.</p>

<pre><code>#!/bin/bash
python processor.py -ad
python processor.py -td
python processor.py -ds
</code></pre>

<p>Is there anything I can do to keep the language model in memory after the program finishes running, or will I just need to modify the python code itself to loop through the different iterations after the model is loaded?</p>
","language-model"
"43841467","How to compute perplexity using KenLM?","2017-05-08 06:52:48","44105678","5","7755","<python><nlp><language-model><kenlm><perplexity>","<p>Let's say we build a model on this:</p>

<pre><code>$ wget https://gist.githubusercontent.com/alvations/1c1b388456dc3760ffb487ce950712ac/raw/86cdf7de279a2b9bceeb3adb481e42691d12fbba/something.txt
$ lmplz -o 5 &lt; something.txt &gt; something.arpa
</code></pre>

<p>From the perplexity formula (<a href=""https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf"" rel=""noreferrer"">https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf</a>) </p>

<p>Applying the sum of inverse log formula to get the inner variable and then taking the nth root, the perplexity number is unusually small:</p>

<pre><code>&gt;&gt;&gt; import kenlm
&gt;&gt;&gt; m = kenlm.Model('something.arpa')

# Sentence seen in data.
&gt;&gt;&gt; s = 'The development of a forward-looking and comprehensive European migration policy,'
&gt;&gt;&gt; list(m.full_scores(s))
[(-0.8502398729324341, 2, False), (-3.0185394287109375, 3, False), (-0.3004383146762848, 4, False), (-1.0249041318893433, 5, False), (-0.6545327305793762, 5, False), (-0.29304179549217224, 5, False), (-0.4497605562210083, 5, False), (-0.49850910902023315, 5, False), (-0.3856896460056305, 5, False), (-0.3572353720664978, 5, False), (-1.7523181438446045, 1, False)]
&gt;&gt;&gt; n = len(s.split())
&gt;&gt;&gt; sum_inv_logs = -1 * sum(score for score, _, _ in m.full_scores(s))
&gt;&gt;&gt; math.pow(sum_inv_logs, 1.0/n)
1.2536033936438895
</code></pre>

<p>Trying again with a sentence not found in the data:</p>

<pre><code># Sentence not seen in data.
&gt;&gt;&gt; s = 'The European developement of a forward-looking and comphrensive society is doh.'
&gt;&gt;&gt; sum_inv_logs = -1 * sum(score for score, _, _ in m.full_scores(s))
&gt;&gt;&gt; sum_inv_logs
35.59524390101433
&gt;&gt;&gt; n = len(s.split())
&gt;&gt;&gt; math.pow(sum_inv_logs, 1.0/n)
1.383679905428275
</code></pre>

<p>And trying again with totally out of domain data:</p>

<pre><code>&gt;&gt;&gt; s = """"""On the evening of 5 May 2017, just before the French Presidential Election on 7 May, it was reported that nine gigabytes of Macron's campaign emails had been anonymously posted to Pastebin, a document-sharing site. In a statement on the same evening, Macron's political movement, En Marche!, said: ""The En Marche! Movement has been the victim of a massive and co-ordinated hack this evening which has given rise to the diffusion on social media of various internal information""""""
&gt;&gt;&gt; sum_inv_logs = -1 * sum(score for score, _, _ in m.full_scores(s))
&gt;&gt;&gt; sum_inv_logs
282.61719834804535
&gt;&gt;&gt; n = len(list(m.full_scores(s)))
&gt;&gt;&gt; n
79
&gt;&gt;&gt; math.pow(sum_inv_logs, 1.0/n)
1.0740582373271952
</code></pre>

<p>Although, it is expected that the longer sentence has lower perplexity, it's strange that the difference is less than 1.0 and in the range of decimals. </p>

<p><strong>Is the above the right way to compute perplexity with KenLM? If not, does anyone know how to computer perplexity with the KenLM through the Python API?</strong></p>
","language-model"
"43777326","Tensorflow RNN: Perplexity per Epoch remains constant","2017-05-04 08:11:48","","1","461","<tensorflow><neural-network><nlp><recurrent-neural-network><language-model>","<p>I am training an RNN-based language-model using Tensorflow. The model is very similar to the PTB model example in the TF tutorials section. However, when I attempt to train the model on my own data, the perplexity of the model does not go down; it remains constant throughout multiple epochs. Could anyone let me know what I might be doing wrong.</p>

<p>I have a feeling that I am not handling the targets properly, but the gist of my code for the targets is:</p>

<pre><code>def batcher(batch_size,unroll_steps,data,pad):
    print(len(data))
    batches = len(data) / batch_size
    inp = []
    target = []
    for i in range(batches):
            #print(len(data[i*batch_size:(i+1)*batch_size]))
            x = data[i*batch_size:(i+1)*batch_size]
            y =  [ line[1:]+[pad] for line in x ]
            yield (x,y)
</code></pre>

<p>That is, I just shift the data by 1 and use that as the target for the next word in a sentence.</p>

<p>The training script and model (class) are seen below</p>

<p>Training script (excerpt):</p>

<pre><code>def train(session, model, folder,batch_size,unroll_steps,epoch):

    word_to_id, id_to_word, train, val = build_inputs(folder,unroll_steps)
    pad = word_to_id['&lt;pad&gt;']
    costs = 0
    iters = 0
    train_size = len(train)
    batch_size = model.batch_size
    batches = train_size / batch_size
    state = session.run(model._initial_state)
    print(""Running epoch %d"" % epoch)
    for i in range(batches):
            fetches = [model.cost, model._final_state, model.logits]
            feed_dict = {}
            x = train[i*batch_size:(i+1)*batch_size]
            y = [ line[1:] +[pad] for line in x ]
            feed_dict[model.input] = x
            feed_dict[model.targets] = y
            feed_dict[model._initial_state] = state
            #print(""Cell-state complete - Running"")
            cost, state, logits = session.run(fetches, feed_dict)
            #print(""Single Run complete"")
            costs += cost
            iters += model.unroll_steps
    print(""\tEpoch %d: Perplexity is %f"" % (epoch, np.exp(costs/iters)))

    return np.exp(costs/iters)
</code></pre>

<p>Model:</p>

<pre><code>import tensorflow as tf

class LM(object):

    def __init__(self, train, max_gradient, batch_size, unroll_steps, vocab, size, layers, learning_rate, init, prob):
            self.batch_size = batch_size
            self.max_gradient = max_gradient
            self.layers = layers
            self.learning_rate = learning_rate
            self.unroll_steps = unroll_steps
            self.init = init
            #with tf. name_scope(""Paramters""):

            with tf.device('/gpu:0'), tf.name_scope(""Input""):
                    self.input = tf.placeholder(tf.int64, shape=[batch_size, unroll_steps], name=""input"")
                    self.targets = tf.placeholder(tf.int64, shape=[batch_size, unroll_steps], name=""targets"")
                    #self.init = tf.placeholder(tf.float32, shape=[], name=""init"")

            with tf.device('/gpu:0'), tf.name_scope(""Embedding""):
                    embedding = tf.Variable(tf.random_uniform([vocab, size], -self.init, self.init), dtype=tf.float32, name=""embedding"")
                    embedded_input = tf.nn.embedding_lookup(embedding, self.input, name=""embedded_input"")

            with tf.device('/gpu:0'), tf.name_scope(""RNN""), tf.variable_scope(tf.get_variable_scope(), reuse = False) as scope:
                    lstm_cell = tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)
                    if train and prob &lt; 1.0:
                            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=prob)
                    cell = tf.contrib.rnn.MultiRNNCell([lstm_cell for _ in range(layers)], state_is_tuple=True)

                    self._initial_state = cell.zero_state(batch_size, tf.float32)
                    outputs = []
                    state = self._initial_state
                    for step in range(unroll_steps):
                            if step &gt; 0: tf.get_variable_scope().reuse_variables()
                            (cell_output, state) = cell(embedded_input[:, step, :], state)
                            outputs.append(cell_output)

            with tf.device('/gpu:0'), tf.name_scope(""Cost""), tf.variable_scope(tf.get_variable_scope(), reuse = False) as scope:
                    output = tf.reshape(tf.concat(outputs,1), [-1,size])
                    softmax_w = tf.get_variable(""softmax_w"", [size, vocab], dtype=tf.float32)
                    softmax_b = tf.get_variable(""softmax_b"", [vocab], dtype=tf.float32)
                    logits = tf.matmul(output, softmax_w) + softmax_b
                    losses = []
                    for logit, target in zip([logits], [tf.reshape(self.targets,[-1])]):
                            target = tf.reshape(target, [-1])
                            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logit,labels=target)
                            losses.append(loss)
                    self.cost = tf.reduce_sum(losses) / batch_size
                    self._final_state = state
                    self.logits = logits
                    scope.reuse_variables()

            if not train:
                    return

            with tf.device('/gpu:0'), tf.name_scope(""Train""), tf.variable_scope(tf.get_variable_scope(), reuse=False):
                    train_variables = tf.trainable_variables()
                    gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, train_variables),self.max_gradient)
                    optimizer = tf.train.AdamOptimizer(self.learning_rate)
                    self.training = optimizer.apply_gradients(zip(gradients, train_variables))
                    tf.get_variable_scope().reuse_variables()
</code></pre>
","language-model"
"43646266","TensorFlow: loss jumps up after restoring RNN net","2017-04-26 23:36:03","43670684","2","1240","<tensorflow><recurrent-neural-network><language-model>","<h2>Environment info</h2>

<ul>
<li>Operating System: Windows 7 64-bit</li>
<li>Tensorflow installed from pre-built pip (no CUDA): 1.0.1</li>
<li>Python 3.5.2 64-bit</li>
</ul>

<h2>Problem</h2>

<p>I have problems with restoring my net (RNN character base language model). Below is a simplified version with the same problem.</p>

<p>When I run it the first time, I get, for example, this.</p>

<pre><code>    ...
    step 160: loss = 1.956 (perplexity = 7.069016620211226)
    step 180: loss = 1.837 (perplexity = 6.274748642468816)
    step 200: loss = 1.825 (perplexity = 6.202084762557817)
</code></pre>

<p>But on the second run, after restoring parameters, I get this.</p>

<pre><code>    step 220: loss = 2.346 (perplexity = 10.446611983898903)
    step 240: loss = 2.346 (perplexity = 10.446709120339545)
    ...
</code></pre>

<p>All the tf variables seem to be correctly restored, including the state, which will be fed to RNN.
Data position is also restored (from 'step'). </p>

<p>I also made a similar program for MNIST recognition model, and this one works fine: the losses before and after the restoring are continuous.</p>

<p>Are there any other parameters or states that should be saved and restored?</p>

<pre><code>    import argparse
    import os
    import tensorflow as tf
    import numpy as np
    import math

    B = 20  # batch size
    H = 200 # size of hidden layer of neurons
    T = 25  # number of time steps to unroll the RNN for
    data_file = 'ptb.train.txt' # any plain text file will do
    checkpoint_dir = ""tmp""

    #----------------
    # prepare data
    #----------------
    data = open(data_file, 'r').read()
    chars = list(set(data))
    data_size, vocab_size = len(data), len(chars)
    print('data has {0} characters, {1} unique.'.format(data_size, vocab_size))
    char_to_ix = { ch:i for i,ch in enumerate(chars) }
    ix_to_char = { i:ch for i,ch in enumerate(chars) }

    input_index_raw = np.array([char_to_ix[ch] for ch in data])
    input_index_raw = input_index_raw[0:len(input_index_raw) // T * T]
    input_index_raw_shift = np.append(input_index_raw[1:], input_index_raw[0])
    input_all = input_index_raw.reshape([-1, T])
    target_all = input_index_raw_shift.reshape([-1, T])
    num_packed_data = len(input_all)

    #----------------
    # build model
    #----------------
    class Model(object):
      def __init__(self):
        self.input_ph = tf.placeholder(tf.int32, [None, T], name=""input_ph"")
        self.target_ph = tf.placeholder(tf.int32, [None, T], name=""target_ph"")
        embedding = tf.get_variable(""embedding"", [vocab_size, H], initializer=tf.random_normal_initializer(), dtype=tf.float32)
        # input_ph is B x T.
        # input_embedded is B x T x H.
        input_embedded = tf.nn.embedding_lookup(embedding, self.input_ph)

        cell = tf.contrib.rnn.BasicRNNCell(H)

        self.state_ph = tf.placeholder(tf.float32, (None, cell.state_size), name=""state_ph"")

        # Make state variable so that it will be saved by the saver.
        self.state = tf.get_variable(""state"", (B, cell.state_size), initializer=tf.zeros_initializer(), trainable=False, dtype=tf.float32)

        # Construct initial_state according to whether restoring or not.
        self.isRestore = tf.placeholder(tf.bool, shape=(), name=""isRestore"")
        zero_state = cell.zero_state(B, dtype=tf.float32)
        self.initial_state = tf.cond(self.isRestore, lambda: self.state, lambda: zero_state)

        # input_embedded : B x T x H
        # output: B x T x H
        # state : B x cell.state_size
        output, state_ = tf.nn.dynamic_rnn(cell, input_embedded, initial_state=self.state_ph)
        self.final_state = tf.assign(self.state, state_)

        # reshape to (B * T) x H.
        output_flat = tf.reshape(output, [-1, H])

        # Convert hidden layer's output to vector of logits for each vocabulary.
        softmax_w = tf.get_variable(""softmax_w"", [H, vocab_size], dtype=tf.float32)
        softmax_b = tf.get_variable(""softmax_b"", [vocab_size], dtype=tf.float32)
        logits = tf.matmul(output_flat, softmax_w) + softmax_b

        # cross_entropy is a vector of length B * T
        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(self.target_ph, [-1]), logits=logits)
        self.loss = tf.reduce_mean(cross_entropy)

        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
        self.global_step = tf.get_variable(""global_step"", (), initializer=tf.zeros_initializer(), trainable=False, dtype=tf.int32)
        self.training_op = optimizer.minimize(cross_entropy, global_step=self.global_step)

      def train_batch(self, sess, input_batch, target_batch, initial_state):
        final_state_, _, final_loss = sess.run([self.final_state, self.training_op, self.loss], feed_dict={self.input_ph: input_batch, self.target_ph: target_batch, self.state_ph: initial_state})
        return final_state_, final_loss

    # main
    with tf.Session() as sess:
      if not tf.gfile.Exists(checkpoint_dir):
        tf.gfile.MakeDirs(checkpoint_dir)

      batch_stride = num_packed_data // B

      # make model
      model = Model()
      saver = tf.train.Saver()

      # always initialize
      init = tf.global_variables_initializer()
      init.run()

      # restore if necessary
      isRestore = False
      ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
      if ckpt:
        isRestore = True
        last_model = ckpt.model_checkpoint_path
        print(""Loading "" + last_model)
        saver.restore(sess, last_model)

      # set initial step
      step = tf.train.global_step(sess, model.global_step) + 1
      print(""start step = {0}"".format(step))

      # fetch initial state
      state =  sess.run(model.initial_state, feed_dict={model.isRestore: isRestore})
      print(""Initial state: {0}"".format(state))

      while True:
        # prepare batch data
        idx = [(step + x * batch_stride) % num_packed_data for x in range(0, B)]
        input_batch = input_all[idx]
        target_batch = target_all[idx]

        state, last_loss = model.train_batch(sess, input_batch, target_batch, state)

        if step % 20 == 0:
          print('step {0}: loss = {1:.3f} (perplexity = {2})'.format(step, last_loss, math.exp(last_loss)))

        if step % 200 == 0:
          saved_file = saver.save(sess, os.path.join(checkpoint_dir, ""model.ckpt""), global_step=step)
          print(""Saved to "" + saved_file)
          print(""Last state: {0}"".format(model.state.eval()))
          break;

        step = step + 1
</code></pre>
","language-model"
"43466541","When loading KenLM language model for scoring sentences should the LM file size be less than RAM size?","2017-04-18 07:48:16","","2","514","<memory><nlp><language-model><kenlm>","<p>When loading language model for scoring sentence should the LM('bible.klm') filesize be less than RAM size?</p>

<pre><code>import kenlm

model = kenlm.LanguageModel('bible.klm')
model.score('in the beginning was the word')
</code></pre>
","language-model"
"43432901","Document Classification tool in C - Compilation error in nested function/scope (may be)","2017-04-16 01:44:20","","0","66","<c><machine-learning><information-retrieval><text-analysis><language-model>","<p><a href=""https://stackoverflow.com/questions/43423803/document-classification-tool-in-c-compilation-error/43432470#43432470"">https://stackoverflow.com/questions/43423803/document-classification-tool-in-c-compilation-error/43432470#43432470</a></p>

<p>In above link-
I get 3 errors after adding -fnested_functions as</p>

<p>gcc -c ./rainbow.c -fnested-functions process_wv.c test_file.c test_hdb_file.c</p>

<p>(The 3 files process_wv, test_file, test_hdb_file are removed from rainbow.c and added as seperate .c files to the directory now)</p>

<p>Output:-</p>

<pre><code>./bow/libbow.h:1345:8: note: forward declaration of 'struct argp_child'
struct argp_child;              /* forward declare this type */
       ^
./rainbow.c:655:5: error: function definition is not allowed here
    {
    ^
./rainbow.c:663:8: warning: extra tokens at end of #endif directive [-Wextra-tokens]
#endif VPC_ONLY
       ^
       //
./rainbow.c:734:3: warning: implicit declaration of function 'do_indexing' is invalid in C99
      [-Wimplicit-function-declaration]
  do_indexing ();
  ^
./rainbow.c:1175:49: warning: passing 'int *' to parameter of type 'socklen_t *' (aka 'unsigned int *') converts between
      pointers to integer types with different sign [-Wpointer-sign]
  newsockfd = accept(rainbow_sockfd, &amp;cli_addr, &amp;clilen);
                                                ^~~~~~~
/usr/include/sys/socket.h:681:69: note: passing argument to parameter here
int     accept(int, struct sockaddr * __restrict, socklen_t * __restrict)
                                                                        ^
./rainbow.c:1586:30: error: use of undeclared identifier 'test_file'
        bow_map_filenames_from_dir (test_file.c, 0, dir, """");


                             ^
</code></pre>

<p>P.S Why is test_file.c unidentified (bow_map_filenames_from_dir in docnames.c) in rainbow.c even though they are inside the same bow-20020213 folder (Permissions are 755 for all)</p>

<p>Regards</p>
","language-model"
"43423112","Statistical text analysis, language modeling and information retrieval program - Rainbow","2017-04-15 06:47:16","43423284","-1","170","<c><compiler-errors><classification><text-analysis><language-model>","<p>I am trying to use the library Rainbow (<a href=""http://www.cs.cmu.edu/~mccallum/bow/src/bow-20020213.tar.gz"" rel=""nofollow noreferrer"">http://www.cs.cmu.edu/~mccallum/bow/src/bow-20020213.tar.gz</a>) for a simple question however have not been able to compile the rainbow.c file.</p>

<p>Errors are</p>

<pre><code>error: expected ""FILENAME"" or &lt;FILENAME&gt;

#include “bow/libbow.h”

         ^

./rainbow.c:23:10: fatal error: 'argp.h' file not found

#include &lt;argp.h&gt;

         ^

2 errors generated.
</code></pre>

<p>Is there any way to avoid that to generate a working compiled file.</p>

<p>This is after -</p>

<p>*</p>

<pre><code>error: 'bow/libbow.h' file not found with &lt;angled&gt; include;
      use ""quotes"" instead
#include &lt;bow/libbow.h&gt;
         ^~~~~~~~~~~~~~
         ""bow/libbow.h""
In file included from ./rainbow.c:22:
./bow/libbow.h:40:10: fatal error: 'malloc.h' file not found
#include &lt;malloc.h&gt;             /* for malloc() and friends. */
         ^
</code></pre>

<p>*</p>

<p>Please kindly help by spending a few minutes if possible as I need that package for my chosen project question submission.</p>
","language-model"
"43359882","How to build Tensorflow speech recognition integrated with language model","2017-04-12 03:43:30","43363549","0","1400","<python><tensorflow><speech-recognition><language-model>","<p>How can I integrate a language model in a tensorflow speech recognition architecture? </p>

<p>There are a bunch of examples out there for building character level speech recognition in Tensorflow (e.g. <a href=""https://github.com/nervanasystems/neon"" rel=""nofollow noreferrer"">https://github.com/nervanasystems/neon</a>, <a href=""https://github.com/buriburisuri/speech-to-text-wavenet"" rel=""nofollow noreferrer"">https://github.com/buriburisuri/speech-to-text-wavenet</a>), which is interesting but practically useless, unless a language model is integrated. I couldn't find an example that uses a language model.</p>

<p>How can I integrate a language model?</p>
","language-model"
"42937324","Reason for eval_config setting parameters to 1 in ptb_word_lm.py","2017-03-21 20:15:47","","2","403","<tensorflow><neural-network><nlp><language-model>","<p>While examining the setting for evaluation in Tensorflow's PTB language model, I am perplexed by this setting for the evaluation in eval_config:</p>

<pre><code>  eval_config = get_config()
  eval_config.batch_size = 1
  eval_config.num_steps = 1
</code></pre>

<p>in <a href=""https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py</a></p>

<p>To the best of my understanding, during evaluation, a window (which could be upto num_steps size) of context words is used to predict the next word, which is stored in a separate target tensor. If num_steps is set to 1, wouldn't it imply that only the preceding word is used for prediction (ignoring a context window size>1) ? Also during evaluation, why is batch_size set to 1 as well. Wouldn't it make sense to have a larger batch being fed into the network, to speech up evaluation ?</p>
","language-model"
"42633713","Python - tool for word learn","2017-03-06 19:11:47","","-5","53","<python><cpu-word><language-model>","<p>Can I do something like this in python?
I have 3 sentences:</p>

<pre><code>I like cats.
I like cats
I like cats
</code></pre>

<p>And it is possible when I give <em>I like</em>, the script returns <em>cats</em>?</p>

<p>Can somebody give me advice/tool or some example?</p>
","language-model"
"42553671","Predicting a probability of a sentence using tensorflow","2017-03-02 11:04:47","","5","370","<python-2.7><tensorflow><recurrent-neural-network><language-model>","<p>I am using this pre-trained <a href=""https://github.com/tensorflow/models/blob/master/lm_1b/"" rel=""noreferrer"">model of tensorflow</a> and trying to get a probability of a sentence. My primary task is, out of several sentences find a sentence with the largest probability. </p>

<p>I am able to predict next words, using this code.</p>

<pre><code>bazel-bin/lm_1b/lm_1b_eval --mode sample \
                           --prefix ""I love that I"" \
                           --pbtxt data/graph-2016-09-10.pbtxt \
                           --vocab_file data/vocab-2016-09-10.txt  \
                           --ckpt 'data/ckpt-*'
</code></pre>

<p>However, how can I get a probability of a sentence from a list of many sentences? </p>
","language-model"
"41626315","Language Model for PocketSphinx","2017-01-13 02:07:04","","0","1961","<speech-recognition><speech-to-text><pocketsphinx><language-model>","<p>I've been working with PocketSphinx to make a speech recognizer for natural language. I don't want to use a grammar but rather a language model.
Is it possible that I can't find anything already-done online?
Everybody is linking <a href=""https://sourceforge.net/projects/cmusphinx/files/Acoustic%20and%20Language%20Models/"" rel=""nofollow noreferrer"">this page</a> but it just contains the acoustic model, I'm looking for the .dmp or ARPA file with the statistical language model.
I could make it by myself but it's such a deal of time and I can't believe there is nothing to download for the english language.</p>
","language-model"
"41057816","How to calculate conditional_frequency_distribution and conditional_probability_distribution for trigrams in nltk python","2016-12-09 10:06:08","41060419","0","1643","<python><nltk><language-model><trigram>","<p>I want to calculate <strong>Conditional Probability Distribution</strong> for my language model but I am not able to do because I need <strong>Conditional Frequency Distribution</strong> which I am not able to generate. This is my code:</p>

<pre><code># -*- coding: utf-8 -*-

import io
import nltk
from nltk.util import ngrams
from nltk.tokenize import sent_tokenize
from preprocessor import utf8_to_ascii

with io.open(""mypet.txt"",'r',encoding='utf8') as utf_file:
    file_content = utf_file.read()

ascii_content = utf8_to_ascii(file_content)
sentence_tokenize_list = sent_tokenize(ascii_content)

all_trigrams = []

for sentence in sentence_tokenize_list:
    sentence = sentence.rstrip('.!?')
    tokens = nltk.re.findall(r""\w+(?:[-']\w+)*|'|[-.(]+|\S\w*"", sentence)
    trigrams = ngrams(tokens, 3,pad_left=True,pad_right=True,left_pad_symbol='&lt;s&gt;', right_pad_symbol=""&lt;/s&gt;"")
    all_trigrams.extend(trigrams)

conditional_frequency_distribution = nltk.ConditionalFreqDist(all_trigrams)
conditional_probability_distribution = nltk.ConditionalProbDist(conditional_frequency_distribution, nltk.MLEProbDist)

for trigram in all_trigrams:
    print ""{0}: {1}"".format(conditional_probability_distribution[trigram[0]].prob(trigram[1]), trigram)
</code></pre>

<p>But I am getting this error:</p>

<pre><code>line 23, in &lt;module&gt;
ValueError: too many values to unpack
</code></pre>

<p>This is my preprocessor.py file which is handling utf-8 chars:</p>

<pre><code># -*- coding: utf-8 -*-

import json


def utf8_to_ascii(utf8_text):
    with open(""utf_to_ascii.json"") as data_file:
        data = json.load(data_file)
    utf_table = data[""chars""]
    for key, value in utf_table.items():
        utf8_text = utf8_text.replace(key, value)
    return utf8_text.encode('ascii')
</code></pre>

<p>And this is my utf_to_ascii.json file which I used to replace utf-8 char to ascii char:</p>

<pre><code>{
 ""chars"": {
          ""“"":"""",
          ""”"":"""",
          ""’"":""'"",
          ""—"":""-"",
          ""–"":""-""
 }
}
</code></pre>

<p>Can someone suggest How can I calculate Conditional Frequency Distribution for trigrams in NLTK?</p>
","language-model"
"41039801","nltk.KneserNeyProbDist is giving 0.25 probability distribution for most of the trigrams","2016-12-08 12:42:08","","2","808","<python><nltk><language-model><trigram>","<p>I am working on Language Modeling using <a href=""https://github.com/nltk/nltk"" rel=""nofollow noreferrer"">nltk</a> I am using this <a href=""http://essaynnotes.cjkwebgroup.com/SubPages/My%20Pet%20Animal.html"" rel=""nofollow noreferrer"">essay</a> as my corpus in <strong>mypet.txt</strong> file. I am getting 0.25 Kneser Ney probability distribution for most of the <strong>trigrams</strong>. I don't know why. Is it right? Why is it doing so? This is my <strong>word_ngram.py</strong> file:</p>

<pre><code>import io
import nltk
from nltk.util import ngrams
from nltk.tokenize import sent_tokenize
from preprocessor import utf8_to_ascii

with io.open(""mypet.txt"",'r',encoding='utf8') as utf_file:
    file_content = utf_file.read()

ascii_content = utf8_to_ascii(file_content)
sentence_tokenize_list = sent_tokenize(ascii_content)

all_tgrams = []
for sentence in sentence_tokenize_list:
    sentence = sentence.rstrip('.!?')
    tokens = nltk.re.findall(r""\w+(?:[-']\w+)*|'|[-.(]+|\S\w*"", sentence)
    tgrams = ngrams(tokens, 3,pad_left=True,pad_right=True,left_pad_symbol='&lt;s&gt;', right_pad_symbol=""&lt;/s&gt;"")
    all_tgrams.extend(tgrams)

frequency_distribution = nltk.FreqDist(all_tgrams)
kneser_ney = nltk.KneserNeyProbDist(frequency_distribution)
for i in kneser_ney.samples():
    print ""{0}: {1}"".format(kneser_ney.prob(i), i)
</code></pre>

<p>This is my <strong>preprocessor.py</strong> file which is handling utf-8 chars:</p>

<pre><code># -*- coding: utf-8 -*-

import json


def utf8_to_ascii(utf8_text):
    with open(""utf_to_ascii.json"") as data_file:
        data = json.load(data_file)
    utf_table = data[""chars""]
    for key, value in utf_table.items():
        utf8_text = utf8_text.replace(key, value)
    return utf8_text.encode('ascii')
</code></pre>

<p>And this is my <strong>utf_to_ascii.json</strong> file which I used to replace utf-8 char to ascii char:</p>

<pre><code>{
 ""chars"": {
          ""“"":"""",
          ""”"":"""",
          ""’"":""'"",
          ""—"":""-"",
          ""–"":""-""
 }
}
</code></pre>

<p>This is sample output of few trigrams:</p>

<pre><code>0.25: ('side', '&lt;/s&gt;', '&lt;/s&gt;')
0.25: ('I', 'throw', 'a')
0.25: ('it', 'to', 'us')
0.25: ('guards', 'the', 'house')
0.0277777777778: ('&lt;s&gt;', 'I', 'am')
0.25: ('a', 'fire', 'broke')
0.125: ('our', 'house', 'at')
0.25: ('that', 'a', 'heap')
0.25: ('is', 'covered', 'with')
0.25: ('with', 'a', 'soft')
0.00862068965517: ('&lt;s&gt;', 'It', 'begins')
0.25: ('swim', '&lt;/s&gt;', '&lt;/s&gt;')
0.25: ('a', 'member', 'of')
0.25: ('bread', '&lt;/s&gt;', '&lt;/s&gt;')
0.25: ('love', '&lt;/s&gt;', '&lt;/s&gt;')
0.25: ('a', 'soft', 'fur')
0.25: ('body', 'is', 'covered')
0.25: ('I', 'bathe', 'it')
0.25: ('it', 'is', 'out')
0.25: ('&lt;s&gt;', 'A', 'thief')
0.25: ('go', 'hunting', '&lt;/s&gt;')
0.025: ('It', 'is', 'loved')
0.25: ('it', 'a', 'loving')
0.25: ('with', 'soap', 'every-day')
0.25: ('other', 'members', 'of')
0.25: ('lying', 'there', 'was')
0.25: ('sensitive', 'to', 'sound')
0.25: ('and', 'the', 'flames')
0.25: ('kitchen', '&lt;/s&gt;', '&lt;/s&gt;')
0.25: ('strong', 'instinct', '&lt;/s&gt;')
</code></pre>
","language-model"
"40796902","Is there any sentence embedding Tensorflow language model?","2016-11-25 02:34:06","","-1","1365","<nlp><tensorflow><recurrent-neural-network><language-model><word-embedding>","<p>I found tensorflow 1b_lm project: <a href=""https://github.com/tensorflow/models/tree/master/lm_1b"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/tree/master/lm_1b</a></p>

<p>I'm just quite confused about the forth example</p>

<blockquote>
  <p>Give a sentence, dump the embedding from the LSTM state.</p>
</blockquote>

<p>However, the results of this example includes 7 '.npy' files. It seems like it just generates every word embedding for every word in the sentence?</p>
","language-model"
"40607574","Negative results using kenlm","2016-11-15 10:33:07","","4","1073","<python><language-model>","<p>I am new to the language modeling and a make a 3grams language model using  <a href=""http://victor.chahuneau.fr/notes/2012/07/03/kenlm.html"" rel=""nofollow noreferrer"">kenlm</a>(or <a href=""https://kheafield.com/code/kenlm/estimation/"" rel=""nofollow noreferrer"">this</a>) from a large text file (~7gb.).
I make a binary file from my language model and call it in python like this:</p>

<pre><code>import kenlm
model = kenlm.LanguageModel(&lt;my .klm file&gt;)
model.score(&lt;my sentence&gt;)
</code></pre>

<p>and i get a negative number as the result.and when i change the sentence for scoring, the result remains negative but changes.I give it exactly one of the large text file sentences but it gives me a bad negative number(in comparison with a sentence that does not in the text file)
I dont know what does negative result means and how can i convert it to positive and normal result to select the most correct sentece between some sentences.</p>
","language-model"
"39784522","Can Artificial Neural Networks Learn Language Models? Paper 2000 Implementation","2016-09-30 05:47:59","","1","134","<nlp><neural-network><language-model>","<p>I am new to research field in NLP. I want to implement a paper <a href=""http://repository.cmu.edu/cgi/viewcontent.cgi?article=2405&amp;context=compsci"" rel=""nofollow"">Can Artificial Neural Networks Learn Language Models?</a> In this paper first time a step was taken so that Neural Network can learn Language Model. I have understood the paper, everything is understandable just some confusions in last section of paper.</p>

<p><strong>I did not found any of its code.</strong> Paper is too old (2000)  <strong>I did not even find the Training data (Communicator Telephone Air Travel Information System) which was used at that time</strong>.</p>

<p>I have also emailed about this to both professors of the paper but email id of one of them is expired and waiting for response from other one.</p>

<p>Can anyone help me in this situation? Your guidelines would be valuable for new comers in research field. I would be thankful to you.</p>
","language-model"
"39708567","Word prediction : neural net versus n-gram approach","2016-09-26 17:09:43","39733219","2","3421","<nlp><neural-network><language-model>","<p>For example if I attempt to predict the next word in a sentence I can use a bi gram approach and compute the probabilities of a word occurring based on the previous word in the corpus.</p>

<p>If instead I use a neural net to predict the next word. The training data consists of word pairs where each pair contains the current and next word in the corpus. Training the net uses an input value as a vectorized representation of the word , the output value is a vectorized representation of next word in the corpus.</p>

<p>I expect the neural net to perform better but I'm not sure why ?</p>

<p>When is it better to use a neural net versus a classical approach. In this case a neural net versus an n-gram model. Apologies if this question is ambiguous. </p>

<p>Maybe the answer is trial and error and check which model has faster performance and makes better predictions ?</p>

<p>The neural net will perform better as making the prediction is just a vector multiplication whereas using a n-gram model to predict requires a probability calculation. </p>
","language-model"
"39613555","N-grams - not in memory","2016-09-21 10:09:33","","1","93","<python><n-gram><language-model>","<p>I have 3 milion abstracts and I would like to extract 4-grams from them. I want to build a language model so I need to find the frequencies of these 4-grams. </p>

<p>My problem is that I can't extract all these 4-grams in memory. How can I implement a system that it can estimate all frequencies for these 4-grams? </p>
","language-model"
"39389585","Raise MemoryError when I am fitting a sequence to sequence LSTM using Keras+Theano","2016-09-08 11:21:00","","2","676","<python><keras><lstm><language-model>","<p>I was trying to implement a sequence to sequence language model. During training process, the model takes in a sequence of 50d word vectors generated by GloVe, and output 1-to-V(V is the size of vocabulary) vector meaning the next word which thus can be regarded as the distribution of next word corresponding to the input word vector at current timestep in test process, and I tried with a 112-word vocabulary.</p>

<p>Then, I built two models as following:</p>

<pre><code>model1 = Sequential()
model1.add(LSTM(112, return_sequences=True, input_shape=(31, 50)))

model2 = Sequential()
model2.add(LSTM(112, return_sequences=True, input_shape=(31, 50)))
model2.add(TimeDistributed(Dense(112, activation=""linear"")))
</code></pre>

<p>When I tried to fit them by</p>

<pre><code>model.fit(X, Y, batch_size=128, nb_epoch=256, validation_rate=0.1)
</code></pre>

<p>The first model <code>model1</code> crashed and raised MemoryError, but the second model <code>model2</code> normally finished. X has the shape of <code>(number_of_sentences, max_words_in_one_sentence, 50)</code>, and Y has the shape of <code>(number_of_sentences, max_words_in_one_sentence, 112)</code>. In this example, <code>number_of_sentences=10000, max_words_in_one_sentence=13</code>.</p>

<p>I am wondering what happened when I appended a new time-distributed-dense to a LSTM layer, and which one is the model I want to implement my language model.</p>
","language-model"
"39373940","Tensorflow: Recurrent neural network training pairs & the effect on the loss function","2016-09-07 15:27:51","","0","111","<neural-network><tensorflow><recurrent-neural-network><language-model>","<p>I am looking at code for an RNN Language Model. I am confused as to <strong>1)</strong> how the training pairs (x,y) are constructed and subsequently <strong>2)</strong> how the loss is computed. The code borrows from the Tensorflow RNN tutorial ( <strong><em>reader</em></strong> module ).</p>

<p>Within the reader module, a generator, <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py#L82"" rel=""nofollow""><em>ptb_iterator</em></a>, is defined. It takes in the data as one sequence and <strong>yields</strong> x,y pairs in accordance to the batch size and the number of steps you wish to 'unroll' the RNN. It is best to look at the entire definition first but the part that confused me is this:</p>

<pre><code>for i in range(epoch_size):
  x = data[:, i*num_steps:(i+1)*num_steps]
  y = data[:, i*num_steps+1:(i+1)*num_steps+1]
  yield (x, y)
</code></pre>

<p>which is documented as:</p>

<pre><code>*Yields:
 Pairs of the batched data, each a matrix of shape [batch_size, num_steps].
 The second element of the tuple is the same data time-shifted to the
 right by one.*
</code></pre>

<p>So if understand correctly, for the data sequence <code>[1 2 3 4 5 6]</code> and <code>num_steps = 2</code> then for stochastic gradient descent(i.e. batch_size=1) the following pairs will be generated:</p>

<ol>
<li>x=[1,2] , y=[2,3]</li>
<li>x=[3,4] , y=[5,6]</li>
</ol>

<p><strong>1)</strong> Is this the correct way to do this? Should it not be done so that the pairs are:</p>

<ol>
<li>x=[1,2] , y=[2,3]</li>
<li>x=[2,3] , y=[3,4]
... # allows for more datapoints</li>
</ol>

<p>OR</p>

<ol>
<li>x=[1,2] , y=[3]</li>
<li>x=[2,3] , y=[4]
... # ensures that all predictions are made with context length = num_steps</li>
</ol>

<p><strong>2)</strong> Lastly, given that the pairs are generated as they are in the <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py#L82"" rel=""nofollow"">reader</a> module, when it comes to training, will the loss computed not reflect the RNN's performance over a range of unrolled steps instead of <code>num_steps</code> specified? </p>

<p>For example, the model will make a prediction for x=3 (from x=[3,4]) without considering that 2 came before it (i.e. unrolling the RNN one step instead of two).</p>
","language-model"
"39242569","Get the probability distribution of next word given a sequence using TensorFlow's RNN (LSTM) language model?","2016-08-31 06:47:53","39250533","1","2846","<tensorflow><lstm><language-model>","<p>I'm running TensorFlow's RNN (LSTM) language model example <a href=""https://www.tensorflow.org/versions/r0.10/tutorials/recurrent/index.html"" rel=""nofollow"">here</a>.
It runs and reports the perplexities perfectly.</p>

<p>What I want though is three things:</p>

<ol>
<li><p>Given a sequence (e.g. w1 w5 w2000 w750) give me the probability distribution for the next word over the vocabulary. I don't know how to do it with the model in the tutorial.</p></li>
<li><p>I want the model to return a ranking of the most probable sequences (e.g. n-grams), n can be given as input.</p></li>
</ol>

<p>and</p>

<ol start=""3"">
<li>Given a sequence, I want it's probability.</li>
</ol>

<p>I'm new to TensorFlow and RNNs so plz tell me if you need more information than I have provided.</p>

<p>The code for the language model is <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py"" rel=""nofollow"">here</a>.</p>
","language-model"
"39039793","What is the softmax_w and softmax_b in this document?","2016-08-19 13:03:36","39040494","2","488","<tensorflow><language-model><softmax>","<p>I'm new to TensorFlow and need to train a language model but run into some difficulties while reading the <a href=""https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html#lstm"" rel=""nofollow"">document</a> as shown bellow. </p>

<pre><code>lstm = rnn_cell.BasicLSTMCell(lstm_size)
# Initial state of the LSTM memory.
state = tf.zeros([batch_size, lstm.state_size])

loss = 0.0
for current_batch_of_words in words_in_dataset:
    # The value of state is updated after processing each batch of words.
    output, state = lstm(current_batch_of_words, state)

    # The LSTM output can be used to make next word predictions
    logits = tf.matmul(output, softmax_w) + softmax_b
    probabilities = tf.nn.softmax(logits)
    loss += loss_function(probabilities, target_words)
</code></pre>

<p>I don't understand why this line is needed,</p>

<pre><code>logits = tf.matmul(output, softmax_w) + softmax_b
</code></pre>

<p>Since I learned that once the output is computed out and the target_words are known we can directly work out the loss. It seems that the pseudo-code adds an additional layer. In addition, what is the softmax_w and softmax_b which are not aforementioned. I thought I may have missed something important by raising such a simple question.</p>

<p>Please point me in the right direction, and any suggestions are highly appreciated. Thanks a lot. </p>
","language-model"
"38701848","How to learn two sequences simultaenously through LSTM in Tensorflow/TFLearn?","2016-08-01 14:47:16","","1","506","<python><tensorflow><deep-learning><lstm><language-model>","<p>I am learning LSTM based seq2seq model in Tensorflow platform. I can very well train a model on a given simple seq2seq examples. </p>

<p>However, in cases where I have to learn two sequences at once from a given sequence (for e.g: learning previous sequence and next sequence from the current sequence simultaneously), how can we do it i.e, compute the combined error from both sequence and backpropogate the same error to both sequences?</p>

<p>Here's the snippet to the LSTM code that I am using (mostly taken from ptb example: <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L132"" rel=""nofollow"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L132</a>):</p>

<pre><code>        output = tf.reshape(tf.concat(1, outputs), [-1, size])
        softmax_w = tf.get_variable(""softmax_w"", [size, word_vocab_size])
        softmax_b = tf.get_variable(""softmax_b"", [word_vocab_size])
        logits = tf.matmul(output, softmax_w) + softmax_b
        loss = tf.nn.seq2seq.sequence_loss_by_example(
            [logits],
            [tf.reshape(self._targets, [-1])],
            [weights])
        self._cost = cost = tf.reduce_sum(loss) / batch_size
        self._final_state = state
        self._lr = tf.Variable(0.0, trainable=False)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),config.max_grad_norm)
        optimizer = tf.train.GradientDescentOptimizer(self.lr)
        self._train_op = optimizer.apply_gradients(zip(grads, tvars))
</code></pre>
","language-model"
"38687369","How to run custom seq2seq learning (using pre-calculated word embeddings) encoder-decoder in Tensorflow?","2016-07-31 18:53:56","","0","828","<python><tensorflow><deep-learning><lstm><language-model>","<p>I need to run a encoder-decoder model in Tensorflow. I see that using the available APIs <code>basic_rnn_seq2seq(encoder_input_data, decoder_input_data, lstm_cell)</code> etc, a encoder-decoder system can be created.</p>

<ol>
<li>How can we enter the embeddings such as word2vec in such model? I am
aware that we can do embedding lookup but as per the API
<code>encoder_input_data</code> is a list of 2D Tensor of size batch_size x
input_size. How can each word be represented using its respective word embedding in this setup? Even <code>embedding_rnn_seq2seq</code> internally extracts the embeddings. How to give pre-calculated word embeddings as input? </li>
<li>How can we get the cost/perplexity through the API?</li>
<li>In case of test instances, we may not know the corresponding decoder inputs. How to handle such case?</li>
</ol>
","language-model"
"38665774","TensorFlow reset state during batch = sentence-level language model","2016-07-29 18:55:28","","1","210","<tensorflow><language-model>","<p>What is the best way to build a recurrent language model (e.g. LSTM) that does not cross sentence boundaries? Or put more general, if you present a batch to the model, each row containing multiple sentences, how can you reset the state after seeing each sentence? Is there a special token you can specify to the model?</p>

<p>Thanks!</p>
","language-model"
"38483636","Error at ARPA model training with SRILM","2016-07-20 14:09:23","","3","2443","<speech-recognition><cmusphinx><sphinx4><language-model><srilm>","<p>I have followed <a href=""http://cmusphinx.sourceforge.net/wiki/tutoriallm"" rel=""nofollow noreferrer"">this</a> tutorial.</p>

<p>After I run this code:</p>

<pre><code>ngram-count -kndiscount -interpolate -text train-text.txt -lm your.lm
</code></pre>

<p>It gives me this error:</p>

<blockquote>
  <p>""One of modified KneserNey discounts is negative error in discount
  estimator for order 2.""</p>
</blockquote>

<p>How can I solve this problem?</p>
","language-model"
"38445366","Dynamic LSTM model in Tensorflow","2016-07-18 20:21:11","45698444","1","1227","<tensorflow><deep-learning><lstm><recurrent-neural-network><language-model>","<p>I am looking to design a LSTM model using Tensorflow, wherein the sentences are of different length. I came across a tutorial on PTB dataset (<a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py"" rel=""nofollow"">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py</a>). How does this model capture the instances of varying length? The example does not discuss anything about padding or other technique to handle the variable size sequences.</p>

<p>If I use padding, what should be the unrolling dimension?</p>
","language-model"
"38436939","Writing code for A Neural Probabilistic Language Model Bengio, 2003. Not able to understand the model","2016-07-18 12:38:30","","-2","502","<nlp><deep-learning><language-model>","<p>I'm trying to write code for <a href=""http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"" rel=""nofollow"">A Neural Probabilistic Language Model by yoshua Bengio, 2003</a>, but I'm not able to understand the connections between the input layer and projection matrix and between projection matrix and hidden layer.  I'm not able to get how exactly is the learning for word-vector representation taking place.</p>
","language-model"
"38386009","What is a simple example of a TensorFlow file pipeline for a language model?","2016-07-15 00:06:48","","1","206","<tensorflow><language-model>","<p>I am building a RNN language model in TensorFlow. My raw input consists of files of text. I am able to tokenize them, so that data I am working with is sequences of integers that are indexes into a vocabulary.</p>

<p>Following the example in <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py"" rel=""nofollow""><code>ptb_word_lm.py</code></a>, I have written code to build a language model that gets its training data via the <a href=""https://www.tensorflow.org/versions/r0.9/how_tos/reading_data/index.html#feeding"" rel=""nofollow"">feed_dict</a> method. However, I do not want to be limited to data sets that can fit in memory, so I would like to use <a href=""https://www.tensorflow.org/versions/r0.9/how_tos/reading_data/index.html#reading-from-files"" rel=""nofollow"">file pipelines</a> to read in data instead. I cannot find any examples of how to do this.</p>

<p>The file pipelines examples I've seen all have a tensor of some length <em>n</em> associated with a label that is a tensor of length 1. (The classic example being a 28 x 28 = 784 item tensor representing an MNIST bitmap associated with a single integer label that ranges from 0 to 9.) However, RNN training data consists of a vector of <em>n</em> consecutive tokens and a label also consisting of <em>n</em> consecutive tokens (shifted one ahead of the vector), for example:</p>

<pre><code>""the quick brown fox jumped""
vectors (n=3): the quick brown, quick brown fox, brown fox jumped
labels (n=3): quick brown fox, brown fox jumped, fox jumped EOF
</code></pre>

<p>Can someone give me a code snippet that shows how to write a file pipeline to feed this shape of data into a TensorFlow graph?</p>
","language-model"
"38363672","Train TensorFlow language model with NCE or sampled softmax","2016-07-14 00:19:33","","5","1209","<tensorflow><lstm><softmax><language-model>","<p>I'm adapting the TensorFlow RNN tutorial to train a language model with a NCE loss or sampled softmax, but I still want to report perplexities. However, the perplexities I get are very weird: for NCE I get several millions (terrible!) whereas for sampled softmax I get a PPL of 700 after one epoch (too good to be true?!). I wonder what I'm doing wrong.</p>

<p>Here is my adaptation to the PTBModel:</p>

<pre><code>class PTBModel(object):
  """"""The PTB model.""""""

  def __init__(self, is_training, config, loss_function=""softmax""):
    ...
    w = tf.get_variable(""proj_w"", [size, vocab_size])
    w_t = tf.transpose(w)
    b = tf.get_variable(""proj_b"", [vocab_size])

    if loss_function == ""softmax"":
      logits = tf.matmul(output, w) + b
      loss = tf.nn.seq2seq.sequence_loss_by_example(
          [logits],
          [tf.reshape(self._targets, [-1])],
          [tf.ones([batch_size * num_steps])])
      self._cost = cost = tf.reduce_sum(loss) / batch_size
    elif loss_function == ""nce"":
      num_samples = 10
      labels = tf.reshape(self._targets, [-1,1])
      hidden = output
      loss = tf.nn.nce_loss(w_t, b,                           
                            hidden,
                            labels,
                            num_samples, 
                            vocab_size)
    elif loss_function == ""sampled_softmax"":
      num_samples = 10
      labels = tf.reshape(self._targets, [-1,1])
      hidden = output
      loss = tf.nn.sampled_softmax_loss(w_t, b,
                                        hidden, 
                                        labels, 
                                        num_samples,
                                        vocab_size)

    self._cost = cost = tf.reduce_sum(loss) / batch_size
    self._final_state = state
</code></pre>

<p>The call to this model is like this:</p>

<pre><code>mtrain = PTBModel(is_training=True, config=config, loss_function=""nce"")
mvalid = PTBModel(is_training=True, config=config)
</code></pre>

<p>I'm not doing anything exotic here, changing the loss function should be pretty straightforward. So why does it not work?</p>

<p>Thanks,
Joris</p>
","language-model"
"38241134","Input shape for Keras LSTM/GRU language model","2016-07-07 08:48:01","","4","13853","<python><nlp><keras><lstm><language-model>","<p>I am trying to train a language model on word level in Keras. </p>

<p>I have my X and Y, both with the shape (90582L, 517L)</p>

<p>When I try fit this model:</p>

<pre><code>print('Build model...')
model = Sequential()
model.add(GRU(512, return_sequences=True, input_shape=(90582, 517)))
model.add(Dropout(0.2))
model.add(GRU(512, return_sequences=True))
model.add(Dropout(0.2))
model.add(TimeDistributedDense(1))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
model.fit(x_pad, y_pad, batch_size=128, nb_epoch=2)
</code></pre>

<p>I get the error:</p>

<pre><code>Exception: Error when checking model input: 
expected gru_input_7 to have 3 dimensions, but got array with shape (90582L, 517L)
</code></pre>

<p>I need some guidance as to what the input shape should be? I've done trial and error on all sorts of combinations but it seems I am misunderstanding something fundamental.</p>

<p>In the Keras text generation example, the X matrix had 3 dimensions. I have no idea what the third dimension is supposed to be though.  </p>
","language-model"
"38208132","How do I change the Keras text generation example from being on character level to word level?","2016-07-05 16:15:05","","2","1056","<python><nlp><keras><lstm><language-model>","<p>The above code is more or less what the Keras documentation gives us as a language model. The thing is that this language model predicts characters, not words. Strictly speaking, a language model is supposed to predict full words. </p>

<p>My question is, how do I change this in order to predict full words?</p>

<pre><code>from __future__ import print_function
from keras.models import Sequential
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM
from keras.utils.data_utils import get_file
import numpy as np
import random
import sys

path = ""C:/Users/Cedric     Oeldorf/Desktop/University/Research/Data/Gutenberg/MYDATAFINAL3.txt""
text = open(path).read().lower()
print('corpus length:', len(text))

chars = set(text)
print('total chars:', len(chars))
char_indices = dict((c, i) for i, c in enumerate(chars))
indices_char = dict((i, c) for i, c in enumerate(chars))

# cut the text in semi-redundant sequences of maxlen characters
maxlen = 40
step = 3
sentences = []
next_chars = []
for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i + maxlen])
    next_chars.append(text[i + maxlen])
print('nb sequences:', len(sentences))

print('Vectorization...')
X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)
for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        X[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1

from keras.callbacks import History
histLSTM = History()

# build the model: 2 stacked LSTM
print('Build model...')
model = Sequential()
model.add(LSTM(512, return_sequences=True, input_shape=(maxlen, len(chars))))
model.add(Dropout(0.2))
model.add(LSTM(512, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

model.fit(X, y, batch_size=128, nb_epoch=4, callbacks=[histLSTM])
</code></pre>

<p>My data preprocessing idea so far is:</p>

<pre><code>path = ""C:/MYDATAFINAL3.txt""
text = open(path).read().lower()
print('corpus length:', len(text))

#tokenize corpus and get list of unique words
tok = gensim.utils.simple_preprocess(text, deacc=False)
words = set(tok)
word_indices = dict((c, i) for i, c in enumerate(words))
indices_word = dict((i, c) for i, c in enumerate(words))

sentences1 = text.split('.')
SYMBOLS = '{}()[].,:;+-*/&amp;|&lt;&gt;=~$'
m = [item.translate(None, SYMBOLS).strip() for item in sentences1]
del text

maxlen = 60
step = 3
sentences = []
next_words = []
for i in range(0, len(tok) - maxlen, step):
    sentences.append(tok[i: i + maxlen])
    next_words.append(tok[i + maxlen])
print('nb sequences:', len(sentences))

X = np.zeros((len(sentences), maxlen), dtype=""int32"")
y = np.zeros((len(sentences),maxlen), dtype=""int32"")
</code></pre>

<p>This step isnt working out:</p>

<pre><code>#In X, change boolean to true for every listed character, same for y
for i, sentence in enumerate(sentences):
    for t, words in enumerate(sentence):
        X[i, t,] = word_indices[words]
    y[i, t] = word_indices[words]
</code></pre>

<p>And I don't know what input shape I should be using:</p>

<pre><code>print('Build model...')
model = Sequential()
model.add(GRU(512, return_sequences=True, input_shape=(len(sentences), maxlen)))
model.add(Dropout(0.2))
model.add(GRU(512, return_sequences=True))
model.add(Dropout(0.2))
#model.add(Dense(len(chars)))
#Insert this instead:
model.add(TimeDistributedDense(len(words)))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
model.fit(X, y, batch_size=128, nb_epoch=2)
</code></pre>
","language-model"
"38079307","Using language model tool without any installation","2016-06-28 14:41:35","","1","75","<speech-recognition><cmusphinx><sphinx4><language-model>","<p>I know that there are some language model tools which are <a href=""http://hlt-mt.fbk.eu/technologies/irstlm"" rel=""nofollow"">IRSLM</a>, <a href=""https://github.com/mitlm/mitlm"" rel=""nofollow"">MITLM</a>, <a href=""http://www-speech.sri.com/projects/srilm/"" rel=""nofollow"">SRILM</a> . All of them need to a installation to be able to create a language model etc. </p>

<p>However I need a language model tool which is not needed any installation and can be used offline. </p>

<p>And also there is a <a href=""http://www.speech.cs.cmu.edu/tools/lmtool-new.html"" rel=""nofollow"">lmtool</a> which can create a language model file using the website. </p>

<p>Is there any tool ? </p>
","language-model"
"37947619","Using custom beam scorer in TensorFlow CTC (language model)","2016-06-21 14:45:02","37965110","6","1611","<tensorflow><language-model>","<p>Is it possible to customize beam scorer in TensorFlow CTC implementation from Python side? I see this possibility in comment for CTCBeamSearchDecoder C++ class constructor but wonder how to provide this functionality for Python users?</p>

<p>Specific issue that we have is the plugging of language model into CTC based speech decoder. Language model can possibly be a pre-trained TensorFlow sub-graph, capable of outputting probabilities for beam score adjustment. But we need a way to inject this into beam scorer.</p>
","language-model"
"37897934","TensorFlow Embedding Lookup","2016-06-18 14:15:13","37932988","12","11352","<tensorflow><word2vec><recurrent-neural-network><language-model>","<p>I am trying to learn how to build RNN for Speech Recognition using TensorFlow. As a start, I wanted to try out some example models put up on TensorFlow page <a href=""https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html"" rel=""noreferrer"">TF-RNN</a></p>

<p>As per what was advised, I had taken some time to understand how word IDs are embedded into a dense representation (Vector Representation) by working through the basic version of word2vec model code. I had an understanding of what <code>tf.nn.embedding_lookup</code> actually does, until I actually encountered the same function being used with two dimensional array in <a href=""https://www.tensorflow.org/versions/master/tutorials/recurrent/index.html"" rel=""noreferrer"">TF-RNN</a> <code>ptb_word_lm.py</code>, when it did not make sense any more.</p>

<h3>what I though <code>tf.nn.embedding_lookup</code> does:</h3>

<p>Given a 2-d array <code>params</code>, and a 1-d array <code>ids</code>, function <code>tf.nn.embedding_lookup</code> fetches rows from params, corresponding to the indices given in <code>ids</code>, which holds with the dimension of output it is returning.</p>

<h3>What I am confused about:</h3>

<p>When tried with same params, and 2-d array <code>ids</code>, <code>tf.nn.embedding_lookup</code> returns 3-d array, instead of 2-d which I do not understand why.</p>

<p>I looked up the manual for <a href=""https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#embedding_lookup"" rel=""noreferrer"">Embedding Lookup</a>, but I still find it difficult to understand how the partitioning works, and the result that is returned. I recently tried some simple example with <code>tf.nn.embedding_lookup</code> and it appears that it returns different values each time. Is this behaviour due to the randomness involved in partitioning ?</p>

<p>Please help me understand how <code>tf.nn.embedding_lookup</code> works, and why is used in both <code>word2vec_basic.py</code>, and <code>ptb_word_lm.py</code> i.e., what is the purpose of even using them ?</p>
","language-model"
"37089201","How to calculate perplexity for a language model trained using keras?","2016-05-07 13:33:40","","2","5453","<python><nlp><keras><language-model>","<p>Using Python 2.7 Anaconda on Windows 10</p>

<p>I have trained a GRU neural network to build a language model using keras:</p>

<pre><code>print('Build model...')
model = Sequential()
model.add(GRU(512, return_sequences=True, input_shape=(maxlen, len(chars))))
model.add(Dropout(0.2))
model.add(GRU(512, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(len(chars)))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
</code></pre>

<p>How do I calculate the perplexity of this language model? For example, NLTK offers a perplexity calculation function for its models. </p>
","language-model"
"37019105","How to create Dictionary file from vocab?","2016-05-04 05:22:49","","0","1002","<cmusphinx><sphinx4><language-model>","<p>How to create dictionary(.dict) file for our specific domain Language model. I'm using CMU tool kit to create ARPA format Language model, but in that there is no option to create .dict file. Thanks in advance.</p>
","language-model"
"36338668","language model with SRILM","2016-03-31 16:17:57","36368905","1","1930","<nlp><n-gram><language-model><srilm>","<p>I'm trying to build a language model using SRILM.
I have a list of phrases and I create the model using:</p>

<pre><code>./ngram-count -text corpus.txt -order 3 -ukndiscount -interpolate -unk -lm corpus.lm
</code></pre>

<p>After this I tried to make some example to see the probabilities of different phrases and it turned out that  has a log probability of <em>-0.9</em>.</p>

<p>The problem is that there are some words in the training with a lower log probability. For example there are 5 <em>""abatantuono""</em> and its log probability  is <em>-4.8</em>.</p>

<p>I think this is strange because a phrase <code>&lt;s&gt; &lt;unk&gt; &lt;/s&gt;</code> is more probable  than <code>&lt;s&gt; abatantuono &lt;/s&gt;</code> and in the training set the 3-gram <code>&lt;s&gt; abatantuono &lt;/s&gt;</code> is also present!</p>

<p>This can be seen here:</p>

<pre><code> % ./ngram -lm corpus.lm -ppl ../../../corpus.txt.test -debug 2 -unk
 reading 52147 1-grams
 reading 316818 2-grams
 reading 91463 3-grams
 abatantuono
     p( abatantuono | &lt;s&gt; )     = [2gram] 1.6643e-05 [ -4.77877 ]
     p( &lt;/s&gt; | abatantuono ...)     = [3gram] 0.717486 [ -0.144186 ]
 1 sentences, 1 words, 0 OOVs
 0 zeroprobs, logprob= -4.92296 ppl= 289.386 ppl1= 83744.3

 abatantonno
     p( &lt;unk&gt; | &lt;s&gt; )   = [1gram] 0.00700236 [ -2.15476 ]
     p( &lt;/s&gt; | &lt;unk&gt; ...)   = [1gram] 0.112416 [ -0.949172 ]
 1 sentences, 1 words, 0 OOVs
 0 zeroprobs, logprob= -3.10393 ppl= 35.6422 ppl1= 1270.36

 file ../../../corpus.txt.test: 2 sentences, 2 words, 0 OOVs
 0 zeroprobs, logprob= -8.02688 ppl= 101.56 ppl1= 10314.3
</code></pre>

<p>What do you think the problem could be?</p>

<p>Thank you</p>
","language-model"
"36297422","Incremental language model training with lingpipe","2016-03-30 00:15:44","36344799","0","145","<java><nlp><language-model><lingpipe>","<p>I'm trying to train a <code>DynamicLMClassifier.createNGramProcess(categories,nGram)</code> on a big dataset > 20GB. I'm currently feeding the entire training file as a String to the training methods and for obvious reasons i'm getting a <code>java.lang.OutOfMemoryError: Java heap space</code></p>

<p>Although it might be possible to increase the JVM heap size to support such training i'm interested in finding an incremental method.</p>

<p>The training code looks like this:</p>

<pre><code>char[] csBuf = new char[numChars];
for (int i = 0; i &lt; categories.length; ++i) {
    String category = categories[i];
    File trainingFile = new File(new File(dataDir,category),
                                 category + "".txt"");
    FileInputStream fileIn
        = new FileInputStream(trainingFile);
    InputStreamReader reader
        = new InputStreamReader(fileIn,Strings.UTF8);
    reader.read(csBuf);
    String text = new String(csBuf,0,numChars);
    Classification c = new Classification(category);
    Classified&lt;CharSequence&gt; classified
        = new Classified&lt;CharSequence&gt;(text,c);
    classifier.handle(classified);
    reader.close();
}
</code></pre>

<p>The ideal solution would be to feed classifier.handle() in a loop of N subsets of the training set. Theoretically I think this should be possible since the model only needs to remember ngrams tuples with their respective counts to compute the MLE.</p>
","language-model"
"35761791","Testing accuracy always more than 99%","2016-03-03 02:02:29","","3","384","<theano><keras><lstm><language-model>","<p>I am trying to implement a language model using LSTMs in theano/keras. My network runs fine and I also see that the training loss decreases but the testing accuracy is always above 99% even if I don not train my network for long. 
I have used word2vec vectors and embedded the weights in the Embedding layer. My network looks like:</p>

<pre><code>model = Graph()  
model.add_input(name='input', input_shape=(n_train,), dtype=int)  
model.add_node(Embedding(output_dim=rnn_dim, input_dim=n_symbols, weights=[embedding_weights]),name = 'embedding',input='input')  
model.add_node(LSTM(output_dim=dense_dim,input_dim=rnn_dim), name='forward', input='embedding')  
model.add_node(Dropout(0.5), name='dropout', input='forward')  
model.add_node(Dense(output_size, activation='softmax'), name='softmax', input='dropout')  
model.add_output(name='output', input='softmax')  

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.99, nesterov=True)  
model.compile(sgd, loss={'output': 'categorical_crossentropy'})  
print(""Train..."")  
model.fit({'input': X_train,'output': y_train},
      batch_size=128,
      nb_epoch=1,verbose=1)  
</code></pre>

<p>My training and testing array shapes are:
X_train shape: (100000, 18)
X_test shape: (10000, 18)
y_train shape: (100000, 998)
y_test shape: (10000, 998)</p>

<p>where there are 100000 training and 10000 testing sentences and each sentence contains 18 words. The number of classes for output is 998. </p>

<p>Can anyone suggest why I might not be getting a genuine testing error?</p>
","language-model"
"35506904","Wrong number of dimensions: expected 0, got 1 with shape (1,)","2016-02-19 13:48:36","35509785","1","1917","<theano><recurrent-neural-network><language-model>","<p>I am doing word-level language modelling with a vanilla rnn, I am able to train the model but for some weird reasons I am not able to get any samples/predictions from the model; here is the relevant part of the code:</p>

<pre><code>train_set_x, train_set_y, voc = load_data(dataset, vocab, vocab_enc)  # just load all data as shared variables
index = T.lscalar('index')
x = T.fmatrix('x')
y = T.ivector('y')
n_x = len(vocab)
n_h = 100
n_y = len(vocab)

rnn = Rnn(input=x, input_dim=n_x, hidden_dim=n_h, output_dim=n_y)

cost = rnn.negative_log_likelihood(y)

updates = get_optimizer(optimizer, cost, rnn.params, learning_rate)

train_model = theano.function(
    inputs=[index],
    outputs=cost,
    givens={
        x: train_set_x[index],
        y: train_set_y[index]
    },
    updates=updates
)

predict_model = theano.function(
    inputs=[index],
    outputs=rnn.y,
    givens={
        x: voc[index]
    }
)

sampling_freq = 2
sample_length = 10
n_train_examples = train_set_x.get_value(borrow=True).shape[0]
train_cost = 0.
for i in xrange(n_train_examples):
    train_cost += train_model(i)
    train_cost /= n_train_examples

    if i % sampling_freq == 0:
       # sample from the model     
       seed = randint(0, len(vocab)-1)
       idxes = []
       for j in xrange(sample_length):
           p = predict_model(seed)
           seed = p
           idxes.append(p)
           # sample = ''.join(ix_to_words[ix] for ix in idxes)
           # print(sample)
</code></pre>

<p>I get the error: <strong>""TypeError: ('Bad input argument to theano function with name ""train.py:94""  at index 0(0-based)', 'Wrong number of dimensions: expected 0, got 1 with shape (1,).')""</strong></p>

<p>Now this corresponds to the following line (in the predict_model):</p>

<pre><code> givens={   x: voc[index]   }
</code></pre>

<p>Even after spending hours I am not able to comprehend how could there be a dimension mis-match when:</p>

<pre><code>train_set_x has shape: (42, 4, 109)
voc has shape: (109, 1, 109)
</code></pre>

<p>And when I do train_set_x[index], I am getting <em>(4, 109)</em> which '<strong>x</strong>' Tensor of type fmatrix can hold (this is what happens in <strong>train_model</strong>) but when I do voc[index], I am getting <em>(1, 109)</em>, which is also a matrix but '<strong>x</strong>' cannot hold this, why ? !</p>

<p>Any help will be much appreciated.</p>

<p>Thanks !</p>
","language-model"
"35403070","unable to open Cube language model params for hindi Language in tesseract","2016-02-15 06:47:07","43063234","4","3040","<ocr><tesseract><hindi><language-model>","<p>Tesseract unable to read cube language model.
<code>tesseract 1.png output.txt -l hin</code>
After above command execution following error occur.<br></p>

<pre><code>Cube ERROR (CubeRecoContext::Load): unable to read cube language model params from /usr/share/tesseract-ocr/tessdata/hin.cube.lm
Cube ERROR (CubeRecoContext::Create): unable to init CubeRecoContext object
init_cube_objects(false, &amp;tessdata_manager):Error:Assert failed:in file tessedit.cpp, line 207
Segmentation fault
</code></pre>

<p>Where I get <strong>hin.cube.lm</strong> file and how to deal with that file?</p>
","language-model"
"35129424","RNNLM using theano","2016-02-01 11:29:39","","2","493","<python><nlp><theano><recurrent-neural-network><language-model>","<p>I asked the same question on theano user list, but got no reply, just wondering if anyone can help me here.</p>

<p>I am trying to re-implement the RNNLM of <a href=""http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf"" rel=""nofollow"">http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf</a> based on this nice post.
I tried a toy test case which training data is the first 100 sentences of PTB training data (downloaded from <a href=""http://rnnlm.org/"" rel=""nofollow"">http://rnnlm.org/</a> ), the same data also used for evaluation.</p>

<p><strong>Baseline:</strong></p>

<p>I trained the LM with 25 iterations, using rnnlm-0.4b from <a href=""http://rnnlm.org/"" rel=""nofollow"">http://rnnlm.org/</a>, I got
test log probability: -4107.323481
PPL net: 85.496622</p>

<p>The command lines that to produce the baseline are:</p>

<pre><code>$ rnnlm -train ../ptb/ptb.train.txt.toy -valid ../ptb/ptb.train.txt.toy -rnnlm rnn.model.toy -hidden 10 -class 1 -max-iter 25 -independent -beta 0 -direct-order 0
$ rnnlm -rnnlm rnn.model.toy -test ../ptb/ptb.train.txt.toy -independent
</code></pre>

<p><strong>Using my implementation</strong>, after 25 iterations, there is a large difference in PPL:</p>

<blockquote>
  <p>epoch=24: log probability=-5374.255371 ppl=337.187731</p>
</blockquote>

<p>I am still learning Theano, did i miss something in my implementation?</p>

<p>Thanks</p>

<p>My implementation can be found at <a href=""https://11350770138305416713.googlegroups.com/attach/179c4942cbf157/rnnlm.py?part=0.1&amp;view=1&amp;vt=ANaJVrGZjtk4NfhfR2gY8Xk3x5yDxT5FUczkzSB6PRIGVu7lN_Dr9XHUZcYNE3WSrEc6SCL7vf8O9Xg5oCuzST4Z5oxo0WgCAIjBz7X0SC_4ZnvpsKf4HII"" rel=""nofollow"">here</a>:</p>

<pre><code>#! /usr/bin/env python

import itertools
import codecs
import numpy as np
import nltk
import sys
import time
from datetime import datetime
import theano as theano
import theano.tensor as T

class RNNLM:

    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):
        # Assign instance variables
        self.word_dim = word_dim
        self.hidden_dim = hidden_dim
        self.bptt_truncate = bptt_truncate
        # Randomly initialize the network parameters
        U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))
        V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))
        W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))
        # Theano: Created shared variables
        self.U = theano.shared(name='U', value=U.astype(theano.config.floatX))  # @UndefinedVariable
        self.V = theano.shared(name='V', value=V.astype(theano.config.floatX))  # @UndefinedVariable
        self.W = theano.shared(name='W', value=W.astype(theano.config.floatX))      # @UndefinedVariable
        # We store the Theano graph here
        self.theano = {}
        self.__theano_build__()

    def __theano_build__(self):
        U, V, W = self.U, self.V, self.W
        x = T.ivector('x')
        y = T.ivector('y')
        def forward_prop_step(x_t, s_t_prev, U, V, W):
            s_t = T.tanh(U[:,x_t] + W.dot(s_t_prev))
            o_t = V.dot(s_t)

            return [o_t, s_t]
        [o,s], updates = theano.scan(
            forward_prop_step,
            sequences=x,
            outputs_info=[None, dict(initial=T.zeros(self.hidden_dim))],
            non_sequences=[U, V, W],
            truncate_gradient=self.bptt_truncate,
            strict=True)
        p_x_given_h = T.nnet.softmax(o)

        o_error = T.sum(T.nnet.categorical_crossentropy(p_x_given_h, y))
        logp = T.sum(T.log10(p_x_given_h)[T.arange(y.shape[0]), y])


        # Gradients
        dU = T.grad(o_error, U)
        dV = T.grad(o_error, V)
        dW = T.grad(o_error, W)

        # Assign functions
        self.forward_propagation = theano.function([x], p_x_given_h)
        self.ce_error = theano.function([x, y], o_error)
        self.logp = theano.function([x, y], logp)
        # SGD
        learning_rate = T.scalar('learning_rate')
        self.sgd_step = theano.function([x,y,learning_rate], [], 
                      updates=[(self.U, self.U - learning_rate * dU),
                              (self.V, self.V - learning_rate * dV),
                              (self.W, self.W - learning_rate * dW)])

    def calculate_total_loss(self, X, Y):
        return np.sum([self.ce_error(x,y) for x,y in zip(X,Y)])

    def calculate_loss(self, X, Y):
        # Divide calculate_loss by the number of words
        num_words = np.sum([len(y) for y in Y])
        return self.calculate_total_loss(X,Y)/float(num_words)   

    def calculate_ppl(self, X, Y):
        num_words = np.sum([len(y) for y in Y])
        #print ""word count: "" + str(num_words)
        logp = np.sum([self.logp(x,y) for x,y in zip(X,Y)])
        ppl = 10 ** (-logp/num_words)
        return ppl, logp


def train_with_sgd(model, X_train, y_train, X_valid, y_valid, learning_rate=0.005, nepoch=1, evaluate_loss_after=5):
    # We keep track of the losses so we can plot them later
    losses = []
    num_examples_seen = 0
    for epoch in range(nepoch):
        # For each training example...
        for i in range(len(y_train)):
            model.sgd_step(X_train[i], y_train[i], learning_rate)
            num_examples_seen += 1

        loss = model.calculate_loss(X_train, y_train)
        losses.append((num_examples_seen, loss))
        time = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')

        loss = model.calculate_loss(X_valid, y_valid)
        ppl, logp = model.calculate_ppl(X_valid, y_valid)

        print ""epoch=%d: log probability=%f ppl=%f"" % (epoch,logp,ppl)
        # Adjust the learning rate if loss increases
        if (len(losses) &gt; 1 and losses[-1][1] &gt; losses[-2][1]):
            learning_rate = learning_rate * 0.5  
            print ""Setting learning rate to %f"" % learning_rate

def load_data():
    print ""load data...""

    train = [(""%s %s %s"" % (sentence_end_token, x.strip(), sentence_end_token)).split() for x in codecs.open(""../ptb/ptb.train.txt.toy"", ""r"", ""UTF-8"")]

    print ""Parsed %d sentences."" % (len(train))

    # Count the word frequencies
    word_freq = nltk.FreqDist(itertools.chain(*train))
    print ""Found %d unique words tokens."" % len(word_freq.items())

    vocab = word_freq.most_common()
    index_to_word = [x[0] for x in vocab]
    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])

    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in train])
    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in train])


    vocabulary_size = len(word_to_index)

    return  X_train, y_train, vocabulary_size + 1 



hidden_dim = 10
sentence_end_token = ""eos""
learning_rate = 0.1
nepoc=25
bptt_truncate=100
model_file=None

# Read the data 
X_train, y_train, vocabulary_size = load_data()
print ""vocabulary_size: "" + str(vocabulary_size)
model = RNNLM(vocabulary_size, hidden_dim=hidden_dim)
train_with_sgd(model, X_train, y_train, X_train, y_train, nepoch=nepoc, learning_rate=learning_rate)
</code></pre>
","language-model"
"35051404","nltk language model TypeError:ngarms() got an unexpected keyword argument 'pad_symbol'","2016-01-28 01:37:35","","1","853","<python><nlp><nltk><n-gram><language-model>","<p>I'm executing the following code:</p>

<pre><code>from nltk.corpus import brown
from nltk.model import Ngram
lm = NgramModel(2, brown.words(categories='news'), estimator=None)
</code></pre>

<p>But I got an error:</p>

<p><a href=""https://i.sstatic.net/h7qqB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/h7qqB.png"" alt=""enter image description here""></a> </p>

<p>I really don't know why I do have this problem; is it a bug from nltk code? Has anyone have an idea of what I am doing wrong?</p>

<p>Thank you in advance.</p>
","language-model"
"34502351","What is the next procedure after creating a CMUSphinx language model with my own dictionary?","2015-12-28 23:34:17","34507262","2","455","<java><dictionary><cmusphinx><language-model>","<p>I have created my own CMUSphinx language model for Arabic language for a software that will be listening to a user and apply commands with my own dictionary that I've done it manually by hand, converted ""arpa"" language model type to ""dmp"" language model using the command <code>sphinx_lm_convert -i ar.lm -o ar.lm.dmp</code>, so here is the files that i have so far:</p>

<ul>
<li>.txt (the commands text file) </li>
<li>.wfreq (freq of words file) </li>
<li>.idngram (ngram file) </li>
<li>.dic (dictionary file) </li>
<li>.phone (phonemes file) </li>
<li>.lm (arpa language model file) </li>
<li>.lm.dmp (Darpa Trigram dump language model file)</li>
</ul>

<p>I then recorded my self of saying each word, each word has a its own .wav file and they are all in one folder that is separate from the folder where .dic, .txt, .lm exists.</p>

<p>My question is what is the next step as i was reading here <a href=""http://cmusphinx.sourceforge.net/wiki/tutorial"" rel=""nofollow"">http://cmusphinx.sourceforge.net/wiki/tutorial</a>?</p>

<p>It says that Adapting existing acoustic model is the next step after building the language model, isn't it training the language model?</p>

<p>And if it is training, i have all the files required except the:</p>

<ul>
<li>.transcription  </li>
<li>.fileids</li>
</ul>

<p>what should be inside these two files?</p>

<p>Thank</p>
","language-model"
"33432085","Correct parameters for wngram2idngram?","2015-10-30 08:58:15","","1","159","<sphinx4><pocketsphinx><language-model>","<p>I am trying to generate the arpa format language model with the following commands:</p>

<pre><code>text2wngram &lt; weather.txt | grep -v ""&lt;/s&gt; &lt;s&gt;"" &gt; weather.wngram
wngram2idngram -vocab weather.vocab &lt; weather.wngram &gt; weather.idngram 
idngram2lm -vocab_type 0 -idngram weather.idngram -vocab weather.vocab -arpa weather.lm
</code></pre>

<p>But second command <strong><em>wngram2idngram</em></strong>  is not working and throwing following error:</p>

<p><strong><em>text2idngram : Error : Must specify idngram file.</em></strong></p>

<p>I change the parameters as follows and it works.</p>

<pre><code>wngram2idngram -vocab weather.vocab -idngram weather.idngram &lt; weather.wngram
</code></pre>

<p>My question is which one is correct?
I am using cmulmtk Version 3.</p>
","language-model"
"33266956","NLTK package to estimate the (unigram) perplexity","2015-10-21 18:48:31","33269399","11","16256","<python-2.7><nlp><nltk><n-gram><language-model>","<p>I am trying to calculate the perplexity for the data I have. The code I am using is: </p>

<pre><code> import sys
 sys.path.append(""/usr/local/anaconda/lib/python2.7/site-packages/nltk"")

from nltk.corpus import brown
from nltk.model import NgramModel
from nltk.probability import LidstoneProbDist, WittenBellProbDist
estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)
lm = NgramModel(3, brown.words(categories='news'), True, False, estimator)
print lm
</code></pre>

<p>But I am receiving the error, </p>

<pre><code>File ""/usr/local/anaconda/lib/python2.7/site-packages/nltk/model/ngram.py"", line 107, in __init__
cfd[context][token] += 1
TypeError: 'int' object has no attribute '__getitem__'
</code></pre>

<p>I have already performed Latent Dirichlet Allocation for the data I have and I have generated the unigrams and their respective probabilities (they are normalized as the sum of total probabilities of the data is 1).</p>

<p>My unigrams and their probability looks like:</p>

<pre><code>Negroponte 1.22948976891e-05
Andreas 7.11290670484e-07
Rheinberg 7.08255885794e-07
Joji 4.48481435106e-07
Helguson 1.89936727391e-07
CAPTION_spot 2.37395965468e-06
Mortimer 1.48540253778e-07
yellow 1.26582575863e-05
Sugar 1.49563800878e-06
four 0.000207196011781
</code></pre>

<p>This is just a fragment of the unigrams file I have. The same format is followed for about 1000s of lines. The total probabilities (second column) summed gives 1. </p>

<p>I am a budding programmer. This ngram.py belongs to the nltk package and I am confused as to how to rectify this. The sample code I have here is from the nltk documentation and I don't know what to do now. Please help on what I can do. Thanks in advance!</p>
","language-model"
"33026460","CMU Sphinx4 - Custom Language Model","2015-10-08 21:44:15","33027132","1","670","<cmusphinx><sphinx4><language-model>","<p>I have a very specific requirement. I am working on an application which will allow users to speak their employee number which is of the format HN56C12345 (any alphanumeric characters sequence) into the app. I have gone through the link: <a href=""http://cmusphinx.sourceforge.net/wiki/tutoriallm"" rel=""nofollow"">http://cmusphinx.sourceforge.net/wiki/tutoriallm</a> but I am not sure if that would work for my usecase.</p>

<p>So my question is three-folds :</p>

<ol>
<li>Can Sphinx4 actually recognize an alphanumeric sequence with high accuracy like an emp number in my case? </li>
<li>If yes, can anyone point me to a concrete example / reference page where someone has built custom language support in Sphinx4 from scratch. I haven't found a detailed step-by-step doc yet on this. Did anyone work on alphanumeric sequence based dictionaries or language models?</li>
<li>How to build an acoustic model for this scenario?</li>
</ol>
","language-model"
"32676183","Using language model in CMU sphnix4 1.0 beta6","2015-09-20 05:36:37","","0","75","<java><nlp><speech-to-text><cmusphinx><language-model>","<p>I am a newbie in Java application development and I am trying to create a sample speech to text application for converting live speech. I tried to use Sphnix4-5prealpha and found it has issue with microphone (<a href=""http://sourceforge.net/p/cmusphinx/bugs/412/"" rel=""nofollow"">http://sourceforge.net/p/cmusphinx/bugs/412/</a>). So I switched back to 1.0 beta 6. I successfully ran the helloWorld and helloNgram programs. I am not sure helloNGram is the right one for me to start with, and even if this is the right one, I have very less idea how to proceed. I cannot find any way to move forward from helloNGram. Can any one please help me with the following two things:</p>

<ol>
<li>From which example should I start?</li>
<li>What will be the high level steps to achieve a generic English speech to text application with good accuracy.  </li>
</ol>
","language-model"
"32643866","How to build a language model from the phonetic transcription?","2015-09-18 04:25:31","","0","576","<speech-recognition><cmusphinx><pocketsphinx><language-model>","<p>I constructed a language model for the language tamil using data from wikipedia dumps ,using the tool CMUCLMTK.Now , how do I generate the phoenetic transcription and replace them in the model.The wiki article (<a href=""http://cmusphinx.sourceforge.net/wiki/phonemerecognition"" rel=""nofollow"">http://cmusphinx.sourceforge.net/wiki/phonemerecognition</a>) says to replace the transcription instead of words .What am I supposed to do now?</p>
","language-model"
"31847682","How to compute skipgrams in python?","2015-08-06 05:44:34","31886292","20","13055","<python><nlp><n-gram><language-model>","<p>A k <a href=""http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf"" rel=""noreferrer"">skipgram</a> is an ngram which is a superset of all ngrams and each  (k-i )skipgram till (k-i)==0 (which includes 0 skip grams). So how to efficiently compute these skipgrams in python?</p>

<p>Following is the code i tried but it is not doing as expected:</p>

<pre><code>&lt;pre&gt;
    input_list = ['all', 'this', 'happened', 'more', 'or', 'less']
    def find_skipgrams(input_list, N,K):
  bigram_list = []
  nlist=[]

  K=1
  for k in range(K+1):
      for i in range(len(input_list)-1):
          if i+k+1&lt;len(input_list):
              nlist=[]
              for j in range(N+1):
                  if i+k+j+1&lt;len(input_list):
                    nlist.append(input_list[i+k+j+1])

          bigram_list.append(nlist)
  return bigram_list

&lt;/pre&gt;
</code></pre>

<p>The above code is not rendering correctly, but print <code>find_skipgrams(['all', 'this', 'happened', 'more', 'or', 'less'],2,1)</code> gives following output</p>

<blockquote>
  <p>[['this', 'happened', 'more'], ['happened', 'more', 'or'], ['more',
  'or', 'less'], ['or', 'less'], ['less'], ['happened', 'more', 'or'],
  ['more', 'or', 'less'], ['or', 'less'], ['less'], ['less']]</p>
</blockquote>

<p>The code listed here also does not give correct output:
<a href=""https://github.com/heaven00/skipgram/blob/master/skipgram.py"" rel=""noreferrer"">https://github.com/heaven00/skipgram/blob/master/skipgram.py</a></p>

<p>print skipgram_ndarray(""What is your name"")  gives:
['What,is', 'is,your', 'your,name', 'name,', 'What,your', 'is,name']</p>

<p>name is a unigram!</p>
","language-model"
"31772479","Convert ngrams count files into ARPA format","2015-08-02 13:42:08","","0","945","<speech-recognition><n-gram><language-model><srilm>","<p>I want to convert all my n-grams files into one <a href=""https://web.archive.org/web/20200215083618/http://www1.icsi.berkeley.edu:80/Speech/docs/HTKBook3.2/node213_mn.html"" rel=""nofollow noreferrer"">ARPA</a> file. It will be used as a Language Model for speech recognition.</p>
<p>I have different n-grams files, 2-grams, 3-grams and 4-grams. Taking 2-grams file as example</p>
<p><code>two grams -- frequency similar degree  32 Writing writes  1 towars their    3 country feature 1 like gold   446 like golf   64</code></p>
<p>How can I achieve this?</p>
","language-model"
"31327126","Accessing terms statistics in Lucene 4","2015-07-09 20:02:07","31332326","2","745","<java><lucene><information-retrieval><language-model>","<p>I have a Lucene index, and I need to access some statistics such as term collection frequency. <code>BasicStats</code> class has this information, however, I could not understand whether this class is accessible. </p>

<p>Is it possible to access <code>BasicStats</code> class in Lucene 4?</p>
","language-model"
"31010166","Why is my Sphinx4 Recognition poor?","2015-06-23 18:03:12","31025378","1","445","<eclipse><speech-recognition><cmusphinx><sphinx4><language-model>","<p>I am learning how to use Sphinx4 using the Maven plug-in for Eclipse. </p>

<p>I took the transcribe demo found on GitHub and altered it to process a file of my own. The audio file is 16bit, mono, 16khz. It is approximately 13 seconds long. I noticed that it sounds like it is in slow motion. </p>

<p>The words spoken in the file are, ""also make sure it's easy for you to access the recording files so you could upload it if asked"". </p>

<p>I am attempting to transcribe the file and my results are horrendous. My attempts at finding forum posts or links that thoroughly explain how to improve the results, or what I am not doing correctly have lead me no where. </p>

<p>I am looking to strengthen the accuracy of the transcription, but would like to avoid having to train a model myself due to the variance in the type of data that my current project will have to deal with. Is this not possible, and is the code I am using off?</p>

<p><strong>CODE</strong> </p>

<p>(NOTE: Audio file available at <a href=""https://instaud.io/8qv"" rel=""nofollow"">https://instaud.io/8qv</a>)</p>

<pre><code>public class App {

public static void main(String[] args) throws Exception {
    System.out.println(""Loading models..."");

    Configuration configuration = new Configuration();

    // Load model from the jar
    configuration
            .setAcousticModelPath(""resource:/edu/cmu/sphinx/models/en-us/en-us"");

    // You can also load model from folder
    // configuration.setAcousticModelPath(""file:en-us"");

    configuration
            .setDictionaryPath(""resource:/edu/cmu/sphinx/models/en-us/cmudict-en-us.dict"");
    configuration
            .setLanguageModelPath(""resource:/edu/cmu/sphinx/models/en-us/en-us.lm.dmp"");

    StreamSpeechRecognizer recognizer = new StreamSpeechRecognizer(
            configuration);
    FileInputStream stream = new FileInputStream(new File(""/home/tmscanlan/workspace/example/vocaroo_test_revised.wav""));
   // stream.skip(44); I commented this out due to the short length of my file

    // Simple recognition with generic model
    recognizer.startRecognition(stream);
    SpeechResult result;

    while ((result = recognizer.getResult()) != null) {
        // I added the following print statements to get more information
        System.out.println(""\ngetWords() before loop: "" + result.getWords());
        System.out.format(""Hypothesis: %s\n"", result.getHypothesis());
        System.out.print(""\nThe getResult(): "" + result.getResult() 
                + ""\nThe getLattice(): "" + result.getLattice()); 

        System.out.println(""List of recognized words and their times:"");
        for (WordResult r : result.getWords()) {
            System.out.println(r);
        }

        System.out.println(""Best 3 hypothesis:"");
        for (String s : result.getNbest(3))
            System.out.println(s);

    }
    recognizer.stopRecognition();

    // Live adaptation to speaker with speaker profiles


    stream = new FileInputStream(new File(""/home/tmscanlan/workspace/example/warren_test_smaller.wav""));
   // stream.skip(44); I commented this out due to the short length of my file

    // Stats class is used to collect speaker-specific data
    Stats stats = recognizer.createStats(1);
    recognizer.startRecognition(stream);
    while ((result = recognizer.getResult()) != null) {
        stats.collect(result);
    }
    recognizer.stopRecognition();

    // Transform represents the speech profile
    Transform transform = stats.createTransform();
    recognizer.setTransform(transform);

    // Decode again with updated transform
    stream = new FileInputStream(new File(""/home/tmscanlan/workspace/example/warren_test_smaller.wav""));
   // stream.skip(44); I commented this out due to the short length of my file
    recognizer.startRecognition(stream);
    while ((result = recognizer.getResult()) != null) {
        System.out.format(""Hypothesis: %s\n"", result.getHypothesis());
    }
    recognizer.stopRecognition();


    System.out.println(""...Printing is done.."");
}
}
</code></pre>

<p>Here is the output (a photo album I took): <a href=""https://i.sstatic.net/RWkLy.jpg"" rel=""nofollow"">https://i.sstatic.net/RWkLy.jpg</a></p>
","language-model"
"30710148","command line parameter in word2vec","2015-06-08 13:14:50","","3","3006","<nlp><word2vec><language-model>","<p>I want to use word2vec to create my own word vector corpus with the current version of the english wikipedia, but I can't find an explanation of the command line parameter for using that program. In the demp-script you can find following:<br>
(text8 is an old wikipedia corpus of 2006)</p>

<pre><code>make
if [ ! -e text8 ]; then
wget http://mattmahoney.net/dc/text8.zip -O text8.gz
gzip -d text8.gz -f
fi
time ./word2vec -train text8 -output vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15
./distance vectors.bin
</code></pre>

<p>What is the meaning of the command line parameter:<br>
<code>vectors.bin -cbow 1 -size 200 -window 8 -negative 25 -hs 0 -sample 1e-4 -threads 20 -binary 1 -iter 15</code></p>

<p>And what are the most suitable values when I have a wikipedia text corpus of around 20GB(.txt file)? I read that for bigger corpora a vector size of 300 or 500 would be better.</p>
","language-model"
"30640125","Is likelihood calculated over the whole training set or a single example?","2015-06-04 09:30:41","30642858","1","1599","<machine-learning><probability><mle><language-model>","<p>Suppose I have a training set of <code>(x, y)</code> pairs, where <code>x</code> is the input example and <code>y</code> is the corresponding target and <code>y</code> is a value <code>(1 ... k)</code> (<code>k</code> is the number of classes).</p>

<p>When calculating the likelihood of the training set, should it be calculated for the <em>whole</em> training set (all of the examples), that is:</p>

<pre><code>L = P(y | x) = p(y1 | x1) * p(y2 | x2) * ...
</code></pre>

<p>Or is the likelihood computed for a specific training example <code>(x, y)</code>?</p>

<p>I'm asking because I saw these <a href=""http://web.engr.oregonstate.edu/~xfern/classes/cs534/notes/logistic-regression-note.pdf"" rel=""nofollow noreferrer"">lecture notes</a> (page 2), where he seems to calculate L_i, that is the likelihood for every training example separately.</p>
","language-model"
"29915993","What is the most efficient way of storing language models in NLP applications?","2015-04-28 09:49:54","29916681","2","1418","<nlp><n-gram><language-model>","<p>How do they usually store and update language models (such as N-gram models)? What kind of structure is the most efficient way for storing these models in databases?</p>
","language-model"
"29869607","How to tune a Machine Translation model with huge language model?","2015-04-25 19:20:28","","5","1512","<nlp><n-gram><machine-translation><moses><language-model>","<p><code>Moses</code> is a software to build machine translation models. And <code>KenLM</code> is the defacto language model software that moses uses.</p>

<p>I have a textfile with 16GB of text and i use it to build a language model as such:</p>

<pre><code>bin/lmplz -o 5 &lt;text &gt; text.arpa
</code></pre>

<p>The resulting file (<code>text.arpa</code>) is 38GB. Then I binarized the language model as such:</p>

<pre><code>bin/build_binary text.arpa text.binary
</code></pre>

<p>And the binarized language model (<code>text.binary</code>) grows to 71GB.</p>

<p>In <code>moses</code>, after training the translation model, you should tune the weights of the model by using <code>MERT</code> algorithm. And this can simply be done with <a href=""https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/mert-moses.pl"">https://github.com/moses-smt/mosesdecoder/blob/master/scripts/training/mert-moses.pl</a>. </p>

<p>MERT works fine with small language model but with the big language model, it takes quite some days to finish. </p>

<p>I did a google search and found KenLM's filter, which promises to filter the language model to a smaller size: <a href=""https://kheafield.com/code/kenlm/filter/"">https://kheafield.com/code/kenlm/filter/</a></p>

<p>But i'm clueless as to how to make it work. The command help gives:</p>

<pre><code>$ ~/moses/bin/filter
Usage: /home/alvas/moses/bin/filter mode [context] [phrase] [raw|arpa] [threads:m] [batch_size:m] (vocab|model):input_file output_file

copy mode just copies, but makes the format nicer for e.g. irstlm's broken
    parser.
single mode treats the entire input as a single sentence.
multiple mode filters to multiple sentences in parallel.  Each sentence is on
    a separate line.  A separate file is created for each sentence by appending
    the 0-indexed line number to the output file name.
union mode produces one filtered model that is the union of models created by
    multiple mode.

context means only the context (all but last word) has to pass the filter, but
    the entire n-gram is output.

phrase means that the vocabulary is actually tab-delimited phrases and that the
    phrases can generate the n-gram when assembled in arbitrary order and
    clipped.  Currently works with multiple or union mode.

The file format is set by [raw|arpa] with default arpa:
raw means space-separated tokens, optionally followed by a tab and arbitrary
    text.  This is useful for ngram count files.
arpa means the ARPA file format for n-gram language models.

threads:m sets m threads (default: conccurrency detected by boost)
batch_size:m sets the batch size for threading.  Expect memory usage from this
    of 2*threads*batch_size n-grams.

There are two inputs: vocabulary and model.  Either may be given as a file
    while the other is on stdin.  Specify the type given as a file using
    vocab: or model: before the file name.  

For ARPA format, the output must be seekable.  For raw format, it can be a
    stream i.e. /dev/stdout
</code></pre>

<p>But when I tried the following, it gets stuck and does nothing:</p>

<pre><code>$ ~/moses/bin/filter union lm.en.binary lm.filter.binary
Assuming that lm.en.binary is a model file
Reading lm.en.binary
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
</code></pre>

<p><strong>What should one do to the Language Model after binarization? Is there any other steps to manipulate large language models to reduce the
computing load when tuning?</strong></p>

<p><strong>What is the usual way to tune on a large LM file?</strong></p>

<p><strong>How to use KenLM's filter?</strong></p>

<p>(more details on <a href=""https://www.mail-archive.com/moses-support@mit.edu/msg12089.html"">https://www.mail-archive.com/moses-support@mit.edu/msg12089.html</a>)</p>
","language-model"
"29804322","how to treat with <s> and </s> in calculating unigram LM?","2015-04-22 17:02:21","29806020","2","357","<nlp><language-model>","<p>I am beginner in NLP and I'm confused how to treat with <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> symbols to calculate counts for unigram model? should I count them or just ignore?</p>
","language-model"
"29151773","n-gram probability count in ARPA file","2015-03-19 17:56:43","","1","849","<nlp><n-gram><language-model>","<p>I start working on a problem related with language modelling, but some calculation does not clear to me. For example consider the following simple text:</p>

<pre><code>I am Sam Sam I am I do not like green eggs and ham
</code></pre>

<p>I have used berkelylm to create the n-gram probability count and the ARPA file. Here is the generated ARPA file:</p>

<pre><code>\data\
ngram 1=12
ngram 2=14
ngram 3=14
ngram 4=13
ngram 5=12
ngram 6=11
ngram 7=10
ngram 8=0
ngram 9=0

\1-grams:
-1.146128   am  -0.062148
-1.146128   like    -0.062148
-1.146128   not -0.062148
-99.000000  &lt;s&gt; -0.062148
-1.146128   green   -0.062148
-1.146128   and -0.062148
-0.669007   I   -0.238239
-0.845098   Sam -0.062148
-1.146128   &lt;/s&gt;
-1.146128   ham -0.062148
-1.146128   eggs    -0.062148
-1.146128   do  -0.062148

\2-grams:
-0.720159   am Sam
-0.597943   Sam I
-0.709435   and ham
-0.709435   not like
-0.709435   like green
-0.720159   Sam Sam
-0.709435   ham &lt;/s&gt;
-0.709435   green eggs
-0.496144   &lt;s&gt; I
-0.377737   I am
-0.597943   am I
-0.709435   do not
-0.709435   eggs and
-1.066947   I do

\3-grams:
-0.597943   Sam Sam I
-0.377737   &lt;s&gt; I am
-0.709435   do not like
-0.720159   I am Sam
-1.066947   am I do
-0.377737   Sam I am
-0.709435   green eggs and
-0.709435   like green eggs
-0.597943   I am I
-0.709435   eggs and ham
-0.709435   and ham &lt;/s&gt;
-0.709435   I do not
-0.709435   not like green
-0.720159   am Sam Sam
</code></pre>

<p>the probability count for the 1-grams are clear me, but it is not clear to me how the 2-grams and 3-grams data are created. 
There are a total of 13 bigrams there and the bigram ""I am"" appears two times So, 2-gram probability count for ""I am"" should be log(2/13) or -0.81291, in log scale, but it is -0.37 in the generated file).</p>

<p>I might missing something because of my lack of experience, but I would appreciate an example to explain a calculation.</p>

<p>Thanks.</p>
","language-model"
"27552886","KenLM perplexity weirdness","2014-12-18 18:01:40","","1","954","<nlp><language-model>","<p>I have 96 files each containing ~10K lines of English text (tokenized, downcased).  If I loop through the files (essentially doing k-fold cross-validation with k=#files) and build a LM (using bin/lmplz) for 95 and run bin/query on the held out file against it, I see a PPL (including OOVs) of 1.0 every time.  But if I run a file against an LM built with all 96 files (so test doc is included in building the LM), I get a PPL of 27.8.</p>

<p>I have more experience with SRILM than KenLM, but I've never seen a perplexity score of 1.  Something feels wrong about that.  Even if I accepted that and attributed it to the test document's sentences occurring in other training data, it wouldn't explain why when I ensure that the test data is included in the training data I get a higher score.  What's going on? </p>

<p>=============================</p>

<p>this also seems strange:</p>

<pre><code>Perplexity including OOVs:  1
Perplexity excluding OOVs:  0.795685
OOVs:   0
</code></pre>
","language-model"
"27173193","Using theano to implement maximum likelihood learning in neural probability language model Python","2014-11-27 14:41:08","","1","474","<python><language-model>","<p>I'm trying to implement maximum likelihood learning for neural probability language model in python from code of log-bilinear model:
<a href=""https://github.com/wenjieguan/Log-bilinear-language-models/blob/master/lbl.py"" rel=""nofollow"">https://github.com/wenjieguan/Log-bilinear-language-models/blob/master/lbl.py</a></p>

<p>I used grad function in theano to compute gradient and try using function train to update parameters of model, but it got errors. Here is my code:</p>

<pre><code>def train(self, sentences, alpha = 0.001, batches = 1000):
    print('Start training...')
    self.alpha = alpha
    count = 0

    RARE = self.vocab['&lt;&gt;']
    #print RARE
    q = np.zeros(self.dim, np.float32)
    #print q
    delta_context = [np.zeros((self.dim, self.dim), np.float32) for i in range(self.context) ]
    #print delta_context
    delta_feature = np.zeros((len(self.vocab), self.dim), np.float32)
    #print delta_feature
    for sentence in sentences:
        sentence = self.start_sen + sentence + self.end_sen
        for pos in range(self.context, len(sentence) ):
            count += 1
            q.fill(0)
            featureW = []
            contextMatrix = []
            indices = []
            for i, r in enumerate(sentence[pos - self.context : pos]):
                if r == '&lt;_&gt;':
                    continue
                index = self.vocab.get(r, RARE)
                print index
                indices.append(index)
                ri = self.featureVectors[index]
                #print ri
                ci = self.contextMatrix[i]
                #print ci
                featureW.append(ri)
                contextMatrix.append(ci)
                #Caculating predicted representation for the target word
                q += np.dot(ci, ri)
            #Computing energy function
            energy = np.exp(np.dot(self.featureVectors, q) + self.biases)
            #print energy
            #Computing the conditional distribution
            probs = energy / np.sum(energy)
            #print probs
            w_index = self.vocab.get(sentence[pos], RARE)


            #Computing gradient
            logProbs = T.log(probs[w_index])
            print 'Gradient start...'
            delta_context, delta_feature = T.grad(logProbs, [self.contextMatrix, self.featureVectors])
            print 'Gradient completed!'
            train = theano.function(
                                    inputs = [self.vocab],
                                    outputs = [logProbs],
                                    updates=((self.featureVectors,self.featureVectors - self.alpha * delta_feature), 
                                             (self.contextMatrix,self.contextMatrix - self.alpha * delta_context)),
                                    name=""train""
                                    )



    print('Training is finished!')
</code></pre>

<p>I have just learnt about Python and neural probability language model, so it is quite difficult to me.
Could you help me, please! Thank you!</p>
","language-model"
"24293394","Need to understand the output format of kenlm querying","2014-06-18 19:13:59","","3","2789","<nlp><n-gram><language-model>","<p>kenlm paper seems good for LM. I feel that minimal documentation is given, felt difficulty in understanding. </p>

<p>So, as part of understanding kenlm, I need to understand the output format of querying the model. Please do provide some detail on it.</p>

<p>I couldn't tag correctly on lm, kenlm as tags are not available.</p>

<p><strong>Details:</strong></p>

<p>Executed:</p>

<pre><code>bin/query trainingdata.binary &lt; temp.txt
</code></pre>

<p>Output:</p>

<pre><code>city=274 2 -3.71333 &lt;/s&gt;=2 1 -0.914832  Total: -4.62817 OOV: 0

new=1037 2 -2.64194 york=2124 2 -2.27023    &lt;/s&gt;=2 1 -0.867251  Total: -5.77943 OOV: 0

samsung=3 2 -2.39176    galaxy=4 3 -0.193832    s5=5 4 -0.536524    &lt;/s&gt;=2 5 -0.595418  Total: -3.71753 OOV: 0

fingers=6 2 -4.25789    crossed=7 3 -1.00535    samsung=3 4 -0.766757   &lt;/s&gt;=2 5 -0.757035  Total: -6.78703 OOV: 0

jessica=8 2 -3.77437    simpson=9 3 -0.45866    collection=10 4 -1.24209    &lt;/s&gt;=2 5 -0.144034  Total: -5.61916 OOV: 0

plexus=11 2 -4.46277    slim=12 3 -0.804323 &lt;/s&gt;=2 4 -0.606899  Total: -5.87399 OOV: 0

under=13 2 -3.23437 armour=14 3 -0.575785   outlet=15 4 -1.32109    &lt;/s&gt;=2 5 -0.18898   Total: -5.32022 OOV: 0

amazon=16 2 -2.05178    seller=17 3 -2.5683 central=18 4 -0.94366   &lt;/s&gt;=2 5 -0.643415  Total: -6.20716 OOV: 0

garcinia=19 2 -2.6464   cambogia=20 3 -0.101819 reviews=21 4 -1.86317   &lt;/s&gt;=2 5 -0.0987858 Total: -4.71017 OOV: 0

womens=22 2 -3.10627    organic=23 3 -1.64262   lube.=24 4 -1.12123 &lt;/s&gt;=2 5 -0.505745  Total: -6.37587 OOV: 0

doc=25 2 -3.00747   mcstuffins=26 3 -0.130808   &lt;/s&gt;=2 4 -0.485077  Total: -3.62336 OOV: 0
&lt;/s&gt;=2 1 -0.975736  Total: -0.975736 OOV: 0

Perplexity including OOVs:  30.9347

Perplexity excluding OOVs:  30.9347
OOVs:   0

Total time including destruction:

Name:query  VmPeak:30664 kB VmRSS:1748 kB   RSSMax:3132 kB  user:0.000999   sys:0   CPU:0.000999    real:0.000817598
</code></pre>
","language-model"
"23862526","Python interface to ARPA files","2014-05-26 04:05:03","24206287","10","2652","<python><nlp><n-gram><language-model>","<p>I'm looking for a pythonic interface to load ARPA files (back-off language models) and use them to evaluate some text, e.g. get its log-probability, perplexity etc.</p>

<p>I don't need to generate the ARPA file in Python, only to use it for querying.</p>

<p>Does anybody have a recommended package?
I already saw <a href=""https://github.com/kpu/kenlm"">kenlm</a> and <a href=""https://github.com/desilinguist/swig-srilm"">swig-srilm</a>, but the first is very hard to set up in Windows and the second seems un-maintained anymore.</p>
","language-model"
"16408163","ARPA language model documentation","2013-05-06 22:14:08","16412537","18","11353","<nlp><speech-recognition><cmusphinx><sphinx4><language-model>","<p>Where can I find documentation on ARPA language model format?</p>

<p>I am developing simple speech recognition app with pocket-sphinx STT engine. ARPA is recommended there for performance reasons.
I want to understand how much can I do to adjust my language model for my custom needs.</p>

<p>All I found is some very brief ARPA format descriptions:</p>

<ul>
<li><a href=""http://kered.org/blog/2008-08-12/arpa-language-model-file-format/"" rel=""noreferrer"">http://kered.org/blog/2008-08-12/arpa-language-model-file-format/</a></li>
<li><a href=""http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html"" rel=""noreferrer"">http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html</a></li>
<li><a href=""http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.html"" rel=""noreferrer"">http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.html</a></li>
</ul>

<p>I am beginner to STT and I have trouble to wrap head around this (n-grams, etc...). I am looking for more detailed docs. Something like documentation on JSGF grammar here:</p>

<p><a href=""http://www.w3.org/TR/jsgf/"" rel=""noreferrer"">http://www.w3.org/TR/jsgf/</a></p>
","language-model"
"7377972","How to use arpa file in voice recognition","2011-09-11 11:45:04","7655927","2","632","<voice-recognition><cmusphinx><language-model>","<p>I have created a ARPA file from a text file using CMU SLM toolkit.</p>

<p>Currently I don't know how to use the generated ARPA file in my project instead of <code>.lm</code> and <code>.dic</code> file.</p>

<p>If any one knows about that please let me know.</p>
","language-model"
"5743390","Creating ARPA language model file with 50,000 words","2011-04-21 11:24:07","6356792","14","7802","<speech-recognition><cmusphinx><n-gram><language-model>","<p>I want to create an ARPA language model file with nearly 50,000 words. I can't generate the language model by passing my text file to the CMU Language Tool. Is any other link available where I can get a language model for these many words?</p>
","language-model"
"5220661","Building openears compatible language model","2011-03-07 14:08:44","6738300","17","6814","<iphone><speech-recognition><language-model>","<p>I am doing some development on speech to text and text to speech and I found the <a href=""http://www.politepix.com/openears/gettingstarted/"" rel=""nofollow noreferrer"">OpenEars</a> API very useful.</p>

<p>The principle of this <a href=""http://www.speech.cs.cmu.edu/SLM_info.html"" rel=""nofollow noreferrer"">cmu-slm</a> based API is it uses a language model to map the speech listened by the iPhone device. So I decided to find a big English language model to feed the API speech recognizer engine. But I failed to understand the format of the voxfourge english data model to use with OpenEars.</p>

<p>Do anyone have any idea that how can I get the .languagemodel and .dic file for English language to work with OpenEars?</p>
","language-model"
"5142952","Sphinx 4 corrupted ARPA LM?","2011-02-28 14:03:23","5149124","1","1194","<speech-recognition><speech-to-text><n-gram><sphinx4><language-model>","<p>I have an ARPA LM generated by <a href=""http://www.phontron.com/kylm/"" rel=""nofollow"">kylm</a>, when running SPHINX I get this exception stack trace:</p>

<pre><code>Exception in thread ""main"" java.lang.RuntimeException: Allocation of search manager resources failed
        at edu.cmu.sphinx.decoder.search.WordPruningBreadthFirstSearchManager.allocate(WordPruningBreadthFirstSearchManager.java:242)
        at edu.cmu.sphinx.decoder.AbstractDecoder.allocate(AbstractDecoder.java:87)
        at edu.cmu.sphinx.recognizer.Recognizer.allocate(Recognizer.java:168)
        at transcribing.Main.main(Main.java:78)
Caused by: java.io.IOException: Corrupt Language Model file:./corpus.arpa at line 2420:Premature EOF
        at edu.cmu.sphinx.linguist.language.ngram.SimpleNGramModel.corrupt(SimpleNGramModel.java:458)
        at edu.cmu.sphinx.linguist.language.ngram.SimpleNGramModel.readLine(SimpleNGramModel.java:404)
        at edu.cmu.sphinx.linguist.language.ngram.SimpleNGramModel.load(SimpleNGramModel.java:307)
        at edu.cmu.sphinx.linguist.language.ngram.SimpleNGramModel.allocate(SimpleNGramModel.java:110)
        at edu.cmu.sphinx.linguist.lextree.LexTreeLinguist.allocate(LexTreeLinguist.java:342)
        at edu.cmu.sphinx.decoder.search.WordPruningBreadthFirstSearchManager.allocate(WordPruningBreadthFirstSearchManager.java:238)
        ... 3 more
Java Result: 1
</code></pre>

<p>Here's an excerpt of the ARPA LM:</p>

<pre><code>[n]
3

[smoother]
kylm.model.ngram.smoother.KNSmoother

[closed]
true

[max_length]
1091

[vocab_cutoff]
0

[start_symbol]
&lt;s&gt;

[terminal_symbol]
&lt;/s&gt;

[unknown_symbol]
&lt;unk&gt;

\data\
ngram 1=406
ngram 2=768
ngram 3=937
\1-grams: 
-99.0000    &lt;s&gt; -0.3630
...
...

\end\
</code></pre>

<p><strong>PS</strong>: <em>there is a new line after <code>\end\</code></em></p>

<p>The exeption says that SPHINX is encountering an unexpected EOF on the last line (isn't it supposed to encounter an EOF there ??)</p>

<p>Please any help !</p>
","language-model"
"5127640","do searching in a very big ARPA file in a very short time in java","2011-02-26 15:05:08","","2","530","<java><n-gram><language-model>","<p>I have an ARPA file which is almost 1 GB. I have to do searching in it in less than 1 minute. I have searched a lot, but I have not found the suitable answer yet. I think I do not have to read the whole file. I just have to jump to a specific line in the file and read the whole line. The lines of the ARPA file do not have the same length. I have to mention that ARPA files have a specific format.</p>

<h2>File Format</h2>

<pre>
\data\

ngram 1=19

ngram 2=234

ngram 3=1013

\1-grams:

-1.7132 puluh -3.8008

-1.9782 satu -3.8368

\2-grams:

-1.5403 dalam dua -1.0560

-3.1626 dalam ini 0.0000

\3-grams:

-1.8726 itu dan tiga

-1.9654 itu dan untuk

\end\
</pre>

<p>As you see in the sample file I have 19 lines of 1-grams, 234 lines of 2-grams and 1013 lines of 3-grams. I give the string part of the line to the program and get the numbers which are at the left and at the right side of the string. The input string can help me to know in which part of the file I have to do searching.I have to find a way not to read the file completely, because my file is very big and reading the whole file takes a lot of time. I think it is a good way to jump to the specific line in the file without using the index file and access to the whole line.</p>

<p>It will be great if you can help me to do my assignment.</p>
","language-model"
"3299960","Language Modelling toolkit","2010-07-21 13:52:04","","2","1233","<java><python><machine-learning><language-model>","<p>I would like to build a <a href=""http://en.wikipedia.org/wiki/Language_model"" rel=""nofollow noreferrer"">language model</a> for a text corpus. Are there good out-of-the-box toolkits which will alleviate my task? The only toolkit I know off is the Statistical Language Modelling(SLM) Toolkit by CMU.</p>

<p>Regards,</p>
","language-model"