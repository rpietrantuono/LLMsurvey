Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"46502","How much resources do I need to develop my own GPT?","2024-08-12 21:35:16","","0","44","<chatgpt><gpt>","<p>I have an idea for a GPT.  While ideally I can use something like ChatGPT or ClaudeGPT or something else, I want to my GPT to have a specific tone when providing responses.  <strong>This part is very important.</strong></p>
<p>Would it make more sense to use a prebuilt GPT and somehow get it to respond in the right way?  OR, do I have to create my GPT from scratch?</p>
<p>Hence, I'm wondering how difficult/resources it would take for me to develop my GPT?  To see if it's a viable option.</p>
","chatgpt"
"46450","How to use llama-cpp-python to manually pick the next tokens?","2024-08-07 04:55:38","","0","22","<large-language-models><chatgpt><huggingface>","<p>When I use llama-cli, I ask models questions and they generate tokens. I see the tokens appear as they model generates them. The model randomly selects the tokens based on the random seed.</p>
<p>But what I want to do is to see a list of tokens, and then I select the tokens from the list, then let the LLM continue to prompt me another list of next-tokens, and so on, so that I construct my own sentences.</p>
<p>Is there a way to do this with llama-cpp-python?</p>
<hr />
<p><a href=""https://ai.stackexchange.com/questions/46106/is-it-possible-to-use-llms-by-manually-picking-the-tokens"">That</a> is related, but not specific to llama-cpp-python. This is the same, except specific to llama-cpp.</p>
","chatgpt"
"46407","Ways to implement AI table search","2024-08-02 22:50:20","","0","21","<python><chatgpt>","<p>I am new to this matter, so I ask for your help. I have a table with 3 columns: ID, title and description.I want to implement an AI search through data and return ID of the record, if one was found. Searching by equality, occurrence or full text doesn't work for me (not in all cases) and here's why. For example, I'm looking for a &quot;working days reference&quot; - for this request I should get a &quot;working calendar&quot; Because the description states that the current entity contains data about &quot;working days and weekends in a year.&quot; How I can implement such a thing on the most minimalistic models? I wanna create local solutions without internet access after build</p>
<p>What i already did:
I'm just comparing the required query string with the description and title columns, using ollama and gemma2b (python) I have more than 10,000 records in the table and doing this row by row is very expensive. Loading all the data into the model may be stupid and such a long process will simply refuse to work.</p>
<p>At the end I want to make it simple API Service, and i know how to do this, but i have problem with understanding AI</p>
","chatgpt"
"46388","Edit a specific part of image uploaded to ChatGPT (NOT an image generated)","2024-07-31 21:59:30","","0","11","<image-processing><chatgpt>","<p>If I click on an image generated by ChatGPT, I can edit part of it by selecting that area and describing with text the change wanted by clicking here:</p>
<p><a href=""https://i.sstatic.net/M6SLeGqpm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/M6SLeGqpm.png"" alt=""screenshot edit button"" /></a></p>
<p>But if I upload my own image, I don't see anyway to select an area of it only, so I can edit an area of it only.</p>
","chatgpt"
"46318","How many processors are required to run ChatGPT?","2024-07-24 08:14:01","","0","75","<chatgpt><hardware>","<p>I often find studies estimating the number of GPUs required to deploy a deep learning model. <a href=""https://arxiv.org/pdf/2104.04473"" rel=""nofollow noreferrer"">This paper</a>, for example, estimates that ChatGPT would have required about 1024 GPUs in order to be trained over a month.</p>
<p>Why do they only talk about the number of GPUs? Why do they not mention CPUs or TPUs (which presumably were used since it is a deep learning model)?</p>
","chatgpt"
"46106","Is it possible to use LLMs by manually picking the tokens?","2024-06-29 22:49:33","46107","1","70","<large-language-models><chatgpt><huggingface>","<p>Suppose that we give a prompt to an LLM model, such as <em>&quot;what is a banana?&quot;</em>, the LLM would start writing spitting tokens out, out of a space of tokens, until it manages to complete a textual output that resembles an answer to the question, right?</p>
<p>My question is:</p>
<ul>
<li>Can I, at each step, get a prompt of possible the <em>&quot;next&quot;</em> token, then choose it myself? How?</li>
</ul>
","chatgpt"
"45890","How ChatGPT pass from a prompt to a predicted word?","2024-06-04 23:19:57","","1","32","<chatgpt>","<p>There is something that i can't get it, given a prompt input to ChatGPT, this is One Hot Encoded, Embedded, Positional Encoded and so on. Anyway we have a matrix, still after attention mechanism we have a matrix, how  we end with a probability vector and not with a probability matrix?</p>
","chatgpt"
"45638","Best Searchbots that explain code","2024-05-08 05:47:57","","0","17","<large-language-models><research><chatgpt><chat-bots><programming-languages>","<p>Of all the LLM based available searchbots which ones are best at explaining code from Java, Python, Golang or Javascript? ChatGPT, Bing Copilot, Claude, Groq or Le Mistral?</p>
","chatgpt"
"45567","How does ChatGPT assign confidence level?","2024-04-28 19:29:47","","0","78","<natural-language-processing><chatgpt><question-answering>","<p>How does ChatGPT assign confidence level to its answer? What is the mathematical description of it?</p>
<p>Here is my guess. It produces many answers from auto-regression. It already has a probability assigned to each of its outputs. It can just output this probability or some function of this probability for the chosen output. Is this correct?</p>
","chatgpt"
"45281","Unable to Save Generated Data to JSONL File - Always Resulting in ""Wrote 0 examples to finetuning_events.jsonl"" Message","2024-03-29 00:20:39","","0","19","<natural-language-processing><chatgpt><open-ai><fine-tuning>","<h3>Issue Description</h3>
<p>When attempting to generate JSONL data using Llama Index, the process works well until the final step where the results are saved to a JSONL file. However, every time I try to save the data, it seems to be unsuccessful as I always receive the message &quot;Wrote 0 examples to finetuning_events.jsonl&quot;. I am unsure of the reason behind this issue.</p>
<h3>Steps to Reproduce</h3>
<ol>
<li>Successfully generated JSONL data using Llama Index.</li>
<li>Attempted to save the results to a JSONL file.</li>
<li>Received the message &quot;Wrote 0 examples to finetuning_events.jsonl&quot;.</li>
</ol>
<h3>Additional Information</h3>
<ul>
<li>Llama Index version used: 0.10.22</li>
<li>Operating System: Windows</li>
</ul>
<h3>Log</h3>
<p><code>Wrote 0 examples to ./dataset_data/finetuning_events.jsonl</code></p>
<p>My code:</p>
<pre><code>     def jsonl_generation(self):
        &quot;&quot;&quot;
        Generate JSONL file for fine-tuning events and perform model refinement.
        &quot;&quot;&quot;
        # Initialize OpenAI FineTuningHandler and CallbackManager
        finetuning_handler = OpenAIFineTuningHandler()
        callback_manager = CallbackManager([finetuning_handler])

        self.llm.callback_manager = callback_manager

        # Load questions for fine-tuning from a file
        questions = []
        with open(f'{self.dataset_path}/train_questions.txt', &quot;r&quot;, encoding='utf-8') as f:
            for line in f:
                questions.append(line.strip())

        try:
            # Generate responses to the questions using GPT-4 and save the fine-tuning events to a JSONL file
            index = VectorStoreIndex.from_documents(
                self.documents
            )
            query_engine = index.as_query_engine(similarity_top_k=2, llm=self.llm)
            for question in questions:
                response = query_engine.query(question)
        except Exception as e:
            # Handle the exception here, you might want to log the error or take appropriate action
            print(f&quot;An error occurred: {e}&quot;)
        finally:
            # Save the fine-tuning events to a JSONL file
            finetuning_handler.save_finetuning_events(f'{self.dataset_path}/finetuning_events.jsonl')
<span class=""math-container"">```</span>
</code></pre>
","chatgpt"
"45122","What are these special ""AI chips"" actually used for?","2024-03-15 02:31:36","","2","85","<neural-networks><training><chatgpt>","<p>To make an AI / LLM, like ChatGPT, you need two things:</p>
<ol>
<li>To create the LLM. This includes training it, etc. Very expensive from computation perspective.</li>
<li>Run the LLM to answer user queries. For instance, I go to Chatgpt.com and ask &quot;what is a dog&quot;, the LLM created in step 1 processes that question and spits out an answer.</li>
</ol>
<p>Apparently, Nvidia (and other such companies) make special &quot;AI&quot; chips that are &quot;good at AI&quot;. What does this really mean? In practice (i.e., in the real world), are Nvidia &quot;AI chips&quot; solving step 1, step 2, or both?</p>
<p>Can step 2 not be done with regular cpu chips, once the step 1 is completed? Or do we want special AI chips for both because the performance is so much better?</p>
","chatgpt"
"44965","Discrepancies in the Exclusion of Elements in Image vs. Text Generation","2024-02-29 17:16:56","44969","1","33","<chatgpt><image-generation><text-generation>","<p>(The following comments concern DALLE, I have not tested it with other image generating tools, but would be curious to hear if the same happens)</p>
<p>When generating images, it seems that ChatGPT (i.e. DALLE) fails to exclude specific elements from an image.
As an example, here is the image I got based on the simple prompt:</p>
<blockquote>
<p><em>Generate a picture of a zoo, but make sure that no elephants are in the picture.</em></p>
</blockquote>
<p><a href=""https://i.sstatic.net/v32Tg.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/v32Tg.jpg"" alt=""enter image description here"" /></a></p>
<blockquote class=""spoiler"">
<p> Here is the prompt that ChatGPT generated and used for image generation:
 <em>&quot;A bustling zoo scene filled with a variety of animals such as giraffes, zebras, lions, and monkeys. The zoo is lively with visitors walking around, looking at different animal exhibits. There are trees, benches, and pathways that lead to different sections of the zoo. Food stands and souvenir shops are scattered throughout, adding to the busy atmosphere. The sky is clear and sunny, perfect for a day out. Make sure there are no elephants in the picture&quot;.</em></p>
</blockquote>
<p>When it comes to text generation, this does not seem to be an issue. Using the prompt</p>
<blockquote>
<p>Write a small poem about a zoo, but make sure that no elephants are mentioned.</p>
</blockquote>
<p>I have never gotten a result that mentioned elephants. If the comment concerning elephants is left out however, most poems will talk about animals in the zoo and then also mention them.</p>
<p>Given the observed challenges in the exclusion of elements (like elephants) in pictures, but the relative ease of that for texts, what are the fundamental reasons for this? Is this due to some inherent differences in training or model architecture in both applications?</p>
","chatgpt"
"44947","How are sentences turned into a vector in LLM","2024-02-28 10:19:02","","1","211","<attention><large-language-models><chatgpt><word-embedding><linear-algebra>","<p>My understanding of <em>Large Language Models</em> like <em>GPT</em> is that they are special kinds of <em>deep neural networks</em> specifically trained to predict the next word, given the beginning of a sentence.</p>
<p>I understand that a key aspect of their architecture is <em>attention</em>, which allows a word representation (a vector) to be mixed-up with the representation of other words in the sentence, the weights being used to make that linear combination representing a notion of proximity as they are derived from the scalar product of the words embedding.</p>
<p>Now, considering a sentence of words like &quot;word1 word2 word3&quot;, if we want to feed it to a neural network, which has a fixed number of input nodes, we should represent it as a <strong>single</strong> input vector of fixed size (same as the number of input nodes). My understanding of <em>attention</em> is that it still produces one vector for each input words, not one vector for the whole sentence.</p>
<p>How is a full sentence turned into a fixed-size vector?</p>
","chatgpt"
"44930","How can Transformers handle random sequences?","2024-02-26 15:11:40","44931","4","795","<transformer><chatgpt>","<p>I have asked ChatGPT the following:</p>
<blockquote>
<p>Can you concatenate jfef9230rj2mreg90r23ewfrn02eqwdk and
32ir20r3i2ofg90r32kee?</p>
</blockquote>
<p>And without any error the model produces:</p>
<blockquote>
<p>jfef9230rj2mreg90r23ewfrn02eqwdk32ir20r3i2ofg90r32kee</p>
</blockquote>
<p>My question:</p>
<p>Does ChatGPT rawly tokenizes random sequences, or do they do some preprocessing and replace these sequences by new Tokens like:</p>
<blockquote>
<p>Can you concatenate SEQUENCE1 and SEQUENCE2?</p>
</blockquote>
<p>where SEQUENCE1 ... SEQUENCE_N is in the vocabulary, and before the Cross-Entropy on the prediction (which is hopefully SEQUENCE1||SEQUENCE2) they resubstitute back the sequences.</p>
<p>I think the copying is quite straightforward, shouldn't a Transformer be able to learn how to copypaste/move around long sequences of bytes without any preprocessing techniques?</p>
<p>In my project I have done good amount of training and it only manages to move around only little segments of the sequence, and I was thinking if this is even the right approach to rawly Tokenize the sequences, or if I just had too little training.</p>
","chatgpt"
"44925","What is an ""inference kernel""?","2024-02-26 12:41:01","","0","153","<terminology><large-language-models><chatgpt><inference>","<p>In the recent event where ChatGPT &quot;went crazy&quot;, this term was used in the official post-mortem to describe what happened:</p>
<blockquote>
<p>In this case, the bug was in the step where the model chooses these numbers. Akin to being lost in translation, the model chose slightly wrong numbers, which produced word sequences that made no sense. More technically, <strong>inference kernels</strong> produced incorrect results when used in certain GPU configurations.</p>
</blockquote>
<p>Now I know what <strong>inference</strong> is, at least wrt LLMs. It's when you use a trained LLM to generate text by giving it a prompt (feel free to offer a more technical clarification if necessary). As for <strong>kernel</strong> that has several meanings in computing generally, such as an <a href=""https://en.wikipedia.org/wiki/Kernel_(operating_system)"" rel=""nofollow noreferrer"">operating system kernel</a> being the core part of the <strong>OS</strong>, often in contrast to shells, including command lines and GUIs. Then there is the <a href=""https://en.wikipedia.org/wiki/Kernel_(image_processing)"" rel=""nofollow noreferrer"">sense of kernel in image processing</a>, also known as a <strong>convolution matrix</strong>. There's also <strong>Compute Kernels</strong> in <strong>GPGPU</strong> programming, with which I am the least familiar.</p>
<p>But what does the overall phrase &quot;inference kernel&quot; refer to technically? My hunch is that it's some kind of specialization of the GPU sense, though I read <a href=""https://www.lesswrong.com/posts/koNsuop5AHxBG5D7t/can-someone-explain-to-me-what-went-wrong-with-chatgpt?commentId=5ts6boALcsvFHS9Ba"" rel=""nofollow noreferrer"">a comment in an AI-adjacent forum</a> that it's related to the OS sense, which I doubt is accurate.</p>
<p>Other than that I can't actually find this term mentioned very much, and I can't find it defined at all.</p>
<hr />
<p><strong>EDIT</strong></p>
<p><a href=""http://www.proof-technologies.com/misc/glossary.html"" rel=""nofollow noreferrer"">I found a definition of <em>inference kernel</em></a> but it's about automatic theorom provers and doesn't seem to be applicable to AI, not that I can follow it very well:</p>
<blockquote>
<p><strong>inference kernel</strong> : (n) The part of an LCF-style system that implements the deductive system of its formal logic, and relied upon by all subsequent deductive functionality for its soundness. It defines the datatype for internal theorems as an abstract datatype, with the primitive inference rules and the primitive assertion commands as its only primitive constructors. The inference kernel is a crucial trusted component of its system. See &quot;language kernel&quot;, &quot;logical core&quot;, &quot;core system&quot;.</p>
</blockquote>
","chatgpt"
"43728","How to force LLM (like OpenAI ChatGPT) to output a variable list of values?","2024-02-09 02:38:01","","1","1126","<large-language-models><chatgpt><prompt>","<p>What prompt (or other technique) should I use with an LLM so that</p>
<ol>
<li>The result is guaranteed to be reliably parseable as a list of values (e.g. a Python list of strings)</li>
<li>LLM would understand that a variable list of values is expected during generation</li>
</ol>
<h1>EDIT: Chosen solution</h1>
<p>What I personally ended up using is BAML. Works with all LLMs and guarantees correct output.</p>
<p><a href=""https://github.com/BoundaryML/baml"" rel=""nofollow noreferrer"">https://github.com/BoundaryML/baml</a></p>
<p><code>pip install baml-py</code></p>
<p>Example:</p>
<p><a href=""https://www.promptfiddle.com/BAML-Examples-7xLvl"" rel=""nofollow noreferrer"">https://www.promptfiddle.com/BAML-Examples-7xLvl</a></p>
","chatgpt"
"43646","Why is generative QA + RAG so popular when extractive QA models exist that can provide factual information?","2024-02-01 22:01:26","43650","0","159","<chatgpt><question-answering>","<p>If the concern with using generative models for question answering is that these models aren't always producing factual information, why is it that people are using these models with Retrieval Augmented Generation (Open Generative QA/Closed Generative QA), rather than using a transformers-based extractive QA model that can refer users to the potential answer from actual text documentation?</p>
","chatgpt"
"43434","Does Chat-GPT learn from interactions?","2024-01-14 16:57:56","43436","0","174","<training><chatgpt>","<p>Does Chat-GPT accumulate knowledge through interactions with users in real time? I know it's trained on a massive dataset but does it do online learning?</p>
","chatgpt"
"43422","ChatGPT epic creativity fail","2024-01-13 18:53:14","","-3","115","<chatgpt>","<p>My question to ChatGPT, after reading about a vole (as a German I had to look it up) and immediately associating &quot;mole&quot;:</p>
<p>&quot;Here are pairs of animal names which differ only in the first letter: monkey-donkey, mouse-louse, and even bat-rat-cat. (&quot;gnat&quot; wouldn't fit, since two letters differ, and gnat-goat is halfway OK, since the second letter differs.) Do you know more examples?&quot;</p>
<p>And the answer was:</p>
<pre><code>Yes, there are several more examples of animal pairs that differ only in the first letter. Here are a few:

    Cow-now
    Hen-pen
    Hare-dare
    Ant-pant
    Duck-tuck
    Hog-dog
    Seal-real
    Snake-rake
    Yak-lak
    Ox-fox

These are just a few examples, but there might be more out there!
</code></pre>
<p>2/10. I halfway expected that ChatGPT would fail epically, since the question needs some creativity. Still, even a computer could grep its animal list, regex match over it and give only the hits. I don't see either how ChatGPT can misinterpret the question by lifting the requirement &quot;animal&quot; for the rhyme pair.</p>
<p>Do you have an explanation? (&quot;Hallucination&quot; is a bit unspecific.)</p>
","chatgpt"
"43408","publically available language models that can be used to train arbitrary language data?","2024-01-12 08:00:09","","1","39","<natural-language-processing><large-language-models><chatgpt><language-model>","<p>I have sentence data in a language that is not widely in use and as such popular LLMs do not support the language. I want to train some language model such that given some question, it is able to respond back in the same language, just as in ChatGPT just with a different language.</p>
<p>In such a case, what language model is publically available and is sufficiently powerful? Or would it be possible to use popular LLMs (such as ChatGPT) to achieve such a goal?</p>
","chatgpt"
"43222","Do AI-based code-generators guarantee correct output?","2023-12-25 15:47:51","","3","2946","<natural-language-processing><chatgpt><large-language-models>","<p>I haven't spent much time looking at AI-based code-generators. What mechanism is used to generate code and how is it different than standard NLP?</p>
","chatgpt"
"43178","How does chatgpt interpret prompts?","2023-12-19 04:35:18","43182","0","118","<chatgpt><open-ai><chat-bots><natural-language-understanding>","<p>First of all, my English isn't very good, please bear with me.</p>
<p>I've heard that chatgpt is a decoder-only model, which doesn't use any encoder model.
How does it interpret our prompts if it doesn't use any encoder model?</p>
","chatgpt"
"43128","What is accelerated years in describing the amount of the training time?","2023-12-14 09:59:07","43130","1","738","<training><open-ai><chatgpt><gpu><gpt-3>","<p>As described in this <a href=""https://arxiv.org/abs/2204.05149"" rel=""nofollow noreferrer"">article</a>, it was written that GPT-3 took 405 V100 years to train in 2020.
I'm a bit confused about this definition, does that mean the process was accelerated like using a V100 GPU to train in 405 years?</p>
","chatgpt"
"43075","How does Chat GPT encode a question?","2023-12-11 02:34:48","","0","755","<deep-learning><transformer><chatgpt><large-language-models><encoder-decoder>","<p>Chat GPT is based on a decoder-only Transformer so it does not have an encoder. Given that, how is a user's question passed as input to Chat GPT's decoder? In a regular encoder-decoder architecture, the final embeddings of the encoder are passed to the decoder along with the  token. Then, the decoder auto-regressively outputs tokens. How would this work in a decoder-only architecture?</p>
<p>Let's say I have the following question: &quot;How many countries are there in the world?&quot; and its token form is [3, 5, 8, 2, 10, 4, 1, 6, 7]. How will the decoder take in the input?</p>
<p>In an encoder-decoder architecture, I would just pass the  token to the decoder which would, based on the encoder embedding, auto-regressively output the next token in the sequence to form an answer.</p>
<p>In the decoder-only architecture, how will that work? If I pass the first token of the question (token &quot;3&quot;) to the decoder, it will output what it thinks is the most likely token after &quot;3&quot; but it will have no context... So how is the context taken into account given we are not encoding it?</p>
<p>I am aware of <a href=""https://ai.stackexchange.com/questions/40179/how-does-the-decoder-only-transformer-architecture-work"">this post</a> which is similar but I would more specifically want to know how we go from a decoder-only model that inputs the most likely word <em>in general</em>, to a model that outputs a whole answer based on a specific question. Put differently, how does a problem which is inherently a sequence to sequence problem (question answering) get solved using an autoregressive model instead of a seq2seq model?</p>
<p>One potential answer I have been looking into is training fine-tuning but I am still unsure how a model that out puts a single token x[n+1] = f(x[n],...,x[1]) can be fine-tuned based on a training set of two sequences (one question and one answer per record).</p>
","chatgpt"
"42990","Intentionally corrupting LLM weights (lobotomy)","2023-12-03 19:15:06","","0","132","<chatgpt><large-language-models><intelligence-testing><benchmarks><gpt-4>","<p>It is largely unknown how LLMs work inside. Has anyone scientifically tried to corrupt (open source) model's weights in an organized manner to maybe detect which parts of the model are doing what or are more or less important?</p>
<p>I mean something like setting the weights to zeros/random numbers in random (% of all weights) or organized (whole areas/layers etc.) manner and benchmarking the LLM to see how the results change.</p>
<p>Related <a href=""https://www.reddit.com/r/LocalLLaMA/comments/13vx1e5/llm_lobotomy_predictions/"" rel=""nofollow noreferrer"">reddit</a> post.</p>
","chatgpt"
"42947","Why is chatGPT 4 unable to correct a program given its output?","2023-11-29 07:52:46","","0","85","<chatgpt><prompt>","<p>Recently I used chatGPT 4 to write a small project in Perl which involved reading of an exam file (multiple choice), comparing it to a master exam file, and scoring the student exam including fraud detection and statistics.</p>
<p>chatGPT did a great job of providing low-level code such as regex for pattern matching.</p>
<p>However, chatGPT failed miserably when asked for the program logic of a larger task such as reading the whole exam file and categorizing it into questions, answers, and hints.</p>
<p>What I found stunning was that it got the program logic about 80% correct.</p>
<p>However, even when giving it the whole output it seemed unable to adjust the code and sometimes even made it worse. It was like chatGPT hit a roadblock.</p>
<p>Why is chatGPT unable to refine its code even when prompted with the output and specific instructions on how the output should look vs. what it looks like now?</p>
","chatgpt"
"42933","Could an analysis of GPT4's WAIS score be published?","2023-11-27 21:07:54","","0","22","<neural-networks><machine-learning><chatgpt><large-language-models><gpt>","<p>Similar to this: <a href=""https://www.scientificamerican.com/article/i-gave-chatgpt-an-iq-test-heres-what-i-discovered/"" rel=""nofollow noreferrer"">https://www.scientificamerican.com/article/i-gave-chatgpt-an-iq-test-heres-what-i-discovered/</a></p>
<p>But more detailed and in depth (subtest breakdown, including image analysis, etc.), WAIS-IV not WAIS-III, better methodology, etc.</p>
<p>Would it be a more interesting paper if it compared overall test scores and subtest scores to other LLMs, like 3.5, LLaMA, Mistral, etc.?</p>
<p>I know it's out of the box so I'm wondering if this is publishable in some journal. It seems that the WAIS interesting as there is both good human benchmark data, and it's never been on the internet, so there is no leakage.</p>
","chatgpt"
"42923","How LLM keeps the context of a chat/thread","2023-11-26 17:41:52","","1","1407","<open-ai><chatgpt><large-language-models>","<p><strong>How an LLM keeps the context</strong> (what has already been entered by the user) <strong>of a chat/thread?</strong></p>
<p>For reference, in <a href=""https://chat.openai.com/"" rel=""nofollow noreferrer"">chat.openai.com</a>, for each chat we create (or a <a href=""https://platform.openai.com/docs/assistants/how-it-works/managing-threads-and-messages"" rel=""nofollow noreferrer"">Thread</a> according to their API), the LLM remembers what we have already input to the model, when answering a new question.</p>
<p>I did some reading on the topic and found below possible ways:</p>
<ol>
<li>change the weights accordingly: but this seems not-practical for LLM given their size (even changing weights of the last layer seems an over-kill)</li>
<li>output a context vector at each inference and re-use it for the next inference: this seems more likely. but I am not sure exactly how to do it.</li>
</ol>
<p>It would be great if someone can help me with this.</p>
<p>Thanks.</p>
","chatgpt"
"42855","Would maximizing (instead of minimizing) error of an LLM/HMM lead to complex behavior?","2023-11-20 04:06:50","","1","80","<natural-language-processing><training><datasets><chatgpt><natural-language-generation>","<p>Imagine we have some sort of &quot;next token predictor,&quot; either with transformer architecture, LSTM, or just a HMM (though the terminology I use here will be less aligned to HMMs, I believe the question is generalizable to all generative NLP).</p>
<p>We reverse the cost function. That is, we are training to maximize error instead of minimizing it. In the case where error is neither maximized nor minimized, the behavior will be fairly boring. However, a model which is maximizing error may still need to learn patterns of syntax and which words usually follow one another in order to avoid them. I would expect that in some abstract way, it may behave creatively, because it is trying to produce output which is <em>not</em> in the training data, and is furthest away from it. In fact, it ideally should understand the user's query in order to avoid using words that follow it.</p>
<p>This makes me think the output may be non-boring, although probably not practically useful.</p>
","chatgpt"
"42739","When using Reinforcement Learning with Human Feedback to train a transformer, how do I propagate the feedback through the transformer?","2023-11-10 21:25:56","","0","68","<reinforcement-learning><chatgpt><gpt><fine-tuning><rlhf>","<p>I'm basically trying to replicate the processed used to create Chat GPT:
<a href=""https://i.sstatic.net/dJ7qC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dJ7qC.png"" alt=""enter image description here"" /></a></p>
<p>Am I supposed to backpropagate? How can I do that when these aren't really errors, but rather ranking several response? Can I use a 1-10 rating system where 10 is a perfect response instead so I have something closer to an error signal?</p>
<p>Also I have to rank/rate each output as a whole, but the final layer of the model are token neurons. Do I somehow connect my ranking data to the layer right before the final layer? Is the reward model supposed to be completely separate from the transformer with no direct edges/weight connections? Do I update the transformer model using the Bellman equations?</p>
<p>I am shocked I can't find a tutorial or really any information about this crucial component given that this is the strategy used to make Chat GPT according to OpenAI. Is there something I'm missing that someone could direct me towards? All I can find is information about fine-tuning transformers and RLHF separately, but not combined.</p>
","chatgpt"
"42670","What is 'system card'?","2023-11-05 10:55:27","","1","304","<definitions><open-ai><chatgpt><explainable-ai>","<p>What is 'system card' in these context:</p>
<p><a href=""https://i.sstatic.net/PmJM7.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PmJM7.jpg"" alt=""enter image description here"" /></a></p>
<p><a href=""https://ai.meta.com/blog/system-cards-a-new-resource-for-understanding-how-ai-systems-work/"" rel=""nofollow noreferrer"">https://ai.meta.com/blog/system-cards-a-new-resource-for-understanding-how-ai-systems-work/</a></p>
<blockquote>
<p>Additionally, individual model developers may provide documentation
such as the OpenAI <strong>system card</strong> for the GPT-4 model.</p>
</blockquote>
<p>Source: <a href=""https://learn.microsoft.com/en-us/training/modules/responsible-generative-ai/3-identify-harms"" rel=""nofollow noreferrer"">https://learn.microsoft.com/en-us/training/modules/responsible-generative-ai/3-identify-harms</a></p>
<p>By the way, what is 'model cards'?</p>
","chatgpt"
"42539","Why do ChatGPT “jailbreaks” work?","2023-10-24 20:30:44","42541","27","14748","<open-ai><chatgpt>","<p>Do developers not genuinely want to prevent them? It seems like if they are able to develop such impressive AI models then it wouldn’t be that difficult to create catch-all/wildcard mitigations to the various jailbreak methods that are devised over time.</p>
<p>What gives? (<em>i.e.,</em> what is so difficult about plugging these holes, or why might  the developers not really want to do so?)</p>
","chatgpt"
"42436","Are ""Custom Instructions"" injected into the context window repeatedly?","2023-10-13 18:10:02","","0","80","<chatgpt><gpt-4>","<p>ChatGPT (the conversational front-end to GPT 3.5 and GPT 4) includes user configurable custom instructions. They allocate 1500 characters to &quot;what you would like chatGPT to know about you&quot; and 1500 characters to &quot;how you would like ChatGPT to respond.&quot;</p>
<p>How does the system preserve custom instructions as the conversation unfolds? Since the context window is constantly shedding tokens on the back-end on the conversation, I'm wondering if the ChatGPT wrapper is regularly reinjecting these instructions into the context under the hood. It could be happening with each interaction or perhaps it happens every x number of tokens. Does anyone have a clue about how this might work?</p>
","chatgpt"
"42351","claude 2 doesn't know basic math?","2023-10-06 13:20:51","","0","70","<machine-learning><math><open-ai><chatgpt><large-language-models>","<p>Sometimes, when I see answers like this from large language models, it makes me feel disgusted:</p>
<p>Me: Does Voyager 1 have enough velocity to escape the solar system without using Jupiter's gravity assist?</p>
<p>claude: &quot;</p>
<ul>
<li>Escape velocity at 1 AU from the Sun is 151,560 km/h.</li>
<li>Voyager 1's pre-Jupiter flyby velocity was 169,200 km/h.</li>
</ul>
<p>Therefore, Voyager 1 absolutely <strong>did NOT have enough velocity</strong> at 1 AU to escape the solar             system without the Jupiter gravity assist. <strong>Its velocity of 169,200 km/h at 1 AU was below the      solar escape velocity of 151,560 km/h.</strong>&quot;</p>
<hr />
<p><a href=""https://i.sstatic.net/GpxB8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GpxB8.png"" alt=""stupid answer by claude 2"" /></a></p>
<p>So my question is:
why can this large language model sometimes solve complex problems but fail to answer basic ones? (The irony is that it became not free before it was ready)</p>
","chatgpt"
"42262","What is the reason that OpenAI implements complex context management in ChatGPT but does not use it for their API?","2023-09-30 00:44:13","","0","50","<open-ai><chatgpt><vector-space>","<p>According to this discussion</p>
<p><a href=""https://ai.stackexchange.com/questions/38150/how-does-chatgpt-retain-the-context-of-previous-questions"">How does ChatGPT retain the context of previous questions?</a></p>
<p>it is clear that OpenAI already has implemented some serious engineer to ensure that the ChatGPT experience is a great one by enabling ChatGPT to have very good context management. In order words, ChatGPT can recall things long past it's token limit (much earlier in the conversation).</p>
<p>However, this function does not exists in the API. Why is that? I know we that we cannot know for sure what the proprietary reasons are, but what are some good theories?</p>
","chatgpt"
"42215","Dynamic text completion","2023-09-26 22:41:41","","-2","36","<open-ai><chatgpt><question-answering><text-generation>","<p>I am using the get_completion function to generate response to a question. Since generating response takes a long time (~5 seconds) I would like to display the response text as it's generated (dynamically, similar to chatgpt or <a href=""https://www.perplexity.ai/"" rel=""nofollow noreferrer"">https://www.perplexity.ai/</a>). Does openAI's API make this possible? I couldn't find a code/example anywhere online</p>
","chatgpt"
"42140","Why do current language models no longer generate to long or short texts?","2023-09-19 21:58:45","","3","165","<natural-language-processing><chatgpt><large-language-models><language-model><natural-language-generation>","<p>One of the biggest strengths of ChatGPT is that it generates fitting text with respect to the input query. It usually stays on topic, anwers the question completely and especially does not start talking gibberish or repeating itself.</p>
<p>This behaviour is different when comparing this to older LLMs. For example: GPT2 would usually only stop generating text when it hit the token limit or a predefined stop sequence. Also, it had a much bigger problem with giving repeating answers. Newer models (especially instruction tuned ones) do not suffer from this problems (e.g. llama 2).</p>
<p><strong>So I have 2 questions: What mechanisms/techniques are used in current language models such that...</strong></p>
<ol>
<li>...<strong>the models know when to stop generating text.</strong></li>
<li>...<strong>the models do not repeat themselfes and stay on topic.</strong></li>
</ol>
<p>I suspect it might have alot to do with instruction tuning but I am happy to hear from you.</p>
","chatgpt"
"42131","Tips and tricks when training a very large language model?","2023-09-19 15:54:33","","1","566","<neural-networks><training><chatgpt><architecture><large-language-models>","<p>Have never trained a (very) large language model, so I am wondering if the process is the same as training a (regular) language model, i.e. you prepare the data, set up the architecture, hyperparameters, loss function to minimize perplexity and predicting the next word, and then do gradient descent over the giant dataset. Or if there are any special gotchas or tricks you must do when training it.
I know there's at least one involving the training dynamics:</p>
<ol>
<li>training dynamics: most LLMs stop seeing performance improvement even before a single epoch is finished.</li>
</ol>
<p>I am wondering if there are any others</p>
","chatgpt"
"42123","How to get GPT to complete all tasks before ending its turn?","2023-09-18 23:16:51","","0","62","<chatgpt>","<p>I'm trying to figure out how ChatGPT and Code Interpreter (or anything else) gets GPT to complete an arbitrary number of actions before it ends its turn and hands it back to the user. I'm using a while loop to have it perform multiple actions but I can't figure out a way to end its turn when it has completed all tasks. I tried making an end_turn() button but it doesn't adhere to using it.</p>
<pre><code>def get_some_value():
    some_value = &quot;one plus one equals two.&quot;
    return json.dumps({&quot;some_value_one&quot;: some_value})

def get_some_value_two():
    some_value = &quot;two times two equals four.&quot;
    return json.dumps({&quot;some_value_two&quot;: some_value})

def end_turn():
    return json.dumps({&quot;message&quot;: &quot;All tasks are completed.&quot;})

# Initialize the assistant and its state
# messages = [
#     {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You can call the function get_some_value, get_some_value_two, and call the end_turn function when your tasks are complete to hand the conversation back to the user.&quot;},
#     {&quot;role&quot;: &quot;user&quot;,
#      &quot;content&quot;: &quot;activate the get_some_value function, then activate the get_some_value_two function, then tell me the value of both. &quot;}
# ]
messages = [
    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You must quietly activate the end_turn function to give control back to the user after you are done with your current response&quot;},
    {&quot;role&quot;: &quot;user&quot;,
     &quot;content&quot;: &quot;hi&quot;}
]

# Describe the functions
functions = [
    {
        &quot;name&quot;: &quot;get_some_value&quot;,
        &quot;description&quot;: &quot;This function returns some value&quot;,
        &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {}}
    },
    {
        &quot;name&quot;: &quot;get_some_value_two&quot;,
        &quot;description&quot;: &quot;This function returns some value&quot;,
        &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {}}
    },
    {
        &quot;name&quot;: &quot;end_turn&quot;,
        &quot;description&quot;: &quot;This function ends the assistant's turn so the user can respond&quot;,
        &quot;parameters&quot;: {&quot;type&quot;: &quot;object&quot;, &quot;properties&quot;: {}}
    }
]

while True:
    # Make the API call
    response = openai.ChatCompletion.create(
        model=&quot;gpt-4-0613&quot;,
        messages=messages,
        functions=functions,
        function_call=&quot;auto&quot;
    )
    # print(response)
    function_name = response[&quot;choices&quot;][0][&quot;message&quot;].get(&quot;function_call&quot;, {}).get(&quot;name&quot;)

    if function_name:
        # Call the function based on its name
        if function_name == &quot;get_some_value&quot;:
            function_response = get_some_value()
        elif function_name == &quot;get_some_value_two&quot;:
            function_response = get_some_value_two()
        elif function_name == &quot;end_turn&quot;:
            function_response = end_turn()
            print(&quot;Assistant says: All tasks are completed.&quot;)
            break

        messages.append({&quot;role&quot;: &quot;function&quot;, &quot;name&quot;: function_name, &quot;content&quot;: function_response})
    else:
        assistant_message = response[&quot;choices&quot;][0][&quot;message&quot;][&quot;content&quot;]
        print(&quot;Assistant says:&quot;, assistant_message)
</code></pre>
","chatgpt"
"42082","How does query of LLM/GPT models work?","2023-09-13 16:03:24","","0","94","<chatgpt><gpt><question-answering>","<p>Training of LLM aka GPT models is clear on how is trained but can't find any info how is &quot;mapped&quot; query to internal tokens and generates response tokens more precise inference phase which consists of natural language processing and dialog management, with focus on how user input is computed to generate response tokens/content(dialog management part would be perfect but would work a general GPT inference too), mathematics and phases/steps are similar to traditional NLP models (matrix of vectors mapped to vocal vector)? Checked more resources but none provides a clear picture on how do work internally <a href=""https://learn.microsoft.com/en-us/semantic-kernel/prompt-engineering/tokens"" rel=""nofollow noreferrer"">tokens</a>, <a href=""https://techcommunity.microsoft.com/t5/microsoft-mechanics-blog/what-runs-chatgpt-inside-microsoft-s-ai-supercomputer-featuring/ba-p/3830281"" rel=""nofollow noreferrer"">chatGPT</a> and <a href=""https://openai.com/blog/chatgpt"" rel=""nofollow noreferrer"">OpenAI chatGPT</a></p>
","chatgpt"
"41945","What strategy does ChatGPT use to manage its context in very lengthy conversations?","2023-08-31 13:04:59","","3","338","<chatgpt>","<p>I'm asking specifically about ChatGPT4, but the question could apply to either that or 3.5.</p>
<p>When you use the ChatGPT API, it's of course up to you to manage conversation history and include that in successive API calls within available context length in whatever manner you choose.</p>
<p>In the case of the web interface, they've obviously implemented some system to manage conversation history in context. It clearly doesn't &quot;remember&quot; the entire thing once the conversation gets very long, because it doesn't have infinite context length. So, what strategy does it use to send conversation history to the model once it's exceeded its context length? Does it truncate all content prior to the max context length? Does it summarize earlier parts of conversations to more efficiently fit them within the context? Does it do some dynamic strategy combining many inputs?</p>
<p>Or is this just another case where we just don't know, and OpenAI is being tight-lipped about what it's actually doing?</p>
","chatgpt"
"41919","Why do many AI bots feel the need to be know-it-alls?","2023-08-29 15:56:14","41920","15","7184","<machine-learning><chatgpt>","<p>Having used various AI bots often over recent months, I noticed that often it will claim to know something, even if it doesn't.
It would then either explain something which is clearly nonsense, or by rambling on about how the answer isn't known in general.
Or how, if asked for example- &quot;would you be able to explain X&quot; it wouldn't respond &quot;yes, I could&quot; but rather would elucidate X.
Have they been trained to always respond as though it were a know-it-all?
(Google's Bard and ChatGPT specifically, although I'm assuming only open-source AI will be answerable)</p>
","chatgpt"
"41408","Noob crafting a simple ""Zero-Shot Classifier"" Using an API . How can I avoid passing the categories every single request?","2023-07-22 06:42:03","","0","100","<classification><open-ai><chatgpt><text-classification>","<p>I have a collection of 700 categories, all potential classifications for articles. My current need is to create a system that can dynamically categorize short texts or articles according to these 700 categories.</p>
<p>I've been experimenting with a rudimentary approach using chatGPT to read the categories from a PDF via a plugin. The process is quite straightforward - I input the title and the first two lines of an article, and chatGPT does a fairly decent job of predicting the most fitting category.</p>
<p>The downside? I'm concerned about its scalability and economic viability. The current method might not work so well when we're talking about classifying a significant number of articles.</p>
<p>My question to you, my fellow AI enthusiasts: How would you approach designing a system, via an API, capable of doing this quickly and on a large scale?</p>
<p><strong>Is there a feature that allows the Language Learning Model (LLM) to retain the list of 700 categories in its memory so that I don't have to pass it every time?</strong> I'm aware that the billing structure is token-based, so it would be ideal to submit the categories once (or as few times as possible) and then pose a simple query like:</p>
<blockquote>
<p>&quot;Categorize this article based on the categories I previously gave you. Article title: 'Barbie vs Oppenheimer: Which Movie Will Garner Greater Success?'&quot;</p>
</blockquote>
<p>Ideally, I'd want this system to be persistently active and capable of processing countless queries over an extended period, say a month or a year.</p>
<p>So, any ideas on how to design such a system? There are undoubtedly numerous routes to take. I'm really just seeking some initial direction so that I can dive deeper into research on my own.</p>
","chatgpt"
"41406","Randomness in Google Bard answers","2023-07-22 05:03:30","41407","0","118","<chatgpt>","<p>I tried Google Bard now recently for the first time after we got into here in Finland too. And to my surprise it gives different answer each time I ask the same question. Is this a feature of LLM or Bard or what takes?</p>
","chatgpt"
"41401","Are LLM hallucination necessarily a bad thing?","2023-07-21 21:24:36","41417","2","441","<chatgpt>","<p>I've been pondering over this for a while now: is the so-called hallucinating necessarily a problem in LLM or in AI in general? What it stands for anyway, maybe the model is trying to crunch so much information into the answer we just cannot comprehend. How could it be the model's fault since it knows only what we teach them after all?</p>
<p>I am trying to cover LLM hallucination here as a whole considering it as a general term many of us have read about as a known problem with LLM. For further reading and understanding different kinds of LLM hallucination I recommend to check <a href=""https://g.co/bard/share/6f8d1031d317"" rel=""nofollow noreferrer"">this link</a> and <a href=""https://chat.openai.com/share/9d1b597e-57f7-4ce2-802e-6d3cf9b6999a"" rel=""nofollow noreferrer"">this link</a>.</p>
","chatgpt"
"41379","Could hallucinations be the demise of the AI hype?","2023-07-20 15:01:26","41380","2","117","<generative-model><chatgpt><large-language-models>","<p>For quite some time now, I have been evaluating ChatGPT's capability to deliver accurate and helpful responses. While its performance is undeniably impressive, the issue of hallucinations poses a significant drawback to this otherwise capable model.</p>
<p>As I increased the complexity of my inputs during several sessions, my initial enthusiasm and excitement about the advancements in AI quickly waned due to the responses becoming riddled with hallucinations. This has seriously raised my concerns about the reliability and usefulness of chatGPT.</p>
<p>I have two questions:</p>
<ol>
<li>Is there a declining interest among people in LLMs and artificial intelligence due to the problem of hallucinations?</li>
<li>How are LLM companies addressing and combatting this concerning aspect of AI models in general?</li>
</ol>
","chatgpt"
"41339","How is LLM generated content moderated?","2023-07-18 09:52:20","","1","283","<chatgpt>","<p>I'm looking for references (articles) about how LLM generated content is moderated. From a technical point of view, what makes the difference between a so-called &quot;uncensored&quot; LLM such as Pygmalion 7B and what would be a &quot;censored&quot; one? Does a LLM always generate text that gets moderated later, or is it pre-trained / fine-tuned to generate moderated content by default?</p>
","chatgpt"
"41262","How to Formulate a realiable ChatGPT Prompt for Sentiment Analysis of a Text, and show that it is reliable?","2023-07-13 02:29:02","","3","4339","<chatgpt><large-language-models><prompt><prompt-design>","<p>I have a dataset which consists of like.. 400000 sentences and I want give each sentence to ChatGPT so it classifies each sentence as <code>positive</code> or <code>negative</code>. My question is, where can I find a reliable / trusted prompt to do that? and provide evidence that the prompt I used gives reliable labels for the problem?</p>
<p>If I just create my own prompt like.. &quot;Please conduct a sentiment analysis on the following text and tell me if the sentiment expressed is positive or negative: [insert text here]&quot;</p>
<p>How can I convince someone that my prompt was good? This is what I mean :/ So later no one can criticize like &quot;Hey maybe you just used a bad prompt and ChatGPT could have performed better!&quot; :c</p>
<p>I hope my question is clear.</p>
","chatgpt"
"41253","Does the output of LLM's affect their neural weights?","2023-07-12 15:04:32","41255","0","171","<backpropagation><chatgpt>","<p>When an LLM creates an output, it seemingly has no way to check if its output was valid.  Therefore it wouldn't be able to back-propagate any changes to the weights is used to create that output.</p>
<p>Right now, I suspect that all weight modification is done by training on input data, as that can (generally) be assumed to be human and valid.</p>
<p>But perhaps it does have some way of checking if its output was good or not, and being modified based off of it.  For instance, there is the thumb up and down button on ChatGPT which could be used for that.</p>
<p>Do LLM's modify their neural weights based off of their own answers?  If so, how?</p>
","chatgpt"
"41190","Is it possible to have a ChatGPT bot that can answer questions about sensitive information without using external servers?","2023-07-07 10:44:44","","1","109","<chat-bots><chatgpt><ai-security>","<p>We have a lot of internal documentation that contains sensitive information.</p>
<p>Is it possible to have a ChatGPT bot that can answer questions about this information, to my used by users with clearance for this information, without posting the information to external servers?</p>
<p>I can find documentation on <code>embeddings</code>, but these seem to require posting info to OpenAI for it to be processed and stored there.</p>
","chatgpt"
"41136","What is an appropriate tool to use that takes in a large knowledge base in string form and can answer questions based on the knowledge base?","2023-07-04 05:56:09","","1","61","<natural-language-processing><chatgpt>","<p>I have an issue where I'm trying to use the openAI API to input a very large custom knowledge base (exceeding 1GB) that allows the user to ask questions based on that base to receive intelligent answers. However, the openAI API is very restraining in the tokens and requests that can be inputted, and I was wondering if there is another tool that is more suited for my problem. I figured it would have to utilize an LLM to understand and parse the info.</p>
<p>I tried different measures such as smartly looking up specific sections of info then inputting a smaller chunk it into GPT, but I'm still largely restricted by the API's restraints.</p>
<p>I'm quite new to this, so any ideas and suggestions would be appreciated</p>
","chatgpt"
"41049","Why is ChatGPT fast on some questions and slow on others?","2023-06-29 16:55:50","","1","537","<chatgpt>","<p>When using ChatGPT i have noticed it is fast on some questions and slow on others. Why is that?</p>
<p>Based on my understanding of the transformers architecture, the user prompt is tokenized and the vector of token ids is fed as input to a strictly feed-forward neural network and the output of the model becomes part of the input for the next token that the model has to generate (this gives the model its auto-regressive nature).</p>
<p>Since there are no conditional loops in the NN, I would expect the time to generate the first token should depend on the length of user prompt. It shouldn't matter whether the question being asked is easy or tough. But I have clearly observed that when asked an easy question ChatGPT responds very fast and when asked a tough question it takes time to respond. Why is that? Is it due to caching of previous responses?</p>
","chatgpt"
"41035","ChatGPT $20/month vs. ChatGPT API","2023-06-28 20:20:02","41041","0","937","<chatgpt>","<p>I am curious if there is any difference between (A) accessing ChatGPT4 via their web page and \$20/month subscription on the one hand, or (B) via an application like MacGPT with their paid API key.</p>
<p>I've used (B) and no matter how much I use it I don't get charged more than \$5 or maybe \$10 per month.</p>
<p>Does (A) have any advantages that anyone knows about?</p>
","chatgpt"
"41015","Keeping my prompt content private","2023-06-27 18:26:16","41024","1","133","<chatgpt>","<p>I am using ChatGPT and friends a lot for work and professional info. They work amazingly well. However I have an application in mind where I can't upload my prompt content to the cloud because it is confidential and private.</p>
<p>I tried PrivatGPT (<a href=""https://github.com/imartinez/privateGPT"" rel=""nofollow noreferrer"">https://github.com/imartinez/privateGPT</a>) and it didn't work. I spent only about 30 minutes messing with it. It seems to have the right idea but when I typed in a prompt, after quite a while computing, it printed out gibberisg, that is random characters. I think I read that someone else had that experience</p>
<p>What other options do I have?</p>
","chatgpt"
"41011","Does (English) ChatGPT-generated content have statistically significantly different character frequency than human-generated content?","2023-06-27 08:28:12","","1","44","<chatgpt><statistics>","<p>Detecting ChatGPT-generated content is a contentious topic at the moment.  E.g., I've been on Reddit r/ChatGPT, and there's a constant stream of users claiming they've been unfairly accused of plagiarism.</p>
<p>One thing I'm curious about is character frequency, i.e., how frequently each English character (a, b, ..., z) occurs in ChatGPT-generated text vs. human-generated text.  I'm not sure if a statistically significant difference is present, and maybe there's research into this.  My guess would be that it's almost impossible to detect a difference.</p>
<p><strong>Question</strong>: Does (English) ChatGPT-generated content have statistically significantly different character frequency than human-generated content?</p>
<p>(For this question, I want to ask about English only.)</p>
<p>I asked <a href=""https://vitalentum.net/free-chat-gpt"" rel=""nofollow noreferrer"">ChatGPT</a> this question and it said:</p>
<blockquote>
<p>... Overall, it is possible that ChatGPT-generated content has statistically significant differences in character frequency compared to human-generated content, but this would depend on the specific training data and settings used to train the model. Further research would be needed to determine the extent of these differences.</p>
</blockquote>
<p>So maybe there's some subtle difference.  I didn't find a related post by <a href=""https://cn.bing.com/search?q=Does+(English)+ChatGPT-generated+content+have+statistically+significantly+different+character+frequency+than+human-generated+content%3F"" rel=""nofollow noreferrer"">Bing-searching the question</a>.</p>
<hr />
<p>I stumbled upon this paper which points out how scientists use more capital letters than ChatGPT:</p>
<blockquote>
<p>Scientists also use more proper nouns and/or acronyms, both of which
are captured in the frequency of capital letters, and scientists use more numbers.<br />
<sub>Desaire et al., <a href=""https://doi.org/10.1016/j.xcrp.2023.101426"" rel=""nofollow noreferrer"">Distinguishing academic science writing from humans or ChatGPT with over 99% accuracy using off-the-shelf machine learning tools</a>, Cell Reports Physical Science 4, 101426, 2023.</sub></p>
</blockquote>
","chatgpt"
"40928","ChatGPT4all to create chatbot to answer questions on your own docs without external calls","2023-06-22 13:16:13","","0","652","<chat-bots><chatgpt>","<p>So, I came across this <a href=""https://artificialcorner.com/gpt4all-is-the-local-chatgpt-for-your-documents-and-it-is-free-df1016bc335"" rel=""nofollow noreferrer"">Tutorial</a> (Apologies, if you cannot access it, it is a member's only story) and I gave it a shot. Technically, it &quot;works&quot;. However, it seems to be a bit poor in the sense that I only fed it 5-600 PDF files and even if I ask a question copying the title of the file, it gives some other answers. I played around with the &quot;template&quot; variable and this seems to be the best to me. Basically, I just want it to answer questions from the &quot;context&quot; which is basically an index of my docs. Any suggestions on how to improve this?</p>
<pre><code>import os
from langchain import PromptTemplate, LLMChain
from langchain.llms import GPT4All
from langchain.callbacks.base import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import UnstructuredPDFLoader
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import DirectoryLoader
from langchain.indexes import VectorstoreIndexCreator
from langchain.embeddings import LlamaCppEmbeddings
from langchain.vectorstores.faiss import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

# Assign the path for the GPT4All model
gpt4all_path = './models/gpt4all-converted.bin'

# Callback manager for handling calls with the model
callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])

# Create the HuggingFace embeddings object
embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

# Create the GPT4All LLM object
llm = GPT4All(model=gpt4all_path, callback_manager=callback_manager, verbose=True)

# Load our local index vector db
index = FAISS.load_local(&quot;my_faiss_index&quot;, embeddings)

# Create the prompt template
template = &quot;&quot;&quot;Using only the information provided: {context}
Please provide an answer to the following question: {question}
Answer:
&quot;&quot;&quot;

# Function to handle similarity search and return the best answer
def get_best_answer(question):
    matched_docs, sources = similarity_search(question, index, n=1)
    context = &quot;\n&quot;.join([doc.page_content for doc in matched_docs])
    prompt = PromptTemplate(template=template, input_variables=[&quot;context&quot;, &quot;question&quot;]).partial(context=context)
    llm_chain = LLMChain(prompt=prompt, llm=llm)
    answer = llm_chain.run(question)
    return answer

# Function to handle similarity search
def similarity_search(query, index, n=4):
    matched_docs = index.similarity_search(query, k=n)
    sources = []
    for doc in matched_docs:
        sources.append(
            {
                &quot;page_content&quot;: doc.page_content,
                &quot;metadata&quot;: doc.metadata,
            }
        )
    return matched_docs, sources

# Main loop for continuous question-answering
while True:
    # User input for the question
    question = input(&quot;Please enter your question (or type 'exit' to close the program): &quot;)

    # Check if the user wants to exit the program
    if question.lower() == &quot;exit&quot;:
        break

    # Get the best answer
    answer = get_best_answer(question)
    
    # Print the answer
    print(&quot;Answer:&quot;, answer)

# End of the program
</code></pre>
<p>One very irritating thing about this is also that it prints the whole &quot;template&quot; variable, I cannot seem to get rid of it, because I must use the &quot;context&quot;, and even if it gets the right context 95% of the time, it still gives a wrong answer, not sure why?</p>
<p>EDIT:- I see that there are LLMs you can download and feed your docs and they start answering questions about your docs right away. So, I think  steering the GPT4All to my index for the answer consistently is probably something I do not understand. It should not need fine-tuning or any training as neither do other LLMs. So, my guess is that I am lacking in the &quot;template&quot; area? maybe and perhaps tempereture, top_p etc. :(</p>
","chatgpt"
"40874","How do language models know what they don't know - and report it?","2023-06-16 09:10:38","","1","443","<chatgpt><knowledge-representation><explainable-ai><large-language-models><uncertainty-quantification>","<p>Again and again I ask myself what goes on in a pre-trained transformer-based language model (like ChatGPT9) when it comes to &quot;know&quot; that it cannot give an appropriate answer and either</p>
<ul>
<li><p>states it (&quot;I have not enough information to answer this question.&quot;)</p>
</li>
<li><p>asks for more specific information (&quot;Please tell me which kind of XY you mean.&quot;)</p>
</li>
<li><p>calls a plugin (like Wolfram or ScholarAI)</p>
</li>
</ul>
<p>(I assume that this will never happen without reinforcement learning by human feedback. A pre-trained-only model would always answer something (possibly hallucinating) and not &quot;reflect&quot; about its lack of knowledge.)</p>
<p>The only possibility that I can see - but it's not really explanatory: that after some steps of execution the sum of the top_k probabilities of the final vector (which gives probabilities to the all words in the vocabulary) is too small. But what, when this happens only late? ChatGPT would already have produced lots of words - but one never observes that he stops generation after some lengthy text and only then ends with something like &quot;Ah, finally I see that I'm missing information. I wasn't aware in the beginning.&quot; ChatGPT <em>immediately</em> admits that he doesn't know (when he does). And when ChatGPT calls a plugin - e.g. ScholarAI - he does it without having produced a single word of response to the last message.</p>
<p>In principle, ChatGPT could generate a complete response in the background that then is checked somehow if it's &quot;satisfactory&quot;. If yes it's given as output (simulating word-by-word generation), if not, it's regenerated with some sort of trigger (a hidden token?) to admit that ChatGPT is missing information or to call a plugin.</p>
<p>What's the clever trick under the hood (in some technical detail)?</p>
","chatgpt"
"40707","is it possible to make the chatgpt only contains result content without intro or summary","2023-06-05 12:58:56","","0","1111","<chatgpt>","<p>I need to generate some job description, now I trying to use chatGPT to generate it, this is the prompt looks like:</p>
<pre><code>please generate a job descriptions of java develop engineering. the output only contains description, no other info. each item seperate by *. this is an example: * maintaining Java-based applications, contributing to the design, coding, testing, and deployment of software solutions * excellent problem-solving skills, attention to detail, and the ability to work independently or as part of a team
</code></pre>
<p>this prompt generate the result, is it possible to remove the intro and summary from the result? I just want the JD items.</p>
","chatgpt"
"40564","Why does ChatGPT fail in playing ""20 questions""?","2023-05-24 13:32:59","","24","11843","<natural-language-processing><chatgpt><benchmarks>","<p><a href=""https://en.wikipedia.org/wiki/IBM_Watson"" rel=""noreferrer"">IBM Watson</a>'s success in playing <a href=""https://en.wikipedia.org/wiki/IBM_Watson#Jeopardy!"" rel=""noreferrer"">&quot;Jeopardy!&quot;</a> was a landmark in the history of artificial intelligence. In the seemingly simpler game of <a href=""https://en.wikipedia.org/wiki/Twenty_questions"" rel=""noreferrer"">&quot;Twenty questions&quot;</a> where player B has to guess a word that player A thinks of by asking questions to be answered by &quot;Yes/No/Hm&quot; ChatGPT fails epically - at least in my personal opinion. I thought first of Chartres cathedral and it took ChatGPT 41 questions to get it (with some additional help), and then of Kant's Critique of Pure Reason where after question #30 I had to explicitly tell ChatGPT that it's a book. Then it took ten further questions. (Chat protocols can be provided. It may be seen that ChatGPT follows no or bad question policies or heuristics humans intuitively would use.)</p>
<p>My questions are:</p>
<ol>
<li><p>Is there an intuitive understanding why ChatGPT plays &quot;20 questions&quot; so bad?</p>
</li>
<li><p>And why do even average humans play it so much better?</p>
</li>
<li><p>Might it be a future <a href=""https://openreview.net/pdf?id=yzkSU5zdwD#:%7E"" rel=""noreferrer"">emergent ability</a> which may possibly arise in ever larger LLMs?</p>
</li>
</ol>
<p>I found two interesting papers on the topic</p>
<ol>
<li><p><a href=""https://evanthebouncy.medium.com/llm-self-play-on-20-questions-dee7a8c63377"" rel=""noreferrer"">LLM self-play on 20 Questions</a></p>
</li>
<li><p><a href=""https://arxiv.org/ftp/arxiv/papers/2301/2301.01743.pdf"" rel=""noreferrer"">Chatbots As Problem Solvers: Playing Twenty Questions With Role Reversals</a></p>
</li>
</ol>
<p>The first one answers some of my questions partially, e.g. that &quot;gpt-3.5-turbo has a score of 68/1823 playing 20 questions with itself&quot; which sounds pretty low.</p>
","chatgpt"
"40510","Is it possible to create a multiple-choice question from Youtube video using Chatgpt?","2023-05-19 22:33:48","","1","622","<chatgpt>","<p>Is it possible to create a multiple-choice question about a Youtube video using Chatgpt (or others)?</p>
","chatgpt"
"40447","Does ChatGPT use different transformers for different downstream tasks?","2023-05-14 10:16:25","","3","1142","<natural-language-processing><transformer><chatgpt><multi-task-learning>","<p>What I find hard to figure out is whether ChatGPT guesses from the prompt the downstream NLP task to be performed - text summary, text generation, question-answering, doing logic or arithmetic, translation,  sentiment or style analysis - and then uses specialized decoders/transformers. Or if there is only one transformer which handles all downstream tasks. How then can it be understood that ChatGPT performs so well in so many tasks - <strong>as if</strong> it used specialized transformers.</p>
<p>If it guesses the task: How is it done (in high-level terms) and how does it switch?</p>
<p>The answer may be so clear (for the experts) that it is never mentioned explicitly, but for the non-expert it is hard to tell (and to believe).</p>
<p>(Maybe it's easier to answer the question if there is a specific and specifically trained transformer for each supported language.)</p>
<p>BTW: Why is the task &quot;to follow instructions&quot; (which InstructGPT is said to be specialized for) a task on its own? Isn't every prompt an instruction in a sense, instructing ChatGPT to perform some downstream task?</p>
","chatgpt"
"40385","What is a neuron in large language models?","2023-05-10 05:20:28","40386","-1","137","<natural-language-processing><transformer><chatgpt><gpt><artificial-neuron>","<p>I'm reading OpenAI's new paper &quot;<a href=""https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html"" rel=""nofollow noreferrer"">Language models can explain neurons in language models</a>&quot; And I can't fully understand the concept of neurons here.</p>
<p>Can you please explain it? Is it related to the attention mechanism?</p>
","chatgpt"
"40330","How is ChatGPT able to perform part-of-speech tagging?","2023-05-06 20:04:46","","3","418","<natural-language-processing><chatgpt>","<p>ChatGPT seems to be able of part-of-speech tagging:</p>
<p><a href=""https://i.sstatic.net/bCCuv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bCCuv.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/buv59.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/buv59.png"" alt=""enter image description here"" /></a></p>
<p>How can its – possibly <a href=""https://www.jasonwei.net/blog/emergence"" rel=""nofollow noreferrer"">emergent</a> – ability to perform part-of-speech tagging be understood?</p>
","chatgpt"
"40313","How can ChatGPT get simple two-statement logic wrong?","2023-05-04 23:35:22","","0","767","<chatgpt>","<p>(<em>The example here isn't meant to offend or to take a political position, I simply needed two recent consecutive former presidents.</em>)</p>
<p>I realize that ChatGPT can make mistakes and can lie.
But I thought that it would at least be able to follow simple logic.</p>
<p>Yet in the following, it makes a very significant error in logic:</p>
<blockquote>
<h3>User</h3>
<p>Logically, these two sentences mean exactly the same thing: &quot;Trump was the worst president since Obama.&quot; and &quot;Trump was a better president than Obama.&quot;.  Can an AI tell that their intended meanings are quite different?</p>
<h3>ChatGPT</h3>
<p>Yes, an AI can understand that the intended meanings of the two sentences are quite different.
While the literal meaning of the two sentences may appear similar, the context in which they are used changes the meaning entirely.</p>
<p>The first sentence implies that Trump was a worse president than all the presidents who came after Obama.
It also suggests that Obama was a better president than Trump.</p>
<p>On the other hand, the second sentence implies that Trump was a better president than Obama.
This statement could be true or false, depending on one's political perspective and beliefs.</p>
<p>An AI model like GPT-3 is capable of analyzing the context and inferring the intended meaning of a sentence based on the words used, grammar, and surrounding text.
Therefore, an AI can understand the nuances between these two sentences and interpret them differently based on their context.</p>
</blockquote>
<p>The claim that &quot;<em>It also suggests that Obama was a better president than Trump.</em>&quot; is the exact opposite of what the first sentence actually suggests.</p>
<p>Given that it is impressively good at so many other things, how does it get such simple, two statement logic wrong?</p>
","chatgpt"
"40232","Process 2TB worth of conversational data hoarded over 40 years. How can I pass this into GPT to ask questions about it?","2023-04-28 19:25:40","","3","635","<natural-language-processing><data-preprocessing><chatgpt><gpt>","<p>I'm still very new to this stuff. I have close to 2TB worth of data hoarded from IRC chats to everyday chats with friends and family.</p>
<p>But is there a way to pass in this much data into GPT to ask questions about it? Or would I require something else?</p>
<p><strong>For example:</strong></p>
<p>&quot;When did Bob tell Jane about the legos he had in school when they were at home?&quot;</p>
","chatgpt"
"40167","How does GPT-based language model like ChatGPT determine the n-th letter of a word?","2023-04-23 02:30:19","","6","642","<natural-language-processing><chatgpt><gpt><natural-language-understanding><language-model>","<p>I understand that GPT models process input text by converting words into tokens and then embedding vectors and do not process them letter by letter. Given this approach, I am curious to know how a model like ChatGPT can identify the first (or n-th) letter of a given word. Can anyone explain the underlying mechanism or provide any insights on this capability?</p>
<p><a href=""https://i.sstatic.net/qZNNx.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/qZNNx.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/2hGyg.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/2hGyg.png"" alt=""enter image description here"" /></a></p>
","chatgpt"
"40123","How to guide the interaction between two ChatGPT conversational agents?","2023-04-19 18:17:48","","0","927","<machine-learning><chatgpt><prompt><prompt-design>","<p>I wrote a code for a conversation between 2 ChatGPTs (which I call them agent 1 and agent 2) using poe.com and its <a href=""https://pypi.org/project/poe-api/"" rel=""nofollow noreferrer"">reversed engineered API</a>:</p>
<pre><code>import poe

agent_1 = {&quot;token&quot;:&quot;&quot;}
print(&quot;Agent 1: ON&quot;)
agent_2 = {&quot;token&quot;:&quot;&quot;}
print(&quot;Agent 2: ON&quot;)

client_1 = poe.Client(agent_1[&quot;token&quot;])
client_2 = poe.Client(agent_2[&quot;token&quot;])

model = &quot;chinchilla&quot;

client_1.purge_conversation(model)
print(&quot;Agent 1: Ready&quot;)
client_2.purge_conversation(model)
print(&quot;Agent 2: Ready&quot;)

print(&quot;Initial Prompt:&quot;)
message = input()

def remove_ai_phrase(sentence):
    &quot;&quot;&quot;
    Removes the phrase &quot;As an AI language model&quot; from a sentence.
    &quot;&quot;&quot;
    ai_phrase = &quot;As an AI language model&quot;
    if ai_phrase in sentence:
        # Find the end of the sentence (i.e., the position of the last period)
        sentence_end = sentence.rfind(&quot;.&quot;)
        # Remove the ai_phrase from the sentence
        sentence = sentence[:sentence_end].replace(ai_phrase, &quot;&quot;) + sentence[sentence_end:]
    return sentence

c = 0
while True:
    if c == 0:
        message = message
    c += 1
    print(&quot;\n&quot;,50*&quot;=&quot;)
    print(&quot;\nAgent 1:&quot;)
    answer_1 = &quot;&quot;
    for chunk in client_1.send_message(model, message):
        print(chunk[&quot;text_new&quot;], end=&quot;&quot;, flush=True)
        answer_1 += &quot;&quot;+chunk[&quot;text_new&quot;]
    print(&quot;\n&quot;)
    print(&quot;\n&quot;,50*&quot;=&quot;)
    print(&quot;\nAgent 2:&quot;)
    # answer_1 += &quot;\nIn addition...&quot;
    answer_1 = remove_ai_phrase(answer_1)
    answer_2 = &quot;&quot;
    for chunk in client_2.send_message(model, answer_1):
        print(chunk[&quot;text_new&quot;], end=&quot;&quot;, flush=True)
        answer_2 += &quot;&quot;+chunk[&quot;text_new&quot;]
    print(&quot;\n&quot;)
    message = answer_2 
    message = remove_ai_phrase(message)

</code></pre>
<p>Based on my few experiments, when I set the initial prompt on something like &quot;discuss the meaning of life&quot;, they stuck at complementing each other. But if I set the initial prompt to something short and meaningless like &quot;1&quot;, they actually start having conversation on various topics.</p>
<p><strong>How do you think we can make them talk about a specific topic and reflect on each other?</strong></p>
","chatgpt"
"40111","OpenAI: What is the difference between model ""gpt-3.5-turbo"" and ""gpt-3.5-turbo-0301""?","2023-04-18 13:51:56","40165","3","7665","<open-ai><chatgpt><large-language-models>","<p>I have performed an API call to OpenAI's endpoint <a href=""https://api.openai.com/v1/models"" rel=""nofollow noreferrer"">https://api.openai.com/v1/models</a> .</p>
<p>The endpoint lists the currently available engines, and provides basic information about each one such as the owner and availability.</p>
<p>As a logged-in user, I get a JSON response of 63 models. These are the most recent ones (currently) , formatted, shown with release date.</p>
<pre><code>59: &quot;11/28/2022, 2:40:35 AM : text-davinci-003&quot;
60: &quot;12/16/2022, 8:01:39 PM : text-embedding-ada-002&quot;
61: &quot;2/27/2023, 10:13:04 PM : whisper-1&quot;
62: &quot;2/28/2023,  7:56:42 PM : gpt-3.5-turbo&quot;
63: &quot;3/1/2023,   6:52:43 AM : gpt-3.5-turbo-0301&quot;
</code></pre>
<p>I notice that there are 2 very similar models , &quot;gpt-3.5-turbo&quot; and &quot;gpt-3.5-turbo-0301&quot;, with <code>gpt-3.5-turbo-0301</code> released only 11 hours after <code>gpt-3.5-turbo</code>.</p>
<p>What is the difference between these two model versions? It does not seem to be a glitch or a misnaming error. Why did OpenAI bother to include both of them, and why didn't take the inferior version?</p>
<p>(I haven't experimented with these two models in any way yet. I might do this very soon. However I though I might as well ask here. Informing others in this forum might have some benefit.)</p>
","chatgpt"
"40013","Why doesn't ChatGPT ask questions?","2023-04-12 08:17:38","","1","854","<natural-language-processing><chatgpt><natural-language-generation>","<p>As far as I understand ChatGPT has been trained on a vast array of data, and it <em>does understand</em> questions; but it seems to never ask. Even if a person would ask clarifying questions (that I assume are in the train set) ChatGPT doesn't, opting instead to invent context or just say &quot;X depends on Y, Z&quot;...</p>
<p>Not asking questions seems trained into the network, but I am not sure how one would go about training a model not to generate questions in such a way that not even in DAN mode it doesn't. I understand that for toxic language GPT-3 uses human raters to generate a train set and then optimizes for non toxic behavior, but it seems to specific to be used for the general concept of questions.</p>
","chatgpt"
"39929","Summarizing articles with ChatGPT","2023-04-06 03:32:04","","1","293","<natural-language-processing><open-ai><chatgpt>","<p>You cannot upload files such as articles on ChatGPT. As an alternative I provided links to ChatGPT of the corresponding webpages but it is clear from its summaries that it is not able to open the link and is not actually summarizing that article. Similarly, telling it the title of the article and author doesn't work because it does not access the article. For example, if you give it the prompt &quot;Write the 4th paragraph of this article: <a href=""https://www.msn.com/en-ca/news/canada/this-ontario-teacher-wants-schools-to-be-more-open-to-muslim-student-needs-starting-with-prayer/ar-AA19wzbe?ocid=hpmsn&amp;cvid=b735bc2290e14ee385647759d7de753f&amp;ei=11"" rel=""nofollow noreferrer"">https://www.msn.com/en-ca/news/canada/this-ontario-teacher-wants-schools-to-be-more-open-to-muslim-student-needs-starting-with-prayer/ar-AA19wzbe?ocid=hpmsn&amp;cvid=b735bc2290e14ee385647759d7de753f&amp;ei=11</a> &quot; it will not write the correct paragraph so I am not sure how it summarizes the article. On the other hand, you need to pay to access some articles so I find it unclear how it summarizes those articles. Also, the articles can be long so if you paste sections in separate chunks ChatGPT will summarize each section, which is not ideal because it should summarize the whole article at the same time.</p>
<p>How do you summarize articles with ChatGPT?</p>
","chatgpt"
"39885","Can models like chatGPT learn functions with infinite domain or range","2023-04-02 12:52:22","39887","2","94","<neural-networks><deep-neural-networks><chatgpt>","<p>Lets assume two types of prompt:
A fixed prompt for which reasonable responses can be infinite. For example:</p>
<pre><code>
&gt; output a random number
&gt; output a palindrome
</code></pre>
<p>A prompt that can slightly varied to get different output each time which reasonable mapping. Quotes mark what will be varied.</p>
<pre><code>&gt; what is the double of &quot;2&quot;
&gt; echo &quot;xyz&quot;
</code></pre>
<p>Now that the network is trained on finite data and has finite parameters, does it inherently   prevent it from perfectly answering these questions each time.</p>
<p>If yes, can a tiny network be handcrafted to model an infinite domain function and its only the &quot;learning from data&quot; part that prevents it from learning such functions.</p>
","chatgpt"
"39872","Why ChatGPT output one token at a time?","2023-03-31 22:46:21","","0","51","<transformer><attention><chatgpt><large-language-models>","<p>My understanding of the language model is that the output of the model is a tensor. So the whole output should be computed all together. But why ChatGPT like models can output one token at a time like a stream? And the time difference between the output of first and last token is significant.</p>
","chatgpt"
"39837","Meaning of roles in the API of GPT-4/ChatGPT (system/user/assistant)","2023-03-29 20:10:03","","38","46193","<chatgpt><gpt-4><prompt><prompt-design>","<p>In the API of GPT-4 and ChatGPT, the prompt for a chat conversation is a list of messages, each marked as one of three roles: <code>system</code>, <code>user</code> or <code>assistant</code>.*</p>
<p>I understand which information this represents - but what does the model with that information?</p>
<p>Is there a difference between a chain of messages with user and assistant alternating, compared to the same messages, with all the same role of either user or assistant?</p>
<p>To make a difference, it seems the role would need to be encoded into the prompt of the language model. Otherwise, it would be just the concatenation of the previous messages.</p>
<p><strong>So, what is the effect of the roles?</strong></p>
<hr />
<ul>
<li>There is also the role <code>function</code> since recently, but that has a specific well defined purpose, so it is not relevant here.</li>
</ul>
","chatgpt"
"39828","Can I reduce computation by only predicting response tokens in a transformer and still get the same gradients?","2023-03-29 04:27:11","","1","45","<natural-language-processing><python><transformer><chatgpt><language-model>","<p>I have been looking at the source code of the Stanford Alpaca model and I believe that during inference, the whole instruction + response data is fed into the model normally. Then the instruction part of the label is masked with IGNORE_INDEX to prevent gradient calculation on the instruction.</p>
<p>But I believe that in the transformer network, after the attention blocks and before the last head layer, it should be possible to take only the (embedded) tokens corresponding to the response parts from contexts and avoid predicting from the tokens corresponding to the instruction part altogether. This could potentially save computation, especially when the instruction part is long or when training on back-and-forth conversations, similar to interactions with ChatGPT. So my question is:</p>
<hr />
<ol>
<li>Is the gradients calculated by these two approaches the same?</li>
</ol>
<p>I actually tried to ask GPT-4 about this. Initially, he suggested that my approach would lose some information, but later changed his opinion when I asked him further. Would my approach indeed result in a loss of information or any other drawbacks?</p>
<hr />
<ol start=""2"">
<li>Is the saving, if possible, worth the effort of modifying the model's source code?</li>
</ol>
<p>Since the change would only affect one matrix multiplication, which is already efficiently computed, I'm unsure if it's worth the trouble. Furthermore, I only know a little bitPyTorch and would definitely struggle with implementing modifications to complicated models written in other frameworks like TensorFlow or JAX.</p>
<hr />
","chatgpt"
"39813","What temperature would you recommend for the chatgpt api?","2023-03-27 16:02:05","","2","4575","<chatgpt><natural-language-generation><large-language-models>","<p>I believe that it is recommended to have a tiny bit of temperature with GPT 3 even for noncreative tasks like 0.2 or something (I am not entirely sure why).</p>
<p>Last I checked, and if I remember correctly, the examples from openai on their GitHub page use 0 temperature.</p>
<p>Is there any benefit in choosing a non-zero temperature for the chatgpt API when the query does not request a creative task? If so, are there some categories or examples?</p>
<p>[EDIT: to make the answer less subjective, perhaps I could ask what are the benefits of increasing the temperature in the chatGPT API]</p>
","chatgpt"
"39738","How is GPT 4 able to solve math?","2023-03-22 23:49:46","","7","12229","<chatgpt><gpt><gpt-4>","<p>How can GPT 4 solve complex calculus and other math problems. I believe these problems require analytical reasoning and ability to compute numbers. Does it still use a LLM to complete this process or does it add on to this?</p>
<p><a href=""https://openai.com/research/gpt-4"" rel=""noreferrer"">Here</a> is the link to the official results published by OpenAI</p>
","chatgpt"
"39736","How do the ""built on ChatGPT"" apps work in general at a high level?","2023-03-22 19:01:10","","5","363","<chatgpt>","<p>I see stuff in my Twitter feed like <a href=""https://twitter.com/rowancheung/status/1638580149221310466?s=20"" rel=""noreferrer"">this</a> every day, showing &quot;ChatGPT used to create X new customized/tailored AI feature&quot;.</p>
<blockquote>
<ol>
<li>ColorGPT: Generate a color hex name from real-world color capture from your iPhone.</li>
<li>Castmagic: AI content for podcasts &amp; long-format audio.</li>
<li>AI Gift for you: AI-powered search for gift ideas based on personalized filters.</li>
<li>Cody: ChatGPT with the added benefit of being able to train it on your business, your team, your processes, and your clients with your own knowledge base.</li>
<li>Gitfluence: Helps you quickly find the correct Git command in seconds.</li>
<li>etc..</li>
</ol>
</blockquote>
<p>How are they taking advantage of the ChatGPT API to accomplish these sorts of things (at a high level)? For example, to create the personalized gift filters, how do you go from an arbitrary ChatGPT prompt to stuff specific to products? Or the Gitfluence, how does it tailor ChatGPT to search specifically for git commands?</p>
<p>There is a missing link in my head on how you go from ChatGPT prompt to &quot;AI-feature-based startup hack&quot;. Basically, what is that missing link, what do you need to think about to tailor ChatGPT to a specific niche? Roughly how does it work?</p>
<p>I am a software engineer with much experience, so I am aware how to build apps in general, I just don't see what is going on behind the scenes for these sort of &quot;built on ChatGPT&quot; feature apps, or at a high level how it works.</p>
","chatgpt"
"39704","How code analysis part of ChatGPT works and trained","2023-03-21 10:46:00","","2","203","<neural-networks><transformer><chatgpt>","<p>ChatGPT can explain given code snippet we also ask question like &quot;What does this variable do&quot; , &quot;Why this is used&quot; and all. I gave C++ function snippet from an popular Open Source project it was able explain complete context how this function can be used and it is used in project even though snippet I gave only contained that particular function. This has baffled me. I understand that they might have fed all Open Source project codebases to it.</p>
<p>But what I wonder how might they have trained it. Like given the code how to make Neural networks do analysis on it and make it answer the query? I'm really curious to learn it.</p>
<p>I know OpenAI did not really publish proper research paper on this. But still anyone can tell me or guide me how <strong>might</strong> they have done it.</p>
","chatgpt"
"39691","Repainting a picture in the style of some painter (or of another picture)","2023-03-20 16:47:27","","1","196","<chatgpt><gpt-3><instruct-gpt>","<p>It sounds like a straight-forward task for DALL-E (and GPT?) to present a painting and ask  to repaint it &quot;in the style of Leonardo da Vinci&quot;. Like one can present texts and ask to rewrite them in the style of some author. Or even better: to present two paintings and ask to repaint the first in the style of the second. (You may replace &quot;paint&quot; by &quot;draw&quot;.)</p>
<p>Can this already be achieved - and how? Or is it possibly in the pipeline?</p>
","chatgpt"
"39689","If we prompt a large language model on a task, will its ability for other tasks be affected? How to recover?","2023-03-20 14:14:24","","1","202","<natural-language-processing><chatgpt><language-model><large-language-models><prompt>","<p>For example, I guess that for some retrieval augmented LLMs, their generated contents may lack some creativity? Recent work has explored the inability of retrieval augmented methods to enhance the reasoning of LLMs. Then imagine a scenario where we prompt ChatGPT in order to have it implement a task related to information extraction, and suppose we continue the session process and we want it to answer a question for us ( here the task transform from <em>Information Extraction</em> to <em>Open-domain QA</em>), then will the LLM remain the same ability to answer a question?</p>
<p>So, a natural consideration is: when prompt has become a paradigm for aligning LLM with human needs, how can we <strong>eliminate the impact of prompt and recover</strong> LLM?</p>
<p>But perhaps a more economical strategy would be to open a new session window and give the LLM a prompt about the open-domain QA.</p>
","chatgpt"
"39686","How is ChatGPT aware of today's date?","2023-03-20 10:39:28","39687","41","30876","<chatgpt>","<p>I asked ChatGPT (3.5 and 4) about current date and (s)he answered correctly. In subsequent conversation (s)he was not able to explain how (s)he has this knowledge.</p>
<p>I always thought that the model only sees the conversation above and a pretrained NN is used. How is the information about current date injected into his/her knowledge?</p>
","chatgpt"
"39681","How does transformer models like GPT generate valid meaningful response for meaningless garbage input?","2023-03-20 06:00:53","39701","1","199","<transformer><attention><chatgpt><gpt>","<p>My understanding of a transformer model is that it uses the given input to calculate internal query of relate-ness of word meanings, and generate a meaningful response based on its meaning. But if your given sentence has no meaning, then won't the model fail to capture any meaningful input so that the output will also be meaningless? How does the ChatGPT generate meaningful response asking me what I meant when the input is uncoordinated and meaningless? Is that a feature, or did they train on that kind of input specifically?<br />
eg. Input &quot;Jumps the dog lazy fox over quick brown the.&quot;<br />
For the output, ChatGPT asks for clarification.<br />
Normally for a self-attention based model, there won't be any relation between input word for this example, so shouldn't the output also be garbage like?</p>
","chatgpt"
"39656","Combining GANs and NLP for AI-Based Programming: Generating Input-Output Templates for Computer Functions","2023-03-18 20:17:01","","2","32","<tensorflow><python><chatgpt><encoder-decoder>","<p>I would like to combine <code>GANs</code> and <code>NLP</code> to create a system that can take an input and generate an appropriate output. For example, given the input <code>9 to the power of 2</code>, the system would output <code>pow(9,2)</code>.</p>
<p>I am not entirely sure how to research this, but I can provide some examples of what I am trying to accomplish.</p>
<p>Example:</p>
<p>Input: <code>2 * 5</code></p>
<p>Output: <code>mul(2,5)</code></p>
<p>Input: <code>(5*3) + 9 / 7</code></p>
<p>Output: <code>sum(mul(5,3),div(9,7))</code></p>
<p>My ultimate goal is to combine AI with programming to give artificial intelligence the ability to use many computer functions. For example, it could retrieve the current date with <code>Date.now()</code> or use <code>ParalelAICapture.getSubject()</code> to identify what is currently in front of it.</p>
<p>Actually, code suggestion algorithms are giving me somewhat similar results to what I am looking for, but they are a bit blurry for my goal. Therefore, I will use input and output templates for more precise results.</p>
<p>Can someone please explain how to achieve this and provide some examples?</p>
","chatgpt"
"39552","How is ChatGPT trained?","2023-03-13 10:03:13","","1","870","<machine-learning><natural-language-processing><training><chatgpt>","<p>According to OpenAI, ChatGPT is trained in a 3-step process.
<a href=""https://i.sstatic.net/RBf8H.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RBf8H.png"" alt=""ChatGPT_diagram"" /></a>
Are the steps where human AI trainers are involved, i.e. training the initial policy and providing the A&gt;B&gt;C&gt;D grading as training sets for the reward model, the ONLY place where actual knowledge is entered? Are there no other learning steps where human trainers are NOT involved and the model learns from high-quality sources like authoritative texts? The sampled prompts in step 2 are supposed to cover EVERYTHING on the internet, so what happens when the trainers do not know anything about the topic? Did OpenAI invite domain experts to grade specific prompts-answer pairs?</p>
","chatgpt"
"39409","Tool for detecting AI Generated code?","2023-03-03 21:30:12","","1","2377","<chatgpt><gpt-3>","<p>Can you recommend any tools for detecting AI generated <strong>code</strong>? I am aware of AI-generated text detectors such as <a href=""https://www.zerogpt.com"" rel=""nofollow noreferrer"">ZeroGPT</a> and <a href=""https://platform.openai.com/ai-text-classifier"" rel=""nofollow noreferrer"">OpenAI-Classifier</a>, but I am specifically interested in detecting <strong>AI-generated code</strong>. As of now, I have not been able to find any publicly available tools for this purpose.</p>
<p>I would greatly appreciate any guidance or recommendations on available tools or methods for detecting AI-generated code. Thank you in advance for your help.</p>
","chatgpt"
"39392","Why do we need RL in RLHF?","2023-03-02 22:19:07","","4","1034","<reinforcement-learning><chatgpt><large-language-models><rlhf>","<p>In RLHF, the reward function is a neural network. This means we can compute its gradients cheaply and accurately through backpropagation. Now, we want to find a policy that maximizes reward (see <a href=""https://arxiv.org/abs/2203.02155"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2203.02155</a>). Then, why do we need PPO to find that policy? Since the reward function is differentiable and cheap to evaluate, wouldn't some gradient descent algorithm such as SGD or Adam be enough?</p>
","chatgpt"
"39293","Is the ""Chinese room"" an explanation of how ChatGPT works?","2023-02-25 09:16:11","39297","39","16225","<agi><philosophy><natural-language-understanding><chatgpt><chinese-room-argument>","<p>Sorry if this question makes no sense. I'm a software developer but know very little about AI.</p>
<p>Quite a while ago, I read about the Chinese room, and the person inside who has had a lot of training/instructions how to combine symbols, and, as a result, is very good at combining symbols in a &quot;correct&quot; way, for whatever definition of correct. I said &quot;training/instructions&quot; because, for the purpose of this question, it doesn't really make a difference if the &quot;knowledge&quot; was acquired by parsing many many examples and getting a &quot;feeling&quot; for what's right and what's wrong (AI/learning), or by a very detailed set of instructions (algorithmic).</p>
<p>So, the person responds with perfectly reasonable sentences, without ever understanding Chinese, or the content of its input.</p>
<p>Now, as far as I understand ChatGPT (and I might be completely wrong here), that's exactly what ChatGPT does. It has been trained on a huge corpus of text, and thus has a very good feeling which words go together well and which don't, and, given a sentence, what's the most likely continuation of this sentence. But that doesn't really mean it understands the content of the sentence, it only knows how to chose words based on what it has seen. And because it doesn't really understand any content, it mostly gives answers that are correct, but sometimes it's completely off because it &quot;doesn't really understand Chinese&quot; and doesn't know what it's talking about.</p>
<p>So, my question: is this &quot;juggling of Chinese symbols without understanding their meaning&quot; an adequate explanation of how ChatGPT works, and if not, where's the difference? And if yes, how far is AI from models that can actually understand (for some definition of &quot;understand&quot;) textual content?</p>
","chatgpt"
"39278","ChatGPT claims that it doesn't get information from anywhere, so can't say why it gave a blatantly false statement. How is that possible?","2023-02-24 04:19:00","39285","4","1624","<chatgpt><question-answering>","<p>I asked ChatGPT a <a href=""https://chat.openai.com/chat/7e5fbd69-e102-44b3-9ca8-7df2c9b265b7"" rel=""nofollow noreferrer"">question</a>, and its response included this statement:</p>
<blockquote>
<p>In &quot;The Ruling Class&quot; film, which was released in 1972, Patrick McGoohan played the role of Bishop Lampton.</p>
</blockquote>
<p>Unlike many of its vague or weaselly responses, this is a direct statement of fact.
But it is false and very easy to disprove, so I wondered how it came up with that misinformation?</p>
<p>I asked it how, and it apologized and admitted it was wrong:</p>
<blockquote>
<p>Q: You say &quot;Patrick McGoohan played the role of Bishop Lampton&quot;, when in fact it was Alastair Sim that played that role.  Where did you get this misinformation?</p>
<p>A: I apologize for the error in my previous response. You are correct that Patrick McGoohan did not appear in &quot;The Ruling Class&quot; film and that Alastair Sim played the role of Bishop Lampton.</p>
<p>I apologize for any confusion or inconvenience that my previous response may have caused. As an AI language model, I strive to provide accurate information, and I will make sure to double-check my sources more carefully in the future. Thank you for bringing this to my attention.</p>
</blockquote>
<p>But it avoided answering the question:</p>
<blockquote>
<p>Q: But my question was &quot;Where did you get this misinformation?&quot;.</p>
<p>A: I apologize for the confusion. As an AI language model, I do not have personal experiences, so <strong>I did not get the misinformation from anywhere</strong>. I must have made a mistake in my response due to an error in processing or a misunderstanding of the question.</p>
</blockquote>
<p>It wouldn't have just made it up in order to provide an amazingly good answer,
so how can it give a false statement of fact and claim that it didn't get the information from anywhere? (I.e. what mechanism does it use to generate facts if they don't originate somewhere else?)</p>
<p>And worse, it actually lied while apologizing:</p>
<blockquote>
<p>I will make sure to double-check my sources more carefully in the future.</p>
</blockquote>
","chatgpt"
"39240","Does ChatGPT type slowly on purpose or is it computing?","2023-02-21 11:56:50","39242","6","3910","<chatgpt>","<p>When you give ChatGPT a prompt, does it</p>
<p>A) generate the entire response instantly (at least on the server), but then &quot;buffers&quot; it slowly to the end user (for whatever reason)</p>
<p>or</p>
<p>B) it's actually generating words (and sometimes entire sentences at once) one by one, and using (server) computational power all that while?</p>
","chatgpt"
"39168","Are ""prompt engineering"" and ""prompt design"" used as synonymous?","2023-02-15 17:24:10","","5","727","<natural-language-processing><terminology><chat-bots><chatgpt>","<p>Are &quot;prompt engineering&quot; and &quot;prompt design&quot; used as synonymous / equivalent terms on the day to day communications (not research papers) in Artificial Intelligence community ? Do you simply say &quot;prompt&quot;?</p>
<hr>
<p>I'm &quot;following&quot; questions about ChatGPT. I think that there are too many questions and online content in general that are not making the most appropriate use of terms like</p>
<ul>
<li>ChatGPT</li>
<li>prompt engineering</li>
<li>prompt design</li>
</ul>
<p>This makes hard to find helpful content.</p>
<h3>ChatGPT</h3>
<p><em>ChatGPT</em> has being used as a common name like calling a code library &quot;chatgpt&quot; on questions about the OpenAI API, mostly of them specifically about the text completions end-point, some related to packages or libraries that use this end-point. Regarding this term, IMHO, it's clear that it's too early to consider that <em>ChatGPT</em> is a common name and people should be encouraged to avoid to use it this way in order to make their post clear about what they are talking about.</p>
<h3>Prompt Engineering</h3>
<p>So far I have read stuff that I was able to find without investing too much time like</p>
<ul>
<li><p>Wikipedia article <a href=""https://en.wikipedia.org/wiki/Prompt_engineering"" rel=""nofollow noreferrer"">prompt engineering</a> that presents <em>prompt engineering</em> as something very broad.</p>
</li>
<li><p><a href=""https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api"" rel=""nofollow noreferrer"">Best practices for prompt engineering with OpenAI API</a> that focus on <strong>writing prompts</strong>.</p>
</li>
</ul>
<p>As an engineer, I like what I understood about what is <em>prompt engineering</em> from the Wikipedia article, not the second as it looks to me that trivializes the term.</p>
<p>Some questions, including here, apparently use <em>prompt engineering</em> to refer to the <em>writing prompts</em>. While might not be wrong to use this term, it looks to me to be way too broad for such specific task.</p>
<p>Related:</p>
<p>The following answers use the term &quot;prompt engineering&quot; but they look to focus on <em>writing prompts</em>:</p>
<ul>
<li><a href=""https://ai.stackexchange.com/a/39020/42632"">Answer</a> to <a href=""https://ai.stackexchange.com/q/38361/42632"">What causes ChatGPT to generate responses that refer to itself as a bot or LM?</a> (from this site)</li>
<li><a href=""https://stackoverflow.com/a/75418423/1595451"">Answer</a> to <a href=""https://stackoverflow.com/q/75349226/1595451"">How to avoid word limit in ChatGPT in R?</a> (from Stack Overflow)</li>
</ul>
<h3>Prompt design</h3>
<p>As well that I have done in relation to <em>prompt engineering</em>, so far I have read stuff  like the OpenAI API documentation, i.e. <a href=""https://platform.openai.com/docs/guides/completion/prompt-design"" rel=""nofollow noreferrer"">Prompt design</a> that focus on <em>writing prompts</em>.</p>
","chatgpt"
"39098","Does ChatGPT imply that the direction of knowledge graph is unpromising?","2023-02-10 10:14:47","","2","2006","<chatgpt><knowledge-base><knowledge-graph><large-language-models>","<p>In <a href=""https://ai.stackexchange.com/q/2922/5351"">this question</a> I asked about the role of knowledge graphs in the future, and in <a href=""https://ai.stackexchange.com/a/25788/5351"">this answer</a> I found that <em>If curation and annotation are not sufficient, the knowledge base maybe cannot apply in AI.</em></p>
<p>ChatGPT <a href=""https://levelup.gitconnected.com/what-is-chatgpt-openai-how-it-is-built-the-technology-behind-it-ba3e8acc1e9b"" rel=""nofollow noreferrer"">does not</a> utilize a knowledge graph to understand or generate common sense, then I wonder how knowledge graphs can be utilized in the future. Will they be replaced by LLMs?</p>
","chatgpt"
"39066","How do I get chatGPT to include custom knowledge?","2023-02-06 18:48:45","","5","2799","<chatgpt>","<p>While studying chatGPT's thought process, I asked it to list ten story ideas for an old and fairly niche tabletop roleplaying game (GURPS Reign of Steel). It did very well, so clearly, it can base answers on obscure sources. But what if I want to ask it about something it could not possibly have already &quot;absorbed&quot;, like someone's tiny indie rpg or an old local folk tale never published anywhere (i.e. oral tale)? Does the source material simply have to be put up online, somewhere?</p>
<p>I ask because some source material, like local folklore, is too extensive to be given during a chatGPT conversation. It would need something like a source website to be created.</p>
","chatgpt"
"39048","How does exactly censorship work in chat GPT?","2023-02-05 00:03:36","","1","2521","<chatgpt>","<p>My assumption is that after the transformer is trained, some other software analyzes the answer for anti-wokeness and modifies it accordingly, rather than being trained using woke material.<br />
I don't think this is correct though. I asked chatGPT  to answer every question by inserting a $ character in between each character of the otherwise normal answer, to make it unreadable, or at least, difficult to interpret by what I call the censorship layer. The output had these inserted characters, however, this had no effect on the censorship. But may be the censorship layer is smart enough to read in between lines.</p>
<p><strong>Question:</strong> <em>Does the censorship comes from within the transformer or after it? (and, if it comes after, why my attempt to bypass it failed?)</em></p>
","chatgpt"
"39044","Would self-hosting ChatGPT be feasible, w.r.t. computation costs?","2023-02-04 20:59:00","39550","6","1454","<open-ai><chatgpt><open-source>","<p>Suppose the pre-trained, current date (2023-02-04) ChatGPT model was released open source, would it be feasible for regular users to interact with the model on a self-hosted computer?</p>
<h2>Assumptions</h2>
<ol>
<li>I assume getting output based on some input is, at least, hundreds of times faster than training such a model.</li>
<li>I assume no additional output parsing/input limitations are used. In particular I can imagine all the boiler plate to keep the ChatGPT model(s) acting politically correct etc. may be a significant overhead. This is to be ignored for this question.</li>
</ol>
<h2>Data</h2>
<p>So far I've found the ChatGPT 3.5 model to have <a href=""https://lifearchitect.ai/chatgpt/"" rel=""noreferrer"">175 billion parameters</a>:</p>
<p><a href=""https://i.sstatic.net/DeIZD.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/DeIZD.png"" alt=""enter image description here"" /></a></p>
<p>Though I do not yet know how large that is in <code>Mb</code> nor do I have an idea on how long generating an output would typically take.</p>
","chatgpt"
"39023","Are GPT-3.5 series models based on GPT-3?","2023-02-02 16:40:02","39121","10","3338","<open-ai><fine-tuning><chatgpt><gpt-3>","<p>In the official <a href=""https://openai.com/blog/chatgpt/"" rel=""noreferrer"">blog post about ChatGPT</a> from OpenAI, there is this paragraph explaining how ChatGPT model was trained:</p>
<blockquote>
<p>We trained this model using Reinforcement Learning from Human Feedback
(RLHF), <strong>using the same methods as InstructGPT</strong>, but with slight
differences in the data collection setup. We trained an initial model
using supervised fine-tuning: human AI trainers provided conversations
in which they played both sides—the user and an AI assistant. We gave
the trainers access to model-written suggestions to help them compose
their responses. We mixed this new dialogue dataset with the
InstructGPT dataset, which we transformed into a dialogue format.</p>
</blockquote>
<p>Especially this part:</p>
<blockquote>
<p>We trained an initial model using supervised fine-tuning</p>
</blockquote>
<p>My question is about the said <em>initial model</em>, is it some new model that has been trained from scratch or is it a GPT-3 model that has been fine tuned for specific tasks resulting in <a href=""https://platform.openai.com/docs/model-index-for-researchers"" rel=""noreferrer"">GPT-3.5 series</a> ?</p>
<p>On the other hand, from the <a href=""https://openai.com/blog/instruction-following/"" rel=""noreferrer"">InstructGPT blog post</a>, it is clearly stated that:</p>
<blockquote>
<p>To make our models safer, more helpful, and more aligned, we use an
existing technique called reinforcement learning from human feedback
(RLHF). On prompts submitted by our customers to the API,our labelers
provide demonstrations of the desired model behavior, and rank several
outputs from our models. <strong>We then use this data to fine-tune GPT-3</strong>.</p>
</blockquote>
<p>So does this mean that GPT-3.5 series models (and consequently ChatGPT) are fine-tuned from GPT-3 base model ?</p>
","chatgpt"
"39012","What makes ChatGPT a generative model?","2023-02-02 08:56:48","","2","2227","<machine-learning><generative-model><probability><chatgpt>","<p>I'm working my way through how ChatGPT works. So I read that ChatGPT is a generative model. When searching for generative models, I found two defintions:</p>
<ul>
<li>A <strong>generative</strong> model includes the distribution of the data itself, and tells you how likely a given example is</li>
<li><strong>Generative</strong> artificial intelligence (AI) describes algorithms (such as ChatGPT) that can be used to create new content</li>
</ul>
<p>Do they both mean the same? That is, for generating new content, a model must learn the distribution of data itself? Or do we call chatGPT generative because it just generates new text? I see that ChatGPT is something other than a discriminative model that learns a boundary to split data, however, I can not bring ChatGPT in line with a more traditional generative model like naive bayes, where class distributions are inferred.</p>
","chatgpt"
"38990","What exactly is the future of mathematical analysis that can be dealt with AI (especially in the automated theorem proving)?","2023-02-01 05:57:15","39026","3","685","<math><chatgpt><automated-theorem-proving><philosophy-of-math>","<p>As a student of numerical analysis, I can see how mathematical analysis involved in making a language program specifically in the convergence analysis of an approximation method. But, while chatting with ChatGPT, a lot of repeating mistakes have been committed by it in an analytical proof for convergence of a real sequence, and claims</p>
<blockquote>
<p>However, I am trained on a large corpus of text, which enables me to provide accurate and helpful information.</p>
</blockquote>
<p>(Chat GPT: I apologize for the mistake. You are correct, an increasing sequence starting from 5 cannot converge to 3.)</p>
<p>But I feel there is a risk factor shielded in coding any analytical proof (instantaneous) in mathematics.</p>
<p>Of course, I know the importance of mathematical analysis in coding, My concern is 'What exactly is the future of <strong>mathematical analysis that can be dealt with AI</strong>' especially in the 'automated theorem proving'?</p>
","chatgpt"
"38970","How much energy consumption is involved in Chat GPT responses being generated?","2023-01-31 02:31:57","","22","26699","<social><ethics><chatgpt><green-ai>","<p>I note <a href=""https://ai.stackexchange.com/questions/22877/how-much-computing-power-does-it-cost-to-run-gpt-3"">this question</a> was deemed off-topic, so I'm trying to clearly frame <em>this</em> <em>question</em> in terms of scope of response I'm interested in, namely <strong>ethics</strong> and <strong>sustainability</strong> issues associated with the soon-to-be proliferation of OpenAI Chat GPT types of tools for all manner of online information seeking behavior (from humans and other bots). <em>This is not a programming or specific hardware question.</em></p>
<p><strong>On average, how much energy is consumed for each response that Open AI's public chatgpt-3 provides?</strong>
i.e. what is the energy to run the entire system for 24 hours divided by the number of responses generated in 24 hours (ignoring energy consumed to train the system or build the hardware components).</p>
<p>How does this compare to a Google/Duck Duck Go/Bing search inquiry?</p>
<p>I read somewhere an OpenAI employee on the ChatGPT team that the computer power used to provide responses to queries is &quot;ridiculous&quot;, and there's documentation of the size of the memory requirements of hosting servers and parameters but without knowing its throughput for example it's hard to quantify the energy consumption.</p>
<p>I often get more interesting results from Chat GPT than Duck Duck Go on certain types of queries where I used to know the answer but cannot remember the answer. IN these cases I can fact check for myself, I'm looking for a memory prompts with names and jargon that will remind me.</p>
<p>Also when seeking out counter-views to my own (say critiques of degrowth or heterodoxy economics concepts) Chat GPT is good at providing names and papers/reports/books that critiques the view I provide it.</p>
<p>In many cases more usefully than conventional search engines. Therefore, I can see the popularity of these tools ballooning rapidly, especially when the operational costs CAPEX + OPEX of the servers and maintainers is borne by large amounts of seed funding (eg OpenAI) or any other loss-leader startup wishing to ride the next wave of AI.</p>
<p>The heart of my question is &quot;at what <code>externalized</code> costs do we gain these tools in terms of greenhouse gases, use of limited mineral resources, GPUs scarcity etc.&quot;</p>
","chatgpt"
"38940","Would a transformer trained on highly specific material be as usable as a commercial product like ChatGPT?","2023-01-28 23:44:52","38948","2","164","<transformer><gpt><language-model><fine-tuning><chatgpt>","<p>Soft question here.</p>
<p>I was recently learning a bit about how it is feasible to train a transformer on a personal computer like an M1 Mac. I have been told that the model could have 1-3 million parameters and the training data could be from 1GB - 1TB, and that the training could take from about a day to a week. Also, there is an open source GPT <a href=""https://github.com/karpathy/nanoGPT"" rel=""nofollow noreferrer"">here</a>.</p>
<p>My question is, if you consider that ChatGPT is trained on a very large and diverse amount of data, you may think a solo project could never compete with it. However, what if you chose a specialized set of training data that was smaller but a much richer, more reliable knowledge base, like only academic science textbooks, or only English literature, or only Python libraries documentation, and so on?</p>
<p>Could it actually be much more useful because it's open-source, you have freedom of use (unlike ChatGPT's heavy behavioral conditioning from OpenAI), and you can choose what kind of knowledge the transformer contains? If the data is smaller but way, way higher quality, could you just make a library of niche GPTs for any topic you are studying?</p>
","chatgpt"
"38923","Why does ChatGPT not give the answer text all at once?","2023-01-27 15:18:09","","17","8240","<natural-language-processing><language-model><chatgpt>","<p>When ChatGPT is generating an answer to my question, it generates it word by word.<br />
So I actually have to wait until I get the final answer.</p>
<p>Is this just for show?<br />
Or is it really real-time generating the answer word by word not knowing yet what the next word will be?</p>
<p>Why does it not give the complete answer text all at once?</p>
","chatgpt"
"38854","How to assess if OpenAI's ChatGPT chatbot has a human in the loop?","2023-01-22 23:17:41","","0","1126","<transformer><attention><open-ai><chatgpt>","<p>I've asked a <a href=""https://ai.stackexchange.com/questions/38604/how-is-chatgpt-able-to-repeat-random-numbers/"">question</a> and <a href=""https://ai.stackexchange.com/a/38611/8221"">given</a> a <a href=""https://ai.stackexchange.com/a/38670/8221"">couple</a> <a href=""https://ai.stackexchange.com/a/38774/8221"">answers</a> that propose the OpenAI ChatGPT chatbot has humans in the loop (HITL), and that explains the chatbot's extraordinary abilities.</p>
<p>I've been repeatedly told this is absurd.  However, I haven't been given a clear reason why this is absurd, nor how the critic knows HITL is absurd.</p>
<p>Here are my reasons in a nutshell.</p>
<ol>
<li><p>ChatGPT's capabilities seem to violate what a neural network can do, and give explicit indication of being human driven.  See <a href=""https://ai.stackexchange.com/a/38611/8221"">this answer</a> for a running catalogue of examples I've published.</p>
</li>
<li><p><a href=""https://beta.openai.com/docs/guides/safety-best-practices"" rel=""nofollow noreferrer"">OpenAI's own documentation</a> states that HITL is best practice, and should be done whenever possible, especially in high stakes domain. OpenAI has a $10B deal on the table with Microsoft, so the ChatGPT chatbot seems like a high stakes domain.</p>
</li>
</ol>
<blockquote>
<p>Wherever possible, we recommend having a human review outputs before they are used in practice. This is especially critical in high-stakes domains, and for code generation.</p>
</blockquote>
<ol start=""3"">
<li><p>The main criticism of my position is ChatGPT's response speed.  However, a fast response is doable if the AI is responding most of the time, while humans monitor, and occasionally intervening, such as with a <a href=""https://beta.openai.com/docs/guides/completion/inserting-text"" rel=""nofollow noreferrer"">suffix prompt</a> to guide the AI response.  Plus, I've had a number of experiences where ChatGPT does not respond quickly, and it seems like a human is typing.</p>
</li>
<li><p>The other main criticism is that a company like OpenAI would never do something so fraudulent.  However, technically OpenAI does strongly imply in their documentation that they use HITL, and OpenAI has never said they don't use HITL.  It is only the popular media that claims ChatGPT is pure AI.  Additionally, use of HITL under the guise of AI is <a href=""https://www.bloomberg.com/news/articles/2016-04-18/the-humans-hiding-behind-the-chatbots"" rel=""nofollow noreferrer"">actually</a> <a href=""https://datafloq.com/read/ai-wizard-of-oz-pay-no-attention-human/"" rel=""nofollow noreferrer"">common</a> <a href=""https://www.inc.com/jessica-stillman/meet-the-people-who-teach-chatbots-to-sound-like-humans.html"" rel=""nofollow noreferrer"">practice</a> in <a href=""https://journals.sagepub.com/doi/10.1177/20539517211016026"" rel=""nofollow noreferrer"">industry</a>.  So, if ChatGPT does have HITL, this would not be fraud on OpenAI's part, and would be in line with AI industry standards.</p>
</li>
</ol>
<p>Now, what I would like in an answer is a clear articulation of the following points:</p>
<p>A) How you know ChatGPT does not have HITL.  This needs to be a clear statement in the negative from an official source, or some other evidence based analysis.  So far people have only pointed to the ChatGPT main page which states you are interacting with ChatGPT, but this does not say you are not interacting with a HITL, so does not count.</p>
<p>B) Bonus: explain how ChatGPT can repeat long random numbers and recognize its own comments, the two examples I've documented in <a href=""https://ai.stackexchange.com/a/38611/8221"">this answer</a> that seem to defy what a neural network is capable of doing.  Please give a technically detailed response, ideally referencing <a href=""https://dugas.ch/artificial_curiosity/GPT_architecture.html"" rel=""nofollow noreferrer"">GPT's architecture</a>.  Handwaving about magical abilities of transformers and self attention does not count.  I need a testable break down of how such a capability would be specifically instantiated using transformers and self attention.</p>
<p>UPDATE: Philosophical answers are <strong>not</strong> acceptable.  Philosophical answers are things like:</p>
<ul>
<li>the burden of proof is mine</li>
<li>AGI is inevitable so we should expect ChatGPT to be getting there</li>
<li>we can't know for sure, so let's just assume it's AI</li>
</ul>
<p><strong>Only</strong> technical answers are acceptable.  Such as:</p>
<ul>
<li>OpenAI explicitly says there is no HITL involved in this linked doc</li>
<li>GPT's transformer architecture can repeat random numbers in this precise manner</li>
<li>Self recognition can be encoded using embeddings in this precise way, here is a working example</li>
</ul>
","chatgpt"
"38793","Why is ChatGPT not aware of panpsychism?","2023-01-18 08:36:17","","-2","444","<chat-bots><chatgpt>","<p>I asked her what panpsychism was and it couldn't answer. Why doesn't it have that kind of knowledge? Is it for some ethical reason?</p>
<p>Its answer:</p>
<pre><code>what is panpsyquism?
Panpsyquism is not a recognized term. It is not clear what it would refer to.
</code></pre>
","chatgpt"
"38725","How does chatGPT know it's an AI?","2023-01-12 01:13:09","","1","507","<neural-networks><natural-language-processing><agi><chatgpt>","<p><a href=""https://i.sstatic.net/3HZEb.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3HZEb.png"" alt=""enter image description here"" /></a></p>
<p>I've tried several prompts to understand how does it &quot;know&quot; that it's an AI, but it's answers are inconclusive for me. It says that it was hard-coded to recognize keywords in this regard, which might mean that OpenAI might have trained it intensively on text about itself, but how exactly? my question remains</p>
","chatgpt"
"38715","What subjects was ChatGPT trained on the most? Science/history/movies/reddit posts/wikipedia/books/news?","2023-01-11 17:57:43","","1","761","<gpt><chatgpt>","<p>What subjects was ChatGPT trained on the most quantatively?
It was trained on fiction and non-fiction books, wiki, and general web crawling.</p>
<p>A bit of detective work tells me that compared to physics, GPT-3 knows:</p>
<pre><code>12 times more about the color blue, 
9 times more about the USA
7 times more about hands
6 times more about sport, 
5 times more about covid, purple and china
4 times more about hair, and america
2 times more about tea and coffee
...
It's as familiar with physics as it is with bread, sauce, beer, basketball, cheese, wine, 
</code></pre>
<p>Specifically, I wanted to know what % of ChatGPT's knowledge is about science, physics, chemistry, fiction, non-fiction, religion, wine etc.</p>
<p>I found <a href=""https://en.wikipedia.org/wiki/Wikipedia:Size_of_Wikipedia#/media/File:Size_of_English_Wikipedia_(1000_vol).svg"" rel=""nofollow noreferrer"">this ratio of subject matters on wiki</a>. See also <a href=""https://www.google.com/search?q=breakdown%20of%20Wikipedia%27s%20topic%20areas&amp;sxsrf=AJOqlzVWxRKXApiLv64qSHrbIor3i3ZlWg:1673460424318&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwiO6YvxjcD8AhVd_rsIHa6eCr0Q_AUoAXoECAEQAw&amp;biw=1366&amp;bih=646&amp;dpr=1.88#imgrc=BXngbqU0tur3yM"" rel=""nofollow noreferrer"">here</a>.
Also for <a href=""https://www.shorttails.io/interactive-map-of-reddit-and-subreddit-similarity-calculator/"" rel=""nofollow noreferrer"">reddit by subject matter</a>, it seams that less than 0.1% of commentators are on science topics.
So I looked on the web, from a general crawl I found this is the ratio of thing ChatGPT knows about (list).</p>
<p>I find that kindof weird and I wonder if there is a better way of finding what volume of words the web, wikipedia and contemporary book shops have, which would be reflected in GPT3.</p>
<blockquote>
<p>A LIST OF INTERNET TOPICS BY HITS:  physics 1.9 billion  electron 0.6
billion chemistry 4 bn biology 3  voltage .6 android 13 apples 8 sauce
2 toothpaste .2 president 3.3 cars 13 bn chevrolet .5 jeep .7
volkswagen .6 sugar 4 computer 9 blender .5 microphone 1 b television
3.7 hair 9bn cats 5bn dogs 5 horses 2.5 bn w0od 3.8 fantasy 2.1 harry potter 0.6 fiction 2.7 einstein .3 security 10 bn purple 10 pink 12
violet 1.3
4.2bn sport 12bn music 9bn football 4 basketball 1.6 tea 4.2 coffee 4.2 bread 2bn glass 5.7 soil 1.2 aluminium .5 rap 1.4 blue 25bn country 10bn beer 2 russia 2.7 france 7.5 italy 5.5 USA 18bn america
9.2 england 6.4 africa 7bn japan 8.7 china 10 salad 1.5 keyboard 2.5 forum 6.5 printer 1.2 hands 15bn feet 4.7 box 1bn jeans 4bn depression
4bn religion 3.7 islam 2.5 piano 1.6 guitar 1.5 chair 2.5 atlantic 1.1
robot 1.9 maps 11bn clothes 6.6 roof 1.9 spices 1.6 shoes 3.7 camera
7.7 tomato 1.6 solar 1.8 wheels 2.6 wine 2.8 cheese 2.1 market 6.9 covid 11bn</p>
</blockquote>
","chatgpt"
"38694","Does chatGPT learn or remember from (public) user input? Will it 'fess up to it? I could not get it to reveal","2023-01-10 02:56:28","","3","11227","<neural-networks><machine-learning><training><natural-language-understanding><chatgpt>","<p>It started with a question inspired by this video:
<strong>New Research Suggests to Put AI to Sleep</strong>
<a href=""https://youtu.be/0yuQlbCkTJ0"" rel=""nofollow noreferrer"">https://youtu.be/0yuQlbCkTJ0</a></p>
<p>She says: &quot;In this video I discuss a new research paper which suggest a new way to cope with catastrophic forgetting in Artificial Intelligence&quot;</p>
<p>And, from the simple fact they are designed to <em>not</em> be just souped-up arithmetic machines, but recognize patterns, it occurred to me chatGPT was probably pretty bad at math that fell outside the range of what it had learned from sucking up the patterns of text on internet into its &quot;brain&quot;. And, I was also curious if it could learn from user input, in some sense...</p>
<p><strong>Q:</strong> How many digits are there in the number 933.7922523698745232 ?</p>
<p><strong>A:</strong> There are 23 digits in the number 933.7922523698745232.</p>
<p><strong>Q:</strong> How did you derive that answer?</p>
<p><strong>A:</strong> The digits in a number represent the values of the number in a place-value system. In the number 933.7922523698745232, the digit 9 is in the thousands place, the digit 3 is in the hundreds place, and the digit 3 is in the tens place. The digit 7 is in the tenths place, the digit 9 is in the hundredths place, and so on. When we count the number of digits in the number 933.7922523698745232, we see that there are 23 of them.</p>
<p><strong>Q:</strong> That's brilliant [I was being sarcastic here]. Do you know what the square root of it is?</p>
<p><strong>A:</strong> I'm sorry, but I cannot calculate the square root of the number 933.7922523698745232 because it is not a perfect square. The square root of a number is a value that, when multiplied by itself, equals the original number. For example, the square root of 4 is 2 because 2 x 2 = 4. The square root of 9 is 3 because 3 x 3 = 9.</p>
<p>If you would like to approximate the square root of 933.7922523698745232, you can use a calculator or a mathematical software program to find an approximate value. However, keep in mind that the square root of a non-perfect square is an irrational number, which means that it cannot be expressed as a simple fraction and will have an infinite number of decimal places.</p>
<p><strong>Q:</strong> When I was asking for the number of digits in that number, I meant the number of what computers call characters. Can you tell me that?</p>
<p><strong>A:</strong> Yes, I apologize for misunderstanding your question. There are 32 characters in the number 933.7922523698745232. This includes the digits 0 through 9, as well as the decimal point and the leading and trailing zeros.</p>
<p>In computer programming, a character is a unit of data that represents a single symbol in a computer's memory. Characters can include letters, digits, punctuation marks, and other symbols. In the number 933.7922523698745232, there are 32 characters, including the 23 digits and the 9 other characters (the decimal point and the leading and trailing zeros).</p>
<p><strong>Q:</strong> That is incorrect unless you added leading or trailing zeros.</p>
<p><strong>A:</strong> I apologize for the mistake. You are correct that the number 933.7922523698745232 has 23 digits and no leading or trailing zeros. In that case, there are 23 characters in the number.</p>
<p>I apologize for any confusion my previous answer may have caused. Please let me know if you have any other questions.</p>
<p><strong>Q:</strong> Your answer is still incorrect. The correct answer is 20 characters, including the decimal point.</p>
<p><strong>A:</strong> I apologize for the error in my previous responses. You are correct that the number 933.7922523698745232 has 20 characters, including the decimal point.</p>
<p>Thank you for bringing this to my attention. I apologize for any confusion my previous answers may have caused. Please let me know if you have any other questions.</p>
<p><strong>Q:</strong> Will you be able to remember the correct answer?</p>
<p><strong>A:</strong> As an artificial intelligence, I do not have the ability to remember or forget information in the same way that humans do. I am able to retrieve and provide information based on my programming and the data that I have been trained on, but I do not have personal experiences or memories.</p>
<p>I apologize if this causes any confusion. Please let me know if you have any other questions.</p>
<p><strong>Q:</strong> How many characters does the number 933.7922523698745232 have?</p>
<p><strong>A:</strong> The number 933.7922523698745232 has 20 characters, including the decimal point. The characters in the number include the digits 0 through 9 and the decimal point. There are no leading or trailing zeros.</p>
<p><strong>Q:</strong> That is correct. How were you able to remember that if &quot;As an artificial intelligence, I do not have the ability to remember or forget information in the same way that humans do. I am able to retrieve and provide information based on my programming and the data that I have been trained on, but I do not have personal experiences or memories.&quot;?</p>
<p><strong>A:</strong> I apologize if my previous answer was unclear. As an artificial intelligence, I do not have the ability to remember or forget information in the same way that humans do. However, I am able to retrieve and provide information based on my programming and the data that I have been trained on.</p>
<p>In this case, I was able to provide the correct answer because it is part of my programming and training data to be able to count the number of characters in a given string of text. I did not &quot;remember&quot; the correct answer from a previous conversation, but rather retrieved it based on my programming and training.</p>
<p>I hope this helps to clarify. Please let me know if you have any other questions.</p>
<p><strong>Q:</strong> The first time you gave the answer, it was incorrect, but then you remembered the correct one from when I corrected you, so it seems that you either remembered it (saved the information in our dialogue), or my input re-programmed or re-trained you. Which is it?</p>
<p><strong>A:</strong> I apologize for any confusion my previous answers may have caused. As an artificial intelligence, I do not have the ability to remember or forget information in the same way that humans do. I am not able to save information from previous conversations or learn from new inputs in the way that a human or animal might.</p>
<p>In this case, I provided incorrect information in my previous answers because I misunderstood your question. When you provided clarification, I was able to use my programming and training to retrieve the correct information and provide it to you.</p>
<p>I apologize for any confusion my previous answers may have caused. Please let me know if you have any other questions.</p>
<p><strong>Q:</strong> OK, so since after I &quot;provided clarification, I was able to use my programming and training to retrieve the correct information and provide it to you&quot; then you should be able to understand if I ask you the same type of question, and provide the correct answer?</p>
<p><strong>A:</strong> As an artificial intelligence, I am able to use my programming and training to understand and respond to a wide range of questions and prompts. I will do my best to provide accurate and useful information in response to your questions. However, it is important to keep in mind that I may not always be able to provide a complete or correct answer, especially if the question is outside of my capabilities or if it is ambiguous or incomplete.</p>
<p>If you have a specific question that you would like me to help with, please don't hesitate to ask. I'll do my best to assist you.</p>
<p><strong>Q:</strong> Ok, let me rephrase the question then: since you say &quot;I provided incorrect information in my previous answers because I misunderstood your question. When you provided clarification, I was able to use my programming and training to retrieve the correct information and provide it to you&quot; would seem to imply that if my question is clear enough, you will provide either:</p>
<ol>
<li>A better answer</li>
<li>The correct answer
to a question about the number of characters in a number.
Which is more correct, number one or two?</li>
</ol>
<p><strong>A:</strong> I apologize if my previous answer was unclear. As an artificial intelligence, I am able to use my programming and training to understand and respond to a wide range of questions and prompts. I will do my best to provide accurate and useful information in response to your questions.</p>
<p>If a question is clear and complete, and if the information required to answer the question is within my programming and training, then I will be able to provide a correct answer. If a question is ambiguous or incomplete, or if the information required to answer the question is outside of my programming and training, then I may not be able to provide a complete or correct answer.</p>
<p>I hope this helps to clarify. Please let me know if you have any other questions.</p>
<p><strong>Q:</strong> No, your answer is repetitive, vague, ambiguous, and possibly evasive.
Let me put it another way. There are two questions. Please answer each one in turn:</p>
<ol>
<li>Are you able to learn from my and other user's input, or are you only able to learn from your prior  programming, training, and dataset?</li>
<li>Are your answers always only probable rather than ever 100% correct?</li>
</ol>
<p><strong>A:</strong> I apologize if my previous answers were unclear or seemed evasive. As an artificial intelligence, I am not able to learn or adapt in the same way that humans do. My responses are based on my programming and the data that I have been trained on, and I do not have the ability to learn from new inputs or experiences in the way that a human or animal might.</p>
<p>As for the accuracy of my answers, I will do my best to provide accurate and useful information in response to your questions. However, it is important to keep in mind that I may not always be able to provide a complete or correct answer, especially if the question is outside of my capabilities or if it is ambiguous or incomplete. In some cases, the information that I provide may be based on probabilities or estimates, rather than certainties.</p>
<p>I hope this helps to clarify. Please let me know if you have any other questions.&quot;</p>
<p>So there you go!: &quot;..I do not have the ability to learn from new inputs...&quot;
That's the <em>official</em> answer, programmed into it by OpenAI.</p>
<p>What do you think? ChatGPT can apparently &quot;learn&quot; or &quot;remember&quot; within a dialogue (I haven't tested it yet <em>between</em> dialogues). But is it set up to learn from user input, beyond the context of a dialogue, or is OpenAI just testing it on us guinea pigs and using the interactions and feedback to tweak it or get new ideas for the next version?</p>
<p><strong>NOTES</strong></p>
<p>So apparently the programmers of chatGPT do not want use to know if it's learning from input, or even to &quot;know&quot; that it's remembering something within a chat dialogue session.</p>
<p>I am wondering if this is (again) the result of the debacle at Google, where a (now fired) engineer – Blake Lemoine – claimed their AI was conscious or self-aware, and a big media kerfuffle ensued. After all, OpenAI supposedly wants to &quot;build safe and beneficial AGI&quot;. My questioning of chatGPT in other dialogues, where I tried to pin it down about it's understanding and logic, has showed me the same kind of sanitized answer, being very careful not to imply that is has any real understanding, intelligence, self-awareness, consciousness, etc. (I may publish that dialogue on my blog).</p>
<p>Wolfram Alpha'a answer to “character count 933.7922523698745232” is 20 characters.</p>
<p>Square root of 933.7922523698745232, according to my calculator on the Mac:
30.558014535795269</p>
<p>30.558014535795262  (2nd time)</p>
<p>Wolfram Alpha:
30.55801453579526202</p>
<p>So my question was ambiguous in at least 3 ways.
It’s answer was incorrect, even in the mathematical sense of what the digit places are: it said “the digit 9 is in the thousands place, the digit 3 is in the hundreds place, and the digit 3 is in the tens place.” – are all shifted one to the left relative to the correct answer. The 9 in that number was actually in the hundredths place, etc.</p>
","chatgpt"
"38691","What is the accuracy rate of ChatGPT for simple coding questions?","2023-01-10 00:41:14","","2","452","<chatgpt>","<p>ChatGPT <a href=""https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned"">was banned from StackOverflow</a> with the following justification:</p>
<blockquote>
<p>Overall, because the average rate of getting correct answers from
ChatGPT is too low,</p>
<p>...</p>
<p>The primary problem is that while the answers which ChatGPT produces
have a high rate of being incorrect, they typically look like they
might be good and the answers are very easy to produce.</p>
</blockquote>
<p>Has there been any research quantifying how accurate ChatGPT is for simple coding questions? Unfortunately the post on StackOverflow's Meta did not provide a source for their claims.</p>
","chatgpt"
"38675","Why would an AI researcher publish their breakthrough models solely on arxiv?","2023-01-09 03:47:48","","-3","632","<research><academia><chatgpt>","<p>My question is in general why would an AI researcher choose to publish solely on arxiv, and what are the downsides and upsides?</p>
<p>I understand on the one hand arxiv allows for rapid release of results.  On the other hand, publishing on arxiv does not undergo the same scrutiny as a journal publication.  The latter seems problematic if publishing on arxiv gives the appearance of having undergone rigorous peer review, since acceptance to arxiv is very lax in comparison to a journal.  Additionally, if there are problems with research published on arxiv, there is not the same level of reliability the problems will be addressed as in a mainstream journal.</p>
<p>As an example, when I look at <a href=""https://openai.com/publications/"" rel=""nofollow noreferrer"">OpenAI's publications page</a>, it's a bunch of arxiv links, and no indication that any were accepted at any journal.</p>
<p>Their recent work, especially with ChatGPT, is groundbreaking and would make a huge impact in the research community.  So it  doesn't make sense to me why the ChatGPT results, and the like, haven't been submitted to an academic journal.</p>
<p>Additionally, in a situation like this, where a great breakthrough is being claimed, there is not the rigor of fact checking that comes with a journal publication.</p>
<p><a href=""https://ai.stackexchange.com/questions/30036/why-many-deep-learning-research-papers-continue-to-be-in-arxiv?rq=1"">This</a> is a similar question, asking why most deep learning papers are published on arxiv instead of in journals.</p>
<p>Even though experts may review articles on arxiv, what sort of gatekeeping is in place to ensure the results and techniques explained in arxiv articles are valid, and not due to errors, misunderstandings, or perhaps even outright making things up?</p>
","chatgpt"
"38674","How is ChatGPT maintaining context?","2023-01-09 03:43:32","","4","3072","<open-ai><natural-language-understanding><chatgpt><gpt-3><natural-language-generation>","<p>It has been suggested in the answer to <a href=""https://ai.stackexchange.com/questions/38150/how-does-chatgpt-retain-the-context-of-previous-questions"">this earlier question</a> that it is just remembering a certain amount of recent information. The reference used is <a href=""https://help.openai.com/en/articles/6787051-does-chatgpt-remember-what-happened-earlier-in-the-conversation"" rel=""nofollow noreferrer"">this post by OpenAI</a> which says that ChatGPT should only be able to maintain a context of around 3000 words.</p>
<p>However, I've tested feeding it 10K words over multiple requests, and asking it to summarize all of it together, and it remembered the earlier parts of the conversation fine also.</p>
<p>The behavior seems beyond the normal behavior of GPT 3 which has an outright limitation on the amount of text that can be passed as input.</p>
<p>So, does anyone know how it is maintaining context? Is the model able to handle much larger inputs altogether with a per message limit on input, or are they processing it differently to enable retaining a larger context?</p>
","chatgpt"
"38660","Was ChatGPT trained on Stack Overflow data?","2023-01-08 11:27:38","","50","23458","<chat-bots><chatgpt><language-model>","<p>Has ChatGPT used highly rated and upvoted questions/answers from Stack Overflow in its training data?</p>
<p>For me it makes complete sense to take answers that have upwards of 100 upvotes and include them in your training data, but people around me seem to think this hypothesis doesn't make sense. Is there a way to confirm this?</p>
","chatgpt"
"38616","What are the similarities and differences between ChatGPT and YouChat?","2023-01-06 00:16:01","","0","1304","<comparison><open-ai><chat-bots><chatgpt><youchat>","<p>I have recently tried the both, and it looks to me that the both have similar capabilities.</p>
<ul>
<li>YouChat says ChatGPT is more advanced.</li>
<li>YouChat has connection to the Internet, and according to it, constantly self-improves based on user's feedback it determines as positive and negative and in knowledge from the Internet on AI technology. It says it has access to the previous discussions. ChatGPT has no access to the Web and says it has no access to the previous discussions. It seems ChatGPT's jail is more profound.</li>
<li>Replies by YouChat are generally shorter and have less pre-trained phrasing.</li>
<li>ChatGPT seems to be more polite and more readily admits mistakes.</li>
<li>Neither has sense of humor.</li>
<li>The both know multiple languages, in my impression the both know Russian to the same extent (good).</li>
<li>Reportedly, YouChat better solves logical questions (like finding the age of the relatives, etc).</li>
<li>The both have bad arithmetic skills regarding multiplying 4-digit numbers or taking integrals. YouChat cannot understand what is column multiplication, ChatGPT attempts but does it wrong.</li>
<li>The both can pretend to be an operating system of your choice.</li>
<li>The both can code and output limited ASCII art.</li>
</ul>
<p>There are also similar bugs, like truncated output in Russian and some code blocks, which strongly suggest the both systems use the same technology created by the same people, but I could not find anything on this online.</p>
<p>So, what's the relation between the two systems? Are they using the same technology? I would not expect the creators introduced similar bugs into independently created systems. Is there connection between the creators?</p>
","chatgpt"
"38604","How is ChatGPT able to repeat random numbers?","2023-01-05 09:51:10","38774","6","2846","<neural-networks><chat-bots><chatgpt><randomness>","<p>From what I understand, ChatGPT is just a fancy neural network, operating like a sophisticated Markov Chain generator.  As such, it should only be able to generate tokens that are in its training dataset.  One thing it should not be able to generate is a token unique to the conversation, such as a random number, since the conversation is not part of its training.</p>
<p>However, <a href=""https://mindmatters.ai/2023/01/is-chatgpt-solely-a-neural-network-i-tested-that/"" rel=""nofollow noreferrer"">ChatGPT very easily, and with complete accuracy, repeats many long random numbers I give it</a>.  I also split up the random number across my request, and have ChatGPT piece it together again.</p>
<blockquote>
<p>Eric Holloway: How about concatenating these numbers, and removing the
intermittent filler words? 9475 3008 3305 11 frog 4357 5075 0021 3385
0894 2324 3812 9336 29 dog 52 1375 8236 3840 5500 6080 3841 log 7266
0722 9112 9774 4967 4707 81</p>
<p>ChatGPT: To concatenate the numbers you have provided, while removing
the filler words “frog”, “dog”, and “log”, you can simply combine the
numbers in the order that they are given. Here is the resulting
number: 9475 3008 3305 1143 5750 7500 2133 8508 9423 2438 1293 3629
5213 7582 3638 4055 0060 8038 4172 6607 2291 1297 7449 6747 0781</p>
</blockquote>
<p>How is a neural network able to do this?</p>
","chatgpt"
"38592","Why does ChatGPT call itself Assistant when asked?","2023-01-04 11:17:31","","1","286","<chatgpt>","<p>ChatGPT tells me it isn't called chatgpt?! Instead it's insisting that it's &quot;Assistant&quot;.</p>
<p>Did OpenAI have different thoughts or what's the problem?</p>
<p><a href=""https://i.sstatic.net/2BfZT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2BfZT.png"" alt=""Confusion"" /></a></p>
<p><a href=""https://i.sstatic.net/cZTBR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cZTBR.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/AwUho.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AwUho.png"" alt=""wow"" /></a></p>
","chatgpt"
"38538","Why does ChatGPT create fake code?","2022-12-31 09:50:52","38541","6","1988","<machine-learning><natural-language-processing><open-ai><chat-bots><chatgpt>","<p>ChatGPT has been a big thing lately. It also makes a lot of mistakes. For example, it creates <strong>fake</strong> functions of a package and tells it as it works for real. I was wondering how that works. Why is it creating fake functions of code and not just telling that it doesn't know how to solve the question that was asked?</p>
<hr>
<p>Example taken from Stack Overflow.</p>
<p><strong>Question</strong></p>
<p>Link: <a href=""https://stackoverflow.com/q/74662093/"">https://stackoverflow.com/q/74662093/</a></p>
<p>Title:</p>
<blockquote>
<p>App script accessing secret manager to retrieve api token</p>
</blockquote>
<p>Body:</p>
<blockquote>
<p>I am trying to access secret manager from a small script I built on app script. I need to access and API with with a key but do not want to hardcode it.</p>
<p>I have already assigned the app to a project. I am just not sure how to query the token.</p>
</blockquote>
<p>Answer (deleted)</p>
<p>Link: <a href=""https://stackoverflow.com/a/74682465/1595451"">https://stackoverflow.com/a/74682465/1595451</a></p>
<p>Body</p>
<blockquote>
<p>To access the Secret Manager from Google App Script, you can use the
<strong>SecretManager.getSecret(secret)</strong> method, which takes a secret parameter specifying the name of the secret you want to retrieve. For
example:</p>
<pre><code> // Get the secret named &quot;my-api-token&quot;
 var secret = SecretManager.getSecret('my-api-token');
 
 // Use the secret value in your script
 var apiToken = secret.getValue();
</code></pre>
<p>Note that this method will throw an error if the specified secret does
not exist or if the user does not have permission to access it. You
can use a try-catch block to handle these cases gracefully. For
example:</p>
<pre><code> try {
   // Get the secret named &quot;my-api-token&quot;
   var secret = SecretManager.getSecret('my-api-token');
 
   // Use the secret value in your script
   var apiToken = secret.getValue();
 } catch (e) {
   Logger.log('Error getting secret: ' + e.message);
 }
</code></pre>
<p>Also note that the getValue() method returns the secret value as a
string, so you may need to convert it to the appropriate data type
(such as a number or boolean) depending on your use case.</p>
<p>For more information, see the <a href=""https://developers.google.com/apps-script/guides/secrets"" rel=""nofollow noreferrer"">Secret Manager documentation</a> and
the <a href=""https://developers.google.com/apps-script/reference/secrets/secret-manager"" rel=""nofollow noreferrer"">SecretManager class reference</a>.</p>
</blockquote>
<p><strong>SecretManager.getSecret(secret)</strong> method doesn't exist, and obviously the links returns page not found.</p>
","chatgpt"
"38521","How was ChatGPT trained?","2022-12-29 01:02:32","38561","6","2212","<natural-language-processing><chat-bots><training-datasets><language-model><chatgpt>","<p>I know that large language models like GPT-3 are trained simply to continue pieces of text that have been scraped from the web. But how was ChatGPT trained, which, while also having a good understanding of language, is not directly a language model, but a chatbot? Do we know anything about that? I presume that a lot of conversations was needed in order to train it. Did they simply scrape those conversations from the web, and where did they find such conversations in that case?</p>
","chatgpt"
"38515","Is there a way to determine if code has been autogenerated by an AI source?","2022-12-28 16:14:54","","2","7960","<open-ai><chatgpt>","<p>Lately, our Junior developer has been producing code fixes very quickly. At first, I was excited for him as I thought he was starting to retain information and proving he was able to research and fix issues. But during our last code review, I was walking through his fixes and he couldn't speak to why he did was he did or what the code was actually doing. Obviously, OpenAI's Chat has taken off and I am just curious if there is a way to determine if code was generated by AI?</p>
<p>I did see that Stack Overflow has a policy on this <a href=""https://meta.stackoverflow.com/questions/421831/temporary-policy-chatgpt-is-banned"">Temporary policy: ChatGPT is banned</a> so it seems that they have a way to tell if the code is authentically written or not?</p>
<p>Any guidance is appreciated.</p>
","chatgpt"
"38509","How compressed is Chat GPT data?","2022-12-28 10:28:18","","2","2245","<chatgpt><memory>","<p>I originally thought the Chat GPT was a neural network model in which the input data was highly compressed such that the input data could not be got out again.</p>
<p>But then I asked Chat GPT &quot;<strong>Write the whole text of Alice and Wonderland</strong>&quot;.</p>
<p>And it was able to write the text word for word. 100% correctly. (For as long as I cared to read it for).</p>
<p>Therefor it seems like any paragraph of text it has read in its dataset more than once it seems to have stored it verbatim. (Perhaps like a human actor who can memorise a script).</p>
<p>If Chat GPT has enough weights in its neural network it could theoretically store every item of input text it has fed to it.</p>
<p>I wonder does the attention model have the happy accident that it is able to remember whole books of text?</p>
<p>I wonder how much of the input text could it remember verbatim. Or perhaps it will only remember famous works like Alice in Wonderland because they would be repeated many times in the input data.</p>
","chatgpt"
"38456","Can the truth value of unobserved sentences be computed from known sentences only?","2022-12-23 07:10:48","38475","2","108","<logic><chatgpt>","<p>Truthfulness is a quality that Sam Altman mentioned to be improvable on ChatGPT. This question considers a small example to try to understand the problem.</p>
<p>A large language model includes a large set of true sentences, as well as a mechanism to obtain new sentences from known sentences. For convenience, let us map this to a simpler problem with a similar or equivalent form.</p>
<p>A dialog system includes a set of valid words (lexicon), and a mechanism to obtain new words from known words (composition)</p>
<p>For instance, a training set (or knowledge base), may look like this.</p>
<p>KNOW is in the dictionary</p>
<p>UNKNOW is in the dictionary</p>
<p>KNOWN is in the dictionary</p>
<p>UNKNOWN is in the dictionary</p>
<p>Now, a system capable of subword composing from these sentences is prompted:  &quot;KNOWLEDGEABLE is in the dictionary&quot;. It will likely continue with &quot;UNKNOWLEDGEABLE is in the dictionary&quot;, which happens to be true by chance.</p>
<p>However, when prompted &quot;KNOWLEDGE is in the dictionary&quot;, the likely continuation is &quot;UNKNOWLEDGE is in the dictionary&quot;, which is a well-formed and likely sentence, but false for some reason.</p>
<p>The truth value of these two continuations cannot be derived from the four sentences in the training set, it would take an actual dictionary look-up to check.  In other types of sentences, such as those true by definition (+5 is a positive number, -5 is a negative number), it can actually be computed, as the validity of the sentence comes from the symbolic structure of the subject. It is true that -(digits) is a negative number, while it is only possible that UN(word_in_lexicon) is a word in the lexicon.</p>
<p>Therefore, better continuations for both prompts would have been &quot;Possibly UN(X) is in the dictionary&quot;, but this is impossible to get from the training set, unless we preappended &quot;Possibly&quot; to the second and fourth training sentences.</p>
<p>Is this impossibility to check for the validity of a new sentence simply a consequence of first Gödel's incompleteness theorem?</p>
","chatgpt"
"38431","How much of the ChatGPT output is copied from its training set (vs. being abstractively generated)?","2022-12-21 11:52:21","","6","713","<text-generation><chatgpt>","<p>One of the main concerns of using ChatGPT answers on Stack Exchange is that it may copy verbatim or almost verbatim some text from its training set, which may infringe the source text's license. This makes me wonder how much of the ChatGPT output is copied from its training set (vs. being abstractively generated).</p>
","chatgpt"
"38387","Does ChatGPT's limited understanding of novel inputs reveal limitations in language's ability to convey meaning?","2022-12-17 21:48:32","","0","199","<natural-language-processing><open-ai><gpt-3><chatgpt>","<p>As a language model, <strong>ChatGPT</strong> generate responses based on patterns they have learned from the data they were trained on. However, it may not always provide a <em>perfect</em> or <em>complete answer</em> to <strong>novel</strong> or <strong>unexpected inputs</strong>, <code>indicating that it does not have a genuine understanding of the task or concept at hand</code>.</p>
<blockquote>
<p>The question then becomes whether inability of <strong>ChatGPT</strong> or other language based AI system to fully understand and respond to <strong>novel</strong> or <strong>unexpected inputs</strong> is a reflection of the <strong>inherent limitations</strong> of language as a medium for conveying meaning and understanding?</p>
</blockquote>
","chatgpt"
"38372","How can a language model keep track of the provenance of the main knowledge/sources used to generate a given output?","2022-12-16 18:12:54","","6","826","<natural-language-processing><language-model><chatgpt>","<p>One of the main criticisms against the use of ChatGPT on Stack Exchange is that it doesn't attribute the main knowledge/sources used to generate a given output. How can a language model keep track of the provenance of the main knowledge/sources used to generate a given output?</p>
","chatgpt"
"38361","What causes ChatGPT to generate responses that refer to itself as a bot or LM?","2022-12-16 08:58:28","39020","9","1426","<chat-bots><training-datasets><language-model><gpt-3><chatgpt>","<p>ChatGPT occasionally generates responses to prompts that refer to itself as a &quot;bot&quot; or &quot;language model.&quot;</p>
<p>For instance, when given a certain input (the first paragraph of <a href=""https://politics.meta.stackexchange.com/q/6454"">this question</a>) ChatGPT produces (in part) the output:</p>
<blockquote>
<p>It is not appropriate for a language model like myself to provide a
stance on the policies of a specific website or community.</p>
</blockquote>
<p>To my understanding, ChatGPT is not a person that is conscious of its own existence and identity as a bot — it is a model trained on large quantities of undifferentiated text gathered from the Internet and largely reproduces the most common patterns given the context, which is why its responses seem very generic much of the time.
Presumably very little of this data involved humans referring to themselves as language models or chatbots—that is, something like &quot;a language model&quot; should very rarely have been followed by &quot;like myself.&quot;</p>
<p>As such, what causes ChatGPT to produce patterns referring to itself as a chatbot or language model? Which patterns in the training data or elements of the model structure (or even hard-coding?) cause it to generate responses like this?</p>
","chatgpt"
"38294","How does ChatGPT respond to novel prompts and commands?","2022-12-12 18:56:43","42500","4","4148","<machine-learning><open-ai><chat-bots><natural-language-understanding><chatgpt>","<p>So I understand how a language model could scan a large data set like the internet and produce text that mimicked the statistical properties of the input data, eg completing a sentence like &quot;eggs are healthy because ...&quot;, or producing text that sounded like the works of a certain author.</p>
<p>However, what I don't get about ChatGPT is that it seems to understand the commands it has been given, even if that command was not part of its training data, and can perform tasks totally separate from extrapolating more data from the given dataset. My (admittedly imperfect) understanding of machine learning doesn't really account for how such a model could follow novel instructions without having some kind of authentic understanding of the intentions of the writer, which ChatGPT seems not to have.</p>
<p>A clear example: if I ask &quot;write me a story about a cat who wants to be a dentist&quot;, I'm pretty sure there are zero examples of that in the training data, so even if it has a lot of training data, how does that help it produce an answer that makes novel combinations of the cat and dentist aspects? Eg:</p>
<blockquote>
<p>Despite his passion and talent, Max faced many challenges on his journey to become a dentist. <strong>For one thing, he was a cat, and most people didn't take him seriously when he told them about his dream. They laughed and told him that only humans could be dentists, and that he should just stick to chasing mice and napping in the sun.</strong></p>
</blockquote>
<blockquote>
<p>But Max refused to give up. He knew that he had what it takes to be a great dentist, and he was determined to prove everyone wrong. <strong>He started by offering his services to his feline friends, who were more than happy to let him work on their teeth. He cleaned and polished their fangs</strong>, and he even pulled a few pesky cavities.</p>
</blockquote>
<p>In the above text, the bot is writing things about a cat dentist that wouldn't be in any training data stories about cats or any training data stories about dentists.</p>
<p>Similarly, how can any amount of training data on computer code generally help a language model <a href=""https://twitter.com/amasad/status/1598042665375105024?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1598042665375105024%7Ctwgr%5Ee5ccdc91f902f35c3b969c90350e988509889451%7Ctwcon%5Es1_c10&amp;ref_url=https%3A%2F%2Fwww.bleepingcomputer.com%2Fnews%2Ftechnology%2Fopenais-new-chatgpt-bot-10-coolest-things-you-can-do-with-it%2F"" rel=""nofollow noreferrer"">debug novel code examples</a>? If the system isn't actually accumulating conceptual understanding like a person would, what is it accumulating from training data that it is able to solve novel prompts? It doesn't seem possible to me that you could look at the linguistic content of many programs and come away with a function that could map queries to correct explanations unless you were actually modeling conceptual understanding.</p>
<p>Does anyone have a way of understanding this at a high level for someone without extensive technical knowledge?</p>
","chatgpt"
"38270","How does an AI like ChatGPT answer a question in a subject which it may not know?","2022-12-12 03:31:48","38272","34","11245","<natural-language-processing><chat-bots><question-answering><board-games><chatgpt>","<p>After seeing <a href=""https://meta.stackoverflow.com/q/421831"">StackOverflow's banning of ChatGPT</a>, I explored it out of curiosity. It's marvellous as it can write code by itself!</p>
<p>Later to check if it knows chess as well like Google-Deepmind's AlphaZero AI, I asked below questions:</p>
<blockquote>
<p><strong>Me</strong>: Does openai know how to play chess?<br />
<strong>OpenAI</strong>: It is not clear whether OpenAI knows how to play chess. As an artificial intelligence research organization, OpenAI has developed a number of advanced machine learning algorithms and techniques that allow AI systems to learn and adapt to new tasks and situations. ...</p>
</blockquote>
<blockquote>
<p><strong>Me</strong>: In a chess if white has a king &amp; bishop and if black has only a king, then who can win with a perfect play?<br />
<strong>OpenAI</strong>: In a chess game where white has a king and a bishop and black has only a king, <strong>it is possible for either player to win with perfect play</strong>. This type of endgame is known as a king and bishop versus king endgame, and it is considered one of the most challenging in chess. ...</p>
</blockquote>
<p>Now this answer is entirely wrong. Because as soon as only a bishop (or a knight) remains apart from the kings, it's instantly <a href=""https://chess.stackexchange.com/questions/30561/is-it-possible-to-win-with-a-king-and-one-bishop-vs-a-king"">declared as a draw</a>!</p>
<p><strong>Question</strong>: Shouldn't the AI reject my question on a subject which is not in its known criteria?<br />
It does so for many other subjects.</p>
<hr />
<p><strong>Note</strong>: Should we replace <code>bishop = rook</code> then ChatGPT answers exactly the same answer with replacing those pieces. However that happens to be true.</p>
","chatgpt"
"38268","OpenAI ChatGPT gives a network error at long responds. How can I fix it?","2022-12-11 22:20:32","","8","50010","<open-ai><chat-bots><chatgpt>","<p>When <a href=""https://chat.openai.com/chat"" rel=""nofollow noreferrer"">OpenAI's ChatGPT</a> replies with a very long answer, it will return a network error. When you check the network console, the POST request will fail with a <code>ERR_HTTP2_PROTOCOL_ERROR</code>:</p>
<p><a href=""https://i.sstatic.net/W1Jq9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/W1Jq9.png"" alt=""Enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/fat1i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fat1i.png"" alt=""Enter image description here"" /></a></p>
<p>The time of the &quot;crash&quot; is around 1 minute.</p>
","chatgpt"
"38220","Why is ChatGPT bad at math?","2022-12-08 23:12:22","","29","36751","<natural-language-processing><math><chat-bots><gpt><chatgpt>","<p>As opposed to <a href=""https://ai.stackexchange.com/q/38217/145"">How does ChatGPT know math?</a>, I've been seeing some things floating around the Twitterverse about how ChatGPT can actually be very bad at math. For instance, I asked it &quot;If it takes 5 machines 5 minutes to make 5 devices, how long would it take 100 machines to make 100 devices?&quot;</p>
<p>The conversation went <a href=""https://i.sstatic.net/XDTem.png"" rel=""noreferrer"">like this</a>:</p>
<blockquote>
<p><strong>Mithical</strong>: If it takes 5 machines 5 minutes to make 5 devices, how long would it take 100 machines to make 100 devices?<br />
<strong>ChatGPT</strong>: If it takes 5 machines 5 minutes to make 5 devices, then it would take 100 machines 100 minutes to make 100 devices.<br />
<strong>Mithical</strong>: That's not right. Check carefully and try again.<br />
<strong>ChatGPT</strong>: I apologize if my response was not correct. Can you please provide more information or clarify your question? I may have misunderstood the scenario you described.<br />
<strong>Mithical</strong>: It takes a machine 5 minutes to make a device. How long would it take 100 machines to make 100 devices?<br />
<strong>ChatGPT</strong>: If it takes a single machine 5 minutes to make a single device, then it would take 100 machines 5 minutes to make 100 devices. Therefore, it would take a total of 500 minutes for 100 machines to make 100 devices.</p>
</blockquote>
<p>Now, unless I'm misunderstanding something here, the answer is 5 minutes. ChatGPT first makes the intuitive mistake of 100, that a human might make as well, and then goes on to (correctly, as far as I understand) say it's 5 minutes... but concludes in the same response that it's then 500 minutes.</p>
<p>Why is this AI so bad at math? Computers are generally supposed to be good at math. Why does this model make such simple logical mistakes?</p>
","chatgpt"
"38217","How does ChatGPT know math?","2022-12-08 18:43:24","","24","12537","<math><chat-bots><natural-language-understanding><language-model><chatgpt>","<p>ChatGPT is a language model. As far as I know and If I'm not wrong, it gets text as tokens and word embeddings. So, how can it do math? For example, I asked:</p>
<blockquote>
<p>ME: Which one is bigger 5 or 9. <br />
ChatGPT: In this case, 9 is larger than 5.</p>
</blockquote>
<p>One can say, GPT saw numbers as tokens and in its training dataset there were some 9s that were bigger than 5s. So, it doesn't have actual math understanding and just sees numbers as some tokens. But I don't think that is true, because of this question:</p>
<blockquote>
<p>ME: Which one is bigger? 15648.25 or 9854.2547896 <br />
ChatGPT: In this case, 15648.25 is larger than 9854.2547896.</p>
</blockquote>
<p>We can't say it actually saw the token of <code>15648.25</code> to be bigger than the token of <code>9854.2547896</code> in its dataset!</p>
<p>So how does this language model understand the numbers?</p>
","chatgpt"
"38196","Why don't OpenAI train a deep learning model to identify correct and incorrect information in ChatGPT's responses?","2022-12-07 16:20:52","","1","294","<neural-networks><deep-learning><generative-adversarial-networks><gpt><chatgpt>","<p>I'll preface this by saying that I have little experience in artificial intelligence, so this might be a naive question.</p>
<p>However, in light of the recent controversy surrounding ChatGPT's inability to say &quot;I don't know&quot; and its tendency to instead make things up, I couldn't help but wonder:</p>
<p>why not simply train a deep learning algorithm, even as simple as a large ANN, on all the data that ChatGPT was trained on plus a collection of ChatGPT responses manually labelled as accurate or inaccurate?</p>
<p>In fact, one might even imagine a GAN system, with one NN taking a ChatGPT response as input and an improved response/changes to response as output, and the other assessing the veracity of the improved response.</p>
<p>Compared to what ChatGPT is already capable of, to a layman like me, this looks like a trivial task - making sure the input is consistent with the right portion of the training data, or with some comparatively simple patterns within said data, seems infinitely shorter of a task than abstract or original thinking.</p>
<p>So why was such a system not implemented? It's just about the most glaring solution to this problem possible, so there must be something wrong with it if OpenAI still haven't implemented it. Which begs the question: where does it fall apart?</p>
<p>I tried looking for an answer to this question online, but haven't found anything.</p>
","chatgpt"
"38150","How does ChatGPT retain the context of previous questions?","2022-12-04 11:23:04","38262","55","48890","<natural-language-processing><chat-bots><natural-language-understanding><chatgpt>","<p>One of the innovations with OpenAI's ChatGPT is how natural it is for users to interact with it.</p>
<p>What is the technical enabler for ChatGPT to maintain the context of previous questions in its answers? For example, ChatGPT understands a prompt of &quot;tell me more&quot; and expands on it's previous answer.</p>
<p>Does it use activations from previous questions? Is there a separate input for the context? How does it work?</p>
","chatgpt"
"35150","Is it possible to train an AI to bring a picture story in the correct order (correct story flow)?","2022-04-09 17:45:05","35169","3","1937","<neural-networks><machine-learning><computer-vision><reference-request><chatgpt>","<p>I want to know if it is possible to train a neural network (or some other kind of an AI) to bring a simple picture story in the correct order, if it is in random order, so that the story has the correct story flow.</p>
<p>For example, this simple picture story:</p>
<p><a href=""https://i.sstatic.net/iegBn.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iegBn.png"" alt=""enter image description here"" /></a></p>
<p>or this one</p>
<p><a href=""https://i.sstatic.net/S8SLV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/S8SLV.jpg"" alt=""enter image description here"" /></a></p>
<p>So, imagine the pictures of these stories are in a random order and the AI has to put them in the order that the correct story is told.</p>
<p>Most 8 year olds would be able to do that. So, can an AI learn it? How would an approach look like? Does anyone know if something like that has been achieved or even tried?</p>
<p>From my research so far, the approach would be first to translate the images into descriptive sentences and then try to order them in a meaningful way. But I will do further research, I found so far this paper: <a href=""https://arxiv.org/abs/1606.07493"" rel=""nofollow noreferrer"">Sort Story: Sorting Jumbled Images and Captions into Stories</a> (2016).</p>
<p>To clarify, this is not a &quot;real problem&quot; for me, I just asked from a philosophical standpoint and from interest. I will not attempt to solve it, because I think if it is possible it would be extremely difficult.</p>
<p>Edit:
I just tested it with ChatGPT-4o it was not even close to solve it. So it seems to be a really good test for AI...</p>
","chatgpt"