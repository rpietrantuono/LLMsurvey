Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"46536","In a vision transformer, are the patch outputs for the last layer unused?","2024-08-17 10:13:48","","0","11","<neural-networks><deep-learning><transformer><vision-transformer>","<p>In a vision transformer (<a href=""https://arxiv.org/pdf/2010.11929"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2010.11929</a> ), it seems like the final MLP head for prediction is attached only to the last layer's [cls] token embedding (Figure 1 and Eqn 4).</p>
<p>Does this mean that:</p>
<ol>
<li>other tokens' (i.e. image patches') final layer outputs are unused?</li>
<li>final transformer block gets trained only on the [cls] token?</li>
<li>as a consequence of (2), final layer outputs for other tokens will not have meaningful outputs, and another application should not use those outputs?</li>
<li>for efficiency, final transformer block can be run only for the [cls] token, ignoring other tokens?</li>
</ol>
","transformer"
"46516","How to convert a positionally encoded predicted embedding from a decoder to its matching token?","2024-08-14 20:14:41","46521","0","24","<natural-language-processing><transformer><embeddings><positional-encoding><tokenization>","<p>Is it valid to just subtract the positional encoding from a predicted output if the decoder was also positionally encoded? Or does masking take care of this problem, and the decoder should only learn the embedding (not positionally encoded)</p>
","transformer"
"46514","Sequence to Sequence vs Sequence to Token","2024-08-14 18:19:36","","0","8","<transformer><models><sequence-modeling><seq2seq>","<p>I came here to ask for some clarification about the subject that is in the topic.
Denote: Seq2Seq, Seq2Tok</p>
<p>What I am trying to understand if there is any use of the output sequence in Seq2Seq models or it is always the last token that is taken from the output sequence which is trained to be the next token in the input sequence?</p>
<p>One example that I thought of is that in transformers all the sequence output is used in the training process. But what about inference?</p>
<p>To put my question in different way:
I understand that Seq2Seq and Seq2Tok are almost the same thing, but I am trying to figure out what is the meaning of the sequence output as a whole regarding of the last token of it.</p>
","transformer"
"46485","natural language to command systems, which transformer model?","2024-08-10 13:23:08","","0","19","<neural-networks><deep-learning><transformer><machine-translation>","<p>I want to train a transformer model on a dataset that maps natural language to a command system.</p>
<p>Example:</p>
<pre><code>text: find element A in file with name file1.txt and return its first line
cmd: find A file1.txt -line 1
</code></pre>
<p>I am thinking of a choice between opennmt and huggingface models. I have my metrics for evaluation, my data preprocessing code. I have to note that this command system is a small one with around 40 commands, and these commands have no nestsed structures. I mean we don't have nested subcommands, only commands and options with them.</p>
<p>My question is what else can you say about approaching this problem from a model point of view, any suggestions?</p>
","transformer"
"46459","How can I change the tokens BERT uses so that each digit is a separate token?","2024-08-07 17:31:29","","0","30","<transformer><pytorch><bert><tokenization>","<p>Rather than have the tokenizer generate this sort of thing:</p>
<pre><code>&quot;$1009 Dollars&quot; =&gt; [&quot;$&quot;, &quot;100#&quot;, &quot;9&quot;, &quot;Dollars&quot;]
</code></pre>
<p>I'd like to have:</p>
<pre><code>&quot;$1009 Dollars&quot; =&gt; [&quot;$&quot;, &quot;1#&quot;, &quot;0#&quot;, &quot;0#&quot;, &quot;9&quot;, &quot;Dollars&quot;]
</code></pre>
<p>Is that possible? I know I'd need to add all the <code>n#</code> tokens, but I'd need to remove a lot of tokens as well. I'm guessing I'd need to build my transformer from scratch?</p>
<p>For reference, here's some example code that tokenizes this string:</p>
<pre><code>from transformers import AutoTokenizer, BertModel
import torch

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-large-uncased&quot;)

inputs = tokenizer(&quot;$1009 Dollars&quot;, return_tensors=&quot;pt&quot;)
ids = inputs['input_ids']
for token_id in ids:
    token = tokenizer.convert_ids_to_tokens(token_id)
    print(token)
</code></pre>
<p>Here's the result I get:</p>
<pre><code>['[CLS]', '$', '100', '##9', 'dollars', '[SEP]']
</code></pre>
<p>How can I control what tokens go into my model?</p>
","transformer"
"46437","Es posible utilizar inteligencia artificial para crear perfiles trolls, indistinguible un ser humano real, para posicionar ciertas tendencias?","2024-08-06 09:13:17","","0","8","<transformer>","<p>Hago la pregunta desde la idea de asesorarme sobre la creacion de estos perfiles</p>
","transformer"
"46324","Summary Generation","2024-07-24 10:40:21","","0","20","<natural-language-processing><transformer><large-language-models><huggingface>","<p>I want to create a summary from a list of some bullet points and keywords . Most NLP and Transformer based models are not very well suited for short sentences and bullet point.
Bullet points are usually very small 1-4 words .
Most summarisers just concat the points together that is not what I am looking for
I don't wish to use a api for this a hugging face , transformer, or an LLM model would be great that i could run locally .</p>
<p>For Example:</p>
<ul>
<li>Had breakfast : NO</li>
<li>Had Lunch : NO</li>
<li>Went for a jog : YES</li>
</ul>
<p>Output :
Did not have breakfast and lunch . I went for a jog</p>
","transformer"
"46306","Transformer Loss Function for Music Generation","2024-07-22 15:45:39","","0","20","<transformer><objective-functions><pytorch>","<p>I am working on a Midi Generation project that takes tracks as inputs, and outputs a complimentary track of notes.</p>
<p>The tracks are basically a list of notes created of:</p>
<ul>
<li>Time</li>
<li>Duration</li>
<li>Pitch</li>
<li>Velocity</li>
</ul>
<p>I am one-hot encoding the pitch and duration (128 possibilities each), to run cross-entropy. My problem is that the pitch and duration seem to not even start to converge, staying at the max <code>ln(128) == ~4.85</code>, while the time's MSE loss seems to go down.</p>
<p>Also, I don't know if I am handling the masked outputs well.</p>
<p>The model:</p>
<pre><code>def forward(self, src, trg, src_mask, tgt_mask):

    src_emb = self.pos_enc(src)
    tgt_emb = self.pos_enc(trg)

    outs = self.transformer(
        src_emb,
        tgt_emb,
        src_key_padding_mask=src_mask,
        tgt_key_padding_mask=tgt_mask
    )

    time = self.time_ff(outs)
    time = self.relu(time)

    duration = self.duration_ff(outs)
    duration = self.relu(duration)

    pitch = self.pitch_ff(outs)
    pitch = self.softmax(pitch)

    velocity = self.velocity_ff(outs)
    velocity = self.softmax(velocity)

    concatenated_output = torch.cat([time, duration, pitch, velocity], dim=-1)
    return concatenated_output
</code></pre>
<p>The Loss Function:</p>
<pre><code>def midi_loss_fn(output, target):
    '''
    MSE + Cross Entropy Loss
    '''
    out_mask = output.isnan()
    tgt_mask = target == -2
    mask = ~(out_mask | tgt_mask)

    out_times,      tgt_times       = output[...,0][mask[...,0]], target[...,0][mask[...,0]]
    out_durations,  tgt_durations   = output[...,1][mask[...,1]], target[...,1][mask[...,1]]

    out_pitches,    tgt_pitches     = output[...,2:130][mask[...,2:130]], target[...,2:130][mask[...,2:130]]
    out_velocities, tgt_velocities  = output[...,130:258][mask[...,130:258]], target[...,130:258][mask[...,130:258]]

    time_loss =     F.mse_loss(out_times, tgt_times)
    duration_loss = F.mse_loss(out_durations, tgt_durations)
    pitch_loss =    F.cross_entropy(out_pitches.reshape(-1, 128), tgt_pitches.reshape(-1, 128))
    velocity_loss = F.cross_entropy(out_velocities.reshape(-1, 128), tgt_velocities.reshape(-1, 128))

    return loss + 1e-8
</code></pre>
","transformer"
"46288","Are there any non-transformer LLMs?","2024-07-19 13:41:39","","1","91","<reference-request><transformer><large-language-models><model-request>","<p>Almost all LLMs are based on the transformer architecture, but are there any examples of ones that don't use transformers?</p>
","transformer"
"46275","Combinig output of two different machine learning models for accurate invoice data extraction: Is this a viable approach?","2024-07-17 18:12:58","","0","26","<natural-language-processing><training><transformer><accuracy><ensemble-learning>","<p>I am working (trying to work) on a project to extract relevant information from invoices. Currently I don't achieve much good accuracy so am trying to come up with some new ideas. I am considering combining two machine learning models: Lilt and YOLO, but the specific models arent important, I want to ask you about my proposed workflow:</p>
<p><strong>Initial Processing with Lilt:</strong>
The invoice is first processed by the Lilt model and OCR engine to extract structured data based on the layout and content.  Lilt's output is captured in a structured format, such as JSON.</p>
<p><strong>Secondary Processing with YOLO:</strong>
The same invoice is then processed by the YOLO model to detect and extract specific fields or text blocks (e.g., invoice number, date, total amount). YOLO's output is similarly captured in a structured format with confidence scores.</p>
<p><strong>Comparison and Reconciliation:</strong>
I will implement a function to compare the outputs from Lilt and YOLO.</p>
<p><strong>For each field:</strong>
If both models provide a value, I will compare the confidence scores and choose the higher one.
If only one model provides a value, I will use that value.</p>
<p>My goal is to leverage the strengths of both models to achieve higher accuracy in extracting invoice data. However, I have some concerns and would appreciate feedback on the following points:</p>
<pre><code>Integration Complexity: Are there best practices for integrating outputs from two different models effectively?

Performance: Will running two models sequentially significantly impact processing time, and how can this be optimized?

Data: The models would be trained not on same datasets. I am currently working with one czech invoice dataset in Layout/Lilt format that i had obtain but YOLO expect different input so i would need to annotate new dataset. Would that be a problem?

Accuracy: Is it common to combine output of several transformers?
</code></pre>
<p>Has anyone implemented a similar approach, and what lessons did you learn? Are there alternative methods that might achieve better results? Is my idea just too complicated or straight up useless?</p>
<p>Any insights, suggestions, or resources would be greatly appreciated!</p>
","transformer"
"46268","How to add/embed categorical features in transformers network?","2024-07-17 08:22:01","","0","20","<transformer><embeddings><categorical-data><tokenization>","<p>I would like to give more context to my transformers by adding some metadata related to each token. This metadata is mostly categorical (3 fields, with 3 possible values for each field).
In addition of positional embedding (same shape as the tokens, added to them), how can I add this extra information to the tokens before each transformer block ?</p>
<p>I have few options in mind :</p>
<ul>
<li>One-hot encoding of the metadata : <code>t_extra</code> with shape <code>[9,]</code>, like <code>[0,0,1,0,1,0,1,0,0]</code> for the three variables with class <code>{2, 1, 0}</code>.. Then these extra token is <strong>concatenated</strong> to the existing token. This increases <code>d_model</code> (from 512 to 521). Is it a good idea to concat <code>t_extra</code> and increase the token size ?</li>
<li>Like the first approach, one-hot encoding, but I repeat  (+ 0-padding) the vector of size <code>[9,]</code> up to have <code>t_extra</code> with the same size of the tokens (<code>d_model = 512</code>). And I add it to the tokens, like the PE : <code>Add([t, pe, t_extra])</code>. With this option, I don't known if one-hot encoding is good idea, should I scale the <code>t_extra</code> by a factor 2 or 5 ? such that PE encodding adds variations in <code>[-1, 1]</code> (sine &amp; cosine) and the categorical encodding (in <code>t_extra</code>) adds variations / offsets in <code>{0, 2}</code> (or <code>{0, 5}</code> depending of the scale) at some positions in the token.</li>
</ul>
<p>Is there some existing approach about categorical embedding in transformer ?</p>
","transformer"
"46252","Normalizing the embedding space of an encoder language model with respect to categorical data","2024-07-15 23:12:22","","0","22","<natural-language-processing><classification><transformer><word-embedding>","<p>Suppose we have a tree/hierarchy of categories (e.g. categories of products in an e-commerce website), each node being assigned a title. Assume that the title of each node is semantically accurate, meaning it's consistent with the category (the titles of the children nodes) it represents. Now, take an encoder language model (like BERT or Word2Vec) and produce embeddings for each node/category. My goal is to ensure that these embeddings are representative of the categorical hierarchy. I wonder:</p>
<ul>
<li>Should the embeddings (mostly) be consistent with the categorical hierarchy, even if the categorical hierarchy is very niche and/or uneven (meaning that categorical granularity is not necessarily evenly distributed, if that makes sense)? By consistent, I mean that the embeddings of sibling nodes under one parent should be closer to each other than to children nodes of other parents, and closer to their parent's embeddings than to other parents' embeddings. Is there a nice metric to measure/validate this?</li>
<li>If not, what would be good ways to re-map a latent space to make it &quot;nicer&quot; with respect to the categorical hierarchy? I'm thinking either 1) we directly transform the latent space or 2) fine-tune the language encoder model (i.e. BERT). For example: 1) let's say that the shape formed by a category/parent's sub-categories/children is a thin oval, would it be possible and make sense to want to map the space such that this thin oval becomes a circle? Also for 2) would fine-tuning BERT along the task of classifying leaf embeddings' under the correct parent embeddings do the same thing?</li>
</ul>
<p>I think that this requires the categories to not be so niche and to have some correspondence to regular language usage (and even more after providing context), which is my case.</p>
<p>This is just my intuition, so let me know your thoughts. I would be interested in exploring anything from the most simple heuristic approaches to SOTA NLP techniques that could pertain to this problem.</p>
","transformer"
"46205","How do Transformer models ensure unique token representations when combining embeddings and positional encodings?","2024-07-10 10:48:11","46207","0","73","<transformer><embeddings><positional-encoding>","<p>In Transformer models, token embeddings are combined with positional encodings through element-wise addition to incorporate positional information. However, this raises a concern about the potential for different tokens in different positions to end up with identical embeddings.</p>
<p>For example, consider the following:</p>
<ul>
<li>Embedding of token <span class=""math-container"">$A$</span>: <span class=""math-container"">$[0.5, 0]$</span></li>
<li>Positional encoding at position <span class=""math-container"">$1$</span>: <span class=""math-container"">$[1, 0]$</span></li>
<li>Combined representation for token <span class=""math-container"">$A$</span> at position <span class=""math-container"">$1$</span>: <span class=""math-container"">$[0.5 + 1, 0 + 0] = [1.5, 0]$</span></li>
<li>Embedding of token <span class=""math-container"">$B$</span>: <span class=""math-container"">$[1, 0]$</span></li>
<li>Positional encoding at position <span class=""math-container"">$2$</span>: <span class=""math-container"">$[0.5, 0]$</span></li>
<li>Combined representation for token <span class=""math-container"">$B$</span> at position <span class=""math-container"">$2$</span>: <span class=""math-container"">$[1 + 0.5, 0 + 0] = [1.5, 0]$</span></li>
</ul>
<p>In this case, the combined representations for different tokens at different positions are the same.</p>
<p>Given that positional encodings are added element-wise to token embeddings, how do Transformer models ensure the uniqueness of token representations to prevent different token-position combinations from resulting in the same embedding vector? Specifically, how do they handle cases where different embeddings and positional encodings could potentially sum to the same vector?</p>
","transformer"
"46110","How does casual and padding mask work in decoder-only models?","2024-06-30 14:00:13","","0","37","<natural-language-processing><transformer><attention>","<p>I am trying to implement a decoder-only model from scratch using PyTorch, but I am confused about how the masking works. From what I understand, when we have encoder-decoder architecture, the padding mask is in the encoder and the casual mask is in the decoder. Then, with the decoder-only architecture, do we combine them, and if we do, how will this look?</p>
","transformer"
"46087","Would converting embeddings updated by a transformer into tokens (eg: by searching for the nearest embeddings) produce results that make sense?","2024-06-26 20:04:10","","1","45","<transformer><large-language-models>","<p>In the transformer architecture, one of the step is to update the embeddings, allowing words to pass information to whichever words they are relevant to. For example in the sentence &quot;a small cat is walking in the garden&quot; we want the embedding of &quot;small&quot; to cause a change to &quot;cat&quot; and to moves it to a different part of the embedding space that more specifically encodes a &quot;small cat&quot;. &quot;small&quot; is an adjective but the same thing can be done with verbs, pronouns, ....</p>
<p>If we would take the updated embeddings and start looking for the closest tokens (the ones that minimize distance), what would we got ? Are the updated embeddings a vector that only make sense inside the transformer layers and for prediction (the last step), or do they somehow still have a relation with the initial embeddings used for tokens ?</p>
<p>Is this plausible that the updated embedding of &quot;small cat&quot; to be closer to &quot;kitty&quot; than &quot;cat&quot; ?</p>
","transformer"
"46080","Filter scientific papers based on abstract information","2024-06-25 21:43:03","46096","0","93","<transformer><question-answering>","<p>I have a data frame that contains different numbers of columns, the important ones are:</p>
<pre><code>Title Abstract
</code></pre>
<p>The length (number of words) varies between each row, with a total of 850 rows, but all are scientific papers related to science (immunology context).
I created this file using a Python script that checks for different combinations of keywords in a specific context to search in the literature the papers in a straightforward way.
To move forward I would like to reduce the number of rows, in this case, the number of papers that are useful for my research. With this in mind, I started using transformers to analyze each abstract in my data frame and check if it is useful for my research based on a specific question and context, so my focus is the transformers related to the <strong>question-answering</strong> process.</p>
<p>I'm completely new in this world of neural network architecture (transformers), so I do not know if I'm doing the process correctly or if the approach that I chose is correct.
Based on the research that I did my current approach is:</p>
<pre><code>1. Use transformers from Hugging Face.
2. Select a dataset that works with Biological data context.
3. Select a model that is for the question-answering process in the context of biological data. 
4. Upload the dataset in my workspace and train the model (?)
5. Define your question and define the context.
6. Upload your dataset (data frame) that you want to analyze and generate output with long and short answers (?)
</code></pre>
<p>So far, I think that I'm in point 3, fixing some issues related to the question-answering model (not related to this post). However, I doubt if all of this is necessary to perform my goal, <strong>reduce the number of papers that are useful for my research (that I need to include to perform discussion process in a paper writing process) based on a question and context.</strong></p>
<p>Is this process correct for my goal?</p>
<p>Do you know if is there a better way?</p>
<p>If the process is correct, what model do you recommend for biological context?</p>
","transformer"
"46067","How do transformer models handle negation in sentiment analysis","2024-06-25 00:58:21","","0","61","<transformer><attention><gpt><bert>","<p>I'm trying to understand how transformer models, such as BERT or GPT, handle negation in sentiment analysis. Specifically, I'm curious about how these models manage to correctly interpret sentences where negation changes the sentiment, such as &quot;The movie is not good.&quot;</p>
<p>A simple model using word embeddings + global averaging fails to handle negation properly. Intuitively, for example, if &quot;good&quot; has a positive sentiment score and &quot;bad&quot; has a negative sentiment score, a model might misinterpret &quot;not good&quot; by simply averaging the scores of &quot;not&quot; and &quot;good&quot;.</p>
<h3>Example Without Negation</h3>
<p>Consider the following sentences with sentiment words:</p>
<ul>
<li>&quot;The movie is good.&quot;</li>
<li>&quot;The movie is awesome.&quot;</li>
<li>&quot;The movie is terrible.&quot;</li>
</ul>
<p>Suppose we have the following word embeddings representing sentiment scores:</p>
<ul>
<li>&quot;good&quot; = [10]</li>
<li>&quot;awesome&quot; = [12]</li>
<li>&quot;terrible&quot; = [-10]</li>
</ul>
<p>Neutral words (assuming embeddings around 0):</p>
<ul>
<li>&quot;the&quot; = [0]</li>
<li>&quot;movie&quot; = [0]</li>
<li>&quot;is&quot; = [0]</li>
</ul>
<p>For these sentences, a simple global average of the sentiment scores works well:</p>
<ul>
<li>&quot;The movie is good&quot; = average([0, 0, 0, 10]) = 10 / 4 = 2.5 (positive sentiment)</li>
<li>&quot;The movie is awesome&quot; = average([0, 0, 0, 12]) = 12 / 4 = 3 (positive sentiment)</li>
<li>&quot;The movie is terrible&quot; = average([0, 0, 0, -10]) = -10 / 4 = -2.5 (negative sentiment)</li>
</ul>
<h3>Example With Negation</h3>
<p>Now, consider the sentence &quot;The movie is not good.&quot; In this case, the sentiment should be negative due to the presence of &quot;not.&quot; However, averaging the scores naively might not handle this correctly. For example:</p>
<ul>
<li>&quot;The movie is not good&quot; = average([0, 0, 0, -5 (for not), 10 (for good)]) = (0 + 0 + 0 - 5 + 10) / 5 = 5 / 5 = 1 (incorrectly positive)</li>
<li>&quot;The movie is not bad&quot; = average([0, 0, 0, -5, -9]) = (0 + 0 + 0 - 5 - 9) / 5 = -14 / 5 = -2.8 (incorrectly negative)</li>
</ul>
<h3>How Transformers Handle Negation</h3>
<p>Can someone explain, with a concrete example, how a transformer model like BERT or GPT can correctly understand and model negation in a sentence? Specifically, I'm interested in:</p>
<ul>
<li>How the self-attention mechanism captures the relationship between words like &quot;not&quot; and &quot;good&quot;.</li>
<li>An example with numerical values to illustrate the process.</li>
</ul>
","transformer"
"46054","How is the bidirectional context achieved in BERT?","2024-06-23 11:11:39","46212","0","49","<transformer><attention><gpt><bert><language-model>","<p>I have read the paper &quot;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&quot; by Jacob Devlin et al. (2018) and &quot;Improving Language Understanding by Generative Pre-training&quot; by Alec Radford et al. (2018).</p>
<p>I have understand that the largest difference between BERT and GPT is the direction of the context. BERT is bidirectional, while GPT is unidirectional. I have understand the difference visually thorugh Figure 3 of the BERT paper.
However, I have question about how the difference is achieved in more detailed level, such as implementation level. I have understand that the shortest way is to look at the code, but I am wondering if there is any explanation in the paper.
Especially, I am wondering how multiple outputs from previous transformer layers are combined as the input of the next layer in BERT (or so does GPT).</p>
<p>I have also looked at the paper &quot;Attention is All You Need&quot; by Vaswani et al. (2017) and found the following equation
<span class=""math-container"">$$ Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V $$</span>
, but I am not sure about where does it implies the unidirectional context in the transformer architecture. (I think original transformer is unidirectional.)</p>
<p>I would appreciate it if you could explain how the bidirectional context is achieved in BERT.</p>
","transformer"
"45949","Is a small transformer model able to effectively handle any input length provided it is fine-tuned on it?","2024-06-11 11:12:03","","0","41","<transformer>","<p>Suppose we have a transformer LLM which can do a task such as summarising.</p>
<p>I know transformer <em>can</em> technically handle any input length (assume we are not using learned positional embeddings) because the architecture doesn’t define a fix length for input. However, the quadratic complexity and unavailability of long text data puts a practical limit on sequence lengths used for training and they are not effective beyond the sequence length they are trained on.</p>
<p>Suppose we have enough hardware resources and data of a particular long input length. Will the model be able to perform the <em>same</em> task effectively on that, say, 1 million length? Or will we need a model with more parameters?</p>
<p>I would appreciate any reason or empirical result which shows this would or wouldn’t work. Also will this depend on the complexity of a task? If so why would the said complexity increase with input length?</p>
","transformer"
"45947","What about the loss and custom metric with per-pair weights in multi-class classification?","2024-06-10 22:56:46","","1","20","<transformer><accuracy><cross-entropy><multiclass-classification>","<p>Let's suppose that we have a multi-class classification problem with 5 classes: 0, 1, 2, 3, 4. The order is not random, they are neighbors. For example, imagine that a labelling is 1. If the prediction for it is 1, it is the best. If the prediction is 0 or 2, it is also good, but not the best. If the prediction is 3 is worse, while if the prediction is 4, it is the worst.</p>
<p>I want neither the classical cross entropy which simply penalizes any misclassification equally, nor the weighted one which simply penalizes more the classes with less samples in an imbalanced dataset. I want to specifically and per-pair penalize any of the possible (25 in my case) combinations of prediction-label.</p>
<p>So I have 4 specific questions:</p>
<ol>
<li><p>How is this so much customized loss called? What do I have to google and what do you suggest me?</p>
</li>
<li><p>What about the custom metric? I am talking about a custom metric for monitoring during the training, not in meta-training. I suppose it is something more advanced than the F1 score or the Fβ score?</p>
</li>
<li><p>Can these weights be somehow trainable? For example, let's say, by using Gradient Reversal layers?</p>
</li>
<li><p>Is this problem really multi-class classification or should I switch to something else? Generally speaking, is it good that I focus on improving the loss and the custom metric for monitoring, because I feel my model (Transformer) is doing very well but needs full human help and customization, or I should totally focus on its architecture and the classical - default accuracy - cross-entropy are more than enough?</p>
</li>
</ol>
","transformer"
"45936","Classifier-Free-Guidance with Transformers","2024-06-09 13:29:04","46258","0","53","<deep-learning><natural-language-processing><transformer>","<p><a href=""https://i.sstatic.net/v8te0AAo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/v8te0AAo.png"" alt=""enter image description here"" /></a></p>
<p>I'm working on music generation using transformers.
Using the decoder part for the audio tokens with text conditioning by the T5 encoder</p>
<p><a href=""https://i.sstatic.net/fztWztX6.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fztWztX6.png"" alt=""enter image description here"" /></a>
In Classifier-Free-Guidance, the text conditioning randomly dropped with a probability of <span class=""math-container"">$p_{uncond}$</span>. So when it's dropped, what should the cross-attention input be, or how is the transformer architecture modified?</p>
<p>Is it like when the text condition is not present the middle sub-block is skipped with an if statement?</p>
","transformer"
"45926","Why decoder only model require left padding?","2024-06-08 10:22:19","","0","57","<natural-language-processing><transformer><natural-language-understanding>","<p>We used Gemma 2B model to infer, and tried left and right padding. &quot;right&quot; padding is giving us different answer compared to left padding.
Why do we use left padding for decoder only model, please provide technical answer.</p>
<p>I have also check
<a href=""https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa"">While fine-tuning a decoder only LLM like LLaMA on chat dataset, what kind of padding should one use?</a> but there was no technical answer for it.</p>
","transformer"
"45923","Why I am getting different KV cache?","2024-06-08 08:02:18","","0","27","<natural-language-processing><transformer>","<p>I have taken Squad 2.0 dataset for inferencing Gemma 2B model.</p>
<p>When I provided model with 1st datapoint truncating till 36 tokens and same datapoint truncating till 80 tokens.</p>
<p>I am getting slightly different KV cache for first 36 tokens, why is that so?</p>
<p><strong>Example:</strong></p>
<p><strong>Input 1 Token:</strong> [[     2, 235309,  19647, 235307]]</p>
<p><strong>Input 2 Token:</strong> [[     2, 235309,  19647, 235307,   1261,    573,   2872]]</p>
<p>Above is just example not actual prompt. Input 2 is extension of input 1.</p>
<p>When giving input 1 and input 2 to model we will get KV cache. The KV cache of first 4 tokens should be same theoretically. But I am getting difference in both the KV cache only when Input 2 has more than 54 tokens in my case. Otherwise I am getting same KV cache.</p>
<p>Please help me figure out why this case is happening.</p>
","transformer"
"45899","Will an AI LLM learn a language if fed during training with a large corpus of undeciphered language?","2024-06-06 00:34:05","","2","69","<training><transformer><large-language-models><machine-translation>","<p>AIs can learn many languages just by being trained on their corpora.</p>
<p>What will happen if we in addition would train it on a large corpus of undeciphered language, like Minoan or Etruscan? Will it be able to learn and decipher it?</p>
<p>Or it necessarily needs a human feedback? I have seen the LLMs can write in obscure languages, like Livonian. Were they provided human feedback from a Livonian speaker?</p>
","transformer"
"45866","Can transformer attention make predictions based on analogy?","2024-06-01 16:35:20","45941","1","88","<transformer><attention><reasoning>","<p>Suppose I have included 3 examples of an idiosyncratic sentence for training by a transformer:</p>
<ul>
<li>Example 1: <strong>Asdfogiug likes Zsdfoiusdhf and Zsdfoiusdhf likes Asdfogiug too.</strong></li>
<li>Example 2: <strong>Bsodifhas likes Zsdfoiusdhf and Zsdfoiusdhf likes Bsodifhas too.</strong></li>
<li>Example 3: <strong>Clkwjehrq likes Zsdfoiusdhf and Zsdfoiusdhf likes Clkwjehrq too.</strong></li>
</ul>
<hr />
<p>Now we prompt the above-trained transformer with the following:</p>
<p><strong>Dlwkjerhtw likes Zsdfoiusdhf and Zsdfoiusdhf likes ______________</strong></p>
<hr />
<p>Can our transformer complete the prompt correctly?</p>
<p>How would the attention mechanism know to generate <strong>&quot;Dlwkjerhtw&quot;</strong> (a word which it has never seen before anywhere else) as the predicted next word in our prompt?</p>
","transformer"
"45850","Unexpected results using ORPO trl","2024-05-29 22:23:04","","0","25","<transformer><fine-tuning><mistral>","<p>For studying purposes, I've created a very small dataset about a fictional city called &quot;Auryn&quot;:</p>
<p><a href=""https://huggingface.co/datasets/celsowm/auryn_dpo_orpo_english"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/celsowm/auryn_dpo_orpo_english</a></p>
<p>So, my goal is to &quot;inject&quot; new knowledge on an LLM as mistral, so I tried this:</p>
<pre><code>import torch, gc, sys
from peft import LoraConfig, prepare_model_for_kbit_training, PeftModel
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM
from trl import setup_chat_format, ORPOConfig, ORPOTrainer
from datasets import load_dataset

torch_dtype = torch.bfloat16

if torch.cuda.get_device_capability()[0] &gt;= 8:
    attn_implementation = &quot;flash_attention_2&quot; #pip install -qqq flash-attn
else:
    attn_implementation = &quot;eager&quot;
    
#Lora
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch_dtype,
    bnb_4bit_use_double_quant=True
#     llm_int8_enable_fp32_cpu_offload=True
)

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']
)

# Load Token
#model_name = &quot;meta-llama/Meta-Llama-3-8B-Instruct&quot;
model_name = &quot;mistralai/Mistral-7B-v0.1&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Carregando Modelo &quot;base&quot;
model=AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map='auto',
    torch_dtype=torch_dtype,
    attn_implementation=attn_implementation
)

# checking params
def print_trainable_parameters(model):
    trainable_params=0
    all_params=0
    for _, param in model.named_parameters():
        all_params+=param.numel()
        if param.requires_grad:
            trainable_params+=param.numel()
    print(f&quot;trainable params: {trainable_params} || all params: {all_params} || trainable%: {100 * trainable_params/all_params:.2f}&quot;)

print_trainable_parameters(model)

# Set chat format and feeze pretrained weights
model, tokenizer = setup_chat_format(model, tokenizer)
model = prepare_model_for_kbit_training(model)
print_trainable_parameters(model)


#load dataset
dataset_name = &quot;celsowm/auryn_dpo_orpo_english&quot;
dataset = load_dataset(dataset_name, split=&quot;all&quot;) #download_mode='force_redownload'

def process(row):
    
    prompt_user = {&quot;content&quot;: row[&quot;prompt&quot;], &quot;role&quot;: &quot;user&quot;}

    row[&quot;prompt&quot;] = tokenizer.apply_chat_template(
                    [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: row[&quot;prompt&quot;]}],
                    tokenize=False,
                    add_generation_prompt=True
                )
    row[&quot;chosen&quot;] = tokenizer.apply_chat_template([prompt_user, {&quot;content&quot;: row[&quot;chosen&quot;], &quot;role&quot;: &quot;assistant&quot;}], tokenize=False)
    row[&quot;rejected&quot;] = tokenizer.apply_chat_template([prompt_user, {&quot;content&quot;: row[&quot;rejected&quot;], &quot;role&quot;: &quot;assistant&quot;}], tokenize=False)
    return row

dataset = dataset.map(process)
dataset = dataset.train_test_split(test_size=0.1)

orpo_args=ORPOConfig(
    learning_rate=5e-6,
    beta=0.1,
    gradient_accumulation_steps=1,
    gradient_checkpointing=True,
    lr_scheduler_type=&quot;cosine&quot;,
    max_length=1024,
    max_prompt_length=2048,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    optim=&quot;adamw_torch&quot;,
    output_dir=&quot;output/auryn_orpo_english&quot;,
    overwrite_output_dir=True,
    bf16=True,
)

trainer=ORPOTrainer(
    model=model,
    args=orpo_args,
    train_dataset=dataset[&quot;train&quot;],
    eval_dataset=dataset[&quot;test&quot;],
    peft_config=peft_config,
    tokenizer=tokenizer
)

trainer.train()
trainer.save_model(orpo_args.output_dir)
tokenizer.save_pretrained(orpo_args.output_dir)

#Merge
del trainer, model
gc.collect()

torch.cuda.empty_cache()

tokenizer=AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch_dtype,
    device_map=&quot;cpu&quot;,
)

model, tokenizer = setup_chat_format(model, tokenizer)

# Merge adapter with base model
model = PeftModel.from_pretrained(model, orpo_args.output_dir)
model = model.merge_and_unload()

merged_dir = &quot;output/auryn_orpo_english_merged&quot;
tokenizer.save_pretrained(merged_dir)
model.save_pretrained(merged_dir) 
</code></pre>
<p>After that I tried this:</p>
<pre><code>import torch, sys
from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, pipeline

torch_dtype = torch.bfloat16

if torch.cuda.get_device_capability()[0] &gt;= 8:
    attn_implementation = &quot;flash_attention_2&quot; #pip install -qqq flash-attn
else:
    attn_implementation = &quot;eager&quot;
    
#Quantização com QLora
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch_dtype,
    bnb_4bit_use_double_quant=True,
    llm_int8_enable_fp32_cpu_offload=True
)

model_name = &quot;output/auryn_orpo_english_merged&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Carregando Modelo &quot;base&quot;
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map='auto',
    torch_dtype=torch_dtype,
    attn_implementation=attn_implementation
)

# Create a conversation pipeline
conversation = pipeline(&quot;conversational&quot;, model=model, tokenizer=tokenizer)

# Define the chat history
chat_history = [
    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who founded the city of Auryn?&quot;},
]

# Generate a response
response = conversation(chat_history)
print(response)
</code></pre>
<p>The response:</p>
<pre><code>Conversation id: b82f1b79-6c21-4f6e-8af5-1ec587e21ef1
user: Who founded the city of Auryn?
assistant: user
Who founded the city of Auryn?
I was expecting something like: Jonathan Auryn (chosen)
</code></pre>
<p>So, what did I do wrong?
Thanks in advance !</p>
","transformer"
"45839","Spatial vs spatiotemporal methods for object counting in low frame-rate videos","2024-05-28 15:39:59","","0","22","<computer-vision><transformer><long-short-term-memory><object-detection><vision-transformer>","<p>I'm currently working on an object counting/density estimation task using low frame rate video (~2 fps) in a traffic setting. I've explored a lot of literature on both spatial methods (i.e. using only individual images) and spatiotemporal methods (i.e. utilizing the sequential nature of video frames); many of these are based on CNN-LSTM or transformer models. It seems to me that generally, if video is available, using spatiotemporal models can be more powerful.</p>
<p>My question is this:
If I'm using pretty low frame rate video and the vehicles are moving 10-30 mph, is there a significant benefit to using a spatiotemporal over just spatial features? I ask because it seems more complicated to to the former, and if it doesn't bring significant benefits given my frame rate, I'd rather stick with something a bit simpler. Does anyone have any experience implementing either of these approaches for low frame rate video?</p>
","transformer"
"45818","Why are the Q and K matrices two separate matrices in attention?","2024-05-25 20:07:33","45819","0","58","<transformer><attention><weights><linear-algebra>","<p>If I understand correctly the attention layer is represented as
<span class=""math-container"">$$
\begin{align}
&amp;softmax(\frac{Q K^T}{\sqrt{d_k}}) V  \\
= &amp;softmax(\frac{(s W_q) (s W_k)^T}{\sqrt{d_k}}) V \\
= &amp;softmax(\frac{(s W_q) (W_k^T s^T)}{\sqrt{d_k}}) V \\
= &amp;softmax(\frac{s (W_q W_k^T) s^T}{\sqrt{d_k}}) V \\
\end{align}
$$</span></p>
<p>Wouldn't it be better to take <span class=""math-container"">$W_q W_k^T$</span> as a single parameter?</p>
","transformer"
"45816","What’s more efficient in multihead attention: multiply QKV by $W_i$ then split or linearly project QKV $h$ times into dimensions $d_k$?","2024-05-25 16:45:53","","0","30","<machine-learning><deep-learning><transformer><pytorch><encoder-decoder>","<p>I’m looking to bridge two implementations of multihead attention.</p>
<p><strong>Approach 1: Multiply and Split</strong></p>
<p>Each of the queries, keys, and values is multiplied by a separate square weight matrix of size (embedding size, embedding size). The transformed embeddings are then split into <span class=""math-container"">$h$</span> number of subsets (heads), where each subset has a dimension of (embedding size / h).</p>
<p><strong>Approach 2: Direct Projection</strong></p>
<p>Each of the queries, keys, and values is multiplied by a weight matrix of size (embedding size, <span class=""math-container"">$d_k$</span>), where
<span class=""math-container"">$d_k$</span> is the dimension of each head, and typically <span class=""math-container"">$d_k = \text{embedding size} / h$</span>. The projected embeddings are then reshaped into <span class=""math-container"">$h$</span> heads, each of dimension <span class=""math-container"">$d_k$</span>.</p>
<p><strong>Question</strong></p>
<p>Are these equivalent mathematical implementations with differing efficiencies?</p>
<p>I’d like to grasp both the math and the PyTorch implementations here. There are similar answers here, but no one goes into explaining the differences in efficiency or the mathematical similarities between both.</p>
","transformer"
"45813","Why can't we use only keys to calculate self-attention?","2024-05-25 11:27:00","","0","33","<deep-learning><natural-language-processing><computer-vision><transformer><attention>","<p>I was reading about the self-attention mechanism and the paper suggests to have 3 things to be computed: Key, Query and Value. As far as I understood the reason for having Value is to allow adjustments to the initial embedding (after the positional encoding) depending on the context (that's intuitive). However, I don't get why we need the Query there. Why can't we do the similarity calculations using only the Keys?</p>
","transformer"
"45780","How does BERT know how to where to add segment embeddings (i.e. to differentiate between two sentences packed in a single token sequence)","2024-05-23 01:45:39","","0","19","<transformer><bert>","<p>In addition to using a special [SEP] token to distinguish between two sentences, I understand that BERT also adds special learned embeddings to each sentence:</p>
<p>&quot;we add a learned embedding to every token indicating whether it belongs
to sentence A or sentence B&quot; (from the BERT paper)</p>
<p>To my understanding, BERT has no way of differentiating between the two sentences in the raw input apart from the [SEP] token between them, but the model architecture doesn't specifically recognize this as a special token; (from my understanding) the model learns to treat this as a separating token while pretraining (Is this correct? I'm not too sure about whether I understand this part correctly either). If this is true, how does the model know where to add the additional learned embedding to sentence A and B (i.e. how does it know which part of the input sequence is A and which part is B, so it can add the additional embeddings appropriately)?</p>
<p>Just a quick note that I am not very familiar with these topics, so please correct me if there are any additional bugs in my understanding that you can pick out! This would be very appreciated--I am quite certain I'm missing some key understanding here.</p>
","transformer"
"45778","Incredibly High CrossEntropyLoss in Sequence-to-Sequence Generation","2024-05-22 17:43:17","","0","11","<deep-learning><transformer><pytorch><cross-entropy>","<p>I'm trying to do SMILES chemical representation prediction from a large dataset (Around 5M Samples) to teach it do predict another downstream task. The model's part responsible for generating the data is a decoder embedding layer that roughly looks like this:</p>
<pre><code>self.decoder_embedding = nn.Embedding(len(tokenizer), hidden_size)
decoder_layer = nn.TransformerDecoderLayer(
    d_model=hidden_size,
    nhead=heads,
    dim_feedforward=hidden_size,
    dropout=dropout,
    batch_first=True,
    norm_first=True
)
self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=2)
self.smiles_generation_head = nn.Linear(hidden_size, 125)
</code></pre>
<p>The inputs to the model includes <code>smiles_tokens</code> which have been randomly masked of shape [batch_size, 125] and a padding attention mask. The output is simply <code>pretext_predictions</code> of shape [batch_size, 125].</p>
<pre><code>def mask_tokens(inputs, tokenizer, mask_prob=0.15):
    masked_inputs = inputs.clone()
    mask_token_id = tokenizer.mask_token_id

    # Mask a random selection of tokens
    random_mask = torch.rand(masked_inputs.shape) &lt; mask_prob
    masked_inputs[random_mask] = mask_token_id

    return masked_inputs

masked_smiles_tokens = mask_tokens(non_share_smiles_tokens, tokenizer)

smiles_tokens = model(masked_smiles_tokens, *features)
</code></pre>
<p>When this is passed into the loss function, it starts at an incredibly high loss value (roughly 10K and above). When the batch size is increased, the loss value also doubles and sometimes triples to around 30K or above.</p>
<pre><code>smiles_tokens = nn.ConstantPad1d(
    (0, 125 - smiles_tokens.shape[1]),
    0
)(smiles_tokens).float()
padding_mask = smiles_tokens != 0
loss = loss_fn(
    pretext_predictions[padding_mask],
    smiles_tokens[padding_mask],
)
</code></pre>
<p>This is the loss function definition:</p>
<pre><code>def loss_fn(inputs, targets):
    ce_criterion = nn.CrossEntropyLoss(reduction='mean')
    ce_loss = ce_criterion(inputs, targets)
    return ce_loss
</code></pre>
<p>What is causing this high loss value? I tired normalizing my input features apart from the SMILES token indices and that doesn't seem to solve the issue. I noticed that when I create random tensors like this:</p>
<pre><code>import torch
import torch.nn as nn
import random

# Example tensors (replace with your actual data)
predictions = torch.randn(16, 50).softmax(dim=1)
targets = torch.randint(0, 100, (16, 50)).float()
mask = torch.randint(0, 2, (16, 50))

# Create the loss function
loss_fn = nn.CrossEntropyLoss() 

# Calculate the loss
loss = loss_fn(predictions[mask], targets[mask])

print(loss)
</code></pre>
<p>It also result in huge loss values.</p>
<pre><code>tensor(9981.9561)
</code></pre>
<p>My ultimate aim is a BCE task and the loss from BCE is very small (0 to 1) compared to the CE loss. This makes it difficult to asses whether the model is making any progress. What should I do? Do I simply just find another loss function? NLLLoss also doesn't seem to being doing well but CE Loss uses that under the hood so I'm guessing it stems from that?</p>
<p>Note: <code>pretext_predictions</code> are logits, and <code>smiles_tokens</code> are indices for the tokenizer vocabulary of size 37. The learning rate is 1e-3 using Adam and model size is only 5M parameters.</p>
","transformer"
"45708","Determining optimal data size for generalization in transformer encoders, particularly for Time-Series signal data","2024-05-14 21:56:30","","0","43","<deep-learning><natural-language-processing><transformer><time-series>","<p>I'm currently experimenting with training a model that employs a single transformer encoder on time-series signal data. Despite having a relatively small dataset of around 50 examples, each with a sequence length of approximately 1000, the model seems to excel at understanding and memorizing these examples. However, I'm concerned about its generalization capabilities given the limited amount of data.</p>
<p>I'm wondering: <strong>How much data is typically required for a transformer encoder to generalize well,</strong> especially in the context of time-series signal data? Is there a recommended range or guideline for the amount of training data that can help ensure better generalization performance? Additionally, are there specific strategies or techniques that can enhance generalization in transformer models when working with small datasets?</p>
<p>Any insights, experiences, or references would be greatly appreciated. Thank you!</p>
","transformer"
"45691","What should be Relationship between embedding dimension and context length?","2024-05-13 12:59:22","45697","2","129","<natural-language-processing><transformer><large-language-models>","<p>What should we keep hidden dimension/embedding dimension (d_model as per attention is all you need paper), greater, equal, or smaller to the context length (n)?</p>
<p>Is there any such relationship between the embedding dimension and context length?</p>
<p>How will it affect the LLM?</p>
","transformer"
"45653","xLSTM parallel computation - mismatch in dimensions","2024-05-09 14:09:19","","0","104","<neural-networks><machine-learning><transformer><recurrent-neural-networks><long-short-term-memory>","<p>In <a href=""https://arxiv.org/abs/2405.04517"" rel=""nofollow noreferrer"">this</a> recent paper, a new architecture is proposed, called xLSTM. I've implemented the sequential version in PyTorch, but it's slower than I would like, so I'm now implementing the parallel version that's explained in the appendix (page 25-26). I feel like this page might contain a mistake, or maybe I'm missing something, so I wanted to check here.</p>
<p>The issue is as follows. We consider an input sequence <span class=""math-container"">$X\in\mathbb{R}^{T\times d}$</span> and we obtain two matrices <span class=""math-container"">$F, I\in\mathbb{R}^{T\times T}$</span> (see the paper for details), which combine into <span class=""math-container"">$D\in\mathbb{R}^{T\times T}$</span>. All well and good. Now, it is stated that <span class=""math-container"">$Q, K, V\in\mathbb{R}^{T\times d}$</span>. I believe this should be <span class=""math-container"">$\mathbb{R}^{T\times e}$</span> where <span class=""math-container"">$e$</span> denotes the embedding dimension, but that is not the main source of confusion. The point is that we have the equation <span class=""math-container"">$C=QK^T\odot D$</span> in the paper, and subsequently, <span class=""math-container"">$H$</span> is obtained as <span class=""math-container"">$CV$</span>. But <span class=""math-container"">$QK^T\in \mathbb{R}^{T\times e\times e}$</span>, which makes sense since this is a sequence of linear transformations which will transform the sequence <span class=""math-container"">$V\in\mathbb{R}^{T\times e}$</span>. But then the dimensions of <span class=""math-container"">$QK^T$</span> do not match those of <span class=""math-container"">$D$</span>, so the equation <span class=""math-container"">$QK^T\odot D$</span> does not make sense to me.</p>
<p>Have I missed something here, or is there an issue with the equation in the paper?</p>
","transformer"
"45591","Understanding different methods of covariance parametrization","2024-04-30 10:14:06","45594","0","20","<transformer><loss><uncertainty-quantification>","<p>In the paper <a href=""https://arxiv.org/pdf/2112.02143"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2112.02143</a> it is noted that there are different ways to &quot;parametrize&quot; covariance. (page 4)
<a href=""https://i.sstatic.net/pBemXqfg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pBemXqfg.png"" alt=""enter image description here"" /></a></p>
<p>What does &quot;parametrizing&quot; covariance mean exactly? In the model described in the paper, prediction uncertainty in the form of covariance is estimated using an MLP. The resulting covariance matrix is used in an uncertainty-reduction loss function to train the model. I do not really understand what different parametrizations of covariance there are. As far as I know there exists simply one covariance of model predictions.</p>
<p>I also looked at the paper <a href=""https://arxiv.org/pdf/1910.14215"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1910.14215</a> which they reference. In it there are mentions of epistemic and aleatoric uncertainty however I do not really see how one can distinguish between the 2 when building this model, or if that is even what is meant by covariance &quot;parametrization&quot;.</p>
","transformer"
"45578","What does ""position-wise"" fully connected mean?","2024-04-29 13:53:47","45581","0","75","<transformer><feedforward-neural-networks>","<p>I understand the architecture of a position-wise feed-forward network as described in (<a href=""https://nn.labml.ai/transformers/feed_forward.html"" rel=""nofollow noreferrer"">https://nn.labml.ai/transformers/feed_forward.html</a>). However I do not understand what &quot;position wise&quot; refers to. I assume &quot;position&quot; refers to where a data point is inside the input vector/matrix. What makes this architecture &quot;position-wise&quot;?</p>
","transformer"
"45566","When using local self-attention how do keys and queries wind up having compatible dimensions if if different sized convolutions are used to make them?","2024-04-28 17:26:24","","0","18","<transformer><attention>","<p>In the paper <a href=""https://arxiv.org/pdf/2112.02143"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2112.02143</a> on page 3 in figure 1 an architecture is described.
<a href=""https://i.sstatic.net/OlzLr5U1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OlzLr5U1.png"" alt=""enter image description here"" /></a></p>
<p>As can be seen in the upper left, the keys for Local self-attention are generated using a 3x3 convolution on the input. The same input is then concatenated with the output of that and further processed.  From my understanding the input is a stream of velocity and acceleration vectors. So a 3xm matrix where the (2<em>n)-th column is the n-th velocity measurement and the (2</em>n+1)-th column the n-th acceleration measurement. I would assume that applying a 3x3 convolution to this creates a 1x(m-2) matrix. How would one concatenate that with a 3xm matrix?</p>
","transformer"
"45556","Is there a relationship between tokens and parameters in LLMs?","2024-04-27 19:35:10","45560","1","122","<transformer><large-language-models>","<p>What the question says.</p>
<p>In a transformer architecture, is there a relationship between number of tokens and number of parameters?</p>
<p>Can you have a LLM with a small number of parameters but a large context window (many tokens) or viceversa?</p>
","transformer"
"45555","Is there any purpose of altering neural network architecture if validation loss does not decrease but training loss does?","2024-04-27 18:15:10","","0","22","<neural-networks><natural-language-processing><transformer><gradient-descent>","<p>I am training a transformer based neural network and the validation loss is not decreasing, but the training loss does decrease. I am wondering if it's possible to debug or change the architecture such that this is reversed, or if I definitely need to debug my dataset.</p>
","transformer"
"45552","What if the sum of word embedding and positional embedding becomes same for different words?","2024-04-27 04:45:06","","0","45","<transformer><word-embedding>","<p>In Transformers, we add the positional embedding with the word embedding and then process it. But, what if the sum of the embeddings become same for different words at different position of a sentence? How would the model differentiate it?</p>
","transformer"
"45543","How to Interpret Cross Attention","2024-04-25 20:02:10","","1","74","<transformer><attention><seq2seq><encoder-decoder>","<p>I am a bit confused on what cross attention mechanisms are doing. I understand that the currently decoded output is usually the query and the conditioning/input (from an encoder) is the key and value. The query is multiplied by the key to make an attention matrix that details how much each element of the currently decoded output relates to each element of the input and then this matrix is applied to the value. Isn't the output of this cross attention mechanism an enriched version of the input and not the currently decoded output since the attention matrix is being applied to values. I would think that the output of the entire mechanism should be and enriched version of the currently decoded output based on what the input is.</p>
<p>A side thought of mine is that the cross attention mechanism does in fact produce an enriched version of the input which is then added back into the currently decoded output through a skip connection.</p>
","transformer"
"45516","why explicit reference of batchsize is required in attention","2024-04-23 17:04:30","","0","36","<transformer><attention><batch-size>","<p>When I read the code for multi-head attention, I noticed all people consider the batch_size in the <code>forward()</code></p>
<p>For example, the code below was provided by GPT:</p>
<pre><code>class MultiHeadAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(MultiHeadAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads
        
        assert self.head_dim * heads == embed_size, &quot;Embedding size needs to be divisible by heads&quot;
        
        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)
        
    def forward(self, values, keys, query, mask):
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]
        
        # Split the embedding into self.heads different pieces
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = query.reshape(N, query_len, self.heads, self.head_dim)
        
        values = self.values(values)
        keys = self.keys(keys)
        queries = self.queries(queries)
        
        # Dot product attention
        energy = torch.einsum(&quot;nqhd,nkhd-&gt;nhqk&quot;, [queries, keys])
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float(&quot;-1e20&quot;))
        
        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)
        out = torch.einsum(&quot;nhql,nlhd-&gt;nqhd&quot;, [attention, values]).reshape(N, query_len, self.heads * self.head_dim)
        
        # Apply the final linear layer and return
        out = self.fc_out(out)
        return out
</code></pre>
<p>I don't understand why we need <code>N=query.shape[0]</code>. I know the N here represents the batchsize, but I don't know why it is necessary considering pytorch would automatically deal with the batchsize as it did in CNN. Instead, I think <code>values=values.reshape(value_len,self.heads, self.head_dim)</code> is better.</p>
<p>And I asked GPT, he told me the batchsize is important when split the queries into multiple-head. But I think <span class=""math-container"">$d_{model}=n_{head}\cdot d_k$</span>, therefore it is nothing to do with the batchsize.</p>
","transformer"
"45508","Is token mask masked in attention of encoders of bert?","2024-04-23 03:53:05","","0","34","<transformer><attention><bert><padding>","<p>I have recently researched on Bert structure. And the paper says we will mask some token at the input in 80%, 10% input be changed and 10% left remained. But I wonder if the mask token in the input be masked in attention of encoders layer? In that case, the input token of padding token and mask token is the same (equal 0). is it fine?</p>
","transformer"
"45493","How to get Complexity per Layer, Sequential Operations and Maximum Path Length in CNN architecture?","2024-04-21 16:29:41","","0","42","<deep-learning><convolutional-neural-networks><transformer><attention>","<p><a href=""https://i.sstatic.net/gwwKfw5I.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gwwKfw5I.png"" alt=""enter image description here"" /></a></p>
<p>In the paper <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention is all you need</a>, here is Table 1, can someone explain what architecture is referred to in the &quot;Convolution&quot; row and hence describe the other 3 columns in it? The other ones are pretty clear, for example Recurrent, takes <span class=""math-container"">$O(d^2)$</span> operations in one time-step to multiply the hidden state with the weight matrix, and there are <span class=""math-container"">$n$</span> such time-steps, which need to be done sequentially (<span class=""math-container"">$O(n)$</span>), also making the information from the first and last tokens in the sentence to travel <span class=""math-container"">$O(n)$</span> steps. For Self-attention, every word in the sentence, attends to every other, so <span class=""math-container"">$O(n^2)$</span> pairs, dot product attention taking <span class=""math-container"">$O(d)$</span> operations. Clearly, the path length is <span class=""math-container"">$O(1)$</span> and there are no sequential operation, all these <span class=""math-container"">$O(n^2)$</span> pairs can be computed independently. The &quot;Convolution&quot; row is not clear to me!</p>
","transformer"
"45488","Why does the algorithm in ""Self-attention Does Not Need $O(n^{2})$ Memory"" require $O(log n)$ memory when $k, v$ pairs are not ordered?","2024-04-20 22:58:38","","0","113","<transformer><attention><implementation><softmax><complexity-theory>","<p>I am reading <a href=""https://arxiv.org/abs/2112.05682"" rel=""nofollow noreferrer"">Self-attention Does Not Need <span class=""math-container"">$O(n^{2})$</span> Memory</a> which proposes an algorithm that requires <span class=""math-container"">$O(1)$</span> memory for one query and <span class=""math-container"">$O(log n)$</span> memory for self-attention, in theory. In practice the authors claim it is <span class=""math-container"">$O(\sqrt{n})$</span>. <span class=""math-container"">$n$</span> here is the sequence length.</p>
<p>I am having quite the difficulty understanding why the algorithm requires <span class=""math-container"">$O(log n)$</span> memory when the sequence of <span class=""math-container"">$k, v$</span> pairs is not ordered. This supposes the query is fixed (not yet self-attention).</p>
<p>Here is the standard attention algorithm:</p>
<p><span class=""math-container"">$s_{i} = dot(q, k_{i})$</span>, <span class=""math-container"">$s_{i} = \frac{e^{s_{i}}}{\sum_{j}^{}{e^{s_{j}}}}$</span>, <span class=""math-container"">$attention(q,k,v)=\sum_{j}^{}{v_{i}s_{i}'}$</span></p>
<p>Here is the suggested algorithm:</p>
<p><span class=""math-container"">$s_{i} = dot(q, k_{i})$</span>, <span class=""math-container"">$s_{i} = e^{s_{i}}$</span>, <span class=""math-container"">$attention(q,k,v)=\frac{\sum_{j}^{}{v_{i}s_{i}'}}{\sum_{j}^{}{s_{j}'}}$</span></p>
<p>For ordered sequences of <span class=""math-container"">$(k, v)$</span> pairs, and computing the terms sequentially, it is clear that the memory complexity is <span class=""math-container"">$O(1)$</span>. But I don't understand why it is <span class=""math-container"">$O(log n)$</span> for unordered sequences.</p>
<p>From my understanding, if we have a sequence of length <span class=""math-container"">$n$</span>, we store one scalar <span class=""math-container"">$s^{*} \in \mathbb{R}$</span>, this will constitute the <span class=""math-container"">$\sum_{j}^{}{s_{j}'}$</span> in the attention computation. We also store a vector <span class=""math-container"">$v^{*}\in \mathbb{R}^{d}$</span> that will constitute the <span class=""math-container"">$\sum_{j}^{}{v_{i}s_{i}'}$</span> in the attention computation. I don't understand why these quantities will be affected by the order of the pairs, the summation is commutative so whatever the order in which we process the pairs we should get the same value at the end.</p>
<p>I'm sure I'm missing something but I can't quite figure it out. I do not think it is the case, but maybe the authors mean by &quot;inputs are provided in a different order&quot; that we don't receive pairs of keys and values but they come individually? I don't think it is the case because then wouldn't the most efficient way to keep track of which values' indices were used for which keys' indices is to keep two hash tables, which leads to <span class=""math-container"">$O(n)$</span>?</p>
","transformer"
"45485","Decision Transformer: more than a ""trajectory picking"" algorithm?","2024-04-20 15:19:11","46174","1","32","<transformer>","<p>I'm studying the <a href=""https://sites.google.com/berkeley.edu/decision-transformer"" rel=""nofollow noreferrer"">Decision Transformer</a> for some offline reinforcement learning tasks. The basic idea is to collect a huge quantity of data generated by a real experimental device (let's say an arm manipulator) and then apply offline reinforcement learning for determining a policy, which maximize for instance the current consumption.</p>
<p>After reading the work of <a href=""https://arxiv.org/pdf/2005.01643.pdf"" rel=""nofollow noreferrer"">Sergey Levine</a> et al. and look of some of his videos (really good by the way), I thought at offline reinforcement learning as a new possibility to learn a policy from collected real data, avoiding to use a simulator or something like that. But as stated in the link above at page 29:</p>
<p><em>&quot;A reasonable question we might ask in regard to datasets for offline RL is: in which situations might we actually expect offline RL to yield a policy that is significantly better than any trajectory in the training set? While we cannot expect offline RL to discover actions that are better than any action illustrated in the data, we can expect it to effectively utilize the compositional structure inherent in any temporal process. This idea is illustrated in Figure 4: if the dataset contains a subsequence illustrating a way to arrive at state 2 from state 1, as well as a separate subsequence illustrating how to arrive at state 3 from state 2, then an effective offline RL method should be able to learn how to arrive at state 3 from state 1, which might provide for a substantially higher final reward than any of the subsequences in the dataset. When we also consider the capacity of
neural networks to generalize, we could imagine this sort of “transitive induction” taking place on a portion of the state variables, effectively inferring potentially optimal behavior from highly suboptimal components. This capability can be evaluated with benchmarks that explicitly provide data containing this structure, and the D4RL benchmark suite provides a range of tasks that exercise this capability.&quot;</em> <a href=""https://arxiv.org/pdf/2005.01643.pdf"" rel=""nofollow noreferrer"">Sergey Levine</a></p>
<p>So as far as I understood, it is (at the moment) only auspicable to determine an offline policy by &quot;combining&quot; subsequences together in order to achieve a desired task or to reach a desired return.</p>
<p>Then I move over and discovered the <a href=""https://sites.google.com/berkeley.edu/decision-transformer"" rel=""nofollow noreferrer"">Decision Transformer</a>, which look very elegant and powerful, since they would basically replace the whole RL concept with a single Transformer.
Despite the fact, that the benchmark results of the DT reported in the paper are in some cases better than CQL and BC (respectively for Conservative Q Learning and Behavioral Cloning), I have to admit, that DTs just combine &quot;trajectory pieces&quot; and do not determine a &quot;new&quot; policy for a specific task.</p>
<p>Is it right? Or am I missing something?</p>
<p>I'm looking for a good reason, to apply DTs in my tasks in order to determine an optimal policy (offline) compared to conventional offline methods.</p>
","transformer"
"45437","How to construct source padding mask for embedded audio?","2024-04-13 04:58:32","","0","23","<transformer><data-preprocessing><audio-processing><padding>","<p>I'm attempting a music transcription task - similar to speech recognition but with music and notes (string representations) instead of speech audio and sentences. The model consists of a CNN audio encoder and a vanilla transformer in PyTorch.</p>
<p>In order to batch my data, I have to pad my audio (and labels) to the same length. This is where I'm struggling to construct a padding mask for audio, especially when it needs to go through an embedding process before being fed into transformer's encoder layers.</p>
<p>My first intuition is to simply pad the audio with zeros to the longest length within each batch. The transformer in PyTorch expects a source key padding mask of shape (N, S) for batched input, where N is the batch size and S is the source sequence length. Let's say that a batch of audio is of shape (N, T), which would be passed through the CNN frontend and added sinusoidal positional encoding, resulting in a shape of (N, T', n_state), where T' &lt; T depending on the stride used. Does anyone have any pointers on how create a mask for the padded audio?</p>
","transformer"
"45403","Attention Mechanism: Why don't we just use a simple dot product instead of the Q, K, V matrices?","2024-04-09 09:38:57","","0","36","<natural-language-processing><transformer><attention><gpt>","<p>I am currently learning about Transformers by reading Richard Turner's paper <a href=""https://arxiv.org/pdf/2304.10557.pdf"" rel=""nofollow noreferrer"">&quot;An Introduction to Transformers&quot;</a>. On page 3 of the paper he gave a &quot;naive&quot; approach to build the attention matrix by a simple dot product of the input vector <span class=""math-container"">$X$</span> (then passing through a softmax function) - given input vector <span class=""math-container"">$X$</span> (a <span class=""math-container"">$D \times N$</span> matrix) we can calculate the <span class=""math-container"">$n, n'$</span>-th entry of the attention matrix <span class=""math-container"">$A$</span> as:</p>
<p><span class=""math-container"">$$A_{n,n'} = \frac{\exp( \textbf{x}_n^\top  \textbf{x}_{n'})}{\sum_{n''=1}^N \exp(\textbf{x}_{n''}^\top \textbf{x}_{n'})}$$</span></p>
<p>Written in matrix form it would be:</p>
<p><span class=""math-container"">$$A = \text{softmax}(X^{\top}X)$$</span></p>
<p>Where <span class=""math-container"">$\text{softmax}(M)$</span> means to apply the regular softmax function to each <em>column</em> of the matrix <span class=""math-container"">$M$</span>.</p>
<p>He later on explained that this approach &quot;entangles information about the similarity between locations in the sequence with the content of the sequence itself&quot;, and thus improving it by applying a linear transformation <span class=""math-container"">$U$</span> to the vector <span class=""math-container"">$\textbf{x}_n$</span>:</p>
<p><span class=""math-container"">$$A_{n,n'} = \frac{\exp( \textbf{x}_n^\top U^\top U \textbf{x}_{n'})}{\sum_{n''=1}^N \exp(\textbf{x}_{n''}^\top U^\top U \textbf{x}_{n'})}$$</span></p>
<p>or</p>
<p><span class=""math-container"">$$A = \text{softmax}(X^\top U^\top UX)$$</span></p>
<p>And later, he again improving it further by using two separate linear transformations <span class=""math-container"">$U_k$</span> and <span class=""math-container"">$U_q$</span>, since &quot;the numerator
in this construction is symmetric and this could be a disadvantage&quot;:</p>
<p><span class=""math-container"">$$A_{n,n'} = \frac{\exp \left( \textbf{x}_n^\top U_{\textbf{k}}^\top U_{\textbf{q}}^{} \textbf{x}_{n'} \right)}{\sum_{n''=1}^N \exp \left(\textbf{x}_{n''}^\top U_{\textbf{k}}^\top U_{\textbf{q}}^{} \textbf{x}_{n'}\right)}$$</span></p>
<p>or</p>
<p><span class=""math-container"">$$ A = \text{softmax}(X^\top U_k^\top U_qX)$$</span></p>
<p>My problem is that the exact disadvantage of the two approaches above and the exact improvements each new approach provides isn't explained very clearly in the paper. Is it possible for anyone to offer a more detailed (both intuitively and mathematically) explanation on the disadivantage of each of the naive approaches and how the improvement would help?</p>
","transformer"
"45398","why we use learnable positional encoding instead of Sinusoidal positional encoding","2024-04-09 08:59:00","","1","287","<machine-learning><deep-learning><natural-language-processing><transformer><bert>","<p>In the original paper of <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">transformers</a> they using positional encoding to capture the position of each word in the sentence and for calculate that it using sin and cos ,like shom in the image.<a href=""https://i.sstatic.net/pFtun.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pFtun.png"" alt=""Sinusoidal positional encoding"" /></a> In Bert and the author architecture that based on transformers ,they use learnable position encoding ,so they initialize the vectors of positional encoding randomly and then start to adjust them in training.<br>My question is why we use the learnable positional encoding, what is the objective?</p>
","transformer"
"45378","Train my own LLM on a smaller corpus of text?","2024-04-07 12:21:57","","0","293","<natural-language-processing><transformer><large-language-models><open-ai><huggingface>","<p>Would it be possible to train my own LLM on a smaller corpus of text, lets say some coding documentation that I then want to ask questions about using the model?</p>
<hr />
<p>If so, are there any recommended ways of doing this, i.e is there a prebuilt architecture or library I can use, and just provide the corpus of text.</p>
","transformer"
"45350","Compare two songs content using Audio Spectogram Transformer","2024-04-04 15:08:32","","1","21","<deep-learning><transformer><deep-neural-networks><features>","<p>I'm trying to establish a similarity metric between two songs. To do this I'm using the <a href=""https://huggingface.co/docs/transformers/en/model_doc/audio-spectrogram-transformer"" rel=""nofollow noreferrer"">AST model on HuggingFace</a>. This model basically works in a way very similar to a ViT but applied to spectograms of the audio file. My basic approach is to take the output of a hidden layer for each song passed through the network, yielding a (1, 1214, 768) dim tensor.</p>
<pre><code>features = model(**inputs, output_hidden_states=True).hidden_states[3].detach().cpu().numpy()[0]
</code></pre>
<p>I then compare the features between two songs as follows:</p>
<pre><code>def compare_vecs(a, b):
    result = np.einsum('ij,ij-&gt;i', a, b)
    norm_A = np.linalg.norm(a, axis=1)
    norm_B = np.linalg.norm(b, axis=1)
    return np.sum(result) / (np.sum(norm_A) * np.sum(norm_B))

</code></pre>
<p>(this corresponds to dot producting the 768 dim vectors against each other than summing the list of 1214 dot product scores and normalizing)</p>
<p>I have tried several different <code>compare_vecs</code> implementations (e.g max, min, average, sum) and a few different layer indicies from which I grab the features (last layer, first layer, third layer)... nothing seems to really output anything that is &quot;wow&quot;.</p>
<p>I feel like I'm kind of shooting in the dark here and have a few questions:</p>
<ol>
<li><p>How does the AST handle different song lengths? It seems that when you extract features from the raw array of sound data the output is always (1, 1024, 128) -- what happens that the audio file length is normalized to a fixed dimension spectogram? (I couldn't find this on <a href=""https://arxiv.org/abs/2104.01778"" rel=""nofollow noreferrer"">the original paper</a>)</p>
</li>
<li><p>What impact does this have on how we compare feature vectors? Could I be comparing a very squished song against a very stretched song and getting total garbage?</p>
</li>
<li><p>Which hidden layers should I be using to extract the features from?</p>
</li>
<li><p>How should I actually be comparing the features mathematically?</p>
</li>
</ol>
","transformer"
"45329","How the Q,K,V be calculated in multi-head attention","2024-04-02 08:58:12","","0","245","<machine-learning><deep-learning><natural-language-processing><transformer><attention>","<p>I want to understand the transformer architecture, so I start with self attention and I understand their mechanism, but when I pass to the multi-head attention I find some difficulties like how calculate Q , K and V for each head.
I find many way to calculate Q , K and V but I don't know which  way is correct.<br>
<strong>method 1:</strong>
<img src=""https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png"" alt=""this image illustrate how calcule these matrices "" />
<strong>method 2:</strong>
I find this method in YouTube.
<a href=""https://i.sstatic.net/HEZHv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HEZHv.png"" alt=""enter image description here"" /></a>
<strong>method 3:</strong>
in this method it just for headi we have:
Qi=xWqi
Ki=xWki
Vi=xWvi
so I don't know which method is correct
there is the links of my references
<a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">The Illustrated Transformer</a>
<a href=""https://youtu.be/bCz4OMemCcA?si=4cJa0YojjkPgzcs0"" rel=""nofollow noreferrer"">YouTube video</a></p>
","transformer"
"45301","Problems with understanding instruction fine-tuning","2024-03-31 04:55:27","","0","22","<natural-language-processing><transformer><large-language-models><gpt>","<p>I'm trying to read up on instruction fine-tuning, but I think I have a big misunderstanding.</p>
<p>As I understand, instruction datasets typically have 3 components: (a) the instruction (b) the output/response, and (c) and an optional input. Now, according to this <a href=""https://arxiv.org/pdf/2308.10792.pdf"" rel=""nofollow noreferrer"">paper</a>: &quot;Based on the collected IT dataset, a pretrained model can be directly fine-tuned in a fully-supervised manner, where given the instruction and the input, the model is trained by predicting each token in the output sequentially.&quot; This makes sense to me, i.e, the response/output is the ground truth the model is expected to predict.</p>
<p>However, when I check many tutorials (e.g., this <a href=""https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/instruction-tune-llama-2-int4.ipynb"" rel=""nofollow noreferrer"">tutorial notebook</a>), it seems that instructions, inputs, and outputs are all combined into a single text sample. But I can't tell from the notebook how the training now works. What is now the ground truth for the supervised training. Or is this now treated as a next-word-prediction task?</p>
<p>What am I missing? Or are these indeed two different approaches for instruction tuning. Sorry if those a stupid questions!</p>
","transformer"
"45255","Why doesn't my toy transformer model ""grok""?","2024-03-26 21:20:39","","0","41","<deep-learning><python><transformer><gpt>","<p>I'm working on reproducing the results by Neel Nanda on teaching a small transformer to perform modular addition:
<code>(operand_1+operand_2)%mod_value</code>.</p>
<p>The expectation for this demo is for the train loss to quickly decrease while the test loss remains high due to memorization. Then the model &quot;groks&quot;, meaning learns the generalized solution, and slowly transitions from memorization to this solution.</p>
<p>As a result, the test loss plummets much farther into training
I'm implementing the transformer from scratch with TinyGrad. I prefer TinyGrad since I understand it a lot better than PyTorch(debugging is easier) and it offers better METAL accelerator support. This model is simpler than the typical transformer -- it doesn't have biases or normalization, and we train over the entire training set in a single batch. The optimizer is AdamW with a very high weight decay to encourage faster generalization.</p>
<p>As far as I can tell, I've properly matched the parameters stated in the original material. I assume it's a bug with my Transformer code -- this implementation was based off <a href=""https://github.com/tinygrad/tinygrad/blob/2befdf86d9f992f5d4f081122088b39688b153d0/extra/models/transformer.py#L41"" rel=""nofollow noreferrer"">this</a>, but I removed the unnecessary components. It handles positional embeddings differently, so this may be the cause.</p>
<p><a href=""https://i.sstatic.net/XrE6i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XrE6i.png"" alt=""training loss"" /></a></p>
<p><a href=""https://i.sstatic.net/wHyZw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wHyZw.png"" alt=""test loss"" /></a></p>
<p>While the train loss should immediately decrease to close to 0 after just a few hundred epochs, the model is unable to get a better score than about 1.5 for both the training and test loss. I'll paste plots for the train and test loss over 5000 epochs below.</p>
<p>My code:</p>
<pre><code>from tinygrad import Tensor
from tinygrad.nn import LayerNorm
from tinygrad.nn.state import get_parameters
from tinygrad.nn.optim import AdamW
from tinygrad.helpers import CI
from tinygrad import TinyJit
import random
import numpy as np
from tqdm import trange

mod = 113

operand_1 = Tensor.arange(113).unsqueeze(0).repeat([113,1]).flatten(0)
operand_2 = Tensor.arange(113).unsqueeze(0).repeat([113,1]).T.flatten(0)
equals = Tensor.full_like(operand_1, 113)

dataset = Tensor.stack([operand_1, operand_2, equals], dim=1)
targets = Tensor((dataset[:,0].numpy() + dataset[:,1].numpy()) % mod).unsqueeze(1)

train_split = .3
cutoff = int((mod**2)*train_split)
indices = np.random.permutation(mod**2)
train_indices = Tensor(indices[:cutoff])
test_indices = Tensor(indices[cutoff:])


train_ds = dataset[train_indices]
train_targets = targets[train_indices]
test_ds = dataset[test_indices]
test_targets = targets[test_indices]

class MultiHeadAttention:
    def __init__(self, num_heads, embed_dim):
        self.num_heads, self.embed_dim = num_heads, embed_dim
        self.head_dim = self.embed_dim // self.num_heads

        self.query = (
            Tensor.scaled_uniform((self.embed_dim, self.embed_dim)),
            Tensor.zeros(self.embed_dim),
        )
        self.key = (
            Tensor.scaled_uniform((self.embed_dim, self.embed_dim)),
            Tensor.zeros(self.embed_dim),
        )
        self.value = (
            Tensor.scaled_uniform((self.embed_dim, self.embed_dim)),
            Tensor.zeros(self.embed_dim),
        )

        self.out = (
            Tensor.scaled_uniform(self.embed_dim, self.embed_dim),
            Tensor.zeros(self.embed_dim),
        )

    def __call__(self, x, mask=None):
        bz = x.shape[0]
        QKV = [
            x.linear(weight, bias).reshape(
                bz, self.num_heads, -1, self.head_dim
            )
            for weight, bias in [self.query, self.key, self.value]
        ]

        A = (QKV[0] @ QKV[1].transpose(-2, -1) / self.head_dim**0.5).softmax(
            -1
        )
        # A += mask if mask else 0

        out = (
            (A @ QKV[2])
            .transpose(1, 2)
            .reshape(bz, -1, self.embed_dim)
            .linear(*self.out)
        )

        return out


class TransformerBlock:
    def __init__(self, num_heads, embed_dim, ffn_dim):
        # no bias or layernorm!
        self.attention = MultiHeadAttention(num_heads, embed_dim)
        self.ffn1 = (
            Tensor.scaled_uniform((embed_dim, ffn_dim)),
        )
        self.ffn2 = (
            Tensor.scaled_uniform((ffn_dim, embed_dim)),
        )

    def __call__(self, x, mask=None):
        x = x + self.attention(x, mask)
        x = x + x.linear(*self.ffn1).relu().linear(*self.ffn2)
        return x


class Transformer:
    def __init__(
        self,
        vocab_size,
        context_length,
        num_layers,
        embed_dim,
        ffn_dim,
        num_heads,
    ):
        self.vocab_size = vocab_size
        self.context_length = context_length
        self.embed_dim = embed_dim
        self.ffn_dim = ffn_dim
        self.num_heads = num_heads

        self.embed = Tensor.scaled_uniform(
            (context_length + vocab_size, embed_dim)
        )
        self.blocks = [
            TransformerBlock(num_heads, embed_dim, ffn_dim)
            for _ in range(num_layers)
        ]
        self.unembed = Tensor.scaled_uniform((embed_dim, vocab_size))

    def __call__(self, x):
        B, T = x.shape
        assert (
            T &lt;= self.context_length
        ), &quot;Input shape must be (batch, context_length)&quot;

        positional_embeddings = Tensor.eye(T).unsqueeze(0).expand([B, T, T])

        x = x.one_hot(self.vocab_size)
        x = positional_embeddings.cat(x, dim=2).flatten(end_dim=1)
        x = (x @ self.embed).reshape(B, T, self.embed_dim)
        x = x.sequential(self.blocks)
        x = (x.reshape((-1, self.embed_dim)) @ self.unembed).log_softmax()
        return x.reshape((B, T, self.vocab_size))

def loss_fn(logits : Tensor, labels):
    log_probs = logits.log_softmax(axis=-1)
    correct = log_probs.gather(idx=labels, dim=-1)[:,0]
    return -correct.mean()

def train(
    model,
    X_train,
    Y_train,
    X_test,
    Y_test,
    optim,
    steps=1, # Only one step is needed for full batch training
    lossfn=lambda out, y: out.sparse_categorical_crossentropy(y),
    allow_jit=True,
):
    def train_step(x, y):
        # network
        out = model(x)[:,-1]
        loss = lossfn(out, y)
        optim.zero_grad()
        loss.backward()
        optim.step()
        return loss.realize()

    def test_step(x, y):
        out = model(x)[:,-1]
        loss = lossfn(out, y)
        optim.zero_grad()
        return loss.realize()

    if allow_jit:
        train_step = TinyJit(train_step)

    with Tensor.train():
        train_losses = []
        test_losses = []
        for i in (t := trange(steps, disable=CI)):
            train_loss = train_step(X_train, Y_train).numpy()
            test_loss = test_step(X_test, Y_test).numpy()
            train_losses.append(train_loss)
            test_losses.append(test_loss)
            t.set_description(&quot;train loss: %.2f test loss: %.2f&quot; % (train_loss, test_loss))
    return [train_losses, test_losses] 

model = Transformer(114,3,1,128,512,4)
optimizer = AdamW(get_parameters(model), lr=.001, b1=.9, b2=.98, wd=1)
train_losses, test_losses = train(model, train_ds, train_targets, test_ds, test_targets, optimizer, 5000, lossfn=loss_fn)
</code></pre>
","transformer"
"45215","How to teach Gemma model my mother tongue (Kannada - one of the oldest Indic languages)","2024-03-23 19:54:01","","0","82","<transformer><large-language-models><fine-tuning>","<p>I'm interested in teaching the Gemma 2B model my mother tongue (<a href=""https://en.wikipedia.org/wiki/Kannada"" rel=""nofollow noreferrer"">Kannada</a> - one of the oldest Indic languages). The pre-trained model doesn't work well with the mentioned language, so I thought of teaching the model but I'm very new to this field of training and fine-tuning LLMs. But based on some research I've planned the below steps:</p>
<ol>
<li><p>Prepare the Dataset - luckily I found <a href=""https://huggingface.co/datasets/Cognitive-Lab/Kannada-Instruct-dataset"" rel=""nofollow noreferrer"">some dataset</a> on HF.</p>
</li>
<li><p>Train the Tokenizer for the language and enrich the vocabulary of the Gemma model with the newly obtained tokens from the dataset using a newly trained Tokenizer.</p>
</li>
<li><p>Here is where I'm confused</p>
<p>a. Should I simply fine-tune the model? - maybe using LoRA or QLoRA kind of techniques.</p>
<p>b. I read about a concept called <a href=""https://arxiv.org/abs/2302.03241"" rel=""nofollow noreferrer"">Continually Pre Train</a>, should I give this a try?</p>
</li>
</ol>
<p>3 (b) looks promising to me but I'm not able to find much explanations and code examples on the internet about how to achieve this.</p>
<p><strong>Note:</strong> I'm looking for a very basic model and performing this for learning purposes and I should be able to do this on my Google Colab Notebook.</p>
<p>I'm seeking some guidance here on how to proceed and some code examples for achieving this.</p>
","transformer"
"45188","Correctly applying softmax in self attention layer","2024-03-21 22:48:48","","0","58","<transformer><softmax>","<p>I'm trying to understand how to apply softmax in self attention layer.
Let's say we have Query and Key matrix where the last row is for Paddings</p>
<p><a href=""https://i.sstatic.net/zvQCV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zvQCV.png"" alt=""enter image description here"" /></a></p>
<p>In this case Z = Q*K_t would be something like this:</p>
<p><a href=""https://i.sstatic.net/Nbd6i.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Nbd6i.png"" alt=""enter image description here"" /></a></p>
<p>When we add the Mask we have:</p>
<p><a href=""https://i.sstatic.net/Z1LuL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Z1LuL.png"" alt=""enter image description here"" /></a></p>
<p>Now the problem. When I apply the softmax function to each row, I get that the last one is not going to be populated with 0s. By defining S = softmax(Z) what we get is:</p>
<p><a href=""https://i.sstatic.net/MhycC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/MhycC.png"" alt=""enter image description here"" /></a></p>
<p>To get the output Y of the self attention layer, we have to multiply S by V (Values matrix):</p>
<p><a href=""https://i.sstatic.net/haU5Y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/haU5Y.png"" alt=""enter image description here"" /></a></p>
<p>And as a result, we have that y41 and y42 are both functions of Padding values.
How to get rid of this problem?</p>
","transformer"
"45187","Is there any standardized notation for drawing neural network diagrams?","2024-03-21 20:51:40","","1","45","<neural-networks><convolutional-neural-networks><transformer>","<p>Is there any standardized notation for drawing neural network diagrams? For example, for circuits there is a universal set of symbols used to draw different types of circuits why not for neural networks?</p>
","transformer"
"45110","Dimension of the embedding matrix in a transformer during inference","2024-03-13 11:46:36","","0","24","<transformer><inference>","<p>I have a bit of confusion understanding the dimensions of the input embedding matrix at inference time in transformers. Some sources say that you start with an  token and you fill with  tokens up to some max length parameter (which max length and how do you decide that???), some other sources say you fill with  tokens until again some max length. Which is the right one? Why wouldn't make sense to make it dynamic? (1xn_embeddings at first step, 2xn_embeddings at second step and so on...)</p>
<p>And let's say the PAD way is the right one, at the end of the decoding step we would have a matrix of dimensions (max_length x n_embeddings). To compute the next token, do we consider only the row of the output matrix related to the last predicted token, and argmax over it?</p>
<p>Thanks in advance for your answers, I hope the questions are clear!</p>
","transformer"
"45098","Why different noise in GAN generate different images?","2024-03-12 18:45:12","45100","4","1344","<machine-learning><convolutional-neural-networks><natural-language-processing><computer-vision><transformer>","<p>I understand that noise <span class=""math-container"">$z$</span> serves as the input to the generator. Noise <span class=""math-container"">$z$</span> is essentially a vector of random numbers, typically from Gaussian distribution with chosen size of like <span class=""math-container"">$100$</span>. However, I don't understand how different noise can produce different images.</p>
<p>Sorry if this sounds like stupid question, but I've been experimenting with GANs and noticed that if I keep the noise constant, the generated image remains the same, seems like the numbers in the noise vector are linked to generating specific image, but I haven't come across an explanation for this online.</p>
","transformer"
"45079","How do transformer-based architectures generate contextual embeddings?","2024-03-10 19:17:11","","1","83","<machine-learning><deep-learning><convolutional-neural-networks><natural-language-processing><transformer>","<p>How do transformer-based architectures like Roberta generate contextual embeddings? The articles I've read keep saying that transformer encoders work bidirectionally. Because of self-attention, they can look at every token, unlike RNN/LSTM, which can only process the previous hidden state. I'm not sure how the Transformer accomplishes that.</p>
","transformer"
"45062","Fine tuning or just feature extraction or both using Roberta?","2024-03-08 19:54:47","45063","1","96","<neural-networks><machine-learning><deep-learning><natural-language-processing><transformer>","<p>I'm reading a program that use the pre-trained Roberta model (roberta-base). The code first extracts word embeddings from each caption in the batch, using the last hidden state of the Roberta model. Then, the model is trained to align these word embeddings with the image features (pixels) of the image through a type of attention mechanism. Then the models are updated using attention loss function. This iterative process continues until the training is complete, so I guess the word embeddings will be different after each epoch ? This is a multi-modal problem.</p>
<p>When I compare the Roberta model after training with the pre-trained model (roberta-base), I notice that every parameters the trained Roberta model are different, seems like the new model has updated the parameters. I'm not sure whether this is a form of fine-tuning or just feature extraction or both ?</p>
","transformer"
"45005","How do I code so that the embedding output and input share the same weight matrices?","2024-03-04 00:22:41","45012","0","141","<transformer><pytorch><attention><large-language-models><embeddings>","<p>I am trying to implement the <em>Attention is All You Need</em> paper from scratch. The authors mentioned in section 3.4 that <em>&quot;In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]&quot;</em> [30] here is the paper <em>Using the output embedding to improve language models</em> by Ofir Press and Lior Wolf.</p>
<p>Excuse me but</p>
<ol>
<li><p>could you explain why we need output embedding between encoder stack and the decoder stack? Isn't the output of encoder d_model dimension vectors (d_model = 512 in the original paper) so they don't need embedding layer in-between? Can I treat this as a linear layer?</p>
</li>
<li><p>could you explain how the weights can be shared between the pre-softmax linear layer and the input embedding? Since in the original paper, it was about the translation between German and English, shouldn't the weight matrices be different for pre-softmax linear layer and the input embedding since the number of vocabularies for two languages aren't the same?</p>
</li>
</ol>
<p>Thank you for reading my question!</p>
","transformer"
"44984","AI chat bot that answers by focusing only on 30 textbooks","2024-03-02 14:42:39","","1","87","<training><transformer><chat-bots><transfer-learning><fine-tuning>","<p>I don't even know what I'm looking for and what's the terminology, so here I am asking this question.</p>
<h1>Background</h1>
<p>Assume I have 30 textbooks. I want to have an AI chatbot like ChatGPT which answers the questions that I ask from those 30 textbooks. I mean I don't want to ask questions from outside those 30 textbooks. Moreover, I don't expect the AI chatbot to answer based on anything other than those 30 textbooks. However, I want the AI chatbot to be as intelligent as ChatGPT or other popular ones.</p>
<p>Maybe I want the AI chatbot to limit their attention to those 30 textbooks while answering my questions. Right? I'm not sure.</p>
<h1>Question</h1>
<p>What keywords should I look for? What free or open-source models can I use? What's the procedure to create my own AI chatbot for those 30 textbooks? Am I missing anything else regarding the tools to achieve my objectives? I'd appreciate any help or hints. Thanks.</p>
","transformer"
"44941","My small BERT can't even overfit on a sentiment analysis task","2024-02-27 16:27:57","44942","0","41","<transformer><sentiment-analysis><huggingface>","<p>I'm trying to train (from scratch) a miniature BERT model on SST2, a simple binary sentiment analysis task with inputs of maybe 5-20 words at a time. As you can see in my code, my approach is a little non-standard in a few ways:</p>
<ol>
<li>The model is quite small, 4 attention heads of 64 dimensions each and only four transformer layers.</li>
<li>I'm not using a tokenizer, i.e. I'm just using unicode code points as tokens, and restricting the vocabulary to just the characters in this dataset (so my vocabulary is about 50 tokens).</li>
</ol>
<p>Nevertheless, I'm surprised that my model can't even seem to overfit on the <em>training</em> set. After something like 10 epochs, I see no drop in training loss at all. Should I just train longer? Am I doing something fundamentally wrong?</p>
<pre><code>import datasets
from tokenizers.normalizers import BertNormalizer
from transformers.models.bert import BertForSequenceClassification, BertConfig

import torch
from torch.nn.functional import cross_entropy
from torch.optim import Adam
from torch.utils.data import DataLoader

def normalize(sentence):
    normalized = normalizer.normalize_str(sentence)
    normalized = normalized.replace(&quot;æ&quot;, &quot;ae&quot;)
    return normalized

sst2 = datasets.load_dataset(&quot;sst2&quot;)

normalizer = BertNormalizer(
    clean_text=False,
    handle_chinese_chars=False,
    strip_accents=True,
    lowercase=True
)

train_text = sst2[&quot;train&quot;][&quot;sentence&quot;]
test_text  = sst2[&quot;test&quot;][&quot;sentence&quot;]
characters = set()
max_sentence_length = 0
for sentence in [*train_text, *test_text]:
    normalized = normalize(sentence)
    characters.update(normalized)
    max_sentence_length = max(max_sentence_length, len(normalized))

character_to_id = {}
for index, character in enumerate(characters):
    character_to_id[character] = index

n_vocabulary = len(characters)
n_heads = 4
dimension = 64
n_layers = 4

config = BertConfig(
    vocab_size=n_vocabulary,
    num_hidden_layers=n_layers,
    num_attention_heads=n_heads,
    hidden_size=n_heads * dimension,
    intermediate_size=256
)
model = BertForSequenceClassification(config).to(&quot;cuda&quot;)

n_train = len(train_text)
token_ids = torch.empty((n_train, max_sentence_length), dtype=torch.int32)
attention_mask = torch.zeros((n_train, max_sentence_length))
for i in range(len(train_text)):
    normalized = normalize(train_text[i])
    for j, c in enumerate(normalized):
        if j &gt;= max_sentence_length:
            token_ids = torch.cat((token_ids, torch.empty(n_train, 1)), dim=1)
            attention_mask = torch.cat((attention_mask, torch.zeros(n_train, 1)), dim=1)
            max_sentence_length += 1
        token_ids[i, j] = character_to_id[c]
    attention_mask[i, :j + 1] = 1

train_x = token_ids.to(&quot;cuda&quot;)
train_y = torch.tensor(sst2[&quot;train&quot;][&quot;label&quot;]).to(&quot;cuda&quot;)
train_mask = attention_mask.to(&quot;cuda&quot;)

batch_size = 128
loader = DataLoader(
    list(zip(train_x, train_y, train_mask)),
    batch_size=batch_size
)

optimizer = Adam(model.parameters())
initial_loss = None
while True:
    for batch_x, batch_y, batch_mask in loader:
        outputs = model(input_ids=batch_x, attention_mask=batch_mask)
        loss = cross_entropy(outputs.logits, batch_y)
        if initial_loss is None:
            initial_loss = float(loss)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()    
        print(f&quot;{float(loss):.4} ({100*float(loss)/initial_loss:.2}%)     &quot;, end=&quot;\r&quot;)
    print(&quot;epoch&quot;)
<span class=""math-container"">```</span>
</code></pre>
","transformer"
"44933","Why do Transformer decoders use masked self attention when producing new tokens?","2024-02-26 23:51:31","","1","261","<natural-language-processing><transformer><attention><gpt><encoder-decoder>","<p>I've been reading that transformer decoders use masked self attention so that the decoder can't cheat by looking ahead. For example, when predicting the 6th token in the sequence we shouldn't have access to the 7th token.</p>
<p>However, why can't the decoder perform full self attention on all previously predicted tokens? When predicting the 6th token, why can't the third token embedding have access to the 5th token. Wouldn't this system of representation offer richer context. Some explanations that I have seen online have stated that this system would violate the nature of autoregressive token generation, however we still aren't looking at the 7th token or anything after to predict the sixth token, we are just allowing all the already predicted tokens to attend to each other. The presence of every single token in a generated sequence is only the result of everything that came before it which still sounds very autoregressive.</p>
<p>In this previous post:
<a href=""https://ai.stackexchange.com/questions/40917/what-if-we-drop-the-causal-mask-in-auto-regressive-transformer"">What if we drop the causal mask in auto-regressive Transformer?</a></p>
<p>The answer mentions:
<strong>Allowing available tokens to attend to each other would violate the autoregressive property and potentially introduce information leakage from future tokens, leading to incorrect predictions.</strong></p>
<p>I'm not sure what this really means or where exactly the information leakage would be coming from, since the 6th token would have no information about the 7th. I know that doing self attention like this increases the complexity, however is there any actual accuracy or quality reasons why we don't do this.</p>
","transformer"
"44930","How can Transformers handle random sequences?","2024-02-26 15:11:40","44931","4","795","<transformer><chatgpt>","<p>I have asked ChatGPT the following:</p>
<blockquote>
<p>Can you concatenate jfef9230rj2mreg90r23ewfrn02eqwdk and
32ir20r3i2ofg90r32kee?</p>
</blockquote>
<p>And without any error the model produces:</p>
<blockquote>
<p>jfef9230rj2mreg90r23ewfrn02eqwdk32ir20r3i2ofg90r32kee</p>
</blockquote>
<p>My question:</p>
<p>Does ChatGPT rawly tokenizes random sequences, or do they do some preprocessing and replace these sequences by new Tokens like:</p>
<blockquote>
<p>Can you concatenate SEQUENCE1 and SEQUENCE2?</p>
</blockquote>
<p>where SEQUENCE1 ... SEQUENCE_N is in the vocabulary, and before the Cross-Entropy on the prediction (which is hopefully SEQUENCE1||SEQUENCE2) they resubstitute back the sequences.</p>
<p>I think the copying is quite straightforward, shouldn't a Transformer be able to learn how to copypaste/move around long sequences of bytes without any preprocessing techniques?</p>
<p>In my project I have done good amount of training and it only manages to move around only little segments of the sequence, and I was thinking if this is even the right approach to rawly Tokenize the sequences, or if I just had too little training.</p>
","transformer"
"43903","Would AlphaZero perform better if made with transformers?","2024-02-24 01:20:29","","1","264","<reinforcement-learning><convolutional-neural-networks><transformer><alphazero><alphago>","<p>AlphaZero utilized a residual convolutional neural network to estimate move policy and position value.  If it was rebuilt today, would it be more efficient and powerful if they used a transformer architecture instead?  Assume the same amount of compute power and training time is available, and purely self-play, aiming for as fair a comparison as possible.</p>
","transformer"
"43896","How to structure encoder and decoder input sequences when building transformers model from scratch","2024-02-23 16:53:50","","0","20","<transformer><pytorch><inference>","<p>I built a transformers model from scratch in PyTorch.  I trained it on a novel in the public domain.  My sequences are 30 tokens and the first encoder and decoder sequences, for example, are tokens 0-30 and 1-31, respectively.</p>
<p>In this way, my inputs do not have any padding tokens at all (I drop the last sequence that is incomplete).</p>
<p>For inference, I encode an input sentence with padding for the encoder input, whereas the decoder input is just a BOS token with padding.</p>
<p>My question is, is this the proper way to set up the sequences?  Seems that the way I have it set up is problematic -  the model never sees any padding tokens in the inputs, which leads to problems when it encounters them during inference.</p>
<p>I suppose one alternative would be to assign a sentence (with padding as necessary) to each sequence.  But then what would the decoder input contain?  Also in general this is much more data processing than the current approach.</p>
<p>For reference, the reproducible code is on this SO thread (although the question there is a slightly different one):
<a href=""https://stackoverflow.com/questions/77984828/generate-prediction-sequence-with-transformers-model-built-from-scratch"">https://stackoverflow.com/questions/77984828/generate-prediction-sequence-with-transformers-model-built-from-scratch</a></p>
","transformer"
"43846","Gradually increasing CPU load on using sentence embeddings model with kmeans","2024-02-20 13:24:01","","0","46","<neural-networks><machine-learning><transformer>","<p>I am having a ML based production application, using flask, deployed on GCP server using gunicorn workers. In each incoming request, a text sentence is received.</p>
<p>It is using sentence transformers (All-MiniLM-L6-v2 model), which is loaded globally one time, to create embeddings of the incoming text and then use pre trained kmeans (also loaded globally) to predict/map it to a intent cluster. Basically, goal is to find intent of the sentence.</p>
<p>I have ample resources and the requests are also constant in number and texts are also similar, but still each day the CPU load is gradually increasing. Avg response time on 1st day was around 200 ms average, after 10 days now it is 400 ms.</p>
<p>I have tried deleting the embedding variable using 'del' command in the code itself, also forcing python garbage collector using 'gc.collect()' in a thread which executes after the main process execution is completed, but still the issue is coming.</p>
<p>One thing I have noticed is that if I dont use del and gc.collect(), the RAM starts to go down gradually. With both these, RAM is constant but now CPU usage is gradually going up day by day, hence the load and response time.</p>
<p>I have spent weeks on this issue trying to debug it but have got no solution, any help would be appreciated.</p>
","transformer"
"43837","Are video generation also great at next frame video prediction?","2024-02-19 17:34:14","","0","42","<transformer><diffusion-models><next-frame-video-prediction><video-prediction><video-generation>","<p>If I have a good video generation model like OpenAI's new Sora, will it be capable of doing just as well at next frame video prediction?</p>
","transformer"
"43820","Does transformers' self-attention mechanism process tokens independently, or entire sequence at a time?","2024-02-18 02:51:04","44938","1","124","<neural-networks><tensorflow><transformer><attention>","<p>About attention: the Query, Key and Value vectors (before the linear transformations) are just the entire sequence, that is being inputted, or just each token? Chat-GPT nor Youtube didn't give me a clear answer. But, I thought. If we feed in each sequence straight into the Attention mechanism, then the linear layers, which are supposed to transform these inputs, won't be able to accept that, because sequences can be different length, and linear layers' input shape is fixed. And if we process each token independently, we have to store the other tokens somewhere, and then have loops to iterate through each token etc.</p>
<p>So, I decided to find some code, where people create that Multi-Head attention, and here's what I found:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers


class MultiHeadAttention(layers.Layer):
    def __init__(self, model_dim, n_heads, rate=0.1, initializer='glorot_uniform'):
        super(MultiHeadAttention, self).__init__()
        self.n_heads = n_heads
        self.model_dim = model_dim

        assert model_dim % self.n_heads == 0

        self.head_dim = model_dim // self.n_heads

        self.wq = layers.Dense(model_dim, kernel_initializer=initializer)
        self.wk = layers.Dense(model_dim, kernel_initializer=initializer)
        self.wv = layers.Dense(model_dim, kernel_initializer=initializer)
        
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)
        
        self.wo = layers.Dense(model_dim, kernel_initializer=initializer)

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.n_heads, self.head_dim))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, q, k, v, mask=None):
        batch_size = tf.shape(q)[0]

        q = self.wq(q)  
        k = self.wk(k)  
        v = self.wv(v)  

        q = self.split_heads(q, batch_size) 
        k = self.split_heads(k, batch_size)  
        v = self.split_heads(v, batch_size) 

        dh = tf.cast(self.head_dim, tf.float32)
        qk = tf.matmul(q, k, transpose_b=True)
        scaled_qk =  qk / tf.math.sqrt(dh)
        
        if mask is not None:
            scaled_qk += (mask * -1e9) 

        attn = self.dropout1(tf.nn.softmax(scaled_qk, axis=-1))
        attn = tf.matmul(attn, v) 

        attn = tf.transpose(attn, perm=[0, 2, 1, 3]) 
        original_size_attention = tf.reshape(attn, (batch_size, -1, self.model_dim)) 

        output = self.dropout2(self.wo(original_size_attention))
        return output
class TransformerBlock(layers.Layer):
    def __init__(self, emb_dim, n_heads, mlp_dim, 
                 rate=0.1, initializer='glorot_uniform', eps=1e-6, activation='gelu'):
        super(TransformerBlock, self).__init__()
        self.attn = MultiHeadAttention(emb_dim, n_heads, initializer=initializer)
        self.mlp = tf.keras.Sequential([
            layers.Dense(mlp_dim, activation=activation, kernel_initializer=initializer), 
            layers.Dense(emb_dim, kernel_initializer=initializer),
            layers.Dropout(rate)
        ])
        self.ln1 = layers.LayerNormalization(epsilon=eps)
        self.ln2 = layers.LayerNormalization(epsilon=eps)

    def call(self, inputs, mask=None):
        x = self.ln1(inputs)
        x = inputs + self.attn(x, x, x, mask) 
        x = x + self.mlp(self.ln2(x))
        return x
</code></pre>
<p>So, here I see that they basically feed in the entire sequence into the Multi-Head attention:</p>
<p>x = self.ln1(inputs)
x = inputs + self.attn(x, x, x, mask)</p>
<p>But again, sequences can be different length, but linear layers accept inputs of fixed length. Is it me not understanding something? I also read about Padding and Masking, that is used during training. Is it also used during inference?</p>
","transformer"
"43734","Eval loss when fine-tuning in an unsupervised way/pretraining?","2024-02-09 15:55:19","","0","63","<transformer><fine-tuning><validation-loss>","<p>I'm fine-tuning the base Mixtral 8x7B model (4-bit quantized) with Lora on my own data, following these guidelines: <a href=""https://www.stochastic.ai/blog/xfinance-vs-bloomberg-gpt"" rel=""nofollow noreferrer"">https://www.stochastic.ai/blog/xfinance-vs-bloomberg-gpt</a></p>
<p>I'm first fine-tuning it in a self-supervised way, and then using the <code>SFTTrainer</code> on some conversational data as a final step. I'm not a deep learning engineer so I have no idea what I'm doing really, but I tried a few different hyperparameters at first to see which got the train loss down quickest.</p>
<p>I then chose the best out of them and trained for about 8 epochs over my text data (5MB of text data). train loss was still quite high (0.6), the eval loss kept increasing. However, I assume the eval loss at this stage with my setup is irrelevant for this NSP self-supervised stage of training.</p>
<p>Then finally, I used the <code>SFTTrainer</code> on conversational data for like 100 epochs (only 100 Q&amp;A pairs, but high-quality and some with extensive answers of about 4k tokens). The train loss got extremely low but the eval loss kept rising. Weirdly I got pretty good results when testing the final model.</p>
<p>My first question is how do I know when to stop the first stage (self-supervised) of training, do I need to create some advanced validation loss function to see if the model is accurately learning patterns in the unstructured text data? Secondly, why would my eval loss in the second stage of training keep rising even though with each checkpoint, the performance seems to be getting better when I tried the model out after training?</p>
","transformer"
"43706","Getting started with training local LLM using python","2024-02-06 08:05:12","","0","790","<training><transformer><large-language-models><gpt>","<p>As I'm completely new to this field, I find it hard to get started given the requirements I have. I'm a bit overwhelmed by all the models and options that are available. Even though it wasn't difficult to run an LLM locally:</p>
<pre><code><span class=""math-container"">$&gt; brew install llm
$</span>&gt; llm -m gpt4all-13b-snoozy-q4_0  &quot;Tell me a joke&quot;
</code></pre>
<p>despite being very nice, it is not what I'm looking for. I want to train a model with my own data. Also, I want to build a website around it, so it means I can access the LLM with APIs.</p>
<p>So, as you can see I have some requirements. What would be a great place to start reading? A good tutorial to get started would be very much appreciated!</p>
","transformer"
"43630","Why feed forward neural network (FFN) in transformer block has a ""contract and expand"" pattern?","2024-01-31 18:02:31","","1","332","<transformer><pytorch>","<p>I noticed that in many (every ?) transformer architecture, the FFN (i.e the MLP network at the end of one transformer block) consists of two linear layers (with an activation) where the first layer goes from D1 channels to D2 channels and the second layer goes back from D2 to D1.</p>
<p>For instance we can see it in this FFN class in DistilBert :
<a href=""https://github.com/huggingface/transformers/blob/345b9b1a6a308a1fa6559251eb33ead2211240ac/src/transformers/models/distilbert/modeling_distilbert.py#L455"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/345b9b1a6a308a1fa6559251eb33ead2211240ac/src/transformers/models/distilbert/modeling_distilbert.py#L455</a></p>
<p>What justifies this &quot;expand and contract&quot; pattern ? Why not just using two identical linear layers or anything else ?</p>
","transformer"
"43624","What does Figure 3 in the BERT paper represent?","2024-01-31 10:44:33","","2","50","<deep-learning><transformer><bert>","<p>The <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">BERT paper</a> has the following diagram (Figure 3):</p>
<p><a href=""https://i.sstatic.net/UA8hj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UA8hj.png"" alt=""enter image description here"" /></a></p>
<p>It's captioned &quot;Differences in pre-training model architectures&quot;.
However, I thought the BERT architecture was just a stack of attention blocks like this (from <a href=""https://huggingface.co/blog/bert-101"" rel=""nofollow noreferrer"">huggingface</a>):</p>
<p><a href=""https://i.sstatic.net/LZja3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LZja3.png"" alt=""enter image description here"" /></a></p>
<p>My question: <strong>What is the BERT diagram supposed to mean?</strong></p>
<p>My best guess is that it represents the process of predicting an output sequence from a given input sequence. In other words, in the GPT model, one first samples with a sequence length of 1 to get next token T1, then we embed T1 via E2 and sample with a sequence of length 2 to get T2, and so on.</p>
<p>In BERT, I assume we mask all input tokens but the first, then sample the whole thing.</p>
<p>The problem is that I don't understand what the <code>Trm</code> blocks mean, or how this relates to the BERT architecture.</p>
","transformer"
"43589","How come all the multi-headed self-attention layers don't end up learning the same aspect of a natural language?","2024-01-27 21:09:43","","1","41","<deep-learning><transformer><attention>","<p>How come all the multi-headed self-attention layers don't end up learning the same aspect of a natural language? Since we don't dictate ahead of time what the self-attention layers focus on, how do we ensure they don't &quot;converge&quot;?</p>
<p>(My question is partly influenced by the notion of kernels in computer vision. Those kernels also focus on different aspects of the image, but they don't end up learning the same thing since they're specified ahead of time.)</p>
","transformer"
"43576","Handling Variable Output Token Dimensionality in Transformer Decoders During Inference","2024-01-26 14:56:46","","0","54","<neural-networks><transformer><attention>","<p>I'm curious about something in the decoder part of the Transformers architecture. From what I understand, the Keys and Values come from the output of the encoder part of the Transformer. I understand that we have as many queries, keys and values as we have input tokens (actually if it is multiheaded attention we may have more (i.e. 10 heads means 10 queries for each word)).</p>
<p>In the decoder part, we basically match (by doing a scalar product) a query that the output &quot;asks&quot; to those keys and values to know which part of the input the decoder should look at.</p>
<p>However, if I'm not mistaken, there are as many queries as there are previously outputted output tokens. Therefore as many outputs of the multi-head attention as there are previously outputted output tokens.</p>
<p>How / when does this dimensionality that depends on the current amount of output tokens disappear? It has to disappear since &quot;at the end&quot; there is a standard feed-forward network which size is fixed* and cannot depend on the number of output tokens.
I'm talking about inference when one token at a time is generated but the transformer still inputs to the decoder <strong>all</strong> the previous generated output tokens. I'm not sure about that last part especially since it says &quot;Outputs (shifted right)&quot; in the paper. Maybe it only inputs a fixed number of previously outputted tokens? But that feels wrong.</p>
<p>* or maybe each of those outputs is fed through the last feed forward network but then how is a word determined with those <span class=""math-container"">$n_{previously\_outputted\_tokens}$</span> outputs of this last feed forward network?</p>
","transformer"
"43565","Can positional encodings in transformers be added","2024-01-25 23:27:52","","0","35","<transformer><backpropagation>","<p>Here's a basic GPT2 implementation:</p>
<pre><code>class GPT(nn.Module):
    def __init__(self, vocab_size, seq_len, model_dim, n_heads, n_layers):
        super().__init__()
        self.seq_len = seq_len
        self.wte = nn.Embedding(vocab_size, model_dim)
        self.wpe = nn.Embedding(seq_len, model_dim)
        self.h = nn.Sequential(
            *[DecodeLayer(model_dim, n_heads) for _ in range(n_layers)]
        )
        self.ln_f = nn.LayerNorm(model_dim)

    def forward(self, x):
        embeds = self.wte(x) + self.wpe(torch.arange(0, self.seq_len).to(x.device)) # [seq_len, model_dim]
        embeds = self.h(embeds)
        embeds = self.ln_f(embeds)
        # (seq_len, model_dim)  x (model_dim, vocab_size) =&gt; seq_len vocab_size
        return embeds @ self.wte.weight.T
</code></pre>
<p>The positional embeddings are added to the input with:</p>
<pre><code>self.wpe(torch.arange(0, self.seq_len).to(x.device))
</code></pre>
<p>I wonder, if it would be possible to just do: <code>self.wpe = nn.Parameter(seq_len, model_dim)</code>, and change the line in the forward pass to:</p>
<pre><code>embeds = self.wte(x) + self.wpe
</code></pre>
","transformer"
"43560","Where U-Net and Convolutional layers are settled in Stable Diffusion model?","2024-01-25 17:56:43","","0","74","<transformer><diffusion-models><convolutional-layers><u-net><vision-transformer>","<p>When I read about Stable Diffusion model, they usually talk about adjusting convolution layers or U-Net weights. I believe they both should be related together and the U-Net is the part that accepts the encoded image+text embedding from the VAE encoder and uses convolutional layers to extract features from the image, then adds the noise to this features, then denoises them and sends the output as a latent vector/matrix to to the VAE decoder.</p>
<p>But I am not sure if my understanding is completely correct?</p>
","transformer"
"43544","How to train ViT on smaller datasets?","2024-01-24 17:00:33","","0","25","<neural-networks><deep-learning><transformer><attention><vision-transformer>","<p>I know ViTs aren't made for small datasets and low resolution. But have you ever reached traditional CNN accuracy using ViT on CIFAR10/100.</p>
<p>I have been playing around with ViT on CIFAR10 and 100. But am not able to get it over 75% accuracy on CIFAR10.</p>
<p>I have tried these configurations for the architecture:</p>
<pre><code>patch sizes: 4 and 8
dimensions: 512 and 786
depth/transformer blocks: 8 and 10
attention heads: 8, 10 and 12
mlp dimension: 512, 2048 and 3072
</code></pre>
<p>I have tried SGD and Adam with different learning rates (0.1, 0.01) and (0.01, 0.001) respectively, with learning rate step decreasing after 100 and 175 epochs.</p>
<p>Also using weight decay of 1e-4 and 1e-5 with some image augmentation such as random flip, random rotate, minimal color jittering, and random affines.</p>
<p>Any suggestions to improve the validation accuracy to reach ~90% ?</p>
","transformer"
"43511","Any suggestions for transformer finetuning techniques ablation study?","2024-01-22 11:57:04","","0","44","<transformer><large-language-models><fine-tuning>","<p>I'm planning to fine tune a 7b parameter model for a research project. I understand the different steps of model fine tuning, namely</p>
<ol>
<li>Supervised fine tuning - where we train model on curated examples to impart domain specific knowledge (could be instruction fine tuning as well)</li>
<li>DPO/PPO - this step is done for model alignment to human preferences</li>
</ol>
<p>I understand there is a huggingface library which has built wrappers around these both but I'm trying to get an in depth understanding. So how I see a language model with depth N is that the first N-1 layers are building a representation and last layer is a policy network, i.e. takes the representation and spits out a probability distribution over the tokens. Now my main question, in SFT and DPO/PPO is there any literature which describes the following and provides benchmarks/ablation studies for the following -</p>
<ol>
<li>Doing SFT (no frozen parameters) and only using adapters for DPO/PPO</li>
<li>Doing SFT by freezing all layers except the last and followed by DPO/PPO on last layer</li>
<li>Doing SFT using adapters/LoRA, merging the weights, followed by adapters for DPO/PPO</li>
<li>Doing SFT using adapters/LoRA, merging the weights, followed by DPO/PPO on only last layer</li>
<li>Vanilla option, i.e. SFT followed by DPO/PPO no frozen weights.</li>
</ol>
<p>Any reference for literature or anecdotal evidence?</p>
","transformer"
"43507","Is the multi-headed projection matrix in self-attention redundant?","2024-01-21 23:44:31","","1","94","<transformer><attention><performance><linear-algebra>","<p>As I understand it, the forward pass for a transformer model looks as follows:</p>
<pre><code>x += self_attention(x)
x = layernorm(x)
x += ffn(x)
</code></pre>
<p>Breaking that down a bit (excuse the hand-waving, this is meant to be illustrative):</p>
<pre><code>def self_attention(x):
  qkv_list = get_qkvs(x)
  heads = [softmax(q @ t(v) / sqrt(d))@v for (q,k,v) in qkv_list]
  concatenated_heads = concat(heads)
  projected_values = concatenated_heads @ W_O # projection matrix for concatenated heads
  return projected_values
</code></pre>
<p>and</p>
<pre><code>def ffn(x):
  return (relu(x @ W_1 + b1))@W_2 + b2
</code></pre>
<p>Perhaps I am missing something obvious, but I note that if one were to drop the layernorm in between the self-attention and FFN module, you have two linear projections in a row (<span class=""math-container"">$W_O$</span>, <span class=""math-container"">$W_1$</span>). <span class=""math-container"">$W_O$</span> is a costly matrix: it should be dimensions (n_head * d_head x d_embed) = (d_embed x d_embed), so contributes (n_layers * d_embed^2) parameters over the course of the network.</p>
<p>My question is: if you drop the layernorm, it appears the <span class=""math-container"">$W_O$</span> matrix would be entirely redundant, and you could save a massive amount of compute by essentially fusing these two operations into a single learned matrix (that matrix would both be mixing the streams from the different heads like <span class=""math-container"">$W_O$</span> does, and up-projecting into the FFN embedding layer like <span class=""math-container"">$W_1$</span> does). This means either:</p>
<ul>
<li>The layernorm is incredibly important, and I assume this has been shown somewhere empirically?</li>
<li>I've missed something completely obvious about the forward pass / even if you dropped the layernorm the <span class=""math-container"">$W_O$</span> matrix is not redundant for some reason</li>
</ul>
<p>Would love to be corrected!</p>
","transformer"
"43464","Overcoming the quadratic scaling in transformer architecture","2024-01-17 17:49:57","43476","0","95","<reference-request><transformer><papers><gpt>","<p>Do you know any papers that try to overcome quadratic scaling problems by attending lower dimensional representations in the dimension of tokens?</p>
<p>For example, let's say that the input to the transformer is <code>(batch_size, token_number, hidden_dimension)</code>.</p>
<p>Do you think it would make some sense to do let's say one Transformer Block (<code>TB</code>) and then map (by some weight matrix) <code>token_number</code> to let's say <code>token_number/2</code> and do <code>TB</code> again?</p>
<p>The graph might look like this:</p>
<p><code>input_tokens</code> -&gt; <code>TB</code> -&gt; <code>hidden_tokens</code> -&gt; <code>TB</code> -&gt; ... -&gt; <code>output_tokens</code> (might be equal to <code>input_tokens</code> in dimension)</p>
<p>So, our shapes would be:</p>
<p><code>(B, T, H)</code> -&gt; <code>(B, T/2, H)</code> -&gt; <code>(B, T/4, H)</code> -&gt; ... -&gt; <code>(B, T/4, H)</code> -&gt; <code>(B, T/2, H)</code> -&gt; <code>(B, T, H)</code></p>
<p>where B - batch_size, T - token_number, H - hidden_dimension</p>
","transformer"
"43386","Are K and V values reused in each decoder layer's cross-Attention in the original ""Attention is all you need"" paper?","2024-01-10 18:41:36","","0","50","<machine-learning><natural-language-processing><transformer><attention>","<p>I'm working with Transformers and have a question about the encoder-decoder structure. In each decoder layer's cross-attention, are the K and V pairs from the corresponding encoder layer reused for all cross-attention operations within that decoder layer? Or are they recalculated for each operation?</p>
<p>Also, if K and V are reused, doesn't this limit the decoder's effectiveness? It seems like only the Query values are changing and propagating through the model. Since most of the meaning is stored in the V layer, wouldn't using the same K and V hinder the decoder's abilities?</p>
","transformer"
"43370","Why are rows of Attention Weights in a Hopfield Transformer the same?","2024-01-08 22:44:31","","0","33","<transformer><attention><weights><dimensionality-reduction><hopfield-network>","<p>I'm working on building a Hopfield Transformer using the github code from the paper (<a href=""https://github.com/ml-jku/hopfield-layers/tree/master/hflayers"" rel=""nofollow noreferrer"">https://github.com/ml-jku/hopfield-layers/tree/master/hflayers</a>) to forecast a timeseries dataset with 48 variables, 20 past timesteps, and 4 future timesteps. I'm then working to extract the attention weight matrix.</p>
<p>I'm noticing that the attention weight matrix has rows that are all the same.</p>
<p>The way I'm building this Transformer is with a source dataset of shape [176, 48, 20] and a target dataset of shape [176, 48, 4] where 176 is the batch size, 48 is the &quot;sequence&quot; length, and 20 (or 4) is the embedding dimension. In the encoder, I first embed and perform sequential encodings on the 20 timesteps to obtain a [176, 48, 1280] tensor, which gets sent to the Hopfield Encoder from the github (or the hopfield attention layer, either way it works the same), and then to the Decoder, with a linear layer that reshapes the source and target data shapes to match.</p>
<p>**My model does appear to significantly outperform a vanilla transformer. However when extracting the attention weight matrix, I'm noticing that the rows appear the same: **</p>
<p><a href=""https://i.sstatic.net/48lxo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/48lxo.png"" alt=""Example pulled out from the code I'm running"" /></a></p>
<p>Is this supposed to happen, or am I missing something about Hopfield Transformer functionality in the context of my data shape? I'm not sure what to do to resolve this - I'm happy to provide code if it helps!</p>
","transformer"
"43327","Why is the sinusoidal model classified as absolute positional encoding in some literature?","2024-01-05 18:30:29","43488","2","113","<natural-language-processing><transformer><positional-encoding>","<p>I am currently reading in depth about positional encodings, and as we know there are two types of positional encodings: Absolute and relative.</p>
<h4>My question:</h4>
<p>Why is the sinusoidal model classified as absolute positional encoding in some literature, given that in Vaswani's original paper it was said that it captures relative relationships between words, and this has been proven <a href=""https://blog.timodenk.com/linear-relationships-in-the-transformers-positional-encoding/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>However, while I was reading a <a href=""https://direct.mit.edu/coli/article/48/3/733/111478/Position-Information-in-Transformers-An-Overview"" rel=""nofollow noreferrer"">research</a>, it was mentioned that projections that occur in the attention layer destroy this:</p>
<blockquote>
<p>Indeed, sinusoidal position embeddings exhibit useful properties in theory. Yan et al. (2019) investigate the dot product of sinusoidal position embeddings and prove important properties:
(1) The dot product of two sinusoidal position embeddings depends only on their relative distance. That is, <img src=""https://latex.codecogs.com/svg.image?&amp;space;P_t%5ET.P_%7Bt+r%7D"" alt="" P_t^T.P_{t+r}"" /> is independent of <img src=""https://latex.codecogs.com/svg.image?t"" alt=""t"" />.
(2) <img src=""https://latex.codecogs.com/svg.image?&amp;space;P_t%5ET.P_%7Bt+r%7D=P_t%5ET.P_%7Bt-r%7D"" alt=""P_t^T.P_{t+r}=P_t^T.P_{t-r}"" />, which means that sinusoidal position embeddings are unaware of direction. However, in practice the sinusoidal embeddings are projected with two different projection matrices, which destroys these properties.</p>
</blockquote>
<p>Is this the reason?</p>
","transformer"
"43309","What am I doing wrong that result in a graph indicating better gradients in non-scaled dot-product attention compared to the scaled version?","2024-01-04 08:18:27","","0","39","<transformer><backpropagation><pytorch><attention><gradient>","<p>I'm trying to visualize how the gradients change as we're increasing <span class=""math-container"">$d_{k}$</span> in the scaled dot-product attention and compare it to its non scaled version but I'm failing to produce a reasonable graph that transcribes the theory. The authors of the <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention Is All You Need</a> paper explain that non scaling the dot-product leads to having large values which pushes the softmax into regions where its gradients are small, which is understandable because the values inside the softmax have such a high variance around a mean of 0 that the softmax will produce a mixture of values very close to 0 and 1.</p>
<p>I'm trying to visualize that using the following Python code:</p>
<pre><code>def scaled_softmax_dot_product(Q, K, scale):
    attention_logits = torch.matmul(Q, K.T)
    attention_logits /= scale
    return F.softmax(attention_logits, dim=-1)

mean_grads_with_scaling = []
mean_grads_without_scaling = []
dk_values = range(1, 321)

for d_k in dk_values:
    Q = torch.randn(16, d_k, requires_grad=True)
    K = torch.randn(16, d_k, requires_grad=True)

    # Compute attention with scaling
    output_scaled = scaled_softmax_dot_product(Q, K, scale=torch.sqrt(torch.tensor(d_k, dtype=torch.float)))
    output_scaled.backward(torch.ones_like(output_scaled))
    grad_with_scaling = Q.grad.mean().item()
    Q.grad.zero_()
    K.grad.zero_()

    # Compute attention without scaling
    output_unscaled = scaled_softmax_dot_product(Q, K, scale=1.0)
    output_unscaled.backward(torch.ones_like(output_unscaled))
    grad_without_scaling = Q.grad.mean().item()
    Q.grad.zero_()
    K.grad.zero_()

    mean_grads_with_scaling.append(grad_with_scaling)
    mean_grads_without_scaling.append(grad_without_scaling)

plt.figure(figsize=(10, 6))
plt.plot(dk_values, mean_grads_with_scaling, label='With Scaling', color='blue')
plt.plot(dk_values, mean_grads_without_scaling, label='Without Scaling', color='red')
plt.xlabel('Dimension of Keys (d_k)')
plt.ylabel('Mean Gradients')
plt.title('Evolution of Mean Gradients with and without Scaling')
plt.legend()
plt.show()
</code></pre>
<p>As you can see in the code I'm just computing the softmax of the non/scaled dot-product with respect to <span class=""math-container"">$Q$</span> and I was expecting the mean of the gradients for the non scaled dot-product to converge to <span class=""math-container"">$0$</span> as <span class=""math-container"">$d_k$</span> increases but that's not what the graph shows:</p>
<p><a href=""https://i.sstatic.net/EjCOV.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EjCOV.png"" alt=""enter image description here"" /></a></p>
<p>I'd love to understand what is the error that I'm making to for getting this graph, or if I am misunderstanding the statement.</p>
","transformer"
"43288","Comparing the performances of GPTs with deep learning in the field of binary files and their related reports","2024-01-02 07:38:23","","1","43","<deep-learning><transformer><generative-model><large-language-models><gpt>","<p>Regarding the case study of a dataset including binary files (containing assembly code) and reports related to each file (the content of the static analysis of the file as well as the analysis of the dynamic functionality of the file, with a specific format), can it be generally claimed that the learning on this dataset with the use of GPTs will be more effective than training by other deep learning methods?
What exactly should be considered for this comparison?
An example of assembly code of a binary file may be this simple code:</p>
<pre><code>global    _start

          section   .text
_start:   mov       rax, 1                  ; system call for write
          mov       rdi, 1                  ; file handle 1 is stdout
          mov       rsi, message            ; address of string to output
          mov       rdx, 13                 ; number of bytes
          syscall                           ; invoke operating system to do the write
          mov       rax, 60                 ; system call for exit
          xor       rdi, rdi                ; exit code 0
          syscall                           ; invoke operating system to exit

          section   .data
message:  db        &quot;Hello, World&quot;, 10      ; note the newline at the end
</code></pre>
<p>And an example of features that a report can present are:
hash of binary file, sections of file (.text , etc.), strings, network connections, activities carried out in the registry, etc.</p>
","transformer"
"43241","Do I need residual block in a transformer model if vanishing gradients don't exist?","2023-12-27 21:36:58","","0","56","<neural-networks><transformer><attention>","<p><a href=""https://i.sstatic.net/jzvCW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jzvCW.png"" alt=""Self Attention Diagram"" /></a></p>
<p>In this example l’Afrique is x^3 and the attention is being computed for this word A^3(l’Afrique). In the image above Andrew Ng indicates that the word with the biggest wieght in the computation of A^3 will be visite not l’Afrique itself because visite gives more context to l’Afrique. So when all the V values are summed A^3 will contatin more of visite embedding than l’Afrique embedding. So my question is this theoretically if vanishing gradients did not exist would a residual block still be required for a transformer network to work because it would need to maintain the information of the original word x^3 and combine that with A^3 since A^3 has more information about X^2 than X^3 ?</p>
","transformer"
"43235","What is the approximate minimum coding rate for NLP datasets?","2023-12-27 03:29:52","","0","11","<natural-language-processing><transformer><information-theory>","<p>I just realized that it is actually practical to use information theory to compute the maximum viable compression for datasets &amp; that it is easiest to compute for discrete datasets. This makes me wonder what the minimum coding rate (i.e. minimum required bit representation) for well known NLP datasets is? Also approximately how close to this minimum coding rate do modern transformers achieve in the embedding space?</p>
","transformer"
"43232","If you throw additional compute at DQN will you get the same results as if you threw compute at transformers?","2023-12-26 16:54:50","","0","22","<reinforcement-learning><training><transformer>","<p>Recent performance in transformer based AI models have done really well by just throwing additional compute and data into the training. Can you expect similar levels of results by throwing more simulations at a Deep Q learning agent?</p>
","transformer"
"43156","Transformers - how do the decoder attention input matrices look like, in terms of future tokens?","2023-12-16 21:22:40","43194","0","109","<transformer><attention><encoder-decoder>","<br>
I have a question regarding the original transformer implementation (as in ""Attention is all you need"").<br>
Assuming I want to translate English to German.
<ol>
<li>In the Decoder part, in the self-attention layers, the input is only the German tokens the model already translated, but never the future tokens.
So in the input matrices X=K=Q=V, what are the values in those rows? some placeholder null token?</li>
<li>If so, what happens in the (not self) attention afterwards? How does its Q matrix look like as a result from the previous self attention? What bothers me is that its Q matrix would have null/random-valued rows, which would affect the attention result.</li>
</ol>
<p>Much appreciated.</p>
","transformer"
"43150","Which input embeddings are learned during pre-training in BERT? What about during fine-tuning?","2023-12-16 04:54:39","","0","80","<natural-language-processing><transformer><bert>","<p>I was reading the 2019 BERT paper and they mention how they use wordpieces that are then represented as the sum of token embeddings, segment embeddings, and positional embeddings. What is unclear to me is whether the token, segment and positional embeddings are all learned during pre-training AND whether they are modified during fine-tuning.</p>
<h2>Pre-Training:</h2>
<ul>
<li><strong>Sentence Embeddings:</strong> The paper says that the sentence embeddings are learned during pre-training (&quot;we add a learned embedding to every token indicating whether it belongs to sentence A or sentence B&quot;). <strong>am pretty confident about this.</strong></li>
<li><strong>Positional Embeddings:</strong> The paper does not say whether the positional embeddings are learned during pre-training. However, BERT relies on the transformer architecture from the &quot;Attention Is All You Need&quot; pa[er and I believe the positional embeddings in THAT paper are learned during what would pre-training for the BERT model. <strong>- Not so confident about this.</strong></li>
<li><strong>Token Embeddings:</strong> I'm guessing the token embeddings aren't learned during pre-training but before pre-training. Specifically, I hypothesize that the authors simply use the word piece embeddings approach to break up sentences into word &quot;pieces&quot; AND map each of these pieces to numbers, but this mapping is not modified during pre-training. <strong>- I'm very uncertain about this.</strong></li>
</ul>
<p><strong>Are positional embeddings and token embeddings learned during pre-training?</strong></p>
<h2>Fine-Tuning:</h2>
<p>Essentially the same question but for fine-tuning: are these embeddings typically modified during fine-tuning?</p>
","transformer"
"43097","Could we use another attention module on the outputs of the attention heads in the transformer architecture?","2023-12-11 14:20:36","","0","27","<transformer><attention>","<p>Before concatenating the heads in MHAtt, could we add another attention module with the heads as input to combine them?</p>
<p>Thus, for each head we would get a value matrice enriched with the outputs of the other heads, possibly allowing for an even more coherent view on the input as each head's output is additionally improved by insights from the other heads.</p>
","transformer"
"43075","How does Chat GPT encode a question?","2023-12-11 02:34:48","","0","755","<deep-learning><transformer><chatgpt><large-language-models><encoder-decoder>","<p>Chat GPT is based on a decoder-only Transformer so it does not have an encoder. Given that, how is a user's question passed as input to Chat GPT's decoder? In a regular encoder-decoder architecture, the final embeddings of the encoder are passed to the decoder along with the  token. Then, the decoder auto-regressively outputs tokens. How would this work in a decoder-only architecture?</p>
<p>Let's say I have the following question: &quot;How many countries are there in the world?&quot; and its token form is [3, 5, 8, 2, 10, 4, 1, 6, 7]. How will the decoder take in the input?</p>
<p>In an encoder-decoder architecture, I would just pass the  token to the decoder which would, based on the encoder embedding, auto-regressively output the next token in the sequence to form an answer.</p>
<p>In the decoder-only architecture, how will that work? If I pass the first token of the question (token &quot;3&quot;) to the decoder, it will output what it thinks is the most likely token after &quot;3&quot; but it will have no context... So how is the context taken into account given we are not encoding it?</p>
<p>I am aware of <a href=""https://ai.stackexchange.com/questions/40179/how-does-the-decoder-only-transformer-architecture-work"">this post</a> which is similar but I would more specifically want to know how we go from a decoder-only model that inputs the most likely word <em>in general</em>, to a model that outputs a whole answer based on a specific question. Put differently, how does a problem which is inherently a sequence to sequence problem (question answering) get solved using an autoregressive model instead of a seq2seq model?</p>
<p>One potential answer I have been looking into is training fine-tuning but I am still unsure how a model that out puts a single token x[n+1] = f(x[n],...,x[1]) can be fine-tuned based on a training set of two sequences (one question and one answer per record).</p>
","transformer"
"43065","What is the meaning of ""dimensionality of the embeddings and hidden states""?","2023-12-09 19:02:30","","0","107","<machine-learning><deep-learning><natural-language-processing><recurrent-neural-networks><transformer>","<p>I was reading the GPT-2 and LSTM documents and noticed that they use the terms &quot;dimension of embedding and hidden state&quot;. For GPT-2, the size is <span class=""math-container"">$768$</span>, and for LSTM, the size is <span class=""math-container"">$256$</span>. What does that mean ? Are there any visual graphs or figures for these concepts ? Very difficult to understand any of this.</p>
<p>Thanks</p>
","transformer"
"43048","Is Softmax Necessary as the Activation Function for Self-Attention Mechanisms?","2023-12-07 22:15:46","","1","994","<deep-learning><transformer><attention><softmax>","<p>I’m curious about the mathematical reasoning behind the use of the softmax function as the activation function in self-attention mechanisms within neural networks. Specifically, I’m interested in understanding if there is a theoretical basis that necessitates the use of softmax over other activation functions.</p>
<p>Softmax is commonly employed to convert raw attention scores into a probability distribution, ensuring that the sum of attention weights equals 1. This normalization allows the model to effectively focus on certain parts of the input sequence. However, I wonder if there are alternative activation functions that could be less constraining and still allow the optimization process to determine the best way to allocate attention, similar to how tanh or other activations work in different layers of a neural network.</p>
<ol>
<li>Is there a mathematical justification for the necessity of softmax in self-attention mechanisms?</li>
<li>Could other activation functions, perhaps with fewer constraints, be used effectively in place of softmax, allowing the optimization process more flexibility?</li>
</ol>
<p>Any insights or references to relevant literature would be greatly appreciated.</p>
","transformer"
"42995","Does a decoder in transformer model generate output embeddings like the following?","2023-12-04 10:10:37","","-1","272","<transformer><decoder>","<p><strong>Encoder:</strong></p>
<pre><code>Input: [A, B, C, D] (word embeddings)
Output: [C1, C2, C3, C4] (contextual representations)
The encoder processes the input sequence [A, B, C, D] and generates contextual representations [C1, C2, C3, C4]. The specific calculations involved in the encoder, such as self-attention and feed-forward layers, are not shown in this example.
</code></pre>
<p><strong>Decoder:</strong></p>
<pre><code>Input: [C1, C2, C3, C4] (contextual representations)
Output: [A', B', C', D'] (word embeddings)
The decoder takes the contextual representations [C1, C2, C3, C4] as input and generates word embeddings [A', B', C', D'] for the output sequence. The decoder generates one word embedding at a time, conditioned on the previously generated embeddings and the contextual representations.
</code></pre>
<p>Let's illustrate the decoding process step by step:</p>
<pre><code>Step 1:

Input: [C1] (contextual representation of the &lt;start&gt; token)
Output: [A'] (word embedding for the first output word)
Step 2:

Input: [CA', C2] (contextual representations and word embedding generated so far)
Output: [B'] (word embedding for the second output word)
Step 3:

Input: [A', B',C3] (contextual representations and word embeddings generated so far)
Output: [C'] (word embedding for the third output word)
Step 4:

Input: [A', B', C',C4] (contextual representations and word embeddings generated so far)
Output: [D'] (word embedding for the fourth output word)
</code></pre>
","transformer"
"42992","Can you illustrate how the weights in transformer model generated from a training sentence can be generalized to an unseen test sentence?","2023-12-04 02:00:36","","0","47","<transformer><weights><generalization>","<p>Can you show how the weights in transformer model are generalizable?</p>
","transformer"
"42986","Masking during Instruction Tuning for LLM finetuning","2023-12-03 13:14:04","","0","430","<natural-language-processing><transformer><large-language-models>","<p>I'm currently trying to learn more about LLMs particularly generative decoder only models such as the GPT family of models. I do have one question about masking though.</p>
<p>For me the way masking is performed left-to-right makes sense during pre-training but it is weird to me during instruction fine-tuning when dealing with the input prompt. I therefore have two questions:</p>
<ul>
<li>Is the input prompt masked in the same way as the text during pre-training?</li>
<li>And if so why is it done in this way and have there been studies on it</li>
</ul>
","transformer"
"42968","When are Transformers better than LSTMs in time-series tasks such as classification?","2023-12-01 01:33:55","","1","1528","<classification><transformer><long-short-term-memory><time-series>","<p>I’m working on a time-series classification problem and trying to decide whether to use a Transformer or an LSTM.</p>
<p>From what I’ve learned, Transformers are better suited for capturing long-range dependencies in time-series data vs LSTM. The transformers are an excellent option for processing word data for Natural Language tasks, and frameworks are currently built to represent time-series data, such as TabTransformer and TimeSeriesTransformer models.</p>
<p>But I’m unsure if that’s always the case.</p>
<p>Are there specific use cases where Transformers are better than LSTMs for time-series classification?</p>
<p>What are some key differences between the two architectures in representing time-series data?</p>
","transformer"
"42939","Prefix tuning in LLM uses learnable vectors to fine tune the model","2023-11-28 14:59:33","","0","93","<neural-networks><machine-learning><deep-learning><natural-language-processing><transformer>","<p>I would like to implement a new architecture for Transformer.</p>
<p>Below description is my thought.</p>
<p>Prefix tuning in LLM uses learnable vectors to fine tune the model.</p>
<p>Is there a way to use the output generated by the Neural network as prefix?</p>
<p>Thanks</p>
<p><a href=""https://i.sstatic.net/YsUZ1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YsUZ1.png"" alt=""enter image description here"" /></a></p>
","transformer"
"42799","Understanding the transformer at inference time","2023-11-14 08:11:32","","0","241","<transformer>","<p>Let's consider language translation and let <span class=""math-container"">$I_1,\ldots,I_{N_i}$</span> be the <span class=""math-container"">$N_i$</span> input tokens. My understanding is that the encoder produces <span class=""math-container"">$N_i$</span> embeddings, which I will refer to as <span class=""math-container"">$E_1,\ldots,E_{N_i}$</span>. In what follows, let's consider what happens at inference time and not during training. My understanding is that the decoder has multiple inputs and they possibly change when he produces the different output tokens. If considering a single decoder module, I guess it makes sense to distinguish between the external inputs that are fed into the multi-head attention module and the ones that go into the masked multi-head attention module as shown below.<br>
<a href=""https://i.sstatic.net/rdHA8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rdHA8.png"" alt=""enter image description here"" /></a><br>
Here are now my questions:</p>
<ol>
<li>Is it correct that what comes from the left into the multi-head attention modules are always the encoder embeddings <span class=""math-container"">$E_1,\ldots,E_{N_i}$</span>?</li>
<li>I believe to understand that the bottom input to the masked multi-head attention module is not always the same and depends on the iteration we find ourselves in, i.e. which output token we predict and which decoder module is considered if there are multiple. More precisely, is it true that for the first prediction, i.e. when predicting the first output token, the sole input to the first decoder module is the embedding of the 'START' token plus positional encoding?</li>
<li>What are the bottom inputs of the different decoder modules when predicting the individual output tokens?</li>
</ol>
","transformer"
"42787","What is the input to an encoder-decoder transformer in next word prediction task?","2023-11-13 21:27:57","","1","65","<natural-language-processing><transformer><encoder-decoder>","<p>I'm trying to understand how encoder-decoder architectures are used, or if they are used at all, for generative tasks that do not require an explicit prompt (ie. machine translation, summarization, etc.).</p>
<p>From my understanding, decoder-only models autoregressively predict the next token in a sequence given its previous predictions. This makes sense, as we can simply keep feeding it tokens already predicted during inference. But how is this done when there is an encoder involved? For machine translation, we have the sequence in the source language to feed to the encoder. Similarly, we can feed it a passage to summarize for summarization. What would we feed the encoder if we simply wanted next word prediction? Do we feed it the sequence we want it to complete? I haven't found any examples of this task being performed. Does this mean that encoder-decoder models aren't needed for this task?</p>
","transformer"
"42785","Modifying Cross Entropy Loss to work with multiple correct target sequences?","2023-11-13 21:09:42","","1","34","<transformer><machine-translation><seq2seq>","<p>Let's say I'm training a transformer model to perform a seq to seq task, but there are multiple correct answers. For example, the following outputs would all be considered correct:</p>
<p>source: A B C -&gt; target: C B D</p>
<p>source: A B C -&gt; target: C D E B E</p>
<p>...</p>
<p>source: A B C -&gt; target: D E C B E</p>
<p>The way I'm currently handling this is to augment the dataset by duplicating all the data and randomly assigning correct targets to each instance of each input. This works fairly well and has been tried before in previous papers, however I'm wondering if there is a way to modify the loss function to measure the minimum cross entropy loss to all possible answers. I could loop over them and take the min but with batching the complexity would quickly scale to be an issue. If there's any other ideas I could use, or ways to implement taking the min in a memory/time efficient way, I would appreciate the help!</p>
","transformer"
"42765","Why is it called multi-headed attention?","2023-11-12 14:13:40","42811","0","112","<neural-networks><terminology><transformer><attention>","<p>Why do we call the attention layer in transformers <strong>multi-headed</strong> attention when in practice all the attention matrices from different heads <strong>(W,K,V)</strong> for a single layer are concatenated to perform the calculation in one go and then the result is multiplied by another matrix Wo to get the dimensions required?</p>
<p>My understanding of self-attention calculation is based on the following blog: <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">https://jalammar.github.io/illustrated-transformer/</a></p>
","transformer"
"42761","Masking in Decoder of Transformer","2023-11-12 12:36:05","","1","294","<natural-language-processing><transformer><decoder>","<p>I understand that the masked multi-head attention block ensures that generation of token at time step t doesn't rely on subsequent tokens of the input. But the residual connection which adds the input to the output of masked multi-head attention block adds some of the information from future time steps which is then used to construct the query matrix for multi-head attention block.</p>
<p>Shouldn't some kind of mask be applied before adding the input values to the output of masked multi-head attention block as well?</p>
<p><em>Asking this question in the context of training process.</em></p>
<p><img src=""https://kikaben.com/transformers-encoder-decoder/images/figure-1.png"" alt=""Reference Image for Decoder Architecture"" /></p>
","transformer"
"42738","Are the outputs of layers in Attention Is All You Need interpretable by mapping to tokens?","2023-11-10 15:28:05","","0","16","<transformer><attention>","<p>In the basic transformer model from 2017, I'm a bit confused what the outputs of each layer are supposed to be. Are they embeddings? If so, does that mean you could examine a given output from a given encoder and decoder layer and covert it to tokens basically? In other words, for a given layer output, suppose you used a similarity search over the token embeddings for the entire vocabulary, and then matched the most similar words to the embeddings from the chosen layer. Would this give you some sense of what words it was basically considering at that point in the architecture?</p>
","transformer"
"42559","Why encoders are required in Transformers","2023-10-26 17:09:51","","0","203","<natural-language-processing><transformer><attention><natural-language-understanding><encoder-decoder>","<p>In the original Transformers paper why encoder is added when a decoder alone can do what an encoder can do (like multi-head attention, feed-forward NN etc....).</p>
<p>I mean even a decoder also has the same components that an encoder has</p>
<p><a href=""https://i.sstatic.net/i7Rkm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/i7Rkm.png"" alt=""enter image description here"" /></a></p>
","transformer"
"42432","From where do the Encoders in Transformers gets Input Embedding from?","2023-10-13 14:10:45","","0","640","<transformer><autoencoders><word-embedding><embeddings>","<p>In Transformers Encoders, from where do the Encoders get Input Embedding from?</p>
<p>So when a sentence is given to a transformer-based model it first tokenises the sentence and each token is mapped with some integer in vocabulary. After that how does those tokens (integer) is converted to Input Embeddings?</p>
<p>If initial embeddings are obtained from some pre-trained model, how did that model was able to generate embeddings for a word?</p>
<p><a href=""https://i.sstatic.net/KNx4U.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KNx4U.png"" alt=""enter image description here"" /></a></p>
","transformer"
"42386","Time Series Classification using Transformer Encoder","2023-10-09 18:06:48","","0","132","<neural-networks><machine-learning><transformer><dense-layers>","<p>Lets say I have a collection of tensors, each tensor representing a time series with 64 points and 4 features. The dimension of each tensor would be [64,4]. I am trying to classify these series. For that I am first passing these tensors into a Transformer Encoder (having 2 attention heads and 2 encoder layers) that outputs a tensor of the same dimension. This output tensor is being flattened and passes onto a dense layer for classification. Is there some advantage of passing the time series through the encoder and classifying the encoded output over directly passing the original tensors to the dense layer.</p>
<p>I tried this experimentally and saw no significant increase in accuracy when using the transformer encoder. However, the data I had was quite simple and not enough to make any conclusions. Also an expert I know insists that the model with the input processed by a transformer should work better.</p>
<p>One thing I observed was a steeper decrease in loss when using the encoded tensors for classification.</p>
<p>I also referred to this resource on this matter: <a href=""https://www.linkedin.com/pulse/time-series-classification-model-based-transformer-gokmen/"" rel=""nofollow noreferrer"">https://www.linkedin.com/pulse/time-series-classification-model-based-transformer-gokmen/</a></p>
","transformer"
"42313","Aren't context lengths for transformers an artificial restriction?","2023-10-04 07:45:30","42321","3","1050","<natural-language-processing><transformer><attention><gpt><decoder>","<p>Let's focus on the case of decoder-only transformers, where I am using algorithm 10 from &quot;Formal Algorithms for Transformers&quot; by Mary Phung and Marcus Hutter as a reference.</p>
<p><a href=""https://i.sstatic.net/Me8rq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Me8rq.png"" alt=""Algorithm 10 from &quot;Formal Algorithms for Transformers&quot; by Mary Phung and Marcus Hutter"" /></a>: <a href=""https://i.sstatic.net/ZWC9o.png"" rel=""nofollow noreferrer"">https://i.sstatic.net/ZWC9o.png</a></p>
<p>Previously I thought that the maximum context length is very much <strong>built into</strong> the transformer, for example as the dimension of a layer of weights. After studying this algorithm I am surprised because it seems more like an <strong>artificial restriction</strong>! Because this is a topic of active research I would like to know if I am misunderstanding something.</p>
<p>The way I see it, if I had access to the weights of GPT-2 right now, I could almost execute it on any number of tokens I like right away (If I had sufficient memory to compute this). The MHA algorithm is just carried out over a larger sequenece. There are only two issues, which are points where the context window <span class=""math-container"">$l_{max}$</span> appears:</p>
<ol>
<li>The positional encoding has only <span class=""math-container"">$l_{max}$</span> positions</li>
<li>During training the weights were never optimized to attend over more than <span class=""math-container"">$l_{max}$</span> tokens.</li>
</ol>
<p>But these issues seem rather easy to resolve:</p>
<ol>
<li><p>Use some positional encoding which has infinitely many positions. The first encoding vectors are nicely spread around while the later ones are closer to eachother, due to the nature of fitting an infinite sequence of vectors into more or less a unit ball/sphere. But this is not an issue: It is natural for the positional encoding to become more vague as the token is further and further in the past.</p>
</li>
<li><p>Train 50% on context lengths around <span class=""math-container"">$l_{max}$</span>, 25% on context lengths around <span class=""math-container"">$2 l_{max}$</span>, 12.5% on context lengths around <span class=""math-container"">$4 l_{max}$</span> and so on...</p>
</li>
</ol>
<p>I can imagine the following issues appearing:</p>
<p>A) Memory becomes larger than what is available on a single &quot;unit&quot; (GPU?) so you have to start moving data back and forth to execute your transformer, which is just terribly inefficient during training and also inference, so it is really pointless to train on such large context windows.</p>
<p>B) Perhaps the transformer just doesn't learn well with this procedure for some reason.</p>
<p>These issues are still rather &quot;soft&quot; issues though. As far as I can tell, I could use the architecture of GPT-2 (modified positional encoding) to create 1000000 context window LLMs, in theory. So, am I missing something?</p>
<p>Thank you!</p>
","transformer"
"42312","How to force Transformer to give more weight to certain tokens","2023-10-04 07:04:03","","0","323","<neural-networks><machine-learning><deep-learning><transformer><pytorch>","<p>I'm developing an encoder-decoder based transformer model and I would like to ask if there are ways to incentivize or penalize certain tokens during training.</p>
<p>I'm working on a translation task where the encoder input must be decoded into its proper product name. I have labels such as brand, name, and unit of measure, etc which are available during training but not on inference.</p>
<p>Currently when predicting the brand portion (which usually appears early in the sequence) of the output, the heatmap shows that it does not give focus to the latter part of the encoder which produce an output that the brand and product name, and unit of measure does not belong to each other.</p>
<p>I was thinking if there's a way to force the transformer during training to give more weight to different token types other that its own.</p>
<p>For example:</p>
<ol>
<li>Brand tokens (decoder) should give more weight to name tokens (encoder) than other brand tokens (encoder)</li>
<li>Name tokens (decoder) should give more to brand token (encoder) and unit of measure token (encoder)</li>
</ol>
","transformer"
"42292","Why is there a shared matrix W in graph attention networks instead of the query-key-value trio like in regular transformers?","2023-10-02 15:39:56","42296","1","336","<transformer><attention><graph-neural-networks>","<p>In section 2.1 of the <a href=""https://arxiv.org/pdf/1710.10903.pdf"" rel=""nofollow noreferrer"">Graph attention network paper</a></p>
<p>The graph attention layer is described as</p>
<blockquote>
<p>as an initial step, a shared
linear transformation, parametrized by a weight matrix, W ∈ RF ′×F , is applied to every node. We then perform self-attention on the nodes—a shared attentional mechanism a : RF ′
× RF ′→ R computes attention coefficients eij = a(Whi, Whj ) that indicate the importance of node j’s features to node i.</p>
</blockquote>
<p>(forgive my amateur formatting)</p>
<p>The function <code>a</code> represents a fully connected neural network that takes the concatenated vector of <code>Whi</code> and <code>Whj</code>, then outputs a single value which is pushed through a <code>softmax</code> to get the attention score <code>aij</code>. Then, the embedding vectors <code>Whj</code> (for all neighboring nodes of <code>i</code>) are weighted and summed by the attention score as normal.</p>
<p>If my understanding is correct, this means that matrix W represents the query, the key and the value transformation matrices all in one. But how can it do that? I feel that the difference may lie in the way the additive attention is calculated vs the dot-product one, but I cannot comprehend how this works. Why can we use a shared matrix here and not there? Is it theoretical or technical?</p>
","transformer"
"42247","How to get Llama-2 Rotary Embeddings?","2023-09-29 07:46:08","","-1","691","<neural-networks><natural-language-processing><transformer><word-embedding><large-language-models>","<p>I want to get the Llama-2 rotary embeddings. I do <code>print(model)</code> and get the following output:
<a href=""https://i.sstatic.net/jFu2Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jFu2Z.png"" alt=""enter image description here"" /></a></p>
<p>In the picture I highlight the rotary embeddings.</p>
<p>How can get the rotary embeddings and how can I interpret the output? What means 32x LLamaDecoderLayer and in its round brakets are four layer plus LlamaRotaryEmbeddings?</p>
<p>It's possible to get the embeddings as the first hidden-state <code>hidden_state[0]</code> and I want to know, which hidden-state represents the rotary embeddings.
Am I right, that there are several rotary embeddings?</p>
<p>Thanks in forward.</p>
<p>Best regards.</p>
","transformer"
"42173","What is the policy model in RLHF for LLMs?","2023-09-22 18:37:55","","0","22","<neural-networks><transformer><large-language-models><rlhf><decoder>","<p>What is the policy model doing explicitly in an LLM with RLHF setup?</p>
<p>From my understanding, LLMs generate in a way that is no different from any of their predecessors: beam search decoding, potentially with some sampling.</p>
<p>Does this mean the policy model is specifying how to generate the next token?</p>
","transformer"
"42116","Transformer decoder. Causal masking during inference?","2023-09-18 08:58:21","","0","1491","<transformer><decoder>","<p>I understand how causal masking in the self-attention layer of the decoder works and why we use it <strong>during training</strong>. What I want to ask is: <em>should we use causal masking during inference</em> ?</p>
<p>Consider a machine translation task where you need to translate the sentence <br />
<code>[&quot;I&quot;, &quot;am&quot;, &quot;going&quot;, &quot;to&quot;, &quot;the&quot;, &quot;cinema&quot;]</code> <br />
from english into german. During inference the encoder encodes the input sentence and the decoder starts generating the output sentence token by token. Let's say the following is generated until now:<br />
<code>[&quot;&lt;START&gt;&quot;, &quot;Ich&quot;, &quot;gehe&quot;, &quot;ins&quot;]</code> <br />
and you have to generate the next token. What you need to do is forward the currently generated sequence through the decoder and it will output a probability distribution for the next token. The question is: <em><strong>Do we need to use causal masking here</strong></em>?</p>
<p>Using the causal mask:
<span class=""math-container"">$$
\text{mask} = 
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 1 &amp; 1 &amp; 1
\end{pmatrix}
$$</span>
in the self-attention layer of the decoder would force each of the generated tokens to attend only to previous tokens. However, in my opinion, there is no need to use any masking here. The tokens that are already generated could simply attend to each other in order to better predict the next token.</p>
<p>However, reference implementations that I have been looking at continue using the causal masking during inference. See for example:</p>
<ul>
<li>Annotated Transformer from Harvad<br />
<a href=""http://nlp.seas.harvard.edu/annotated-transformer/#greedy-decoding%5C"" rel=""nofollow noreferrer"">http://nlp.seas.harvard.edu/annotated-transformer/#greedy-decoding\</a>
Here they explicitly pass the causal mask as a parameter when decoding.</li>
<li>Andrej Karpathy's minGPT<br />
<a href=""https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L283%5C"" rel=""nofollow noreferrer"">https://github.com/karpathy/minGPT/blob/37baab71b9abea1b76ab957409a1cc2fbfba8a26/mingpt/model.py#L283\</a>
Here the decoder is called as-is, thus the causal mask will not be deactivated, but will be applied.</li>
</ul>
<p>Is there a reason for using causal masking during inference?<br />
Any thoughts on the matter would be appreciated. If you know of any research papers that discuss this topic or if you have seen somewhere an implementation that does not use casual masking during inference, please share a link.</p>
","transformer"
"42075","Why is dot-product and not Euclidean distance used for attention?","2023-09-13 04:23:21","","1","468","<transformer><deep-neural-networks><attention>","<p>In models using attention (eg Transformer architectures) we used scaled dot-product to measure similarity rather than (negative or inverse) Euclidean distance. Why is this the case?</p>
<p>Does Layer Normalization make these basically identical anyways? Or are there statistical, geometric, or training dynamics problems caused by treating embeddings as locations rather than directions? Or is this choice due to the extra computational overhead of using Euclidean distance?</p>
","transformer"
"42005","Why is an encoder + decoder model with L by L layers the same speed as as decoder only model with 2 L layers?","2023-09-05 22:46:35","","2","86","<natural-language-processing><transformer>","<p>I was watching this lecture: <a href=""https://youtu.be/27rNqGrTdSI?t=2295"" rel=""nofollow noreferrer"">https://youtu.be/27rNqGrTdSI?t=2295</a></p>
<p>In it the presenter stated that:</p>
<p>&quot;An encoder + decoder model with L by L layers is actually the same speed as as decoder only model with 2 L layers because in the end the input and targets still have to be processed and decoder only concatenates inputs and targets to do most tasks. Technically, Encoder Decoder models are a form of sparsity, they are also sequentially sparse, they have dedicated parameters to inputs and targets, they different parameters for inputs and targets, parameter to flop ratio behaves differently from decoder only models&quot;</p>
<p>Why is &quot;An encoder + decoder model with L by L layers the same speed as as decoder only model with 2 L layers&quot; true?</p>
","transformer"
"41963","Do I need to manually segment/augment my data for transformer training?","2023-09-02 23:51:54","","0","36","<transformer>","<p>If my dataset consists of the sentence &quot;I have an apple&quot; do I need to feed my model the separate examples &quot;I&quot; -&gt; &quot;have&quot;, &quot;I have&quot; -&gt; &quot;an&quot;, and &quot;I have an&quot; -&gt; &quot;apple&quot; or is that essentially what the masking does, and I only have to feed in the entire sentence at once?</p>
","transformer"
"41887","Does Embeddings and Vector Databases solve the need of having longer context windows?","2023-08-26 02:27:00","","0","163","<transformer><large-language-models>","<p>I am learning to use the OpenAI API to build LLM-based agents. I recently came across the concept of vector databases, which use embeddings to convert text into vectors and store them in a database for easy retrieval. This technique has been shown to be very useful for long-memory applications.</p>
<p>My question is whether it is necessary to have longer context windows in models that use the embedding + database technique. I know that there has been some research on expanding the context window of transformers, such as the <a href=""https://arxiv.org/abs/2304.11062"" rel=""nofollow noreferrer"">Scaling Transformer to 1M tokens and beyond with RMT</a> paper and <a href=""https://arxiv.org/abs/2307.02486"" rel=""nofollow noreferrer"">Microsoft's LongNet</a> architecture. At what point does increasing the context window lead to better performance than using a vector database approach? Are there any examples or experiments that demonstrate this?</p>
","transformer"
"41875","Backpropagation in a transformer","2023-08-25 03:06:47","","0","75","<python><transformer><pytorch><time-series>","<p>I have a transformer for timeseries forcasting based on this article <a href=""https://arxiv.org/abs/2001.08317"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2001.08317</a></p>
<p>Given a source containing <span class=""math-container"">$src=(x_{t-5},x_{t-4},x_{t-3},x_{t-2},x_{t-1})$</span> and a target of <span class=""math-container"">$tgt=(x_{t-1},x_{t},x_{t+1})$</span>, the model then tries to output <span class=""math-container"">$out=(x_t,x_{t+1},x_{t+2})$</span>.</p>
<p>When training, I currently provide the model with the <span class=""math-container"">$src$</span> and <span class=""math-container"">$tgt$</span> to produce <span class=""math-container"">$out$</span>, which I then compare with real data using the LMSE metric. This approach seems to have limitted success.</p>
<p>However, similar to the NLP tasks, I want to first input <span class=""math-container"">$src$</span> and <span class=""math-container"">$tgt=(x_{t-1})$</span> to the model. Then, I could measure the loss and backpropagate before trying training again on <span class=""math-container"">$src$</span> and <span class=""math-container"">$tgt=(x_{t-1},x_{t})$</span> and continue like this until the model predicts the required length of the output.</p>
<p>Alternatively, I could do the same thing, but backpropagate only in the end.</p>
<p>Which of these strategies is better? Is there an alternative that comes to mind?</p>
","transformer"
"41858","How can BERT/Transformer models accept input batches of different sizes?","2023-08-23 23:22:40","","0","336","<neural-networks><natural-language-processing><transformer><bert>","<p>I understand that all inputs in a batch need to be of the same size. However, it seems BERT/Transformers models can accept batches with different sizes as input.</p>
<p>How is that possible? I thought we needed to pad all examples in a batch to <code>model.max_input_size</code>, however, it seems HuggingFace does <code>Dynamic Padding</code> that allows sending batches of different lengths (till the time they are smaller than <code>max_input_size</code>)</p>
<p><a href=""https://i.sstatic.net/zC1WS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zC1WS.png"" alt=""enter image description here"" /></a></p>
<p>Link: <a href=""https://mccormickml.com/2020/07/29/smart-batching-tutorial/"" rel=""nofollow noreferrer"">https://mccormickml.com/2020/07/29/smart-batching-tutorial/</a><br />
Link2: <a href=""https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding</a></p>
","transformer"
"41813","How does GPT like Decoder only conversational models distunguish the source of text?","2023-08-18 20:49:01","","0","62","<natural-language-processing><transformer><chat-bots><gpt><seq2seq>","<p>In a conversational setting where two sources of text (user and the model) follow each other like below</p>
<p>User: some text bla bla
Model: another text bah bah
User: bla bla bla
Model: bah bah</p>
<p>and so on, how does the model differentiate the texts written by the user and the model?
I assume in an encoder-decoder setting (like T5 or BART), texts from two sources can be differentiated by giving user texts as encoder input and model's earlier responses as decoder input.</p>
<p>How about GPT-like Decoder only models?
Relating to that, what is the common method to train models for long conversational setting like above?</p>
","transformer"
"41802","Concatenation of Feature vectors in transformers before passing to fcnn","2023-08-17 12:08:07","","1","284","<machine-learning><natural-language-processing><transformer><attention>","<p>** As I am new to the field , the question might feel little abstract and naïve considering my experience.
I am studying the Transformer architecture and trying to understand the various components within it. I am not able to understand the purpose of concatenating the attention head vectors, when the researchers could have used some other method to fuse the vectors. I wanted to understand the intuition behind that. I got a very good conversation on stack exchange itself (<a href=""https://ai.stackexchange.com/questions/20948/combine-two-feature-vectors-for-a-correct-input-of-a-neural-network?newreg=666bdbcf8c0f4470ba6d740b00f4050f"">Combine two feature vectors for a correct input of a neural network</a>). Taking the guidance from the link I wanted to understand whether the attention head outputs are linearly separable and that's why the researchers decided to concatenate the vectors? Or is it something related to direct vector sum for subspaces (not sure), or there is any proof relating to Information Theory wherein somebody has shown that the information loss in lesser that way (I am speculating). Please guide me through the concept.</p>
","transformer"
"41783","Can pretraining be continued after RLHF?","2023-08-16 08:02:15","","0","90","<reinforcement-learning><transformer><unsupervised-learning><pretrained-models><rlhf>","<p>Assume you have a pretrained transformer language model (M1) which already underwent reinforcement learning by human feedback (M2). I assume that it is in principle possible to continue the pretraining after RLHF with some additional documents, e.g. high-quality scientific papers (e.g. the whole arXiv), yielding M3.</p>
<p>My question is: Would there be a difference in the quality of answers to scientific questions (that are covered in the additional documents), between M3 (pretraining+RLHF+pretraining) and a model M4 for which pretraining with the additional documents was continued immediately on M1 (pretraining+pretraining+RLHF)?</p>
<p>The question concerns the interference of pretraining and RLHF. RLHF of course has to be performed on a pretrained model (and doesn't affect the linguistic and implicit world knowledge of the pretrained model too much), but continuation of pretraining only after RLHF might cause more trouble: both general world and aligned knowledge might be reduced, and scientific knowledge not so much advanced.</p>
<p>Is there an argument that makes it plausible that pretraining+RLHF+pretraining should work well -- or on the contrary would not work at all?</p>
","transformer"
"41670","Why use exponential and log in Positional Encoding of Transformer","2023-08-06 00:25:59","","0","438","<transformer><positional-encoding>","<p>This code snippet is from <a href=""https://huggingface.co/blog/annotated-diffusion"" rel=""nofollow noreferrer"">here</a> under the section named &quot;Position embeddings&quot;.</p>
<pre><code>class SinusoidalPositionEmbeddings(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, time):
        device = time.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings
</code></pre>
<p>The code is implementing Positional Encoding that was introduced in Transformer model.<br />
I don't understand why we have to use <code>torch.exp</code> and <code>math.log</code>. Is this related to scaling or non-negative issue?</p>
<p>Many thanks ahead!</p>
","transformer"
"41653","Understanding embedding outputs in transformer models like CLIP","2023-08-04 15:33:47","","0","387","<transformer><embeddings>","<p>I'm working with OpenAI's CLIP model and trying to understand the output of the text encoder. When I input a short prompt like &quot;cat&quot;, the output is a tensor of shape [77, 1024]. My understanding is that the 1024 represents the dimensionality of the embeddings, and the 77 represents the maximum sequence length that the model can handle.</p>
<p>Given that &quot;apple&quot; would be tokenized into far fewer than 77 tokens, I'm assuming that the remaining tokens are padding tokens. However, when I inspect the tensor, I don't see any zero values. I was expecting the embeddings for the padding tokens to be zero vectors, but this doesn't seem to be the case.</p>
<p>My current hypothesis is that only the first few 1024-dimensional vectors in the tensor (corresponding to the tokens in my input) are significant, and the remaining vectors (corresponding to padding tokens) do not carry meaningful information about my input. Is this understanding correct?</p>
<p>Also, could someone explain why the embeddings for the padding tokens are not zero vectors? How does the model ensure that these padding tokens do not contribute to the output?</p>
","transformer"
"41532","diagonal of the hypercube in cross-attention","2023-07-29 06:48:14","","1","47","<machine-learning><transformer><attention>","<p>In Cross-attention, divide the similarity between <span class=""math-container"">$Q$</span> and <span class=""math-container"">$K$</span> by <span class=""math-container"">$\sqrt{d_k}$</span>. <br>
Here <span class=""math-container"">$\sqrt{d_k}$</span> is the diagonal of the hypercube, is this a coincidence or famous theorem?
<span class=""math-container"">$$ \frac{QK^T}{\sqrt{d_k}} $$</span>
<a href=""https://i.sstatic.net/OiFF9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/OiFF9.png"" alt=""enter image description here"" /></a></p>
","transformer"
"41508","Confusion About Triangle Mask in Transformer Decoder","2023-07-27 20:04:05","","0","219","<transformer><language-model>","<p>I have some confusion about the implementation of the triangle mask in the transformer decoder. I understand the reasoning for the mask, it prevents the network from 'cheating' by looking ahead at the next token, but the way it's implemented seems strange. First of all, why do we apply the mask to the product <span class=""math-container"">$QK^T$</span>? Why not just set the rows representing unseen tokens to zero or some special token before sending the matrix to the transformer? Also, I don't understand the reasoning of a triangle mask in general. Suppose we are training with a sequence of length <span class=""math-container"">$\ell_s$</span>, and we are trying to predict token <span class=""math-container"">$k$</span> with <span class=""math-container"">$k &lt; \ell_s$</span>. Why don't we have to mask all entries <span class=""math-container"">$QK^T_{i,j}$</span> if <span class=""math-container"">$i &gt; k$</span> or <span class=""math-container"">$j &gt; k$</span>? With a standard triangle mask it seems like we still incorporate information computed from 'unseen' tokens.</p>
","transformer"
"41505","Which situation will helpful using encoder or decoder or both in transformer model?","2023-07-27 14:41:18","41540","5","3200","<transformer><encoder-decoder><vision-transformer>","<p>I have some questions about using (encoder / decoder / encoder-decoder) transformer models, included (language) transformer or Vision transformer.</p>
<p>The overall form of a transformer consists of an encoder and a decoder. Depending on the model, you may use only the encoder, only the decoder, or both. However, for what purpose do model designers use only encoders, only decoders, or both?</p>
<p>I already knew that encoders in transformers are as known as taking in a sequence of input data and generates a fixed-length representation of it. This representation can then be fed into the decoder to generate an output sequence.
In other words, the encoder can be thought of as a kind of compression that extracts features from the data. And the decoder can be thought of as playing a role in returning the compressed information in the encoder to its original state.
So I'm wondering why some models work without having both an encoder and a decoder.</p>
<p>Few days ago, I think use only encoders are useful to classifying classes. Because <strong>DOSOVITSKIY, Alexey, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</strong> paper shows only encoder to classification images. Decoders are useful in generative things, because <strong>WANG, Jianfeng, et al. Git: A generative image-to-text transformer for vision and language. arXiv preprint arXiv:2205.14100, 2022.</strong> paper using encoder to encode the visual information from the input image into a representation that can be used by the text decoder to generate text. Then, to generate text, they give the 'encoder's output and the text' as the decoder's input.</p>
<p>But, I am sure about that my think are wrong because of BERT and GPT. <strong>BERT</strong> using encoder and does not have a decoder. <strong>GPT</strong> uses decoder and does not have a encoder. A typical user thinks that BERT and GPT equally answer the question asked by the user. So they think BERT and GPT provide the same service. However, in terms of model structure, BERT and GPT are completely different.</p>
<h1>So, I have two questions about each functional part that makes up the transformer.</h1>
<ol>
<li>what does encoder and decoder do in transformer? The transformer referred to here can be text or image.</li>
<li>For what purpose do model designers use only encoders, only decoders, or both encoders and decoders?</li>
</ol>
<p>Thank you.</p>
","transformer"
"41485","While fine-tuning a decoder only LLM like LLaMA on chat dataset, what kind of padding should one use?","2023-07-26 00:31:09","41549","4","8176","<transformer><pytorch><padding>","<p>While fine-tuning a decoder only LLM like LLaMA on chat dataset, what kind of padding should one use?</p>
<p>Many papers use Left Padding, but is right padding wrong since transformers gives the following warning if using right padding &quot; A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set padding_side='left' when initializing the tokenizer.&quot;</p>
<p>The attention mask will anyways ignore the padding tokens.</p>
","transformer"
"41477","Why in Multi-Head Attention implementation should we use $3$ linear layers for Q, K, V instead of $3 * h$ layers?","2023-07-25 16:31:11","41479","1","674","<transformer><pytorch><attention><implementation><dense-layers>","<p>I have been trying to implement a Transformer architecture using PyTorch by following the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a> paper as well as the <a href=""http://nlp.seas.harvard.edu/annotated-transformer/"" rel=""nofollow noreferrer"">The Annotated Transformer</a> blog post to compare my code with theirs. And I noticed that in their implementation of the Multi-Head Attention they have used three <code>nn.Linear(d_model, d_model)</code> to project the input of the encoder before splitting these projections into <code>(n_heads, d_k)</code> matrices for the attention. But as my understanding of the paper goes, we need to have <code>n_heads</code> of <code>nn.Linear(d_model, d_k)</code> for each of the queries, keys and values as we can see in the Multi-Head Attention's diagram here from the paper:</p>
<p><a href=""https://i.sstatic.net/0alvY.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0alvY.png"" alt=""enter image description here"" /></a></p>
<p>We clearly see as many <code>nn.Linear</code> layers as there are of heads. As well as the explanation of the authors:</p>
<p><a href=""https://i.sstatic.net/0BWfC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0BWfC.png"" alt=""enter image description here"" /></a></p>
<p>Each <span class=""math-container"">$head_{i}$</span> uses <span class=""math-container"">$W_{i}^{Q}$</span>, <span class=""math-container"">$W_{i}^{K}$</span> and <span class=""math-container"">$W_{i}^{V}$</span>. So in my implementation I did this:</p>
<pre><code>class MultiHeadedAttention(nn.Module):
  def __init__(self, d_model=512, h=8):
    super(MultiHeadedAttention, self).__init__()
    self.d_model = d_model
    self.h = h
    self.d_k = d_model // h
    self.query_linears = nn.ModuleList([nn.Linear(d_model, self.d_k) for i in range(h)])
    self.key_linears = nn.ModuleList([nn.Linear(d_model, self.d_k) for i in range(h)])
    self.value_linears = nn.ModuleList([nn.Linear(d_model, self.d_k) for i in range(h)])
    self.projection_layer = nn.Linear(h * self.d_k, d_model)

  def forward(self, Q, K, V, mask=None):
    batch_size = Q.size(0)
    queries = torch.cat([linear(Q).view(batch_size, 1, -1, self.d_k) for linear in self.query_linears], dim=1)
    keys = torch.cat([linear(K).view(batch_size, 1, -1, self.d_k) for linear in self.key_linears], dim=1)
    values = torch.cat([linear(V).view(batch_size, 1, -1, self.d_k) for linear in self.value_linears], dim=1)

    x = scaled_dot_product_attention(queries, keys, values, mask)

    x = x.transpose(1, 2)
    x = x.contiguous()
    x = x.view(batch_size, -1, self.h * self.d_k)
    x = self.projection_layer(x)
    return x
</code></pre>
<p>But I'm surely missing a key piece of understanding. And I'd be really grateful if someone can point it out to me.</p>
<p>Thank you.</p>
","transformer"
"41440","Can transformers autoregressively generate a sequence of embeddings (instead of predictions)?","2023-07-24 02:20:26","","1","373","<machine-learning><transformer><embeddings><sequence-modeling>","<p>Is it theoretically possible to use a transformer architecture to autoregressively generate a sequence of embedding vectors, instead of discrete tokens?</p>
<p>For example, if I were to provide an input of a stream of audio embeddings in the format (batch_size, seq_len, embed_dim), would it theoretically be model be able to train a transformer to predict the next audio embedding in the sequence, using a linear layer instead of creating token embeddings?</p>
<p>If this is possible, what loss function would best be used in this scenario? (for non-discrete data)</p>
","transformer"
"41439","Modern graduate-level machine learning books with focus on generative models","2023-07-24 01:40:49","","1","592","<reference-request><transformer><academia><diffusion-models>","<p>I'm looking for a modern machine learning book with graduate-level treatment of more recent topics such as diffusion and generative models, transformers etc.</p>
<p>I have a hard copy of <em>Deep Learning</em> by Goodfellow and Bengio; while I liked the book and read it extensively when it was published, it is a bit dated now.</p>
<p>I'm considering <em><a href=""https://probml.github.io/pml-book/book2.html"" rel=""nofollow noreferrer"">Probabilistic Machine Learning: Advanced Topics</a></em> by Kevin Patrick Murphy. But maybe there are better alternatives.</p>
<p>I need this for my comprehensive Ph.D. examination. Any suggestions are very appreciated.</p>
<p>P.S. I'm also aware of this <a href=""https://ai.stackexchange.com/questions/23507/what-are-other-examples-of-theoretical-machine-learning-books"">stack post</a>, but the list of references there is quite dated or introductory.</p>
","transformer"
"41438","Can someone help me understand the intuition behind the query, key and value matrices in the transformer architecture?","2023-07-24 00:08:57","41442","9","2147","<neural-networks><transformer><attention>","<p>I have been working mechanically with transformers, hoping that with time clarity about what the query, key, and value matrices represent will develop; but I am still lost. Would greatly benefit from a simplified explanation.</p>
","transformer"
"41367","How is the padding mask incorporated in the attention formula?","2023-07-19 22:29:35","41383","1","643","<natural-language-processing><transformer><attention><sequence-modeling><padding>","<p>I have been looking for the answer in other questions but no one tackled that. I want to ask you how is the padding mask considered in the formula of attention?</p>
<p>The attention formula taking into account a causal mask is:
<span class=""math-container"">$Attention(Q, K, V) = softmax(\frac{QK^{T} + CausalMask}{\sqrt{d_{k}}})V$</span></p>
<p>But how do we add the padding mask? The aim of a padding mask is to mask the padding positions as they're used just to make the batching feasible. But I don't know how this mask is added in the Attention formula.</p>
<p>Does it make sense if we do element-wise multiplication of the Attention matrix with a tensor of ones of shape (batch size, sequence length, <span class=""math-container"">$d_{model}$</span>) and whatever sentence <span class=""math-container"">$s$</span> in that batch and wherever position <span class=""math-container"">$p$</span> is a padding token then the tensor[s, p, :] is zeros?</p>
<p>Thank you in advance for your help!</p>
","transformer"
"41352","Why does LLM inference cost scale in both input tokens and output tokens?","2023-07-19 02:55:02","41388","0","1798","<transformer><open-ai><embeddings><large-language-models><inference>","<h2>EDIT</h2>
<p>This question was flawed. See <a href=""https://ai.stackexchange.com/a/41388/68775"">my answer</a> with help from commenters.</p>
<hr />
<h2>Original question</h2>
<p>This question has been asked in other forums <a href=""https://community.openai.com/t/why-does-pricing-vary-by-input-tokens-instead-of-only-output-tokens/21833"" rel=""nofollow noreferrer"">[1]</a> <a href=""https://www.reddit.com/r/OpenAI/comments/11vvx66/whats_the_rationale_for_charging_for_input_tokens/"" rel=""nofollow noreferrer"">[2]</a> but I'm not sure I understand the claims, which are (EDIT: the following are based on my faulty assumption that pad tokens are added up to the maximum context window):</p>
<ol>
<li>Each forward pass takes less resources when more of the context window is padding.</li>
<li>Forward passes are run on the input tokens.</li>
<li>Forward passes with fewer non-pad input tokens are smaller tensor operations.</li>
</ol>
<p>Hypothesis 1 seems the most plausible to me from a performance engineering standpoint (sparse math, etc). Does it fall out naturally from just writing basic JAX code or would it require manual optimization (if so, what tricks can be used?)? There does <a href=""https://arxiv.org/abs/2210.03052"" rel=""nofollow noreferrer"">seem to be some research on this</a>.</p>
<p>Hypothesis 2 and 3 seem wrong based on my surface-level understanding of the Transformer architecture.</p>
<p>I've tried a few open-source LLMs locally and neither on those nor ChatGPT have I noticed any difference in latency based on how much text was in the context window. But I haven't done actual rigorous benchmarking yet.</p>
<p>The reason this is relevant is due to document lookup-based applications. Looking at the <a href=""https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"" rel=""nofollow noreferrer"">OpenAI cookbook for Q&amp;A using embeddings</a>, I can see that:</p>
<ul>
<li>most of the token usage seems to come from the long Wikipedia article pasted into the context</li>
<li>the price per query is quite prohibitive to frequent usage:</li>
</ul>
<blockquote>
<p>For gpt-3.5-turbo using ~1,000 tokens per query, it costs ~0.002 per query, or ~500 queries per dollar (as of Apr 2023)
For gpt-4, again assuming ~1,000 tokens per query, it costs ~0.03 per query, or ~30 queries per dollar (as of Apr 2023)</p>
</blockquote>
<p>My prior intuition would have been that optimal usage of LLMs would be to keep the context filled with inexpensive text (e.g. from NN search and/or cheaper LMs) and to have the LLM generate terse responses. But the input token cost model changes the strategy, as it means that users need to be sparing about the size and quantity of documents that they paste into the context window.</p>
","transformer"
"41274","Why does averaging attention-weighted positions reduce the effective resolution in transformers?","2023-07-13 18:03:33","43851","1","119","<neural-networks><transformer><attention><weights><multilayer-perceptrons>","<p>I was reading this <a href=""http://nlp.seas.harvard.edu/annotated-transformer/"" rel=""nofollow noreferrer"">blog post from Harvard</a> and it says in its background paragraph about transformers that the number of operations required to relate signals from two arbitrary input or output positions doesn't grow in the distance between positions with the transformers architecture <strong>but this comes at the &quot;cost of reduced effective resolution due to averaging attention-weighted positions&quot;</strong>.</p>
<p>I don't understand why does the averaging of attention-weighted position reduces the effective resolution. I have developed below the calculations needed to get the attention output, for the sake of simplicity I'm only considering one head:</p>
<p>The definition is:</p>
<p><span class=""math-container"">$A = Attention(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$</span></p>
<p>I think we can see this in two ways, either we see that <span class=""math-container"">$softmax(\frac{QK^{T}}{\sqrt{d_{k}}})$</span> is an <span class=""math-container"">$(m, m)$</span> matrix if <span class=""math-container"">$m$</span> is the number of input tokens, let us denote this matrix <span class=""math-container"">$M$</span>, and since <span class=""math-container"">$V$</span> is a the input matrix <span class=""math-container"">$X$</span> multiplied by a weight matrix <span class=""math-container"">$W^{V}$</span>, that gives us <span class=""math-container"">$A = M*X*W^{V}$</span> and we can clearly see that every element of <span class=""math-container"">$A$</span> is bound to all element in <span class=""math-container"">$X$</span> and that makes the resolution as high as possible.</p>
<p>We can also see it if we completely develop all the calculations inside the matrix <span class=""math-container"">$A$</span>. I will do just one element here for the sake of simplicity: <span class=""math-container"">$A_{1,1} = \sum_{j=1}^{m}\frac{e^{Q_{1}K_{j}^{T}}V_{j, 1}}{\beta_{1}}$</span></p>
<p>Where <span class=""math-container"">$m$</span> is the number of input tokens, <span class=""math-container"">$Q_{i}, K_{j}$</span> are the i-th and j-th rows of <span class=""math-container"">$Q$</span> and <span class=""math-container"">$K$</span> respectively and <span class=""math-container"">$v_{j, 1}$</span> is the first element of the j-th row in <span class=""math-container"">$V$</span>. And <span class=""math-container"">$\beta_{1} = \sum_{j=1}^{m}e^{Q_{1}K_{j}^{T}}$</span> (the softmax denominator).</p>
<p>Let's say <span class=""math-container"">$X$</span>, the input matrix is of shape <span class=""math-container"">$(m, d_{e})$</span>, then <span class=""math-container"">$Q = X*W^{Q}$</span> is of shape <span class=""math-container"">$(m, d_{k})$</span> and <span class=""math-container"">$K = X*W^{K}$</span> is of shape <span class=""math-container"">$(m, d_{k})$</span> too if we consider that <span class=""math-container"">$W^{Q}$</span> and <span class=""math-container"">$W^{K}$</span> are both of shape <span class=""math-container"">$(d_{e}, d_{k})$</span>.</p>
<p>Now, If we examine the product <span class=""math-container"">$Q_{1}K_{j}^{T}$</span>, it is <span class=""math-container"">$\sum_{l=1}^{d_{k}}Q_{1,l}K_{j,l}^{T}=\sum_{l=1}^{d_{k}}(\sum_{p=1}^{d_{e}}X_{1,p}W_{p,l}^{Q})(\sum_{p=1}^{d_{e}}X_{j,p}W_{p,l}^{k})$</span>, we can see that with the indices <span class=""math-container"">$p$</span> going from <span class=""math-container"">$1$</span> to <span class=""math-container"">$d_{e}$</span> and the <span class=""math-container"">$j$</span> index going from <span class=""math-container"">$1$</span> to <span class=""math-container"">$m$</span>, we cover all the elements of <span class=""math-container"">$X$</span> in the first position of <span class=""math-container"">$A$</span>. Granted that this is inside an exponential and that we divide by the softmax denominator. But if we develop the <span class=""math-container"">$V_{j, 1}$</span>, it is <span class=""math-container"">$\sum_{p=1}^{d_{v}}X_{j,p}W_{p,1}^{V}$</span>, and again we cover all the elements of <span class=""math-container"">$X$</span>.</p>
<p>My guess is that there is a subtlety in the claim of the article that stems from the &quot;effective resolution&quot;, but I wonder what a full resolution would be in that case, since even in the fully connected networks we have a linear transformation of inputs so we can argue that we're not directly using all the inputs.</p>
<p>I hope someone can clarify this for me and help me gain more understanding of the transformers architecture.</p>
<p>Thank you!</p>
","transformer"
"41249","What is considered the pre-fill, and what is considered the decoding phase in this process?","2023-07-12 11:35:18","","1","2283","<machine-learning><deep-learning><natural-language-processing><transformer><large-language-models>","<p>I've seen conflicting information about this online so I'm looking for clarification. I'm dealing with the causal LLaMAF model specifically.</p>
<p>I used to think that a sequence of tokens is generated in, and a sequence of probabilities for the next token in the sequence is generated as output. This generated output token is then appended to the sequence of tokens, and fed in the model again.</p>
<p>However, I know understand that there's a key value cache that's generated for each sequence of tokens fed in. This key value cache stores a precomputed matrix that can be used in future computations to prevent having to recompute previously seen tokens.</p>
<p>So, the new workflow is feed in token list -&gt; generate next token and key value cache -&gt; feed in next token -&gt; generate next token and key value cache for current token</p>
<p>What is considered the pre-fill, and what is considered the decoding phase in this process? Does the prefill phase involve feeding through the token list one by one passes to generate the kv cache? Why does the prefill phase take significantly longer?</p>
","transformer"
"41214","How do open source LLMs compare to GPT-4?","2023-07-09 08:54:25","","5","3474","<transformer><open-ai><large-language-models><gpt-4><open-source>","<p>I have heard some back and forth regarding open source LLMs like Llama.</p>
<p>I have heard that on certain benchmarks they perform close, the same or better than GPT-4, but caveats that they tend to lack the diversity and range of GPT-4, and also fail to be equivalent in ways certain benchmarks or metrics don’t capture fully.</p>
<p>GPT-4 has about 170 trillion parameters, I believe?</p>
<p>It seems like the biggest open source models are all in the billions - like Bloom or the new <a href=""https://huggingface.co/tiiuae/falcon-40b-instruct"" rel=""noreferrer"">Falcon 40b</a>.</p>
<p>There are techniques where they refine GPT-4’s output into a smaller amount of training data that supposedly hits all the marks and does just as well; but again, I don’t know if that’s only true under the reductionist of view of a particular benchmark-questionnaire.</p>
<p>So, do open source models actually compete with GPT-4, and why or why not? Is the whole situation a matter of scale, that a commercial venture like OpenAI can foot the massive bill of training a multi-trillion parameter model that no open source AI project can afford, on top of them having expertise in model design, making GPT-4 continually the state-of-the-art? Or is there any open source model that truly can compare in terms of usability?</p>
","transformer"
"41176","Inference process and flow, and role of GPU, CPU, and RAM","2023-07-06 18:58:50","","1","84","<transformer><gpu><hardware><inference><memory>","<p>This is a noob question.</p>
<p>I load a HuggingFace transformer model into GPU and create a HuggingFace pipeline using that model. Then I run inference on the model using the pipeline.</p>
<p>I would be glad to read in some depth about the actual process flow of the data, in particular the role of GPU, CPU, and RAM in this process.</p>
<p>For instance,</p>
<ol>
<li>I see a spike in CPU usage when I run inference. What causes it?</li>
<li>If I have multiple CPUs, and run multiple inference tasks simultaneously, will they be parallelized?</li>
<li>Does it make sense to use something like joblib for inference? Given that I am loading the model into GPU.</li>
</ol>
","transformer"
"41163","Does fine-tuning a multilingual transformer model allow it to generalize to languages unseen in the fine-tuning dataset?","2023-07-06 00:48:59","41165","0","146","<natural-language-processing><transformer><fine-tuning>","<p>Example: <a href=""https://huggingface.co/google/umt5-base"" rel=""nofollow noreferrer"">https://huggingface.co/google/umt5-base</a></p>
<blockquote>
<p>Note: UMT5 was only pre-trained on mC4 excluding any supervised training. Therefore, this model has to be fine-tuned before it is useable on a downstream task.</p>
</blockquote>
<p>The model was pre-trained on a whole lot of languages. Let's suppose I devise a fine-tuning dataset to classify sentences (like user feedback). But due to resource constraints, only a few languages will be included. In my fine-tuned task, will the model be able to generalize to languages it was pre-trained, but not fine-tuned on?</p>
","transformer"
"41161","How can an decoder-only transformer be used for document embedding?","2023-07-05 21:06:47","41328","5","2874","<transformer>","<p>GPT3 and 4 are both examples of decoder-only models. However OpenAI offers an text embedding API endpoint based on these models. This begs the general question how can one obtain text embeddings from a decoder-only transformer model?</p>
","transformer"
"41062","When do we apply a mask onto our padded values during attention mechanisms","2023-06-30 14:02:21","","0","833","<transformer><attention>","<p>When we are applying a mask onto the padded values in an input sequence, it is typically done through setting the padded values as negative infinity. For example, a tensor of values <code>[1,2,3,0,0]</code> should result in a padding mask of <code>pad_mask = [True, True, True, False, False]</code> (or the opposite depending on your flavour). However, if we apply the mask i.e <code>attention_scores = attention_scores.masked_fill_(pad_mask.T == False, float('-inf')) </code>before applying softmax, won't we get the 4th and 5th row of the attention_scores as 'nan' when we softmax attempts to calculate the probability distribution along each row?</p>
<p>Does that mean the step of where to apply the mask is incorrect, and we should apply a zero-ing out of the pad token rows in the attention_score matrix after applying the softmax function? or is there another key concept/step I am missing here</p>
","transformer"
"40917","What if we drop the causal mask in auto-regressive Transformer?","2023-06-21 20:55:04","","2","2100","<natural-language-processing><training><transformer><attention><inference>","<p>I understand the triangular causal mask in the attention is used to prevent tokens from <a href=""https://ai.stackexchange.com/questions/23889/what-is-the-purpose-of-decoder-mask-triangular-mask-in-transformer"">&quot;looking into the future&quot;</a>, but why do we want to prevent that?</p>
<p>Let's suppose we have a model with context length <span class=""math-container"">$T = 8$</span>. At inference time, we want to predict the 5th token with the previous 4 ones, so we truncate the upper-left submatrix and the causal mask looks like the following</p>
<pre><code>[1, 0, 0, 0]
[1, 1, 0, 0]
[1, 1, 1, 0]
[1, 1, 1, 1]
</code></pre>
<p>In this case, the model has no access to the 5th token because we did not feed it into the model in the first place, so it certainly cannot cheat by peeking ahead at the ground truth. Why do we still need the causal mask?</p>
<p>Moreover, the second row <code>[1, 1, 0, 0]</code> prevents the 2nd token from &quot;attending to&quot; the 3rd and 4th tokens. This makes sense if we are trying to predict the 3rd token, but we are actually predicting the 5th one. Why don't we allow available tokens to attend to each other?</p>
","transformer"
"40866","Why is it said transformers are more parallelizable than RNN's?","2023-06-15 21:36:05","","2","343","<recurrent-neural-networks><transformer><batch-size>","<p>The parallelization of transformers and RNNs (Recurrent Neural Networks) is often discussed. It's commonly said that transformers are more parallelizable than RNNs. However, this is a rather vague statement that merits further discussion.</p>
<p>One could argue that an RNN can be made as parallelizable as desired by simply adding more instances to each batch.</p>
<p>What is generally meant by saying transformers are more parallelizable is that transformers lack time-dependent operations. In other words, given an input, all operations can be done at once (although not all, since one layer needs to be computed before the next).</p>
<p>Contrast this with an RNN, where computations from one time step are carried forward and used in the next. Some people argue that this time-dependence makes RNNs less parallelizable.</p>
<p>However, one could also argue that to achieve a level of parallelization in an RNN similar to that in a transformer, one could simply increase the batch size by a factor equal to the number of time steps in the sequence. This way, both the transformer and the RNN would perform the same training in the same time frame.</p>
<p>A potential counterpoint to this argument might be that this would increase the memory requirements for the RNN, as the entire unfolding of the network needs to be stored. However, the memory requirement for an RNN grows linearly with the size of the input, whereas for a transformer, it grows quadratically.</p>
<p>Therefore, one could argue that, given their linear memory growth, RNNs should actually require less memory than transformers, making them just as parallelizable.</p>
<p>I look forward to getting some clarity on this issue. Does the parallelization advantage of transformers over RNNs lie in something more than just memory and time-step computations?</p>
<p>I understand there might be other reasons why to choose a transformer over a RNN, for instance the vanishing gradient problem, but I'm particularly interested in the statement &quot;Transformers are more parallelizable than RNN's&quot; because it doesn't seem obvious to me why this would be true.</p>
<p>There might also be good reasons for not wanting to increase the batch size, I'm interested on that too.</p>
","transformer"
"40851","Why shouldn't the attention matrices $W^Q$, $W^K$, $W^V$ be the same?","2023-06-15 05:50:36","","0","736","<natural-language-processing><transformer><attention><explainable-ai><vector-semantics>","<p>My question is why the attention head matrices <span class=""math-container"">$W^Q$</span>, <span class=""math-container"">$W^K$</span>, <span class=""math-container"">$W^V$</span> should not be the same <span class=""math-container"">$W = W^Q =W^K= W^V$</span>. In my understanding of transformer-based language models  <strong>one</strong> attention head is responsible for <strong>one</strong> syntactic or semantic relation between any two words in the context. One might think that such a relation is represented by <strong>one</strong> matrix <span class=""math-container"">$W$</span> that projects the full word embeddings <span class=""math-container"">$x_i$</span> from their full semantic space to a semantic subspace responsible for this relation. Here we could - in principle - calculate scores <span class=""math-container"">$\sigma_{ij}$</span> as &quot;similiarities&quot; between two projected words <span class=""math-container"">$Wx_i$</span> and <span class=""math-container"">$Wx_j$</span> and then calculate the weighted sum of the projected tokens <span class=""math-container"">$Wx_k$</span>.</p>
<p>I wonder why this would not work, and why we need three different matrices.</p>
<p>Another way around: What does it mean to calculate the score as the dot-product of two vectors from two different semantic subspaces? Is this still some kind of similiarity (which lies at the heart of word embeddings)? And doesn't it sound like comparing apples and pears?</p>
<p>Or viewed differently: How similar are the three matrices of an attention head in practice, e.g. when considering some 100<span class=""math-container"">$\times$</span>100 attention heads of a large transformer model like ChatGPT?</p>
","transformer"
"40792","Can transformer models be used to convert code from one programming language to another?","2023-06-10 15:03:33","","0","241","<python><transformer><open-ai>","<p>There was a <a href=""https://ai.stackexchange.com/questions/17140/can-sequence-to-sequence-models-be-used-to-convert-source-code-from-one-programm"">question</a> like this in 2019. I hope things have changed since then.</p>
<p>Concretely, I am looking for a way to train a transformer model to convert code from SAS to Python. I guess the method does not depend on the pair of programming languages requested as long as one has enough data for training.</p>
<p>There are paid tools, e.g. <a href=""https://www.codeconvert.ai/"" rel=""nofollow noreferrer"">CodeConert</a> and <a href=""https://sas2py.com/solutions.html"" rel=""nofollow noreferrer"">sas2py</a>, that do this, but I can not find out what the are based on and how they were built.</p>
<p>It appears that one (very bad) way could be to use interpreters that translate one programming language, e.g. Java, to English and then from English to the second programming language, e.g. Python. This must be very prone to mistakes and seems like a wrong approach.</p>
","transformer"
"40750","Why can decoder-only transformers be so good at machine translation?","2023-06-07 18:46:38","","1","811","<transformer><machine-translation><encoder-decoder>","<p>In my understanding encoder-decoder transformers for translation are trained with sentence or text pairs. How can it be explained in simple (high-level) terms that decoder-only transformers (e.g. GPT) are so <a href=""https://arxiv.org/pdf/2302.09210.pdf"" rel=""nofollow noreferrer"">good at machine translation</a>, even though they are not trained on sentence or text pairs but only on unrelated multilingual training data? Why can decoder-only transformers do without it? Or did I get something wrong?</p>
<p>Are the documents in the training data containing accidentally sentence pairs near each other possibly enough?</p>
","transformer"
"40740","Can you confirm that the transformer works strictly deterministically and there is no randomness inside or between the attention layers?","2023-06-07 10:53:52","","5","1749","<transformer><attention><randomness><layers>","<p>On a high-level <a href=""https://www.linkedin.com/pulse/temperature-check-guide-best-chatgpt-feature-youre-using-berkowitz/"" rel=""nofollow noreferrer"">temperature and randomness affect the output of a generative language model</a>:</p>
<ol>
<li><p>Lower temperature: Produces more focused, conservative, and consistent responses.</p>
</li>
<li><p>Moderate temperature: Strikes a balance between creativity and consistency. This setting can be useful for general content generation, where a blend of accuracy and inventiveness is desired.</p>
</li>
<li><p>Higher temperature: Generates more creative, diverse, and unexpected outputs.</p>
</li>
</ol>
<p>What I'm not sure of is <strong>where exactly</strong> randomness (controlled by the temperature) comes into play. I believe to have understood that it's only after a transformer has done its deterministic work, suggesting some probable next words.</p>
<p>Can you confirm that the transformer works strictly deterministically and there is no randomness inside or between the attention layers?</p>
","transformer"
"40686","Are transformer models better than comparable-complexity MLP-based models?","2023-06-02 13:46:28","","3","648","<transformer><multilayer-perceptrons>","<p>I've watched the outstanding Andrej Karpathy's <a href=""https://www.youtube.com/watch?v=VMj-3S1tku0&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&amp;index=1&amp;t=0s"" rel=""nofollow noreferrer"">From Zero to Hero course</a>. In the last lecture, he introduces Transformer decoder architecture, which is able to produce Shakespear-like text. However, there was no direct comparison of the achieved cross-entropy loss (~1.4) with simple MLP models he talked about in the first 5 lectures.</p>
<p>What if one trains an MLP based model with a similar number of parameters/layers, the same context length and also including layer normalization, feed forward and dropout, would the result be substantially worse? Would the training take longer? Are there direct comparisons like that in the literature?</p>
","transformer"
"40628","Is there any reference about backpropagation of the Transformer's multi-head layer?","2023-05-29 03:03:20","","1","363","<reference-request><transformer><backpropagation>","<p>Is there any reference about backpropagation of the Transformer's multi-head layer or multi-head attention (MHA)? I have searched various journals but have not found one yet.</p>
","transformer"
"40614","How can a transformer encoder attend to future tokens?","2023-05-27 18:24:15","","2","269","<transformer>","<p>What does attending to future tokens mean? From my understanding, the transformer model works by inputting a prompt and predicting the next word in a sequence and this process just keeps repeating while attending to the words from the prompt and the already generated words. However, the explanations for transformers all mention the prevention of attending to future tokens while generating the next word in a sequence. How do these models attend to a word that hasn’t been generated yet?</p>
","transformer"
"40607","How to use deep relative trust to measure the distance between two RNNs, and between two transformers?","2023-05-27 03:22:26","","0","30","<recurrent-neural-networks><transformer>","<p>I want to measure how different two networks, i.e., between two RNNs and between
two transformers. I read that <a href=""https://proceedings.neurips.cc/paper/2020/file/f4b31bee138ff5f7b84ce1575a738f95-Paper.pdf"" rel=""nofollow noreferrer"">Deep relative trust</a> can be used to measure the distance between two NNs. Can it be used on RNNs and Transformers?</p>
<p>Because in the paper, as far as I understand, it is only applied to MLPs?
Can I simply loop for each layer and compute the deep relative trust?</p>
<p>I want to plot the difference (distance) between two neural nets (as the x axis) vs the differece between the two nets' generated solution (as the y axis)?</p>
","transformer"
"40590","What is MLM & NSP loss function","2023-05-26 05:01:58","","1","876","<natural-language-processing><transformer><objective-functions><bert>","<p>Two objective functions are used during the BERT language
model pretraining step.</p>
<p>The first one is masked language
model (MLM) that randomly masks
15% of the
input tokens and the objective is to predict the vocabulary</p>
<p>The second objective
is the next sentence prediction (NSP) task. This is a binary
classification task for predicting whether two sentences are
subsequent in the original text.</p>
<p>I am looking for such objective function as mathematical definition.</p>
","transformer"
"40497","Should I be layer freezing when fine-tuning an LLM?","2023-05-18 16:07:04","","3","3323","<transformer><performance><fine-tuning><catastrophic-forgetting>","<p>I've had it in my head that generally speaking, it's better to freeze layers when fine-tuning an LLM, as per this quote from HuggingFace's <a href=""https://huggingface.co/blog/peft"" rel=""nofollow noreferrer"">article</a>:</p>
<blockquote>
<p>PEFT approaches only fine-tune a small number of (extra) model
parameters while freezing most parameters of the pretrained LLMs,
thereby greatly decreasing the computational and storage costs. This
also overcomes the issues of catastrophic forgetting, a behaviour
observed during the full finetuning of LLMs. PEFT approaches have also
shown to be better than fine-tuning in the low-data regimes and
generalize better to out-of-domain scenarios. It can be applied to
various modalities, e.g., image classification and stable diffusion
dreambooth.</p>
</blockquote>
<p>I think what I might be confused by is what is meant by the &quot;(extra)&quot; part. It led me to try fine-tuning a BERT model in PyTorch by freezing all parameters except for the final feed-forward of the transformer responsible for sequence classification:</p>
<pre><code>for param in model.parameters():
    param.requires_grad = False

for param in model.classifier.parameters():
    param.requires_grad = True
</code></pre>
<p>However, this caused my model to get <em>significantly</em> worse evaluation metrics on my test set than before I did this. This lead me to the following conclusions:</p>
<ul>
<li><p>My dataset of ~100K datapoints is not of a &quot;low-data regime&quot; and therefore doesn't benefit from PEFT? But doesn't it say this generalizes better to &quot;out-of-domain scenarios&quot;? How do I know the particular seq classification I'm doing with BERT is out-of-domain? Because it isn't specifically a next-sequence prediction task?</p>
</li>
<li><p>Is this the cost of misinterpreting the &quot;(extra)&quot; model parameters part? I'm fine-tuning a small number of extant model parameters here, not extra.</p>
</li>
</ul>
<p>I'm just confused here. The quote I've showed here makes me believe my PEFT model should've outperformed a regular fine-tuning.</p>
","transformer"
"40493","Do batches need to be sequential in Transformer traning?","2023-05-18 09:33:18","40494","0","99","<transformer><pytorch>","<p>When training a transformer model (I'm using nn.TransformerEncoder from pytorch) is it better to use sequential batches (for example, three sequences <code>[[1,2,3,4], [5,6,7,8], [9,10,11,12]]</code> where next sequence continues where the previous ended)?</p>
<p>Or can I use non-sequential or randomly picked sequences from my dataset (for example, here are three randomly picked sequences <code>[[1,2,3,4], [56, 57, 58, 59], [22, 23, 24, 25]]</code>)?</p>
<p>I can make a test myself, but I was just wondering if that's already known.</p>
","transformer"
"40451","Shape of biases in Transformer's Feedforward Network","2023-05-14 14:10:11","40575","0","54","<transformer><feedforward-neural-networks><linear-algebra>","<p>In transformer network (<a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Vaswani et al., 2017</a>), the feedforward networks have equation:</p>
<p><span class=""math-container"">$$\mathrm{FNN}(x) = \max(0, xW_1 + b_1) W_2 + b_2$$</span></p>
<p>where <span class=""math-container"">$x \in \mathbb{R}^{n \times d_\mathrm{model}}$</span>, <span class=""math-container"">$W_1 \in\mathbb{R}^{d_\mathrm{model} \times d_{ff}}$</span>, <span class=""math-container"">$W_2 \in\mathbb{R}^{d_{ff} \times d_\mathrm{model}}$</span>.</p>
<p>We know that the biases <span class=""math-container"">$b_1$</span> and <span class=""math-container"">$b_2$</span> are vectors.</p>
<p>But, for the equation to work the shape of <span class=""math-container"">$b_1$</span> and <span class=""math-container"">$b_2$</span> must agree, i.e., <span class=""math-container"">$b_1 \in\mathbb{R}^{n \times d_{ff}}$</span> and <span class=""math-container"">$b_2 \in\mathbb{R}^{n \times d_\mathrm{model}}$</span>.</p>
<p>My question: is it true that</p>
<p><span class=""math-container"">$b_1 = \begin{bmatrix} (b_1)_{1} &amp; (b_1)_{2} &amp; \dots &amp; (b_1)_{d_{ff}}\\ (b_1)_{1} &amp; (b_1)_{2} &amp; \dots &amp; (b_1)_{d_{ff}} \\ \vdots &amp; \vdots &amp;  &amp; \vdots \\ (b_1)_{1} &amp; (b_1)_{2} &amp; \dots &amp; (b_1)_{d_{ff}} \end{bmatrix}$</span>
and
<span class=""math-container"">$b_2 = \begin{bmatrix} (b_2)_{1} &amp; (b_2)_{2} &amp; \dots &amp; (b_2)_{d_\mathrm{model}}\\ (b_2)_{1} &amp; (b_2)_{2} &amp; \dots &amp; (b_2)_{d_\mathrm{model}} \\ \vdots &amp; \vdots &amp;  &amp; \vdots \\ (b_2)_{1} &amp; (b_2)_{2} &amp; \dots &amp; (b_2)_{d_\mathrm{model}} \end{bmatrix}$</span> ?</p>
","transformer"
"40447","Does ChatGPT use different transformers for different downstream tasks?","2023-05-14 10:16:25","","3","1142","<natural-language-processing><transformer><chatgpt><multi-task-learning>","<p>What I find hard to figure out is whether ChatGPT guesses from the prompt the downstream NLP task to be performed - text summary, text generation, question-answering, doing logic or arithmetic, translation,  sentiment or style analysis - and then uses specialized decoders/transformers. Or if there is only one transformer which handles all downstream tasks. How then can it be understood that ChatGPT performs so well in so many tasks - <strong>as if</strong> it used specialized transformers.</p>
<p>If it guesses the task: How is it done (in high-level terms) and how does it switch?</p>
<p>The answer may be so clear (for the experts) that it is never mentioned explicitly, but for the non-expert it is hard to tell (and to believe).</p>
<p>(Maybe it's easier to answer the question if there is a specific and specifically trained transformer for each supported language.)</p>
<p>BTW: Why is the task &quot;to follow instructions&quot; (which InstructGPT is said to be specialized for) a task on its own? Isn't every prompt an instruction in a sense, instructing ChatGPT to perform some downstream task?</p>
","transformer"
"40443","Would initializing transformers with pre-trained word embedding speed up the training of transformers?","2023-05-13 23:27:52","","3","375","<deep-learning><transformer><word-embedding>","<p>I read the answers for that question <a href=""https://ai.stackexchange.com/questions/26235/what-kind-of-word-embedding-is-used-in-the-original-transformer"">What kind of word embedding is used in the original transformer?</a>. It says that transforms like bert start the first word embedding layer with random values.</p>
<p>Initializing the first word embedding layer in transformers with random values works fine but Wouldn't initializing transformers with pre-trained word embedding speed up the training of transformers?</p>
<p>Isn't starting with pre-trained word embedding(vectors that have semantic meaning) is better than starting from scratch?</p>
<p>I am not talking about the performance. I am talking about the speed of the training.</p>
","transformer"
"40425","How are the intuitions and mathematics of attention mechanisms related to those of PageRank?","2023-05-12 07:20:39","","0","168","<comparison><transformer><math><attention>","<p>Excuse me if you find this question too vague and not fitting to this forum and feel free to close it. The overall goal of my question is to get a better intuition of the attention concept and mechanism.</p>
<p>There is a high-level analogy between attention mechanisms (to be specific: in the transformer) and Google's PageRank algorithm: both claim and strive to calculate &quot;relative importances&quot; – of parts of a sentence or of web pages – without a thorough definition of what &quot;importance&quot; actually is. The meaning of &quot;relative importance&quot; as calculated by PageRank is intuitively clear even though it's recursive: the relative importance of a web page is the sum of the relative importances of the pages linking to it. (Graph-theoretically speaking, the relative importances are given by the eigenvector corresponding to the largest eigenvalue of the adjacency matrix.) The idea is, that when looking for web pages on a specific topic one should pay attention to the most &quot;important&quot; web pages (which PageRank helps to find).</p>
<p>I wonder if the high-level analogy can be put a bit deeper: How are – for example – the mathematics of attention mechanisms related to the mathematics of PageRank – if they are? Or is the analogy too superficial and misleading and should be forgotten?</p>
<p>Until now I could not develop an intuitive understanding what the relative importance of a token in a sentence is (on which attention then is focussed): important with respect to what? To other tokens or the sentence or even the &quot;full model&quot; as claimed <a href=""https://aclanthology.org/P19-1282.pdf"" rel=""nofollow noreferrer"">here</a>? Or isn't the goal of attention mechanisms better explained in terms of &quot;what kinds of relations are there between the tokens in a sentence and between the tokens and the sentence as a whole, and how strong are they?&quot; That's the background of my question.</p>
<p>Once again: excuse the vagueness and possibly confusion of this question, I'm aware of it.</p>
","transformer"
"40390","What is the intuition behind position-encoding?","2023-05-10 14:25:49","","1","162","<transformer><word-embedding><positional-encoding>","<p>It is clear that word positions are essential for the meaning of a sentence, and so are essential when feeding a sentence (= sequence of words) as a matrix of word embedding vectors into a transformer. I also have understood roughly how positions are encoded, but what I did not understand in the very begining is <strong>why</strong> just creating a matrix consisting of a number of word embedding vectors (as columns) with the columns in the same order as the words in the sentence does not suffice. A matrix with permutated columns obviously would &quot;mean&quot; something different - and sometimes nothing at all - like a pixel matrix would change its &quot;meaning&quot; when we permutated some pixel columns. Is there an intuitive explanation why position encoding vectors have to be added to the word embedding vectors. Why and how would the position information (which is still present in the input matrix) get lost otherwise?</p>
<p><strike>(I have learned in the meanwhile that transformers are in general <strong>not</strong> permutation invariant, but that there are transformers that <strong>are</strong>: <a href=""https://arxiv.org/abs/1810.00825"" rel=""nofollow noreferrer"">set transformers</a>.)</strike></p>
","transformer"
"40385","What is a neuron in large language models?","2023-05-10 05:20:28","40386","-1","137","<natural-language-processing><transformer><chatgpt><gpt><artificial-neuron>","<p>I'm reading OpenAI's new paper &quot;<a href=""https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html"" rel=""nofollow noreferrer"">Language models can explain neurons in language models</a>&quot; And I can't fully understand the concept of neurons here.</p>
<p>Can you please explain it? Is it related to the attention mechanism?</p>
","transformer"
"40373","How are the transformer encoder outputs handled?","2023-05-09 07:05:01","","0","131","<neural-networks><natural-language-processing><transformer>","<p>According to the Attention Is All You Need paper, the transformer's encoder portion is described as</p>
<blockquote>
<p>The encoder is composed of a stack of N = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.</p>
</blockquote>
<p>How are the outputs of the N<sub>x</sub>=6 identical layers put together? Is it via concatenation, summation, element-wise product, or are the 6 blocks placed in sequential order, etc? Is the same also done for the decoder?</p>
<p><a href=""https://i.sstatic.net/wKCNA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wKCNA.png"" alt=""Transformer architecture"" /></a></p>
","transformer"
"40370","For a transformer decoder, how exactly are K, Q, and V for each decoding step?","2023-05-09 00:05:46","40377","0","175","<transformer><autoencoders><large-language-models><encoder-decoder>","<p>For a transformer decoder, how exactly are K, Q, and V for each decoding step?</p>
<p>Assume my input prompt is &quot;today is a&quot; (good day).</p>
<p>At t= 0 (generation step 0):
K, Q, and V are the projections of the sequence (&quot;today is a&quot;)
Then say the next token generated is &quot;good&quot;.</p>
<p>At <code>t=1</code> (generation step 1):
Which one is true:</p>
<ol>
<li>K, Q, and V are the projections of the sequence (&quot;today is a good&quot;)</li>
<li>K, Q, are the projections of the sequence (&quot;today is a&quot;), and V is the projection of the sequence (&quot;good&quot;)?</li>
</ol>
","transformer"
"40273","What information does the word embedding in Transformers will encode about the word when analysed outside of the model?","2023-05-02 10:15:48","","1","171","<natural-language-processing><transformer><word-embedding>","<p>Word2vec and similar architectures create word embedding vectors as a byproduct from a supervised learning task, where they need to predict the correct context word. Consequently, the inner representation of words inside this network will preserve some form of proximity-based word similarity based on the used corpus. When extracted, we can observe this via measuring cosine similarity between words, which will result in values close to 1 for words often occurring in each other's proximity and close to -1 for words that are highly infrequent together.</p>
<p>Thus, I would consider the word2vec embedding vectors to be quite interpretable regarding their meaning. What about transformers?</p>
<p>Transformers produce a similar inner representation of words, but than they alter them and recombine them through the attention mechanism multiple times, in order to solve the seq2seq learning task. What will the initial embedding before the first encoding really mean, if anything? Do they have any value when separated from the transformer? Like for example, the vectors generated in a word2vec model can be extracted and used in a downstream task. Is it reasonable to use the embedding vectors from a transformer for any downstream task?</p>
","transformer"
"40252","Why are biases (typically) not used in attention mechanism?","2023-04-30 15:37:19","40256","7","3147","<neural-networks><deep-learning><natural-language-processing><transformer><attention>","<p>Watching <a href=""https://youtu.be/kCc8FmEb1nY?t=4767"" rel=""nofollow noreferrer"">this video</a> implementing attention in a transformer. He set query, key, and value biases to <code>False</code> and said &quot;Typically, people don't use biases for these&quot;.</p>
<p>Even in <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention"" rel=""nofollow noreferrer"">official PyTorch code</a> the default bias is <code>False</code>:</p>
<blockquote>
<p>add_bias_kv: If specified, adds bias to the key and value sequences at dim=0. Default: <code>False</code>.</p>
</blockquote>
<p>What is the reason behind that?</p>
","transformer"
"40244","In the attention mechanism, why don't we normalize after multiplying values?","2023-04-29 17:54:30","","0","692","<neural-networks><natural-language-processing><transformer><attention><normalisation>","<p>As this <a href=""https://ai.stackexchange.com/q/21237/23811"">question</a> says:</p>
<blockquote>
<p>In scaled dot product attention, we scale our outputs by dividing the
dot product by the square root of the dimensionality of the matrix:</p>
<p><a href=""https://i.sstatic.net/wLI4m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wLI4m.png"" alt=""enter image description here"" /></a></p>
<p>The reason why is stated that this constrains the distribution of the weights of the output to have a standard deviation of 1.</p>
</blockquote>
<p>My question is why don't we do the same after multiplying to <span class=""math-container"">$V$</span>(values) for the same reason?</p>
","transformer"
"40205","How to optimize transformer inference for prompts shorter than the maximum sequence length?","2023-04-26 10:58:49","","1","453","<natural-language-processing><transformer><inference><production-systems>","<p>As far as I understand, a Transformer has a specific input sequence length that depends on its architecture. So a model like <code>gpt-4</code> has a sequence length of 8192 tokens. As such, I am interested what happens when the input prompt is shorter than that. <a href=""https://ai.stackexchange.com/questions/22957/how-can-transformers-handle-arbitrary-length-input"">This question and answers</a> suggest that the input is simply padded out to the full input sequence length. But that doesn't really seem plausible to me. That would mean that the cost of processing a 100 token prompt would be the same as the cost of processing a 1000 token prompt. And it clearly isn't, as they charge a different price for them - they charge for token count, not API request count.</p>
<p>I am interested to know how variable length input sequences are handled for experimenting with trying to implement my own language model. I have two possible hypothesis. One could be that they actually combine multiple prompts into a single large prompt and tell the model to complete them all. But I don't think that it is the case as the risk of mixing the prompts from different users and the data they have would probably drastically degrade the quality of the output. I also thought that maybe they have different model sizes - like <code>gpt4-1k</code>, <code>gpt4-2k</code>, <code>gpt4-3k</code>, ... - but that seems like a large overhead, as each of these would need to be trained separately.</p>
","transformer"
"40181","Why would increasing layers in PyTorch Transformer significantly increase loss?","2023-04-23 20:46:16","","1","100","<transformer><pytorch><hyperparameter-optimization><machine-translation>","<p>I have a simple <code>torch.nn.Transformer</code> module for machine translation on the <a href=""https://github.com/multi30k/dataset"" rel=""nofollow noreferrer"">Multi30k dataset</a>. It performs pretty well (32.2 Bleu score) but I looked at scaling up model size. Running in a sweep in Weights &amp; Biases I notice that increasing layers has an inverse relationship to performance, it really drops off adding more layers to encoder/decoder.</p>
<p>Why would this happen, could there be something I'm missing (vanishing gradients, different input shapes, learning rate needs to be adjusted?). I had the assumption that a larger model would give at least equal performance if not better.</p>
<p><a href=""https://i.sstatic.net/ISJIg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ISJIg.png"" alt=""comparison of num decoder and encoder layers and val loss"" /></a></p>
<p>I used <a href=""https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/more_advanced/seq2seq_transformer/seq2seq_transformer.py"" rel=""nofollow noreferrer"">the code here</a> as a starting point and converted to Pytorch Lightning.</p>
","transformer"
"40179","How does the (decoder-only) transformer architecture work?","2023-04-23 19:28:30","40180","28","39173","<deep-learning><transformer><attention><gpt><large-language-models>","<p>How does the (decoder-only) transformer architecture work which is used in impressive models such as GPT-4?</p>
","transformer"
"40173","Does Number of Fully connected neural networks changes in transformer architechture based on max length input size?","2023-04-23 11:30:32","","1","72","<transformer><architecture>","<p>Considering the architecture of encoder and decoder in transformer as shown below:
<a href=""https://i.sstatic.net/4pAzL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4pAzL.png"" alt=""enter image description here"" /></a></p>
<ul>
<li><strong>Does each input token after self attention mechanism (z1,z2,z3,...)is passed to it's specific  separate Feed forward neural network or does all the Z's are stacked into one and then passed to single FFNN?</strong></li>
<li><strong>If all the Z's are stacked into one, then how the difference in shapes of different inputs is taken care</strong></li>
<li><strong>If every z has its own Feed forward neural network, how in practical it is implemented with arbitrary input length?</strong></li>
</ul>
","transformer"
"40154","4 Questions on Transformers","2023-04-21 21:16:39","","1","160","<transformer><fine-tuning>","<p>Assume the transformer is trained on 512 max length sentences:</p>
<ol>
<li><p>can we fine-tune it on 256 max-length sentences?</p>
</li>
<li><p>If we can fine-tune it, how is it even possible because the input shapes are different, and how is the change happening in high-level overview from weights matrices to end layer</p>
</li>
<li><p>Does every transformer decoder processes one token at the same time, or does it process all tokens at a time if it doesn't, how the decoder process all tokens at the same time?</p>
</li>
<li><p>I have doubt that the process of output is different for Sentiment Analysis and Text generation in Transformer Decoder architecture because in text generation, the decoder process one token at a time, while the sentiment analysis need not be one-token, it can be all tokens at the same time, right? So, how is this difference in both examples, the decoder is able to capture by its architecture, <strong>Does all decoder process one token at the same time or all tokens at same time?, If not how is same architecture is able to capture both examples as mentioned above?</strong></p>
</li>
</ol>
","transformer"
"40140","How is the next token predicted in transformers?","2023-04-21 00:48:04","","5","3512","<natural-language-processing><transformer><gpt><language-model>","<p>In the transformer (or GPT/decoder only), at the end of the decoder blocks but before the final linear layer you have X vectors (for the X tokens at the input of the decoder). We then want to compute the probabilities for the next token of the sequence - what do we then feed to the linear layer? Is it the last embedding corresponding to the hidden state of the last token in the input sequence?</p>
<p>I've seen some tutorials on youtube on how to make mini gpts but I never quite understood why they feed the entire X vectors/hidden states at the end of the decoder blocks to the linear layer and not just the last vector/hidden state... Wouldn't you have X probability distributions when in reality you only want one? And if we do want the X probability distributions then wouldn't we be completely missing the point of the masked self attention since we would be trying to predict words that are already in the input sequence, so essentially &quot;cheating&quot;?</p>
","transformer"
"40119","Are transformer decoder predictions computed in parallel during training?","2023-04-19 12:30:18","40156","1","684","<transformer>","<p>I've been studying the transformer from the original &quot;Attention is all you need&quot; paper and from various other sources. I have a question about the behaviour of the decoder during training that I cannot find the answer to anywhere.</p>
<p>During inference I understand that the decoder input is its own previously generated token from the prior time step. Tokens are fed into the decoder one-by-one and predictions made one-by-one.</p>
<p>However, during training the target sequence is known and I have read from several sources that the entire sequence is used as the decoder input to allow parallel processing and to improve training efficiency. To keep the decoder autoregressive, a masked attention sub-layer is introduced where a masking matrix is added to the scaled dot product attention mechanism.</p>
<p>So my question is, since during training the decoder input is the entire sequence, is the entire output sequence predicted in parallel (simultaneously), or are tokens predicted one-by-one, as in inference?</p>
<p>To me it makes sense that if the entire target sequence is used as the decoder input, then an entire sequence is output. If it wasn't, the decoder would be using the same input at every timestep whilst being expected to produce different tokens.</p>
","transformer"
"40098","In terms of explainability, is attentive RNN easier to explain than the transformer?","2023-04-17 16:27:12","","1","107","<recurrent-neural-networks><transformer><attention><explainable-ai>","<p>Although the multi-headed attention block of the transformer allows the model to be more expressive (and therefore perform better), it is remarkably more difficult to decompose and therefore to explain (in terms of model transparency).</p>
<p>Would an attentive RNN be considered a better architecture w.r.t. model transparency and attention-based explainability?</p>
","transformer"
"40086","Has anyone tried to train a GPT model predicting the next N tokens instead of the next one token?","2023-04-16 19:25:20","40110","3","756","<ai-design><transformer><loss><gpt>","<p>I have been thinking about how learning via text works on humans: we read words, and often we need to read ahead a few words to understand more clearly the ideas that we read before. Most of the time, just reading the next word in a sentence is not enough for clear understanding.</p>
<p>Has anyone tried (and if no, what is your opinion) to modify GPT to predict the next N tokens instead of the next single token? My intuition says that the loss function would decay faster that way.</p>
","transformer"
"40082","Difference between dot product attention and ""matrix attention""","2023-04-16 10:16:23","40118","4","811","<papers><transformer><attention><sequence-modeling>","<p>As far as I know, attention was first introduced in <a href=""https://arxiv.org/abs/1409.0473"" rel=""nofollow noreferrer"">Learning To Align And Translate</a>.</p>
<p>There, the core mechanism which is able to disregard the sequence length, is a dynamically-built matrix, of shape output_size X input_size, in which every position <span class=""math-container"">$(o, i)$</span> holds the (log) probability that output <span class=""math-container"">$o$</span> should attend to input <span class=""math-container"">$i$</span>.</p>
<p>That (log) probability is obtained by operating a learned function <span class=""math-container"">$a(h, s)$</span>, where <span class=""math-container"">$h$</span> is a hidden state of the input, and <span class=""math-container"">$s$</span> is a cell state of the output.</p>
<p>Please let's disregard the fact that these inputs are RNN-based, and only look at the attention mechanism itself - a dynamic matrix of (log) probabilities is built, each slot is built by a function taking in two vectors, and outputting their &quot;correspondence&quot;.</p>
<hr />
<p>Jump forward to the iconic <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention Is All You Need</a>.</p>
<p>Please disregard the fact that in this paper, <span class=""math-container"">$K$</span> was separated from <span class=""math-container"">$V$</span>, unlike in the previous one.<br />
I just want to look at the mechanism itself.</p>
<p>Let's look only at Multi-Head Attention, and in it, let's look only at the part actually doing the attention: <span class=""math-container"">$ QK^T $</span></p>
<p>Let's assume <span class=""math-container"">$Q$</span> and <span class=""math-container"">$K$</span> are vectors and not matrices, for simplicity. Their attention score is their dot product.</p>
<hr />
<p>Let's compare the core attention mechanisms of &quot;align and translate&quot; against &quot;all you need&quot;.</p>
<p>In &quot;align and translate&quot;, the function learns how two vectors correspond to one another</p>
<p>In &quot;all you need, the function learns to project embeddings into a continuous space, where they can be compared against other such projections by their dot-product.</p>
<p>One could easily implement multi-head-attention with the dynamic matrix method, by a function <span class=""math-container"">$b(k, q)$</span> yielding the (log) probability that the two correspond, and putting that into a dynamic-size matrix.</p>
<hr />
<p><strong>My question is what in the &quot;all you need&quot; core attention method makes it better than the &quot;align and translate&quot; core attention method?</strong></p>
<p>Are there ablation studies for this point?</p>
<p>My intuition tells me it would be easier for a network to learn how to correspond vectors, rather than to learn an entire continuous space.</p>
<hr />
<p>Again, please disregard the other contributions in &quot;all you need&quot;, such as self-attention, separation of key from value, normalization, Transformer, ect.</p>
","transformer"
"40080","Playing around with Transformer - accuracy not improving","2023-04-16 08:49:13","","1","272","<deep-learning><transformer><pytorch><large-language-models>","<p>I am playing around with a decoder only transformer model,</p>
<p>The Colab is here if you find that easier
<a href=""https://colab.research.google.com/drive/1SHyJ9Oa3E4j1x8YFlXQbd1mjUjWhHGOV#scrollTo=60e13119"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1SHyJ9Oa3E4j1x8YFlXQbd1mjUjWhHGOV#scrollTo=60e13119</a>
or see the code below (should run with minimal deps in a notebook)</p>
<h3>Goal:</h3>
<ol>
<li>I am playing around to understand the transformer architecture. I wanted to build a minimal version of a decoder only model and see if i can train the model to predict the last digit in this sequence (think of it as a recall function from a key value store).</li>
<li>Want to get an intuitive understanding on the linear transformations of the weight matrices to the sequence input + positional encodings.</li>
</ol>
<pre><code>n9 v8 a5 p7 k0 j1 i3 e2 g6 c4 c - (should be 4)
d5 t3 q8 r7 y1 i0 c2 n9 s4 u6 i - (should be 0) ...
l8 u9 p5 y3 f7 k0 g6 v4 r1 x2 l
a7 x5 b6 v0 i1 f3 z9 d4 y2 k8 x
m9 h4 g5 t2 l3 f1 w7 b6 a8 j0 g
x0 g7 q9 u2 j8 v4 h3 o1 f5 r6 r
r6 c4 d0 p3 j2 g9 a7 n1 e8 l5 d
r2 z7 y6 x5 v4 u1 s3 a8 l9 p0 z
k0 u3 t1 r4 g8 p2 j5 x9 s7 v6 t
o7 a1 u3 r2 k6 j0 m8 y9 e4 c5 j
</code></pre>
<h3>Questions:</h3>
<ul>
<li>The accuracy is not improving, so i guess there is some fundamental issue with the model. It looks like it is learning that it is a digit, but not what digit.</li>
</ul>
<h3>Code:</h3>
<pre><code>import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import random
import string
import time
import math

if torch.cuda.is_available(): 
    dev = &quot;cuda:0&quot; 
else: 
    dev = &quot;cpu&quot; 
device = torch.device(dev) 

class PositionalEncoding(nn.Module):
    &quot;&quot;&quot;
    compute sinusoid encoding.
    &quot;&quot;&quot;
    def __init__(self, d_model, max_len, device):
        &quot;&quot;&quot;
        constructor of sinusoid encoding class

        :param d_model: dimension of model
        :param max_len: max sequence length
        :param device: hardware device setting
        &quot;&quot;&quot;
        super(PositionalEncoding, self).__init__()

        # same size with input matrix (for adding with input matrix)
        self.encoding = torch.zeros(max_len, d_model, device=device)
        self.encoding.requires_grad = False  # we don't need to compute gradient

        pos = torch.arange(0, max_len, device=device)
        pos = pos.float().unsqueeze(dim=1)
        # 1D =&gt; 2D unsqueeze to represent word's position

        _2i = torch.arange(0, d_model, step=2, device=device).float()
        # 'i' means index of d_model (e.g. embedding size = 50, 'i' = [0,50])
        # &quot;step=2&quot; means 'i' multiplied with two (same with 2 * i)

        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))
        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))
        # compute positional encoding to consider positional information of words

    def forward(self, x):
        # self.encoding
        # [max_len = 512, d_model = 512]

        batch_size, seq_len, d_model = x.size()
        #print(&quot;seq_len: &quot;, seq_len, &quot; batch_size: &quot;, batch_size, &quot;d_model: &quot;, d_model)
        # [batch_size = 128, seq_len = 31]
        #print(&quot;self.encoding: &quot;, self.encoding[:seq_len, :].shape)
        return self.encoding[:seq_len, :d_model]
        # [seq_len = 30, d_model = 512]
        # it will add with tok_emb : [128, 30, 512]   

# Generate random strings
def random_string(length):
    #return &quot;1 2 3 4 5 6 7 8 9&quot;
    a = random.sample(string.ascii_lowercase, length)
    d = random.sample(string.digits, length)
    r = ' '.join([a[i] + d[i] for i in range(length)])
    n = random.randint(0, length)-1
    return r + &quot; &quot; + a[n] + d[n]

# Synthetic Dataset
class RandomStringDataset(Dataset):
    def __init__(self, num_samples, seq_length):
        self.num_samples = num_samples
        self.seq_length = seq_length
        self.data = [random_string(seq_length) for _ in range(num_samples)]

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        input_seq = self.data[idx][:-1]
        target_seq = self.data[idx][1:]
        return input_seq, target_seq

# Tokenizer and detokenizer functions
def tokenize(text):
    return [char for char in text]

def detokenize(tokens):
    return ''.join(tokens)

# Map characters to indices and vice versa
vocab = string.ascii_lowercase + string.digits + &quot; &quot;
char_to_idx = {char: idx for idx, char in enumerate(vocab)}
idx_to_char = {idx: char for idx, char in enumerate(vocab)}


# Convert tokens to tensor
def tokens_to_tensor(tokens):
    indices = [char_to_idx[token] for token in tokens]
    return torch.tensor(indices, device=device)

# Convert tensor to tokens
def tensor_to_tokens(tensor):
    return [idx_to_char[idx.item()] for idx in tensor]

def collate_fn(batch):
    inputs, targets = zip(*batch)
    input_tensors = [tokens_to_tensor(seq) for seq in inputs]
    target_tensors = [tokens_to_tensor(seq) for seq in targets]
    return torch.stack(input_tensors), torch.stack(target_tensors)

class DecoderOnlyTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers):
        super(DecoderOnlyTransformer, self).__init__()
        self.pe = PositionalEncoding(d_model, 128, device)
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.transformer_decoder = nn.TransformerDecoder(
            nn.TransformerDecoderLayer(d_model, nhead),
            num_layers
        )
        self.fc = nn.Linear(d_model, vocab_size)
        
    def forward(self, x):
        x = self.embedding(x)
        pe = self.pe(x)
        #print(&quot;x: &quot;, x.shape)
        #print(&quot;pe: &quot;, pe.shape)
        x = x + pe
        tgt = torch.zeros_like(x)
        output = self.transformer_decoder(x, x)
        output = self.fc(output)
        return output
    
# Hyperparameters

seq_length = 10
batch_size = 16
num_samples = batch_size*1000
learning_rate = 0.001
num_epochs = 100
d_model = 4
nhead = 4
num_layers = 2
vocab_size = len(vocab)

# Create dataset
dataset = RandomStringDataset(num_samples, seq_length)
train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)

# Initialize model, loss, and optimizer
model = DecoderOnlyTransformer(vocab_size, d_model, nhead, num_layers)
model.to(device)


criterion = nn.CrossEntropyLoss()
lr = 5  # learning rate
optimizer = torch.optim.SGD(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)

for epoch in range(num_epochs):
    correct = 0
    count = 0
    total_loss = 0
    start_time = 0
    log_interval = 100
    num_batches = int(num_samples/batch_size)
    for inputs, targets in train_loader:
        
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs.reshape(-1, len(vocab)), targets.flatten())

        probabilities = torch.softmax(outputs[0, -1], dim=-1)
        next_token_idx = torch.argmax(probabilities).item()
        token = idx_to_char[next_token_idx]

        if idx_to_char[targets[0, -1].item()] == token:
            correct+=1
        count+=1
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()
        
        total_loss += loss.item()
        if count % log_interval == 0 and count &gt; 0:
            lr = scheduler.get_last_lr()[0]
            ms_per_batch = (time.time() - start_time) * 1000 / log_interval
            cur_loss = total_loss / log_interval
            ppl = math.exp(cur_loss)
            print(f'| epoch {epoch:3d} | {count:5d}/{num_batches:5d} batches | '
                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '
                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f} | '
                  f'Acc: {correct/count}'
                 )
            total_loss = 0
            start_time = time.time()    
    print(f'Epoch: {epoch + 1}/{num_epochs}, Loss: {loss.item()}  Count: {count}')
<span class=""math-container"">```</span>
</code></pre>
","transformer"
"39919","Machine Translation Transformers: Why Mask in Decoder?","2023-04-05 03:20:18","","3","428","<deep-learning><natural-language-processing><transformer><attention><machine-translation>","<p>I am trying to understand the purpose of masking in the decoder in the <a href=""https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">&quot;Attention is All you Need&quot;</a> paper. Why wouldn't we want to rely on bidirectional context when translating? What goes wrong if masking isn't used in the decoder? Are there successful models in which the decoder doesn't use masking? Thank you!</p>
","transformer"
"39902","How can the Transformer model tell from positional encoding data to the origional data?","2023-04-04 02:11:11","","1","48","<natural-language-processing><transformer><positional-encoding>","<p>I am having trouble understanding positional encoding. Say after the wor2vec or some encoding algo we get the tensor <span class=""math-container"">$[0.7, 0.4, 0.2]$</span> for the second position. Now the final input into the model would add a positional encoding, making it <span class=""math-container"">$[0.7 + \sin(1.0), 0.4 + \cos(1.0), 0.2 + \sin(1.0)]$</span> right? Here's the question:</p>
<p>How can the model know if it's word tensor <span class=""math-container"">$[0.7, 0.4, 0.2]$</span> with positional encoding <span class=""math-container"">$[\sin(1.0), \cos(1.0), \sin(1.0)]$</span>, or if it is word tensor <span class=""math-container"">$[0.7 + \sin(1.0) - \sin(0.0), 0.4 + \cos(1.0) - \cos(0.0), 0.2 + \sin(1.0) - \sin(0.0)]$</span> with positional encoding <span class=""math-container"">$[\sin(0.0), \cos(0.0), \sin(0.0)]$</span>? They both have the same numbers.</p>
","transformer"
"39891","Understanding self attention - How come there is no connection between different states?","2023-04-03 07:18:47","","-1","203","<papers><transformer><attention><sequence-modeling>","<p>During trying to understand transformers by reading <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention is all you need</a>, I noticed the authors constantly refer to &quot;self attention&quot; without explaining it.</p>
<p>The original attention mechanism is introduced in <a href=""https://arxiv.org/abs/1409.0473"" rel=""nofollow noreferrer"">NEURAL MACHINE TRANSLATION
BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a>, in which a the relationship between each output and each input is encoded into a matrix whose size is dependent on input and output sizes.</p>
<p>It seems the paper which first introduced self-attention is <a href=""https://arxiv.org/abs/1703.03130"" rel=""nofollow noreferrer"">A STRUCTURED SELF-ATTENTIVE
SENTENCE EMBEDDING</a>.</p>
<p>I am not finding similar ideas in this paper, and would like an explanation.</p>
<p>In this paper, self-attention is defined like so</p>
<p>Given <span class=""math-container"">$H=(h_1, h_2, ... h_n)$</span>, a sequence of hidden bidirectional LSTM states, each of dimension <span class=""math-container"">$2u$</span>, they try to encode the sentence into a fixed length vector. They do so by weighting the hidden states according to the probabilities (weights) vector <span class=""math-container"">$a$</span> defined as</p>
<p><span class=""math-container"">$a = softmax(W_{s_2}tanh(W_{s_1}H^T))$</span></p>
<p>This can be read as a multi layer perceptron, <span class=""math-container"">$A$</span>, operating on vectors of size <span class=""math-container"">$2u$</span>, with a single layer of activation, with activation function <span class=""math-container"">$tanh$</span>, and output size of 1. <span class=""math-container"">$A$</span>'s output units are log probabilities, meaning &quot;how likely is timestamp <span class=""math-container"">$i$</span> to be important for the sentence embedding&quot;.
<span class=""math-container"">$A$</span> is then used on the <span class=""math-container"">$n$</span> hidden states, <span class=""math-container"">$H$</span>, to obtain <span class=""math-container"">$n$</span> weights, one for each hidden state <span class=""math-container"">$h_i$</span>.</p>
<p>Then they go on to extend this from dimension <span class=""math-container"">$1$</span> to <span class=""math-container"">$r$</span>.</p>
<p>This is all well, but I fail to see what good this <span class=""math-container"">$A$</span> actually does - <strong>it does not use any relationship between two different states <span class=""math-container"">$h_i, h_j, i \ne j$</span> other than that already found by the LSTM, thus (in my mind,) defeating the purpose</strong>.</p>
<p>I expected something like the original attention - creating a <span class=""math-container"">$nxn$</span> matrix, and have some MLP <span class=""math-container"">$a(h_i, h_j)$</span> calculate each cell, thus encoding probability of the relationship between the cells. This was not done here.</p>
<hr />
<p>My questions</p>
<ol>
<li>Is this really the self attention mentioned throughout &quot;attention is all you need&quot;? if not, please refer me to the correct paper</li>
<li>Did I understand this paper correctly, and indeed other than the LSTM, there are no relationships between the different time stamps, thus defeating the purpose of encoding the sentence better than the LSTM? [we could just add another layer into the LSTM, isn't it the same or better?]</li>
<li>Please explain the point of this paper. Adding a global MLP on all LSTM states? That's it?</li>
<li>Why only a single layer? why <span class=""math-container"">$tanh$</span>? why not just a single vector of
size <span class=""math-container"">$1x2h$</span> and no activation at all?</li>
</ol>
","transformer"
"39876","Is it possible for original Vision Transformer (ViT) to do fine-grained semanantic segmentation? if so, how?","2023-04-01 15:25:57","","2","64","<convolutional-neural-networks><computer-vision><transformer><attention><vision-transformer>","<p>As far as I know, in the original ViT, the image is first divided to a fixed size of patch (16x16, for example) then they are flattened and treated as tokens and fed into Transformer.</p>
<p>Without using later more recent techniques (such as Hierarchical patch merging in Swin transformer), I feel like it is not possible to do the <em>fine-grained</em> segmentation at all, the best it can do is to label the whole token (with 16x16 pixel) as one class, since the model no longer understands any spatial information in the token.</p>
<p>Is my understanding correct? I know that it didnt do as well as Swin Transformer which use more fine-grained hierachical patch merging. But can ViT still selectively label some pixels in one patch as one class and the other pixels as another class?</p>
","transformer"
"39872","Why ChatGPT output one token at a time?","2023-03-31 22:46:21","","0","51","<transformer><attention><chatgpt><large-language-models>","<p>My understanding of the language model is that the output of the model is a tensor. So the whole output should be computed all together. But why ChatGPT like models can output one token at a time like a stream? And the time difference between the output of first and last token is significant.</p>
","transformer"
"39867","Redundancy of Value-Projection-matrix in Multi-headed attention in Transformer model","2023-03-31 13:34:09","","2","421","<transformer>","<p>In the original transformer paper &quot;Attention is all you need&quot; in section 3.2.2 it is written:</p>
<p>Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly <strong>project the queries, keys and values h times</strong> with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. These are <strong>concatenated and once again projected</strong>, resulting in the final values, as depicted in Figure 2.</p>
<p>I am wondering why you need to project the values h times if you concatenate and project them once again in the end. It seems to me that this just results in two matrices being learned that are multiplied with each other. This should have the same expressiveness as one matrix. So, in my understanding you could just leave away the h projection steps and simply do the final projection. Am I missing something?</p>
","transformer"
"39849","How does Position embeddings work in Vision Transformer","2023-03-30 15:11:22","","0","193","<machine-learning><deep-learning><computer-vision><transformer><vision-transformer>","<p>I'm a bit confused how the position embedding in happened to each patch in the transformer. I thought Ideally we'd want each patch to have a value of (1, 2, 3, 4....) to describe the position of the patch in the image. but from the implementation <a href=""https://github.com/gupta-abhay/pytorch-vit/blob/main/vit/patch_embed.py"" rel=""nofollow noreferrer"">here</a> there do something like this:</p>
<pre><code> # positional embedding
            self.pos_embed = nn.Parameter(
                torch.zeros(1, num_patches, embedding_dim)
            )
</code></pre>
<p>Which is quite confusing because now we have some sort of mapping instead of just a value appended to each patch. Also, there is some sort of implicit position appended to the patch right? Assume we have a patch embedding output (1, 256, 768); corresponding to (batch, num_patches, position_embedding). since we have 256 patches, then can't our network understand that each patch is in the position of its index value? Why do we have to explicitly define a position embedding for each patch?. Also, please kindly explain the implementation above I'm not sure I understand the mapping and why its initialised to zero</p>
","transformer"
"39828","Can I reduce computation by only predicting response tokens in a transformer and still get the same gradients?","2023-03-29 04:27:11","","1","45","<natural-language-processing><python><transformer><chatgpt><language-model>","<p>I have been looking at the source code of the Stanford Alpaca model and I believe that during inference, the whole instruction + response data is fed into the model normally. Then the instruction part of the label is masked with IGNORE_INDEX to prevent gradient calculation on the instruction.</p>
<p>But I believe that in the transformer network, after the attention blocks and before the last head layer, it should be possible to take only the (embedded) tokens corresponding to the response parts from contexts and avoid predicting from the tokens corresponding to the instruction part altogether. This could potentially save computation, especially when the instruction part is long or when training on back-and-forth conversations, similar to interactions with ChatGPT. So my question is:</p>
<hr />
<ol>
<li>Is the gradients calculated by these two approaches the same?</li>
</ol>
<p>I actually tried to ask GPT-4 about this. Initially, he suggested that my approach would lose some information, but later changed his opinion when I asked him further. Would my approach indeed result in a loss of information or any other drawbacks?</p>
<hr />
<ol start=""2"">
<li>Is the saving, if possible, worth the effort of modifying the model's source code?</li>
</ol>
<p>Since the change would only affect one matrix multiplication, which is already efficiently computed, I'm unsure if it's worth the trouble. Furthermore, I only know a little bitPyTorch and would definitely struggle with implementing modifications to complicated models written in other frameworks like TensorFlow or JAX.</p>
<hr />
","transformer"
"39824","How does a LLM (transformer) pick words from its vocabulary?","2023-03-28 18:23:53","","2","2022","<neural-networks><natural-language-processing><transformer><natural-language-generation><large-language-models>","<p>I have a very rough understanding of the &quot;attention/self attention&quot; mechanism of transformer models and how this can be used to process a set of word vectors provided as an input/prompt to the encoder of a network and how this will produce &quot;attention weights&quot; for the word vectors based on positional encodings and some other learnable parameters (key/query/value transforms). And then these can be &quot;fed&quot; to the decoder part of the network which will also consider word vectors that have been produced by the decoder so far and influence word selection by paying special attention to particular word combinations.</p>
<p>However LLMs clearly produce words in their output/response that do not occur anywhere inside the &quot;prompt&quot; text. So they must be using these &quot;attention weights&quot; to consider words from a wider vocabulary, which could be quite large.</p>
<p>Is it the case that the decoder &quot;considers&quot; each possible word in it's entire vocabulary when producing an output word? For example I'm imagining an input layer to a NN with several thousand nodes (one per word vector in dictionary) on the input then these are &quot;combined&quot; through some operation with attention weights (from the encoder and decoder &quot;attention section&quot;) producing values for most word vectors that are very low (so below the threshold for some activation function) but each word is still &quot;considered&quot; to an extent? Or are only a subset of words considered in some way?</p>
","transformer"
"39817","What's the most efficient way of performing batched training of Causal Language Models?","2023-03-28 07:40:46","","2","444","<training><transformer><gpt><language-model><batch-learning>","<p>I have seen a number of ways to train (yes, train, not fine-tune) these models efficiently with batches. I will illustrate these techniques with the following example dataset and context window:</p>
<pre><code>Context window:
   -----------------
Data samples:
1. ###
2. ################
3. ####
4. ##############
5. ########
6. #########
</code></pre>
<p>Suppose we have a batch size of 2. Our pad token is x</p>
<h2>First technique: Vanilla Padding</h2>
<pre><code>Context window:
   -----------------
batch 1:
1. ###xxxxxxxxxxxxx
2. ################

batch 2:
3. ####xxxxxxxxxx
4. ##############

batch 3: 
5. ########x
6. #########
</code></pre>
<h2>Second technique: Bucketed Padding</h2>
<p>Samples of similar lengths are batched together to minimise the number of pad tokens</p>
<pre><code>Context window:
   -----------------
batch 1:
1. ###x
3. ####

batch 2:
2. ################
4. ##############xx

batch 3: 
5. ########x
6. #########
</code></pre>
<p>this is <em>uniform length batching</em> described in <a href=""https://mccormickml.com/2020/07/29/smart-batching-tutorial/"" rel=""nofollow noreferrer"">this blogpost</a> and referred to as <em>bucketed random sampling</em> in <a href=""https://aclanthology.org/2021.findings-acl.74/"" rel=""nofollow noreferrer"">this paper</a>.</p>
<h2>Third technique: Concatenating samples</h2>
<p>In this technique, we concatenate samples, separating them with a EOS token (E) until they reach the context length. In this way, we have no padding tokens, and the entire context length is used. The attention mask keeps track of where the EOS tokens occur.</p>
<pre><code>Context window:
   -----------------
batch 1:
   ###E############# (1 and part of 2)
batch 2:
   ###E####E######## (rest of 2, 3 and part of 4)
batch 3:
   ######E########E# (rest of 4, 5, part of 6)
batch 4:
   ######## (rest of 6)
</code></pre>
<p>This technique is referenced at 2:28 of <a href=""https://www.youtube.com/watch?v=ma1TrR7gE7I"" rel=""nofollow noreferrer"">this video</a> from <a href=""https://huggingface.co/course/chapter7/6"" rel=""nofollow noreferrer"">this huggingface tutorial</a>.</p>
<p>With this technique, we reduce the number of batches, and only have to pad the final batch if necessary. However, it is unclear to me whether this is &quot;allowed&quot; for causal language modelling, as it is unclear whether this will cause the causal attention mechanism to attend to tokens from previous samples, only ignoring the EOS token (instead of everything before it)</p>
<hr />
<p>Of these 3 techniques, which is the most memory efficient? Which is the most commonly used?</p>
","transformer"
"39704","How code analysis part of ChatGPT works and trained","2023-03-21 10:46:00","","2","203","<neural-networks><transformer><chatgpt>","<p>ChatGPT can explain given code snippet we also ask question like &quot;What does this variable do&quot; , &quot;Why this is used&quot; and all. I gave C++ function snippet from an popular Open Source project it was able explain complete context how this function can be used and it is used in project even though snippet I gave only contained that particular function. This has baffled me. I understand that they might have fed all Open Source project codebases to it.</p>
<p>But what I wonder how might they have trained it. Like given the code how to make Neural networks do analysis on it and make it answer the query? I'm really curious to learn it.</p>
<p>I know OpenAI did not really publish proper research paper on this. But still anyone can tell me or guide me how <strong>might</strong> they have done it.</p>
","transformer"
"39681","How does transformer models like GPT generate valid meaningful response for meaningless garbage input?","2023-03-20 06:00:53","39701","1","199","<transformer><attention><chatgpt><gpt>","<p>My understanding of a transformer model is that it uses the given input to calculate internal query of relate-ness of word meanings, and generate a meaningful response based on its meaning. But if your given sentence has no meaning, then won't the model fail to capture any meaningful input so that the output will also be meaningless? How does the ChatGPT generate meaningful response asking me what I meant when the input is uncoordinated and meaningless? Is that a feature, or did they train on that kind of input specifically?<br />
eg. Input &quot;Jumps the dog lazy fox over quick brown the.&quot;<br />
For the output, ChatGPT asks for clarification.<br />
Normally for a self-attention based model, there won't be any relation between input word for this example, so shouldn't the output also be garbage like?</p>
","transformer"
"39680","How can I not only classify an intent, but also identify slots and values in it?","2023-03-20 05:51:07","39715","0","79","<classification><transformer><attention><bert>","<p>I've been working on text -&gt; intent -&gt; command execution for a particular application and while I've found many papers and code that work well for intent classification (<a href=""https://paperswithcode.com/paper/generalized-intent-discovery-learning-from"" rel=""nofollow noreferrer"">1</a>, <a href=""https://paperswithcode.com/paper/z-bert-a-a-zero-shot-pipeline-for-unknown"" rel=""nofollow noreferrer"">2</a>, etc.), they stop there. For example, given a standard music intent &quot;<em>Play some music by U2</em>&quot; such classifiers return me an intent class <code>play_music</code>. But the information about the artist, track requested aren't part of it. Everytime I search for papers on text -&gt; intent, I end up with classifiers instead of any deep learning models that can not only classify, but maybe give me an importance vector to extract the key parameters from my intents to pass onto the intent class.</p>
<p>Does anyone know of any papers or implementations like this?</p>
<p>Note: I'm not looking to make a voice assistant but a text-&gt;command execution engine for a very specific purpose.</p>
","transformer"
"39639","Where to find the source code for the research paper ""Attention is all you need""?","2023-03-17 12:41:08","","-1","1329","<transformer><attention>","<p>I am reading &quot;Attention is all you need&quot;. I have seen the following link:</p>
<p><a href=""https://i.sstatic.net/LnNQF.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LnNQF.png"" alt=""enter image description here"" /></a></p>
<p>Source: <a href=""https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a></p>
<p>But when I go to <a href=""https://github.com/tensorflow/tensor2tensor"" rel=""nofollow noreferrer"">https://github.com/tensorflow/tensor2tensor</a>, there are too many folders without a guide. Where do you find the source code for the research paper &quot;Attention is all you need&quot;?</p>
","transformer"
"39586","Fine-Tuning T5 with specific penalty","2023-03-14 18:40:48","","0","110","<transformer><machine-translation><fine-tuning>","<p>Currently I am finetuning <code>transformers</code> T5 model for translation task. As part of the dataset, I am given sentences in Japanese, their translation to English, and for every English sentence I am also given a few (English) words which need to be in the translated sentence.</p>
<p>Is there a way to modify the <code>Seq2SeqTrainer</code> and the loss being used in order to penalize the model (while training) for not including the given words in the translated sentence?</p>
","transformer"
"39584","Is the input embedding split along the embedding dimension so that every head of the multi-head-attention module just gets a part of the input data?","2023-03-14 17:19:50","","1","152","<natural-language-processing><transformer><attention><word-embedding><embeddings>","<p>So I found two contradictory explanations of the MHA (multi-head-self-attention-module):</p>
<p>In <strong>the first approach</strong>, the input embedding (= the input matrix) is split along the embedding dimension and all heads are given a subset of the dimensions/features of each word.
Some websites supporting this theory:
<a href=""https://medium.com/@smitasasindran/12-attention-mechanisms-multihead-attention-958041a35553"" rel=""nofollow noreferrer"">https://medium.com/@smitasasindran/12-attention-mechanisms-multihead-attention-958041a35553</a>
<br>-&gt; Quote: &quot;The input has been split into multiple heads, and we are running the attention model separately on each of these heads.&quot;</p>
<p><a href=""https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#3fa3"" rel=""nofollow noreferrer"">https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec#3fa3</a> <br>-&gt; Quote: &quot;In multi-head attention we split the embedding vector into N heads, so they will then have the dimensions batch_size * N * seq_len * (d_model / N).&quot;</p>
<br>
<br>
<p><strong>The second approach</strong> assumes that all heads receive the entire input data, but different weight matrices are used for each head depending on the number of heads.
This theory is well explained on <a href=""https://hungsblog.de/en/technology/learnings/visual-explanation-of-multi-head-attention/"" rel=""nofollow noreferrer"">https://hungsblog.de/en/technology/learnings/visual-explanation-of-multi-head-attention/</a>
<br>-&gt; Quote: &quot;Each head is responsible to fully calculate the attention for the whole embedding, not just for a subset of it and creates h attention matrices&quot;</p>
<p>I tend to the second explanation, but have not been able to find a satisfactory and contradiction-free answer so far.</p>
","transformer"
"39582","Why can't traditional neural networks learn to perform the same tasks that attention layers do?","2023-03-14 16:17:24","","0","39","<neural-networks><deep-learning><transformer><attention>","<p>If your task is to predict <span class=""math-container"">$t_{n+1}$</span> given tokens <span class=""math-container"">$(t_1,...,t_n)$</span>, you could do two things:</p>
<ol>
<li><strong>Straight NN</strong> - feed <span class=""math-container"">$t=(t_1,...,t_n)$</span> into a neural network as an n-dimensional input and train it on predicting <span class=""math-container"">$t_{n+1}$</span> (with all the embedding / layernorm / skip connection stuff that transformer models have)</li>
<li><strong>Attention</strong> - take <span class=""math-container"">$t_n$</span> plus information from <span class=""math-container"">$t_1,...,t_{n-1}$</span> relevant to <span class=""math-container"">$t_n$</span> computed in the attention layer (plus positional encoding) and feed that into a neural network to predict <span class=""math-container"">$t_{n+1}$</span></li>
</ol>
<p>In one case you're putting everything into the NN, equally weighted. In another, you're taking certain information from each token weighted by relevance to <span class=""math-container"">$t_n$</span>.</p>
<p><strong>My question is: why can't the straight NN learn this? What was wrong with the straight NN model so that we needed into introduce attention? If it was helpful to upweight certain things by a relevance metric, why wouldn't the NN learn that?</strong></p>
<p>My only thought so far is long-range dependency: that a NN might forget information far back in the sequence (as in general it will be less useful). For example, 'he walked into the kitchen, took the chocolate from the drawer, opened wide and put it into' – to predict the next word you really need to look all the way back to the start of the sentence.</p>
<p>Thanks for your help!</p>
","transformer"
"39558","Using root and modifiers in translation task","2023-03-13 14:25:00","","1","39","<transformer><machine-translation>","<p>I am doing a project of translation task by finetuning <code>T5</code> model. I am given sentences in Chinese, their translation to English, and for every English sentence I am also given its ROOT and some modifiers of the root. How can I use the ROOT and its modifiers in order to achieve better results?</p>
<p>I can't think about ideas for how to use this additional information.</p>
","transformer"
"39534","Transformer parallelization during training","2023-03-11 22:25:56","","0","215","<machine-learning><deep-learning><transformer>","<p>What does it mean that the decoder can be parallelized during training?</p>
<p>Let's assume a transformer (with both encoder and decoder) is employed for a time-series prediction. I.e. from the input sequence <code>x_0, ..., x_N</code> we want to predict <code>y_0, ..., y_N</code>. Is this the way that parallelization occurs during training?</p>
<ul>
<li>form the batch <code>[], [y_0], ..., [y_0, ..., y_N-1]</code></li>
<li>feed this batch to the transformer, together with the input sequence</li>
<li>we will obtain the batch <code>Y_0, Y_1, ..., Y_N</code></li>
<li>compare against <code>y_0, ..., y_N</code> and form the loss (*)</li>
</ul>
<p>(*) here, some teacher ratio techiques may be employed, so that more passes may be required</p>
","transformer"
"39460","Why does the pass@k metric not ""behave like"" probability?","2023-03-07 22:02:10","40396","3","4370","<transformer><metric>","<p><strong>pass@k</strong> is a metric used to evaluate models that generate code, used for example to evaluate <a href=""https://arxiv.org/abs/2107.03374"" rel=""nofollow noreferrer"">Codex</a>. To evaluate pass@k, you have a dataset of natural language/code pairs, and you pass each NL prompt to the model. For each prompt, it generates <em>k</em> code snippets. If at least one of the code snippets is correct, then the model succeeded at that prompt in <em>k</em> samples. The pass@k is the fraction of prompts for which the model succeeded in this sense.</p>
<p>The samples generated for each prompt are obtained via some stochastic procedure based on the model's output probability distributions on the vocabulary, like randomly choosing the next token based on that distribution or something like that.</p>
<p>So in the Codex paper for example, we see these figures for the largest Codex model:</p>
<ul>
<li>pass@1: 28%</li>
<li>pass@100: 72%</li>
</ul>
<p><strong>These numbers make no sense to me.</strong> Every time we sample the model's prediction for a given prompt, we get back a random code snippet. That output is either correct or incorrect. Each trial is independent. So if the probability of one output being correct is <em>p</em>, the probability of at least one of 100 being correct should be <em>1 - (1 - p)^100</em>. Here <em>p=0.28</em>, so the pass@100 should be like 99.999999%.</p>
<p>Are the trials not independent? What's going on?</p>
","transformer"
"39403","How much do we know about the architectures of the Codex (prototype) models?","2023-03-03 15:28:16","","1","84","<transformer><gpt-3>","<p>The transformer model <strong>Codex</strong> by OpenAI was introduced in <a href=""https://arxiv.org/abs/2107.03374"" rel=""nofollow noreferrer"">a 2021 paper</a>. The paper does not give complete information about the architecture. Below I've quoted all the passages in the paper that give hints as to the architecture:</p>
<blockquote>
<p>...we hypothesized that a specialized GPT model, called Codex, could excel at a variety of coding tasks. This paper describes several early Codex models, whose descendants power GitHub Copilot and the Codex models in the OpenAI API.</p>
</blockquote>
<blockquote>
<p>We fine-tune GPT models containing up to 12B parameters on code to produce Codex.</p>
</blockquote>
<blockquote>
<p>...we hypothesized that it would be beneficial to fine-tune from the GPT-3 (Brown et al., 2020) model family...</p>
</blockquote>
<p>A table in the paper (Table 1) lists various Codex models studied in the paper, giving only the number of parameters for each one: 12M, 25M, 42M, 85M, 300M, 679M, 2.5B, and 12B.</p>
<p>So, what we can glean from this is: the models discussed in the paper are not the production Codex model that powers Copilot, but prototypes (so the production model could in theory be completely different - that's fine, I'm not asking about that one). A number of different size versions of the model are studied in this paper, and the only description of their architecture is that they are &quot;GPT models&quot;.</p>
<p>I'm not sure if this is underspecified, or if I just don't know the field well enough. Saying a model is &quot;a GPT model&quot; does not seem to specify the architecture uniquely, to me. I know I can go read the GPT-3 paper for more information, but <em>at minimum</em> if you say something is say a 300M parameter &quot;GPT-3 model&quot;, it seems to me I still don't know how many layers there are, how many attention heads, etc.</p>
<p>Can we deduce more about the shapes of these models? At least the number of layers and parameters per layer?</p>
","transformer"
"39363","Coding a conversational AI which remembers previous context","2023-02-28 18:13:50","","1","89","<natural-language-processing><transformer><generative-model><language-model>","<p>I am trying to code a proper conversational AI which remembers previous context and answers accordingly (something like a micro ChatGPT). Additionally I want the AI to work on a custom knowledge base stored in a vector database like pinecone.</p>
<p>Are there any implementation references available ?</p>
","transformer"
"39348","Transformers: how does stacking work?","2023-02-28 01:18:38","39366","1","1018","<transformer><encoder-decoder>","<p>An Encoder has as inputs : Q,K,V, but has single output i.e. 3 vs 1</p>
<p>How do you stack those ?</p>
<p>Is there more detailed diagram ?</p>
","transformer"
"39325","Are there versions of attention that do not require a key-value pair, but just act on one input?","2023-02-26 16:49:52","39327","1","240","<machine-learning><transformer><attention>","<p>Are there versions of attention that do not require a key-value pair, but just act on one input? Or does this idea simply not make sense?</p>
","transformer"
"39280","Why is it called a Seq2Seq model if the output is just a number?","2023-02-24 05:33:21","39282","1","68","<transformer><seq2seq>","<p>Why is it called a <code>Seq2Seq</code> model if the output is just a number?</p>
<p>For example, if you are trying to predict a movie's recommendation, and you are inputting a sequence of users and their ratings, shouldn't it be a <code>Seq2Number</code> model since you're only predicting 1 rating at a time?</p>
","transformer"
"39277","Latent Diffusion Model Can't Learn the Latent Space of a VAE for the MNIST-Fashion Dataset","2023-02-24 01:10:10","","1","681","<deep-learning><computer-vision><transformer><autoencoders><diffusion-models>","<p>I'm currently playing around with LDMs on the MNIST-Fashion dataset. I thought the VQVAEs used in the original paper were a bit overkill for what I'm doing (and I don't fully understand how they construct the discretized codebook latent space), so I went with a simple convolutional autoencoder with a kl-regularizer to map to an approximately gaussian latent space. I've run this model a few times and verified that it does reconstruct the original image inputs fairly well.</p>
<p>I run into issues when I try to use this model with my LDM implementation. I first made sure standard diffusion directly in image space works, which it does. I then tried latent diffusion with my trained autoencoder and I can't get the loss to drop below a certain threshold (~1.0), and the image outputs are pretty much gaussian noise still.</p>
<p>As stated in the paper, I sample latent vectors from the encoder part of the autoencoder and scale them with stats collected from the first batch of data like so:</p>
<pre><code>                    batch = autoencoder.encode(batch).sample().detach()
                    # rescale the embeddings to be unit variance
                    if epoch == 0 and step == 0:
                        print(&quot;Calculating scale factor...&quot;)
                        std = batch.flatten().std()
                        scale_factor = 1. / std
                        cfg.scale_factor = scale_factor.item()
                    batch *= scale_factor
</code></pre>
<p>And then pretty much everything else (applying noise, calculating loss, etc) is the same as standard diffusion. Am I missing something, or is the latent space of my simple conv-autoencoder hard to learn for some reason? I would think that, since an autoencoder is just a deterministic mapping from image space to a lower dimensional one, that the LDM should be able to learn the latter just fine, or is there something inherent in the original paper's transformer architectures for the encoder / codebook latent space that is important to learning for the LDM?</p>
","transformer"
"39249","Why do LLMs like GPT-3 or Bloom use Vanilla Transformer instead of long sequence variants like Transformer-XL?","2023-02-21 18:14:31","","3","161","<transformer><large-language-models>","<p>Is there any particular reason that the most recent and successful large language models like GPT-3 or Bloom utilize a vanilla Transformer architecture instead of an arguably superior long sequence architecture like, e.g. Transformer-XL, LongFormer, BigBird, etc.?</p>
<p>In case you have any ideas or insights, please let me know.</p>
","transformer"
"39151","""Attention is all you need"" paper : How are the Q, K, V values calculated?","2023-02-14 10:37:24","39195","8","6248","<transformer><attention><word-embedding>","<p>The seminal <a href=""https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">Attention is all you need</a> paper (Google Brain team, 2017) introduces Transformers and implements the attention mecanism with &quot;queries, keys, values&quot;, in an analogy to a retrieval system.</p>
<p>I understand the whole process of multi-head attention and such (<em>i.e.</em>, what is done with the <span class=""math-container"">$Q$</span>, <span class=""math-container"">$K$</span>, <span class=""math-container"">$V$</span> values and why), but I'm confused on <strong>how these values are computed in the first place</strong>. AFAICT, the paper seems to completely leave that out.</p>
<p>Both Figure 2 of the paper and equations explaining Attention and Multihead attention start with <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span>,<span class=""math-container"">$V$</span> already there :</p>
<p><a href=""https://i.sstatic.net/t6qJz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/t6qJz.png"" alt=""enter image description here"" /></a></p>
<p>The answers regaridng the origin of <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span>,<span class=""math-container"">$V$</span> I've found so far haven't satisfied me :</p>
<ul>
<li><p>In this <a href=""https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms"">similar question</a>, the <a href=""https://stats.stackexchange.com/a/424127/201218"">accepted answer</a> says &quot;<em>The proposed multihead attention alone doesn't say much about how the queries, keys, and values are obtained, they can come from different sources depending on the application scenario.</em>&quot;. If this is the case, then why isn't the computing of <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span>,<span class=""math-container"">$V$</span> made more clear in the paper, at the very least for the task of language translation for which they show some numerical results and so obviously did compute <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span>,<span class=""math-container"">$V$</span> in some way ?</p>
</li>
<li><p>I also see some answers (eg <a href=""https://stats.stackexchange.com/a/463320/201218"">this one on the same question</a>) which say that <span class=""math-container"">$Q$</span>, <span class=""math-container"">$K$</span>, <span class=""math-container"">$V$</span> are the result of multiplication of the input embedding with some matrices <span class=""math-container"">$W$</span>. This is also what is shown in the popular blog post <a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">The Illustrated Transformer</a> :</p>
</li>
</ul>
<p><a href=""https://i.sstatic.net/8QJ6pl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8QJ6pl.png"" alt=""enter image description here"" /></a></p>
<p>These &quot;projection&quot; matrices (<span class=""math-container"">$W^Q$</span>, <span class=""math-container"">$W^K$</span>, <span class=""math-container"">$W^V$</span>) do seem to appear in the the definition of attention in the definition of <span class=""math-container"">$head_i$</span> (see top figure), but according to that equation, these matrices are multiplied by <span class=""math-container"">$Q$</span>, <span class=""math-container"">$K$</span>, <span class=""math-container"">$V$</span> (still appearing out of thin air, so the problem of their definition remains) and so the resulting product can't also be <span class=""math-container"">$Q$</span>, <span class=""math-container"">$K$</span>, <span class=""math-container"">$V$</span>.</p>
<p>How are the <span class=""math-container"">$Q$</span>, <span class=""math-container"">$K$</span>, <span class=""math-container"">$V$</span> values computed ?</p>
","transformer"
"39111","How do they make transformers bigger/deeper?","2023-02-10 20:51:34","","0","90","<transformer><gpt>","<p>I can find a million explanations of the diagram in the original transformer paper:</p>
<p><a href=""https://i.sstatic.net/jIwvJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jIwvJ.png"" alt=""enter image description here"" /></a></p>
<p>But I know that modern GPT models have many millions of weights. Where are they?  Or in other words, how does this thing scale?</p>
","transformer"
"39073","Why does CLIP use a decoder-only transformer for encoding text?","2023-02-07 13:23:44","","4","2251","<natural-language-processing><computer-vision><transformer><vision-transformer>","<p>In CLIP [1], the authors train a model to learn multi-modal (text, vision) embeddings by maximizing the cosine similarity between text and image embeddings produced by text and image encoders.</p>
<p>For the text encoder, the authors choose to use a variant of GPT2 which is a decoder-only transformer, taking the activations of the highest layer of the transformer at the [EOS] token the feature representation of the text (emphasis mine):</p>
<blockquote>
<p>The text encoder is a Transformer (Vaswani et al., 2017) with the architecture modifications described in <strong>Radford et al. (2019)</strong>. As a base size we use a 63M-parameter 12- layer 512-wide model with 8 attention heads. The trans- former operates on a lower-cased byte pair encoding (BPE) representation of the text with a 49,152 vocab size (Sennrich et al., 2015). For computational efficiency, the max sequence length was capped at 76. The text sequence is bracketed with [SOS] and [EOS] tokens and the <strong>activations of the highest layer of the transformer at the [EOS] token are treated as the feature representation of the text</strong> which is layer normalized and then linearly projected into the multi-modal embedding space.</p>
</blockquote>
<p>I found this pretty weird considering that they could have used an encoder (a-la BERT) which to me seem more fitted to act as encoders than decoders. Perhaps they wanted to enable generative text capabilities, but they could've achieved that with an encoder-decoder architecture (a-la T5) too.</p>
<p>I was expecting ablations on the text-encoder architecture, motivating their choices, but found none. Any clue why they made these choices?</p>
<h1>References:</h1>
<p>[1] A. Radford et al., ‘Learning Transferable Visual Models From Natural Language Supervision’, in Proceedings of the 38th International Conference on Machine Learning, Jul. 2021, pp. 8748–8763. Accessed: Feb. 07, 2023. [Online]. Available: <a href=""https://proceedings.mlr.press/v139/radford21a.html"" rel=""nofollow noreferrer"">https://proceedings.mlr.press/v139/radford21a.html</a></p>
","transformer"
"39067","When multiple stacked encoders are used, do the decoders only attend to the output of the final encoder layer?","2023-02-06 20:16:28","40131","0","348","<neural-networks><machine-learning><transformer>","<p>After looking into transformer-based models that used multiple stacked encoders and decoders, I am trying to understand how cross attention in the decoders work. In a transformer with a single encoder/decoder, my understanding is the queries come from the decoder, while the keys and values come from the output of the encoder.</p>
<p>But when multiple stacked encoders are used, do the decoders only attend to the output of the final encoder layer? Or do they attend to all the encoder layers?</p>
<p>To put it visually, does cross attention look like this?</p>
<p><a href=""https://i.sstatic.net/ZYdzv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZYdzv.png"" alt=""Attends hidden state of final encoder layer"" /></a></p>
<p>Or like this?</p>
<p><a href=""https://i.sstatic.net/77WM3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/77WM3.png"" alt=""Attends all encoder layers"" /></a></p>
","transformer"
"39050","Computation required for GPT model to choose likely word from n-options where n < total vocabulary size","2023-02-05 00:36:26","","1","48","<natural-language-processing><math><transformer><gpt><gpt-3>","<p>Let’s imagine two different use cases for a LLM/GPT-3.</p>
<ol>
<li>Predicting the next most likely word in a sequence using all ~50k words in its dictionary (i.e. the standard method of prompting a LLM)</li>
<li>Checking whether &quot;Word-1&quot; is more likely than &quot;Word-2&quot; to be next in a sequence</li>
</ol>
<p>How much more computationally efficient is #2?</p>
<p>My understanding is that the computation of the attention mechanism is dependent on the length of the prompt (so will be the same) and takes up most of the computation needed to get the output (but to what extent, I'm not sure). The difference will be in the decoding stage.</p>
<p>Would the one matrix multiplication in the decoding calculation be the only computation using the smaller 2-row matrix instead of the 50k-row matrix or are there other improvements in efficiency?</p>
","transformer"
"38983","Is there any evidence that the bias terms thelp in the attention mechanism of the transformers?","2023-01-31 20:54:24","","0","302","<transformer><bias>","<p>In the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">original transformer paper</a>, the attention mechanism uses parameter matrices, but no bias terms. However, in more recent implementations I see people often using a bias term when computing &quot;key&quot;, &quot;query&quot;, and &quot;value&quot;. For example, in Andrej Karpathy's recent <a href=""https://github.com/karpathy/nanoGPT/blob/master/model.py"" rel=""nofollow noreferrer"">implementation of GPT</a>, whether a bias term is used can be determined in the config:</p>
<pre><code>bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster
</code></pre>
<p>This makes me wonder whether there is any evidence that the bias terms help. In particular, if, according to Karpathy, not using bias is &quot;a bit better and faster&quot;, why is he using them by default?</p>
","transformer"
"38973","""Following instructions"" as an emergent behaviour in transformer models - isn't this fundamentally different from the models' basic purpose?","2023-01-31 08:54:16","","0","113","<neural-networks><transformer><gpt>","<p>I am not technically familiar with AI or neural networks beyond a tech news reading level of knowledge, so I apologise if this is a dumb question.</p>
<p>I was recently reading <a href=""https://arstechnica.com/gadgets/2023/01/the-generative-ai-revolution-has-begun-how-did-we-get-here/3/"" rel=""nofollow noreferrer"">this article</a> on <a href=""https://www.arstechnica.com"" rel=""nofollow noreferrer"">Ars Technica</a>.  It is a high level description of the history of generative AI models (it's a very good article, highly recommended).</p>
<p>When discussing large language models, the following passage appears:</p>
<blockquote>
<p>But there was also a surprise. The OpenAI researchers discovered that in making the models bigger, they didn’t just get better at producing text. The models could learn entirely new behaviors simply by being shown new training data. In particular, <strong>the researchers discovered that GPT3 could be trained to follow instructions in plain English without having to explicitly design the model that way.</strong> (emphasis added)</p>
</blockquote>
<p>I have three questions:</p>
<ol>
<li><p>Was this instruction-following behaviour truly emergent, as in completely unexpected and unplanned for?</p>
</li>
<li><p>This seems completely at odds with the usual description of transformer models as transforming text of one kind into another.  Following instructions seems, at a conceptual level, to be something much higher? If this is the case, do we have any idea how this emerged, and what properties of the model it is rooted in?</p>
</li>
<li><p>Do we have any idea how wide the scope of this kind of &quot;instruction following&quot; is? I.e. can the models make sense and respond &quot;sensibly&quot; (not correctly - I am not concerned with the correctness of the response, but with its relation to the instruction) to <em>any</em> instruction related to text?  Or are there specific kinds of instructions they are able to comprehend, and others they fail at?</p>
</li>
</ol>
","transformer"
"38940","Would a transformer trained on highly specific material be as usable as a commercial product like ChatGPT?","2023-01-28 23:44:52","38948","2","164","<transformer><gpt><language-model><fine-tuning><chatgpt>","<p>Soft question here.</p>
<p>I was recently learning a bit about how it is feasible to train a transformer on a personal computer like an M1 Mac. I have been told that the model could have 1-3 million parameters and the training data could be from 1GB - 1TB, and that the training could take from about a day to a week. Also, there is an open source GPT <a href=""https://github.com/karpathy/nanoGPT"" rel=""nofollow noreferrer"">here</a>.</p>
<p>My question is, if you consider that ChatGPT is trained on a very large and diverse amount of data, you may think a solo project could never compete with it. However, what if you chose a specialized set of training data that was smaller but a much richer, more reliable knowledge base, like only academic science textbooks, or only English literature, or only Python libraries documentation, and so on?</p>
<p>Could it actually be much more useful because it's open-source, you have freedom of use (unlike ChatGPT's heavy behavioral conditioning from OpenAI), and you can choose what kind of knowledge the transformer contains? If the data is smaller but way, way higher quality, could you just make a library of niche GPTs for any topic you are studying?</p>
","transformer"
"38854","How to assess if OpenAI's ChatGPT chatbot has a human in the loop?","2023-01-22 23:17:41","","0","1126","<transformer><attention><open-ai><chatgpt>","<p>I've asked a <a href=""https://ai.stackexchange.com/questions/38604/how-is-chatgpt-able-to-repeat-random-numbers/"">question</a> and <a href=""https://ai.stackexchange.com/a/38611/8221"">given</a> a <a href=""https://ai.stackexchange.com/a/38670/8221"">couple</a> <a href=""https://ai.stackexchange.com/a/38774/8221"">answers</a> that propose the OpenAI ChatGPT chatbot has humans in the loop (HITL), and that explains the chatbot's extraordinary abilities.</p>
<p>I've been repeatedly told this is absurd.  However, I haven't been given a clear reason why this is absurd, nor how the critic knows HITL is absurd.</p>
<p>Here are my reasons in a nutshell.</p>
<ol>
<li><p>ChatGPT's capabilities seem to violate what a neural network can do, and give explicit indication of being human driven.  See <a href=""https://ai.stackexchange.com/a/38611/8221"">this answer</a> for a running catalogue of examples I've published.</p>
</li>
<li><p><a href=""https://beta.openai.com/docs/guides/safety-best-practices"" rel=""nofollow noreferrer"">OpenAI's own documentation</a> states that HITL is best practice, and should be done whenever possible, especially in high stakes domain. OpenAI has a $10B deal on the table with Microsoft, so the ChatGPT chatbot seems like a high stakes domain.</p>
</li>
</ol>
<blockquote>
<p>Wherever possible, we recommend having a human review outputs before they are used in practice. This is especially critical in high-stakes domains, and for code generation.</p>
</blockquote>
<ol start=""3"">
<li><p>The main criticism of my position is ChatGPT's response speed.  However, a fast response is doable if the AI is responding most of the time, while humans monitor, and occasionally intervening, such as with a <a href=""https://beta.openai.com/docs/guides/completion/inserting-text"" rel=""nofollow noreferrer"">suffix prompt</a> to guide the AI response.  Plus, I've had a number of experiences where ChatGPT does not respond quickly, and it seems like a human is typing.</p>
</li>
<li><p>The other main criticism is that a company like OpenAI would never do something so fraudulent.  However, technically OpenAI does strongly imply in their documentation that they use HITL, and OpenAI has never said they don't use HITL.  It is only the popular media that claims ChatGPT is pure AI.  Additionally, use of HITL under the guise of AI is <a href=""https://www.bloomberg.com/news/articles/2016-04-18/the-humans-hiding-behind-the-chatbots"" rel=""nofollow noreferrer"">actually</a> <a href=""https://datafloq.com/read/ai-wizard-of-oz-pay-no-attention-human/"" rel=""nofollow noreferrer"">common</a> <a href=""https://www.inc.com/jessica-stillman/meet-the-people-who-teach-chatbots-to-sound-like-humans.html"" rel=""nofollow noreferrer"">practice</a> in <a href=""https://journals.sagepub.com/doi/10.1177/20539517211016026"" rel=""nofollow noreferrer"">industry</a>.  So, if ChatGPT does have HITL, this would not be fraud on OpenAI's part, and would be in line with AI industry standards.</p>
</li>
</ol>
<p>Now, what I would like in an answer is a clear articulation of the following points:</p>
<p>A) How you know ChatGPT does not have HITL.  This needs to be a clear statement in the negative from an official source, or some other evidence based analysis.  So far people have only pointed to the ChatGPT main page which states you are interacting with ChatGPT, but this does not say you are not interacting with a HITL, so does not count.</p>
<p>B) Bonus: explain how ChatGPT can repeat long random numbers and recognize its own comments, the two examples I've documented in <a href=""https://ai.stackexchange.com/a/38611/8221"">this answer</a> that seem to defy what a neural network is capable of doing.  Please give a technically detailed response, ideally referencing <a href=""https://dugas.ch/artificial_curiosity/GPT_architecture.html"" rel=""nofollow noreferrer"">GPT's architecture</a>.  Handwaving about magical abilities of transformers and self attention does not count.  I need a testable break down of how such a capability would be specifically instantiated using transformers and self attention.</p>
<p>UPDATE: Philosophical answers are <strong>not</strong> acceptable.  Philosophical answers are things like:</p>
<ul>
<li>the burden of proof is mine</li>
<li>AGI is inevitable so we should expect ChatGPT to be getting there</li>
<li>we can't know for sure, so let's just assume it's AI</li>
</ul>
<p><strong>Only</strong> technical answers are acceptable.  Such as:</p>
<ul>
<li>OpenAI explicitly says there is no HITL involved in this linked doc</li>
<li>GPT's transformer architecture can repeat random numbers in this precise manner</li>
<li>Self recognition can be encoded using embeddings in this precise way, here is a working example</li>
</ul>
","transformer"
"38717","Is $i$ indexing the first or second dimension in $\mathbf{x}_i$, where $\mathbf{x} \in \mathbb{R}^{n\times d}$?","2023-01-11 19:10:13","38727","0","34","<neural-networks><math><transformer><linear-algebra><vector-space>","<p>I was reading the following notes on the math behind transformers and was confused about what <span class=""math-container"">$\mathbf{x}_i$</span> is? If <span class=""math-container"">$\mathbf{x} \in \mathbb{R}^{n\times d}$</span>, then is the <span class=""math-container"">$i$</span> indexing the <span class=""math-container"">$n$</span> or the the <span class=""math-container"">$d$</span>? Am I correct to conclude that it has to be <span class=""math-container"">$n$</span> since the <span class=""math-container"">$W$</span>'s are in <span class=""math-container"">$\mathbb{R}^{d\times k}$</span>?</p>
<p><a href=""https://johnthickstun.com/docs/transformers.pdf"" rel=""nofollow noreferrer"">https://johnthickstun.com/docs/transformers.pdf</a></p>
","transformer"
"38658","Does the position of the tokens in Vision Transformer matter?","2023-01-08 11:08:02","39442","3","706","<computer-vision><transformer><vision-transformer>","<p>I am reading through the <a href=""https://paperswithcode.com/method/vision-transformer"" rel=""nofollow noreferrer"">Vision Transformer</a> paper and other related papers, such as <a href=""https://arxiv.org/pdf/2012.12877.pdf"" rel=""nofollow noreferrer"">DeiT</a> and <a href=""https://arxiv.org/pdf/2203.12119.pdf"" rel=""nofollow noreferrer"">Visual Prompt Tuning (VPT)</a>. I wonder if the position of the tokens that flow through the Transformer encode matters? Let's break it down, assuming we use ViT 16x16:</p>
<ul>
<li>The original Vision Transformer has 1 classifier <code>[CLS]</code> token, along with 16x16=196 image tokens. In general, we have 197 tokens overall, where the position is: <code>[[CLS], [Image Patches] x 196]</code></li>
<li>DeiT further adds one Distillation token to the end of the sequence, so we have 198 overall: <code>[[CLS], [Image Patches] x 196, [Distill Token]]</code></li>
<li>However, I notice a very strange thing with VPT that they add the Prompt token <strong>after the <code>[CLS]</code> but before the Image patches</strong>. So the position of the tokens in VPT is: <code>[[CLS], [Prompts] x N, [Image Patches] x 196]</code>.</li>
</ul>
<p>I wonder does such a thing matter? What if we change the position of these tokens, e.g., putting the Prompt tokens to the last?</p>
","transformer"
"38544","Explanation of Cross-Modality Linear Transformer","2022-12-31 17:37:32","38564","1","85","<transformer>","<p>So I am trying to understand how a Cross-Modality Linear Transformer is different from an a basic transformer. I found the transformer mentioned in this <a href=""https://arxiv.org/pdf/2208.15001.pdf"" rel=""nofollow noreferrer"">paper</a>. Am I correct in understanding that, the transformer  is cross-modality because it is going from one medium to another. In this case, text to motion. Then linear because the time complexity to calculate the attetion weights are linear? I have troubles with understanding transformers. I just was able to understand the basic concept of them. So any help would be great. Thanks</p>
","transformer"
"38397","What's the relationship between number of heads and embedding dimension in Transformers?","2022-12-19 00:18:54","","4","3219","<deep-learning><natural-language-processing><transformer><attention><hyper-parameters>","<p>I am reading the book: Natural Language Processing with Transformers. It has the following paragraph</p>
<blockquote>
<p>Although head_dim does not have to be smaller than the number of embedding dimensions of the tokens (embed_dim), in practice it is chosen to be a multiple of embed_dim so that the computation across each head is constant. For example, BERT has 12 attention heads, so the dimension of each head is 768/12=64.</p>
</blockquote>
<p>While learning transformers, I tried to draw an analogy between CNN filters and multi-headed attention. For instance, increasing the number of filters helps learn different image features, while increasing number of heads help better understand the semantics of a sentence. However, it seems like that the input to the transformer (after being converted to embeddings) is being divided across heads. Perhaps my understanding of multi-head attention is incorrect.</p>
<p>Basically, I want to know why the author is dividing the inputs across the heads rather than feeding all the inputs to it.</p>
","transformer"
"38365","NLP: Question answer with 2 contexts","2022-12-16 12:52:41","","0","94","<natural-language-processing><transformer><question-answering>","<p>Is there a Hugging Face Transformer that takes 2 contexts as input for question answering? For example, I could have transcript of a meeting in first context and agenda of the meeting in the second context. Then ask a question - Did the meeting adhere to the discussion topics given in the agenda? Alternatively, can you suggest how to solve such types of question answering problems that might require multiple contexts?</p>
","transformer"
"38356","Fine Tuning Transformer Model for Machine Translation","2022-12-16 05:37:06","","0","462","<transformer><machine-translation><pretrained-models><seq2seq>","<p>I am working on the Transformer example demonstrated on TensorFlow's website. <a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<p>In this example, Machine Translation model is trained to translate from Portuguese to English. The transformer is coded from scratch and other popular libraries like huggingface are not used.</p>
<p>Let's say I have another dataset which includes pairs of sentences of Portuguese and Finnish and let's say this dataset is fairly small. Since it is a small dataset, I want to use my model trained on Portuguese to English as a PreTrained model for creating the translation model for Portuguese to Finnish.</p>
<p>My question is, what are the key points to consider when using such a PreTrained model and changing ONLY its decoder output structure?</p>
","transformer"
"38340","Why do the values in the cross attentional mechanism within a transformer come from the encoder and not from the decoder?","2022-12-15 10:09:29","","1","1006","<natural-language-processing><transformer><attention>","<p>The transformer architecture contains a cross attention mechanism which is enriching the encoder with information from the decoder. The place where this takes place is visualized in the image below:</p>
<p><a href=""https://i.sstatic.net/L3ifH.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/L3ifH.jpg"" alt=""transformer_architecture"" /></a></p>
<p>The cross attention mechanism within the original transformer architecture is implemented in the following way:</p>
<p><a href=""https://i.sstatic.net/BBwHl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BBwHl.jpg"" alt=""cross_attention_computation"" /></a></p>
<p>The source for the images is <a href=""https://www.youtube.com/watch?v=EixI6t5oif0"" rel=""nofollow noreferrer"">this video</a>.
Why are the values in this step coming from the encoder instead of from the decoder? Is this where e.g. the actual language translation happens within a transformer?</p>
","transformer"
"38069","What type of neural network architecture allows filtering out of unwanted sounds?","2022-11-29 00:44:31","","1","72","<convolutional-neural-networks><transformer><time-series><audio-processing>","<p>I have a use case where I will be inputting audio to a model, and the output of the model will be the same audio except with certain sounds removed (volume set to zero).  The dataset is generated by taking an audio file, duplicating it, and then zeroing out the unwanted sounds (usually a half second long).</p>
<p>I believe a neural network architecture is needed here with the input being the undisturbed audio and its spectrogram.  The output is then the modified/cleaned audio.</p>
<p>What model architectures would work for this use case?  I would potentially like to have this run real-time as a person is speaking.</p>
","transformer"
"38025","How do transformers compare to CNNs in terms of compute budget (and computing time) during inference?","2022-11-25 08:55:29","","1","90","<convolutional-neural-networks><comparison><transformer><bounding-box>","<p>Transformers are data and GPU hungry during training. Is this also true at inference time? How do transformers compare to feedforward CNNs e.g., during bounding box generation at inference time? I haven't found a good comparison of computing time and computational resources.</p>
","transformer"
"37997","What is the intuition behind self-attention?","2022-11-23 04:32:12","38000","5","1411","<neural-networks><transformer><attention>","<p>I've been watching a few lectures on transformers, especially for language translation, though it seemingly becomes more confusing the more I watch.</p>
<p>In <a href=""https://www.youtube.com/watch?v=QvkQ1B3FBqA&amp;list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI&amp;index=2"" rel=""nofollow noreferrer"">this lecture</a>, there seems to be two conflicting views of self-attention. First, there's an Iron Man example (at around 44:25) where the lecturer claims that self-attention helps identify the important aspects of the input, but she details the math in which a &quot;self-attention heat map&quot;, which is the dot product of the query and key matrices, is multiplied by a value matrix giving an output, which I assume is an attention head. It seems like this attention head just encodes how words are related to each other (the map) and the words themselves (the value).</p>
<p>I don't understand how this extracts the important information, or why you only want the important information anyways for translation. Is it extracting the important <strong>relationships between words</strong>, because that would make more sense, but in what way does that relate to her Iron Man example then? What's the relationship of Iron Man to himself? Also, would each attention head contain its own set of three matrices? Is the idea perhaps that words that more words depend on are important? But wouldn't that simply remove single words that still are important for translation?</p>
<p>TLDR: Essentially, how does the idea of relationships between words translate to selecting important words in a sentence (if this is even what self-attention does).</p>
","transformer"
"37916","Use multiple embeddings with the Transformer architecture","2022-11-16 14:40:29","","1","199","<neural-networks><transformer><embeddings>","<p>In their <a href=""https://openai.com/blog/musenet/"" rel=""nofollow noreferrer"">article</a> about their system &quot;<em>MuseNet</em>&quot; OpenAI state the following:</p>
<blockquote>
<p>Embeddings</p>
<p>We added several different kinds of embeddings to give the model more
structural context. In addition to the standard positional embeddings,
we added a learned embedding that tracks the passage of time in a
given sample. This way, all of the notes that sound at the same time
are given the same timing embedding. We then add an embedding for each
note in a chord (this mimics relative attention, since it will be
easier for the model to learn that note 4 needs to look back at note
3, or else at note 4 of the previous chord). Finally, we add two
structural embeddings which tell the model where a given musical
sample is within the larger musical piece. One embedding divides the
larger piece into 128 parts, while the second encoding is a countdown
from 127 to 0 as the model approaches the (end) token.</p>
</blockquote>
<p>I was wondering how to implement and make use of multiple different embeddings for the Transformer architecture. At the moment, the only things that come to my mind would be to sum the embeddings in a similar fashion as is done with the positional encoding, or to somehow concatenate them, where (if the embeddings are of length <code>l</code> each) the final embeddings would be of length <code>2l</code> for each of the elements.</p>
","transformer"
"37853","How are gradients of individual layers computed?","2022-11-12 17:11:07","","2","212","<neural-networks><natural-language-processing><gradient-descent><transformer>","<p>I have been reading some papers recently (example: <a href=""https://arxiv.org/pdf/2012.00363.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2012.00363.pdf</a>) which seem to be training individual layers of, say, a transformer, holding the rest of the model frozen/constant. In the case of the paper I read, this was done in an attempt to minimize parameter changes so as to reduce knowledge &quot;lost&quot; by a model when it is updated for new information.</p>
<p>My question is, how are individual layers of a transformer trained? Like, if we run the transformer and get a gradient, how can we use that gradient to train, say, the first layer, without affecting the rest of the layers at all?</p>
","transformer"
"37832","Why can't AI image generators output verbatim text when prompted to do so?","2022-11-10 13:24:37","37834","4","18278","<transformer><image-generation>","<p>I want to create a splash screen that includes the name of my project.  <a href=""https://en.wikipedia.org/wiki/DALL-E"" rel=""nofollow noreferrer"">DALL-E</a> 2 changed some of the letters in the name, even when I tried putting the name of my project in double-quotes (<code>&quot;</code>).</p>
<p><a href=""https://twitter.com/dandv/status/1603480097683410944/photo/1"" rel=""nofollow noreferrer"">Other prompts</a> to create images with short verbatim text, resulted in text that was not in the prompt:</p>
<p><a href=""https://i.sstatic.net/6KJkV.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6KJkV.jpg"" alt=""DALL-E2 prompt with verbatim text failed"" /></a></p>
<p>However, the OpenAI blog post announcement for DALL-E (1) shows an example of the text &quot;OpenAI&quot; being output correctly:</p>
<p><a href=""https://i.sstatic.net/7wKtf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7wKtf.png"" alt=""OpenAI verbatim text"" /></a></p>
<p>What is the mechanism that causes DALL-E 2 to be unable to output the text as expected?</p>
","transformer"
"37624","Why do transformers have a fixed input length?","2022-10-25 12:07:04","","4","5402","<transformer><models>","<p>From what I understand, Transformer Encoders and Decoders use a fixed number of tokens as input, e.g., 512 tokens. In NLP for instance, different text sentences have a different number of tokens, and the way to deal with that is to truncate the longer sentences and pad the shorter ones. As an additional input, a padding mask must be supplied to the Transformer so that its attention is only focused on the relevant tokens.</p>
<p><strong>My question is: Is there something in the architecture that forces the transformer to have a fixed number of tokens as input? (and not adopt dynamically to the actual input length like RNNs for instance?)</strong></p>
<p>For comparison, I think of fully-convolutional networks or RNNs with variable input lengths. They are agnostic to the actual input dimension because they perform pointwise operations on the different patches.
When applying an RNN model to an n-tokens sentence, you compute the same block n times, and when computing it on a k-tokens sentence you will apply it k times. So this architecture does not require padding or truncating (at least not in theory, I do not refer here to implementation considerations).
In transformers: embedding the tokens, computing attention, and feed-forward can be performed on different lengths of sequences since the weights are applied per token, right? So why do we still truncate and pad to a fixed size? Or perhaps it is feasible but not implemented in practice for other reasons?</p>
<p>I must be missing something...</p>
<p><strong>I'll ask it differently to make my question more clear:</strong>
Say I have an already-trained transformer model, trained on 512 fixed-sized inputs (truncated and padded). At inference time, if I would like to process a single, shorter sentence. Do I have to pad it or not?</p>
<p>Thanks</p>
","transformer"
"37472","How do Transformers compute the words embeddings at inference time since the embeddings are dynamic?","2022-10-17 07:45:17","","0","1238","<transformer><word-embedding><inference>","<p>In Word2Vec, the embeddings don't depend on the context.</p>
<p>But in Transformers, the embeddings depend on the context.</p>
<p>So how are the words' embeddings set at inference time?</p>
","transformer"
"37427","What is the difference between Neurosymbolic AI and Transformer AI","2022-10-14 14:21:10","37456","2","589","<transformer><neurosymbolic-ai>","<p>I'm looking at the AI timeline and I came across Neuro-Symbolic AI (being symbolic AI used in combination with deep learning) and Transformer AI (which I understand as neural networks that take context into account).</p>
<p>If I look at <a href=""https://en.wikipedia.org/wiki/Neuro-symbolic_AI#:%7E:text=Examples%20include%20BERT%2C%20RoBERTa%2C%20and,how%20to%20evaluate%20game%20positions."" rel=""nofollow noreferrer"">Wikipedia</a> then I'll find that GTP-3 is a form of Neuro-Symbolic AI, but its name is clearly a giveaway for Transformer AI (Generative Pre-trained Transformer-3).</p>
<p>So I'm left wondering; what exactly is the different between these two?</p>
","transformer"
"37395","Is the decoder in a transformer Seq2Seq model non parallelizable?","2022-10-12 18:01:00","","1","271","<deep-learning><transformer><seq2seq>","<p>From my understanding, seq2seq models work by first computing a representation of the input sequence, and feeding this to the decoder. The decoder then predicts each token in the output sequence in an autoregressive manner. In this sense, it's limited to processing one time step at a time, as the next token in the sequence depends on the previous (non parallelizable). This is different from the encoder, as it's able to process many time steps in parallel. Is my understanding correct?</p>
","transformer"
"37326","can I add to a language model a prompt with output example?","2022-10-07 20:36:26","","0","52","<transformer><gpt><fine-tuning><prompt>","<p>I want to finetune GPT2 to extract relevant data from a given text. So for (a trivial) example, given the text &quot;<em>the car was manufactured in X, can reach Y km/h, and has Z horse powers</em>&quot;, my desired output would be <em>manufacturer: <code>X</code>, max speed:<code>Y</code>, horsepowers:<code>Z</code></em>.</p>
<p>I don't have a lot of labeled data, so I thought it would be reasonable to take every training sample and add to it a prefix than contains an actual example. That is - Instead of providing the model with the text</p>
<blockquote>
<p>INPUT: a well-known brand, the new <code>X</code>, can reach <code>Y</code> km/h, and has <code>Z</code> horsepower</p>
<p>OUTPUT:</p>
</blockquote>
<p>and expect the model to understand how to fill in the details, I would provide a longer prompt that contains an actual example like</p>
<blockquote>
<p>INPUT: the new <code>BMW</code> can reach up to <code>200</code> kmh. Even though the previous model disappointed the users, the brand-new one rocks a <code>thousand</code> engine horsepower</p>
<p>OUTPUT: Manufacturer: <code>BMW</code>, Max Speed: <code>200</code>, Horse Powers: <code>thousand</code></p>
<p>INPUT: though is mostly considered an outdated version of the cls200, the new <code>Mercedes</code> has the capabilities of reaching up to 100kmh in turns and <code>300</code>kmh overall</p>
<p>OUTPUT:</p>
</blockquote>
<p>Is this considered a common way of engineering the prompt? Do notice that the first provided example is the same for every training sample.</p>
","transformer"
"37283","If GPT-3 is trained on predicting the next token, how is it able to take commands?","2022-10-04 21:46:18","","2","1115","<natural-language-processing><transformer><gpt-3>","<p>From my understanding, GPT-3 is trained on predicting the next token from a sequence of tokens. Given this, how is it able to take commands? For instance, in this example input, wouldn't the statistically most likely prediction be to insert a period and end the sentence?</p>
<pre><code>Input: write me a beautiful sentence

Output: I cannot put into words how much I love you, so I'll just say it's infinite.
</code></pre>
","transformer"
"37267","How do Transformer decoders handle arbitrary length input?","2022-10-03 21:04:37","","4","1582","<natural-language-processing><transformer><machine-translation>","<p>I am working through a Tensorflow Neural Machine Translation tutorial (<a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a>) and am confused about how the decoder handles inputs when making inferences after it has been trained.</p>
<p>In the section where we create a class for translating a sentence (<a href=""https://www.tensorflow.org/text/tutorials/transformer#run_inference"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/transformer#run_inference</a>) it appears that we feed the decoder an array populated with the START token only, then append the last element from the predicted sequence it makes to the growing translated sentence. This does not make sense to me as we trained the transformer on a fixed length sequence that was padded to length MAX_TOKENS=128, so I can't figure out why it would be able to accept input of an arbitrary length tensor.</p>
<p>Here is the code for inference in question that confuses me:</p>
<pre><code>    # As the output language is English, initialize the output with the
    # English `[START]` token.
    start_end = self.tokenizers.en.tokenize([''])[0]
    start = start_end[0][tf.newaxis]
    end = start_end[1][tf.newaxis]

    # `tf.TensorArray` is required here (instead of a Python list), so that the
    # dynamic-loop can be traced by `tf.function`.
    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)
    output_array = output_array.write(0, start)

    for i in tf.range(max_length):
      output = tf.transpose(output_array.stack())
      predictions, _ = self.transformer([encoder_input, output], training=False)

      # Select the last token from the `seq_len` dimension.
      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.

      predicted_id = tf.argmax(predictions, axis=-1)

      # Concatenate the `predicted_id` to the output which is given to the
      # decoder as its input.
      output_array = output_array.write(i+1, predicted_id[0])

      if predicted_id == end:
        break

</code></pre>
<p><code>self.transformer</code> takes in the &quot;encoder_input&quot; which is fed to the encoder and &quot;output&quot; which is fed into the decoder, where we then take the last prediction in the sequence returned from the decoder, append it to &quot;output&quot;, then repeat the process. But I don't understand how the decoder can process a tensor of length 1, then 2, then 3, and so on if it is always expecting a tensor of length 128.</p>
<p>It seems like the input to the decoder should be padded to 128 tokens and then the prediction at the i-th index should be appended to the output (rather than always the last element in the predicted sequence) before repeating.</p>
<p>Is the tutorial mistaken in the implementation on of how this transformer preforms inference? Or am I missing something that Tensorflow does behind the scenes that allow this to work?</p>
<p>Note, I realize the title to this question is similar to <a href=""https://ai.stackexchange.com/questions/22957/how-can-transformers-handle-arbitrary-length-input"">How can Transformers handle arbitrary length input?</a> but the answer given appears to verify that the input to the decoder should be fixed length and padded, which contradicts how inference is done in the tutorial I am referencing. So I hope this question isn't considered a duplicate but I am asking a similar question in the context of the mentioned tutorial.</p>
<p>Thank you for any insight someone might be able to provide!</p>
","transformer"
"37152","Are neural networks a strict special case of a transformer?","2022-09-21 09:04:31","","0","1363","<neural-networks><transformer><feedforward-neural-networks>","<p>Since transformers contain a neural network, are they a strict generalisation of standard feedforward neural networks? In what ways can transformers be interpreted as a generalisation and abstraction of these?</p>
","transformer"
"36949","How to Train a Decoder for Pre-trained BERT Transformer-Encoder?","2022-09-01 12:47:15","","1","597","<transformer><bert><pretrained-models><seq2seq><encoder-decoder>","<p><strong>Context:</strong>
I am currently working on an encoder-decoder sequence to sequence model that uses a sequence of word embeddings as input and output, and then reduces the dimensionality of the word embeddings.</p>
<p>The word embeddings are created using pre-trained models. I want to be able to decode the word embeddings returned by the decoder of the Sequence to Sequence model back to natural language.</p>
<p><strong>Question:</strong> How can I train a Decoder that works with the sequence of word embeddings and the original sentence for this task?</p>
<p>See below for the code that generates the word embeddings:</p>
<pre><code>from typing import List

import numpy as np
import torch
from transformers.tokenization_utils_base import BatchEncoding
from transformers import BertTokenizerFast, BertModel

TOKENIZER = BertTokenizerFast.from_pretrained('bert-base-uncased')
MODEL = BertModel.from_pretrained('bert-base-uncased')

def get_word_indices(sentence: str, separator=&quot; &quot;) -&gt; List:
    sent = sentence.split(sep=separator)
    return list(range(len(sent)))

def encode_sentence(sentence: str) -&gt; BatchEncoding:
    encoded_sentence = TOKENIZER(sentence)
    return encoded_sentence

def get_hidden_states(encoded: BatchEncoding, layers: list = [-1, -2, -3, -4]) -&gt; torch.Tensor:
    with torch.no_grad():
        output = MODEL(**encoded)
    hidden_states = output.hidden_states
    output = torch.stack([hidden_states[i] for i in layers]).sum(0).squeeze()
    return output

def get_token_ids(word_index: int, encoded: BatchEncoding):
    token_ids = np.where(np.array(encoded.word_ids()) == word_index)
    return token_ids

def embed_words(sentence: str) -&gt; torch.Tensor:
    word_indices = get_word_indices(sentence)
    encoded_sentence = encode_sentence(sentence)
    hidden_states = get_hidden_states(encoded_sentence)
    word_embeddings = []
    for word_index in word_indices:
        # Get the ids of the word in the sentence
        # Important, because BERT sometimes splits words into subwords
        token_ids = get_token_ids(word_index, encoded_sentence)
        # Get all the hidden states for each word (or subwords belonging to one word)
        # Average the hidden states in case of subwords to retrieve word embedding
        word_embedding = hidden_states[token_ids].mean(dim=0)
        word_embeddings.append(word_embedding)
    return torch.stack(word_embeddings)
<span class=""math-container"">```</span>
</code></pre>
","transformer"
"36946","Surrogate model to produce time series from parameter set","2022-09-01 04:04:39","","1","54","<recurrent-neural-networks><python><transformer><time-series>","<p>Say I have a model <span class=""math-container"">$M$</span> that takes in a parameter vector <span class=""math-container"">$\beta$</span>, and produces a (numerical) time series. This could be a complicated model (e.g. a bespoke enzyme reaction model), or something simple like a VAR(1) (e.g. <span class=""math-container"">$x_{t}=\beta_0+\beta_1 x_{t-1}+\varepsilon_t$</span>). I can run this model multiple times with the same parameter vector, and get a different time series each run.</p>
<p>(Even for the same parameter vector, the output may be different each run, as is the case when you simulate a VAR(1) with known variance <span class=""math-container"">$\sigma^2_\varepsilon$</span>)</p>
<p><strong>How can I design/implement a neural network that acts as a surrogate to my model - i.e. when given a parameter set <span class=""math-container"">$\beta$</span>, it produces plausible time series sequences?</strong></p>
","transformer"
"36922","Using similarity score within lstm embedding for attention based mechanism","2022-08-29 16:14:19","","2","58","<machine-learning><transformer><attention>","<p>Yesterday, I found <a href=""https://pubmed.ncbi.nlm.nih.gov/34150797/"" rel=""nofollow noreferrer"">this</a> fascinating paper about predicting various clinical conditions using an attention based LSTM. I don't have any practical experience with attention mechanism or transformers, which might be the reason why I struggle to understand why this approach work. Well, let me first try to summarize what the authors accomplished</p>
<p><a href=""https://i.sstatic.net/vlO4C.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vlO4C.jpg"" alt=""enter image description here"" /></a></p>
<p>As a first step, the use some sort of model to predict the future course of some important clinical variables. Their outcome is denoted by <span class=""math-container"">$X_0...X_t$</span>. As a second step, these features (no timeseries data!) are passed into a transformer like architecture:</p>
<ul>
<li>as an embedding step, the predicted clinical variables along with some static ones are passed to an LSTM that computes several hidden states: <span class=""math-container"">$H_0...H_t$</span></li>
<li>the attention step receives  <span class=""math-container"">$h_0...h_t$</span>. For computing the contribution of each hidden state, the author states that each hidden state <span class=""math-container"">$h_i$</span> is passed to a function that computes the similarity between <span class=""math-container"">$h_i$</span> and <span class=""math-container"">$h_t$</span>, that is the last hidden state of the lstm. From each resulting score <span class=""math-container"">$s_i$</span>, a weight is computed with the softmax function <span class=""math-container"">$$w_i = \frac{exp(s_i)}{\sum_{j=0}^texp(s_j)}$$</span>. The context vector cv is then calculated by computing the weighted sum of hidden states.</li>
<li>the decoder receives the context vector concatenated with <span class=""math-container"">$h_t$</span> and computes the output using two dense layers.</li>
</ul>
<p>What I don't understand are two things:</p>
<ol>
<li>Is it legit to use a lstm as embedding, while the input is no time series data but independent features?</li>
<li>In the presented approach, the weight of an hidden state <span class=""math-container"">$h_i$</span> depends on the similarity to the output of the lstm, i.e. <span class=""math-container"">$h_t$</span>. But isn't an attention-based architecture used precisely to let every input variable contribute equally to the output, and to combat lstm's problem of struggling with long dependencies? With long inputs, i guess that <span class=""math-container"">$h_0$</span> tends to be likely to have larger differences from <span class=""math-container"">$h_t$</span> than, e.g. <span class=""math-container"">$h_{t-1}$</span> simply because it is more in the past. If this is true, <span class=""math-container"">$h_t$</span> automatically would receive a smaller weight.</li>
</ol>
<p>Are my thoughts correct or have I missed something?</p>
","transformer"
"36726","Shuffling vs Non-shuffling train/test set yields drastically different results","2022-08-12 21:05:29","","0","670","<neural-networks><tensorflow><keras><transformer>","<p>I am currently working with a very deep NN (200mio. to 350mio. params). My data set is roughly of shape (2mio, 350), i.e. 2mio samples and 350 features. In fact, the features are time series. As input to the NN I just pass the current state (1 time step), however that state is derived with some sort of scaling from past states.</p>
<p>Now I made a couple strange observation during training, I can't explain at all:</p>
<ol>
<li>When I shuffle the whole data set and then do train/test split, the classification performance is extremely good, ~0.93 accuracy (3 classes) and identically generalizes onto the test set during the whole training</li>
<li>When I split the data into train/test and just shuffle train, the performance is less on train, but still acceptable (~0.75 accuracy), but performance on test falls off to ~0.36 accuracy</li>
<li>When I just split the data without shuffling, the train performance further drops to ~0.5 and test performance to ~0.3 (which is worse than randomly guessing)</li>
</ol>
<p>What is going on here? I checked if I might have introduced some data leakage or lookahead bias in my data set manipulations, but couldn't find anything.</p>
<p>Anybody has an idea what is going on here?</p>
<p>Cheers</p>
","transformer"
"36688","Last linear layer of the decoder of a transformer","2022-08-10 11:56:41","38979","5","3287","<deep-learning><transformer>","<p>I am learning the transformers architecture from these two sources:</p>
<p><a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">https://arxiv.org/pdf/1706.03762.pdf</a></p>
<p><a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">https://jalammar.github.io/illustrated-transformer/</a></p>
<p>I just wanted to ask about the final step in the decoder. Let's fix testing time. As I understand, the decoder starts with an input of dimension <span class=""math-container"">$(N_{words},d_{emb})$</span>, where <span class=""math-container"">$N_{words}$</span> is the number of words already predicted and <span class=""math-container"">$d_{emb}$</span> is the embedding dimension.</p>
<p>Now if we &quot;follow&quot; the following decoder steps, at each step (after e.g. the attention layers) we should have a vector of dimension <span class=""math-container"">$(N_{words},d_{model})$</span> where <span class=""math-container"">$d_{model}$</span> is the model dimension. In other words, up to the final linear layer we have <span class=""math-container"">$N_{words}$</span> vectors which are <span class=""math-container"">$d_{model}$</span>-dimensional.</p>
<p>Are all these <span class=""math-container"">$N_{words}$</span> vectors fed into the last linear layer (before the softmax) or, as I suspect, only the last of these vectors is used ? In the latter case the last linear layer would be a matrix of dimension <span class=""math-container"">$d_{model}\times N_{vocab}$</span>, where <span class=""math-container"">$N_{vocab}$</span> is the vocabulary dimension.</p>
<p>Is this correct ? Are there any issues in what I wrote ? Unluckily from the online sources I was not able to clarify this point...</p>
<p>PS: I conjectured that the last linear layer is using just the last vector, because than I would understand what happens in training time, one would just use in that case all the output vectors from the decoder, instead of just the last one, to have a parallelized prediction.</p>
","transformer"
"36584","What is the most important predecessor of the transformer model?","2022-08-02 11:07:32","38339","3","163","<natural-language-processing><transformer><attention>","<p>I'm wondering what the origins of the transformer as proposed in <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a> are. The paper itself provides some interesting pointers to the literature on self-attention such as:</p>
<ol>
<li><a href=""https://aclanthology.org/D16-1244.pdf"" rel=""nofollow noreferrer"">A Decomposable Attention Model for Natural Language Inference</a></li>
<li><a href=""https://openreview.net/pdf?id=BJC_jUqxe"" rel=""nofollow noreferrer"">A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING</a></li>
<li><a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</a></li>
</ol>
<p>It seems like using attentional mechanisms was widespread (at least in combination with recurrent networks) and 'A Decomposable Attention Model for Natural Language Inference' paper from 2016 already conjectured that scaling attention might be feasible.
Is it from this prior work 'only' an engineering leap? Or what additional papers at the time likely influenced the architecture?</p>
","transformer"
"36395","why cross entropy loss has to be multiplied by a batch size during an evaluation in transformer model?","2022-07-19 02:11:38","","1","1301","<transformer><pytorch><cross-entropy><evaluation-functions>","<p>I am trying to look through a code of the transformer model from Pytorch. However,
I do not understand why batch size needs to multiply with cross-entropy loss given that loss is calculated based on data at a given timestep.</p>
<p>This is from the line: <strong>&quot;total_loss += batch_size * criterion(output_flat, targets).item()&quot;</strong></p>
<p><a href=""https://github.com/pytorch/tutorials/blob/master/beginner_source/transformer_tutorial.py#L323"" rel=""nofollow noreferrer"">This is the section</a> of code:</p>
<pre><code>def evaluate(model: nn.Module, eval_data: Tensor) -&gt; float:
    model.eval()  # turn on evaluation mode
    total_loss = 0.
    src_mask = generate_square_subsequent_mask(bptt).to(device)
    with torch.no_grad():
        for i in range(0, eval_data.size(0) - 1, bptt):
            data, targets = get_batch(eval_data, i)
            batch_size = data.size(0)
            if batch_size != bptt:
                src_mask = src_mask[:batch_size, :batch_size]
            output = model(data, src_mask)
            output_flat = output.view(-1, ntokens)
            total_loss += batch_size * criterion(output_flat, targets).item()
    return total_loss / (len(eval_data) - 1)
</code></pre>
","transformer"
"36372","Rationalle behind SE3 Transformer?","2022-07-17 18:53:05","36397","1","456","<neural-networks><transformer><graph-neural-networks>","<p>I have just finished reading the SE3 transformer paper (SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks) by Fuchs et-al and although I'm sure I understand less than 50% of the math involved, I have a lingering question:</p>
<p>Is there a reason one can't use distance matrix representation, e.g.:
<span class=""math-container"">$$
 M[i,j]=d(i,j),  \forall i &gt; j
$$</span>
as a valid rotation translation equivarient representation for point cloud data? Why is there a need for a neural network to encode such representation?</p>
<p>(of course, it is not a one - to - one representation, inverted/flipped point clouds can still map to the same distance matrix, but in most practical applications this distinction carry little significance)</p>
","transformer"
"36342","How special tokens in BERT-Transformers work?","2022-07-14 15:43:51","","0","499","<neural-networks><deep-learning><natural-language-processing><transformer>","<p>&quot;[SEP] tokens are useful to differentiate the questions from answers through type_ids&quot; Yes, but how is this helping model to understand that &quot;I should look paragraph and generate answers from here&quot;? We don't have if-else inside the model that will say: &quot;if type_id==1, generate questions from here&quot;</p>
<p>The same question appears for this example too:</p>
<p>[CLS] previous question [Q] current question [SEP] text [EOS]</p>
<p>How model says: &quot;I should look at the previous question, understand the meaning from here too with the current question, and answer the question.&quot; We need if-else in here too like:</p>
<p>If there is a previous question:
get the meaning from it and use it with the current question</p>
<p>One more example based on this paper:<a href=""https://arxiv.org/pdf/2005.01107v1.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2005.01107v1.pdf</a>
In this paper, we have a dataset like this: <a href=""https://huggingface.co/datasets/iarfmoose/question_generator"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/iarfmoose/question_generator</a></p>
<p>In the t5 transformer, if we don't have those  and  tokens, the model will not learn anything. And I have no idea how specifying  and  we are helping the model to generate questions based on that answer in the context.</p>
<p>I hope I am clear with my question.</p>
","transformer"
"36192","How does GPT use the same embedding matrix for both input and output?","2022-07-04 20:35:57","40133","1","3666","<natural-language-processing><transformer><word-embedding><embeddings><gpt>","<p>My understanding is that GPT uses the <strong>same embedding matrix</strong> for both inputs and output: Let <span class=""math-container"">$V$</span> be the vocab size, <span class=""math-container"">$D$</span> the number of embedding dimensions, and <span class=""math-container"">$E$</span> be a <span class=""math-container"">$V \times D$</span> embedding matrix:</p>
<ul>
<li>On input, if <span class=""math-container"">$x$</span> is a one-hot <span class=""math-container"">$V$</span>-dimensional vector, GPT uses <span class=""math-container"">$Ei$</span>.</li>
<li>On output, if <span class=""math-container"">$\hat y$</span> is a <span class=""math-container"">$D$</span>-dimensional prediction vector, GPT uses softmax(<span class=""math-container"">$E^\top{\hat y}$</span>) as its predictions.</li>
</ul>
<h2>Q1. Is the above correct?</h2>
<p>I cannot find this stated clearly in the paper, but it is stated explicitly <a href=""https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens"" rel=""nofollow noreferrer"">here</a>.  It's also clearly implied by the parameter count listed <a href=""https://jalammar.github.io/illustrated-gpt2/"" rel=""nofollow noreferrer"">here</a>, and argued for as best practice <a href=""https://paperswithcode.com/method/weight-tying"" rel=""nofollow noreferrer"">here</a>.  Yet, for example, Karpathy's mini-GPT implementation seems to use two different matrices:</p>
<pre><code>self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd) # &lt;--- This would be E
self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))
self.drop = nn.Dropout(config.embd_pdrop)
# transformer
self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])
# decoder head
self.ln_f = nn.LayerNorm(config.n_embd)
self.head = nn.Linear(config.n_embd, config.vocab_size, bias=False) # &lt;--- This has the same dimensions as Etranspose but is clearly a different matrix

</code></pre>
<h2>Q2. If it is correct, how does can it work?</h2>
<p>This seems to be tasking <span class=""math-container"">$E$</span> with two very different, even opposing, functions:</p>
<ul>
<li>Map vocab to their <em>meaning</em> on the input side; higher magnitude indicates &quot;more meaning&quot;</li>
<li>Map meaning to the <em>most likely</em> vocab on the output side; higher magnitude indicates greater likelihood</li>
</ul>
<p>When outputting, we want the softmax to be highest when the word is most likely; magnitude of the output matrix should be roughly proportional to how likely the word is two appear.</p>
<p>Yet, when inputting, magnitude has <em>nothing to do with likelihood</em>.  Magnitude on the input side captures some element of meaning: perhaps how extreme or intense the meaning is, perhaps another aspect (not necessarily easily interpreted).</p>
","transformer"
"35990","Why are embeddings added, not concatenated?","2022-06-20 05:57:34","","15","3469","<neural-networks><deep-learning><transformer><attention><embeddings>","<p>Let's consider the following example from BERT</p>
<p><a href=""https://i.sstatic.net/YLGSz.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/YLGSz.png"" alt=""enter image description here"" /></a></p>
<p>I cannot understand why &quot;the input embeddings are the <em>sum</em> of the token embeddings, the segmentation embeddings, and the position embeddings&quot;. The thing is, these embeddings carry different types of information, so intuitively adding them together doesn't really make sense. I mean, you cannot add 2 meters to 3 kilograms, but you can make a tuple (2 meters, 3 kilograms), so I think it's more natural to concatenate these embedding together. By adding them together, we are assuming the information about token, segmentation, and position can be simultaneously represented in the same embedding space, but that sounds like a bold claim.</p>
<p>Other transformers, like ViTMAE, seem to follow the trend of <a href=""https://github.com/huggingface/transformers/blob/v4.20.0/src/transformers/models/vit_mae/modeling_vit_mae.py#L795"" rel=""noreferrer"">adding position embeddings to other &quot;semantic&quot; embeddings</a>. What's the rationale behind the practice?</p>
","transformer"
"35945","Why do transformer Key Query Value layers not have biases or activations?","2022-06-15 22:58:09","","1","813","<transformer><dense-layers>","<p>Transformers use just matrices to transform input embeddings, which is halfway to being a connected dense layer (add a bias and activation). So, why don't transformers have dense layers for encoding input into Query Key Value?</p>
","transformer"
"35833","What considerations should I take to train my transformer model?","2022-06-08 05:32:18","","1","68","<training><transformer><image-segmentation><testing>","<p>I want to train my vision transformer model on a benchmark for an image segmentation task: (<a href=""https://arxiv.org/abs/2110.08733"" rel=""nofollow noreferrer"">LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation</a>) (<a href=""https://github.com/Junjue-Wang/LoveDA"" rel=""nofollow noreferrer"">GitHub</a>), but I don't get any acceptable result from my model.</p>
<p>I don't have a good result either in evaluation nor (obviously) in test.</p>
<p>I'm frustrated since I have trained my model with different parameters: learning rate : [0.001,0001, 6e-e].</p>
<p>The training datasets include about 3000 images, the number of epochs is 1500.</p>
<p>I have used different loss functions: Cross Entropy, Dice loss, Focal loss, and a combination of those losses.</p>
<p>So, I'm wondering if anyone has any experience that could help me find a solution and improve my model's performance?</p>
","transformer"
"35548","When exactly does the split into different heads in Multi-Head-Attention occur?","2022-05-17 12:42:28","35560","6","4854","<transformer><pytorch><attention>","<p>I am confused by the Multi-Head part of the Multi-Head-Attention used in Transformers. My question concerns the implementations in Pytorch of <a href=""https://pytorch.org/docs/stable/_modules/torch/nn/modules/activation.html#MultiheadAttention"" rel=""noreferrer"">nn.MultiheadAttention</a> and its forward method multi_head_attention_forward and whether these are actually identical to the paper. Unfortunately, I have been unable to follow along the original code of the paper. So I could not check whether the implementations in Pytorch are acutally identical to the paper.</p>
<p>Please forgive the excessive use of illustrations. However, I hope it will improve understanding my problem.</p>
<p>What is the correct order for calculating the Queries Q, Keys K and Values V and splitting the operation into the individual Attention-Heads? Unfortunately most explanations I found online while helpful for understanding the general principle and intuition of Multi-Head-Attention did not go into the details of the implementation.</p>
<p>In the original paper <a href=""https://arxiv.org/abs/1706.03762"" rel=""noreferrer"">Attention is all you need</a> Multi-Head-Attention is explained as followed:</p>
<p><a href=""https://i.sstatic.net/LmHJi.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/LmHJi.png"" alt=""enter image description here"" /></a></p>
<p>First, according to my current understanding, if we have a sequence of vectors with 512-dimensions (like in the original Transformer) and we have <span class=""math-container"">$h=8$</span> Attention-Heads (again like the original), every Attention-Head attends to <span class=""math-container"">$512/8=64$</span> entries of the input vector used to calculate the Attention in the corresponding head. So the first Attention-Head attends to the first 64 entries, the second to the second 64 entries and so on. However, if the split is conducted before calculating Q,K,V this would refer to the first 64 entries of X (this does not seem match the explanation in the paper I believe) while in the other case it would refer to the first 64 entries of Q,K,V.</p>
<p>In the text they say &quot;project the queries, keys and values h times with different, learned linear projections to <span class=""math-container"">$d_k,d_k$</span> and <span class=""math-container"">$d_v$</span> dimensions and since they set <span class=""math-container"">$d_k=d_v=d_{model}/h=512/8=64$</span>. Therefore, if we actually have single matrices for every Attention-Head h we would have</p>
<p><span class=""math-container"">$W^Q_i,W^K_i,W^V_i \in \mathbb{R}^{512x64} \forall i \in h$</span>.</p>
<p>This matches the illustration found here <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">https://jalammar.github.io/illustrated-transformer/</a></p>
<p><a href=""https://i.sstatic.net/ehiUv.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/ehiUv.png"" alt=""enter image description here"" /></a></p>
<p>It is explained that the input X is transformed into the Queries, Keys and Values for the different attention heads by using different projection matrices which are learned during training.</p>
<p>This seems to indicate that the split into the individual Attention-Heads is conducted after the calculation of <span class=""math-container"">$Q,K,V$</span> (or rather during the calculation). Since we have <span class=""math-container"">$h=8$</span> this leads in sum to <span class=""math-container"">$3*8*512*64=3*512*512$</span> learnable parameters in total (if we ignore the bias). Thus as far as the overall number of parameters is concerned we would have the same number if we would instead use three big matrices which concatenate the matrices of the individual Attention-Heads.</p>
<p><span class=""math-container"">$W^Q=[W^Q_1,W^Q_2,...,W^Q_h] \in \mathbb{R}^{512x512}$</span> <br />
<span class=""math-container"">$W^K=[W^K_1,W^K_2,...,W^K_h] \in \mathbb{R}^{512x512}$</span> <br />
<span class=""math-container"">$W^Q=[W^V_1,W^V_2,...,W^V_h] \in \mathbb{R}^{512x512}$</span></p>
<p>In the explanation from the same author of GPT-2 (this model has an embedding dimension of 768 and 12 Attention-Heads, instead of 512 and 8 like the original Transformer) found here <a href=""https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention"" rel=""noreferrer"">https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention</a></p>
<p><a href=""https://i.sstatic.net/I8fz9.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/I8fz9.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/V75eY.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/V75eY.png"" alt=""enter image description here"" /></a></p>
<p>the Queries Q, Keys K and Values V are first calculated by multiplying the input with one big matrix which is the concatenation of <span class=""math-container"">$W^Q,W^K,W^V$</span>. If the input for calculating Q, K and V is identical (which is the case for self-attention), it is clear to me that you can use <span class=""math-container"">$W_{concat}=[W^Q,W^K,W^V]$</span> and obtain <span class=""math-container"">$[Q,K,V]$</span>, since you essentially still multiply the input with each weight matrix separately.</p>
<p>Then you can split the result to again obtain <span class=""math-container"">$Q,K,V$</span> as individual matrices (The image displays <span class=""math-container"">$q_9,k_9,v_9$</span> as an example, not the complete matrices). Then <span class=""math-container"">$q_9,k_9,v_9$</span> are again split into 12 vectors which results in a matrix of dimension <span class=""math-container"">$(12x64)$</span>.</p>
<p>So overall here we did not use individual matrices per Attention-Head but only one larger matrix.</p>
<p>Is this method mathematically identical to the one using individual smaller matrices per Attention-Head?</p>
<p>It appears that this is the way the calculation is implented in Pytorch, if <span class=""math-container"">$d_{model}=kdim=vdim$</span> Though here unlike in the paper which used <span class=""math-container"">$d_k$</span> and <span class=""math-container"">$d_v$</span> as names, <span class=""math-container"">$kdim$</span> and <span class=""math-container"">$vdim$</span> refer to the dimension of all Attention-Heads summed up, e.g. <span class=""math-container"">$kdim=d_k*num_{heads}$</span>(=512 for the original Transformer).</p>
<p>So  In the documentation of nn.modules.MultiheadAttention the model either creates three separate projection matrices to generate the Queries, Keys and Values or one big matrix (if the dimensions are identical). The following is part of the <code>_init_</code> function.</p>
<pre><code>if self._qkv_same_embed_dim is False:
            self.q_proj_weight = Parameter(torch.empty((embed_dim, embed_dim), **factory_kwargs))
            self.k_proj_weight = Parameter(torch.empty((embed_dim, self.kdim), **factory_kwargs))
            self.v_proj_weight = Parameter(torch.empty((embed_dim, self.vdim), **factory_kwargs))
            self.register_parameter('in_proj_weight', None)
        else:
            self.in_proj_weight = Parameter(torch.empty((3 * embed_dim, embed_dim), **factory_kwargs))
            self.register_parameter('q_proj_weight', None)
            self.register_parameter('k_proj_weight', None)
            self.register_parameter('v_proj_weight', None)
</code></pre>
<p>If we stay in the standard case of <code>_qkv_same_embed_dim=True</code> the input is passed through a <code>nn.linear</code> as part of <code>_in_projection_packed</code> which is using <code>self.in_proj_weight</code> as the weight</p>
<pre><code>if not use_separate_proj_weight:
        assert in_proj_weight is not None, &quot;use_separate_proj_weight is False but in_proj_weight is None&quot;
        q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
    else:
        assert q_proj_weight is not None, &quot;use_separate_proj_weight is True but q_proj_weight is None&quot;
        assert k_proj_weight is not None, &quot;use_separate_proj_weight is True but k_proj_weight is None&quot;
        assert v_proj_weight is not None, &quot;use_separate_proj_weight is True but v_proj_weight is None&quot;
        if in_proj_bias is None:
            b_q = b_k = b_v = None
        else:
            b_q, b_k, b_v = in_proj_bias.chunk(3)
        q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)

def _in_projection_packed(
    q: Tensor,
    k: Tensor,
    v: Tensor,
    w: Tensor,
    b: Optional[Tensor] = None,
) -&gt; List[Tensor]:
    r&quot;&quot;&quot;
    Performs the in-projection step of the attention operation, using packed weights.
    Output is a triple containing projection tensors for query, key and value.
    Args:
        q, k, v: query, key and value tensors to be projected. For self-attention,
            these are typically the same tensor; for encoder-decoder attention,
            k and v are typically the same tensor. (We take advantage of these
            identities for performance if they are present.) Regardless, q, k and v
            must share a common embedding dimension; otherwise their shapes may vary.
        w: projection weights for q, k and v, packed into a single tensor. Weights
            are packed along dimension 0, in q, k, v order.
        b: optional projection biases for q, k and v, packed into a single tensor
            in q, k, v order.
    Shape:
        Inputs:
        - q: :math:`(..., E)` where E is the embedding dimension
        - k: :math:`(..., E)` where E is the embedding dimension
        - v: :math:`(..., E)` where E is the embedding dimension
        - w: :math:`(E * 3, E)` where E is the embedding dimension
        - b: :math:`E * 3` where E is the embedding dimension
        Output:
        - in output list :math:`[q', k', v']`, each output tensor will have the
            same shape as the corresponding input tensor.
    &quot;&quot;&quot;
    E = q.size(-1)
    if k is v:
        if q is k:
            # self-attention
            return linear(q, w, b).chunk(3, dim=-1)
        else:
            # encoder-decoder attention
            w_q, w_kv = w.split([E, E * 2])
            if b is None:
                b_q = b_kv = None
            else:
                b_q, b_kv = b.split([E, E * 2])
            return (linear(q, w_q, b_q),) + linear(k, w_kv, b_kv).chunk(2, dim=-1)
    else:
        w_q, w_k, w_v = w.chunk(3)
        if b is None:
            b_q = b_k = b_v = None
        else:
            b_q, b_k, b_v = b.chunk(3)
        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)


def _in_projection(
    q: Tensor,
    k: Tensor,
    v: Tensor,
    w_q: Tensor,
    w_k: Tensor,
    w_v: Tensor,
    b_q: Optional[Tensor] = None,
    b_k: Optional[Tensor] = None,
    b_v: Optional[Tensor] = None,
) -&gt; Tuple[Tensor, Tensor, Tensor]:
    r&quot;&quot;&quot;
    Performs the in-projection step of the attention operation. This is simply
    a triple of linear projections, with shape constraints on the weights which
    ensure embedding dimension uniformity in the projected outputs.
    Output is a triple containing projection tensors for query, key and value.
    Args:
        q, k, v: query, key and value tensors to be projected.
        w_q, w_k, w_v: weights for q, k and v, respectively.
        b_q, b_k, b_v: optional biases for q, k and v, respectively.
    Shape:
        Inputs:
        - q: :math:`(Qdims..., Eq)` where Eq is the query embedding dimension and Qdims are any
            number of leading dimensions.
        - k: :math:`(Kdims..., Ek)` where Ek is the key embedding dimension and Kdims are any
            number of leading dimensions.
        - v: :math:`(Vdims..., Ev)` where Ev is the value embedding dimension and Vdims are any
            number of leading dimensions.
        - w_q: :math:`(Eq, Eq)`
        - w_k: :math:`(Eq, Ek)`
        - w_v: :math:`(Eq, Ev)`
        - b_q: :math:`(Eq)`
        - b_k: :math:`(Eq)`
        - b_v: :math:`(Eq)`
        Output: in output triple :math:`(q', k', v')`,
         - q': :math:`[Qdims..., Eq]`
         - k': :math:`[Kdims..., Eq]`
         - v': :math:`[Vdims..., Eq]`
    &quot;&quot;&quot;
    Eq, Ek, Ev = q.size(-1), k.size(-1), v.size(-1)
    assert w_q.shape == (Eq, Eq), f&quot;expecting query weights shape of {(Eq, Eq)}, but got {w_q.shape}&quot;
    assert w_k.shape == (Eq, Ek), f&quot;expecting key weights shape of {(Eq, Ek)}, but got {w_k.shape}&quot;
    assert w_v.shape == (Eq, Ev), f&quot;expecting value weights shape of {(Eq, Ev)}, but got {w_v.shape}&quot;
    assert b_q is None or b_q.shape == (Eq,), f&quot;expecting query bias shape of {(Eq,)}, but got {b_q.shape}&quot;
    assert b_k is None or b_k.shape == (Eq,), f&quot;expecting key bias shape of {(Eq,)}, but got {b_k.shape}&quot;
    assert b_v is None or b_v.shape == (Eq,), f&quot;expecting value bias shape of {(Eq,)}, but got {b_v.shape}&quot;
    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)

def linear(
    input: Tensor, weight: Tensor, bias: Optional[Tensor] = None,
    scale: Optional[float] = None, zero_point: Optional[int] = None
) -&gt; Tensor:
    r&quot;&quot;&quot;
    Applies a linear transformation to the incoming quantized data:
    :math:`y = xA^T + b`.
    See :class:`~torch.nn.quantized.Linear`
    .. note::
      Current implementation packs weights on every call, which has penalty on performance.
      If you want to avoid the overhead, use :class:`~torch.nn.quantized.Linear`.
    Args:
      input (Tensor): Quantized input of type `torch.quint8`
      weight (Tensor): Quantized weight of type `torch.qint8`
      bias (Tensor): None or fp32 bias of type `torch.float`
      scale (double): output scale. If None, derived from the input scale
      zero_point (long): output zero point. If None, derived from the input zero_point
    Shape:
        - Input: :math:`(N, *, in\_features)` where `*` means any number of
          additional dimensions
        - Weight: :math:`(out\_features, in\_features)`
        - Bias: :math:`(out\_features)`
        - Output: :math:`(N, *, out\_features)`
    &quot;&quot;&quot;
    if scale is None:
        scale = input.q_scale()
    if zero_point is None:
        zero_point = input.q_zero_point()
    _packed_params = torch.ops.quantized.linear_prepack(weight, bias)
    return torch.ops.quantized.linear(input, _packed_params, scale, zero_point)
</code></pre>
<p>Then later the Queries, Keys and Values are split up into the individual Attention-Heads:</p>
<pre><code>    #
    # reshape q, k, v for multihead attention and make em batch first
    #
    q = q.contiguous().view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)
    if static_k is None:
        k = k.contiguous().view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
    else:
        # TODO finish disentangling control flow so we don't do in-projections when statics are passed
        assert static_k.size(0) == bsz * num_heads, \
            f&quot;expecting static_k.size(0) of {bsz * num_heads}, but got {static_k.size(0)}&quot;
        assert static_k.size(2) == head_dim, \
            f&quot;expecting static_k.size(2) of {head_dim}, but got {static_k.size(2)}&quot;
        k = static_k
    if static_v is None:
        v = v.contiguous().view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)
    else:
        # TODO finish disentangling control flow so we don't do in-projections when statics are passed
        assert static_v.size(0) == bsz * num_heads, \
            f&quot;expecting static_v.size(0) of {bsz * num_heads}, but got {static_v.size(0)}&quot;
        assert static_v.size(2) == head_dim, \
            f&quot;expecting static_v.size(2) of {head_dim}, but got {static_v.size(2)}&quot;
        v = static_v
</code></pre>
<p>Therefore, it appears to me that both ways of calculations should be equal and the one using the bigger matrices is just more efficient to compute. Is this correct?</p>
<p>In this case, I ask myself why nn.MultiheadAttention requires that <code>embed_dim</code> is divisible by <code>num_heads</code>, since the split into the individual Attention-Heads is actually conducted after generating <span class=""math-container"">$Q,K,V$</span>. Should then not <span class=""math-container"">$d_q,d_k,d_v$</span> be made to be divisible by <code>num_heads</code>? Since these dimensions do not have to be equal to the dimension of the inputs?</p>
<p>Thanks for any advice in advance!</p>
","transformer"
"35547","Positional Encoding of Time-Series features","2022-05-17 12:22:24","","3","3434","<transformer><time-series><positional-encoding>","<p>I’m trying to use a Transformer Encoder I coded with weather feature vectors which are basically 11 features about the weather in the dimension <code>[batch_size, n_features]</code>.</p>
<p>I have a data point per day, so this is a time-series but there are no evidences of the time in my vectors and I know I must use positional encoding to tell my network “this vector is from the 21/05/2021, this one is from the 20/04/2019, etc”.</p>
<p>The usual positional encoding with sin &amp; cos used in NLP doesn’t seem to fit my problem as it encodes the position relative to other words in the sentence and my features are independant values (the temperature of the day doesn’t come after the amount of rain for instance). Order of the data matters between two different inputs but not within the features.</p>
<p>I don’t really know how to do encode my feature vectors for my transformer encoder to get something like <code>positional_encoded = original_vector + date_encoding</code> which I think could be great but I could also just add the day of year as a feature - would it be enough for a transformer?</p>
<p>What would be the best way to do so?</p>
","transformer"
"35405","What does ""position"" in ""each position in the decoder"" denote in the Transformer's original paper?","2022-05-03 13:38:04","","1","130","<terminology><papers><transformer><attention>","<p>I am reading <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention is All You Need</a> and I feel confused about the word &quot;position&quot; in this paper, by the way I'm not native English speaker which may cause my confusion which has confused me for a few days.</p>
<p>In this paragraph from 3.2.3 of the paper,</p>
<blockquote>
<p>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.</p>
</blockquote>
<p>&quot;each position in the decoder&quot;, where is it actually, in which part of decoder? What does &quot;each position in the decoder&quot; really want to mean? Each layer in the decoder? Or something else like position of vectors? And what does &quot;all positions&quot; mean? Same with the former &quot;position&quot;? Could you please give an example to explain?</p>
<p>I've already known how masked attention works. It uses masking to prevent the encoder from attending to positions in outputs it should't attend to during training because entire translation'd be given when training.</p>
","transformer"
"35290","When training a seq2seq model is it better to train using the models outputs or expected outputs?","2022-04-21 02:04:05","","1","98","<natural-language-processing><recurrent-neural-networks><transformer><attention><seq2seq>","<p>When training any seq2seq model you have a <code>target</code> and a <code>source</code>. The <code>source</code> may be a sentence such as:</p>
<blockquote>
<p>I_walked_the_dog</p>
</blockquote>
<p>And the target being</p>
<blockquote>
<p>_walked_the_dogg</p>
</blockquote>
<p>Where as you can see the expected output for the initial <code>I</code> is a space <code>_</code>. My question is, at training time, whether to use the models previous outputs for predicting the next output, or to run the training simultaneously using the expected outputs. To illustrate this more clearly, see below:</p>
<p><a href=""https://i.sstatic.net/kEOcz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kEOcz.png"" alt=""enter image description here"" /></a></p>
<p>The incentive for training using the expected outputs is that all time steps can be trained simultaneously, so it speeds up training by a factor of the sequence lengths. However, it means that training is not representative of what the network will realistically be doing at testing time, as at testing time the network will not have perfect previous outputs, but rather only it's own.</p>
","transformer"
"35161","Attention: Isn't it redundant to apply a linear layer to both the keys and values?","2022-04-11 06:32:37","35173","2","211","<transformer><attention>","<p>Transformer attention is calculated <span class=""math-container"">$Attention(X) =X W^V\times \text{columnwise-softmax} (Att(X))$</span> where the attention attention matrix is <span class=""math-container"">$$Att(X) = Q \times K = {X W}^Q \times ({X W}^K)^T = {X W}^Q (W^K)^T X^T $$</span></p>
<p>But then couldn't one of <span class=""math-container"">$W^Q$</span> and <span class=""math-container"">$W^K$</span> be absorbed into the other? So one of them is redundant say <span class=""math-container"">$W^K$</span> and so attention only needs the parameters from <span class=""math-container"">$W^Q$</span> and <span class=""math-container"">$W^V$</span>.</p>
<p><span class=""math-container"">$$Attention(X) = {XW}^V \times \text{columnwise-softmax}({XW}^Q  X^T).$$</span></p>
<p><a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">Diagrams</a> I use for thinking.</p>
<p><img src=""https://jalammar.github.io/images/t/self-attention-matrix-calculation.png"" alt="""" /></p>
<p><img src=""https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png"" alt="""" /></p>
","transformer"
"35016","How to use structural information in a Transformer?","2022-03-29 21:54:08","","1","207","<deep-learning><transformer><graph-neural-networks><gnn>","<p>I am performing a Neural Machine Translation (NMT) task. In my case, input data has relational information.</p>
<p>I know I can use a Graph Neural Network (GNN) and use a Graph2Seq model. But I can't find a good generational model for GNN.</p>
<p>So I want to use Transformer. But then the challenge is how can I embed structural information there? Is there any open source artefact for Relational Transformer that I can use out of the box?</p>
<p>Any pointers?</p>
","transformer"
"34998","How to include additional numeric input in the Transformer architecture","2022-03-28 14:28:16","","2","443","<transformer>","<p>I want to apply the <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Transformer</a> architecture to my machine translation task, and provide the decoder with an additional parameter in the range of <code>[0,1]</code>. This parameter should provide the decoder with some additional information on <em>how</em> to translate the sequence, think, e.g., how pompous the resulting translation should be.</p>
<p>Although I have <em>some</em> idea on how to approach this task, I would like to ask for help and additional input. I have found a paper by <a href=""https://arxiv.org/abs/1811.04716"" rel=""nofollow noreferrer"">Libovický et al.</a>, which studied multi-source sequence-to-sequence tasks. Here are some of the ideas that came to my mind:</p>
<ul>
<li>Use an additional encoder, to encode this (single) numerical value, and apply one of the approaches mentioned by Libovický et al. This <em>feels</em> strange in the sense that we are encoding only a single element, and attending to it.</li>
<li>In a similar fashion, use an additional encoder, but this time instead of providing a single numerical value, create a sequence of length <code>n</code>, where all elements are the same input parameter. E.g., if the parameter was <code>0.5</code> and <code>n = 3</code>, the resulting sequence would be <code>[0.5 0.5 0.5]</code>. Then, use masked multi-head attention to attend to these output values. Note that this approach is based on my intuition only. I lack the needed experience to judge whether this approach could actually provide benefits over the previous one.</li>
<li>Add an additional input to the decoder, applying masked multi-head attention to it, but not necessarily positional encoding. Then, sum up the result of this operation and the &quot;standard&quot; input, resume with this interim result. I believe that for this approach I would also have to create a &quot;fake&quot; sequence, where each element is the same numerical parameter.</li>
</ul>
<p>To sum it up, I am looking for a way to provide the decoder with an additional input, in my case a single numerical value.</p>
","transformer"
"34760","Dimensions of a Transformer model and purpose of masking","2022-03-08 13:30:58","34763","1","1007","<neural-networks><machine-learning><natural-language-processing><transformer>","<p>I'm currently studying the Transformer model (<strong>Attention is all you need</strong>) and after reading it I still have some questions for which I get conflicting answers if I google them:</p>
<ul>
<li>What exactly are the dimensions of the input to the encoder of a transformer, from what I've seen you can input sentences with dynamic lengths but in the paper it seems like all layers expect the K/Q/V matrices to have dimensions of <code>d_model, d_k</code> or <code>d_v/d_q</code></li>
<li>Same question for the decoder, what exactly are the input dimensions and how do the attention layers handle dynamic dimensions if its possible</li>
<li>Another question is is masking only used for training where you input the whole sentence as the input to the decoder or does it have another purpose than that (especially in inference)</li>
</ul>
","transformer"
"34758","How ""Patch Merging"" works in SWIN-Transformers?","2022-03-08 11:25:43","","0","2781","<neural-networks><machine-learning><deep-learning><transformer><vision-transformer>","<p>In the <a href=""https://arxiv.org/abs/2103.14030"" rel=""nofollow noreferrer"">SOTA paper: SWIN-Transformers</a>, the authors have tried their best to explain everything clearly. I have got an idea of how it works except the <code>Patch Merging</code> part. I found some blogs and other things explaining this but still I am not able to comprehend how the shape changes and how come the <strong>SHAPE</strong> of windows are changed at the time when the only thing thy are doing is Concatenating the neighbouring <code>2x2</code> patches. Could someone please explain it in Laymen terms or maybe a video link or something intuitive explanation.</p>
<ul>
<li><a href=""https://towardsdatascience.com/swin-vision-transformers-hacking-the-human-eye-4223ba9764c3"" rel=""nofollow noreferrer"">This awesome blog explains every detail clearly</a></li>
<li><a href=""https://blog.csdn.net/cangafuture/article/details/116396156"" rel=""nofollow noreferrer"">Implementation in <code>Pytorch</code> from scratch with explanation</a></li>
<li><a href=""https://keras.io/examples/vision/swin_transformers/"" rel=""nofollow noreferrer""><code>Keras / Tensorflow</code> Implementation</a></li>
</ul>
<p>Below are some intuitive images which I am trying to grasp in bits and pieces<a href=""https://i.sstatic.net/n39a1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/n39a1.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/3IQMx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3IQMx.jpg"" alt=""enter image description here"" /></a></p>
","transformer"
"34739","How do we reduce the output dimensions of BERT?","2022-03-05 17:38:08","","0","2183","<machine-learning><natural-language-processing><transformer><bert>","<p>The output dimensions of BERT are 768-dimensional, is it possible to reduce them to a lower, custom number? For example, if there is another BERT-based transformer model which has a lower count of ouput dimensions, if it's possible to fine tune BERT on MLM to output lower dimension encodings etc.</p>
<p>And if not, are there any possible workarounds for this issue?</p>
","transformer"
"34710","Are positional embeddings computed during or before training?","2022-03-03 11:14:43","","0","180","<natural-language-processing><transformer><positional-encoding>","<p>I'm trying to practically frame the concept of <em>positional embeddings</em> as introduced in the original <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">paper</a>.</p>
<p>As far as I've understood, what we do is basically creating some other vectors in addition to the original <em>embeddings</em> of our inputs. So if I have my input <span class=""math-container"">$X \in \mathbb{R}^{n \times d}$</span> with shape <code>(64,103)</code> (so <span class=""math-container"">$n$</span> here is the batch size), I will be creating a matrix <span class=""math-container"">$P \in \mathbb{R}^{n \times d}$</span> where each <span class=""math-container"">$d$</span> dimensional vector will contain information about positions. Now, these vectors are generated from sinusoid functions with an initial frequency of <code>1e-4</code>, and then initial embeddings <span class=""math-container"">$X \in \mathbb{R}^{n \times d}$</span> and positional embeddings <span class=""math-container"">$P \in \mathbb{R}^{n \times d}$</span> are summed together to get a new input <span class=""math-container"">$X' \in \mathbb{R}^{n \times d}$</span>.</p>
<p>Now, all this process is happening <em>before</em> the start of training right? Positional embeddings are not <em>learned/modified</em> during backpropagation?</p>
","transformer"
"34657","What is multi-head attention doing mathematically, and how is it different from self-attention?","2022-02-26 10:48:30","34659","3","1603","<deep-learning><comparison><transformer><attention>","<p>I'm trying to understand the difference between the concept of <em>self-attention</em> and <em>multi-head attention</em>. The latter is not actually too clear to me.</p>
<p>I understand that, in the case of self-attention, we start with a feature matrix <span class=""math-container"">$X \in \mathbb{R}^{n \times d}$</span>, and then we use the same linear transformation <span class=""math-container"">$W$</span> to produce</p>
<p><span class=""math-container"">\begin{align}
Q &amp;= XW \\ 
K &amp;= XW \\
V &amp;= XW
\end{align}</span></p>
<p>and then we compute the following</p>
<p><span class=""math-container"">$$X' = \text{softmax} \left(\frac{Q\cdot K^T}{\sqrt{d}} \right)V$$</span></p>
<p>where <span class=""math-container"">$X' \in \mathbb{R}^{n \times d}$</span> is a new version of the input matrix, where the pairwise interactions between the points will be encoded.</p>
<p>What is multi-head attention doing, from a mathematical point of view, and what's the difference? I know we are going to use three <em>different</em> linear transformations in this case (so no weight-sharing), but what exactly will be encoded using three different <span class=""math-container"">$W$</span>? Maybe it's more the conceptual view that it's not too clear in this case.</p>
","transformer"
"34134","Is a Transformer a good choice for multivariate signal classification?","2022-01-12 14:28:38","","1","1004","<classification><transformer><time-series>","<p>I am working on a problem regarding the multi-classification of multivariate time signals. So I have multiple signals and try to train an algorithm on them. My current approach is to build a neural network with LSTM-layers and it works pretty fine.</p>
<p>I have read that LSTMs are pretty outdated because of the transformer architectures. I found some papers about the idea to use them for signal classification (see: <a href=""https://arxiv.org/abs/2103.14438"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2103.14438</a>). There was an example on the TensorFlow page regarding univariate signal classification (see <a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">https://keras.io/examples/timeseries/timeseries_transformer_classification/</a>). I think it is rather a research question than a common approach for this type of problem.</p>
<p>To my questions:</p>
<ul>
<li><p>Would you recommend implementing a Transformer for this type of problem? Do you think, it is a more &quot;state of the art&quot; approach?</p>
</li>
<li><p>Do you know some example projects?</p>
</li>
</ul>
","transformer"
"34124","Are there any works that deal with 2D pose estimation in videos?","2022-01-11 18:45:04","","0","126","<convolutional-neural-networks><computer-vision><reference-request><transformer><pose-estimation>","<p>Since pose estimation is often a task where spatial-temporal context should be helpful in finding subsequent key points, I thought there should be many papers on it. However, I could not find any work that deals with 2D pose estimation in videos.</p>
<p>Am I missing something, or am I just hindered by a large number of papers on 3D position estimation?</p>
","transformer"
"32667","Why is BERT/GPT capable of ""for-all"" generalization?","2021-12-07 11:30:31","","0","162","<transformer><bert><logic><gpt><reasoning>","<p>As shown in the figure:</p>
<p><a href=""https://i.sstatic.net/P5U4w.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/P5U4w.jpg"" alt=""BERT for-all generalization"" /></a></p>
<p>Why does token prediction work when &quot;Socrates&quot; is replaced with &quot;Plato&quot;?</p>
<p>From the point of view of symbolic logic, the above example effectively performs the logic rule:</p>
<pre><code>∀x. human(x) ⇒ mortal(x)
</code></pre>
<p>How might we explain this ability?  Moreover, how is this learned in just a few shots of examples?</p>
<p>I think this question is key to understanding the Transformer's logical reasoning ability.</p>
<p>Below are excerpts from 2 papers:</p>
<p><a href=""https://i.sstatic.net/ifuUG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ifuUG.png"" alt=""excerpt 1"" /></a></p>
<p><a href=""https://i.sstatic.net/AodI6.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AodI6.jpg"" alt=""excerpt 2"" /></a></p>
","transformer"
"32526","Why do Transformers have a sequence limit at inference time?","2021-11-26 15:32:55","34263","6","2879","<machine-learning><natural-language-processing><transformer><architecture><sequence-modeling>","<p>As far as I understand, Transformer's time complexity increases quadratically with respect to the sequence length. As a result, during training to make training feasible, a maximum sequence limit is set, and to allow batching, all sequences smaller are padded.</p>
<p>However, after a Transformer is trained, and we want to run it on a single sequence at inference time, the computational costs are far less than training. Thus, it seems reasonable that I would want to run the transformer on a larger input sequence length during inference time. From a technical perspective, this should be feasible.</p>
<p>I keep reading online that a Transformer cannot be run on a sequence size larger than the one seen during training. Why is this? Is it because the network weights will be unfamiliar with sequences of this length? Or is it more fundamental?</p>
","transformer"
"32500","What does it mean to apply decomposition at inference-time in a machine translation system?","2021-11-23 05:50:17","32532","0","35","<natural-language-processing><terminology><transformer><machine-translation><inference>","<p>I'm reading <a href=""https://aclanthology.org/2020.wat-1.21.pdf"" rel=""nofollow noreferrer"">this paper</a> for sub-character decomposition for logographic languages and the authors mention decomposition at inference-time. They're using Transformer architecture.</p>
<p>More specifically, the authors write:</p>
<blockquote>
<p>We propose a flexible inference-time sub-character decomposition procedure which targets unseen characters, and show that it aids adequacy and reduces misleading overtranslation in unseen character translation.</p>
</blockquote>
<p>What do inference-time and inference-only decomposition mean in this context? My best guess would be that inference-time would be at some point during the decoding process, but I'm not 100% clear on whether that's the case and, if so, when exactly.</p>
<p>I'm going to keep digging and update if I find something helpful. In the meantime, if anyone needs more context just let me know.</p>
","transformer"
"32396","Is Positional Encoding always needed for using Transformer models correctly?","2021-11-14 19:44:12","32429","2","503","<deep-learning><natural-language-processing><transformer><positional-encoding>","<p>I am trying to make a model that uses a <em>Transformer</em> to see the relationship between several data vectors, but the order of the data is not relevant in this case, so I am not using the <em>Positional Encoding</em>.</p>
<p>Since the performance of models using Transformers is quite improved with the use of this part, do you think that if I remove that part I am breaking the potential of Transformers or is it correct to do so?</p>
","transformer"
"32385","Why do language models produce different outputs for same prompt?","2021-11-12 22:13:07","","2","2636","<deep-learning><transformer><language-model><natural-language-generation><forward-pass>","<p>For conventional 'Neural Networks', the weights simply act as a transformation in highly multi-dimensional space; for a forward pass, the output is always the same since there is no stochastic weighting component in the process.</p>
<p>However, in Transformers (self-attention based encoder-decoder type architecture to be specific) we get different outputs with the same prompts (assuming <span class=""math-container"">$T &gt; 0$</span>). This doesn't make sense to me because the set of weights are always static, so the <em>probability distribution</em> produced should be the same; this simple decoding should yield the same output.</p>
<p>However, in practice, we observe that it is not actually the case.
Any reasons why?</p>
","transformer"
"32341","What is input (and shape) to K/V/Q of self-attention of EACH Decoder block of Language-translation model Transformer's tokens during Inference?","2021-11-09 15:48:28","","3","2371","<transformer><attention><machine-translation><language-model><encoder-decoder>","<p>Transformer model of the original <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention paper</a> has a decoder unit that works differently during Inference than Tranining.</p>
<p>I'm trying to understand the shapes used during decoder (both self-attention and enc-dec-attention blocks), but it's very confusing. I'm referring to <a href=""https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452"" rel=""nofollow noreferrer"">this link</a> and also the original <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention paper</a></p>
<p>In Inference, it uses all previous tokens generated until that time step (say <code>k</code>th time-step), as shown in the diagram below and explained at <a href=""https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452"" rel=""nofollow noreferrer"">this link.</a></p>
<p><a href=""https://i.sstatic.net/0Lt8b.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/0Lt8b.png"" alt="""" /></a></p>
<p>Another diagram that shows self-attention and enc-dec-attention within decoder:</p>
<p><a href=""https://i.sstatic.net/BbCxU.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BbCxU.png"" alt=""enter image description here"" /></a></p>
<p><strong>Question:</strong></p>
<p>However when I look at actual shapes of the QKV projection in the decoder self-attention, and feeding of the decoder self-attention output to the &quot;enc-dec-attention&quot;'s Q matrix, I see only 1 token from the output being used.</p>
<p>Let's <strong>assume 6 deocder blocks one after the other in the decoder stack</strong> (which is the base transformer model).</p>
<p>I'm very confused how the shapes for all matrices in the <strong>Decoder blocks after decoder-1</strong> of the decoder-stack (more specifically decoder-block-2 decoder-3, decoder-4..., decoder-6 of the decoder stack) self-attention and enc-dec-attention can match up with variable length of input to the decoder during inference. I looked at several online material but couldn't find answer.
I see only the BGemms in the decoder's self-attention (not enc-dec-attention) using the variable shapes until all previous <code>k</code> steps, but all other Gemms are fixed size.</p>
<ul>
<li>How is that possible? Is only 1 token (last one from decoder output) is being used for qkv matmuls in self-attention and Q-matmul in enc-dec-attention (which is what I see when running the model)?</li>
<li>Could someone elaborate how all these shapes for QKV in self-attention and Q in enc-dec-attention match up with decoder input length being different at each time-step?**</li>
</ul>
","transformer"
"32287","Where do the characteristics of self-attention come into play in Linformer's proof that self-attention is low rank?","2021-11-05 02:23:20","","1","175","<papers><transformer><proofs><linear-algebra><linformer>","<p>In Linformer's proof that self-attention is low rank in their <a href=""https://arxiv.org/pdf/2006.04768.pdf"" rel=""nofollow noreferrer"">paper</a>, I don't see how it doesn't generalize to every matrix. They don't utilize any specifics of self-attention (the entire proof feels like it's in equation 20 utilizing JL, and I do not see where characteristics of self-attention come into play).</p>
<p>What am I missing?</p>
<p><a href=""https://i.sstatic.net/y7dbG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/y7dbG.png"" alt=""enter image description here"" /></a></p>
","transformer"
"32270","Do Vision Transformers handle arbitrary sequence lengths the same way as normal Transformers?","2021-11-03 21:10:12","32273","5","2051","<neural-networks><computer-vision><transformer><vision-transformer>","<p>Does ViT do handle arbitrary sequence lengths using masking the same way the normal Transformer does?</p>
<p>The <a href=""https://arxiv.org/abs/2010.11929"" rel=""noreferrer"">ViT paper</a> doesn't mention anything about it, so I assume it uses masking like the normal Transformer.</p>
","transformer"
"32236","Is there a notion of location in Transformer architecture in subsequent self-attention layers?","2021-10-31 08:10:13","","3","109","<transformer><attention><positional-encoding>","<p>Transformer architecture (without position embedding) is by the very construction equivariant to the <strong>permutation</strong> of tokens. Given query <span class=""math-container"">$Q \in \mathbb{R}^{n \times d}$</span> and keys <span class=""math-container"">$K \in \mathbb{R}^{n \times d}$</span> and some permutation matrix <span class=""math-container"">$P \in \mathbb{R}^{n \times n}$</span>, one has:
<span class=""math-container"">$$
Q \rightarrow P Q, K \rightarrow P K 
$$</span>
<span class=""math-container"">$$\begin{align}
A &amp;= 
\text{softmax} \left(\frac{Q K^T}{\sqrt{d}} \right) \\ &amp;\rightarrow 
\text{softmax} \left(\frac{R Q K^T R^T}{\sqrt{d}} \right) \\
&amp;= 
R \ \text{softmax} \left(\frac{Q K^T}{\sqrt{d}} \right) R^T \\
&amp;= R A R^T
\end{align}$$</span>
Without the <strong>positional embedding</strong> (learned of fixed), that breaks permutation symmetry to translational there is no notion of location. And with the positional embedding one introduces a notion - token <span class=""math-container"">$x$</span> is on the <span class=""math-container"">$k$</span>-th position.</p>
<p>However, I wonder, whether this notion makes sense after the first self-attention layer.</p>
<p>The operation of producing the output (multiplication of attention by the value)
<span class=""math-container"">$$
x_{out} = A V
$$</span>
outputs some weighted sum of <strong>all</strong> tokens in the sequence, the token at the <span class=""math-container"">$k$</span>-th position now has information from all the sequence.</p>
<p>So, I wonder, whether the notion of <em>position</em> (absolute or relative) still makes sense in this case?</p>
<p><a href=""https://i.sstatic.net/Y4VbC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Y4VbC.png"" alt=""enter image description here"" /></a></p>
<p>My guess is, that since Transformers involve skip connections, these transfer the notion of the location to the next layers. But this depends on the relative magnitude between the activation of the given self-attention layer and the skip-connections</p>
","transformer"
"32184","What is the Intermediate (dense) layer in between attention-output and encoder-output dense layers within transformer block in PyTorch implementation?","2021-10-25 20:05:50","","5","1843","<natural-language-processing><pytorch><transformer><bert>","<p>In PyTorch, transformer (BERT) models have an intermediate dense layer in between attention and output layers whereas the BERT and Transformer papers just mention the attention connected directly to output fully connected layer for the encoder just after adding the residual connection.</p>
<p>Why is there an intermediate layer within an encoder block?</p>
<p>For example,</p>
<blockquote>
<p>encoder.layer.11.attention.self.query.weight<br />
encoder.layer.11.attention.self.query.bias<br />
encoder.layer.11.attention.self.key.weight<br />
encoder.layer.11.attention.self.key.bias<br />
encoder.layer.11.attention.self.value.weight<br />
encoder.layer.11.attention.self.value.bias<br />
encoder.layer.11.attention.output.dense.weight<br />
encoder.layer.11.attention.output.dense.bias<br />
encoder.layer.11.attention.output.LayerNorm.weight<br />
encoder.layer.11.attention.output.LayerNorm.bias<br />
<strong>encoder.layer.11.intermediate.dense.weight<br />
encoder.layer.11.intermediate.dense.bias</strong><br />
encoder.layer.11.output.dense.weight<br />
encoder.layer.11.output.dense.bias<br />
encoder.layer.11.output.LayerNorm.weight<br />
encoder.layer.11.output.LayerNorm.bias</p>
</blockquote>
<p>I am confused by this third (intermediate dense layer) in between attention output and encoder output dense layers</p>
","transformer"
"32172","Is there any point in adding the position embedding to the class token in Transformers?","2021-10-24 19:52:54","","3","519","<neural-networks><transformer><positional-encoding>","<p>The popular implementations of ViTs by <a href=""https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py"" rel=""nofollow noreferrer"">Ross Wightman</a> and <a href=""https://github.com/lucidrains/vit-pytorch/blob/main/vit_pytorch/vit.py"" rel=""nofollow noreferrer"">Phil Wang</a> add the position embedding to the class tokens as well as to the patches.</p>
<p>Is there any point in doing so?</p>
<p>The purpose of introduction positional embeddings to the Transformer is clear - since in the original formulation Transformer is <strong>equivariant</strong> to permutations of tokens, and the original task doesn't respect this symmetry one needs to break it in some way to translational symmetry only - and this goal is achieved via the positional embedding (learned or fixed).</p>
<p>However, the class token is somehow distinguished from the other tokens in the image, and there is no notion for him to be located in the <code>[16:32, 48:64]</code> slice of the image.</p>
<p>Or this choice is simply a matter of convenience? And additional parameter, indeed, has a negligible cost, and there is no benefit as well as harm of the addition of positional embedding to the [CLS] or any special token?</p>
","transformer"
"32158","What are the major layers in a Vision Transformer?","2021-10-22 17:46:34","32159","2","687","<neural-networks><deep-learning><transformer><layers><vision-transformer>","<p>Currently, I am studying deepfake detection using deep learning methods. Convolution neural networks, recurrent neural networks, long-short term memory networks, and vision transformers are famous deep learning-based methods that are used in deepfake detection, as I found in my study.</p>
<p>I was able to find that CNNs, RNNs and LSTMs are multilayered neural networks, but I found very little about the neural network layers in a Vision Transformer. (Like a typical CNN has an input layer, pooling layer, and a fully connected layer, and finally an output layer. RNN has an input layer, multiple hidden layers and an output layer.)</p>
<p>So, what are the <strong>main neural network layers</strong> in a <strong>Vision Transformer</strong>?</p>
","transformer"
"32120","Why does research on faster Transformers focus on the query-key product?","2021-10-19 21:22:36","","1","55","<deep-learning><natural-language-processing><transformer><attention><computational-complexity>","<p>A lot of recent research on Transformers has been devoted to reducing the cost of the self-attention mechanism:</p>
<p><span class=""math-container"">$$\text{softmax}\left(\frac{Q K^T}{\sqrt{d}} \right)V,$$</span></p>
<p>As I understand it, the runtime, assuming <span class=""math-container"">$\{Q, K, V\}$</span> are each of shape <span class=""math-container"">$(n, d)$</span>, is <span class=""math-container"">$O(n^2 d + n d^2)$</span>. In general, the issue is the <span class=""math-container"">$n^2 d$</span> term, because the sequence length <span class=""math-container"">$n$</span> can be much bigger than the model dimension <span class=""math-container"">$d$</span>. So far, so good.</p>
<p>But as far as I can tell, current research focuses on speedups for <span class=""math-container"">$Q K^T$</span>, which is <span class=""math-container"">$O(n^2 d)$</span>. There's less focus on computing <span class=""math-container"">$A V$</span>, where <span class=""math-container"">$A = \text{softmax} \left(\frac{Q K^T}{\sqrt{d}} \right)$</span> -- which also has complexity <span class=""math-container"">$O(n^2 d)$</span>.</p>
<p>Why is the first matrix product the limiting factor?</p>
<p>Examples of these faster Transformer architectures include <a href=""https://arxiv.org/abs/2004.05150"" rel=""nofollow noreferrer"">Longformer</a>, which approximates <span class=""math-container"">$QK^T$</span> as a low-rank-plus-banded matrix, <a href=""https://arxiv.org/abs/2102.03902"" rel=""nofollow noreferrer"">Nystromformer</a>, which approximates <span class=""math-container"">$\text{softmax}(QK^T)$</span> as a low-rank matrix with the Nystrom transformation, and <a href=""https://arxiv.org/abs/2007.14062"" rel=""nofollow noreferrer"">Big Bird</a>, which approximates it with a low-rank-plus-banded-plus-random matrix.</p>
","transformer"
"32110","In layman terms, what does ""attention"" do in a transformer?","2021-10-19 12:05:58","","3","1174","<neural-networks><papers><transformer><attention>","<p>I heard from many people about the paper titled <strong><a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a></strong> by <em>Ashish Vaswani et al</em>.</p>
<p>What actually does the &quot;attention&quot; do in simple terms? Is it a function, property, or some other thing?</p>
","transformer"
"32103","Can I use the transformers for the prediction of historical data?","2021-10-19 05:25:43","","1","191","<neural-networks><deep-learning><datasets><transformer>","<p>Can I use the transformers for the prediction of wind power with the historical data?</p>
<p>Dataset</p>
<p>Datetime, Ambient temperature (Degree), Dewpoint (Degree), Relative Humidity\n (%), Air Pressure, Wind Direction (Degree), Wind Speed at 76.8 m (m/sec), Power Generated(kW).</p>
<p>15 years of data from 2007 to 2021 with a sampling time of 1 hour​</p>
","transformer"
"32048","What is the purpose of hard distillation?","2021-10-13 20:07:19","","3","203","<image-recognition><transformer>","<p>In order to get a smaller model, one often uses larger model, that performs reasonably well on the data as a teacher, and uses the information from large model to train the smaller one.</p>
<p>There are several strategies to do this:</p>
<ul>
<li><p>Soft distillation</p>
<p>Given logits of the teacher one adds the KL-divergence between the student logits and teacher logits to  the loss:
<span class=""math-container"">$$
\mathcal{L}_{loss} = (1 - \alpha) \mathcal{L}_{BCE} (y_{student}, y_{true}) + 
\lambda  \mathcal{L}_{KL} (y_{student}, y_{teacher})
$$</span>
Intuition behind this approach is clear - logits are more informative than a single target label and seemingly allow for faster training.</p>
</li>
<li><p>Hard distillation</p>
<p>One adds the <code>BCE</code> between student logits and teacher model outputs as if they were true labels.
<span class=""math-container"">$$
\mathcal{L}_{loss} = (1 - \alpha) \mathcal{L}_{BCE} (y_{student}, y_{true}) + 
\lambda  \mathcal{L}_{BCE} (y_{student}, y_{teacher})
$$</span></p>
</li>
</ul>
<p>And the benefit of the last approach is unclear to me. For the perfect model, one will have no difference with the vanilla training procedure, and for the case, where the teacher makes mistakes, we will optimize the wrong objective.</p>
<p>Despite these concerns, it was shown experimentally in several papers, and in
the <a href=""https://arxiv.org/abs/2012.12877"" rel=""nofollow noreferrer"">Deit</a>, that this objective can improve performance. Even more, it is better, than soft distillation.</p>
<p><a href=""https://i.sstatic.net/E3cKi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/E3cKi.png"" alt=""enter image description here"" /></a></p>
<p>Why can this be the case?</p>
","transformer"
"32036","Is the multi-head attention in the transformer a weighted adjacency matrix?","2021-10-12 23:15:00","","2","628","<deep-learning><transformer><attention>","<p>Are multi-head attention matrices weighted adjacency matrices?</p>
<p>The job of the multi-head-attention mechanism in transformer models is to determine how likely a word is to appear after another word. In a sense this makes the resulting matrix a big graph with nodes and edges, where a node represents a word and an edge the likelihood to appear after that. So basically it is an adjacency matrix that is created.</p>
","transformer"
"31760","How to generate a response while considering past questions as well?","2021-09-20 03:40:33","","2","109","<natural-language-processing><python><transformer><question-answering>","<pre><code>User: What is the tallest mountain?
Agent: Everest
User: Where is it located? # Agent hears: &quot;Where is Everest located?&quot;
Agent: Nepal
</code></pre>
<p>I want to be able to generate a sequence that has been generated using the user's current query as well as the past conversation.</p>
<p>More specifically, I am using Google's T5 for closed-book question answering, but, instead of trivial questions, we use the user's frequently asked queries.</p>
<p>I want to be able to encode their past questions and the agent's past answers, then use them to generate the agent's next answer. How can I do that?</p>
","transformer"
"31689","Why do the authors of the T5 paper say that the ""architectural changes are orthogonal to the experimental factors""?","2021-09-14 19:58:32","31696","1","329","<machine-learning><natural-language-processing><papers><transformer>","<p>Here's a quote from the <code>T5 paper</code> (T5 stands for &quot;Text-to-Text Transfer Transformer&quot;) titled <a href=""https://www.jmlr.org/papers/volume21/20-074/20-074.pdf"" rel=""nofollow noreferrer"">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> by <em>Colin Raffel et al.</em>:</p>
<blockquote>
<p>To summarize, our model is roughly equivalent to the original
Transformer proposed by Vaswani et al. (2017) with the exception of
removing the Layer Norm bias, placing the layer normalization outside
the residual path, and using a different position embedding scheme.
Since these architectural changes are <strong>orthogonal</strong> to the experimental
factors we consider in our empirical survey of transfer learning, we
leave the ablation of their impact for future work.</p>
</blockquote>
<p>What exactly does 'orthogonal' mean in this context? Also, is it just me or have I seen the word used in a similar way before, but can't remember where?</p>
","transformer"
"30491","Is there a proper initialization technique for the weight matrices in multi-head attention?","2021-09-01 08:41:49","","8","5041","<transformer><attention><weights><weights-initialization>","<p>Self-attention layers have 4 learnable tensors (in the vanilla formulation):</p>
<ul>
<li>Query matrix <span class=""math-container"">$W_Q$</span></li>
<li>Key matrix <span class=""math-container"">$W_K$</span></li>
<li>Value matrix <span class=""math-container"">$W_V$</span></li>
<li>Output matrix <span class=""math-container"">$W_O$</span></li>
</ul>
<p>Nice illustration from  <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">https://jalammar.github.io/illustrated-transformer/</a></p>
<p><a href=""https://i.sstatic.net/X6STQ.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/X6STQ.png"" alt=""enter image description here"" /></a></p>
<p>However, I do not know how should one choose the default initialization for these parameters.</p>
<p>In the works, devoted to MLP and CNNs, one chooses <code>xavier/glorot</code> or <code>he</code> initialization by default, as they can be shown to approximately preserve the magnitude in the forward and backward pass, <a href=""https://arxiv.org/abs/2012.05760"" rel=""noreferrer"">as shown in these notes</a>.</p>
<p>However, I wonder, whether there is some study of good initialization for Transformers. The default implementation in <code>Tensorflow</code> and <code>PyTorch</code> use <code>xavier/glorot</code>.</p>
<p>Probably, any reasonable choice will work fine.</p>
","transformer"
"30379","Transformer model is very slow and doesn't predict well","2021-08-26 10:02:44","30404","0","3379","<deep-learning><tensorflow><keras><transformer><time-series>","<p>I created my first transformer model, after having worked so far with LSTMs. I created it for multivariate time series predictions - I have 10 different meteorological features (temperature, humidity, windspeed, pollution concentration a.o.) and with them I am trying to predict time sequences (24 consecutive values/hours) of air pollution. So my input has the shape <code>X.shape = (75575, 168, 10)</code> - 75575 time sequences, each sequence contains 168 hourly entries/vectors and each vector contains 10 meteo features. My output has the shape <code>y.shape = (75575, 24)</code> - 75575 sequences each containing 24 consecutive hourly values of the air pollution concentration.</p>
<p>I took as a model an <a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">example</a> from the official keras site. It is created for classification problems, I only took out the <code>softmax</code> activation and in the last dense layer I set the number of neurons to 24 and I hoped it would work. It runs and trains, but it does worse predictions than the LSTMs I have used on the same problem and more importantly - it is very slow - 4 min/epoch. Below I attach the model and I would like to know:</p>
<p>I) Have I done something wrong in the model? can the accuracy or speed be improved? Are there maybe some other parts of the code I need to change for it to work on regression, not classification problems?</p>
<p>II) Also, can a transformer at all work on multivariate problems of my kind (10 features input, 1 feature output) or do transformers only work on univariate problems? Tnx</p>
<pre><code>def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):

    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):

        # Normalization and Attention
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        x = layers.MultiHeadAttention(
            key_dim=head_size, num_heads=num_heads, dropout=dropout
        )(x, x)
        x = layers.Dropout(dropout)(x)
        res = x + inputs

        # Feed Forward Part
        x = layers.LayerNormalization(epsilon=1e-6)(res)
        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(dropout)(x)
        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
        x = x + res

    x = layers.GlobalAveragePooling1D(data_format=&quot;channels_first&quot;)(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(mlp_dropout)(x)
    x = layers.Dense(24)(x)
    return keras.Model(inputs, x)

model_tr = build_transformer_model(input_shape=(window_size, X_train.shape[2]), head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], mlp_dropout=0.4, dropout=0.25)
model_tr.compile(loss=&quot;mse&quot;,optimizer='adam') 
m_tr_history = model_tr.fit(x=X_train, y=y_train, validation_split=0.25, batch_size=64, epochs=10, callbacks=[modelsave_cb])
</code></pre>
","transformer"
"30341","Why does a transformer not use an activation function following the multi-head attention layer?","2021-08-24 09:55:16","30392","12","7530","<transformer><attention>","<p>I was hoping someone could explain to me why in the transformer model from the &quot;Attention is all you need&quot; paper there is no activation applied after both the multihead attention layer and to the residual connections.  It seems to me that there are multiple linear layers in a row, and I have always been under the impression that you should have an activation between linear layers.</p>
<p>For instance when I look at the different flavors of resnet they always apply some sort of non linearity following a linear layer.  For instance a residual block might look something like...</p>
<p>Input -&gt; Conv -&gt; BN -&gt; Relu -&gt; Conv -&gt; (+ Input) -&gt; BN -&gt; Relu</p>
<p>or in the case of pre-activation...</p>
<p>Input -&gt; BN -&gt; Relu -&gt; Conv -&gt; BN -&gt; Relu -&gt; Conv -&gt; (+ Input)</p>
<p>In all the resnet flavors I have seen, they never allow two linear layers to be connected without a relu in-between.</p>
<p>However in the the transformer...</p>
<p>Input -&gt; Multihead-Attn -&gt; Add/Norm -&gt; Feed Forward(Dense Layer -&gt; Relu -&gt; Dense Layer) -&gt; Add/Norm</p>
<p>In the multihead attention layer it performs the attention mechanism and then applies a fully connected layer to project back to the dimension of its input.  However, there is no non linearity between that and feed forward network (except for maybe the softmax used in part of the attention.)  A model like this would make more sense to me...</p>
<p>Input -&gt; Multihead-Attn -&gt; Add/Norm -&gt; <strong>Relu</strong> -&gt; Feed Forward(Dense Layer -&gt; Relu -&gt; Dense Layer) -&gt; Add/Norm -&gt; <strong>Relu</strong></p>
<p>or something like the pre-activated resnet...</p>
<p>Input -&gt; Relu -&gt; Multihead-Attn -&gt; Add/Norm -&gt; Input2 -&gt; <strong>Relu</strong> -&gt; Feed Forward(Dense Layer -&gt; Relu -&gt; Dense Layer) -&gt; Add/Norm(Input2)</p>
<p>Can anyone explain why the transformer is the way it is?</p>
<p>I have asked a similar question when I was looking at the architecture of wavenet on another forum but I never really got a clear answer. In that case it did not make sense to me again why there was no activation applied to the residual connections.
(<a href=""https://www.reddit.com/r/MachineLearning/comments/njbjfb/d_is_there_a_point_to_having_layers_with_just_a/"" rel=""noreferrer"">https://www.reddit.com/r/MachineLearning/comments/njbjfb/d_is_there_a_point_to_having_layers_with_just_a/</a>)</p>
","transformer"
"30299","How is Google Translate able to translate texts of arbitrarily large length?","2021-08-21 22:38:39","","1","62","<recurrent-neural-networks><transformer><machine-translation>","<p>Sequence-to-sequence models with attention are known to be limited by a maximum sequence length. So how can we handle sequences of arbitrarily large size? Do we just set a very large maximum sequence length?</p>
","transformer"
"30239","What is the difference between a vision transformer and image-based relational learning?","2021-08-18 13:22:51","30339","0","223","<comparison><papers><transformer><architecture><vision-transformer>","<p>I am trying to figure out the difference between the architecture used in <a href=""https://arxiv.org/pdf/2010.11929.pdf"" rel=""nofollow noreferrer"">this</a> and <a href=""https://arxiv.org/pdf/1806.01830.pdf"" rel=""nofollow noreferrer"">this</a> paper. It looks like both used multi-headed self-attention and therefore should be the same in principle.</p>
","transformer"
"30092","What is the bit memory task?","2021-08-09 14:53:07","30110","0","134","<transformer><attention><memory>","<p>I learned from <a href=""https://read.deeplearning.ai/the-batch/issue-102/"" rel=""nofollow noreferrer"">this post</a> about the so-called bit memory:</p>
<blockquote>
<p>They froze its self-attention and feed-forward layers and, in separate copies, fine-tuned peripheral layers on each on a wide range of tasks: Bit memory (memorizing strings of bits), Bit XOR (performing logical operations on pairs of strings of bits), ListOps (parsing and performing mathematical operations), MNIST, CIFAR-10 (classification of images), CFAR-10 LRA (classification of flattened, greyscale images), and remote homology detection (predicting what kind of protein structure an amino acid is part of).</p>
</blockquote>
<p>I wonder what the &quot;bit memory&quot; task is? Is it an identity function as described in <a href=""https://krbnite.github.io/Deep-Learning-Nanodegree-Notes-on-Autoencoders/"" rel=""nofollow noreferrer"">this post</a>? Or the <a href=""https://paperswithcode.com/method/memory-network"" rel=""nofollow noreferrer"">memory network</a>?</p>
","transformer"
"29903","Why people always say the Transformer is parallelizable while the self-attention layer still depends on outputs of all time steps to calculate?","2021-07-29 10:18:33","","1","3528","<transformer><attention><implementation>","<p>When compared to an RNN seq-to-seq model, people always say the Transformer is parallelizable. In the original <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a> paper, it also said that</p>
<blockquote>
<p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states <span class=""math-container"">$h_t$</span>, as a function of the previous hidden state <span class=""math-container"">$h_{t−1}$</span> and the input for position <span class=""math-container"">$t$</span>. This inherently sequential nature precludes parallelization within training examples</p>
</blockquote>
<p>I use the <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">The illustrated Transformer</a> to help to explain my question here. It said (You can search those sentences):</p>
<blockquote>
<p>Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.</p>
</blockquote>
<p><strong>However</strong>, actually in the self-attention layer, in order to calculate the select V, it needs the <em>key</em> values of all time steps! So each &quot;time step&quot; is not fully independent. There exist an operation in the layer that depends on the output, here is the key from all &quot;time step&quot;.</p>
<p>In the original paper, the same block is repeated 6 times. That means there are at least 6 points where the flow of independent operation of each &quot;time step&quot; or each token to wait for the others. Yes, it is better, but why do they call it parallelizable?</p>
","transformer"
"28863","Positional Encoding in Transformer on multi-variate time series data hurts performance","2021-07-27 19:18:50","","2","637","<tensorflow><pytorch><transformer><time-series><positional-encoding>","<p>I set up a transformer model that embeds positional encodings in the encoder. The data is multi-variate time series-based data.</p>
<p>As I just experiment with the positional encoding portion of the code I set up a toy model: I generated a time series that contains the log changes of a sine function and run a classification model that predicts whether the subsequent value is positive or negative. Simple enough. I also added a few time series with random walks to try to throw off the model.</p>
<p>Predictably, the model very quickly reaches a categorical accuracy of around 99%. Without positional encoding that happens already in the 3rd epoch. However, with positional encoding (I use the same implementation as proposed in the &quot;Attention is all you need&quot; paper), it takes over 100 epochs to reach a similar accuracy level.</p>
<p>So, clearly, all else being equal, learning with positional encoding takes much longer to reach an equal accuracy level than without positional encoding.</p>
<p>Has anyone witnessed similar observations? Apparently adding the positional encodings to the actual values seems to confuse the model. I have not tried concatenations yet. Any advice?</p>
<p>Edit: Or does it simply mean that learned positional encodings perform better than sin/cos encodings? I have not made any special provisions to encourage learned positional encodings, I simply either added the positional encodings to the actual values or I did not.</p>
","transformer"
"28818","In Computer Vision, what is the difference between a transformer and attention?","2021-07-25 04:01:48","28841","12","5764","<computer-vision><comparison><transformer><attention>","<p>Having been studying computer vision for a while, I still cannot understand what the difference between a transformer and attention is?</p>
","transformer"
"28620","What is the proper way to process continuous sequence data, such as time-series, using the Transformer?","2021-07-11 14:37:10","","4","2706","<deep-learning><keras><transformer><time-series><sequence-modeling>","<p>What is the right way to input continuous, temporal (time-series) data into the Transformer? Assume we're using the basic TransformerBlock <a href=""https://keras.io/examples/nlp/text_classification_with_transformer/"" rel=""nofollow noreferrer"">here</a>.</p>
<p>Since data is continuous with no tokens, Token embedding can be directly skipped. How about positional encoding? I tried this example, removing Token embedding while keeping positional encoding but ended up with shape-related errors. Skipping both token and positional encoding resulted in a network that runs and trains but results were relatively poor compared to the LSTM benchmark with the same data.</p>
<p>I am unsure if the positional encoding is still needed.</p>
<p>Overall, my question is, what is the proper way to process continuous sequence data, such as time-series, using the Transformer architecture?</p>
","transformer"
"28326","Why class embedding token is added to the Visual Transformer?","2021-06-19 08:22:29","34781","8","5698","<computer-vision><classification><papers><transformer><vision-transformer>","<p>In the <a href=""https://arxiv.org/abs/2010.11929"" rel=""noreferrer"">famous work on the Visual Transformers</a>, the image is split into patches of a certain size (say 16x16), and these patches are treated as tokens in the NLP tasks. In order to perform classification, a <strong>CLS</strong> token is added at the beginning of the resulting sequence:
<span class=""math-container"">$$
[\textbf{x}_{class}, \textbf{x}_{p}^{1}, \ldots, \textbf{x}_{p}^{N}]
,$$</span>
where <span class=""math-container"">$ \textbf{x}_{p}^{i}$</span> are image patches. There multiple layers in the architecture and the state of the <strong>CLS</strong> token on the output layer is used for classification.</p>
<p>I think this architectural solution is done in the spirit of NLP problems (BERT in particular). However, for me, it would be more natural not to create a special token, but perform <em>1d Global Pooling</em> in the end, and attach an <code>nn.Linear(embedding_dim, num_classes)</code> as more conventional CV approach.</p>
<p>Why it is not done in this way? Or is there some intuition or evidence that this would perform worse than the approach used in the paper?</p>
","transformer"
"28232","What is the best way to generate German paraphrases?","2021-06-13 18:24:49","","1","176","<transformer><natural-language-understanding><seq2seq><natural-language-generation>","<p>What is the best method to generate German paraphrases? The state-of-the-art are seq2seq transformer models, like T5, but they only work for English sentences. I found the multilingual MT5 model, but how do you fine-tune this for German?</p>
","transformer"
"28113","How do autoregressive attention mechanism work in multi-headed attention?","2021-06-05 16:32:52","","0","763","<neural-networks><deep-learning><python><transformer><attention>","<p>[LONG POST!!] I am working on a DNN model that works as an improviser to generate music sequences. The idea of generating music is based on taking a sequence of music nodes (their index representation) and generating sequences that are distinctive with more context and coherent structure as well as capturing syntactic and structural information from the original sequences. Therefore I am dealing with a time series dataset. Similar work was reported in &quot;<a href=""https://arxiv.org/pdf/2002.03854.pdf"" rel=""nofollow noreferrer"">Attentional Networks for music generation</a>&quot; but in our case, we have a different model architecture and different dataset.
It has been known that Transformer (attention) suffers in multivariate time series dataset (Source: <a href=""https://towardsdatascience.com/attention-for-time-series-classification-and-forecasting-261723e0006d"" rel=""nofollow noreferrer"">Attention for time series forecasting and classification</a>). But given these problems were reported two years ago, the SOTA should be better by now. For that reason, my target is to use the attention mechanism in a way to overcome these challenges.</p>
<p>Recently I have been using the <a href=""https://www.tensorflow.org/addons/api_docs/python/tfa/layers/MultiHeadAttention"" rel=""nofollow noreferrer"">multiheaded attention layer from TF</a> and testing with head size between 128 and 3074 and head number from 1 to 10 and dropout from 0.1 to 0.5. Based on the results there was no noticeable improvement in the model performance, it seems that the multi-headed attention layer didn't have contribution during training.</p>
<p>Therefore and after carefully reading the literature I found that autoregressive attention is the best option for this types of problem. Basically, by making the attention autoregressive, it will compute the attention over the previous (decoder) outputs in such a way as to avoid using future information to make current predictions (to preserve the notion of causality). So the attention has to designed so that at each time step it needs to be autoregressive, for example, use previously generated sequences as extra input while generating the next symbol.</p>
<p>In &quot;<a href=""https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1174/reports/2755456.pdf"" rel=""nofollow noreferrer"">Autoregressive Attention for Parallel Sequence Modeling</a>&quot; paper they introduced the autoregressive attention mechanism in order to maintain the causality in the decoder. I didn't understand what they mean in Section 3.3 which describe the implementation of autoregressive attention. My problem is in the autoregressive implementation, in the paper they stated that autoregressive mechanics was implemented using the masking technique which changes all of the elements in the upper-right triangle and the diagonal
to −∞ 3 to ensure that all the scores that would introduce future information into the attention calculation are equal to 0 after the softmax. I was hoping to see how it was implemented in the code to get a better idea of how it works.</p>
<p>Here is how the attention is implemented in tensorflow:</p>
<pre><code>def multiHeadedAttentionLayer(cell_input):  
    cell_state = None    

    if cell_state is None:
        cell_state = cell_input

    mha = tfa.layers.MultiHeadAttention(head_size=128, num_heads=5, dropout = 0.5)  
    cell_output = mha([cell_input, cell_state])   
    cell_state = cell_input

    return cell_output
</code></pre>
<p>Then the function is recalled in the model architecture with the rest of the layers (below is a section of the model architecture only):</p>
<pre><code>x = MaxPooling1D(pool_size=2)(x) # previous layer
x = multiHeadedAttentionLayer(x) # attention layer
x = LSTM(lstmneurons, kernel_regularizer=regularizers.l2(kreg3_rate), dropout=dropout3_rate, recurrent_dropout=dropout4_rate)(x) # following layer
x = BatchNormalization()(x) # following layer
</code></pre>
<p>etc....</p>
<p>Based on my intuition the autoregression should take the output results and feed them back to the input at every time step, so my questions are:</p>
<p>Why do we need the masking technique?</p>
<p>How to implement the masking technique in this case?</p>
<p>Is there is a code for the autoregressive attention that I have a look at for reference?</p>
<p>Is my current intuition about autoregressive attention correct as shown in the diagram?</p>
<p><a href=""https://i.sstatic.net/zldoC.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zldoC.png"" alt=""Autoregressive Attention"" /></a></p>
","transformer"
"28032","Has positional encoding been used in convolutional layers?","2021-05-31 12:07:45","","0","391","<convolutional-neural-networks><reference-request><transformer><convolution><positional-encoding>","<p>Positional encoding (PE) is an essential part of the self-attention layers in the transformer architectures since without adding it in some way (fixed of learnable) to the input embeddings model has ultimately no notion of order and is permutationally equivariant and the given token attends to the far separate and local tokens identically.</p>
<p>The convolution operation with a local filter, say of size <span class=""math-container"">$3, 5$</span> for 1D convolutions or <span class=""math-container"">$3 \times 3, 5 \times 5$</span> for 2D convolutions, has some notion of locality by construction. However, within this neighborhood, all pixels are treated in the same way.</p>
<p>However, it may be the case, that it is important, that the given pixel is the central for the application of this filter, whereas the other is close to the boundary. For small filters
<span class=""math-container"">$3 \times 3$</span> it is probably not an issue, but for the larger - injection of PE can be useful.</p>
<p>Has this question been investigated in the literature? Are there any architectures with PE + convolutions?</p>
","transformer"
"28031","Visualizing encoder-attention after ResNet in terms of ResNet input","2021-05-31 10:46:29","","0","174","<pytorch><transformer><attention><data-visualization>","<p>I have a transform-encoder only architecture, which has the following structure:</p>
<pre><code>      Input
        |
        v
     ResNet(-50)
        |
        v
fully-connected (on embedding dimension)
        |
        v
positional-encoding
        |
        v
transformer encoder
        |
        v
Linear layer to alphabet.
</code></pre>
<p>I am trying to visualize the self-attention of the encoder layer to check how each input of the attention attends other inputs. (E.g. <a href=""https://github.com/jessevig/bertviz"" rel=""nofollow noreferrer"">https://github.com/jessevig/bertviz</a>)</p>
<p>Where I encounter difficulty is in how I can visualize these activations in terms of the original input of the ResNet and not its output, in order to make my model visually interpretable.</p>
<p>Do you have any ideas or suggestions?</p>
","transformer"
"27979","What makes a transformer a transformer?","2021-05-27 08:21:00","","6","407","<deep-learning><definitions><transformer>","<p>Transformers are modified heavily in recent research. But what exactly makes a transformer a transformer? What is the core part of a transformer? Is it the <em>self-attention</em>, the <em>parallelism</em>, or something else?</p>
","transformer"
"27947","Is it realistic to train a transformer-based model (e.g. GPT) in a self-supervised way directly on the Mel spectrogram?","2021-05-24 21:31:55","27953","2","433","<transformer><gpt><audio-processing><embeddings><self-supervised-learning>","<p>In music information retrieval, one usually converts an audio signal into some kind &quot;sequence of frequency-vectors&quot;, such as STFT or Mel-spectrogram.</p>
<p>I'm wondering if it is a good idea to use the transformer architecture in a self-supervised manner -- such as auto-regressive models, or BERT in NLP -- to obtain a &quot;smarter&quot; representation of the music than the spectrogram itself. Such smart pretrained representation could be used for further downstream tasks.</p>
<p>From my quick google search, I found several papers which do something similar, but -- to my surprise -- all use some kind of symbolic/discrete music representation such as scores. (For instance <a href=""https://arxiv.org/pdf/1809.04281.pdf"" rel=""nofollow noreferrer"">here</a> or <a href=""https://arxiv.org/pdf/1912.05537.pdf"" rel=""nofollow noreferrer"">here</a>).</p>
<p>My question is this:</p>
<blockquote>
<p>Is it realistic to train such an unsupervised model directly on the
Mel spectrogram?</p>
</blockquote>
<p>The loss function would not be &quot;log softmax of next word probability&quot;, but some kind of l2-distance between &quot;predicted vector of spectra&quot; and &quot;observed vector of spectra&quot;, in the next time step.</p>
<p>Did someone try it?</p>
","transformer"
"27836","Are there any successful applications of transformers of small size (<10k weights)?","2021-05-16 19:35:23","","2","45","<applications><transformer><sequence-modeling><efficiency><seq2seq>","<p>In the problems of NLP and sequence modeling, the Transformer architectures based on the <strong>self-attention mechanism</strong> (proposed in <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention Is All You Need</a>) have achieved impressive results and now are the first choices in this sort of problem.</p>
<p>However, most of the architectures, which appear in the literature, have a lot of parameters and are aimed at solving rather complicated tasks of language modeling (<a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">[1]</a>, <a href=""https://arxiv.org/abs/2005.14165"" rel=""nofollow noreferrer"">[2]</a>). These models have a large number of parameters and are computationally expensive.</p>
<p>There exist multiple approaches to reduce the computational complexity of these models, like <a href=""https://medium.com/pytorch/bert-distillation-with-catalyst-c6f30c985854?sk=1a28469ac8c0e6e6ad35bd26dfd95dd9"" rel=""nofollow noreferrer"">knowledge distillation</a> or multiple approaches to deal with the <span class=""math-container"">$O(n^2)$</span>
computational complexity of the self-attention (<a href=""https://arxiv.org/abs/2006.04768"" rel=""nofollow noreferrer"">[3]</a>, <a href=""https://arxiv.org/abs/2009.14794"" rel=""nofollow noreferrer"">[4]</a>).</p>
<p>However, these models are still aimed at language modeling and require quite a lot of parameters.</p>
<p>I wonder whether there are successful applications of transformers with a very small number of parameters (1k-10k), in the signal processing applications, where inference has to be performed in a very fast way, hence heavy and computationally expensive models are not allowed.</p>
<p>So far, the common approaches are CNN or RNN architectures, but I wonder whether there are some results, where lightweight transformers have achieved SOTA results for these extremely small models.</p>
","transformer"
"27679","How do transformers understand data and answer custom questions?","2021-05-06 15:40:03","40213","2","134","<neural-networks><transformer><human-like><gpt-3>","<p>I recently heard of GPT-3 and I don't understand how the attention models and transformers encoders and decoders work. I heard that GPT-3 can make a website from a description and write perfectly factual essays. How can it understand our world using algorithms and then recreate human-like content? How can it learn to understand a description and program in HTML?</p>
","transformer"
"27657","Representing variable-length sequences","2021-05-05 13:53:42","","1","65","<transformer><sequence-modeling><matrices>","<p>I want to train a model over variable-length sequential data (e.g. the temperature at different times of day) where the output depends on what the temperature is at a time <code>T</code>.</p>
<p>Ideally, I want to represent the input using a variable-length compacted format of [temperature, duration]. Alternatively, I can divide a matrix into time slices where each cell contains the current temperature.</p>
<p>I prefer the compacted format as it is more space-efficient and allows me to represent arbitrary-length durations, but I am afraid that a Transformer architecture won't be able to figure out what the temperature is at a time <code>T</code> using the compact format.</p>
<p>Is it safe to compact sequential inputs?</p>
","transformer"
"27471","Embedding from Transformer-based model from paragraph or documnet (like Doc2Vec)","2021-04-22 18:50:17","","2","27","<natural-language-processing><transformer><bert><embeddings>","<p>I have a set of data that contains the different lengths of sequences. On average the sequence length is 600. The dataset is like this:</p>
<pre><code>S1 = ['Walk','Eat','Going school','Eat','Watching movie','Walk'......,'Sleep']
S2 = ['Eat','Eat','Going school','Walk','Walk','Watching movie'.......,'Eat']
.........................................
.........................................
S50 = ['Walk','Going school','Eat','Eat','Watching movie','Sleep',.......,'Walk']
</code></pre>
<p>The number of unique actions in the dataset are fixed. That means some sentences may not contain all of the actions.</p>
<p>By using Doc2Vec (Gensim library particularly), I was able to extract embedding for each of the sequences and used that for later task (i.e., clustering or similarity measure)</p>
<p>As transformer is the state-of-the-art method for NLP task. I am thinking if Transformer-based model can be used for similar task. While searching for this technique I came across the &quot;sentence-Transformer&quot;-
<a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a>. But it uses a pretrained BERT model (which is probably for language but my case is not related to language) to encode the sentences. Is there any way I can get embedding from my dataset using Transformer-based model?</p>
","transformer"
"27326","What part of the Vaswani et al. is the ""transformer""?","2021-04-14 12:36:52","","3","114","<neural-networks><deep-learning><terminology><transformer>","<p>Which part of this is the transformer?</p>
<p><a href=""https://i.sstatic.net/qrbxN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qrbxN.png"" alt=""enter image description here"" /></a></p>
<p>Ok, the caption says the whole thing is the transformer, but that's back in 2017 when the paper was published. My question is about how the community uses the term &quot;transformer&quot; now.</p>
<p>I'm not looking for an inline response to these questions. They are all a way of asking the same general thing.</p>
<ul>
<li>Is this whole thing a transformer?</li>
<li>What parts or what relationships between parts make it a transformer?</li>
<li>Equivalently, what aspects can I change before it becomes something else and not a transformer?</li>
<li>If I only care about self-attention I suppose I don't need the right hand column. If I just keep the self-attention, is it still a transformer?</li>
</ul>
<p>Context about me is I've just become familiar with transformers and have not read much literature on them since this paper.</p>
","transformer"
"27254","How to Select Model Parameters for Transformer (Heads, number of layers, etc)","2021-04-10 15:24:46","","3","571","<natural-language-processing><transformer><hyperparameter-optimization><attention><gpt>","<p>Is there a general guideline on how the Transformer model parameters should be selected, or the range of these parameters that should be included in a hyperparameter sweep?</p>
<ul>
<li>Number of heads</li>
<li>Number of encoder &amp; decoder layers</li>
<li>Size of transformer model (<code>d_model</code> in Pytorch)</li>
<li>Size of hidden layers</li>
</ul>
<p>Are there general guidelines like number of decoder layers should be equal to encoder layers? Thank you</p>
","transformer"
"27044","Can an existing transformer model be modified to estimate the next most probable number in a sequence of numbers?","2021-03-28 03:17:29","","1","315","<transformer><attention><bert><gpt><forecasting>","<p>Models based on the transformer architectures (GPT, BERT, etc.) work awesome for NLP tasks including taking an input generated from words and producing probability estimates of the next word as the output.</p>
<p>Can an existing transformer model, such as GPT-2, be modified to perform the same task on a sequence of numbers and estimate the next most probable number? If so, what modifications do we need to perform (do we still train a tokenizer to tokenize integers/floats into token IDs?)?</p>
","transformer"
"27038","Why does GPT-2 Exclude the Transformer Encoder?","2021-03-27 19:55:30","","20","13588","<natural-language-processing><transformer><attention><bert><gpt>","<p>After looking into transformers, BERT, and GPT-2, from what I understand, GPT-2 essentially uses only the decoder part of the original transformer architecture and uses masked self-attention that can only look at prior tokens.</p>
<p>Why does GPT-2 not require the encoder part of the original transformer architecture?</p>
<p><strong>GPT-2 architecture with only decoder layers</strong></p>
<p><a href=""https://i.sstatic.net/Kb8Gq.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Kb8Gq.png"" alt=""enter image description here"" /></a></p>
","transformer"
"27009","Why (not) using pre-processing before using Transformer models?","2021-03-25 18:37:05","","0","314","<natural-language-processing><pytorch><transformer><data-preprocessing>","<p>Regarding the use of pre-processing techniques before using Transformers models, I read <a href=""https://stackoverflow.com/a/63986348/13745968"">this post</a> that apparently says that these measures are not so necessary nor interfere so much in the final result.</p>
<p>The arguments raised seemed to me quite convincing, but someone would know how to explain better, perhaps with a bibliographic reference, why is it not so necessary to use these techniques?</p>
","transformer"
"26973","How to construct Transformers to predict multidimensional time series?","2021-03-24 07:48:23","26980","3","1757","<deep-learning><transformer>","<p>There is plenty of information describing Transformers in a lot of detail how to use them for NLP tasks. Transformers can be applied for time series forecasting.  See for example <a href=""https://papers.nips.cc/paper/2020/file/c6b8c8d762da15fa8dbbdfb6baf9e260-Paper.pdf"" rel=""nofollow noreferrer"">&quot;Adversarial Sparse Transformer for Time Series Forecasting&quot;</a> by Wu et al.</p>
<p>For understanding it is best to replicate everything according to already existing examples. There is a very nice example for LSTM with flights dataset <a href=""https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/"" rel=""nofollow noreferrer"">https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python/</a>.</p>
<p>I guess I would like to know how to implement transformers for at first univariate (flight dataset) and later for multivariate time series data. What should be removed from the Transformer architecture to form a model that would predict time series?</p>
","transformer"
"26531","How are certain machine learning models able to produce variable-length outputs given variable-length inputs?","2021-02-24 06:42:08","26534","3","2087","<machine-learning><natural-language-processing><recurrent-neural-networks><transformer><natural-language-generation>","<p>Most machine learning models, such as multilayer perceptrons, require a fixed-length input and output, but generative (pre-trained) transformers can produce sentences or full articles of variable length. How is this possible?</p>
","transformer"
"26530","How is the transformers' output matrix size arrived at?","2021-02-24 04:02:16","26549","1","262","<transformer><attention><encoder-decoder>","<p>In <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">this</a> tensorflow article, the comments in the code say that MHA should output with one of the dimensions being the sequence length of the query/key. However, that means that the second MHA in the decoder layer should output something with one of the dimensions being the input sequence length, but clearly it should actually be the output sequence length! From all that I have read on transformers, it seems that the output of the left side of the SHA should be a matrix with dimensions q_seq_length x q_seq_length, and the output of the right side of the SHA should be v_seq_length x d_model. These matrices can't even multiply when using the second MHA in the decoder to incorporate the encoder output! Please help. I would appreciate a clear-cut explanation. Thanks</p>
","transformer"
"26413","Sentiment analysis does not handle neturals","2021-02-17 01:15:00","26414","0","86","<python><transformer><bert><sentiment-analysis>","<p>I'm writing some financial tools,  I've found highly performant models for question and answering but when it comes to sentiment analysis I haven't found anything that good.  I'm trying to use huggingface:</p>
<pre><code>from transformers import pipeline
classifier = pipeline('sentiment-analysis')
print(classifier(&quot;i'm good&quot;))
print(classifier(&quot;i'm bad&quot;)) 
print(classifier(&quot;i'm neutral&quot;))
print(classifier(&quot;i'm okay&quot;)) 
print(classifier(&quot;i'm indifferent&quot;)) 
</code></pre>
<p>Which returns results</p>
<blockquote>
<p>[{'label': 'POSITIVE', 'score': 0.999841034412384}]</p>
</blockquote>
<blockquote>
<p>[{'label': 'NEGATIVE', 'score': 0.9997877478599548}]</p>
</blockquote>
<blockquote>
<p>[{'label': 'NEGATIVE', 'score': 0.999396026134491}]</p>
</blockquote>
<blockquote>
<p>[{'label': 'POSITIVE', 'score': 0.9998164772987366}]</p>
</blockquote>
<blockquote>
<p>[{'label': 'NEGATIVE', 'score': 0.9997762441635132}]</p>
</blockquote>
<p>The scores for all of the neutral words come up very high in a positive or negative direction,  I would of figured the model would put the score lower.</p>
<p>I've looked at some of the more <a href=""https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"" rel=""nofollow noreferrer"">fine-tuned models</a> yet they seem to perform the same.</p>
<p>I would assume there would be some pretrained models which could handle these use cases.  If not, How can I find neutral sentiments?</p>
","transformer"
"26284","What does the outputlayer of BERT for masked language modelling look like?","2021-02-08 19:16:14","","1","623","<natural-language-processing><transformer><attention><word-embedding><bert>","<p>In the tutorial <a href=""https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/"" rel=""nofollow noreferrer"">BERT – State of the Art Language Model for NLP</a> the masked language modeling pre-training steps are described as follows:</p>
<blockquote>
<p>In technical terms, the prediction of the output words requires:</p>
<ol>
<li>Adding a classification layer on top of the encoder output.</li>
</ol>
<p>2.Multiplying the output vectors by the embedding matrix, transforming them into the vocabulary dimension.</p>
<p>3.Calculating the probability of each word in the vocabulary with softmax.</p>
</blockquote>
<p>In the Figure below this process is visualized and also from the tutorial.</p>
<p>I am confused about what exactly is done. Does it mean that each output vector O is fed into a fully connected layer with embedding_size neurons and then multiplied by the embedding matrix from the input layer?</p>
<p>Update:</p>
<p>In the tutorial <a href=""http://jalammar.github.io/illustrated-gpt2/"" rel=""nofollow noreferrer"">The Illustrated GPT-2 (Visualizing Transformer Language Models)</a> I found an explanation for GPT-2 which seems to be similar to my question.</p>
<p>In the tutorial is said that each output vector is multiplied by the input embedding matrix to get the final output.</p>
<p>Does the same mechanic apply to BERT?</p>
<p><img src=""https://miro.medium.com/max/698/0*ViwaI3Vvbnd-CJSQ.png"" alt=""Text"" /></p>
","transformer"
"26235","What kind of word embedding is used in the original transformer?","2021-02-05 18:51:59","26246","20","13440","<natural-language-processing><transformer><attention><word-embedding>","<p>I am currently trying to understand transformers.</p>
<p>To start, I read <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">Attention Is All You Need</a> and also <a href=""https://nlp.seas.harvard.edu/2018/04/03/attention.html"" rel=""noreferrer"">this</a> tutorial.</p>
<p>What makes me wonder is the word embedding used in the model. Is word2vec or GloVe being used? Are the word embeddings trained from scratch?</p>
<p>In the tutorial linked above, the transformer is implemented from scratch and nn.Embedding from pytorch is used for the embeddings. I looked up this function and didn't understand it well, but I tend to think that the embeddings are trained from scratch, right?</p>
","transformer"
"26184","What is the purpose of ""alignment"" in the self-attention mechanism of transformers?","2021-02-04 04:31:46","","1","896","<neural-networks><transformer><attention><machine-translation>","<p>I've been reading about transformers &amp; have been having some difficulty understanding the concept of <em>alignment</em>.</p>
<p>Based on this <a href=""https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3#16cb"" rel=""nofollow noreferrer"">article</a></p>
<blockquote>
<p>Alignment means matching segments of original text with their corresponding segments of the translation.</p>
</blockquote>
<p>Does this mean that, with transformers, we're adding the fully translated sentences as inputs too? What's the purpose of alignment? How exactly do these models figure out how to match the different segments together? I'm pretty sure there's some underlying assumption/knowledge that I'm not fully getting -- but I'm not entirely sure what.</p>
","transformer"
"25986","How does the embeddings work in vision transformer from paper?","2021-01-26 04:28:19","","4","1651","<neural-networks><computer-vision><transformer><embeddings><vision-transformer>","<p>I get the part from <a href=""https://arxiv.org/abs/2010.11929"" rel=""nofollow noreferrer"">the paper</a> where the image is split into <code>P</code> say <code>16x16</code> (smaller images) patches and then you have to <code>Flatten</code> the 3-D (16,16,3) patch to pass it into a <code>Linear</code> layer to get what they call &quot;Liner Projection&quot;. After passing from the Linear layer, the patches will be vectors but with some &quot;meaning&quot; to them.</p>
<p>Can someone please explain how the two types of embeddings are working?</p>
<p>I visited <a href=""https://github.com/gupta-abhay/pytorch-vit/"" rel=""nofollow noreferrer"">this implementation on github</a>, looked at the code too and looked like a maze to me.</p>
<p>If someone could just explain how these embeddings are working in laymen's terms, I'll look at the code again and understand.</p>
","transformer"
"25962","Is using a LSTM, CNN or any other neural network model on top of a Transformer(using hidden states) overkill?","2021-01-25 03:34:28","","1","647","<deep-learning><natural-language-processing><transformer><text-classification><sentiment-analysis>","<p>I have recently come across transformers, I am new to Deep Learning. I have seen a <a href=""https://arxiv.org/pdf/2007.10819.pdf"" rel=""nofollow noreferrer"">paper</a> using   CNN and BiLSTM on top of a transformer, the paper uses a transformer(XLM-R) for sentiment analysis in code-mixed domain. But many of the blogs only use a normal feed formal network on top of the transformer.</p>
<p>I am trying to use transformers for sentiment analysis, short text classification.</p>
<p>Is it overkill to use models like CNN and BiLSTM on top of the transformer considering the size of the data it is trained on and its complexity?</p>
","transformer"
"25483","Transformers: How to use the target mask properly?","2020-12-31 16:22:20","25517","2","5494","<pytorch><transformer>","<p>I try to apply Transformers to an unusual use case - predict the next user session based on the previous one. A user session is described by a list of events per second, e.g. whether the user watches a particular video, clicks a specific button, etc. Typical sessions are around 20-30 seconds, I pad them to 45 seconds. Here's a visual example of 2 subsequent sessions:</p>
<p><a href=""https://i.sstatic.net/ndcOw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ndcOw.png"" alt=""enter image description here"" /></a></p>
<p><code>x</code> axis is time in seconds, <code>y</code> axis is the list of events (black line divides the 2 sessions). I extend the vocabulary with 2 additional tokens - start and end of a session (<code>&lt;sos&gt;</code> and <code>&lt;eos&gt;</code>), where <code>&lt;sos&gt;</code> is a one-hot vector at the very beginning and <code>&lt;eos&gt;</code> - similar vector at the end of the session (which makes this long red line).</p>
<p>Now I use these extended vectors of events as <em>embeddings</em> and want to train a Transformer model to predict the next events in the current session based on previous events in this (target) session and all events in the previous (source) session. So pretty much like seq2seq autoregressive models, but in a bit unusual settings.</p>
<p>Here's the problem. When I train a Transformer using the built-in PyTorch components and square subsequent mask for the target, my generated (<em>during training</em>) output is too good to be true:</p>
<p><a href=""https://i.sstatic.net/hdCym.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hdCym.png"" alt=""enter image description here"" /></a></p>
<p>Although there's some noise, many event vectors in the output are modeled exactly as in the target. After checking train-val-test split is correct, my best guess is that the model cheats by attending to the same day in the target, which the mask should have prevented. The mask is (5x5 version for brevity):</p>
<pre><code>[[0., -inf, -inf, -inf, -inf],
 [0., -inf, -inf, -inf, -inf],
 [0., 0., -inf, -inf, -inf],
 [0., 0., 0., -inf, -inf],
 [0., 0., 0., 0., -inf]]
</code></pre>
<p>Note that since I use <code>&lt;sos&gt;</code> in both - source and target - <code>mask[i, i]</code> is set to <code>-inf</code> (except for <code>mask[0, 0]</code> for numerical reasons), so the output timestamp <code>i</code> should not attend to the target timestamp <code>i</code>.</p>
<p>The code for the model's <code>forward</code> method:</p>
<pre><code>def forward(self, src, tgt):
    memory = self.encoder(src)
    out = self.decoder(tgt, memory, self.tgt_mask.type_as(tgt))
    out = torch.sigmoid(out)
    return out
</code></pre>
<p>I also tried to avoid the target mask altogether and set it to all <code>-inf</code> (again, except for the first column for numerical stability), but the result is always the same.</p>
<p>Am I using the mask the wrong way? If the mask looks fine, what other reasons could lead to such a &quot;perfect&quot; result?</p>
<hr />
<p>After shifting the target to the right as suggested in the accepted answer I get the following result:</p>
<p><a href=""https://i.sstatic.net/iXT00.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iXT00.jpg"" alt=""enter image description here"" /></a></p>
<p>Which is much more realistic. One suspicious thing is that <code>out[t]</code> now resembles <code>tgt[t - 1]</code>, but it can be explained by the fact that the user state tends to be &quot;sticky&quot;, e.g. if a user watches a video at <code>t - 1</code>, most likely he will watch it at <code>t</code> as well.</p>
","transformer"
"25442","How can I use this Reformer to extract entities from a new sentence?","2020-12-29 13:48:54","","1","176","<transformer><named-entity-recognition><inference><reformer>","<p>I have been looking at the NER example with Trax in <a href=""https://github.com/google/trax/blob/master/trax/examples/NER_using_Reformer.ipynb"" rel=""nofollow noreferrer"">this notebook</a>. However, the notebook only gives an example for training the model. I can't find any examples of how to use this model to extract entities from a new string of text.</p>
<p>I've tried the following:</p>
<ul>
<li>Instantiate the model in 'predict' mode. When trying this I get the same error reported in <a href=""https://github.com/google/trax/issues/556"" rel=""nofollow noreferrer"">https://github.com/google/trax/issues/556</a> <code>AssertionError: In call to configurable 'SelfAttention' (&lt;class 'trax.layers.research.efficient_attention.SelfAttention'&gt;)</code></li>
<li>Instantiate the model in 'eval' mode and then running <code>model(sentence)</code> as I would with other models. In this case the instantiation works but I get the following error when running the model: <code>TypeError: Serial.forward input must be a tuple or list; instead got &lt;class 'numpy.ndarray'&gt;</code>. Presumably this is because in 'eval' mode the model needs 2 entries passed in rather than one sentence</li>
</ul>
<p>How can I use this Reformer to extract entities from a new sentence?</p>
","transformer"
"25329","Recent deep learning textbook (i.e. covering at least GANs, LSTM and transformers and attention)","2020-12-21 19:00:46","25339","4","858","<deep-learning><reference-request><generative-adversarial-networks><transformer><attention>","<p>I am searching for an academic (i.e. with maths formulae) textbook which covers (at least) the following:</p>
<ul>
<li>GAN</li>
<li>LSTM and transformers (e.g. seq2seq)</li>
<li>Attention mechanism</li>
</ul>
<p>The closest match I got is <em>Deep Learning</em> (2016, MIT Press) but it only deals with part of the above subjects.</p>
","transformer"
"25315","What is MNLI-(m/mm)?","2020-12-20 21:44:30","30245","1","2772","<terminology><transformer><bert><natural-language-understanding>","<p>I came across the term <strong>MNLI-(m/mm)</strong> in Table 1 of the paper <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>. I know what MNLI stands for, i.e. Multi-Genre Natural Language Inference, but I'm just unsure about the <strong>-(m/mm)</strong> part.</p>
<p>I tried to find some information about this in the paper <a href=""https://arxiv.org/pdf/1804.07461.pdf"" rel=""nofollow noreferrer"">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</a>, but this explained only the basic Multi-Genre Language Inference concept. I assume that the <strong>m/mm</strong> part was introduced later, but this doesn't make any sense because the BERT paper appeared earlier.</p>
<p>It would be nice if someone knows this or has a paper that explains this.</p>
","transformer"
"25253","Can the attention mechanism improve the performance in the case of short sequences?","2020-12-17 22:00:33","","1","195","<deep-learning><natural-language-processing><transformer><attention><performance>","<p>I am aware that the attention mechanism can be used to deal with long sequences, where problems related to gradient vanishing and, more generally, representing effectively the whole sequence arise.</p>
<p>However, I was wondering if attention, applied either to seq2seq RNN/GRU/LSTM or via Transformers, can contribute to improving the overall performance (as well as giving some sort of interpretability through the attention weights?) in the case of relatively short sequences (let's say around 20-30 elements each).</p>
","transformer"
"25222","In attention models with multiple layers, are weight matrices shared across layers?","2020-12-16 19:56:54","25230","1","1434","<transformer><attention><weights>","<p>In articles that describe neural architectures with multiple attention layers of the same form, are the weight matrices usually the same across the layers?  Consider as an example, &quot;Attention is all you need&quot;. The authors stack several layers of multi-head self-attention in which each layer has the same number of heads.  Each head <span class=""math-container"">$i$</span> involves a trainable weight matrix <span class=""math-container"">$W_{i}^{Q}$</span>.  There is no subscript, superscript, or any other indication that this matrix is different for each layer.  My questions is this: are there separate <span class=""math-container"">$W_{i}^{Q}$</span> for layers <span class=""math-container"">$1,2,3,...$</span> or is this a single matrix shared throughout layers?</p>
<p>My intuition is that the authors of the paper wanted to cut down on notation, and that the matrices are different in different layers.  But I want to be sure I understand this, since I see the same kind of thing in many other papers as well.</p>
","transformer"
"25217","In the multi-head attention mechanism of the transformer, why do we need both $W_i^Q$ and ${W_i^K}^T$?","2020-12-16 15:58:47","25225","2","1179","<deep-learning><transformer><attention><weights><efficiency>","<p>In the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention is all you need</a> paper, on the 4th page, we have equation 1, which describes the self-attention mechanism of the transformer architecture</p>
<p><span class=""math-container"">$$
\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V
$$</span></p>
<p>Everything is fine up to here.</p>
<p>Then they introduce the multi-head attention, which is described by the following equation.</p>
<p><span class=""math-container"">$$
\begin{aligned}
\text { MultiHead }(Q, K, V) &amp;=\text { Concat}\left(\text {head}_{1}, \ldots, \text {head}_{\mathrm{h}}\right) W^{O} \\
\text { where head}_{\mathrm{i}} &amp;=\text {Attention}\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)
\end{aligned}
$$</span></p>
<p>Once the multi-head attention is motivated at the end of page 4, they state that for a single head (the <span class=""math-container"">$i$</span>th head), the query <span class=""math-container"">$Q$</span> and key <span class=""math-container"">$K$</span> inputs are first linearly projected by <span class=""math-container"">$W_i^Q$</span> and <span class=""math-container"">$W_i^K$</span>, then dot product is calculated, let's say <span class=""math-container"">$Q_i^p =  Q W_i^Q$</span> and <span class=""math-container"">$K_i^p = K W_i^K$</span>.</p>
<p>Therefore, the dot product of the projected query and key becomes the following from simple linear algebra.</p>
<p><span class=""math-container"">$$Q_i^p {K_i^p}^\intercal = Q W_i^Q {W_i^K}^T K^T =  Q W_i K^T,$$</span></p>
<p>where</p>
<p><span class=""math-container"">$$W_i = W_i^Q {W_i^K}^T$$</span></p>
<p>Here, <span class=""math-container"">$W$</span> is the outer product of query projection by the key projection matrix. However, it is a matrix with shape <span class=""math-container"">$d_{model} \times d_{model}$</span>. Why did the authors not define only a <span class=""math-container"">$W_i$</span> instead of <span class=""math-container"">$W_i^Q$</span> and <span class=""math-container"">$W_i^K$</span> pair which have <span class=""math-container"">$2 \times d_{model} \times d_{k}$</span> elements? In deep learning applications, I think it would be very inefficient.</p>
<p>Is there something that I am missing, like these 2 matrices <span class=""math-container"">$W_i^Q$</span> and <span class=""math-container"">$W_i^K$</span> should be separate because of this and that?</p>
","transformer"
"25173","How to handle long sequences with transformers?","2020-12-13 20:25:08","","2","341","<deep-learning><natural-language-processing><transformer><time-series><attention>","<p>I have a time series sequence with 10 million steps. In step <span class=""math-container"">$t$</span>, I have a 400 dimensional feature vector <span class=""math-container"">$X_t$</span> and a scalar value <span class=""math-container"">$y_t$</span> which I want to predict during inference time and I know during the train time. I want to use a <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">transformer model</a>. I have 2 questions:</p>
<ol>
<li>If I want to embed the 400 dimensional input feature vector into another space before feeding into the transformer, what are the pros and cons of using let's say 1024 and 64 for the embedding space dimension? Should I use a dimension more than 400 or less?</li>
<li>When doing <em>position embedding</em>, I cannot use a maximum position length of 10 million as that blows up the memory. What is the best strategy here if I want to use maximum position length of 512? Should I chunk the 10 million steps into blocks of size 512 and feed each block separately into the transformer? If so, how can I connect the subsequent blocks to take full advantage of parallelization while keeping the original chronological structure of the sequence data?</li>
</ol>
","transformer"
"25148","What is different in each head of a multi-head attention mechanism?","2020-12-12 22:22:23","25149","12","8513","<neural-networks><natural-language-processing><papers><transformer><attention>","<p>I have a difficult time understanding the &quot;multi-head&quot; notion in the original <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""noreferrer"">transformer paper</a>. What makes the learning in each head unique? Why doesn't the neural network learn the same set of parameters for each attention head? Is it because we break <em>query, key</em> and <em>value</em> vectors into smaller dimensions and feed each portion to a different head?</p>
","transformer"
"25097","Can we use transformers for audio classification tasks?","2020-12-10 12:45:18","25116","1","1115","<reference-request><applications><transformer><sequence-modeling><audio-processing>","<p>Since transformers are good at processing sequential data, can we also use them for audio classification problems (same as RNNs)?</p>
","transformer"
"25096","When we translate a text from one language to another, how does the frequency of various POS tags change?","2020-12-10 10:14:32","","1","11","<natural-language-processing><transformer><language-model><pos-tagging>","<p>When we translate a text from one language to another, how does the frequency of various POS tags change?</p>
<p>So, let's say we have a text in English, with 10% nouns, 20% adjectives, 15% adverbs, 25% verbs, etc., which we now translate to German, French, or Hindi. Can we say that in these other languages the POS tag frequency will remain the same as earlier?</p>
","transformer"
"25065","Why does the loss stops reducing after a point in this Transformer Model?","2020-12-08 11:08:19","","2","45","<machine-learning><deep-learning><tensorflow><keras><transformer>","<h2>Context</h2>
<p>I was making a Transformer Model to convert English Sentences to German Sentences. But the loss stops reducing after some time.</p>
<h2>Code</h2>
<pre><code>import string
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, LSTM, RepeatVector, Dense, Dropout, BatchNormalization, TimeDistributed, AdditiveAttention, Input, Concatenate, Flatten
from tensorflow.keras.layers import Activation, LayerNormalization, GRU, GlobalAveragePooling1D, Attention
from tensorflow.keras.optimizers import Adam
from tensorflow.nn import tanh, softmax
import time
from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy
from numpy import array
from tensorflow.keras.utils import plot_model
from sklearn.utils import shuffle
import time
import tensorflow as tf
from numpy import array
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.datasets.imdb import load_data

def load_data(filename):
    file = open(filename, 'r')
    text = file.read()
    file.close()
    return text

def to_lines(text):
    return text.split('\n')

def clean_data(pair):
    pair = 'start_seq_ ' + pair + ' end_seq_'

    re_print = re.compile('[^%s]' % re.escape(string.printable))
    table = str.maketrans('', '', string.punctuation)
    tokens = [token.translate(table) for token in pair.split()]
    tokens = [token.lower() for token in tokens]
    tokens = [re_print.sub('', token) for token in tokens]
    tokens = [token for token in tokens if token.isalpha()]
    return tokens

lines = to_lines(load_data('/content/drive/My Drive/spa.txt'))

english_pair = []
german_pair = []
language = []
for line in lines:
    if line != '':
        pairs = line.split('\t')
        english_pair.append(clean_data(pairs[0]))
        german_pair.append(clean_data(pairs[1]))

        language.append(clean_data(pairs[0]))
        language.append(clean_data(pairs[1]))

english_pair = array(english_pair)
german_pair = array(german_pair)
language = array(language)

def create_tokenizer(data):
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts(data)
    return tokenizer

def max_len(lines):
    length = []
    for line in lines:
        length.append(len(line))
    return max(length)

tokenizer = create_tokenizer(language)

vocab_size = len(tokenizer.word_index) + 1

max_len = max_len(language)

def create_sequences(sequences, max_len):
    sequences = tokenizer.texts_to_sequences(sequences)
    sequences = pad_sequences(sequences, maxlen=max_len, padding='post')
    return sequences

X1 = create_sequences(english_pair, max_len)
X2 = create_sequences(german_pair, max_len)
Y = create_sequences(german_pair, max_len)


X1, X2, Y = shuffle(X1, X2, Y)

training_samples = int(X1.shape[0] * 1.0)

train_x1, train_x2, train_y = X1[:training_samples], X2[:training_samples], Y[:training_samples]
test_x1, test_x2, test_y = X1[training_samples:], X2[training_samples:], Y[training_samples:]

train_x2 = train_x2[:, :-1]
test_x2 = test_x2[:, :-1]
train_y = train_y[:, 1:].reshape(-1, max_len-1)
test_y = test_y[:, 1:].reshape(-1, max_len-1)

train_x2 = pad_sequences(train_x2, maxlen=max_len, padding='post')
test_x2 = pad_sequences(test_x2, maxlen=max_len, padding='post')

train_y = pad_sequences(train_y, maxlen=max_len, padding='post')
test_y = pad_sequences(test_y, maxlen=max_len, padding='post')
</code></pre>
<p>All code above just prepares the Data, so if you want you can skip that part.
Code After this starts implementing the Transformer Model.</p>
<pre><code>class EncoderBlock(tf.keras.layers.Layer):
    def __init__(self, mid_ffn_dim, embed_dim, num_heads, max_len, batch_size):
        super(EncoderBlock, self).__init__()
        # Variables
        self.batch_size = batch_size
        self.max_len = max_len
        self.mid_ffn_dim = mid_ffn_dim
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.attention_vector_len = self.embed_dim // self.num_heads
        if self.embed_dim % self.num_heads != 0:
            raise ValueError('I am Batman!')

        # Trainable Layers
        self.mid_ffn = Dense(self.mid_ffn_dim, activation='relu')
        self.final_ffn = Dense(self.embed_dim)

        self.layer_norm1 = LayerNormalization(epsilon=1e-6)
        self.layer_norm2 = LayerNormalization(epsilon=1e-6)

        self.combine_heads = Dense(self.embed_dim)

        self.query_dense = Dense(self.embed_dim)
        self.key_dense = Dense(self.embed_dim)
        self.value_dense = Dense(self.embed_dim)

    def separate_heads(self, x):
        x = tf.reshape(x, (-1, self.max_len, self.num_heads, self.attention_vector_len))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def compute_self_attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output

    def self_attention_layer(self, x):
        query = self.query_dense(x)
        key = self.key_dense(x)
        value = self.value_dense(x)

        query_heads = self.separate_heads(query)    
        key_heads = self.separate_heads(key)
        value_heads = self.separate_heads(value)

        attention = self.compute_self_attention(query_heads, key_heads, value_heads)

        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) 
        attention = tf.reshape(attention, (-1, self.max_len, self.embed_dim))

        output = self.combine_heads(attention)
        return output
        
    def get_output(self, x):
        attn_output = self.self_attention_layer(x)
        out1 = self.layer_norm1(x + attn_output)

        ffn_output = self.final_ffn(self.mid_ffn(out1))

        encoder_output = self.layer_norm2(out1 + ffn_output)
        return encoder_output

class DecoderBlock(tf.keras.layers.Layer):
    def __init__(self, mid_ffn_dim, embed_dim, num_heads, max_len, batch_size):
        super(DecoderBlock, self).__init__()
        # Variables
        self.batch_size = batch_size
        self.max_len = max_len
        self.mid_ffn_dim = mid_ffn_dim
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.attention_vector_len = self.embed_dim // self.num_heads
        if self.embed_dim % self.num_heads != 0:
            raise ValueError('I am Batman!')

        # Trainable Layers

        self.query_dense1 = Dense(self.embed_dim, name='query_dense1')
        self.key_dense1 = Dense(self.embed_dim, name='key_dense1')
        self.value_dense1 = Dense(self.embed_dim, name='value_dense1')

        self.mid_ffn = Dense(self.mid_ffn_dim, activation='relu', name='dec_mid_ffn')
        self.final_ffn = Dense(self.embed_dim, name='dec_final_ffn')

        self.layer_norm1 = LayerNormalization(epsilon=1e-6)
        self.layer_norm2 = LayerNormalization(epsilon=1e-6)
        self.layer_norm3 = LayerNormalization(epsilon=1e-6)

        self.combine_heads = Dense(self.embed_dim, name='dec_combine_heads')

        self.query_dense2 = Dense(self.embed_dim, name='query_dense2')
        self.key_dense2 = Dense(self.embed_dim, name='key_dense2')
        self.value_dense2 = Dense(self.embed_dim, name='value_dense2')

    def separate_heads(self, x):
        x = tf.reshape(x, (-1, self.max_len, self.num_heads, self.attention_vector_len))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def compute_self_attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output

    def masking(self, x):
        b = []
        for batch in range(x.shape[0]):
            bat = []
            for head in range(x.shape[1]):
                headd = []
                for word in range(x.shape[2]):
                    current_word = []
                    for represented_in in range(x.shape[3]):
                        if represented_in &gt; word:
                          current_word.append(np.NINF)
                        else:
                          current_word.append(0)
                    headd.append(current_word)
                bat.append(headd)
            b.append(bat)
        return b

    def compute_masked_self_attention(self, query, key, value):
        score = tf.matmul(query, key, transpose_b=True)
        score = score + self.masking(score)
        score = tf.convert_to_tensor(score)
                
        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)
        scaled_score = score / tf.math.sqrt(dim_key)
        weights = tf.nn.softmax(scaled_score, axis=-1)
        output = tf.matmul(weights, value)
        return output

    def masked_self_attention_layer(self, x):
        query = self.query_dense1(x)
        key = self.key_dense1(x)
        value = self.value_dense1(x)

        query_heads = self.separate_heads(query)    
        key_heads = self.separate_heads(key)
        value_heads = self.separate_heads(value)

        attention = self.compute_masked_self_attention(query_heads, key_heads, value_heads)

        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) 
        attention = tf.reshape(attention, (-1, self.max_len, self.embed_dim))

        output = self.combine_heads(attention)
        return output

    def second_attention_layer(self, x, encoder_output):
        query = self.query_dense2(x)
        key = self.key_dense2(encoder_output)
        value = self.value_dense2(encoder_output)

        query_heads = self.separate_heads(query)    
        key_heads = self.separate_heads(key)
        value_heads = self.separate_heads(value)

        attention = self.compute_self_attention(query_heads, key_heads, value_heads)

        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) 
        attention = tf.reshape(attention, (-1, self.max_len, self.embed_dim))

        output = self.combine_heads(attention)
        return output
      
    def get_output(self, x, encoder_output):
        masked_attn_output = self.masked_self_attention_layer(x)
        out1 = self.layer_norm1(x + masked_attn_output)

        mutli_head_attn_output = self.second_attention_layer(out1, encoder_output)
        out2 = self.layer_norm2(out1 + mutli_head_attn_output)

        ffn_output = self.final_ffn(self.mid_ffn(out2))
        decoder_output = self.layer_norm3(out2 + ffn_output)
        return decoder_output

embed_dim = 512
mid_ffn_dim = 1024

num_heads = 8
max_len = max_len
batch_size = 32

encoder_block1 = EncoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)
encoder_block2 = EncoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)
encoder_block3 = EncoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)

decoder_block1 = DecoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)
decoder_block2 = DecoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)
decoder_block3 = DecoderBlock(mid_ffn_dim, embed_dim, num_heads, max_len, batch_size)

# Define Loss and Optimizer
loss_object = SparseCategoricalCrossentropy()
optimizer = Adam()

embedding = Embedding(vocab_size, embed_dim, name='embedding')
position_embedding = Embedding(vocab_size, embed_dim)

final_transformer_layer = Dense(vocab_size, activation='softmax')

def positional_embedding(x):
    positions = tf.range(start=0, limit=max_len, delta=1)
    positions = position_embedding(positions)
    return x + positions

def train_step(english_sent, german_sent, german_trgt):
    with tf.GradientTape() as tape:
        english_embedded = embedding(english_sent)
        german_embedded = embedding(german_sent)

        english_positioned = positional_embedding(english_embedded)
        german_positioned = positional_embedding(german_embedded)

        # Encoders
        encoder_output = encoder_block1.get_output(english_positioned)
        encoder_output = encoder_block2.get_output(encoder_output)
        encoder_output = encoder_block3.get_output(encoder_output)

        # Decoders
        decoder_output = decoder_block1.get_output(german_positioned, encoder_output)
        decoder_output = decoder_block2.get_output(decoder_output, encoder_output)
        decoder_output = decoder_block3.get_output(decoder_output, encoder_output)

        # Final Output
        transformer_output = final_transformer_layer(decoder_output)

        # Compute Loss
        loss = loss_object(german_trgt, transformer_output)

    variables = embedding.trainable_variables + position_embedding.trainable_variables + encoder_block1.trainable_variables + encoder_block2.trainable_variables
    variables += encoder_block3.trainable_variables + decoder_block1.trainable_variables + decoder_block2.trainable_variables + decoder_block3.trainable_variables
    variables += final_transformer_layer.trainable_variables

    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))

    return float(loss)

def train(epochs=10):
    batch_per_epoch = int(train_x1.shape[0] / batch_size)
    for epoch in range(epochs):
        for i in range(batch_per_epoch):
            english_sent_x = train_x1[i*batch_size : (i*batch_size)+batch_size].reshape(batch_size, max_len)
            german_sent_x = train_x2[i*batch_size : (i*batch_size)+batch_size].reshape(batch_size, max_len)
            german_sent_y = train_y[i*batch_size : (i*batch_size)+batch_size].reshape(batch_size, max_len, 1)

            loss = train_step(english_sent_x, german_sent_x, german_sent_y)

            print('Epoch ', epoch, 'Batch ', i, '/', batch_per_epoch, 'Loss ', loss)

train()

</code></pre>
<p>And the Code is done! But the loss stops reducing at around value of 1.2 after some time. Why is this happening?</p>
<h2>Maybe Important</h2>
<p>I tried debugging the model, by passing random input integers, and the model was still performing the same way it did when I gave real Sentences as input.</p>
<p>When I tried training the model with just 1 training sample, the loss stops reducing at around 0.2. When I train it with 2 training samples, the result was the approximately the same as when I trained it with 1 training sample.</p>
<p>When I stopped shuffling the dataset the loss gone till around 0.7 and again stopped learning.</p>
<p>I tried simplifying the model by removing some encoder and decoder blocks but the results were approximately the same. I even tried making the model more complex but the results were again approximately the same.</p>
","transformer"
"25055","What is the gradient of an attention unit?","2020-12-08 00:49:59","","3","3012","<neural-networks><natural-language-processing><gradient-descent><transformer><attention>","<p>The paper <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a> describes the Transformer architecture, which describes attention as a function of the queries <span class=""math-container"">$Q = x W^Q$</span>, keys <span class=""math-container"">$K = x W^K$</span>, and values <span class=""math-container"">$V = x W^V$</span>:</p>
<p><span class=""math-container"">$\text{Attention(Q, K, V)} = \text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right) V \\
= \text{softmax}\left( \frac{x W^Q (W^K)^T x}{\sqrt{d_k}} \right) x W^V$</span></p>
<p>In the Transformer, there are 3 different flavors of attention:</p>
<ol>
<li><strong>Self-attention in the Encoder</strong>, where the queries, keys, and values all come from the input to the Encoder.</li>
<li><strong>Encoder-Decoder attention in the Decoder</strong>, where the queries come from the input to the Decoder, and the keys and values come from the output of the Encoder</li>
<li><strong>Masked self-attention in the Decoder</strong>, where the queries, keys and values all come from the input to the Decoder, and, for each token, the <span class=""math-container"">$\text{softmax}\left( \frac{QK^T}{\sqrt{d_k}} \right)$</span> operation is masked out (zero'd out) for all tokens to the right of that token (to prevent look-ahead, which is cheating during training).</li>
</ol>
<p>What is the gradient (i.e. the partial derivatives of the loss function w.r.t. <span class=""math-container"">$x$</span>, <span class=""math-container"">$W^Q$</span>, <span class=""math-container"">$W^K$</span>, <span class=""math-container"">$W^V$</span>, and any bias term(s)) of each of these attention units? I am having a difficult time wrapping my head around derivating a gradient equation because I'm not sure how the softmax function interacts with the partial derivatives, and also, for the Encoder-Decoder attention in the Decoder, I'm not clear how to incorporate the encoder output into the equation.</p>
","transformer"
"25053","What is the cost function of a transformer?","2020-12-07 23:18:06","25094","8","6247","<neural-networks><natural-language-processing><objective-functions><transformer><attention>","<p>The paper <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""noreferrer"">Attention Is All You Need</a> describes the transformer architecture that has an encoder and a decoder.</p>
<p>However, I wasn't clear on what the cost function to minimize is for such an architecture.</p>
<p>Consider a translation task, for example, where give an English sentence <span class=""math-container"">$x_{english} = [x_0, x_1, x_2, \dots, x_m]$</span>, the transformer decodes the sentence into a French sentence <span class=""math-container"">$x_{french}' = [x_0', x_1', \dots, x_n']$</span>. Let's say the true label is <span class=""math-container"">$y_{french} = [y_0, y_1, \dots, y_p]$</span>.</p>
<p>What is the object function of the transformer? Is it the MSE between <span class=""math-container"">$x_{french}'$</span> and <span class=""math-container"">$y_{french}$</span>? And does it have any weight regularization terms?</p>
","transformer"
"25041","Is the Decoder mask (triangular mask) applied only in the first decoder block, or to all blocks in Decoder?","2020-12-07 13:46:00","25054","2","487","<transformer><attention>","<p>The Decoder mask, also called &quot;look-ahead mask&quot;, is applied in the Decoder side to prevent it from attending future tokens. Something like this:</p>
<pre><code>[0, 1, 1, 1, 1]
[0, 0, 1, 1, 1]
[0, 0, 0, 1, 1]
[0, 0, 0, 0, 1]
[0, 0, 0, 0, 0]
</code></pre>
<p>But is this mask applied only in the first Decoder block? Or to all its blocks?</p>
","transformer"
"25015","Transformers: how to get the output (keys and values) of the encoder?","2020-12-06 09:23:08","25030","2","726","<natural-language-processing><transformer><bert><attention>","<p>I was reading the paper <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a>.</p>
<p>It seems like the last step of the encoder is a LayerNorm(relu(WX + B) + X), i.e. an add + normalization. This should result in a <span class=""math-container"">$n$</span> x <span class=""math-container"">$d^{model}$</span> matrix, where <span class=""math-container"">$n$</span> is the length of the input to the encoder.</p>
<p>How do we convert this <span class=""math-container"">$n$</span> x <span class=""math-container"">$d^{model}$</span> matrix into the keys <span class=""math-container"">$K$</span> and values <span class=""math-container"">$V$</span> that are fed into the decoder's encoder-decoder attention step?</p>
<p>Note that, if <span class=""math-container"">$h$</span> is the number of attention heads in the model, the dimensions of <span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> should both be <span class=""math-container"">$n$</span> x <span class=""math-container"">$\frac{d^{model}}{h}$</span>. For <span class=""math-container"">$h=8$</span>, this means we need a <span class=""math-container"">$n$</span> x <span class=""math-container"">$\frac{d^{model}}{4}$</span> matrix.</p>
<p>Do we simply add an extra linear layer that learns a <span class=""math-container"">$d^{model}$</span> x <span class=""math-container"">$\frac{d^{model}}{4}$</span> weight matrix?</p>
<p>Or do we use the output of the final Add &amp; Norm layer, and simply use the first <span class=""math-container"">$\frac{d^{model}}{4}$</span> columns of the matrix and discard the rest?</p>
","transformer"
"25013","Transformers: how does the decoder final layer output the desired token?","2020-12-06 08:25:45","25026","5","1073","<natural-language-processing><transformer><bert><attention>","<p>In the paper <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a>, this section confuses me:</p>
<blockquote>
<p>In our model, we share the same weight matrix between the two embedding layers [in the encoding section] and the pre-softmax linear transformation [output of the decoding section]</p>
</blockquote>
<p>Shouldn't the weights be different, and not the same? Here is my understanding:</p>
<p>For simplicity, let us use the English-to-French translation task where we have <span class=""math-container"">$n^e$</span> number of English words in our dictionary and <span class=""math-container"">$n^f$</span> number of French words.</p>
<ul>
<li><p>In the encoding layer, the input tokens are <span class=""math-container"">$1$</span> x <span class=""math-container"">$n^e$</span> one-hot vectors, and are embedded with a <span class=""math-container"">$n^e$</span> x <span class=""math-container"">$d^{model}$</span> learned embedding matrix.</p>
</li>
<li><p>In the output of the decoding layer, the final step is a linear transformation with weight matrix <span class=""math-container"">$d^{model}$</span> x <span class=""math-container"">$n^f$</span>, and then applying softmax to get the probability of each french word, and choosing the french word with the highest probability.</p>
</li>
</ul>
<p>How is it that the <span class=""math-container"">$n^e$</span> x <span class=""math-container"">$n^{model}$</span> input embedding matrix share the same weights as the <span class=""math-container"">$d^{model}$</span> x <span class=""math-container"">$n^f$</span> decoding output linear matrix? To me, it seems more natural for both these matrices to be learned independently from each other via the training data, right? Or am I misinterpreting the paper?</p>
","transformer"
"24831","What is the difference between the positional encoding techniques of the Transformer and GPT?","2020-11-23 22:03:11","34346","8","3658","<comparison><transformer><gpt><positional-encoding>","<p>I know the original Transformer and the GPT (1-3) use two slightly different <strong>positional encoding</strong> techniques.</p>
<p>More specifically, in GPT they say positional encoding is <em>learned</em>. What does that mean? OpenAI's papers don't go into detail very much.</p>
<p>How do they really differ, mathematically speaking?</p>
","transformer"
"24643","Why are ""Transformers"" called this way?","2020-11-16 12:07:39","","6","228","<neural-networks><terminology><transformer><attention>","<p>What is the reason behind the name &quot;Transformers&quot;, for Multi Head Self-Attention-based neural networks from <a href=""https://arxiv.org/abs/1706.03762"" rel=""noreferrer"">Attention is All You Need</a>?</p>
<p>I have been googling this question for a long time, and nowhere I can find any explanation.</p>
","transformer"
"24409","Is there a pretrained (NLP) transformer that uses subword n-gram embeddings for tokenization like fasttext?","2020-11-03 22:44:04","","4","447","<transformer><word-embedding><bert>","<p>I know that several tokenization methods that are used for tranformer models like WordPiece for Bert and BPE for Roberta and others. What I was wondering if there is also a transformer which uses a method for tokenization similarly to the embeddings that are used in the fasttext library, so based on the summations of embeddings for the n-grams the words are made of.</p>
<p>To me it seems weird that this way of creating word(piece) embeddings that can function as the input of a transformer isn't used in these new transformer architectures. Is there a reason why this is not tried yet? Or is this question just an result of my inability to find the right papers/repo's.</p>
","transformer"
"23993","When do the ensemble methods beat neural networks?","2020-10-09 21:14:47","","4","181","<convolutional-neural-networks><transformer><ensemble-learning><random-forests><gradient-boosting>","<p>In many applications and domains, computer vision, natural language processing, image segmentation, and many other tasks, neural networks (with a certain architecture) are considered to be by far the most powerful machine learning models.</p>
<p>Nevertheless, algorithms, based on different approaches, such as ensemble models, like <em>random forests</em> and <em>gradient boosting</em>, are not completely abandoned, and actively developed and maintained by some people.</p>
<p>Do I correctly understand that the neural networks, despite being very flexible and universal approximators, for a certain kind of tasks, regardless of the choice of the architecture, are not the optimal models?</p>
<p>For the tasks in computer vision, the core feature, which makes CNNs superior, is the <em>translational invariance</em> and the encoded ability to capture the proximity properties of an image or some sequential data. And the more recent <em>transformer</em> models have the ability to choose which of the neighboring data properties is more important for its output.</p>
<p>But let's say I have a dataset, without a certain structure and patterns, some number of numerical columns, a lot of categorical columns, and in the feature space (for classification task) the classes are separated by some nonlinear hypersurface, would the ensemble models be the optimal choice in terms of performance and computational time?</p>
<p>In this case, I do not see a way to exploit CNNs or attention-based neural networks. The only thing that comes to my head, in this case, is the ordinary MLP. It seems that, on the one hand, it would take significantly more time to train the weights than the trees from the ensemble. On the other hand, both kinds of models work without putting prior knowledge to data and assumptions on its structure. So, given enough amount of time, it should give a comparable quality.</p>
<p>Or can there be some reasoning that neural network is sometimes bound to give rather a poor quality?</p>
","transformer"
"23898","Any comparison between transformer and RNN+Attention on the same dataset?","2020-10-04 23:48:19","23904","3","2379","<natural-language-processing><recurrent-neural-networks><long-short-term-memory><transformer><attention>","<p>I am wondering what is believed to be the reason for superiority of transformer?</p>
<p>I see that some people believe because of the attention mechanism used, it’s able to capture much longer dependencies. However, as far as I know, you can use attention also with RNN  architectures as in the famous paper attention is introduced(<a href=""https://arxiv.org/pdf/1409.0473"" rel=""nofollow noreferrer"">here</a>)).</p>
<p>I am wondering whether the only reason for the superiority of transformers is because they can be highly parallelized and trained on much more data?</p>
<p>Is there any experiment comparing transformers and RNN+attention trained on the exact same amount of data comparing the two?</p>
","transformer"
"23889","What is the purpose of Decoder mask (triangular mask) in Transformer?","2020-10-03 20:18:01","23923","11","10129","<natural-language-processing><transformer><attention>","<p>I'm trying to implement transformer model using <a href=""https://www.tensorflow.org/tutorials/text/transformer#create_the_transformer"" rel=""noreferrer"">this tutorial</a>.  In the decoder block of the Transformer model, a mask is passed to &quot;<strong>pad and mask future tokens in the input received by the decoder</strong>&quot;. This mask is added to attention weights.</p>
<pre><code>import tensorflow as tf

def create_look_ahead_mask(size):
    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)
    return mask
</code></pre>
<p>Now my question is, how is doing this step (adding mask to the attention weights) equivalent to revealing the words to model one by one? I simply can't grasp the intuition of it's role. Most tutorials won't even mention this step like it's very obvious. Please help me understand. Thanks.</p>
","transformer"
"23809","Transformer Language Model generating meaningless text","2020-09-28 18:48:17","","0","201","<natural-language-processing><pytorch><transformer><text-generation>","<p>I currently learning on Transformers, so check my understanding I tried implementing a small transformer-based language model and compare it to RNN based language model. Here's the code for transformer. I'm using PyTorch inbuilt layer for Transformer Encoder</p>
<pre><code>class TransformerLM_1(nn.Module):

    def __init__(self, head, vocab_size, embedding_size, dropout = 0.1, device = 'cpu', 
                 pad_idx = 0, start_idx = 1, end_idx = 2, unk_idx = 3):
      
        super(TransformerLM_1, self).__init__()
      
        self.head = head
        self.embedding_size = embedding_size
        self.vocab_size = vocab_size
        self.device = device
        self.embed = WordEmbedding(self.vocab_size, self.embedding_size, pad_idx)
        self.postional_encoding = PostionalEncoding(embedding_size, device)
        self.decoder = nn.TransformerEncoderLayer(self.embedding_size, self.head)
        self.out_linear = nn.Linear(self.embedding_size, vocab_size)
        self.dropout = dropout
        self.pad_idx = pad_idx
        self.start_idx = start_idx
        self.end_idx = end_idx
        self.unk_idx = unk_idx
        self.device = device

    
    def make_src_mask(self, src_sz):
        mask = (torch.triu(torch.ones(src_sz, src_sz)) == 1).transpose(0, 1)
        mask = mask.float().masked_fill(mask == 0, 10e-20).masked_fill(mask == 1, float(0.0))
        mask = mask.to(self.device)
        return mask

    def forward(self, x):
        dec_in = x.clone()[:, :-1]
        src_mask = self.make_src_mask(dec_in.size()[1])
        src = self.embed(dec_in)
        src = self.postional_encoding(src) 
        src = src.transpose(0,1)
        transformer_out = self.decoder(src, src_mask)
        out = self.out_linear(transformer_out)
        return out
</code></pre>
<p>I'm using teacher forcing to make it converge faster. From what I saw from the results, the text generated by the RNN model is better than transformer's.</p>
<p>Here is sample generated text with the expected</p>
<pre><code>Expected: you had to have been blind not to see the scenario there for what it was and is and will continue to be for months and even years a part of south carolina that has sustained a blow that the red cross expects will cost that organization alone some <span class=""math-container"">$ n million &lt;eos&gt; 
Predicted: some &lt;unk&gt; been the been &lt;unk&gt; not be $</span> the total has was the may has &lt;unk&gt; the that that be to the &lt;unk&gt; the 

Expected: citicorp and chase are attempting to put together a new lower bid &lt;eos&gt; 
Predicted: a are &lt;unk&gt; carries n't to the together with &lt;unk&gt; jersey than 

Expected: it ' s amazing the amount of money that goes up their nose out to the dog track or to the tables in las vegas mr . katz says &lt;eos&gt; 
Predicted: &lt;unk&gt; ' s &lt;unk&gt; comeback money of the in mr to their &lt;unk&gt; and of &lt;unk&gt; &lt;unk&gt; or or &lt;unk&gt; the money 

Expected: moreover while asian and middle eastern investors &lt;unk&gt; gold and help &lt;unk&gt; its price silver does n't have the same &lt;unk&gt; dealers say &lt;eos&gt; 
Predicted: the production the routes &lt;unk&gt; of its 

Expected: a board of control spokesman said the board had not seen the claim and declined to comment &lt;eos&gt; 
Predicted: the board said declined of said 

Expected: property capital trust said it dropped its plan to liquidate because it was n't able to realize the value it had expected &lt;eos&gt; 
Predicted: the claims markets said its was n &lt;unk&gt; to sell insolvent of was n't disclosed to sell its plan 

Expected: similarly honda motor co . ' s sales are so brisk that workers &lt;unk&gt; they have n't had a saturday off in years despite the government ' s encouragement of more leisure activity &lt;eos&gt; 
Predicted: the honda ' credit . s s &lt;unk&gt; 

Expected: we expect a big market in the future so in the long term it will be profitable &lt;eos&gt; 
Predicted: it can it &lt;unk&gt; board 

Expected: u . k . composite or &lt;unk&gt; insurers which some equity analysts said might be heavily hit by the earthquake disaster helped support the london market by showing only narrow losses in early trading &lt;eos&gt; 
Predicted: the . s . s trading sell said which &lt;unk&gt; traders market said the be able in the the earthquake 

Expected: this will require us to define and &lt;unk&gt; what is necessary or appropriate care &lt;eos&gt; 
Predicted: &lt;unk&gt; is be the $ &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; &lt;unk&gt; is the to &lt;unk&gt; and or 
</code></pre>
<p>As you can see Transformer fails to grasp grammar compared to RNN. Is there anything wrong with my understanding?</p>
<p>EDIT</p>
<p>This is one example that caught my eye</p>
<pre><code>Expected: also the big board met with angry stock specialists &lt;eos&gt; 
Predicted: also met specialists board met the stock big with after 
</code></pre>
<p>Most of the words predicted have is from the expected but in a different order. I have read that transformers are permutation invariant which is the reason why we include positional encoding with the word embedding.</p>
","transformer"
"23803","How are weight matrices in attention learned?","2020-09-28 10:05:19","","2","281","<deep-learning><natural-language-processing><backpropagation><transformer><attention>","<p>I have been looking into transformers lately and have been reading tons of tutorials. All of them address the intuition behind attention, which I understand. However, they treat learning the weight matrices (for query, key, and value) as it is the most trivial thing.</p>
<p>So, how are these weight matrices learned? Is the error just backpropagated, and the weights are updated accordingly?</p>
","transformer"
"23703","How to implement or avoid masking for transformer?","2020-09-22 19:02:56","","1","95","<convolutional-neural-networks><natural-language-processing><python><pytorch><transformer>","<p>When it comes to using Transformers for image captioning is there any reason to use masking?</p>
<p>I currently have a resnet101 encoder and am trying to use the features as the input for a transformer model in order to generate a caption for the image, is there any need to use masking? and what would I mask if I did need to?</p>
<p>Any help would be much appreciated</p>
<p>Thanks in advance.</p>
","transformer"
"23611","Are there transformer-based architectures that can produce fixed-length vector encodings given arbitrary-length text documents?","2020-09-15 16:16:37","","8","932","<natural-language-processing><reference-request><autoencoders><transformer><bert>","<p><a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">BERT</a> encodes a piece of text such that each token (usually words) in the input text map to a vector in the encoding of the text. However, this makes the length of the encoding vary as a function of the input length of the text, which makes it more cumbersome to use as input to downstream neural networks that take only fixed-size inputs.</p>
<p>Are there any transformer-based neural network architectures that can encode a piece of text into a fixed-size feature vector more suitable for downstream tasks?</p>
<p><strong>Edit:</strong> To illustrate my question, I’m wondering whether there is some framework that allows the input to be either a sentence, a paragraph, an article, or a book, and produce an output encoding on the same, fixed-sized format for all of them.</p>
","transformer"
"23517","BERT: After pretraining 880000 step, why fine-tune not work?","2020-09-11 06:22:39","23519","0","67","<transformer><bert><transfer-learning><pretrained-models><fine-tuning>","<p>I am using pretraining code from <a href=""https://github.com/NVIDIA/DeepLearningExamples"" rel=""nofollow noreferrer"">https://github.com/NVIDIA/DeepLearningExamples</a></p>
<p>Pretrain parameters:</p>
<pre><code> 15:47:02,534: INFO tensorflow 140678508230464   init_checkpoint: bertbase3layer-extract-from-google
 15:47:02,534: INFO tensorflow 140678508230464   optimizer_type: lamb
 15:47:02,534: INFO tensorflow 140678508230464   max_seq_length: 64
 15:47:02,534: INFO tensorflow 140678508230464   max_predictions_per_seq: 5
 15:47:02,534: INFO tensorflow 140678508230464   do_train: True
 15:47:02,535: INFO tensorflow 140678508230464   do_eval: False
 15:47:02,535: INFO tensorflow 140678508230464   train_batch_size: 32
 15:47:02,535: INFO tensorflow 140678508230464   eval_batch_size: 8
 15:47:02,535: INFO tensorflow 140678508230464   learning_rate: 5e-05
 15:47:02,535: INFO tensorflow 140678508230464   num_train_steps: 10000000
 15:47:02,535: INFO tensorflow 140678508230464   num_warmup_steps: 10000
 15:47:02,535: INFO tensorflow 140678508230464   save_checkpoints_steps: 1000
 15:47:02,535: INFO tensorflow 140678508230464   display_loss_steps: 10
 15:47:02,535: INFO tensorflow 140678508230464   iterations_per_loop: 1000
 15:47:02,535: INFO tensorflow 140678508230464   max_eval_steps: 100
 15:47:02,535: INFO tensorflow 140678508230464   num_accumulation_steps: 1
 15:47:02,535: INFO tensorflow 140678508230464   allreduce_post_accumulation: False
 15:47:02,535: INFO tensorflow 140678508230464   verbose_logging: False
 15:47:02,535: INFO tensorflow 140678508230464   horovod: True
 15:47:02,536: INFO tensorflow 140678508230464   report_loss: True
 15:47:02,536: INFO tensorflow 140678508230464   manual_fp16: False
 15:47:02,536: INFO tensorflow 140678508230464   amp: False
 15:47:02,536: INFO tensorflow 140678508230464   use_xla: True
 15:47:02,536: INFO tensorflow 140678508230464   init_loss_scale: 4294967296
 15:47:02,536: INFO tensorflow 140678508230464   ?: False
 15:47:02,536: INFO tensorflow 140678508230464   help: False
 15:47:02,536: INFO tensorflow 140678508230464   helpshort: False
 15:47:02,536: INFO tensorflow 140678508230464   helpfull: False
 15:47:02,536: INFO tensorflow 140678508230464   helpxml: False
 15:47:02,536: INFO tensorflow 140678508230464 **************************
</code></pre>
<p>Pretrain loss: (I remove nsp_loss)</p>
<pre><code>{'throughput_train': 1196.9646684552622, 'mlm_loss': 0.9837073683738708, 'nsp_loss': 0.0, 'total_loss': 0.9837073683738708, 'avg_loss_step': 1.200513333082199, 'learning_rate': '0.00038143058'}
{'throughput_train': 1230.5063662500734, 'mlm_loss': 1.3001925945281982, 'nsp_loss': 0.0, 'total_loss': 1.3001925945281982, 'avg_loss_step': 1.299936044216156, 'learning_rate': '0.00038143038'}
{'throughput_train': 1236.4348949169155, 'mlm_loss': 1.473339319229126, 'nsp_loss': 0.0, 'total_loss': 1.473339319229126, 'avg_loss_step': 1.2444063007831574, 'learning_rate': '0.00038143017'}
{'throughput_train': 1221.2668264552692, 'mlm_loss': 0.9924975633621216, 'nsp_loss': 0.0, 'total_loss': 0.9924975633621216, 'avg_loss_step': 1.1603020071983337, 'learning_rate': '0.00038142994'}
</code></pre>
<p>Fine-tune code:</p>
<pre><code>self.train_op = tf.train.AdamOptimizer(0.00001).minimize(self.loss, global_step=self.global_step)
</code></pre>
<p>Fine-tune accuracy: (restore from my ckpt pretrained from <a href=""https://github.com/NVIDIA/DeepLearningExamples"" rel=""nofollow noreferrer"">https://github.com/NVIDIA/DeepLearningExamples</a>)</p>
<pre><code>epoch 1:
training step 895429, loss 4.98, acc 0.079
dev loss 4.853, acc 0.092

epoch 2:
training step 895429, loss 4.97, acc 0.080
dev loss 4.823, acc 0.092

epoch 3:
training step 895429, loss 4.96, acc 0.081
dev loss 4.849, acc 0.092

epoch 4:
training step 895429, loss 4.95, acc 0.082
dev loss 4.843, acc 0.092
</code></pre>
<p>Without restore the pretrained ckpt:</p>
<pre><code>epoch 1:
training step 10429, loss 2.48, acc 0.606
dev loss 1.604, acc 0.8036
</code></pre>
<p>Restore the google's BERT-Base pretrained ckpt. Or restore from a pretrained ckpt pretrained from <a href=""https://github.com/guotong1988/BERT-GPU"" rel=""nofollow noreferrer"">https://github.com/guotong1988/BERT-GPU</a></p>
<pre><code>epoch 1:
training loss 1.89, acc 0.761
dev loss 1.351, acc 0.869
</code></pre>
","transformer"
"23332","What is the weight matrix in self-attention?","2020-08-29 16:36:47","","5","2471","<neural-networks><transformer><attention>","<p>I've been looking into self-attention lately, and in the articles that I've been seeing, they all talk about &quot;weights&quot; in attention. My understanding is that the weights in self-attention are not the same as the weights in a neural network.</p>
<p>From this article, <a href=""http://peterbloem.nl/blog/transformers"" rel=""noreferrer"">http://peterbloem.nl/blog/transformers</a>, in the additional tricks section, it mentions,</p>
<p>The query is the dot product of the query weight matrix and the word vector,
<code>ie, q = W(q)x</code> and the key is the dot product of the key weight matrix and the word vector, <code>k = W(k)x</code> and similarly for the value it is <code>v = W(v)x</code>. So my question is, where do the weight matrices come from?</p>
","transformer"
"23221","How is BERT different from the original transformer architecture?","2020-08-24 14:56:35","23683","30","17305","<natural-language-processing><comparison><transformer><bert>","<p>As far as I can tell, BERT is a type of Transformer architecture. What I do not understand is:</p>
<ol>
<li><p>How is Bert different from the original transformer architecture?</p>
</li>
<li><p>What tasks are better suited for BERT, and what tasks are better suited for the original architecture?</p>
</li>
</ol>
","transformer"
"22957","How can Transformers handle arbitrary length input?","2020-08-10 04:01:13","22960","38","22810","<natural-language-processing><recurrent-neural-networks><long-short-term-memory><transformer>","<p>The transformer, introduced in the paper <a href=""https://arxiv.org/abs/1706.03762"" rel=""noreferrer"">Attention Is All You Need</a>, is a popular new neural network architecture that is commonly viewed as an alternative to recurrent neural networks, like LSTMs and GRUs.</p>
<p>However, having gone through the paper, as well as several online explanations, I still have trouble wrapping my head around how they work. How can a non-recurrent structure be able to deal with inputs of arbitrary length?</p>
","transformer"
"22752","Using transformer but masking in reverse direction/smart sampling for desired final word?","2020-07-29 18:37:36","","0","53","<natural-language-processing><transformer>","<p>I'm trying to generate rhymes, so it would be very helpful to have a language model where I could input a final word, and have it output a sequence of words that ends with that word.</p>
<p>I could train my own model and reverse the direction of the mask, but I was wondering if there was any way I could use a pretrained model but apply a different mask to achieve this goal.</p>
<p>If this isn't possible, what's the best way to sample a forward-predicting model to achieve the highest probability sentence ending with a particular word?</p>
<p>Thanks!</p>
","transformer"
"22738","What is the memory complexity of the memory-efficient attention in Reformer?","2020-07-29 10:27:50","","2","188","<deep-learning><papers><transformer><space-complexity><reformer>","<p>When I read the paper, <a href=""https://arxiv.org/pdf/2001.04451.pdf"" rel=""nofollow noreferrer"">Reformer: The Efficient Transformer</a>, I cannot get the same complexity of the memory-efficient method in Table 1 (p. 5), which summarizes time/memory complexity of scaled dot-product, memory efficient, and LSH attention.</p>
<p>The memory complexity of the memory-efficient method is as follow:</p>
<p><span class=""math-container"">$$\max \left(b n_{h} l d_{k}, b n_{h} l^{2}\right)$$</span></p>
<p><span class=""math-container"">$b$</span>: batch size
<span class=""math-container"">$l$</span>: sequence length
<span class=""math-container"">$n_h$</span>: the number of attention head
<span class=""math-container"">$d_k$</span>: the dimension of query or key</p>
<p>To the best of my knowledge, the memory-efficient method will do a loop for each query, therefore, the whole attention matrix will not show up.</p>
<p>So, shouldn't the memory complexity be <span class=""math-container"">$\max(b n_h l d_k, b n_h l)=(b n_h l d_k)$</span> instead of <span class=""math-container"">$\max(b n_h l d_k,b n_h l^2)$</span>?</p>
","transformer"
"22700","Is it good practice to save NLP Transformer based pre-trained models into file system in production environment","2020-07-27 13:12:35","","1","118","<models><pytorch><word-embedding><transformer><pretrained-models>","<p>I have developed a multi label classifier using BERT. I'm leveraging Hugging Face Pytorch implementation for transformers.</p>
<p>I have saved the pretrained model into the file directory in dev environment. Now, the application is ready to be moved the production environment.</p>
<p>Is it a good practice to save the models into file system in prod ?
Can I serialize the model files and word embeddings into any DB and read again ?</p>
","transformer"
"22673","What exactly are the ""parameters"" in GPT-3's 175 billion parameters and how are they chosen/generated?","2020-07-26 08:12:32","22715","27","22070","<recurrent-neural-networks><open-ai><transformer><attention><gpt>","<p>When I studied neural networks, parameters were learning rate, batch size etc. But even GPT3's ArXiv paper does not mention anything about what exactly the parameters are, but gives a small hint that they might just be sentences.</p>
<p><a href=""https://i.sstatic.net/LWEEQ.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/LWEEQ.png"" alt=""enter image description here"" /></a></p>
<p>Even tutorial sites like <a href=""https://www.analyticsvidhya.com/blog/2019/07/openai-gpt2-text-generator-python/"" rel=""noreferrer"">this one</a> start talking about the usual parameters, but also say <code>&quot;model_name: This indicates which model we are using. In our case, we are using the GPT-2 model with 345 million parameters or weights&quot;</code>. So are the 175 billion &quot;parameters&quot; just neural weights? Why then are they called parameters? <a href=""https://arxiv.org/pdf/2005.14165.pdf"" rel=""noreferrer"">GPT3's paper</a> shows that there are only 96 layers, so I'm assuming it's not a very deep network, but extremely fat. Or does it mean that each &quot;parameter&quot; is just a representation of the encoders or decoders?</p>
<p><a href=""https://i.sstatic.net/dthvC.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/dthvC.png"" alt=""enter image description here"" /></a></p>
<p>An excerpt from <a href=""https://minimaxir.com/2019/09/howto-gpt2/"" rel=""noreferrer"">this website</a> shows tokens:</p>
<blockquote>
<p>In this case, there are two additional parameters that can be passed
to gpt2.generate(): truncate and include_prefix. For example, if each
short text begins with a &lt;|startoftext|&gt; token and ends with a
&lt;|endoftext|&gt;, then setting prefix='&lt;|startoftext|&gt;',
truncate=&lt;|endoftext|&gt;', and include_prefix=False, and length is
sufficient, then gpt-2-simple will automatically extract the shortform
texts, even when generating in batches.</p>
</blockquote>
<p>So are the parameters various kinds of tokens that are manually created by humans who try to fine-tune the models? Still, 175 billion such fine-tuning parameters is too high for humans to create, so I assume the &quot;parameters&quot; are auto-generated somehow.</p>
<p>The <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">attention-based</a> paper mentions the <a href=""http://jalammar.github.io/illustrated-gpt2/"" rel=""noreferrer"">query-key-value weight</a> matrices as the &quot;parameters&quot;. <strong>Even if it is these weights, I'd just like to know what kind of a process generates these parameters, who chooses the parameters and specifies the relevance of words? If it's created automatically, how is it done?</strong></p>
","transformer"
"22383","What is the big fuzz about SHA-RNN versus Transformers?","2020-07-07 17:52:47","","0","310","<recurrent-neural-networks><long-short-term-memory><transformer><attention>","<p>In his paper introducing SHA-RNN (<a href=""https://arxiv.org/pdf/1911.11423.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1911.11423.pdf</a>) Stephen Merity states that neglecting one direction of research (in this case LSTMs) over another (transformers) merily because the SOTA in transformers are due to using more computing power is not the way to go.</p>
<p>I agree that finding neat tricks in AI/ML is equally (if not more) important than just throwing more computing power at the problem. However I am a little bit confused.</p>
<p>The main difference (since they both use attention units) between his SHA-RNN and transformers seem to be the fact that SHA-RNN uses LSTMs to &quot;encode the position of words&quot;, where transformers do position encoding by using cosine and sine functions.</p>
<p>My confusion comes from the fact that LSTMs need to be handled sequentially and thus they cannot use this large advantage of GPUs, being able to compute things in parallel, whilst transformers can. Wouldn't this mean that (assuming LSTMs and positional encoding are able to acquire the same results), training using LSTMs would take longer than transformers and thus need more computing power, thus defeating the initial puporse of this paper? Or am I misinterpreting this?</p>
<p>Basically my question comes down to &quot;Why would an SHA-RNN be less computationally expensive than a transformer?&quot;</p>
","transformer"
"22154","Do transformers have success in other domains different than NLP?","2020-06-24 23:55:48","22830","2","225","<deep-learning><natural-language-processing><applications><sequence-modeling><transformer>","<p>Everybody knows how successful transformers have been in NLP. Is there known work on other domains (e.g that also have a sequential natural way of occurring, such as stock price prediction or other problems)?</p>
","transformer"
"22080","Is the self-attention matrix softmax output (layer 1) symmetric?","2020-06-22 21:32:15","","1","1588","<transformer><attention><softmax>","<p>Let's assume that we embedded a vector of length 49 into a matrix using 512-d embeddings. If we then multiply the matrix by its transposed version, we receive a matrix of 49 by 49, which is symmetric. Let's also assume we do not add the positional encoding and we only have only one attention head in the first layer of the transformer architecture.</p>
<p>What would the result of the softmax on this 49 by 49 matrix look like? Is it still symmetric, or is the softmax correctly applied for each line of the matrix, resulting in a non-symmetric matrix? My guess would be that the matrix should not be symmetric anymore. But I'm unsure about that.</p>
<p>I ask this to verify if my implementation is wrong or not, and what the output should look like. I have seen so many sophisticated and different implementations of the transformer architecture with different frameworks, that I can't answer this question for myself right now (confusion). I still try to understand the basic building blocks of the transformer architecture.</p>
","transformer"
"21632","What are the keys and values of the attention model for the encoder and decoder in the ""Attention Is All You Need"" paper?","2020-06-04 07:18:40","","4","1012","<deep-learning><natural-language-processing><transformer><attention>","<p>I have recently encountered the paper on NLP. It is very new to me and I am still unable to see how that works. I have used all the resources over there from the original paper to Youtube videos and the very famous ""Illustrated Transformer"".</p>

<p>Suppose I have a training example of ""I am a student"" and I have the respective French as ""Je suis etudient"". </p>

<p>I want to know how these 3 words are converted to 4 words. What are the query, keys, values? </p>

<p>This is my understanding of the topic so far.</p>

<p><strong>The encoder part is:</strong></p>

<ul>
<li><p>Query: a single word embedded in a vector form. such as ""I"" expressed as a vector of length 5 as <span class=""math-container"">$[.2, 0.1, 0.4, 0.9, 0.44]$</span>.</p></li>
<li><p>Keys: the matrix of all the vectors or in simple words, a matrix that has all the words from a sentence in the form of embeddings.</p></li>
<li><p>Values = Keys</p></li>
</ul>

<p><strong>For decoder:</strong></p>

<ul>
<li><p>Query: the input word in the form of a vector (which is output given by the decoder from the previous pass).</p></li>
<li><p>Keys = values = outputs from the encoder's layers.</p></li>
</ul>

<p><strong>BUT there are 2 different attention layers and one of which do not use the encoder's output at all</strong>. So, what are the keys and values now? (I think they are just like encoder, but just the generated <strong>until that pass</strong>)?</p>
","transformer"
"21588","How to understand the matrices used in the Attention layer?","2020-06-02 18:33:32","","2","105","<deep-learning><recurrent-neural-networks><long-short-term-memory><transformer><attention>","<p>Attention-scoring mechanism seems to be a commonly-used component in various seq2seq models, and I was reading about the original ""Location-based Attention"" in Bahadanau well-known paper at <a href=""https://arxiv.org/pdf/1506.07503.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1506.07503.pdf</a>. (it seems this attention is used in various forms of GNMT and text-to-speech sythesizers like tacotron-2 <a href=""https://github.com/Rayhane-mamah/Tacotron-2"" rel=""nofollow noreferrer"">https://github.com/Rayhane-mamah/Tacotron-2</a>).</p>

<p>Even after repeated readings of this paper and other articles about Attention-mechanism, I'm confused about the dimensions of the matrices used, as the paper doesn't seem to describe it. My understanding is:</p>

<ul>
<li><p>If I have decoder hidden dim 1024, that means <span class=""math-container"">${s_{i-1}}$</span> vector is 1024 length.</p></li>
<li><p>If I have encoder output dim 512, that means <span class=""math-container"">$h_{j}$</span> vector is 512 length.</p></li>
<li><p>If total inputs to encoder is 256, then number of <span class=""math-container"">$j$</span> can be from 1 to 256.</p></li>
<li><p>Since <span class=""math-container"">$W x S_{i-1}$</span> is a matrix multiply, it seems <span class=""math-container"">$cols(W)$</span> should match <span class=""math-container"">$rows(S_{i-1})$</span>, but <span class=""math-container"">$rows(W)$</span> still remain undefined. Same seems true for matrices <span class=""math-container"">$V, U, w, b$</span>.</p></li>
</ul>

<p>This is page-3/4 from the paper above that describes Attention-layer:</p>

<p><a href=""https://i.sstatic.net/xKD2U.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xKD2U.png"" alt=""enter image description here""></a></p>

<p>I'm unsure how to make sense of this. Am I missing something, or can someone explain this?</p>

<p>What I don't understand is:</p>

<ul>
<li><p>What is the dimension of previous alignment (denoted by <span class=""math-container"">$alpha_{i-1})$</span>? Shouldn't it be total values of <span class=""math-container"">$j$</span> in <span class=""math-container"">$h_{j}$</span> (which is 256 and means total different encoder output states)?</p></li>
<li><p>What is the dimension of <span class=""math-container"">$f_{i,j}$</span> and convolution filter <span class=""math-container"">$F$</span>? (the paper says <span class=""math-container"">$F$</span> belongs to <span class=""math-container"">$kxr$</span> shape but doesn't define <span class=""math-container"">$'r'$</span> anywhere). What is <span class=""math-container"">$'r'$</span> and what does <span class=""math-container"">$'k x r'$</span> mean here?</p></li>
<li><p>How are these unknown dimensions for matrices <span class=""math-container"">$'V, U, w, b'$</span> described above determined in this model?</p></li>
</ul>
","transformer"
"21536","Why does the BERT NSP head linear layer have two outputs?","2020-05-30 23:50:37","22113","1","329","<machine-learning><natural-language-processing><transformer><bert>","<p>Here's the code in question. </p>

<p><a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py#L491</a></p>

<pre><code>class BertOnlyNSPHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, pooled_output):
        seq_relationship_score = self.seq_relationship(pooled_output)
        return seq_relationship_score
</code></pre>

<p>I think it was just ranking how likely one sentence would follow another? Wouldn't it be one score?</p>
","transformer"
"21385","Transformer encoding for regression","2020-05-21 19:25:46","","1","253","<natural-language-processing><transformer>","<p>I have a string of characters encoding a molecule. I want to regress some properties of those molecules. I tried using an LSTM that encodes all one hot encdoed characters, and then I take the last hidden state fed into a linear layer to regress the property. This works fine, but I wanted to see if transformers can do better, since they are so good in NLP.</p>

<p>However, I am not quiet sure about two things:</p>

<ol>
<li>Pytorch transformer encoder layer has two masking parameters: ""src_mask"" and ""src_key_padding_mask"". The model needs the whole string to do the regression, so I dont think I need ""src_mask"", but I do padding with 0 for parallel processing, is that what ""src_key_padding_mask"" is for?</li>
<li>What output from the transformer do I feed into the linear regression layer? For the LSTM I took the last hidden output. For the transformer, since everything is processed in parallel, I feel like I should rather use the sum of all, but it doesn't work well. Instead using only the last state works better, which seems arbitrary to me. Any ideas on how to properly do this, how do sentiment analysis model do it?</li>
</ol>
","transformer"
"21277","Can you use transformer models to do autocomplete tasks?","2020-05-19 01:32:57","","2","200","<neural-networks><recurrent-neural-networks><transformer>","<p>I've researched online and seen many papers on the use of RNNs (like LSTMs or GRUs) to autocomplete for, say, a search engine, character by character. Which makes sense since it inherently predicts character-by-character in a sequential manner. </p>

<p>Would it be possible to use the transformer architecture instead to do search autocomplete? If so, how might such a model be adapted? </p>
","transformer"
"21237","Why does this multiplication of $Q$ and $K$ have a variance of $d_k$, in scaled dot product attention?","2020-05-18 01:28:13","25057","8","6701","<neural-networks><machine-learning><natural-language-processing><transformer><attention>","<p>In scaled dot product attention, we scale our outputs by dividing the dot product by the square root of the dimensionality of the matrix:</p>
<p><a href=""https://i.sstatic.net/wLI4m.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/wLI4m.png"" alt=""enter image description here"" /></a></p>
<p>The reason why is stated that this constrains the distribution of the weights of the output to have a standard deviation of 1.</p>
<p>Quoted from <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""noreferrer"">Transformer model for language understanding | TensorFlow</a>:</p>
<blockquote>
<p>For example, consider that <span class=""math-container"">$Q$</span> and <span class=""math-container"">$K$</span> have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of <span class=""math-container"">$d_k$</span>. Hence, square root of <span class=""math-container"">$d_k$</span> is used for scaling (and not any other number) because the matmul of <span class=""math-container"">$Q$</span> and <span class=""math-container"">$K$</span> should have a mean of 0 and variance of 1, and you get a gentler softmax.</p>
</blockquote>
<p>Why does this multiplication have a variance of <span class=""math-container"">$d_k$</span>?</p>
<p>If I understand this, I will then understand why dividing by <span class=""math-container"">$\sqrt({d_k})$</span> would normalize to 1.</p>
<p>Trying this experiment on 2x2 arrays I get an output of 1.6 variance:</p>
<p><a href=""https://i.sstatic.net/GCe3t.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/GCe3t.png"" alt=""enter image description here"" /></a></p>
","transformer"
"20176","What is the intuition behind the dot product attention?","2020-04-11 12:53:21","","22","11744","<natural-language-processing><papers><transformer><attention><bert>","<p>I am watching the video <a href=""https://www.youtube.com/watch?v=iDulhoQ2pro&amp;t=6s"" rel=""noreferrer"">Attention Is All You Need</a> by Yannic Kilcher.</p>
<p>My question is: what is the intuition behind the dot product attention?</p>
<p><span class=""math-container"">$$A(q,K, V) = \sum_i\frac{e^{q.k_i}}{\sum_j e^{q.k_j}} v_i$$</span></p>
<p>becomes:</p>
<p><span class=""math-container"">$$A(Q,K, V) = \text{softmax}(QK^T)V$$</span></p>
","transformer"
"20141","What is the time complexity of the forward pass and back-propagation of the sequence-to-sequence model with and without attention?","2020-04-09 21:33:51","","2","1389","<recurrent-neural-networks><transformer><time-complexity><forward-pass><seq2seq>","<p>I keep looking through the literature, but can't seem to find any information regarding the time complexity of the forward pass and back-propagation of the sequence-to-sequence RNN encoder-decoder model, with and without attention.</p>

<p>The paper <a href=""https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""nofollow noreferrer"">Attention is All You Need</a> by Vaswani et. al in 2017 states the forward pass cost is <span class=""math-container"">$O(n^3)$</span>, which makes sense to me (with 1 hidden layer). In terms of <span class=""math-container"">$X$</span> the input length, and <span class=""math-container"">$Y$</span> the output length, it then looks like <span class=""math-container"">$O(X^3 + Y^3)$</span>, which I understand.</p>

<p>However, for training, it seems to me like one back-propagation is at worst <span class=""math-container"">$O(X^3 + Y^3)$</span>, and we do <span class=""math-container"">$Y$</span> of them, so <span class=""math-container"">$O(Y(X^3  Y^3))$</span>. </p>

<p>This is the following diagram, where the green blocks are the hidden states, the red ones are the input text and the blue ones are output text.</p>

<p><a href=""https://i.sstatic.net/iJImA.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iJImA.png"" alt=""enter image description here""></a></p>

<p>If I were to add global attention, as introduced by <a href=""https://arxiv.org/pdf/1508.04025.pdf"" rel=""nofollow noreferrer"">Luong et. al in 2015</a>, the attention adds an extra <span class=""math-container"">$X^2$</span> in there due to attention multiplication, to make an overall inference of <span class=""math-container"">$O(X^3 + X^2 Y^3)$</span>, and training even worse at <span class=""math-container"">$O(XY(X^3 + X^2 Y^3))$</span> since it needs to learn attention weights too. </p>

<p>The following diagram shows the sequence-to-sequence model with attention, 
where <span class=""math-container"">$h$</span>'s are the hidden states, <span class=""math-container"">$c$</span> is the context vector and <span class=""math-container"">$y$</span> is the output word, <span class=""math-container"">$Y$</span> of such output words and <span class=""math-container"">$X$</span> such inputs. This setup is described in the paper <a href=""https://arxiv.org/pdf/1508.04025.pdf"" rel=""nofollow noreferrer"">Effective Approaches to Attention-based Neural Machine Translation</a>, by Luong et al. in 2015.</p>

<p><a href=""https://i.sstatic.net/ybKiS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ybKiS.png"" alt=""enter image description here""></a></p>

<p>Is my intuition correct?</p>
","transformer"
"20075","Why does the transformer do better than RNN and LSTM in long-range context dependencies?","2020-04-07 12:05:31","20084","76","107687","<machine-learning><natural-language-processing><recurrent-neural-networks><long-short-term-memory><transformer>","<p>I am reading the article <a href=""https://towardsdatascience.com/transformers-141e32e69591"" rel=""noreferrer"">How Transformers Work</a> where the author writes</p>

<blockquote>
  <p>Another problem with RNNs, and LSTMs, is that it’s hard to parallelize the work for processing sentences, since you have to process word by word. Not only that but there is no model of <strong>long and short-range dependencies</strong>. </p>
</blockquote>

<p>Why exactly does the transformer do better than RNN and LSTM in <strong>long-range context dependencies</strong>?</p>
","transformer"
"18773","Can you train Transformers sequentially?","2020-03-24 09:02:02","","1","241","<natural-language-processing><pytorch><transformer>","<p>I’m currently trying to train a BART, which is a denoising Transformer created by Facebook researchers. Here’s my Transformer code</p>

<pre><code>import math
import torch
from torch import nn
from Constants import *

class Transformer(nn.Module):
    def __init__(self, input_dim: int, output_dim: int, d_model: int = 200, num_head: int = 8, num_e_layer: int = 6,
                 num_d_layer: int = 6, ff_dim: int = 1024, drop_out: float = 0.1):
        '''
        Args:
            input_dim: Size of the vocab of the input
            output_dim: Size of the vocab for output
            num_head: Number of heads in mutliheaded attention models
            num_e_layer: Number of sub-encoder layers
            num_d_layer: Number of sub-decoder layers
            ff_dim: Dimension of feedforward network in mulihead models
            d_model: The dimension to embed input and output features into
            drop_out: The drop out percentage
        '''
        super(Transformer, self).__init__()
        self.d_model = d_model
        self.transformer = nn.Transformer(d_model, num_head, num_e_layer, num_d_layer, ff_dim, drop_out,
                                          activation='gelu')
        self.decoder_embedder = nn.Embedding(output_dim, d_model)
        self.encoder_embedder = nn.Embedding(input_dim, d_model)
        self.fc1 = nn.Linear(d_model, output_dim)
        self.softmax = nn.Softmax(dim=2)
        self.positional_encoder = PositionalEncoding(d_model, drop_out)
        self.to(DEVICE)

    def forward(self, src: torch.Tensor, trg: torch.Tensor, src_mask: torch.Tensor = None,
                trg_mask: torch.Tensor = None):
        embedded_src = self.positional_encoder(self.encoder_embedder(src) * math.sqrt(self.d_model))
        embedded_trg = self.positional_encoder(self.decoder_embedder(trg) * math.sqrt(self.d_model))
        output = self.transformer.forward(embedded_src, embedded_trg, src_mask, trg_mask)
        return self.softmax(self.fc1(output))

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)    
        self.register_buffer('pe', pe)
</code></pre>

<p>and here’s my training code</p>

<pre><code>def train(x: list):
    optimizer.zero_grad()
    loss = 0.
    batch_sz = len(x)
    max_len = len(max(x, key=len)) + 1  # +1 for EOS xor SOS
    noise_x = noise(x)
    src_x = list(map(lambda s: [SOS] + [char for char in s] + [PAD] * ((max_len - len(s)) - 1), noise_x))
    trg_x = list(map(lambda s: [char for char in s] + [EOS] + [PAD] * ((max_len - len(s)) - 1), x))
    src = indexTensor(src_x, max_len, IN_CHARS).to(DEVICE)
    trg = targetsTensor(trg_x, max_len, OUT_CHARS).to(DEVICE)
    names = [''] * batch_sz

    for i in range(src.shape[0]):
        probs = transformer(src, trg[:i + 1])
        loss += criterion(probs, trg[i])

    loss.backward()
    optimizer.step()

    return names, loss.item()
</code></pre>

<p>As you can see in the train code. I am training it ""sequentially"" by inputting the first letter of the data then computing the loss with the output then inputting the first and second character and doing the same thing, so on and so forth.</p>

<p>This doesn’t seem to be training properly though as the denoising is totally off. I thought maybe there’s something wrong with my code or you can’t train Transformers this way.</p>

<p>I'm taking first name data then noising it then training the Transformer to denoise it, but the output to the Transformers doesn't look remotely like the denoised version or even the noised version of the name. I built a denoising autoencoder using LSTMs and it did way better, but I feel like BART should be way out performing LSTMs cause it's supposedly state of the art NLP neural network model.</p>
","transformer"
"18492","Can transformer be better than RNN for online speech recognition?","2020-03-08 03:48:40","","3","466","<recurrent-neural-networks><transformer><speech-recognition>","<p>Does transformer have the potential to replace RNN end-to-end models for speech recognition for online speech recognition? This mainly depends on accuracy/latency and deploy cost, not training cost. Can transformer support low latency online use case and have comparable deploy cost and better result than RNN models?</p>
","transformer"
"18449","Is it possible to do token classification using a model such as GPT-2?","2020-03-05 21:26:11","","1","251","<natural-language-processing><classification><pytorch><transformer>","<p>I am trying to use <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">PyTorch's transformers</a> as a part of a research project to do sentiment analysis of several types of review data (laptop and restaurant). </p>

<p>To do this, my team is taking a token-based approach and we are using models that can perform token analysis.</p>

<p>One problem we have encountered is that many of the models in <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">PyTorch's transformers</a> do not support token classification, but do support sequence classification. One such model we wanted to test is GPT-2.</p>

<p>In order to overcome this, we proposed using sequence classifiers on single tokens which should work in theory, but possibly at reduced accuracy.</p>

<p>This raises the following questions:</p>

<ul>
<li><p>Is it possible to do token classification using a model such as GPT-2 using <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">PyTorch's transformers</a>?</p></li>
<li><p>How do sequence classifiers perform on single token sequences?</p></li>
</ul>
","transformer"
"18437","How does positional encoding work in the transformer model?","2020-03-05 11:54:02","","2","168","<deep-learning><natural-language-processing><transformer><attention><positional-encoding>","<p>In the transformer model, to incorporate positional information of texts, the researchers have added a positional encoding to the model. <em>How does positional encoding work? How does the positional encoding system learn the positions when varying lengths and types of text are passed at different time intervals?</em></p>
<p>To be more concrete, let's take these two sentences.</p>
<ol>
<li>&quot;She is my queen&quot;</li>
<li>&quot;Elizabeth is the queen of England&quot;</li>
</ol>
<p>How would these sentences be passed to the transformer? What would happen to them during the positional encoding part?</p>
<p>Please explain with less math and with more intuition behind it.</p>
","transformer"
"18257","How does the regression layer in the localization network of a spatial transformer work?","2020-02-26 04:56:38","","1","185","<neural-networks><machine-learning><regression><transformer>","<p>I am trying to understand the <strong>spatial transformer network</strong> mentioned in this paper <a href=""https://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf"" rel=""nofollow noreferrer"">https://papers.nips.cc/paper/5854-spatial-transformer-networks.pdf</a>. <strong>I am clear about the last two stages of the spatial transformer i.e. the grid generator and sampler</strong>. However I am <strong>unable to understand the localization network</strong> which outputs the parameters of the transformation that is applied to the input image. So here are my doubts.</p>

<ol>
<li>Is the network trained on various affine/projective transforms of the input or only the standard input with a standard pose?</li>
<li>If the answer to question 1 is no, then how does the regression layer correctly regress the values of the transformation applied to the image? In other words how does the regression layer know what transformation parameters are required when it has never seen those inputs before?</li>
</ol>

<p>Thanks in advance. </p>
","transformer"
"17992","Pretrained Models for Keyword-Based Text Generation","2020-02-12 16:54:18","","2","342","<transformer><gpt><text-generation>","<p>I'm looking for an implementation that allows me to generate text based on a pre-trained model (e.g. GPT-2).</p>
<p>An example would be <a href=""https://github.com/minimaxir/gpt-2-keyword-generation"" rel=""nofollow noreferrer"">gpt-2-keyword-generation</a> (<a href=""https://minimaxir.com/apps/gpt2-reddit/"" rel=""nofollow noreferrer"">click here for demo</a>). As the author notes, there is</p>
<blockquote>
<p>[...] no explicit mathematical/theoetical basis behind the keywords
aside from the typical debiasing of the text [...]</p>
</blockquote>
<p>Hence my question: <strong>Are there more sophisticated ways of keyword-based text generation</strong> or at least any other <strong>alternatives</strong>?</p>
<p>Thank you</p>
","transformer"
"17031","How does a transformer leverage the GPU to be trained faster than RNNs?","2019-12-10 22:05:26","","4","862","<natural-language-processing><training><transformer><attention><gpu>","<p>How does a transformer leverage the GPU to be trained faster than RNNs?</p>
<p>I understand the parameter space of the transformer might be significantly larger than that of the RNN. But why does the transformer structure can leverage multiple GPUs, and why does that accelerate its training?</p>
","transformer"
"16835","Where should we place layer normalization in a transformer model?","2019-11-28 12:36:12","","2","676","<neural-networks><deep-learning><papers><transformer><layer-normalization>","<p>In <a href=""https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a> paper:</p>
<blockquote>
<p>That is, the output of each sub-layer is <span class=""math-container"">$LayerNorm(x+Sublayer(x))$</span>, where <span class=""math-container"">$Sublayer(x)$</span> is the function implemented by the sub-layer itself. We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized.</p>
</blockquote>
<p>which makes the final formula <span class=""math-container"">$LayerNorm(x+Dropout(Sublayer(x)))$</span>. However, in <a href=""https://github.com/tensorflow/models/blob/0effd158ae1e6403c6048410f79b779bdf344d7d/official/transformer/model/transformer.py#L278-L288"" rel=""nofollow noreferrer"">https://github.com/tensorflow/models/blob/0effd158ae1e6403c6048410f79b779bdf344d7d/official/transformer/model/transformer.py#L278-L288</a>, I see</p>
<pre><code>def __call__(self, x, *args, **kwargs):
  # Preprocessing: apply layer normalization
  y = self.layer_norm(x)

  # Get layer output
  y = self.layer(y, *args, **kwargs)

  # Postprocessing: apply dropout and residual connection
  if self.train:
    y = tf.nn.dropout(y, 1 - self.postprocess_dropout)
  return x + y
</code></pre>
<p>which ends up as <span class=""math-container"">$x+Dropout(Sublayer(LayerNorm(x)))$</span>. Plus there are extra <code>LayerNorm</code>s as final layers in both encoder and decoder stacks.</p>
<p>In a quick test, the performance of this model seems to be better than if I change back to the paper's order of operations. My question is: why? And could it be predicted in advance?</p>
<p>I note that <a href=""https://arxiv.org/pdf/1904.10509.pdf"" rel=""nofollow noreferrer"">Generating Long Sequences with Sparse Transformers</a> uses the <span class=""math-container"">$x+Dropout(Sublayer(LayerNorm(x)))$</span> order, but doesn't discuss it, unlike the other changes it makes to Transformer.</p>
","transformer"
"16516","Is the Mask Needed for Masked Self-Attention During Inference with GPT-2","2019-11-14 11:41:12","","6","2893","<natural-language-processing><attention><transformer><gpt><inference>","<p>My understanding is that masked self-attention is necessary during training of GPT-2, as otherwise it would be able to directly see the correct next output at each iteration. My question is whether the attention mask is necessary, or even possible, during inference. As GPT-2 will only be producing one token at a time, it doesn't make sense to mask out future tokens that haven't been inferred yet.</p>
","transformer"
"16353","Are embeddings in multi-lingual language models comparable across languages?","2019-11-08 09:39:43","","4","352","<deep-learning><natural-language-processing><transformer><language-model>","<p>Facebook has <a href=""https://arxiv.org/pdf/1911.02116.pdf"" rel=""nofollow noreferrer"">just pushed out</a> a bigger version of their multi-lingual language model XLM, called XLM-R. My question is: do these kind of multi-lingual models imply, or even ensure, that their embeddings are comparable between languages? That is, are semantically related words close together in the vector space across languages?</p>

<p>Perhaps the most interesting citation from the paper that is relevant to my question (p. 3):</p>

<blockquote>
  <p>Unlike Lample and Conneau (2019), we do not use language embeddings,
  which allows our model to better deal with code-switching.</p>
</blockquote>

<p>Because they do not seem to make a distinction between languages, and there's just one vocabulary for all trained data, I fail to see how this can be truly representative of semantics anymore. The move away from semantics is increased further by the use of BPE, since morphological features (or just plain, statistical <em>word chunks</em>) of one language might often not be semantically related to the same chunk in another language - this can be true for tokens themselves, but especially so for subword information.</p>

<p>So, in short: how well can the embeddings in multi-lingual language models be used for semantically comparing input (e.g. a word or sentence) of two different languages?</p>
","transformer"
"16191","How to use TPU for real-time low-latency inference?","2019-11-01 01:08:15","","3","469","<natural-language-processing><tensorflow><transformer><google><inference>","<p>I use Google's Cloud TPU hardware extensively using Tensorflow for training models and inference, however, when I run inference I do it in large batches. The TPU takes about 3 minutes to warm up before it runs the inference. But when I read the <a href=""https://cloud.google.com/tpu/docs/faq"" rel=""nofollow noreferrer"">official TPU FAQ</a>, it says that we can do real-time inference using TPU. It says the latency is 10ms which for me is fast enough but I cannot figure out how to write code that does this, since every time I want to pass something for inference I have to start the TPU again.</p>

<p>My goal is to run large Transformer-based Language Models in real-time on TPUs. I guessed that TPUs would be ideal for this problem. Even Google seems to <a href=""https://www.blog.google/products/search/search-language-understanding-bert/"" rel=""nofollow noreferrer"">already do this</a>.</p>

<p>Quote from the <a href=""https://cloud.google.com/tpu/docs/faq"" rel=""nofollow noreferrer"">official TPU FAQ</a>:</p>

<blockquote>
  <p>Executing inference on a single batch of input and waiting for the
  result currently has an overhead of at least 10 ms, which can be
  problematic for low-latency serving.</p>
</blockquote>
","transformer"
"16057","Why is the transformer for time series forecasting faster than RNN?","2019-10-24 09:37:08","","2","421","<recurrent-neural-networks><transformer><prediction><attention><sequence-modeling>","<p>I've been reading different papers which implements the <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Transformer</a> for time series forecasting. Most of the them are claiming that the training time is significantly faster then using a normal RNN. From my understanding when training such a model, you can encode the input in parallel, but the decoding is still sequential unless you're using teacher-forcing. </p>

<p>What makes the transformer faster than RNN in such a setting? Is there something that I am missing?</p>
","transformer"
"15707","How to train a transformer text-to-text model on counterexamples?","2019-10-03 16:14:22","","2","29","<neural-networks><natural-language-processing><generative-adversarial-networks><transformer>","<p>Is it possible to update the weights of a vanilla transformer model using counterexamples alongside examples?</p>

<p>For example, from the <a href=""https://github.com/google-research-datasets/paws"" rel=""nofollow noreferrer"">PAWS</a> data set, given the phrases ""Although interchangeable, the body pieces on the 2 cars are not similar."" and ""Although similar, the body parts are not interchangeable on the 2 cars."" we have the label 0 because it is a counterexample, whereas for the phrases ""Katz was born in Sweden in 1947 and moved to New York City at the age of 1."" and ""Katz was born in 1947 in Sweden and moved to New York at the age of one."" we have the label 1 because it is a positive example of a valid paraphrase.</p>

<p>My goal is to use the transformer model to generate paraphrases, and I am attempting to build a GAN but could not find any references for updating the transformer text-to-text model using counterexamples.</p>
","transformer"
"15524","Why would you implement the position-wise feed-forward network of the transformer with convolution layers?","2019-09-18 23:45:28","","22","11277","<deep-learning><keras><convolution><transformer><feedforward-neural-networks>","<p>The Transformer model introduced in <a href=""https://arxiv.org/abs/1706.03762"" rel=""noreferrer"">""Attention is all you need""</a> by Vaswani et al. incorporates a so-called position-wise feed-forward network (FFN):</p>

<blockquote>
  <p>In addition to attention sub-layers, each of the layers in our encoder
  and decoder contains a fully connected feed-forward network, which is
  applied to each position separately and identically. This consists of
  two linear transformations with a ReLU activation in between.</p>
  
  <p><span class=""math-container"">$$\text{FFN}(x) = \max(0, x \times {W}_{1} + {b}_{1}) \times {W}_{2} + {b}_{2}$$</span></p>
  
  <p>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is <span class=""math-container"">${d}_{\text{model}} = 512$</span>, and the inner-layer has dimensionality <span class=""math-container"">${d}_{ff} = 2048$</span>.</p>
</blockquote>

<p>I have seen at least one implementation in Keras that directly follows the convolution analogy. Here is an excerpt from <a href=""https://github.com/Lsdefine/attention-is-all-you-need-keras/blob/master/transformer.py"" rel=""noreferrer"">attention-is-all-you-need-keras</a>.</p>

<pre><code>class PositionwiseFeedForward():
    def __init__(self, d_hid, d_inner_hid, dropout=0.1):
        self.w_1 = Conv1D(d_inner_hid, 1, activation='relu')
        self.w_2 = Conv1D(d_hid, 1)
        self.layer_norm = LayerNormalization()
        self.dropout = Dropout(dropout)
    def __call__(self, x):
        output = self.w_1(x) 
        output = self.w_2(output)
        output = self.dropout(output)
        output = Add()([output, x])
        return self.layer_norm(output)
</code></pre>

<p>Yet, in Keras you can apply a single <code>Dense</code> layer across all time-steps using the <code>TimeDistributed</code> wrapper (moreover, a simple <code>Dense</code> layer applied to a 2D input <a href=""https://stackoverflow.com/a/44616780/3846213"">implicitly behaves</a> like a <code>TimeDistributed</code> layer). Therefore, in Keras a stack of two Dense layers (one with a ReLU and the other one without an activation) is exactly the same thing as the aforementioned position-wise FFN. So, why would you implement it using convolutions?</p>

<p><strong>Update</strong> </p>

<p>Adding benchmarks in response to the answer by @mshlis:</p>

<pre><code>import os
import typing as t
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import numpy as np

from keras import layers, models
from keras import backend as K
from tensorflow import Tensor


# Generate random data

n = 128000  # n samples
seq_l = 32  # sequence length
emb_dim = 512  # embedding size

x = np.random.normal(0, 1, size=(n, seq_l, emb_dim)).astype(np.float32)
y = np.random.binomial(1, 0.5, size=n).astype(np.int32)
</code></pre>

<hr>

<pre><code># Define constructors

def ffn_dense(hid_dim: int, input_: Tensor) -&gt; Tensor:
    output_dim = K.int_shape(input_)[-1]
    hidden = layers.Dense(hid_dim, activation='relu')(input_)
    return layers.Dense(output_dim, activation=None)(hidden)


def ffn_cnn(hid_dim: int, input_: Tensor) -&gt; Tensor:
    output_dim = K.int_shape(input_)[-1]
    hidden = layers.Conv1D(hid_dim, 1, activation='relu')(input_)
    return layers.Conv1D(output_dim, 1, activation=None)(hidden)


def build_model(ffn_implementation: t.Callable[[int, Tensor], Tensor], 
                ffn_hid_dim: int, 
                input_shape: t.Tuple[int, int]) -&gt; models.Model:
    input_ = layers.Input(shape=(seq_l, emb_dim))
    ffn = ffn_implementation(ffn_hid_dim, input_)
    flattened = layers.Flatten()(ffn)
    output = layers.Dense(1, activation='sigmoid')(flattened)
    model = models.Model(inputs=input_, outputs=output)
    model.compile(optimizer='Adam', loss='binary_crossentropy')
    return model
</code></pre>

<hr>

<pre><code># Build the models

ffn_hid_dim = emb_dim * 4  # this rule is taken from the original paper
bath_size = 512  # the batchsize was selected to maximise GPU load, i.e. reduce PCI IO overhead

model_dense = build_model(ffn_dense, ffn_hid_dim, (seq_l, emb_dim))
model_cnn = build_model(ffn_cnn, ffn_hid_dim, (seq_l, emb_dim))
</code></pre>

<hr>

<pre><code># Pre-heat the GPU and let TF apply memory stream optimisations

model_dense.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)
%timeit model_dense.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)

model_cnn.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)
%timeit model_cnn.fit(x=x, y=y[:, None], batch_size=bath_size, epochs=1)
</code></pre>

<p>I am getting 14.8 seconds per epoch with the Dense implementation:</p>

<pre><code>Epoch 1/1
128000/128000 [==============================] - 15s 116us/step - loss: 0.6332
Epoch 1/1
128000/128000 [==============================] - 15s 115us/step - loss: 0.5327
Epoch 1/1
128000/128000 [==============================] - 15s 117us/step - loss: 0.3828
Epoch 1/1
128000/128000 [==============================] - 14s 113us/step - loss: 0.2543
Epoch 1/1
128000/128000 [==============================] - 15s 116us/step - loss: 0.1908
Epoch 1/1
128000/128000 [==============================] - 15s 116us/step - loss: 0.1533
Epoch 1/1
128000/128000 [==============================] - 15s 117us/step - loss: 0.1475
Epoch 1/1
128000/128000 [==============================] - 15s 117us/step - loss: 0.1406

14.8 s ± 170 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>

<p>and 18.2 seconds for the CNN implementation. I am running this test on a standard Nvidia RTX 2080.
So, from a performance perspective there seems to be no point in actually implementing an FFN block as a CNN in Keras. Considering that the maths are the same, the choice boils down to pure aesthetics. </p>
","transformer"
"15386","Why do both sine and cosine have been used in positional encoding in the transformer model?","2019-09-12 02:03:04","","1","130","<deep-learning><papers><transformer><machine-translation><positional-encoding>","<p>The Transformer model proposed in ""Attention Is All You Need"" uses sinusoid functions to do the positional encoding. </p>

<p>Why have both sine and cosine been used? And why do we need to separate the odd and even dimensions to use different sinusoid functions?</p>
","transformer"
"13862","How to interpret a large variance of the loss function?","2019-08-08 17:49:45","","2","399","<objective-functions><transformer><gpt>","<p>How do I interpret a large variance of a loss function?</p>

<p>I am currently training a transformer network (using the software, but not the model from GPT-2) from scratch and my loss function looks like this:
<a href=""https://i.sstatic.net/LvhRs.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LvhRs.png"" alt=""a plot of the loss function""></a></p>

<p>The green dots are the loss averaged over 100 epochs and the purple dots are the loss for each epoch.</p>

<p>(You can ignore the missing part, I just did not save the loss values for these epochs)</p>

<p>Is such a large variance a bad sign? And what are my options for tuning to get it to converge faster? Is the network to large or too small for my training data? Should I have a look at batch size?</p>

<ul>
<li>Learning rate parameter: 2.5e-4</li>
<li>Training data size: 395 MB</li>
</ul>

<p>GPT-2 parameters:</p>

<pre><code>{
  ""n_vocab"": 50000,
  ""n_ctx"": 1024,
  ""n_embd"": 768,
  ""n_head"": 12,
  ""n_layer"": 12
}
</code></pre>
","transformer"
"12490","Can the decoder in a transformer model be parallelized like the encoder?","2019-05-23 15:36:42","12502","25","9428","<deep-learning><comparison><long-short-term-memory><sequence-modeling><transformer>","<p>Can the decoder in a transformer model be parallelized like the encoder?</p>

<p>As far as I understand, the encoder has all the tokens in the sequence to compute the self-attention scores. But for a decoder, this is not possible (in both training and testing), as self-attention is calculated based on previous timestep outputs. Even if we consider some techniques, like teacher forcing, where we are concatenating expected output with obtained, this still has a sequential input from the previous timestep. </p>

<p>In this case, apart from the improvement in capturing long-term dependencies, is using a transformer-decoder better than say an LSTM, when comparing purely on the basis of parallelization?</p>
","transformer"
"12468","How are the attention weights normalised in the transformer?","2019-05-22 09:07:58","","1","469","<tensorflow><machine-translation><bert><transformer>","<p>In the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Transformer</a> (adopted in <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">BERT</a>), we normalize the attention weights (dot product of keys and queries) using a softmax in the Scaled Dot-Product mechanism. It is unclear to me whether this normalization is performed on each row of the weight matrix or on the entire matrix. In <a href=""https://www.tensorflow.org/alpha/tutorials/text/transformer"" rel=""nofollow noreferrer"">the TensorFlow tutorial</a>, it is performed on each row (axis=-1), and in the <a href=""https://github.com/tensorflow/models/blob/master/official/transformer/model/attention_layer.py"" rel=""nofollow noreferrer"">official TensorFlow code</a>, it is performed on the entire matrix (axis=None). The paper doesn't give many details. </p>

<p>To me, both methods can make sense, but they have a strong impact. If on each row, then each value will have a roughly similar norm, because the sum of its weights is 1. If on the entire matrix, some values might be ""extinguished"" because all of its weights can be very close to zero. </p>
","transformer"
"12118","Why don't people use nonlinear activation functions after projecting the query key value in attention?","2019-05-03 03:15:23","","12","2139","<neural-networks><attention><transformer>","<p>Why don't people use nonlinear activation functions after projecting the query key value in attention?</p>

<p>It seems like doing this would lead to much-needed nonlinearity, otherwise, we're just doing linear transformations.</p>

<p>This observation applies to the transformer, additive attention, etc.</p>
","transformer"
"10989","How do the sine and cosine functions encode position in the transformer?","2019-03-03 19:41:22","","2","265","<deep-learning><natural-language-processing><transformer><attention><positional-encoding>","<p>After going through both the ""<a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">Illustrated Transformer</a>"" and ""<a href=""http://nlp.seas.harvard.edu/2018/04/03/attention.html"" rel=""nofollow noreferrer"">Annotated Transformer</a>"" blog posts, I still don't understand how the sinusoidal encodings are representing the position of elements in the input sequence. </p>

<p>Is it the fact that since each row (input token) in a matrix (entire input sequence) has a unique waveform as its encoding, each of which can be expressed as a linear function of any other element in the input sequence, then the transformer can learn relations between these rows via linear functions? </p>
","transformer"
"10869","How do we know if GPT-2 is a better language model?","2019-02-25 09:51:23","","7","735","<natural-language-processing><transformer><gpt>","<p>You may have heard of GPT2, a new language model. It has recently attracted attention from the general public as the foundation that published the paper, <a href=""https://blog.openai.com/better-language-models/"" rel=""nofollow noreferrer"">OpenAI</a>, ironically refused to share the whole model fearing dangerous implications. Along the paper, they also published a manifesto to justify their choice: &quot;Better Language Models and Their Implications&quot;. And soon a lot of media were publishing articles discussing the choice and its effectiveness to actually prevent bad implications. I am not here to discuss the ethical components of this choice but the actual performance of the model.</p>
<p>The model got my attention too and I downloaded the small model to play with. To be honest I am far from impressed by the results. Some times the first paragraph of the produced text appears to make sense, but nine times out of ten it is giberish by the first or the second sentence. Exemples given in the paper seems to be &quot;Lucky&quot; outputs, cherry picked by human hands. Overall, the paper may suffer from a very strong publication bias.</p>
<p>However, most article we can read on the internet seems to take its powerfulness for granted. <a href=""https://www.technologyreview.com/s/612975/ai-natural-language-processing-explained/"" rel=""nofollow noreferrer"">The MIT technology review</a> wrote:</p>
<blockquote>
<p>The language model can write like a human</p>
</blockquote>
<p><a href=""https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction"" rel=""nofollow noreferrer"">The Guardian</a> wrote</p>
<blockquote>
<p>When used to simply generate new text, GPT2 is capable of writing plausible passages that match what it is given in both style and subject. It rarely shows any of the quirks that mark out previous AI systems, such as forgetting what it is writing about midway through a paragraph, or mangling the syntax of long sentences.</p>
</blockquote>
<p>The model appears generally qualified as a &quot;breakthrough&quot;. These writings do not match my personal experimentation as produced texts are rarely consistent / syntactically correct.</p>
<p>My question is: without the release of the whole model for ethical reasons, how do we know if the model is really that powerful?</p>
","transformer"
"5629","What is regression layer in a spatial transformer?","2018-03-11 13:13:01","5647","3","847","<neural-networks><deep-learning><deepmind><transformer>","<p>I came across this line while reading the <a href=""https://arxiv.org/abs/1506.02025"" rel=""nofollow noreferrer"">original paper</a> on Spatial Transformers by Deepmind in the last paragraph of Sec 3.1:</p>

<blockquote>
  <p>The localisation network function floc() can take any form, such as a fully-connected network or a convolutional network, but should include a final regression layer to produce the transformation parameters θ.</p>
</blockquote>

<p>I understand what regression is, but what is meant by a regression layer?</p>
","transformer"