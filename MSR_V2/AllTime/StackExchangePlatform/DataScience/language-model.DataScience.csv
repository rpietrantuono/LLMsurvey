Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"129543","Generating transaction data for a dataset to train on","2024-06-27 22:30:47","","0","19","<machine-learning><dataset><data><supervised-learning><language-model>","<p>My project is to predict what payment option a customer might use depending on various factors on a checkout screen.</p>
<p>For example here are some of the fields I would have</p>
<hr />
<p><em>Variables</em> : <strong>User_Location   Product_Category First_Time_Purchase    Repeat_Purchase Historical_Payment_Method   Payment_Option_Chosen</strong></p>
<p><em>Variable Values</em>: China    Electronics No                   Yes                Wallet                       Wallet</p>
<hr />
<p>Apologies for the bad formatting, was not sure how else to portray this. Those are some of the example fields I'd like to use and create personas of. The issue is the data is not readily available anywhere. An idea I had was to create some sort of system for generating synthetic data about transactional info.</p>
<p>The data has to be teach the model, lets say someone from China wants to buy fashion item, on their android, they will most likely choose x payment method, however they may also choose y payment method. There has to patterns the ML can recognise and learn from. The issue is I have no idea how to generate 100s of these personas that will allow a ML model to properly learn from these patterns. Perhaps fine tune a LLM that could generate rules based on these type of examples.</p>
<p>Is there a method to do this properly ?</p>
<p>Any suggestions or pointers would be greatly appreciated, as its been a problem I've had for a long time now.</p>
","language-model"
"129347","What are the key quality metrics for large language model releases?","2024-06-08 21:15:47","","0","7","<machine-learning><language-model>","<p>I am a first year PhD student working on improving the release practices of Machine Learning Models, especially pre-trained large language models. I want to understand the above concept for a preliminary learning and opinion from the experts.</p>
<p>I have been reading different articles, but still need experts' opinions.</p>
","language-model"
"129339","What is query generation re-ranking method?","2024-06-08 04:39:28","","0","6","<deep-learning><nlp><language-model><information-retrieval>","<p>I am reading up on reranking methodologies that leverage LLMs. Relevant <a href=""https://arxiv.org/pdf/2304.09542"" rel=""nofollow noreferrer"">literature</a>.</p>
<p>One of the methods suggested is query generation
<a href=""https://i.sstatic.net/nSADiKeP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/nSADiKeP.png"" alt=""enter image description here"" /></a></p>
<p>Or, the same methodology from another <a href=""https://www.rungalileo.io/blog/mastering-rag-how-to-select-a-reranking-model#:%7E:text=colbert%2Dv1%2Den-,LLMs%20for%20Reranking,-As%20LLMs%20grow"" rel=""nofollow noreferrer"">source</a>
<a href=""https://i.sstatic.net/JpoqD6u2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JpoqD6u2.png"" alt=""enter image description here"" /></a></p>
<p>The task is to rank the passages according to their relevance to a given passage</p>
<p>Here they suggest giving the LLM the context and asking it to generate a query. How does this help rank the contexts as per their relevance to the given query? We generate a query given all the contexts.. Now what?</p>
","language-model"
"129249","How to find out that a conversation with a chatbot is likely ended","2024-05-31 09:50:56","","0","10","<python><nlp><language-model><chatbot>","<p>I'm working on a ChatBot with <code>Python</code> and <code>langchain</code>, and I'd like to have a metric that I could use to understand how close we are to the completion of a discussion.</p>
<p>So, for instance, that metric should score high if the last sentence of the history was <code>Goodbye</code> or <code>I see that we both agree</code>, or any other that somehow implies that the probability that we'll continue with the same topic is low.</p>
<p>Is there something that I can use on langchain or another library to help me with this?</p>
","language-model"
"129099","Callback handlers in Langchain","2024-05-16 20:42:19","129161","1","42","<python><language-model><llm>","<p>This might be an odd question, but why is there two codes for the class BaseCallbackHandler?</p>
<p><a href=""https://api.python.langchain.com/en/latest/_modules/langchain_core/callbacks/base.html#BaseCallbackHandler"" rel=""nofollow noreferrer"">https://api.python.langchain.com/en/latest/_modules/langchain_core/callbacks/base.html#BaseCallbackHandler</a></p>
<p><a href=""https://python.langchain.com/v0.1/docs/modules/callbacks/"" rel=""nofollow noreferrer"">https://python.langchain.com/v0.1/docs/modules/callbacks/</a></p>
","language-model"
"128492","How is a causal language model correctly fine-tuned?","2024-03-29 11:55:58","","0","39","<nlp><language-model><finetuning>","<p>I want to fine-tune an SLM like Phi-2 through the huggingface API. I am in doubt how to achieve that, because I see two ways to do that and I am wondering which is the correct way.</p>
<p>The task is just a sequence to sequence mapping. For the sake of simplicity let's just assume we do summarization.</p>
<p>There seem to be two possibilities to achieve this, and here I would like some clarification, if that is true and/or if there is a preferred way to do this.</p>
<p><strong>Possibility 1:</strong></p>
<p>Now, to get a head start I was looking around and trying to find scripts and tutorials. I found <a href=""https://medium.com/thedeephub/optimizing-phi-2-a-deep-dive-into-fine-tuning-small-language-models-9d545ac90a99"" rel=""nofollow noreferrer"">this</a> one here, but I am genuinely confused, because it seems to me the author creates one single prompt with input <em>and</em> target sequence formatted in one place. There is no such thing as a label sequence that is expected to predicted but it's much more trained in a self-supervised or auto-encoder fashion. In code he writes the collate function that creates the dataset like this:</p>
<pre><code>def collate_and_tokenize(examples):

    question = examples[&quot;question&quot;][0].replace('&quot;', r'\&quot;')
    answer = examples[&quot;answer&quot;][0].replace('&quot;', r'\&quot;')
    #unpacking the list of references and creating one string for reference
    references = '\n'.join([f&quot;[{index + 1}] {string}&quot; for index, string in enumerate(examples[&quot;references&quot;][0])])

    #Merging into one prompt for tokenization and training
    prompt = f&quot;&quot;&quot;###System:
Read the references provided and answer the corresponding question.
###References:
{references}
###Question:
{question}
###Answer:
{answer}&quot;&quot;&quot;

    encoded = tokenizer(
        prompt,
        return_tensors=&quot;np&quot;,
        padding=&quot;max_length&quot;,
        truncation=True,
        max_length=2048,
    )

    encoded[&quot;labels&quot;] = encoded[&quot;input_ids&quot;]
    return encoded
</code></pre>
<p>During inference, logically, he then proceeds and feeds something like this:</p>
<pre><code>new_prompt = &quot;&quot;&quot;###System: 
Read the references provided and answer the corresponding question.
###References:
[1] For most people, the act of reading is a reward in itself. However, studies show that reading books also has benefits that range from a longer life to career success. If you’re looking for reasons to pick up a book, read on for seven science-backed reasons why reading is good for your health, relationships and happiness.
[2] As per a study, one of the prime benefits of reading books is slowing down mental disorders such as Alzheimer’s and Dementia  It happens since reading stimulates the brain and keeps it active, which allows it to retain its power and capacity.
[3] Another one of the benefits of reading books is that they can improve our ability to empathize with others. And empathy has many benefits – it can reduce stress, improve our relationships, and inform our moral compasses.
[4] Here are 10 benefits of reading that illustrate the importance of reading books. When you read every day you:
[5] Why is reading good for you? Reading is good for you because it improves your focus, memory, empathy, and communication skills. It can reduce stress, improve your mental health, and help you live longer. Reading also allows you to learn new things to help you succeed in your work and relationships.
###Question:
Why is reading books widely considered to be beneficial?
###Answer:
&quot;&quot;&quot;
</code></pre>
<p>where the model is asked to just generate answer. I get that, it makes sense, but I was expecting the following.</p>
<p><strong>Possibility 2:</strong></p>
<p>In the above mentioned tutorial the author just sets the labels equal to the <code>input_ids</code>. I would have done differently, such that I encode the target sequence and have that be my labels. This is how I feel like OpenAI's API works, but who knows what they are doing in the background ...</p>
<hr />
<p>My questions all go in the direction of: what is correct? Is there a <em>correct</em> way? What is prefered? Do both approaches achieve the same thing?</p>
<p>Thank you. Please ask clarifying questions if I wasn't clear somewhere.</p>
","language-model"
"128295","Converting relational database into vector database","2024-03-14 13:57:36","","0","43","<language-model><databases><llm><vector-database>","<p>Is there any open-source tools for converting relational database to vector database to be used in llm applications? Which steps can be taken in the conversion?</p>
","language-model"
"128259","What is system prompt in fine tuning of GPT3.5 for natural language to sql queries","2024-03-12 11:58:46","","0","16","<language-model><sql><gpt><prompt-engineering>","<p>What exactly is a system prompt while finetuning GPT3.5 or a language model in general? How can I build system prompt for the task of converting natural language to SQL queries</p>
","language-model"
"128203","what is the main difference between ROUGE and BLUE?","2024-03-07 11:58:40","","0","17","<nlp><transformer><model-evaluations><language-model><llm>","<p>Both (ROUGE, BLUE) are useful to find the similarity between machine generated summary and reference summary.</p>
<p>what is the main difference?</p>
","language-model"
"128097","What languages llama2 supports?","2024-02-29 14:34:00","128098","0","29","<language-model>","<p>Which languages llama2 supports? I looked at the docs and huggingface but I couldn't find a list. Just it says usage in other languages than English as out-of-scope.</p>
","language-model"
"128094","How can I get the list of pretrained large language models?","2024-02-29 09:35:39","128156","0","39","<nlp><language-model><finetuning><llm>","<p>Is there any place I can get the list of pre-trained large language models in a neat way? Despite the most common ones like gpt, BARD, llama2, which llm do you suggest that can be used for RAG and fine-tuning? Especially I am looking forward multilingual models.</p>
","language-model"
"128083","How to check the license of a LLM for specific use?","2024-02-28 18:15:42","128085","0","51","<language-model><llm>","<p>How to check if a large language model has a license allowing to fine tune the model and then publish it publicly? How can I be sure that I can use and fine-tune a large language model without requiring permission issues?</p>
","language-model"
"126981","How to choose ideal pretrained model for fine-tuning?","2024-02-22 15:07:39","","0","23","<nlp><language-model><finetuning><llm><pretraining>","<p>I started to work with LLMs lately and want to know how people choose their pre-trained models in their fine-tuning tasks? What is the criteria to choose the base model and which factors affect?</p>
","language-model"
"126980","Can I fine tune MedPaLM model","2024-02-22 14:22:59","","0","69","<nlp><language-model><finetuning><llm>","<p>Is it possible to fine-tune MedPaLM and MedPaLM2; Google's llms trained using PaLM specialized for medical domain. Can we fine-tune these models further to get more specialized models?</p>
","language-model"
"126088","Is Machine Reading Comprehension (MRC) outdated?","2023-12-18 14:04:14","","0","42","<machine-learning><language-model>","<p>I recently went through some litterature about knowledge-enhanced language models and found connections with the Machine Reading Comprehension (MRC) task. However, I couldn't find papers more recent than 2020 on this topic. Therefore, I wonder whether this task is now outdated and if so, is there a known reason for that like it has been replaced by a more adapted task?</p>
","language-model"
"124988","How can I leverage machine learning for log analysis?","2023-12-10 08:33:26","","1","440","<machine-learning><nlp><training><language-model><llm>","<p>I am new to data science and trying to find possibilities of using datascience in tasks. I have a set of logs which I want to convert to json. The logs are more or less of same format and I can write a script which parse them or aggregate them but instead of doing it manually I want to use help of machine learning. This thought is also inspired by the fact that at sometime some new log line may come. I guess pre-trained LLM models can identify the context and information from logs. But I am not sure how exactly can ML be used for for this purpose? The question may sound stupid but please pardon.</p>
","language-model"
"124793","Using LLMs for structured data?","2023-11-27 21:33:17","","0","82","<machine-learning><language-model><structured-data>","<p>I've been trying to work with structured data in language models, and it's proving to be quite challenging. I'm confident that with Langchain, I should be able to solve the problem, but I'm not entirely sure which path to take among all the options the library offers.</p>
<p>My issue is as follows: I have data in the form of dictionaries regarding a series of products, for example, laptops. The data looks like this:</p>
<p>{Identifier 1: X, Identifier 2: Y, Value name: Z}</p>
<p>(Several successive dictionaries like this.)</p>
<p>I want to use this series of dictionaries as context, then feed a different dictionary into the Language Model, and have it tell me if the 'Value name' makes sense given Identifiers 1 and 2. An example would be Identifier 1: laptops, Identifier 2: brand, Value name: Lenovo. In this case, it should return affirmative since Lenovo makes sense as a brand. However, if I input 'oranges,' it should return negative.</p>
<p>Any ideas on which library I could use to tackle this problem?</p>
","language-model"
"124780","Assign layers and weights in BERT","2023-11-27 06:42:24","","0","38","<pytorch><transformer><bert><language-model>","<p>I print the weight names and shape of the BERT transformer. Now, I want to assign the printed weight to the layers in the transformers architecture:
<a href=""https://i.sstatic.net/Dtaun.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Dtaun.png"" alt=""enter image description here"" /></a></p>
<p>In the following, I can assign query, key and value:</p>
<p><a href=""https://i.sstatic.net/wfHeW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wfHeW.png"" alt=""enter image description here"" /></a></p>
<p>But, in the print there are attention.output.dense.weight and attention.output.LayerNorm.weight, where can I find it in the architecture of transformer/attention-head?</p>
<p>Further there is an intermediate.dense.weight and there output.dense.weight and output.LayerNorm.weight. Are they parts of &quot;Add &amp; Norm&quot; after the multi-head-attention blocks?</p>
","language-model"
"124733","Purely extractive Language Model","2023-11-24 11:00:36","124735","0","142","<nlp><generative-models><language-model><parsing><search-engine>","<p>Given an email thread, I am trying to extract the body of the most recent email.</p>
<p>I used to do that with rules. Now I am testing Large Language Models (LLM) to see if I they provide a less ad hoc solution.</p>
<p>Mistral-7B-Instruct, for instance, seems to understand the task and provides acceptable outputs most of the time.</p>
<p>However, in some cases, it explains the email rather than just copy/paste the relevant chunk.</p>
<p>I have tried dozens of prompts, for instance:</p>
<pre><code>instruction = 'Given the email thread bellow the dotted line, extract verbatim the body of the most recent (top) message. Remove all headers, footers and disclaimers. In your response, do not add any text that was not present in the original message'
</code></pre>
<p>And tried to prevent hallucinations by setting the following:</p>
<pre><code>    generation_output = model.generate(
        model_inputs,
        do_sample=True,
        temperature=0.0000001,
        top_p=0.0000001,
        top_k=1,
        max_new_tokens=words
        )
</code></pre>
<p>However, in a few cases, the model still adds explanations and/or hallucinates a bit.</p>
<p>My questions are the following:</p>
<ol>
<li><p>Are you aware of any models that could do a better job without fine-tuning? For instance, purely extractive models (as opposed to generative ones).</p>
</li>
<li><p>If generative models are the way to go, is there a way to force the model to just copy/paste?</p>
</li>
</ol>
<p>Best,</p>
<p>Ed</p>
","language-model"
"124642","Open-Source Large Language Models (LLM): Your experience and recommendation","2023-11-17 22:53:07","","0","178","<transformer><language-model><huggingface><llm>","<p>I’m looking for an open-source LLM for a new project. I want to use it for instructions and to fine-tune the model to a specific domain like legal and rights. Some LLMs are open-source, but they didn’t document, on which training data they trained their model. This makes it a bit complicated in my case.</p>
<p>I’m looking for models, that are open-source and the community knows on which datasets the model was trained.</p>
<p>Do you know open-source LLMs like that and do you have experience with them?</p>
<p>Thank you in advance.</p>
","language-model"
"124591","What is the input to an encoder-decoder transformer in next word prediction task?","2023-11-14 22:21:46","124593","0","225","<nlp><transformer><language-model>","<p>I'm trying to understand how encoder-decoder architectures are used, or if they are used at all, for generative tasks that do not require an explicit prompt (ie. machine translation, summarization, etc.).</p>
<p>From my understanding, decoder-only models autoregressively predict the next token in a sequence given its previous predictions. This makes sense, as we can simply keep feeding it tokens already predicted during inference. But how is this done when there is an encoder involved? For machine translation, we have the sequence in the source language to feed to the encoder. Similarly, we can feed it a passage to summarize for summarization. What would we feed the encoder if we simply wanted next word prediction? Do we feed it the sequence we want it to complete? I haven't found any examples of this task being performed. Does this mean that encoder-decoder models aren't needed for this task?</p>
","language-model"
"124584","Why is 0.7, in general, the default value of temperature for LLMs?","2023-11-14 15:47:29","","1","1304","<sampling><language-model><reference-request><parameter><llm>","<p>I have recently read through a lot of documentation and articles about Large Language Models (LLMs), and I have come to the conclusion that <strong>0.7 is, most of the time, the default value for the temperature parameter</strong>.</p>
<p>See a few quick reference examples where the default value is either 0.7 or 0.75:</p>
<ul>
<li><a href=""https://platform.openai.com/docs/api-reference"" rel=""nofollow noreferrer"">https://platform.openai.com/docs/api-reference</a></li>
<li><a href=""https://algowriting.medium.com/gpt-3-temperature-setting-101-41200ff0d0be"" rel=""nofollow noreferrer"">https://algowriting.medium.com/gpt-3-temperature-setting-101-41200ff0d0be</a></li>
<li><a href=""https://rasa.com/docs/rasa/next/llms/llm-intent/"" rel=""nofollow noreferrer"">https://rasa.com/docs/rasa/next/llms/llm-intent/</a></li>
</ul>
<p>However, I am struggling to find any reference that would explain the rationale for using 0.7.</p>
<p>I understand that lower values of the temperature result in more deterministic outputs and that higher values result in more random outputs.</p>
<p>Nonetheless, why is it more recommended to select temperature=0.7 rather than temperature=0.6 or temperature=0.4 for instance?</p>
<p>In contrast, in &quot;GPT-4 Technical Report&quot;, a value of 0.6 is used as the &quot;best-guess&quot; by the authors. See <a href=""https://arxiv.org/pdf/2303.08774.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2303.08774.pdf</a>, p.24.</p>
<p>So my question would boil down to:</p>
<p><strong>- Is it purely empirical or are there either benchmarks, or mathematical equations, which would substantiate the approach of selecting a temperature close to 0.7?</strong></p>
<p><strong>- <em>If</em> it is purely empirical, what were the empirical reasons leading to the adoption of values close to 0.7? (E.g., is it due to the default parameters used in a highly cited paper?, in a highly used library?, etc.)</strong></p>
<p>Thank you</p>
","language-model"
"124402","Systematic way of selecting internet texts for a machine translation corpus / dataset?","2023-11-03 14:25:13","","0","8","<nlp><dataset><language-model><machine-translation><corpus>","<p>I am currently working on a neural machine translation project and want to gather a corpus (or dataset) of internet texts that are written in standard and <a href=""https://en.wikipedia.org/wiki/Plain_language"" rel=""nofollow noreferrer"">plain language</a>. In theory, it certainly makes sense to try to collect all texts and compile them in a research corpus. In practice, I would like to proceed as systematically as possible in order to find at least the relevant or most suitable texts.</p>
<p>It is possible to find simple language texts relatively quickly, which can also be easily aligned. So the problem for me at the moment is not finding data, but rather finding a systematic way that allows me to prioritize the scraping and merging into a dataset. Ideally, the system is backed up by scientific literature.</p>
<p>At the moment I'm a bit stuck and would be happy to hear from you about known approaches, queries for literature research, known similar projects, specific literature or other such things :)</p>
","language-model"
"124334","【NLP】Is there a model or task that determines contextual similarity?","2023-10-30 15:16:50","","0","13","<machine-learning><deep-learning><nlp><language-model>","<p>I am trying to work on an engagement detection task in which I have to determine if a student is engaged in class.</p>
<p>I am looking for an NLP approach where I can calculate the similarity score of a conversation.</p>
<p>Teacher: &quot;What is your favourite animal?&quot;
Student: &quot;Football.&quot;
The model should output a contextual similarity score. It may not always be question and answer between the teacher and the student, but the core idea to determine if student is distracted.</p>
<p>I am thinking of Q&amp;A and semantic similarity but I am not sure which is better or if there is a more specific name for such a task.</p>
","language-model"
"124293","Since LLMs only provide output in text, how can LLMs do 'action' such as search, run code etc?","2023-10-27 05:23:59","","0","25","<language-model><llm>","<p>Since LLMs only provide output in text, I wonder how LLMs can do 'action' such as search, run code etc. I try to delve deep into how langchain agent works and what I got is that the used prompt is something like this</p>
<pre><code>Assistant is a large language model trained by OpenAI.

Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.

Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.

Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.

TOOLS:
------

Assistant has access to the following tools:

&gt; Search: useful for when you need to answer questions about current events
&gt; Calculator: useful for when you need to do arithmetic task

To use a tool, please use the following format:

```
Thought: Do I need to use a tool? Yes
Action: the action to take, should be one of [Search, Calculator]
Action Input: the input to the action
Observation: the result of the action
```

When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:

```
Thought: Do I need to use a tool? No
AI: [your response here]
```

Begin!

Previous conversation history:
{chat_history}

New input: {input}
{agent_scratchpad}
</code></pre>
<p>I don't understand what happen when LLMs say 'Action: Search. How the Search function was called ?</p>
","language-model"
"124273","TFRobertaSequenceClassification for Address Normalization task","2023-10-26 10:34:07","","0","26","<nlp><bert><normalization><language-model><finetuning>","<p>I have dataset with two column: one with faulty addresses, and other with correct addresses. I want to train a model such that, I can use it later for correcting all the incoming faulty addresses.
I have done tokenization, data splitting task for the same, but I can't make my model to start training.</p>
<p>I get GraphExecution error, which points towrads mismatch of dimension.</p>
<p>Exact error:
<strong>logits and labels must have the same first dimension, got logits shape [16,55] and labels shape [880]</strong></p>
<p>I don't know from where these values are coming from, as my X_Train consists of list with each list being length of <strong>45</strong>, and y_train is a list of length <strong>55.</strong> (All are integer values)</p>
<p>As, I am new to this task, any suggestions, comments, concerns, questions are welcome.</p>
<p>Also, I have tried basic ML approach, but the results were quite poor. Hence, please suggest me on the line of this approach only.</p>
<p>Here are some specifications to my approach:</p>
<pre><code>Tokenization: RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;)
Model: TFRobertaForSequenceClassification.from_pretrained(&quot;roberta-base&quot;)
optimizer: Adam
learning_rate = 1e-5
loss: SparseCategoricalCrossentropy
epochs: 10
batch_size: 16
</code></pre>
<p>I think following are the list of models which we can use:</p>
<p><a href=""https://i.sstatic.net/Shp81.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Shp81.png"" alt=""enter image description here"" /></a></p>
<p>github link used: <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/__init__.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/__init__.py</a></p>
<p>TIA</p>
","language-model"
"124197","Harford 2023 model: explicit reference","2023-10-21 09:18:28","","0","19","<nlp><language-model><reference-request>","<p>I'm citing in a paper the Wizard-Vicuna Uncensored 30B model of Hartford (2023). But I don't have an exact bib reference other than various web links for that model. Could anyone help?</p>
","language-model"
"124168","Semantic Scoring and readability for short sentences","2023-10-18 12:29:54","","0","31","<machine-learning><nlp><language-model>","<p>I am working on short sentences for NLP based classification. I wish to make a assessment if a sentence is readable before training the system on it. Now readability scores are not working since readability score indicate how simple a sentence is not how correct a sentence is.</p>
<p>What I am searching for is something that measures(or scores) the readability of a sentence. For example the sentence 'The sun rises in the east' should be given a high score whereas the sentence 'sun the east in rises' should be given a low score.</p>
<p>Is there any python package for this or could someone indicate some direction for it?</p>
","language-model"
"124135","Using OSRM with Langchain","2023-10-16 08:54:38","","0","13","<language-model><chatbot><open-source>","<p>I am trying to develop a chatbot using Langchain, to give me a routing option between 2 points on a custom map data. I am using free APIs from Huggingface for embedding and Ollama model for the question answering model.</p>
<p>If I need to include a way to display or maybe output the route between 2 places on a custom data using langchain. Is it possible to call osrm API within langchain and display some result?</p>
","language-model"
"124118","How to read CSV File into Vector Store","2023-10-14 18:19:59","","0","947","<python><nlp><language-model><csv>","<p>I have a CSV file, and I am using langchain to read it into the vector store FAISS. My question is, since I have a CSV file, is RecursiveTextSplitter required? Put differently, consider the following codes</p>
<pre><code># Code A ------------------------------
myData = CSVLoader(file_path=myCSVFile&quot;)
finalData = myData.load()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap = 10,
    length_function = len,
)

theFile = text_splitter.transform_documents(finalData)

vector_store = FAISS.from_documents(theFile, embedder)
</code></pre>
<p>Compare to following code:</p>
<pre><code># Code B ------------------------------
myData = CSVLoader(file_path=myCSVFile&quot;)
finalData = myData.load()

vector_store = FAISS.from_documents(finalData, embedder)
</code></pre>
<p>Is code A correct or coe B?</p>
","language-model"
"124098","Decreasing the summary length from langchain load_summarize_chain?","2023-10-12 05:41:38","","0","105","<machine-learning><language-model><llm><chatgpt>","<p>How can i reduce the output size of the summarization in langchain map reduce method ?</p>
","language-model"
"123954","Locating base.py when working on Colab","2023-10-02 16:27:22","","0","20","<python><nlp><language-model><colab>","<p>I have faced an error while working with langchain on colab. There is a post on github which recommends changing some configurations in</p>
<pre><code> local / repo find-&gt; langchain/agents/agent_toolkits/pandas/base.py
</code></pre>
<p>Could you please let me know how I can find and configure base.py while working on colab?</p>
","language-model"
"123921","Why do we want to maximize the average log probability in neural language models?","2023-09-30 12:56:07","","0","78","<nlp><language-model><doc2vec>","<p>I am currently trying to understand the Paragraph Vector framework by reading the paper <a href=""https://proceedings.mlr.press/v32/le14.html"" rel=""nofollow noreferrer"">&quot;Distributed Representation of Sentences and Documents&quot;</a> by Quoc Le and Thomas Mikolov but I have troubles following the formal description since my current understanding of Neural Networks is limited to mainly intuition. In the paper, they explain former techniques of learning word vector representations including Neural Language Models. Their formal description of the training task of Neural Language Models is as follows:</p>
<blockquote>
<p>More formally, given a sequence of training words <span class=""math-container"">$w_1 , w_2 , w_3 , ..., w_T$</span> , the objective of the word vector model is to maximize the average log probability <span class=""math-container"">$\frac{1}{T}\overset{T-k}{\underset{t=k}{\sum}}\log p(w_t | w_{t-k}, ..., w_{t+k})$</span></p>
</blockquote>
<p>Unfortunately, they don't explain why the goal is to maximize the average log probability and how they came up with this formula. Also search requests and looking through <a href=""https://jmlr.csail.mit.edu/papers/volume3/bengio03a/bengio03a.pdf"" rel=""nofollow noreferrer"">&quot;A Neural Probabilistic Language Model&quot;</a> by Bengio et al. did not bring me any further.</p>
<p>My understanding of the training task is that given a context, the model predicts the missing word of a context. Moreover, out of all words in the vocabulary the word with the highest conditional probability given the context is selected as the missing word. Therefore I don't understand why this formula does not include some sort of <span class=""math-container"">$\mathrm{argmax}$</span> expression.</p>
<p>Any help on understanding why this is the goal and additional resources on this would be  appreciated.</p>
","language-model"
"123908","How to get Llama-2 Rotary Embeddings?","2023-09-29 11:31:13","","0","319","<nlp><word-embeddings><transformer><language-model><huggingface>","<p>I want to get the Llama-2 rotary embeddings. I do <code>print(model)</code> and get the following output:
<a href=""https://i.sstatic.net/jFu2Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jFu2Z.png"" alt=""enter image description here"" /></a></p>
<p>In the picture I highlight the rotary embeddings.</p>
<p>How can get the rotary embeddings and how can I interpret the output? What means 32x LLamaDecoderLayer and in its round brakets are four layer plus LlamaRotaryEmbeddings?</p>
<p>It's possible to get the embeddings as the first hidden-state <code>hidden_state[0]</code> and I want to know, which hidden-state represents the rotary embeddings.
Am I right, that there are several rotary embeddings?</p>
<p>Thanks in forward.</p>
<p>Best regards.</p>
","language-model"
"123889","Specifying arguments of HuggingFaceHub","2023-09-28 07:48:29","","0","266","<python><nlp><language-model><huggingface>","<p>In this <a href=""https://python.langchain.com/docs/integrations/llms/huggingface_hub"" rel=""nofollow noreferrer"">tutorial</a>, when specifying</p>
<pre><code>llm = HuggingFaceHub(
    repo_id=repo_id, model_kwargs={&quot;temperature&quot;: 0.5, &quot;max_length&quot;: 64}) 
</code></pre>
<p>only repository id is mentioned, without referring to the task that is to be performed (for instance summarization or text generation). However, one may specify something like</p>
<pre><code>dotaks = transformers.pipeline(
    model=repo_id, 
    tokenizer=tokenizer,
    task='text-generation',
    return_full_text=True,
    temperature=0.5,
    max_new_tokens=124,)
</code></pre>
<p>If I try to remove the task it would not work. What is the difference among the above two approaches? How will the top code decide on the appropriate task if not specified?</p>
","language-model"
"123836","What is source_column argument in csv loader?","2023-09-25 06:46:04","123838","1","641","<python><nlp><language-model><csv>","<p>In <a href=""https://python.langchain.com/docs/integrations/document_loaders/csv"" rel=""nofollow noreferrer"">this tutorial</a>, what is the purpose of source_column argument? Does it act like a primary key in Databases? Thanks in advance.</p>
<pre><code>loader = CSVLoader(file_path=&quot;./example_data/mlb_teams_2012.csv&quot;, source_column=&quot;Team&quot;)
data = loader.load()
</code></pre>
","language-model"
"123765","Can LLM fine-tuning be used to improve a language?","2023-09-19 17:11:58","","0","87","<language-model><finetuning><llm>","<p>I'm Danish, and with all the excitement around open LLM models, I'm feeling a little left out.</p>
<p>Take Llama 2, for example - it was trained on a very small Danish dataset. Just enough to learn the words and be terrible at Danish.</p>
<p>My question is, can fine-tuning (with LORA or QLORA) be used to improve the Danish skills of an existing pretrained language model?</p>
<p>Llama 2, for example, knows Danish words, and so (one would hope) perhaps that's why they exposed it to Danish at all?</p>
<p>Full-on pretraining isn't financially realistic for an individual, and (to my knowledge) we don't have a good open model in Danish yet.</p>
<p>We do however have a pretty large open source training data set. But whoever is using that (if anyone) they're not sharing.</p>
","language-model"
"123729","Why is Spacy sentiment score 0.0 for a sentence?","2023-09-16 17:23:30","123734","1","198","<nlp><sentiment-analysis><language-model><spacy>","<p>I'm trying to get a sentence's sentiment score using Spacy and apparently every sentence I pass gets a score of 0.0. Can someone help me understand what's going wrong here?</p>
<pre><code>import spacy

nlp = spacy.load(&quot;en_core_web_sm&quot;)
doc1 = nlp(&quot;This Movie is really Great!&quot;)
doc2 = nlp(&quot;This Movie was the Worst!&quot;)

print(&quot;Sentiment score for 1st Sentence &quot;,doc1.sentiment)
print(&quot;Sentiment score for 2nd Sentence &quot;,doc2.sentiment)

for token in doc1:
    print(token.text, token.pos_, token.dep_)


for token in doc2:
    print(token.text, token.pos_, token.dep_)
</code></pre>
<p>Which gives the output as:</p>
<pre><code>Sentiment score for 1st Sentence  0.0
Sentiment score for 2nd Sentence  0.0
This DET det
Movie PROPN nsubj
is AUX ROOT
really ADV advmod
Great ADJ acomp
! PUNCT punct
This DET det
Movie PROPN nsubj
was AUX ROOT
the DET det
Worst ADJ attr
! PUNCT punct
</code></pre>
<p>Spacy's documentation says this about <code>Doc.sentiment</code>:</p>
<p><a href=""https://i.sstatic.net/XCQAD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XCQAD.png"" alt=""enter image description here"" /></a></p>
","language-model"
"123718","""text"" parameter in pinecone call from langchain","2023-09-15 16:46:35","","1","34","<python><nlp><word-embeddings><language-model><vector-database>","<p>In <a href=""https://python.langchain.com/docs/integrations/vectorstores/pinecone"" rel=""nofollow noreferrer"">this tutorial</a>, I do not understand what &quot;text&quot; refers to</p>
<pre><code>vectorstore = Pinecone(index, embeddings.embed_query, &quot;text&quot;)
</code></pre>
<p>Could you please help?</p>
","language-model"
"123471","coversational AI chatbot using langchain and chatgpt 3.5?","2023-08-30 08:02:35","","-3","96","<nlp><language-model><chatbot><llm>","<p>can we develop end to end hotel booking coversational AI chatbot using langchain and chatgpt 3.5 ?</p>
","language-model"
"123377","Implementation of spBLEU","2023-08-24 12:50:52","123379","2","433","<nlp><model-evaluations><metric><language-model><llm>","<p>I was looking for a way to explore evaluation metrics for language translation models and I came across spBLEU. I can’t find any implementations/examples that would help me start. Does anyone have a lead on what I can pursue?</p>
<p>thanks in advance!</p>
","language-model"
"123053","Why does everyone use BERT in research instead of LLAMA or GPT or PaLM, etc?","2023-08-03 01:11:29","123060","9","12125","<nlp><bert><language-model><gpt><research>","<p>It could be that I'm misunderstanding the problems space and the iterations of LLAMA, GPT, and PaLM are all based on BERT like many language models are, but every time I see a new paper in improving language models it takes BERT as a based an adds some kind of fine-tuning or filtering or something. I don't understand why BERT became the default in research circles when all anyone hears about publicly is GPT-2,3,4 or more recently LLAMA-2. I have a feeling it has something to do with BERT being open-source, but that can't be the whole story. This question might not be specific enough, please let me know. Thanks.</p>
","language-model"
"122863","How quickly can a transformer self-heal if you wipe out one of its layers?","2023-07-21 12:10:04","","0","52","<transformer><language-model>","<p>Say we have a fully-trained <code>N</code>-layer transformer model (encoder-only, decoder-only, or encoder-decoder), with embedding dimension <code>D</code>, trained for <code>E</code> epochs on <code>S</code> training strings.</p>
<p>Then we take one of those layers, layer <code>L</code>, and reset it back to random weights (all of it, the self-attention weight, the FFN weights, even the layer norm parameters). Assume weights in all other layers are frozen.</p>
<p>What I'm wondering is how quickly it can re-learn. Does it need to see all <code>S x E</code> training examples all over again? Or will it be much quicker, perhaps just needing to see 1% or 10% of the original training data? Or maybe it will go the other way, and refuse to train because the frozen weights around it stop it finding a minima??</p>
<p>I'm also curious if where <code>L</code> is in the stack makes much difference, and if <code>N</code> and <code>D</code> are factors, or the result is about the same however big the transformer is.</p>
<p>(By &quot;re-learn&quot; I mean get back to approximately the same loss/perplexity on validation data that the model had before; I do not mean recreate the exact same weights.)</p>
<p>I'm looking for answers that point to existing papers that have done these experiments, or equally your personal experiments. Or even a &quot;no-one has ever tried this, we don't know&quot;, if you think you can say that with confidence.</p>
<p>Aside: I did a test last year of swapping in a different encoder (of different dimension), in an encoder-decoder model, and wiping out the cross-attention weights, but otherwise not touching the rest of the decoder. It acted as if the whole model had been reset to random weights, and seemed to need retraining from scratch, even though 90+% of it was still &quot;trained&quot;. But that was only a quick test. Before trying again I'd like to have a better idea of how much retraining time might be needed. (And it struck me that wiping out a layer rather than all cross-attention weights might be a cleaner experiment to learn from.)</p>
","language-model"
"122394","Smart Selection of Training Data for Fine-tuning Language Models in Small Domains","2023-06-25 23:13:22","122399","2","50","<nlp><dataset><language-model>","<h2>Background</h2>
<p>I am working to make language models (for example, Stanford's Alpaca model) perform well on a new small domain through fine-tuning on domain-specific dataset <span class=""math-container"">$D$</span>.</p>
<p>If the size of <span class=""math-container"">$D$</span> is <span class=""math-container"">$N$</span>, I would like to find a subset <span class=""math-container"">$n \ll N$</span> to fine-tune the model due to my limited computation resource: I could only afford to fine-tune the model for 24 GPU hours but fine-tuning on the entire <span class=""math-container"">$D$</span> will take 2400 GPU hours.</p>
<h2>Question</h2>
<p>Is there any strategy I could select <span class=""math-container"">$n$</span> smartly so that the model I fine-tuned is likely to perform better than if I choose <span class=""math-container"">$n$</span> in alternative ways? Here are two options I could think of:</p>
<ul>
<li>Randomly select <span class=""math-container"">$n$</span> from <span class=""math-container"">$N$</span>.</li>
<li>Measure the quality of each of <span class=""math-container"">$N$</span> in some way and fine-tune the data by selecting those of higher quality ones. I got this idea from curriculum learning (<a href=""https://arxiv.org/abs/2010.13166"" rel=""nofollow noreferrer"">a survey paper</a> in 2020).</li>
</ul>
<h2>Note</h2>
<p>This question has been cross-posted from <a href=""https://stats.stackexchange.com/q/619713/191779"">CrossValidated</a>.</p>
","language-model"
"122322","memory and context in LLM models","2023-06-22 16:28:25","122331","0","1678","<nlp><language-model>","<p>I have a large document and I may need to introduce a large part of it to my llm for insight generation I know that that text can be chunked into parts and with the right prompt I can get what I want with the langchain memory feature but how long can my model remember past conversations?</p>
","language-model"
"122164","LMM Fine Tuning - Supervised Fine Tuning Trainer (SFTTrainer) vs transformers Trainer","2023-06-14 15:54:10","123467","3","2352","<deep-learning><transformer><language-model><huggingface><finetuning>","<p>When should one opt for the Supervised Fine Tuning Trainer (SFTTrainer) instead of the regular Transformers Trainer when it comes to instruction fine-tuning for Language Models (LLMs)? From what I gather, the regular Transformers Trainer typically refers to unsupervised fine-tuning, often utilized for tasks such as Input-Output schema formatting after conducting supervised fine-tuning. There seem to be various examples of fine-tuning tasks with similar characteristics, but with some employing the SFTTrainer and others using the regular Trainer. Which factors should be considered in choosing between the two approaches?</p>
<p>I looking for Fine Tuning a LLM for generating json to json transformation (matching texts in json) using huggingface and trl libraries.</p>
","language-model"
"122103","Further Training a pre-trained LLM","2023-06-12 09:57:03","","7","4579","<transformer><transfer-learning><language-model><pretraining>","<p>My goal is to use the general knowledge and language understanding of a pre-trained LLM and to continue training on a smaller domain specific corpus to improve the model's knowledge on the domain. What is the best practice approach here without running into issues (e.g. catastrophic forgetting)? Here are some points I consider, but not completely sure about them:</p>
<ul>
<li>use last checkpoint of pre-trained LLM and continue training on custom corpus</li>
<li>training policy and procedure is the same as used for pre-training (MLM etc.)</li>
<li>use a very small learning rate</li>
<li>is it possible to load the model in int8 (bitsandbytes) and continue training without breaking it?</li>
</ul>
<p>Does this approach make sense? Has anyone done this before and has some insights?</p>
<p>Any hints are highly appreciated!</p>
","language-model"
"121866","Fine-tuning a pre-trained LLM for question-answering","2023-05-31 12:56:54","","2","1388","<transformer><language-model><huggingface><text-generation><finetuning>","<h3>Objective</h3>
<p>My goal is to fine-tune a pre-trained LLM on a dataset about Manchester United's (MU's) 2021/22 season (they had a poor season). I want to be able to prompt the fine-tuned model with questions such as &quot;How can MU improve?&quot;, or &quot;What are MU's biggest weaknesses?&quot;. The ideal responses would be insightful/logical and +100 words</p>
<h3>Data</h3>
<ul>
<li>I will simply use text from the relevant wiki page as my data: <a href=""https://en.wikipedia.org/wiki/2021%E2%80%9322_Manchester_United_F.C._season"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/2021%E2%80%9322_Manchester_United_F.C._season</a></li>
<li>How should I structure my data? Should it be a list dictionaries where the keys are the questions and the values are the answers (i.e. a list of question-answer pairs), or a long string containing all the text data (for context), or a combination of both?</li>
</ul>
<h3>Notes</h3>
<ul>
<li>I have mainly been experimenting with variations of Google's T5 (e.g.: <a href=""https://huggingface.co/t5-base"" rel=""nofollow noreferrer"">https://huggingface.co/t5-base</a>) which I have imported from the Hugging Face Transformers library</li>
<li>So far I have only fine-tuned the model on a list of 30 dictionaries (question-answer pairs), e.g.: {&quot;question&quot;: &quot;How could Manchester United improve their consistency in the Premier League next season?&quot;, &quot;answer&quot;: &quot; To improve consistency, Manchester United could focus on strengthening their squad depth to cope with injuries and fatigue throughout the season. Tactical adjustments could also be explored to deal with teams of different strengths and styles.&quot;}</li>
<li>Use of this small dataset (list of 30 dictionaries) has given poor results</li>
</ul>
<h3>Further Questions and Notes</h3>
<ul>
<li>Other than increasing the size of my dataset, is my approach sound?</li>
<li>What would you recommend as a minimum number of dictionaries to train/fine-tune the model on?</li>
<li>I am also aware that I can tune the hyperparameters to improve performance, but for now I am more concerned about my general approach being logical</li>
</ul>
","language-model"
"121654","How can models like Mosaic's MPT-7b or Bloombergs BLOOMGPT take in so many tokens?","2023-05-20 04:58:42","121659","1","102","<language-model>","<p>I've read the paper on <a href=""https://arxiv.org/pdf/2108.12409.pdf"" rel=""nofollow noreferrer"">ALiBi</a>, and I understand that these models are biasing the values made in the query/key multiplication.</p>
<p>But from my understanding, when I build the actual model I give it <code>N</code> input nodes. When I train a model I give it vectors of length <code>N</code>. How then at inference can I give it vectors of length greater than <code>N</code>? Am I misunderstanding how the multiplication of key and query works? Can there be keys of any length?</p>
<p>Edit: I guess my question includes, why isn't there a multiplication error when I use longer keys in my inference?</p>
","language-model"
"121605","LLM powered chat bot enhanced by NER","2023-05-18 10:26:08","","1","297","<nlp><named-entity-recognition><language-model>","<ul>
<li>I have been reading on the capabilities of LLM based conversational agents and have been wondering if there is even possibility for any further enhancement with the addition of NER to such system.</li>
<li>If so, in which case could a conversational agents powered by an LLM like say Dolly 2.0 be enhanced by NER?</li>
</ul>
","language-model"
"121446","Can we train the Dolly v-2 model on a large general purpose unlabelled text?","2023-05-11 07:21:52","","0","160","<nlp><language-model><huggingface>","<p>I am familiar with ML and Deep Learning concepts and have had a look at Dolly and even got the pretrained model running on a Jupyter lab notebook on Databricks.</p>
<p>However when I take a look at their training dataset format, they are all in instruction and response format.</p>
<p>My specific question is that if I have a super large dump of general text that is not labelled in form of instruction and response, can I just train Dolly as an autoregressive language model that will take a piece of text as an input to the generate function later once trained, and just generate text ?</p>
<p>Suggestions would be really appreciated. Thanks</p>
","language-model"
"121412","Easy question on autoregressive LLM","2023-05-09 00:06:56","121414","0","73","<nlp><transformer><language-model>","<p>For LLM decoder, how exactly is the K, Q, V for each decoding step?</p>
<p>Say my input prompt is &quot;today is a&quot; (good day).</p>
<p>At t= 0 (generation step 0):
K, Q, V are the projections of the sequence (&quot;today is a&quot;)
Then say the next token generated is &quot;good&quot;</p>
<p><strong>At t= 1(generation step 1):
Which one is true:</strong></p>
<ul>
<li>K, Q, V are the projections of the sequence (&quot;today is a good&quot;)</li>
</ul>
<p>OR</p>
<ul>
<li>K, Q, are the projections of the sequence (&quot;today is a&quot;) , V is the projection of sequence (&quot;good&quot;)?</li>
</ul>
","language-model"
"121408","Were any LLMs trained on Google books?","2023-05-08 20:19:41","124358","3","230","<text><language-model><google><books><corpus>","<p>An important limiting factor on the performance of large language models, is the amount of training text available. Of course, using e.g. the Gutenberg archive of public domain books is an obvious thing to do. But the majority of extant books are in copyright.</p>
<p>Over the last couple of decades, Google has scanned something like forty million books. Most of these are in copyright, so there are legal obstacles to making them available for download. As far as I know, this is the largest collection of high-quality text in existence. They provide online access to it on a very limited basis. I haven't heard of them ever giving or selling a complete copy to anyone; not sure they would have legal permission to do so.</p>
<p>Has any LLM been trained, either by Google or anyone else, on this corpus?</p>
","language-model"
"121200","Passing target text to gpt2 and T5 for fine tuning to learn text generation task","2023-04-27 20:33:02","121201","0","266","<nlp><language-model><gpt><huggingface><t5>","<p>I have text with each line in following format:</p>
<pre><code>&lt;text-1&gt; some text-1 &lt;text-2&gt; some text-2 &lt;text-3&gt; some text-3
</code></pre>
<p>I want fine tune model to learn generate <code>some text-3</code> after reading <code>some text-1</code> and <code>some text-2</code>. In GPT2 and T5 text generation tutorials, we do specify <code>input-ids</code> for target text i.e. labels, but for GPT2 we dont.</p>
<p>For example in <a href=""https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887"" rel=""nofollow noreferrer"">this T5 text generation tutorial</a>, we can find line:</p>
<pre><code>model_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
</code></pre>
<p>But I could not find any such line in these GPT2 text generation examples:</p>
<ul>
<li><p><a href=""https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=6O-8Kr_m8AHE"" rel=""nofollow noreferrer"">huggingtweets demo</a>,</p>
</li>
<li><p><a href=""https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb#scrollTo=L8kjz49JEa-5"" rel=""nofollow noreferrer"">huggingartists demo</a></p>
</li>
<li><p><a href=""https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272"" rel=""nofollow noreferrer"">Finetune GPT2 for text generation</a></p>
</li>
</ul>
","language-model"
"121015","What is purpose of stacking N=6 blocks of encoder and decoder in transformer?","2023-04-18 20:15:40","","2","1531","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.</p>
<p><a href=""https://i.sstatic.net/7p5lu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7p5lu.png"" alt=""enter image description here"" /></a></p>
<p>What is purpose of stacking <span class=""math-container"">$N=6$</span> blocks of encoder and decoder? Does higher blocks represent longer phrases and learns what longer phrases attend to? While bottommost block represent single word and its attention; something like how first layer of CNN represent pixel and deeper layers represent edges and further deeper layers represents shapes (like nose, hand etc.)?</p>
","language-model"
"121014","What does it exactly mean by ""different representation subspaces"" in transformer?","2023-04-18 20:14:34","","1","146","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.</p>
<p>The paper says:</p>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p>
</blockquote>
<p>What does it exactly mean by &quot;different representation subspaces&quot;. Can you give intuitive example in terms of natural language conversation example. For example, in sentence &quot;Jane went to Africa during summer&quot;, query matrix <span class=""math-container"">$Q$</span> correspoding to word &quot;Africa&quot; can comprise of different queries &quot;Who went to Africa?&quot;, &quot;When went to Africa?&quot;. What are &quot;different representation subspaces&quot; here? Or with any other example of your choice?</p>
","language-model"
"121013","How K and V are extracted from encoder output in transformer?","2023-04-18 20:12:33","","1","290","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.  The paper shows following transformer architecture:</p>
<p><a href=""https://i.sstatic.net/dtJ7Y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dtJ7Y.png"" alt=""enter image description here"" /></a></p>
<p>How <span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> is extracted from <span class=""math-container"">$512$</span> dimensional encoder output (which is then fed to second multi head attention in decoder)?</p>
","language-model"
"121012","Understanding dimensions of vectors at various places in transformer architecture","2023-04-18 20:08:22","","1","809","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.  It says following regarding dimensions of different vectors:</p>
<blockquote>
<ul>
<li>The input consists of queries and keys of dimension <span class=""math-container"">$d_k$</span>, and values of dimension <span class=""math-container"">$d_v$</span>.</li>
<li><span class=""math-container"">$MultiHead(Q, K, V ) = Concat(head_1, ..., head_h) W^O$</span> where <span class=""math-container"">$head_i = Attention(QW_i^Q,KW^K_i,VW^V_i)$</span><br />
where <span class=""math-container"">$W_i^Q\in\mathbb{R}^{d_{model}\times d_k}$</span>, <span class=""math-container"">$W_i^K\in\mathbb{R}^{d_{model}\times d_k}$</span>, <span class=""math-container"">$W_i^V\in\mathbb{R}^{d_{model}\times d_v}$</span>, <span class=""math-container"">$W^O\in\mathbb{R}^{hd_{v}\times d_{model}}$</span></li>
<li><span class=""math-container"">$h=8$</span> parallel attention layers or heads</li>
<li><span class=""math-container"">$d_k=d_v=d_{model}/h=64$</span></li>
</ul>
</blockquote>
<p>From these I figured out dimensions of vectors at different position in the transformers model as follows (in red colored text):</p>
<p><a href=""https://i.sstatic.net/3sAvB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3sAvB.png"" alt=""enter image description here"" /></a></p>
<p>I have following doubts:</p>
<p>Is dimension of <span class=""math-container"">$K$</span> (vector of multiple/all keys that current word-query needs to attend to)  <span class=""math-container"">$=d_k$</span>? Or <span class=""math-container"">$d_k$</span> is just for single key? If for single key, then what is the dimension for <span class=""math-container"">$K$</span>? Same is the doubt with <span class=""math-container"">$Q$</span> and <span class=""math-container"">$d_q$</span>. I feel <span class=""math-container"">$Q$</span> is set of all queries that can apply to single word. <span class=""math-container"">$K$</span> is the set of all keys that single word can attend to. If that is the case, then dimensions of <span class=""math-container"">$Q$</span> must be <span class=""math-container"">$d_q\times\text{number of queries to consider}$</span> and <span class=""math-container"">$K$</span> must be <span class=""math-container"">$d_k\times\text{number of keys to attend for each word-query}$</span> But then what is this number of queries and keys?</p>
","language-model"
"121004","Fine-tuned MLM based RoBERTa not improving performance","2023-04-18 12:42:45","","1","1239","<transformer><bert><attention-mechanism><language-model><huggingface>","<p>We have lots of domain-specific data (200M+ data points, each document having ~100 to ~500 words) and we wanted to have a domain-specific LM.</p>
<p>We took some sample data points (2M+) &amp; fine-tuned RoBERTa-base (using HF-Transformer) using the Mask Language Modelling (MLM) task.</p>
<p>So far,</p>
<ol>
<li>we did 4-5 epochs (512 sequence length, batch-size=48)</li>
<li>used cosine learning rate scheduler (2-3 cycles/epochs)</li>
<li>We used dynamin masking (masked 15% tokens)</li>
</ol>
<p>Since the RoBERTa model is finetuned on domain-specific data, we do expect this model to perform better than the pre-trained-RoBERTa which is trained on general texts (wiki data, books, etc)</p>
<p>We did perform some tasks like Named Entity Recognition (NER), Text Classification, and Embedding generation to perform cosine similarity tasks. We did this on both finetuned domain-specific RoBERTa and pre-trained-RoBERTa.</p>
<p>Surprisingly, the results are the same (very small difference) for both models. We did try Spacy models too, but the results are the same.</p>
<p>Perplexity scores indicate that finetuned MLM-based RoBERTa has a minimal loss.</p>
<ol>
<li>Can anyone please help us understand why MLM based model is NOT performing better?</li>
<li>Should we go for more data OR more epochs OR both, to see some effect?</li>
<li>are we doing anything wrong here? Let me know if any required details are missing. I will update</li>
</ol>
<p>any suggestions OR any valuable links addressing these concerns would be really helpful</p>
<p>Huggingface discussion page: <a href=""https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913</a></p>
","language-model"
"120961","dolly v2 - how does it internally learn to follow instructions","2023-04-16 12:18:06","","2","485","<language-model><gpt>","<p>this is more a curiosity query than anything else. The git repo for dolly gives us an easy way to swap, the training dataset , to train custom models, as long as we follow the format. I however have been through tons of content online BUT am unable to find ANY github repo which shows me exactly how the &quot;instruction&quot; and &quot;context&quot; are parsed in order to generate the final response. Here's my best guess</p>
<ul>
<li>use a pre trained model (gpt2 / gptj etc ) and create the mirror architecture. Now copy the weights from the pre trained model, hence initializing your custom model</li>
<li>now pick the question &amp; input text ( aka instruction and context ), convert them using a tokenizer ( choosing any HF based api ) and MOST IMP concatenate the &quot;instrn&quot; and &quot;context&quot; using some sort of separator ( this is what i read on using distilbert to solve QnA datasets )</li>
<li>for the target / answer, we again, tokenize this thing and ensure that the final layer of the custom model outputs the same dimensionality as the temporal dimension of the output. Meaning if the &quot;answer&quot; is curtailed to 128 words, then the penultimate dimension of the output will be 128.</li>
<li>try some version of cross entropy loss to compare these 2 and then we have the usual loss minimization exercise</li>
</ul>
<p>i even read the InstructGPT paper and their git repo is basically an empty shell. Any pointers to git repos which show how this works, will be deeply appreciated.</p>
","language-model"
"120913","Benchmarking LLMs on technical questions","2023-04-14 10:43:23","120915","1","132","<model-evaluations><language-model>","<p>There are several existing benchmark sets to evaluate the performance of large language models on natural-language comprehension tasks, such as CoQA, LAMBDA, HELLASWAG, LogiQA.</p>
<p>I'm interested in benchmarking large language models on technical tasks such as writing code, where questions might consist of something like 'Write a Python program to print the first ten prime numbers, one per line' and the output would be considered correct if feeding it to the Python interpreter does indeed produce the first ten prime numbers.</p>
<p>Are there any existing benchmark sets of that nature yet?</p>
","language-model"
"120504","Can I use LLM to explain codebase?","2023-03-27 00:39:56","120510","2","2437","<nlp><data-mining><word-embeddings><language-model><gpt>","<p>I am a Data Engineer, and I am currently assigned a task to refactor an outdated code and rectify any bugs present. However, I am unable to comprehend the code written in the existing codebase. Furthermore, the developers who worked on this codebase did not provide any documentation. Consequently, I am inquiring if there is a feasible method to convert the entire codebase into an extensive text document. Subsequently, I would like to utilize ChatGPT to translate the codebase into a comprehensive document(very long text, with folder structure tree and code inside src) that I can use to embedding. I do not require an in-depth explanation of the code; rather, I am seeking a more abstract-level understanding, such as the purpose of specific files, the functionality of particular folders, etc.</p>
","language-model"
"119952","What does Codex take as tokens?","2023-03-04 14:47:45","119961","0","194","<machine-learning><neural-network><language-model><tokenization>","<p>The typical default for neural networks in natural language processing has been to take words as tokens.</p>
<p>OpenAI Codex is based on GPT-3, but also deals with source code. For source code in general, there is no corresponding obvious choice of tokens, because each programming language has different rules for tokenizing. I don't get the impression Codex uses a separate tokenizer for each language.</p>
<p>What does it take as tokens?</p>
","language-model"
"118602","Data Preparation for next word prediction","2023-02-18 06:21:10","","0","222","<nlp><data-cleaning><multiclass-classification><language-model><ai>","<p>In most places, I have seen that when preparing the training data and label for next-word prediction from the corpus one uses a fixed window size say of length 4, and then scans the subsequences of length 4 as X and the next token as y.</p>
<p>For example: Consider this sentence <code>&quot;The quick brown fox jumps over the lazy dog&quot;</code> and a window of size say 4. Then my training data looks something like this as (X, y) pair</p>
<pre><code>[&quot;The quick brown&quot; , &quot;fox&quot;], [&quot;quick brown fox&quot;, &quot;jumps&quot;], [&quot;brown fox jumps&quot;, &quot;over&quot;], .....
</code></pre>
<p>I have the following doubts.</p>
<ol>
<li>When we train a language model over the data it expects the sequence of length 4, but suppose a sentence only contains 2 words say <code>&quot;quick brown&quot;</code> and I need to predict the next word <code>&quot;fox&quot;</code> I know we can pad to sequence of length 4 but my doubt is will model do any good with a sequence of shorter length if it's trained on the fixed sequence of length 4?</li>
<li>Is it a good idea to have all subsequences of length say from 1 to 4 as training data and pad the shorter ones to a maximum length which is 4 in this case? One problem I see is the issue of the underrepresentation of larger lengths and the overrepresentation of smaller lengths.</li>
</ol>
","language-model"
"118371","Some answers given by ChatGPT are just beyond ridiculous, what could be the reasons?","2023-02-08 08:52:56","","1","149","<nlp><language-model><gpt>","<p>Some answers given by ChatGPT are just beyond ridiculous, especially in Chinese (I am Chinese so I ask ChatGPT in both Chinese and English). Here is an example,</p>
<p><a href=""https://i.sstatic.net/GxuvN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/GxuvN.png"" alt=""castration"" /></a></p>
<p>The last answer</p>
<blockquote>
<p>阉割政策是满清末期中国强制对男性实施阉割的政策。这一政策是为了限制人口增长和减少国家财政负担，但是它对受害者造成了巨大的心理和生理影响。阉割政策也破坏了中国传统文化中对男性身份和角色的认识，并对整个社会造成了深远的影响。该政策在1911年的中华民国成立后终止，但对中国文化和历史的影响一直持续到今天。</p>
</blockquote>
<p>which means</p>
<blockquote>
<p>The castration policy was a policy of mandatory castration of men in
China in the late Qing Dynasty. This policy was intended to limit
population growth and reduce the financial burden on the state, but it
had a huge psychological and physical impact on the victims. The
castration policy has also undermined traditional Chinese cultural
understandings of masculinity and roles and has had a profound impact
on society as a whole. The policy ended after the founding of the
Republic of China in 1911, but its influence on Chinese culture and
history continues to this day.</p>
</blockquote>
<p>That is just absurd! And I have many examples like that.</p>
<p>Apparently, its Chinese corpus has something to do with it. What other things have gone wrong?</p>
","language-model"
"118356","Which Publicly Accessible Large Language Models are Very Similar to OpenAI's ChatGPT?","2023-02-07 18:27:20","","3","179","<nlp><language-model><gpt>","<p>What other large language models exist or will soon exist that are VERY similar to OpenAI's ChatGPT in the sense of being fine-tuned or otherwise specifically created for conversational tasks including question answering? Such models can be free to use or require subscription. I'm preferably looking for models that can give companies API access. I am interested in models with fairly high quality question answering performance in terms of accuracy and not having too many &quot;hallucinations&quot;.</p>
<p>Here are some examples I am currently familiar with:</p>
<ul>
<li><strong>InstructGPT</strong> from OpenAI: <a href=""https://openai.com/blog/instruction-following/"" rel=""nofollow noreferrer"">https://openai.com/blog/instruction-following/</a></li>
<li><strong>OpenAssistant</strong> from LAION: <a href=""https://github.com/LAION-AI/Open-Assistant"" rel=""nofollow noreferrer"">https://github.com/LAION-AI/Open-Assistant</a></li>
<li><strong>ChatSonic</strong> from WriteSonic: <a href=""https://writesonic.com/chat"" rel=""nofollow noreferrer"">https://writesonic.com/chat</a></li>
<li><strong>Jasper Chat</strong>: <a href=""https://www.jasper.ai/chat"" rel=""nofollow noreferrer"">https://www.jasper.ai/chat</a></li>
<li><strong>Google Bard</strong> (Google's soon to be released rival to ChatGPT): <a href=""https://blog.google/technology/ai/bard-google-ai-search-updates/"" rel=""nofollow noreferrer"">https://blog.google/technology/ai/bard-google-ai-search-updates/</a></li>
<li><strong>ChatGenie</strong> via WriteCream: <a href=""https://www.writecream.com/chatgenie/"" rel=""nofollow noreferrer"">https://www.writecream.com/chatgenie/</a></li>
<li><strong>YouChat</strong> from you.com: <a href=""https://www.you.com/chat"" rel=""nofollow noreferrer"">https://www.you.com/chat</a></li>
<li><strong>Perplexity AI</strong>: <a href=""https://www.perplexity.ai"" rel=""nofollow noreferrer"">https://www.perplexity.ai</a></li>
<li><strong>Bing Chat</strong> (literally uses ChatGPT): <a href=""https://www.bing.com/"" rel=""nofollow noreferrer"">https://www.bing.com/</a></li>
</ul>
","language-model"
"118338","How Does the Reward Model in ChatGPT Calculate Losses?","2023-02-06 21:08:21","","4","297","<reinforcement-learning><loss-function><language-model><ranking><loss>","<p>Reading the InstructGPT paper(which seems to be what ChatGPT was built off of), I found this equation for the reward function. <a href=""https://i.sstatic.net/Qztv2.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qztv2.png"" alt=""enter image description here"" /></a></p>
<p>However, I'm struggling to understand how this equation is used to translate rankings between different response to a scalar.</p>
<p>For example, if I have four generated responses, A, B, C, and D, and with D being the best response, with D&gt;C&gt;B&gt;A, then how is the loss calculated? How are relative scores converted to absolute losses.</p>
","language-model"
"118273","Specifics about ChatGPT's Architecture","2023-02-03 16:47:49","","1","7132","<nlp><language-model><gpt>","<p>Does anyone know of reliable sources that have written about the architecture of OpenAI's ChatGPT - specifically regarding the following?:</p>
<ul>
<li><p>Number of hidden layers</p>
</li>
<li><p>Number of attention heads</p>
</li>
<li><p>Dimensions of its hidden layers</p>
</li>
<li><p>Sequence length in terms of number of tokens</p>
</li>
<li><p>Number of parameters in the version of the model currently free for public preview. My understanding is there are multiple versions of ChatGPT.</p>
</li>
</ul>
<p>Please note: I'm familiar with GPT-3 already. However, I am not certain that ChatGPT's architecture is the same. Open to contrary evidence, if any.</p>
","language-model"
"118260","ChatGPT's Architecture - Decoder Only? Or Encoder-Decoder?","2023-02-03 08:57:21","","17","20872","<nlp><language-model><gpt>","<p>Does ChatGPT use an encoder-decoder architecture, or a decoder-only architecture? I have been coming across Medium and TowardsDataScience articles suggesting that it has an encoder-decoder architecture (see sources below):</p>
<p>-- <a href=""https://medium.com/@sreedevr/chatgpt-architecture-explained-7fc0cbb7426c"" rel=""noreferrer"">https://medium.com/@sreedevr/chatgpt-architecture-explained-7fc0cbb7426c</a></p>
<p>-- <a href=""https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286"" rel=""noreferrer"">https://towardsdatascience.com/how-chatgpt-works-the-models-behind-the-bot-1ce5fca96286</a></p>
<p>-- <a href=""https://leoneperdigao.medium.com/chatgpt-a-deep-dive-1feade9c4d77"" rel=""noreferrer"">https://leoneperdigao.medium.com/chatgpt-a-deep-dive-1feade9c4d77</a></p>
<p>I even asked ChatGPT myself whether it uses an encoder and decoder, or just a decoder, and it told me it has both an encoder and decoder. Here is one of its responses to the question:</p>
<p><em>&quot;As a language model, ChatGPT does not have a decoder-only architecture, it has an encoder-decoder architecture. Having an encoder-decoder architecture enables language models like ChatGPT to process input sequences and generate outputs based on that input. The encoder processes the input sequence and generates a set of hidden states, which are then used by the decoder to generate the output sequence.
Having both an encoder and a decoder allows ChatGPT to handle a variety of tasks that involve generating text based on input text, such as language translation, summarization, and answering questions. Having an encoder-decoder architecture allows ChatGPT to maintain context and consistency in the generated output, which is important for many NLP tasks.
It is possible to train a decoder-only language model, but it would be limited in its ability to generate coherent text as it would not have access to the context provided by the encoder.&quot;</em></p>
<p>However, I have been under the definite impression for quite some time now that GPT-3 (from which ChatGPT was in part derived) is a decoder-only model. And I take with a grain of salt ChatGPT's explanation of its own architecture given that it seems prone to generating incorrect answers sometimes. Also, with the huge fanfare of ChatGPT and the potential for misinformed authors writing about the model, I wonder if anyone knows of a reliable source that can clarify this question.</p>
<p>Thanks</p>
","language-model"
"118210","Predicting a next word from a sentence of a different lenght than seen in training","2023-02-01 11:56:12","","1","208","<deep-learning><nlp><transformer><language-model>","<p>I am building a custom Decoder-only transformer model, which is being trained on the task of Next Word Prediction. The training procedure is analogous to that of chat GPT models - the input to the model is a sentence of length K (say K=30) and the target is this sentence shifted one to the right, e.g.:</p>
<p>&quot;I would like a cup of&quot; - input</p>
<p>&quot;would like a cup of tea&quot; - output</p>
<p>If I train my model on sentences of a specified lenght, say K=30, how will it perform in inference mode when it is provided much shorter sentences, say of length 3?</p>
","language-model"
"118127","Is there any differences between using text-davinci-003 with the Azure API vs. with the OpenAI API?","2023-01-28 21:40:20","","0","259","<nlp><language-model><azure-ml><gpt><api>","<p>I wonder whether there are any differences between using text-davinci-003 with the Azure API vs. with the OpenAI API.</p>
","language-model"
"117619","ChatGPT with multilingual language","2023-01-08 15:23:10","120945","1","142","<nlp><language-model>","<p>How it can answer with multilingual langauge ?</p>
<p>Maybe it's because it use GPT-3.5, since their dataset has Common Crawl dataset, which has more than 40+ languages.</p>
<p>And as I found that InstructGPT's paper shows that GPT-3 can handle this task as well, but it needs to use more careful prompts.</p>
<p>Maybe I should read the paper &quot;PaLI: A Jointly-Scaled Multilingual Language-Image Model&quot; that has similar work with multilingual language model's work.</p>
","language-model"
"117526","Seeking Example CLIP Model Code Allowing Me to Use a Pretrained Model and Go Directly to Inference Without any Additional Training","2023-01-05 00:54:04","","1","302","<nlp><computer-vision><language-model>","<p>I have been endlessly searching for open-source code (including from OpenAI themselves) which would allow me to take a pretrained CLIP model image encoder (e.g., the ViT B/32) and then solely based on the weights from the pretrained image encoder directly attempt to infer either a corresponding text description &quot;label&quot; from an image I feed the encoder, or vice-versa, will allow me to provide it a prompt after which it will return an image based on that prompt.</p>
<p>Is this even possible without taking some additional dataset of image-text pairs to further train the pretrained CLIP image encoder on?  All the code examples I have come across provide you the pretrained image encoder - but then require you to do MORE training using an extra dataset that (presumably) was never used during pretraining. Some of these code examples then allow you to provide a text prompt, in response to which the model will return an image (from the dataset you fed it during the training process) which aligns with your prompt.</p>
<p>I.e., I don't want to do any extra training. I just want the pretrained image encoder, then provide that image encoder with 1 image, and then have it return to me a matching text description of it. Alternatively, provide the pretrained (not re-trained) image encoder with a text prompt, and have it return to me an image that it was originally pretrained on (or at least an image that it was not re-trained on).</p>
<p>Some examples of code that <strong>ARE NOT</strong> what I am looking for (although very well done) include the following:</p>
<p><a href=""https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/.ipynb_checkpoints/OpenAI%20CLIP%20Simple%20Implementation-checkpoint.ipynb"" rel=""nofollow noreferrer"">https://github.com/moein-shariatnia/OpenAI-CLIP/blob/master/.ipynb_checkpoints/OpenAI%20CLIP%20Simple%20Implementation-checkpoint.ipynb</a></p>
<p><a href=""https://towardsdatascience.com/quick-fire-guide-to-multi-modal-ml-with-openais-clip-2dad7e398ac0"" rel=""nofollow noreferrer"">https://towardsdatascience.com/quick-fire-guide-to-multi-modal-ml-with-openais-clip-2dad7e398ac0</a></p>
<p><a href=""https://colab.research.google.com/github/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb</a></p>
<p>Thank you.</p>
","language-model"
"117444","What size language model can you train on a GPU with x GB of memory?","2023-01-02 01:14:52","118875","8","10155","<nlp><gpu><language-model><memory>","<p>I'm trying to figure out what size language model I will be able to train on a GPU with a certain amount of memory. Let's for simplicity say that 1 GB = 10<sup>9</sup> bytes; that means that, for example, on a GPU with 12 GB memory, I can theoretically fit 6 billion parameters, given that I store all parameters as 16-bit floats. However, in order to use a language model, you typically also need space for storing the input text and the activations of the current layer (and maybe also of the previous layer), and if you are going to train the model, you will typically need space to store the activations of all layers in order to be able to do backpropagation, and if you use an optimizer like Adam, you need space to store the running mean of the partial derivatives (of the loss function with respect to the various parameters, or in other words, the gradient), as well as the running mean of the squares of the partial derivatives.</p>
<p>So, given this complication, could someone tell me what size language models (that is, how many parameters) I will be able to train on a GPU with</p>
<ol>
<li>10 GB of memory (RTX 3080 10 GB)?</li>
<li>12 GB of memory (RTX 3080 12 GB and RTX 3080 Ti)?</li>
<li>16 GB of memory (RTX 4080)?</li>
<li>24 GB of memory (RTX 3090 and RTX 3090 Ti)?</li>
</ol>
<p>For example, Tim Dettmers mentions in <a href=""https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/#When_do_I_need_11_GB_of_Memory"" rel=""nofollow noreferrer"">his blog</a> that you should have at least 24 GB of memory if you do research on transformers. I'm guessing this translates roughly to a language model of a certain size.</p>
","language-model"
"116642","What are MLM and NSP models actually used for after they've been trained?","2022-12-01 15:24:23","116647","-1","350","<machine-learning><deep-learning><nlp><bert><language-model>","<p>I am a Python programmer working with deep learning nets and I have recently built my own language models as well as I have fine-tuned popular models like BERT. MY question is - after these models have been successfully trained, what are they used for? I understand that masked-language models can predict what the masked word is, but what is the point? What is a real-world application of this model? The same question goes for next-sentence prediction models - what is a real-world application?</p>
<p>Thank you.</p>
","language-model"
"116016","Sentiment Analysis models trained on articles / alternative data","2022-11-09 07:20:22","","1","56","<nlp><dataset><language-model>","<p>For a <em>6 class sentence classification task</em> (emotion), I have a list of sentences where I retrieve the sentiment using a language model that was trained on Tweets <a href=""https://github.com/VinAIResearch/BERTweet"" rel=""nofollow noreferrer"">(bertweet)</a>.</p>
<p>It works fine for simplistic sentences where the sentiment is also obvious (someone died, someone won something, someone was afraid of something, etc). However, when applying it to articles, it shows uncontrollable behavior.</p>
<p>Two examples of the <code>sadness</code> class:</p>
<pre><code>How Your Family Can Volunteer During the Pandemic: 99% probability of sadness
There was a massacre in Bosnia where many were slaughtered: 96% probability of sadness
</code></pre>
<p>I have tried removing the softmax to break down the probabilities into absolute values in order to see if there's a difference there, but it seems that it is marginal and the first sentence again is &quot;sadder&quot; than the second one about the massacre.</p>
<p>There are many more such examples for all the other classes. Is there any model that is trained on articles? Possibly click bait titles and the kinds?</p>
","language-model"
"115554","How Exactly Does In-Context Few-Shot Learning Actually Work in Theory (Under the Hood), Despite only Having a ""Few"" Support Examples to ""Train On""?","2022-10-24 23:26:44","","7","2448","<nlp><computer-vision><language-model><gpt><deepmind>","<p>Recent models like the GPT-3 Language Model (Brown et al., 2020) and the Flamingo Visual-Language Model (Alayrac et al., 2022) use in-context few-shot learning. The models are able to make highly accurate predictions even when only presented with a &quot;few&quot; support examples. See diagram below (from Brown et al., 2020).</p>
<p><a href=""https://i.sstatic.net/zgZ8I.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zgZ8I.png"" alt=""enter image description here"" /></a></p>
<p>Yet, it is unclear to me how these models theoretically work behind the scenes, and why they perform so well. The explanation appears to be that few-shot learning works because the model looks at the task description, then looks at the support examples (which are successful examples of how the given task can be fulfilled), and then based on the model's understanding of what the assigned task is and its understanding of the examples given of how the task could be successfully fulfilled it is then able to understand what it is supposed to predict based on the prompt.</p>
<p>Generally speaking, the more support examples the model sees at inference time, the better it will perform (but there is a point at which continuing to add further support examples does not increase performance). However, given that traditional machine learning models need to train on thousands of examples, it would seem unlikely that a model could really fulfill a task just based on a few examples.</p>
<p><strong>My Questions:</strong></p>
<ul>
<li><p>I understand that these models are built on huge pre-trained Language Models or Vision-Language Models having billions of parameters. But is there a commonly understood explanation of how these models are actually able to work (e.g., mathematical intuition) beyond what I have described?</p>
</li>
<li><p>Since these specific models (GPT-3 and Flamingo use &quot;in-context learning,&quot; which I understand to be the same as &quot;meta-learning,&quot; is it the case that what is actually happening in these models is that the massive pre-trained language and/or vision models they are built on are able to learn many <strong>tasks</strong>, and that consequently at inference time the model is able to learn from the few-shot prompt it is given what the new task being asked of it is, and <strong>also</strong> is able to learn the image/text query presented to it at inference time because it has been pre-trained on massive amounts of examples it can refer back to?</p>
</li>
<li><p>And is there a commonly accepted explanation of why these models actually work so well? Or are these three questions still a matter of debate among ML scholars?</p>
</li>
</ul>
","language-model"
"115497","Recommendations of NLP for classifying sentance into tense forms","2022-10-23 08:40:52","","1","28","<nlp><language-model><stanford-nlp>","<p>I have a dataset of tweets. I have to classify each tweet into it's tense forms like whether it's about past, present or future. So for that can you please recommend any pretrained NLP model or method for this task?</p>
","language-model"
"115453","Do large pretrained language models already ""know"" about NLP tasks?","2022-10-21 14:09:46","","5","93","<nlp><transformer><language-model>","<p>Nowadays the state-of-the-art in NLP is to finetune a large pretrained language model such as BERT/GPT etc. on specfic tasks. These language models are pretrained on a huge amount of data and then basically evaluated on popular labeled datasets published for e.g. Question Answering, Machine Translation <a href=""http://nlpprogress.com/"" rel=""noreferrer"">etc.</a>. As those datasets became the de facto default of evaluating these model, those datasets have been published over and over again on various websites. Used and reused for people that are building their own small model etc. So basically these datasets (train and test data) including their e.g. label in classification tasks or the answer in Q/A tasks &quot;stray&quot; around in the internet. So now when training a <em>new</em> large language model (with a novel architecture) it is fed with text data that is often scraped from the internet as well. Wouldn't it be possible that in the training phase of these LMs, the networks have already seen this exact data (and learned on its co-occurence) which they are evaluated on later on? This would basically defeat the purpose of the evaluation as the test data already leaked into the process of pretraining the language models. Are there any prefiltering steps happening in pretraining these models such that this doesn't happen? And secondly, even if the network has seen the exact test data with e.g. test set question+answer among billions of other textual data, would it even pick up on that or is it just too much data anyways for the model to adjust the weights accordingly and &quot;remembering&quot; these exact datapoints.</p>
","language-model"
"115429","What explains T5's recent resurgence?","2022-10-20 19:35:43","120944","2","43","<nlp><language-model><social-network-analysis>","<p>I read on <a href=""https://towardsdatascience.com/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929"" rel=""nofollow noreferrer"">https://towardsdatascience.com/choosing-the-right-language-model-for-your-nlp-use-case-1288ef3c4929</a>:</p>
<p><a href=""https://i.sstatic.net/6cI3W.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/6cI3W.png"" alt=""enter image description here"" /></a></p>
<p>I find the curve for T5 to be particularly interesting. What explains its recent resurgence?</p>
","language-model"
"115182","How are Learned Latent Arrays for the Perceiver Resampler in DeepMind's Flamingo Vision-Language Model Actually Calculated? By which Technique?","2022-10-13 00:31:31","","1","543","<deep-learning><nlp><computer-vision><language-model><deepmind>","<p>In <strong>&quot;Flamingo: a Visual Language Model for Few-Shot Learning&quot;</strong> (Alayrac et al. 2022) <a href=""https://arxiv.org/abs/2204.14198"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2204.14198</a> DeepMind makes use of &quot;learned latent queries&quot; in their &quot;Perceiver Resampler&quot; to ensure that parameters do not scale quadratically the way they do with Transformers. The authors cite the DeepMind article <strong>&quot;Perceiver: General Perception with Iterative Attention&quot;</strong> (Jaegle et al., 2021) <a href=""https://arxiv.org/abs/2103.03206"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2103.03206</a> as inspiration for their creation of Perceiver Resamplers. Perceivers from the Jaegle et al. article involve &quot;learned latent queries&quot; (i.e., queries from learned latent arrays) that cross-attend to image feature-based keys and values. My understanding is that these learned latent arrays are a reduced dimensional representation of the visual feature arrays that are the outputs of the Vision Encoder. However, the Flamingo paper does not explain how the learned latent array is actually computed from the original visual feature array from the Vision Encoder.</p>
<p>In terms of the Perceiver from Jaegle et al., the authors seem to hint that learned latent arrays may be created through some kind of clustering algorithm. They state, &quot;The model can also be seen as performing a fully end-to-end clustering of the inputs with latent positions as cluster centres, leveraging a highly asymmetric cross-attention layer&quot; (pg. 3). But if they use a clustering algorithm of some kind to produce the learned latent arrays, as far as I can see they do not explain how exactly such an algorithm could be reproduced for use in code, and so they leave it somewhat to the imagination to figure out.</p>
<p>I have 2 questions:</p>
<p><strong>1)</strong> Are these learned latent arrays learned from the visual features 𝑋𝑓 that come from Flamingo's Visual Encoder? If not, where are they being learned from?</p>
<p><strong>2)</strong> If so, how exactly (in a way that I might try to replicate the process) are learned latent arrays calculated from visual feature arrays that are outputs from the Vision Encoder?</p>
<p>Thank you for your help.</p>
<p>Source for image below (Alayrac et al. 2022, pg. 11):
<a href=""https://i.sstatic.net/omxGa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/omxGa.png"" alt=""enter image description here"" /></a></p>
","language-model"
"113490","Model for binary classification of links as ""Article"" or ""Other""","2022-08-13 20:01:29","","0","25","<classification><text-classification><language-model><gpt>","<p>I'm creating a web crawler which must:</p>
<ol>
<li>Fetch a web page.</li>
<li>Parse all <code>&lt;a&gt;</code> tags with hrefs on the page.</li>
<li>Classify the tags as either <strong>article</strong> (Meaning the link refers to a news article) or <strong>other</strong> (Refers to anything other than a news article).</li>
</ol>
<p>I have created several working versions of this using:</p>
<ul>
<li>GPT-3</li>
<li>Jurassic-1</li>
<li>BigScience Bloom</li>
</ul>
<p>However, I can't help but think that using these large language models might be overkill in terms of model size and certainly of expense. I know I can use smaller language models (GPT-J and Bloom variants). Are there any models besides large language models which might be better suited to this sort of classification task?</p>
<p>My intuition is that there might be but consider the enormous range of possible links and anchor texts to discern from.</p>
","language-model"
"113177","Do I need training data in multiple languages for a multilingual transformer?","2022-08-02 09:36:49","","0","55","<machine-learning><nlp><transformer><language-model><huggingface>","<p>I am attempting to train a transformer which can categorize sentences into one of n categories. This model should be able to work with a number of different languages - English and Arabic in my case.</p>
<p>Do I need to have labelled training data in both English and Arabic to fine tune a pretrained transformer, such as <a href=""https://huggingface.co/docs/transformers/model_doc/bloom"" rel=""nofollow noreferrer"">BLOOM</a>, or can I fine tune the model using only English samples, and then the model should also work well on Arabic samples for my classificaiton task, since the fine tuning only trained the classification head?</p>
<p>My thoughts are that the pretraining of this model should allow it to transform the same input texts in English and Arabic to the same (or similar) embedding, which the classification head would have learned to then predict these embeddings accurately through the fine tuning.</p>
","language-model"
"112833","Loss on whole sequences in Causal Language Model","2022-07-20 15:20:14","112837","1","1812","<loss-function><sequence><language-model><gpt><cross-entropy>","<p>I'd like to know, from an implementation point of view, when training a Causal Transformer such as GPT-2, if making prediction on whole sequence at once and computing the loss on the whole sequence is something standard ?</p>
<p>When going across examples that vulgarize what happen during training, we have examples like this:</p>
<p><a href=""https://i.sstatic.net/zH5t4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zH5t4.png"" alt=""enter image description here"" /></a></p>
<p>Which suggests that we mask at a certain token in the sequence and make a prediction, then compute the loss for this single token, so the loss would take data of shape <code>(batch_size, num_classes)</code>.</p>
<p>However if I'm correct, since we're talking about causal models, we could predict all tokens at once because the model can only attend to what's on the left of the sequence (and can't attend on the right, so it can't &quot;cheat&quot;), apply the loss on data that would have the shape <code>(batch_size, sequence_length, num_classes)</code> where <code>sequence_length</code> is computed in a single forward pass. And so speedup the training.</p>
<p>Am I correct ? If so, do you know famous repos that do this ? And if not, why would it be wrong ?</p>
<p>Thanks.</p>
","language-model"
"112022","Which format is preferrable to publish book dataset (plain or preprocessed)?","2022-06-21 21:29:13","112033","1","22","<dataset><language-model>","<p>When I decide to publish collection of book texts as a dataset, should I do some preprocessing first or should I publish &quot;plain texts&quot;?</p>
<p>For example, <a href=""https://huggingface.co/datasets/bookcorpus"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/bookcorpus</a> is published as a collection of sentences (so basic preprocessing was done), but <a href=""https://huggingface.co/datasets/bookcorpusopen"" rel=""nofollow noreferrer"">https://huggingface.co/datasets/bookcorpusopen</a> is published with raw texts.</p>
","language-model"
"108981","A multi label text classification problem","2022-03-11 19:32:21","109040","2","88","<nlp><multiclass-classification><text-classification><language-model>","<p>I'm looking to solve a multi label text classification problem but I don't really know how to formulate it correctly so I can look it up.. Here is my problem :</p>
<p>Say I have the document <code>&quot;I want to learn NLP. I can do that by reading NLP books or watching tutorials on the internet. That would help me find a job in NLP.&quot;</code></p>
<p>I want to classify the sentences into 3 labels (for example) <em>objective</em>, <em>method</em> and <em>result</em>. The result would be :</p>
<pre><code>objective : I want to learn NLP

method : I can do that by reading NLP books or watching tutorials on the internet.

result : That would help me find a job.
</code></pre>
<p>As you would have noticed, it's not a classical classification problem, since the classification here depends on the document structure (unless I'm wrong?)</p>
<p>Any idea of the key words to better describe the problem ? or how I might solve it ?</p>
<p>Many thanks!</p>
","language-model"
"108032","Understanding Kneser-Ney Formula for implementation","2022-02-09 17:18:59","","3","62","<nlp><mathematics><language-model><ngrams>","<p>I am trying to implement this formula in Python</p>
<p><span class=""math-container"">$$ \frac{\text{max}(c_{KN}(w^{i}_{i-n+1} - d), 0)}{c_{KN}(w^{i-1}_{i-n+1})} + \lambda(c_{KN}(w^{i-1}_{i-n+1})\mathbb{P}(c_{KN}(w_{i}|w^{i-1}_{i-n+2})$$</span></p>
<p>where</p>
<p><span class=""math-container"">$$
\mathrm{c_{KN}}(\cdot) = \begin{cases}
    \text{count}(\cdot) &amp; \text{for the highest order } \\ % &amp; is your ""\tab""-like command (it's a tab alignment character)
    \text{continuationcount}(\cdot) &amp; \text{otherwise.}
\end{cases}
$$</span></p>
<p>Following this link <a href=""https://medium.com/@dennyc/a-simple-numerical-example-for-kneser-ney-smoothing-nlp-4600addf38b8"" rel=""nofollow noreferrer"">here</a> I was able to understand how to implement the first half of the equation namely</p>
<p><span class=""math-container"">$$\frac{\text{max}(c_{KN}(w^{i}_{i-n+1} - d), 0)}{c_{KN}(w^{i-1}_{i-n+1})} $$</span></p>
<p>but the second half specifically at the moment the <span class=""math-container"">$\lambda(c_{KN}(w^{i-1}_{i-n+1})$</span> term is confusing me. The author in the link states that</p>
<p><span class=""math-container"">$$\lambda(w_{i-1}) = \frac{d}{c(w_{i-1})}\left|\{w:c(w_{i-1},w)&gt;0\}\right|$$</span></p>
<p>but when we are in the highest order the discounting factor <span class=""math-container"">$d$</span> is zero. The author goes on to say &quot;the denominator in the fraction is the frequency of the semifinal word in the special case of a 2-gram, but in the recursion scheme we are developing, we should consider the whole string preceding the final word (well, for the 2-gram case, the semifinal word is the whole string). The term to the right of the fraction means the number (not the frequency) of different final word types succeeding the string&quot;.</p>
<p>He proceeds on with the example but it makes no sense to me. I cannot really find any good material to explain how to calculate this <span class=""math-container"">$\lambda(c_{KN}(w^{i-1}_{i-n+1})$</span> term.</p>
<p>Any suggestions or material related to this would be helpful. Recall I am doing this numerically so I need help decomposing this equation so I can code it and thus I need to understand what each term is.</p>
<p>Following the link above, this is a preliminary implementation I came up with</p>
<pre><code>def ksener_ney_smoothing(previous_tokens, ngram_dict, discounting_factor=0.75):
    suggestions = []
    
    # Start with previous_token (user input)
    previous_ngram = tuple(previous_tokens)
    previous_ngram_minus_last_word = tuple(previous_tokens[:len(previous_tokens)-1]) # w
    len_previous_ngram = len(previous_ngram)
    
    # Pull ngrams for the highest order
    highest_order_ngrams_map = ngram_dict[len_previous_ngram]
    second_higest_order_ngrams_map = ngram_dict[len_previous_ngram - 1]
    
    # Check if the user input is in the highest order ngram map
    if previous_ngram in highest_order_ngrams_map:
#         discounting_factor = 0 # From the link if the users input is in the highest order ngrams then its 0, found no where in literature
        first_num = max(highest_order_ngrams_map[previous_ngram] - discounting_factor, 0)
        first_denom = second_higest_order_ngrams_map[previous_ngram_minus_last_word]
#         print(first_num, &quot; &quot;, first_denom)
        
        l1 = list(list(x) for x in second_higest_order_ngrams_map.keys())
        lamb_denom = [item for sublist in l1 for item in sublist].count(previous_tokens[-2])
        l2 = list(list(x) for x in highest_order_ngrams_map.keys())
        myset = {item[2] for item in l2 if item[:2] == previous_tokens[:len(previous_tokens)-1]}
        lamb = (discounting_factor/lamb_denom)*len(myset)
#         print(lamb)

        pcont_num = [word[-1] for word in list(ngram_dict_ex[3].keys())].count(previous_ngram[-1])
        pcont_denom = len(highest_order_ngrams_map)
        pcont = pcont_num/pcont_denom
        
        return first_num / first_denom + lamb*pcont
    
    else:
        pass
</code></pre>
<p>I was able to match his corpus and numerical examples so should be on the right track, I am just not sure about the recursion part.</p>
","language-model"
"107587","Sequence-to-Sequence Transformer for Neural machine translation","2022-01-28 21:18:56","107609","0","84","<deep-learning><keras><nlp><transformer><language-model>","<p>I am using the tutorial in Keras documentation <a href=""https://keras.io/examples/nlp/neural_machine_translation_with_transformer/"" rel=""nofollow noreferrer"">here</a>. I am new to deep learning. On a different dataset <code>Menyo-20k</code> <a href=""https://zenodo.org/record/4297448"" rel=""nofollow noreferrer"">dataset</a>, of about 10071 total pairs, 7051 training pairs,1510 validation pairs,1510 test pairs. The highest validation accuracy and test accuracy I have gotten is approximately 0.26. I tried the list of things below:</p>
<ol>
<li>Using the following optimizers: <code>SGD, Adam, RMSprop</code></li>
<li>Tried different learning rate</li>
<li>Tried the dropout rate of <code>0.4 and 0.1</code></li>
<li>Tried using different embedding dimensions and feed-forward network dimension</li>
<li>Used <code>Early stopping and patience =3</code>, the model does not go past the <code>13th epoch</code>.
I tried the model itself without changing any parameters, the <code>validation accuracy never got to 0.3</code>, I tried to change the different parameters in order to know what I am doing wrong and I can't figure it out. Please what am I doing wrong? Thank you in advance for your guidance.</li>
</ol>
","language-model"
"106469","Why not rule-based semantic role labelling?","2021-12-27 19:06:10","106473","1","162","<nlp><text><language-model><parsing>","<p>I have recently found some interest in automatic <a href=""https://en.wikipedia.org/wiki/Semantic_role_labeling"" rel=""nofollow noreferrer"">semantic role labelling</a>. Most introductory texts (e.g. Jurafsky and Martin, 2008) present approaches based on supervised machine learning, often using FrameNet (Baker et al. 1998) and PropBank (Kingsbury &amp; Palmer, 2002). Intuitively however, I would imagine that the same problem could be tackled with a grammar-based parser.</p>
<p>Why is this not the case? Or rather, why would these supervised solutions be preferred?
Thanks in advance.</p>
<hr />
<h2>References</h2>
<p>Jurafsky, D., &amp; Martin, J. H. (2009). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall.</p>
<p>Baker, C. F., Fillmore, C. J., &amp; Lowe, J. B. (1998). The Berkeley FrameNet Project. 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1, 86–90. <a href=""https://doi.org/10.3115/980845.980860"" rel=""nofollow noreferrer"">https://doi.org/10.3115/980845.980860</a></p>
<p>Kingsbury, P., &amp; Palmer, M. (2002). From treebank to propbank. In Language Resources and Evaluation.</p>
","language-model"
"106466","Conceptually, how to deal with facts and time in GPT-3 and Language Models","2021-12-27 18:15:58","","1","18","<language-model><gpt>","<p>When exploring text generation using various large language models, I frequently come across generated text which presents facts which are plain out wrong.  I am not talking about fake news or bias, rather I am talking about dated pieces of information which were once correct, but are no longer correct.  When looking around for pros and cons of language models, I don't really see complaints about this as one of the greatest cons.</p>
<p>When we finetune models, and with the pretrained models themselves which are frozen in time, how do we account for information that may not be correct in the future?</p>
<p>The general knowledge learned is wonderful but there is always going to be a drift in knowledge to a point which is less and less relevant.  Take Stack Overflow for example: some questions from the original couple of years still have some truth while others have not aged well and perhaps are currently invalid questions and/or answers.</p>
","language-model"
"106186","Variable batch size for inputs of different length","2021-12-16 12:49:19","","1","313","<machine-learning><nlp><transformer><language-model><learning-rate>","<p>We're fine-tuning a GPT-2 model (using the Adam optimizer) to some posts from a social network. The length of these posts varies quite dramatically, so while some are only two tokens long, others can span hundreds of tokens. We've defined a cutoff at 256, but creating batches randomly and then padding is quite costly in terms of training time. We are now sorting the posts by length and then sampling randomly in consecutive blocks of n posts, where n is the maximum number of posts of 256 tokens that we can fit in a batch without running out of memory. But for batches of smaller posts (like n posts of length 2), this is not utilizing the resources we have and our compute time is quite limited.</p>
<p>So now we're thinking we could pack sequences together in batches such that n*length(post) remains roughly constant across batches. So one batch would be 10 sequences of length 256, while another would be 1280 sequences of length 2. But we're not sure what impact this will have on the training. It seems the learning rate is now scaled to tokens rather than posts, which actually sounds desirable with this kind of variance in number of tokens, but maybe not? Does the learning rate need to be adjusted somehow?</p>
<p>I've seen others do something similar called &quot;tensor packing&quot;, where you would concatenate all the posts and then chop them into chunks of the same size. I don't like the idea of combining posts that have nothing to do with each other like that (so the model could learn predict one from the other), but other than that this seems to do roughly the same thing regarding the learning process and the learning rate, right?</p>
","language-model"
"103815","What does the term ""seed lexicon"" means?","2021-11-04 14:51:28","","1","308","<nlp><language-model><research>","<p>I am reading a research paper (NLP) and found the phrase &quot;seed lexicon&quot;.</p>
<p>Could someone please explain it in detail?</p>
<p>Edit :</p>
<p>A sample paper
<a href=""https://aclanthology.org/2020.osact-1.17.pdf"" rel=""nofollow noreferrer"">Leveraging Affective Bidirectional Transformers for Offensive Language
Detection</a></p>
<p>Check 3rd page right column 5th line.</p>
","language-model"
"103583","how to improve my imbalanced data NLP model?","2021-10-28 14:23:18","","0","329","<nlp><bert><language-model><allennlp>","<p>I want to classify a patient's health as a prediction probability and get the top 10 most ill patients in a hospital. I have patient's condition notes, medical notes, diagnoses notes, and lab notes for each day.</p>
<p>Current approach -</p>
<ol>
<li>vectorize all the notes using spacy's scispacy model and sum all the vectors grouped by patient id and day. (200 columns)</li>
<li>find the unit vectors of the above vectors. (200 columns)</li>
<li>use a moving average function on the vectors grouped by patient id and day.(200 columns)</li>
<li>find the unit vectors of the above moving average vectors (200 columns)</li>
<li>combine all the above columns and use them as independent features.</li>
<li>use a lgbm classifier.</li>
</ol>
<p>The data is imbalanced and the current AUC-ROC is around .78.</p>
<p>What else can I do to improve my AUC-ROC?
Can I use bert for this problem? how should I use it?</p>
<p>I'm currently using a moving average as a patient's health deteriorates over time.</p>
<p>Any suggestion/answer/feedback?</p>
","language-model"
"102235","Best approach for text classification of phrases with little syntactic difference","2021-09-19 20:35:57","102523","2","218","<deep-learning><nlp><svm><text-classification><language-model>","<p>So I have the task of classifying sentences based on their level of 'change talk' shown. Change talk is a psychology term used in counseling sessions to express how much the client wants to change their behavior.</p>
<p>So let's say there are two classes: change talk; and non-change talk.</p>
<p>An example of change talk is: &quot;I have to do this.&quot; or &quot;I can achieve this.&quot;</p>
<p>An example of non-change talk is &quot;I can't do this.&quot; or &quot;I have no motivation.&quot;</p>
<p>My issue is, if I want to take a machine learning approach in classifying these sentences, which is the best approach? SVM's? I do not have a lot of training data. Also - all the tutorials I look at use sentences with obvious words that can easily be classified (e.g. &quot;The baseball game is on tomorrow.&quot; -&gt; SPORT, or &quot;Donald Trump will make a TV announcement tomorrow.&quot; -&gt; POLITICS).</p>
<p>I feel my data is harder to classify as it typically does not have keywords relating to each class.</p>
<p>Some guidance on how people would approach this task would be great.</p>
","language-model"
"102197","Phrase/Token labeling","2021-09-18 06:44:45","","2","69","<machine-learning><deep-learning><nlp><machine-learning-model><language-model>","<p>Looking for suggestions on how to define the following NLP problem and different ways in which it can be modeled to leverage machine learning. I believe there are multiple ways to model this problem.  Deep-learning-based suggestions also work as there is a good amount of data is available for training.</p>
<p>Will evaluate different approaches for the given dataset. Please share relevant papers, blogs, or GitHub repos. Thanks!</p>
<p><strong>Input</strong>: Given a sentence S having words W1 to W10.</p>
<p>S = W1 W2 W3 W4 W5 W6 W7 W8 W9 W10</p>
<p>The sentence has some syntactic and semantic patterns, but it is not exactly freely written natural language but it's in English. These are words, can be punctuation</p>
<p><strong>Output</strong>: should be something like this.</p>
<p>Label1 - W4</p>
<p>Label2 - W3</p>
<p>Label3 - [W2 W1] continuous // semantically related. Means words [W2 W1] in-order are assigned a Label3. <em>Also okay with solutions that don't output in-order.</em></p>
<p>Label4 - [W6 W8]</p>
<p>Label5- W10</p>
<p>Noise- W7, W9. Means words W7 and W9 independently are assigned a
Label3.</p>
<p>Label7- W5</p>
<p>Need to solve the problem. Looking for research/thoughts on how this problem can be defined in different ways to exploit different patterns in the structure of sentences. Looking for similar tasks which are already defined in NLP such as token labeling, parsing which can be used.</p>
<p>Would be really helpful to get the suggestions to the latest research on solving/defining this problem.</p>
","language-model"
"102084","For text classification, would a BoW or Word Embeddings based model ever be better than a Language Model?","2021-09-14 18:47:58","","1","215","<nlp><word-embeddings><bert><text-classification><language-model>","<p>I've done a bit of research, with <a href=""https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"" rel=""nofollow noreferrer"">this</a> being the best as far as objectively measuring quality, but wanted to ask from a theoretical perspective if BoW-based models (e.g. using TF-IDF) or word embeddings-based models (e.g. Word2Vec) would ever be a better choice than a language model (e.g. BERT) for a text classification problem?</p>
<p>The specific problem I'm working on is binary classification of short 2-8 word snippets such as &quot;Air bubble in ampoule&quot; into categories &quot;requires response&quot; or &quot;does not require response&quot;, but I'm more interested in the general question above.</p>
","language-model"
"97098","NLP-Problem, language model BERT?","2021-06-25 13:57:19","","1","46","<machine-learning><deep-learning><nlp><bert><language-model>","<p>Right now I am in the process of deciding on my masters thesis topic. Right now I and my professor are thinking about the possibility of having a large dataset of product requirements given in a natural language. My task would be to develop a domain-specific language (DSL) for these requirements and afterwards to develop a neural network that gets the dataset containing these requirements as an input and transforms each requirement into a requirement in this DSL that I have to develop beforehand.</p>
<p>In theory a topic which I personally find very interesting and would love to learn more about, the only problem is, my current knowledge in this area is very limited so I am trying to read up on it right now.</p>
<p>An example for such a DSL would be presented in this Paper: <a href=""https://www.researchgate.net/publication/281372036_A_Textual_Domain_Specific_Language_for_Requirement_Modelling"" rel=""nofollow noreferrer"">A Textual Domain Specific Language for Requirement Modelling</a>
(It's not exactly the same but a good showcase of the DSL might be derived from it).</p>
<p>For the people that don't want to read the paper there is an example given like this:</p>
<p><code>Natural Language Req: &quot;The Lateral and Vertical 'ERC' and'WRD' labels shall be displayed in a green color&quot;</code></p>
<p>The neural network should then take this requirement as an input and generate the following requirements:</p>
<p>Req1: <strong>The</strong> Lateral_ERC_Label <strong>shall be displayed in</strong> green</p>
<p>Req2: <strong>The</strong> Lateral_WRD_Label <strong>shall be displayed in</strong> green</p>
<p>Req3: <strong>The</strong> Vertical_ERC_Label <strong>shall be displayed in</strong> green</p>
<p>Req4: <strong>The</strong> Vertical_WRD_Label <strong>shall be displayed in</strong> green</p>
<p>As we can see the DSL, in this case, has the keywords &quot;The&quot; and &quot;shall be displayed in&quot; (Bolded) that stay the same for each generated requirement, and in between are the specifications for the requirement.</p>
<p>Now the question is, how is this achievable? Or rather, is this achievable at all? My professor said I should look into BERT for this task but my research has led me to the conclusion that BERT is not really suitable for text generation.</p>
<p>On the other hand, I am not really sure if this above-mentioned problem can be classified as text generation. It looks like a combination problem of text extraction (extracting the necessary keywords) and text generation.</p>
<p>But I am not really sure if I am even looking and researching in the right direction right now. So I would really appreciate it if somebody would be able to help me out here and giving me a push in the right direction.</p>
","language-model"
"95012","Why we shift target(output) by one offset in language modelling","2021-05-28 17:35:31","","1","139","<keras><nlp><rnn><language-model>","<p>I have been working in sequence prediction tasks (very similar to language modelling)  where I want to predict the next token(s)/item(s) given past sequence of tokens. I have always taken an approach like the below for data preparation</p>
<pre><code>[0,1,2,3] -&gt; [4]
[4,5,6,7] -&gt; [8]
</code></pre>
<p>or</p>
<pre><code>[0,1,2,3] -&gt; [4,5,6,7]
[4,5,6,7] -&gt; [8,9,10,11]
</code></pre>
<p>I have recently seen people taking output/target as one offset shifted from X. Since the goal of language modelling is to predict next token. Why and in which situation would one use this data format?</p>
<p>For example a famous book D2l.ai provide <a href=""http://d2l.ai/chapter_recurrent-neural-networks/language-models-and-dataset.html"" rel=""nofollow noreferrer"">example</a> format of the data as</p>
<pre><code>X:  [[27. 28. 29. 30. 31.]
 [12. 13. 14. 15. 16.]]
Y: [[28. 29. 30. 31. 32.]
 [13. 14. 15. 16. 17.]]
</code></pre>
<p>Isn't it predicting token <code>28</code> after tokens <code>[27. 28. 29. 30. 31.]</code>wheras actaly Y(output) should be <code>[32]</code> or <code>[32,33,34,...]</code> ?</p>
<p>How would next prediction (which actually is <code>32</code>) be retrieved after token <code>31</code>?</p>
","language-model"
"94910","Implementing a model for a language to another","2021-05-26 03:57:44","","0","32","<deep-learning><nlp><dataset><language-model><implementation>","<p>I have a dataset of sentences of language X and Y (2 columns, for example, &quot;abc def lang&quot; ==&gt; &quot;xyz pqrt mno uages&quot;). I want to have a output as a table that translates word by word (abc =&gt; xyz, def =&gt; pqrt mno, lang =&gt; uages).</p>
<p>I think we should use deep learning but idk how to implement and what to use. Can you give me some ideas?</p>
<p>Thanks all</p>
","language-model"
"94690","How to predict the sentiment of the entities form the tweet?","2021-05-20 14:53:42","","0","343","<nlp><sentiment-analysis><language-model><spacy><stanford-nlp>","<p>I have a JSON file (tweets.json) that contains tweets (sentences) along with the name of
the author.</p>
<p>Objective 1: Get the most frequent entities from the tweets.
Objective 2: Find out the sentiment/polarity of each author towards each of the entities.</p>
<pre><code>Sample Input:
Assume we have only 3 tweets:
Tweet1 by Author1: Pink Pearl Apples are tasty but Empire Apples are not.
Tweet2 by Author2: Empire Apples are very tasty.
Tweet3 by Author3: Pink Pearl Apples are not tasty.
</code></pre>
<p>Sample output:
Entities in the topics extracted: Share a dictionary with extracted entities as keys and the number of
tweets an entity has mentioned as values.</p>
<pre><code>Example dictionary for the above example: {“Pink Pearl Apples” : 2,
“Empire Apples” : 2}
</code></pre>
<p>Now, objective 2 -</p>
<p><a href=""https://i.sstatic.net/7bMlS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7bMlS.png"" alt=""enter image description here"" /></a></p>
<p>Now using Count Vectorizer I have completed objective 1, now how to get the sentiment based on the entities(Objective 2)</p>
<p>Thanks in Advance!!!</p>
","language-model"
"94448","Dealing with high frequency tokens during masked Language modelling?","2021-05-14 17:19:06","","1","109","<machine-learning><language-model><imbalanced-data><masking>","<p>Suppose I am working with a Masked Language Model to pre-train on a specific dataset. In that dataset, most sequences have a particular token of a high frequency</p>
<pre><code>Sample Sequence:-
&lt;tok1&gt;, &lt;tok1&gt;, &lt;tok4&gt;, &lt;tok7&gt;, &lt;tok4&gt;, &lt;tok4&gt; ---&gt; here tok4 is very frequent in this sequence
</code></pre>
<p>So if I mask some tokens and get the model to train to predict those masked tokens, obviously the model will gain a bias in predicting <code>&lt;tok4&gt;</code> due to its statistical frequency.</p>
<p>Since <code>&lt;tok4&gt;</code> represents important information, 'downsampling' (or removing those frequent tokens) would not be preferred and I would love to have my sequence as intact as possible.</p>
<p>How best should I deal with this? Is there any already established method that can counter this problem?</p>
","language-model"
"94235","Is this a tried alternative to word embedding for NLP?","2021-05-10 04:46:00","","0","288","<deep-learning><nlp><text-classification><language-model>","<p>I'm searching for research related to my idea, but apparently cannot articulate it well enough to the search engines to show me what's been published on this.</p>
<p>My idea: in a deep learning context (text classification), instead of inputting the text as integer vectors representing words in a vocabulary, which get substituted with dense vectors (word embeddings), what if instead, you represent each phrase almost like how a vector of &quot;pixels&quot; represent an image (then use 2d convolutions)? For example, each character, digit, and symbol get an integer value, just like how pixels are represented. So <code>abc</code> becomes <code>0, 1, 2</code>. The phrase could be formatted as a rectangle at word boundaries or something, but the gist of it, a phrase is formulated as a 2d array where each element is a character from some character set.</p>
<p>Is this a known/tried approach, and what name does it go by? From its apparent lack of popularity, I assume this formulation is far worse than word embeddings, but I'm curious about it.</p>
","language-model"
"94205","A simple attention based text prediction model from scratch using pytorch","2021-05-09 09:08:24","","3","1243","<nlp><pytorch><sequence-to-sequence><attention-mechanism><language-model>","<p>I first asked this question in codereview SE but a user recommended to post this here instead.</p>
<p>I have created a simple self attention based text prediction model using pytorch. The attention  formula used for creating attention layer is,</p>
<p><a href=""https://i.sstatic.net/198af.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/198af.png"" alt=""enter image description here"" /></a></p>
<p>I want to validate whether the whole code is implemented correctly, particularly my custom implementation of <code>Attention</code> layer.</p>
<p>Full code</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

import random
random.seed(0)
torch.manual_seed(0)

# Sample text for Training
test_sentence = &quot;&quot;&quot;Thomas Edison. The famed American inventor rose to prominence in the late
19th century because of his successes, yes, but even he felt that these successes
were the result of his many failures. He did not succeed in his work on one of his
most famous inventions, the lightbulb, on his first try nor even on his hundred and
first try. In fact, it took him more than 1,000 attempts to make the first incandescent
bulb but, along the way, he learned quite a deal. As he himself said,
&quot;I did not fail a thousand times but instead succeeded in finding a thousand ways it would not work.&quot; 
Thus Edison demonstrated both in thought and action how instructive mistakes can be. 
&quot;&quot;&quot;.lower().split()

# Build a list of tuples.  Each tuple is ([ word_i-2, word_i-1 ], target word)
trigrams = [([test_sentence[i], test_sentence[i + 1]], test_sentence[i + 2])
            for i in range(len(test_sentence) - 2)]

# print the first 3, just so you can see what they look like
print(trigrams[:3])

vocab = list(set(test_sentence))
word_to_ix2 = {word: i for i, word in enumerate(vocab)}

# Number of Epochs
EPOCHS = 25

# SEQ_SIZE is the number of words we are using as a context for the next word we want to predict
SEQ_SIZE = 2

# Embedding dimension is the size of the embedding vector
EMBEDDING_DIM = 10

# Size of the hidden layer
HIDDEN_DIM = 256

class Attention(nn.Module):
    &quot;&quot;&quot;
    A custom self attention layer
    &quot;&quot;&quot;
    def __init__(self, in_feat,out_feat):
        super().__init__()             
        self.Q = nn.Linear(in_feat,out_feat) # Query
        self.K = nn.Linear(in_feat,out_feat) # Key
        self.V = nn.Linear(in_feat,out_feat) # Value
        self.softmax = nn.Softmax(dim=1)

    def forward(self,x):
        Q = self.Q(x)
        K = self.K(x)
        V = self.V(x)
        d = K.shape[0] # dimension of key vector
        QK_d = (Q @ K.T)/(d)**0.5
        prob = self.softmax(QK_d)
        attention = prob @ V
        return attention

class Model(nn.Module):
    def __init__(self,vocab_size,embed_size,seq_size,hidden):
        super().__init__()
        self.embed = nn.Embedding(vocab_size,embed_size)
        self.attention = Attention(embed_size,hidden)
        self.fc1 = nn.Linear(hidden*seq_size,vocab_size) # converting n rows to 1
        self.softmax = nn.Softmax(dim=1)

    def forward(self,x):
        x = self.embed(x)
        x = self.attention(x).view(1,-1)
        x = self.fc1(x)
        log_probs = F.log_softmax(x,dim=1)
        return log_probs

learning_rate = 0.001
loss_function = nn.NLLLoss()  # negative log likelihood

model = Model(len(vocab),EMBEDDING_DIM,CONTEXT_SIZE,HIDDEN_DIM)
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

# Training
for i in range(EPOCHS):
    total_loss = 0
    for context, target in trigrams:
        # context, target = ['thomas', 'edison.'] the
        
        # step 1: context id generation
        context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)

        # step 2: setting zero gradient for models
        model.zero_grad()

        # step 3: Forward propogation for calculating log probs
        log_probs = model(context_idxs)

        # step 4: calculating loss
        loss = loss_function(log_probs, torch.tensor([word_to_ix2[target]], dtype=torch.long))

        # step 5: finding the gradients
        loss.backward()

        #step 6: updating the weights
        optimizer.step()

        total_loss += loss.item()
    if i%2==0:
        print(&quot;Epoch: &quot;,str(i),&quot; Loss: &quot;,str(total_loss))

# Prediction
with torch.no_grad():
    # Fetching a random context and target 
    rand_val = trigrams[random.randrange(len(trigrams))]
    print(rand_val)
    context = rand_val[0]
    target = rand_val[1]
    
    # Getting context and target index's
    context_idxs = torch.tensor([word_to_ix2[w] for w in context], dtype=torch.long)
    target_idxs = torch.tensor([word_to_ix2[w] for w in [target]], dtype=torch.long)
    print(&quot;Acutal indices: &quot;, context_idxs, target_idxs)
    log_preds = model(context_idxs)
    print(&quot;Predicted indices: &quot;,torch.argmax(log_preds))
</code></pre>
","language-model"
"93161","Training Objective of language model for GPT3","2021-04-17 00:14:58","93167","0","777","<nlp><language-model><gpt>","<p>On page 34 of OpenAI's <a href=""https://arxiv.org/pdf/2005.14165v2.pdf"" rel=""nofollow noreferrer"">GPT-3</a>, there is a sentence demonstrating the limitation of objective function:</p>
<blockquote>
<p>Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important.</p>
</blockquote>
<p>I am not sure if I understand this correctly. In my understanding, the objective function is to maximize the log-likelihood of the token to predict given the current context, i.e., <span class=""math-container"">$\max L \sim \sum_{i} \log P(x_{i} | x_{&lt;i})$</span>. Although we aim to predict every token that appears in the training sentence, the tokens have a certain distribution based on the appearance in human litterature, and therefore we do not actually assign equal weight to every token in loss optimization.</p>
<p>And what should be an example for a model to get the notion of &quot;what is important and what is not&quot;. What is the importance refer to in here? For example, does it mean that &quot;the&quot; is less important compared to a less common noun, or does it mean that &quot;the current task we are interested in is more important than the scenario we are not interested in ?&quot;</p>
<p>Any idea how to understand the sentence by OpenAI?</p>
","language-model"
"89076","Would there be any reason to pretrain BERT on specific texts?","2021-02-07 20:21:47","89078","1","551","<bert><transfer-learning><language-model><pretraining>","<p>So the official BERT English model is trained on Wikipedia and BookCurpos <a href=""https://en.wikipedia.org/wiki/BERT_(language_model)"" rel=""nofollow noreferrer"">(source)</a>.</p>
<p>Now, for example, let's say I want to use BERT for Movies tag recommendation. Is there any reason for me to pretrain a new BERT model from scratch on movie-related dataset?</p>
<p>Can my model become more accurate since I trained it on movie-related texts rather than general texts? Is there an example of such usage?</p>
<p>To be clear, the question is on the importance of <strong>context</strong> (not size) of the dataset.</p>
","language-model"
"88417","what's the motivation behind BERT masking 2 words in a sentence?","2021-01-24 20:03:49","88423","1","462","<nlp><bert><language-model>","<p><a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">bert</a> and the more recent <a href=""https://arxiv.org/pdf/1910.10683.pdf"" rel=""nofollow noreferrer"">t5</a> ablation study, agree that</p>
<blockquote>
<p>using a <strong>denoising</strong> objective always results in better downstream task performance compared to a language model</p>
</blockquote>
<p>where denoising == masked-lm == cloze.</p>
<p>I understand why learning to represent a word according to its bidirectional surroundings makes sense. However, I fail to understand why is it beneficial to learn to mask 2 words in the same sentence, e.g. <code>The animal crossed the road</code> =&gt; <code>The [mask] crossed the [mask]</code>. Why does it make sense to learn to represent <code>animal</code> without the context of <code>road</code>?</p>
<p>Note: I understand that the masking probability is 15% which corresponds to 1/7 words, which makes it pretty rare for 2 words in the same sentence to be masked, but why would it <strong>ever</strong> be beneficial, even with low probability?</p>
<p>Note2: please ignore the masking procedure sometimes replacing mask with a random/same word instead of <code>[mask]</code>, T5 investigates this choice in considerable length and I suspect that it's just an empirical finding :)</p>
","language-model"
"87943","how to programmatically introduce grammatical errors in sentences","2021-01-14 09:45:41","","3","225","<python><nlp><language-model><grammar-inference>","<p>I've a set of sentences in English language. I'm exploring ways to create a dataset of sentences with grammatical errors programmatically. The following options has been tried out randomly -</p>
<ul>
<li>identify verbs, propositions etc. by POS tagging and change the tense or remove them</li>
<li>change the order of 2 or more words</li>
<li>remove commas, colons, semi-colons etc.</li>
</ul>
<p>These are not always fool-proof. Are there any proven ways to approach this problem?</p>
","language-model"
"87389","Inference order in BERT masking task","2020-12-31 20:33:17","87390","1","150","<neural-network><nlp><bert><transformer><language-model>","<p>In BERT, multiple words in a single sentence can be masked at once. Does the model infer all of those words at once or iterate over them in either left to right or some other order?</p>
<p>For example:</p>
<blockquote>
<p>The dog walked in the park.</p>
</blockquote>
<p>Can be masked as</p>
<blockquote>
<p>The [mask] walked in [mask] park.</p>
</blockquote>
<p>In what order (if any) are these tokens predicted?</p>
<p>If you have any further reading on the topic, I'd appreciate it as well.</p>
","language-model"
"86572","BERT uses WordPiece, RoBERTa uses BPE","2020-12-11 19:10:22","86573","0","2335","<bert><transfer-learning><transformer><language-model><tokenization>","<p>In the original <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer""><strong>BERT</strong></a> paper, section <em>'A.2 Pre-training Procedure'</em>, it is mentioned:</p>
<blockquote>
<p>The LM masking is applied after <strong>WordPiece</strong> tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces.</p>
</blockquote>
<p>And in the <a href=""https://arxiv.org/pdf/1907.11692.pdf"" rel=""nofollow noreferrer""><strong>RoBERTa</strong></a> paper, section <em>'4.4 Text Encoding'</em> it is mentioned:</p>
<blockquote>
<p>The original BERT implementation (Devlin et al., 2019) uses a
character-level <strong>BPE</strong> vocabulary of size 30K, which is learned after
preprocessing the input with heuristic tokenization rules.</p>
</blockquote>
<p>I appreciate if someone can clarify why in the <a href=""https://arxiv.org/pdf/1907.11692.pdf"" rel=""nofollow noreferrer"">RoBERTa</a> paper it is said that <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">BERT</a> uses BPE?</p>
","language-model"
"86083","Optimal input setup for character-level text classification RNN","2020-11-29 10:01:00","","0","121","<neural-network><rnn><nlp><text-classification><language-model>","<p>I want to classify 500-character long text samples as to whether they look like natural language using a character-level RNN. I'm unsure as to the best way to feed the input to the RNN. Here are two approaches I've thought of:</p>
<ol>
<li>Provide the whole 500 characters (one per time step) to the RNN, and predict a binary class, <span class=""math-container"">$\{0,1\}$</span>.</li>
<li>Provide shorter overlapping segments (e.g. 10 characters) and predict the next (e.g. 11th) character. Convert this to classification by taking the test input and calculate the joint probability of the observed characters based on predicted next-character distributions.</li>
</ol>
<p>The first approach seems sub-optimal as I don't believe that the 1st character is going to have any effect on the prediction of the 500th character. The second approach gives me diminishingly small likelihoods when you calculate the joint probability.</p>
<p>I'm aiming for a more nuanced language model akin to n-gram frequency counting. I'm using simple RNNs for now but intend to swap to either LSTM or GRU.</p>
","language-model"
"85510","From where does BERT get the tokens it predicts?","2020-11-16 19:00:50","85524","2","475","<nlp><bert><language-model><tokenization>","<p>When BERT is used for masked language modeling, it masks a token and then tries to predict it.</p>
<p>What are the candidate tokens BERT can choose from? Does it just predict an integer (like a regression problem) and then use that token? Or does it do a softmax over all possible word tokens? For the latter, isn't there just an enormous amount of possible tokens? I have a hard time imaging BERT treats it like a classification problem where # classes = # all possible word tokens.</p>
<p>From where does BERT get the token it predicts?</p>
","language-model"
"85486","What is the difference between GPT blocks and Transformer Decoder blocks?","2020-11-16 09:54:05","85489","8","8768","<deep-learning><transformer><language-model>","<p>I know GPT is a Transformer-based Neural Network, composed of several blocks. These blocks are based on the original Transformer's Decoder blocks, but are they exactly the same?</p>
<p>In the original Transformer model, Decoder blocks have two attention mechanisms: the first is pure Multi Head Self-Attention, the second is Self-Attention with respect to Encoder's output. In GPT there is no Encoder, therefore I assume its blocks only have one attention mechanism. That's the main difference I found.</p>
<p>At the same time, since GPT is used to generate language, its blocks must be masked, so that Self-Attention can only attend previous tokens. (Just like in Transformer Decoders.)</p>
<p>Is that it? Is there anything else to add to the difference between GPT (1,2,3,...) and the original Transformer?</p>
","language-model"
"84692","Can I fine-tune the BERT on a dissimilar/unrelated task?","2020-10-30 07:20:30","84695","1","466","<bert><transformer><language-model><tokenization>","<p>In the original BERT paper, section 3 (arXiv:1810.04805) it is mentioned:</p>
<p>&quot;During pre-training, the model is trained on unlabeled data over <strong>different</strong> pre-training tasks.&quot;</p>
<p>I am not sure if I correctly understood the meaning of the word <strong>&quot;different&quot;</strong> here. different means a different <strong>dataset</strong> or a different <strong>prediction task</strong>?</p>
<p>For example if we pre-train the BERT on a &quot;sentence-classification-task&quot; with a big dataset. Then, should I fine-tune it again on the <strong>same</strong> &quot;sentence-classification-task&quot; task on a smaller and task-specific data-set or I can use the trained model for some other tasks such as &quot;sentence-tagging&quot;?</p>
","language-model"
"82590","For an n-Gram model with n>2, do we need more context at end of each sentence?","2020-10-05 15:06:24","","1","64","<nlp><language-model><stanford-nlp><ngrams>","<p>Jurafsky's book says we need to add context to left and <strong>right</strong> of a sentence:</p>
<blockquote>
<p><a href=""https://i.sstatic.net/2tWoX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/2tWoX.png"" alt=""&quot;Note that for these larger ngrams, we’ll need to assume extra context for the contexts to the left and right of the sentence end.&quot;"" /></a></p>
</blockquote>
<p>Does this mean,</p>
<p><strong>for example</strong>, if we've a corpus of three sentences: <code>&quot;John read Moby Dick&quot;</code>, <code>&quot;Mary read a different book&quot;</code>, and <code>&quot;She read a book by Cher&quot;</code>; and after training our tri-gram model on this corpus of three sentences, we need to evaluate the probability of a sentence &quot;John read a book&quot;, i.e. to find <span class=""math-container"">$P(John\; read\; a\; book)$</span> as below,</p>
<blockquote>
<p><span class=""math-container"">$P(John\; read\; a\; book)$</span> <br />
<span class=""math-container"">$=P(&lt;s&gt;&lt;s&gt;John\; read\; a\; book&lt;\backslash s&gt;&lt;\backslash s&gt;)$</span> <br />
<span class=""math-container"">$=P(John|&lt;s&gt;&lt;s&gt;) P(read|&lt;s&gt;John) \; P(a|John\; read) P(book|read\; a) P(&lt;\backslash s&gt;|a\; book)\; P(&lt;\backslash s&gt;|book&lt;\backslash s&gt;)$</span> <br />
<span class=""math-container"">$=\frac{1}{3}\frac{1}{1}\frac{1}{1}\frac{1}{2}\frac{0}{1}\frac{1}{1}$</span>       (without smoothing)</p>
</blockquote>
<p>It would be great, if you let me know if the above understanding is correct?</p>
","language-model"
"82577","In smoothing of n-gram model in NLP, why don't we consider start and end of sentence tokens?","2020-10-05 10:47:45","","4","499","<nlp><language-model><stanford-nlp><ngrams>","<p>When learning Add-1 smoothing, I found that somehow we are adding 1 to each word in our vocabulary, but not considering start-of-sentence and end-of-sentence as two words in the vocabulary. Let me give an example to explain.</p>
<p><strong>Example:</strong></p>
<p>Assume we have a corpus of three sentences:</p>
<p>&quot;<code>John read Moby Dick</code>&quot;, &quot;<code>Mary read a different book</code>&quot;, and &quot;<code>She read a book by Cher</code>&quot;.</p>
<p>After training our bi-gram model on this corpus of three sentences, we need to evaluate the probability of a sentence &quot;John read a book&quot;, i.e. to find <span class=""math-container"">$P(John\; read\; a\; book)$</span></p>
<p>To differentiate <em>John</em> appearing anywhere in a sentence from its appearance at the beginning, and likewise for <em>book</em> appearing at the end, we rather try to find <span class=""math-container"">$P(&lt;s&gt;John\; read\; a\; book&lt;\backslash s&gt;)$</span> after introducing two more words <span class=""math-container"">$&lt;s&gt;$</span> and <span class=""math-container"">$&lt;\backslash s&gt;$</span>,  indicating start of a sentence, and end of a sentence respectively.</p>
<p>Finally, we arrive at the</p>
<blockquote>
<p><span class=""math-container"">$P(&lt;s&gt;John\; read\; a\; book&lt;\backslash s&gt;)$</span> as
<span class=""math-container"">$P(John|&lt;s&gt;)P(read|John)P(a|read)P(book|a)P(&lt;\backslash s&gt;|book)=\frac{1}{3}\frac{1}{1}\frac{2}{3}\frac{1}{2}\frac{1}{2}$</span></p>
</blockquote>
<p><strong>My Question:</strong>
Now, to find <span class=""math-container"">$P(Cher\; read\; a\; book)$</span>, using Add-1 smoothing (Laplace smoothing) shouldn't we add the word 'Cher' that appears first in a sentence? And to that, we must add <span class=""math-container"">$&lt;s&gt;$</span>  and <span class=""math-container"">$&lt;\backslash s&gt;$</span> in our vocabulary. With this, our calculation becomes:</p>
<blockquote>
<p><span class=""math-container"">$P(Cher|&lt;s&gt;)P(read|Cher)P(a|read)P(book|a)P(&lt;\backslash s&gt;|book)=\frac{0+1}{3+13}\frac{0+1}{1+13}\frac{2+1}{3+13}\frac{1+1}{2+13}\frac{1+1}{2+13}$</span></p>
</blockquote>
<p>The 13 added to each numerator is due to the unique word count of the vocabulary which has 11 English words from our 3-sentence corpus plus 2 tokens - start and end of a sentence. In few places, I see 11 is added instead of 13 to the numerator. Wondering what I am missing here.</p>
","language-model"
"82478","Evaluating Language Model on specific topic","2020-10-02 16:56:25","82511","2","86","<machine-learning><nlp><language-model><gpt>","<p>I have finetuned a pretrained Language Model(GPT-2) on a custom dataset of mine. I would like a way of evaluating the ability of my model to generate sentences of a specific predefined topic, given in the form of either a single keyword(e.g. 'Computers') or a bag-of-words(e.g. 'Computers', 'Linux', 'Server'...).</p>
<p>For example given a LM, how relative are the outputs of the model to the topic specified by the word <em>Computers</em>?</p>
<p><strong>What I have already tried:</strong> Generating a large enough number of sentences from the LM and taking the average cosine similarity between these sentences and the target topic(or every word in that topic we have more than one) as described <a href=""https://towardsdatascience.com/cutting-edge-semantic-search-and-sentence-similarity-53380328c655"" rel=""nofollow noreferrer"">here</a> . I am not sure if this is a valid way to go and furthermore the cosine similarity between sentences yields poor results in many cases.</p>
<p>Thanks in advance for any help.</p>
","language-model"
"80639","State-of-the-art Python packages that can evaluate language similarity","2020-08-22 03:34:01","","2","297","<nlp><model-evaluations><similarity><language-model>","<p>I am trying to evaluate the likelihood of generating a specific sentence out of a large set of sentences. To do this, I start from a simple approach: training a custom n-gram language model and calculating the perplexity values for a list of sentences.</p>
<p>I found that the package KenLM (<a href=""https://www.aclweb.org/anthology/W11-2123/"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/W11-2123/</a>) was often used to do this task. However, it's kind of old (published in 2011).</p>
<p>On the other hand, I noticed that the two most famous state-of-the-art NLP packages, BERT and GPT-2, are both about pre-trained models.</p>
<p>I wonder if there is any package newer than KenLM suitable for this kind of likelihood evaluation task.</p>
","language-model"
"77976","When using padding in sequence models, is Keras validation accuracy valid/ reliable?","2020-07-19 19:47:48","","3","235","<machine-learning><keras><tensorflow><sequence-to-sequence><language-model>","<p>I have a group of non zero sequences with different lengths and I am using Keras LSTM to model these sequences. I use Keras Tokenizer to tokenize (tokens start from 1). In order to make sequences have the same lengths, I use <strong>padding</strong>.</p>
<p>An example of padding:</p>
<pre><code># [0,0,0,0,0,10,3]
# [0,0,0,0,10,3,4]
# [0,0,0,10,3,4,5]
# [10,3,4,5,6,9,8]
</code></pre>
<p>In order to evaluate if the model is able to generalize, I use a validation set with 70/30 ratio. In the end of each epoch Keras shows the training and validation accuracy.</p>
<p><strong>My big doubt is</strong> whether Keras validation accuracy is reliable <strong>when using padding</strong> (When you run Keras over several epochs, in the end of each epochs it prints training accuracy and validation accuracy). Because the validation set can simply be sequences of <strong>0's --&gt; [0,0,0]</strong>. Since there are a lot of sequences of 0's (<strong>because of padding</strong>), the model can easily learn and predict the sequences of 0's correctly, and as a result, create a fake high validation accuracy. In other words the model may learn sequences of zeros and not learn the real sequences.</p>
<p>So, does padding influences the validation accuracy in Keras?</p>
","language-model"
"77529","Effect of discounting parameter on Language Model Perplexity","2020-07-10 17:45:15","","1","70","<nlp><language-model>","<p>The general formula for absolute discounting for calculating language model probabilities subtracts a discounting parameter d from the count of the ngram before calculating the probabilities.</p>
<p><a href=""https://i.sstatic.net/B2pzz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B2pzz.png"" alt=""LM with absolute discounting"" /></a></p>
<p>The perplexity of the model first decreases as d increases, and then later starts increasing again.</p>
<p><a href=""https://i.sstatic.net/x3Zxr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/x3Zxr.png"" alt=""Perplexity vs Discounting Parameter"" /></a></p>
<p>What is the reason behind this change? Why does the perplexity decrease first and then start increasing again?</p>
","language-model"
"76652","Why does English ELMo model give embeddings for non-English words?","2020-06-25 10:21:52","","0","429","<tensorflow><word-embeddings><nlp><language-model>","<p>Here's the code from my notebook:</p>
<pre><code>%tensorflow_version 1.x
import tensorflow as tf
import tensorflow_hub as hub

elmo = hub.Module(&quot;https://tfhub.dev/google/elmo/2&quot;, trainable=True)
tf.logging.set_verbosity(tf.logging.ERROR)

def elmo_vectors(x):
    embeddings = elmo(x, signature=&quot;default&quot;, as_dict=True)[&quot;elmo&quot;]
    with tf.Session() as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(tf.tables_initializer())
        return sess.run(embeddings)
</code></pre>
<p>Output for non-English language: (Hindi in this example)</p>
<pre><code>words = ['गोकुल']
v = elmo_vectors(words)
print(v.shape) # (1,1,1024)
print(v[0][0])
# Output: [ 0.3731584   0.5700774  -0.48072845 ... -0.1241736   0.5961436 -0.6986947 ]
</code></pre>
<p>The <a href=""https://tfhub.dev/google/elmo/2"" rel=""nofollow noreferrer"">documentation of the pre-trained ELMo on Tensorflow Hub</a> shows that it was trained only on the English language.<br />
That is, the dataset from 1 billion word benchmark is based on monolingual English data. (<a href=""https://opensource.google/projects/lm-benchmark"" rel=""nofollow noreferrer"">Source</a>)</p>
<p>So, how/why am I getting embeddings for non-English vocabulary words from ELMo using the TF Hub model?</p>
","language-model"
"74717","Generate text using user-supplied keywords","2020-05-23 17:35:17","","2","135","<deep-learning><nlp><language-model><text-generation><gpt>","<p>I've got a use case where I need to generate sentences based on a set of user supplied keywords. Here is an example of what I need:</p>

<h2>User input:</h2>

<p><strong>End-User:</strong> Data Scientists</p>

<p><strong>Region:</strong> Middle East</p>

<p><strong>Country:</strong> UAE</p>

<p><strong>Solution:</strong> BigPanda</p>

<p><strong>Application:</strong> machine learning</p>

<p><strong>Benefits:</strong> lower costs and runtime</p>

<h2><strong><em>Output (Curly-Brackets are just there to highlight):</em></strong></h2>

<p>Learn how {data scientists} in the {Middle East} such as in the {UAE} are using {BigPanda} to streamline their {machine learning} processes to {lower costs and runtime}.</p>

<hr>

<p>So the model needs to use the keywords given by the user and generate similar sentences. I also have a dataset of about 2000 of such sentences, which may come in handy.</p>

<p>One way I thought this could be possible is by using the GPT-2 model and perhaps finetuning it with the dataset, but I haven't been able to figure out how I would actually go about using it for something like this.</p>
","language-model"
"74115","Is BERT a language model?","2020-05-13 12:22:22","74119","9","3853","<nlp><bert><transformer><language-model>","<p>Is BERT a language model in the sense of a function that gets a sentence and returns a probability?
I know its main usage is sentence embedding, but can it also provide this functionality?</p>
","language-model"
"73312","Comparing Language Model of two corpora","2020-04-30 20:13:40","","1","175","<neural-network><lstm><rnn><sequence-to-sequence><language-model>","<p>I know using Conditional Language Model I can learn the probability of a sentence given the corpus I used to train my model. I will then be able to generate meaningful text by sampling from the distribution of sentences.</p>

<p>Now what I want to do is to compare the text generated from the language model of two different corpora on the same topic.</p>

<p>Use case: I want to compare the headline that a right winning vs left winning news outlet would use for a given news content. (my training data would be a large set of headline+news content from both news outlet)</p>
","language-model"
"73260","How to convert subword PPL to word level PPL?","2020-04-29 20:09:43","","1","26","<nlp><pytorch><bert><language-model>","<p>I'm using this formula to covert subword perpexity to word perplexity:
<code>PPL_word = exp(log(PPL_subword) * num_subwords / num_words)</code>
The question is do I need to include the [SEP] and [CLS] tokens when counting subwords?</p>
","language-model"
"69877","Feeding XLM-R embeddings to neural machine translation?","2020-03-18 08:19:33","","0","369","<neural-network><nlp><sequence-to-sequence><machine-translation><language-model>","<p>I’m very new to the field of deep learning. My aim is to make a translation between Catalan to Catalan Sign Language. The grammar of the two languages is different </p>

<blockquote>
  <p>Input: He sells food.
  Output (sign language sentence): Food he sells.</p>
</blockquote>

<p>I've been playing around with XLM-R and go the token id like this</p>

<pre><code>input Ids: [200, 100, 2003, 1037, 3835, 3351, 5012, 300, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</code></pre>

<p>I don't know how to use the embeddings in Sequence to Sequence NMT model. or any other means to do machine translation with a very small data set. The language is low resource language</p>

<pre class=""lang-py prettyprint-override""><code>import torch
from transformers import XLMRobertaModel, XLMRobertaTokenizer

tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')
model = XLMRobertaModel.from_pretrained('xlm-roberta-large')

def get_ids(tokens, tokenizer, max_seq_length):
token_ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = token_ids + [0] * (max_seq_length - len(token_ids))
return input_ids

s = ""test sentence""
stokens = tokenizer.tokenize(s)
print(stokens)
stokens = [""[CLS]""] + stokens + [""[SEP]""]
input_ids = get_ids(stokens, tokenizer, 15)

print(tokenizer.convert_tokens_to_ids(['test']))
print(tokenizer.convert_tokens_to_ids(['▁test']))
print(tokenizer.convert_ids_to_tokens([26130]))
print(tokenizer.convert_ids_to_tokens([30521]))
tokens_tensor = torch.tensor([input_ids])
print(input_ids)
print(tokens_tensor)
<span class=""math-container"">```</span>
</code></pre>
","language-model"
"67914","What are the elements in a BERT word embedding?","2020-02-11 19:10:10","67968","3","3548","<word-embeddings><nlp><bert><language-model>","<p>As far as I understand, BERT is a word embedding that can be fine-tuned or used directly. </p>

<p>With older word embeddings (word2vec, Glove), each word was only represented once in the embedding (one vector per word). This was a problem because it did not take homonyms into account. As far as I understand, BERT tackles this problem by taking context into condsideration. </p>

<p>What does this mean for the word embedding itself? Is there still one vector for each word token? If so, how is context taken into consideration? If no, what is the format of the embedding?</p>
","language-model"
"66207","What is purpose of the [CLS] token and why is its encoding output important?","2020-01-09 17:20:10","66209","72","100277","<nlp><sentiment-analysis><bert><language-model><text-classification>","<p>I am reading <a href=""http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"" rel=""noreferrer"">this article on how to use BERT</a> by Jay Alammar and I understand things up until:</p>

<blockquote>
  <p>For sentence classification, we’re only only interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.</p>
</blockquote>

<p>I have read <a href=""https://datascience.stackexchange.com/questions/46312/what-is-the-vector-value-of-cls-sep-tokens-in-bert"">this topic</a>, but still have some questions:</p>

<p>Isn't the [CLS] token at the very beginning of each sentence? Why is that ""we are only interested in BERT's output for the [CLS] token""? Can anyone help me get my head around this? Thanks!</p>
","language-model"
"65556","The differences between BNF and JSGF in NLP?","2019-12-28 15:45:24","","2","223","<nlp><language-model>","<p>I wonder what the differences are between the <a href=""http://matt.might.net/articles/grammars-bnf-ebnf/"" rel=""nofollow noreferrer"">BNF</a>(Backus-Naur Form) and <a href=""https://en.wikipedia.org/wiki/JSGF"" rel=""nofollow noreferrer"">JSGF</a>(Java Speech Grammar Format)? The former is a kind of context-free grammar taught in <a href=""https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1162/handouts/cgw/cgw-instructions.pdf"" rel=""nofollow noreferrer"">CS224</a>, but I learned that the latter is also being used. Could anyone tell me which one is better and what are their differences?</p>
","language-model"
"65550","Fine tune gpt2 via huggingface API for domain specific LM","2019-12-28 11:10:10","","3","265","<language-model><gpt>","<p>i am using the script in the examples folder to fine-tune the LM for a bot meant to deal with insurance related queries. So if someone were to type ""i am looking to modify my ..."" , the autocomplete suggestions would be "" modify my name "", ""modify my surname"", modify my vehicle number etc </p>

<p>My training data set has plenty of such samples BUT is always prefixed or suffixed by policy details, personal details etc which seems to be throwing off the fine-tuning and in predictions it always includes some random names , numbers /text , like ""i want to modify my father's name into 1235..."" Etc ..i hope u get the idea</p>

<p>One way of dealing with this issue would be to clean up the training dataset using some NER and get rid of specific information (not very impressive) or maybe unfreeze some other layers of the gpt2 model.</p>

<p>Tried exploring the API but it simply allows loading pre trained wirghts and explicitly unfreezes only the last classification layer (?) ..any pointers on resolving this would be extremely helpful.. appreciate any inputs </p>
","language-model"
"65271","How to calculate perplexity in PyTorch?","2019-12-22 10:30:12","65273","1","4455","<lstm><pytorch><nlp><language-model>","<p>I am wondering the calculation of <code>perplexity</code> of a <code>language model</code> which is based on <code>character level LSTM model</code>. I got the code from <a href=""https://www.kaggle.com/francescapaulin/character-level-lstm-in-pytorch"" rel=""nofollow noreferrer"">kaggle</a> and edited a bit for my problem but not the training way. I have added some other stuff to graph and save logs. However, as I am working on a <code>language model</code>, I want to use <code>perplexity</code> measuare to compare different results. In <code>tensorflow</code>, I have done it via this <a href=""https://stackoverflow.com/a/42025194/1404324"">answer</a> and it was easy. I have looked for a way doing it in <code>PyTorch</code> and literally no related result on Google. I need some help, and it is really appreciated. </p>

<p>Here is the related code, I believe:</p>

<pre><code>criterion = nn.CrossEntropyLoss()

# create training and validation data
val_idx = int(len(data)*(1-val_frac))
data, val_data = data[:val_idx], data[val_idx:]

if(train_on_gpu):
    net.cuda()

counter = 0
n_chars = len(net.chars)
for e in range(epochs):
    # initialize hidden state
    h = net.init_hidden(batch_size)

    for x, y in get_batches(data, batch_size, seq_length):
        counter += 1

        # One-hot encode our data and make them Torch tensors
        x = one_hot_encode(x, n_chars)
        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)

        if(train_on_gpu):
            inputs, targets = inputs.cuda(), targets.cuda()

        # Creating new variables for the hidden state, otherwise
        # we'd backprop through the entire training history
        h = tuple([each.data for each in h])

        # zero accumulated gradients
        net.zero_grad()

        # get the output from the model
        output, h = net(inputs, h)

        # calculate the loss and perform backprop
        loss = criterion(output, targets.view(batch_size*seq_length))
</code></pre>
","language-model"
"64120","Any good Implementations of Bi-LSTM bahdanau attention in Keras?","2019-12-02 21:22:22","","3","3032","<deep-learning><keras><sequence-to-sequence><language-model><attention-mechanism>","<p>From past few weeks I'm trying to learn sequence to sequence machine translation modelling but I couldn't find any good examples/tutorials with bahdanau attention implemented. I did come across a ton of examples where people have implemented attention but it's mostly on gru (or) luong attention (or) outdated code.</p>

<p>Has anyone come across any good implementations of bahdanau attention model in keras which you have implemented (or) tried? I really want to learn the coding part but no good material regarding implementation is found? </p>

<p>Please help. </p>
","language-model"
"63731","Transfer learning between Language Model and classification","2019-11-25 11:29:15","","1","141","<classification><nlp><transfer-learning><language-model>","<p>Following this fast.ai <a href=""https://www.youtube.com/watch?v=5gCQvuznKn0&amp;list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&amp;t=2757s"" rel=""nofollow noreferrer"">lecture</a>, I am trying to understand the mechanism of Transfer Learning in NLP from a general Language Model (LM) to a classification problem.</p>

<p>What is exactly taken from the Language Model training? Is it just the word embeddings? Or is it also the weights of the LSTM cell? The architecture of the neural net should be quite different - where in a LM you would output a prediction after every sequence-step, in a classification problem you would only care about the output of the final sequence step. </p>

<p>(I would happy to know what is the general practice, and also if anyone knows how fast.ai does it)</p>
","language-model"
"63124","BERT Model Evaluation Measure in terms of Syntax Correctness and Semantic Coherence","2019-11-14 03:29:42","","2","360","<nlp><bert><language-model>","<p>For example I have an original sentence. The word barking corresponds to the word that is missing.</p>

<pre><code>Original Sentence : The dog is barking.
Incomplete Sentence : The dog is ___________.
</code></pre>

<p>For example, using the BERT model, it predicts the word crying instead of 
the word barking. How will I measure the accuracy of the BERT Model in terms of how syntactically correct and semantically coherent the predicted word is?</p>

<p>(For an instance, there are a lot of incomplete sentences, and the task is to evaluate BERT accuracy based on these incomplete sentences.)</p>

<p>In other words, how will I measure the distance in terms of semantics in terms of model between the two words <code>barking</code> and <code>crying</code>.</p>

<p>Please help.</p>
","language-model"
"61849","Language modelling for Spell Checker","2019-10-17 05:32:52","","1","600","<neural-network><lstm><rnn><model-selection><language-model>","<p>I am working on spell checkers, I want to create a spell checker, I am confused about which model to use</p>

<ol>
<li>Word-Level modelling</li>
<li>Character-Level modelling</li>
</ol>

<p>plus I am preferring neural networks over Peter Norvig or N-gram/K-gram or any other ""Vanilla"" algorithm so that networks capture context</p>

<p>Example:
Input: I have aplied fo r credit cart
Output: I have applied for credit card</p>

<p>I have gone through models which use character level BLSTM with noise makers/error models and attention mechanism but not satisfied by result. <a href=""https://towardsdatascience.com/creating-a-spell-checker-with-tensorflow-d35b23939f60"" rel=""nofollow noreferrer"">https://towardsdatascience.com/creating-a-spell-checker-with-tensorflow-d35b23939f60</a></p>

<p>One of the article I have gone through is to use OpenNMT, but results were disastrous. <a href=""https://medium.com/scribd-data-science-engineering/neural-spelling-corrections-and-the-importance-of-accuracy-977c0063d20f"" rel=""nofollow noreferrer"">https://medium.com/scribd-data-science-engineering/neural-spelling-corrections-and-the-importance-of-accuracy-977c0063d20f</a></p>
","language-model"
"58677","Skip-gram trained on The Hobbit: no improvement in the similarity of the word representation","2019-09-04 14:52:51","","1","27","<neural-network><nlp><language-model>","<p>I've trained a simple skipgram NNLM (window size = 5) on The Hobbit. This is the rough pseudocode: </p>

<pre><code>for e in  epochs:  
    for i in idx:   
     loss_batch = 0
     for b in batch_size:
         input = data[i+b]
         output = nnlm(input)
         loss_window = neg_log_lhd(output, target)
         loss_batch += loss_window

     loss_batch /=batch_size  
     loss_batch.backward()
</code></pre>

<p><code>idx</code> here is the index of the first input word in the batch. Loss is accumulated over the whole batch (if <code>batch_size</code>=128, it means there are 128 sliding windows for which the loss is calculated) and backpropagated before moving to the next one.</p>

<p>The model seems to converge, judging by the curve of the loss function, but I'm concerned about the vector representation of words. I computed cosine similarity for 4 different words: 'bilbo','baggins', 'gandalf', 'thorin'. I expected the similarity between the first two to be very high, as they are used interchangeably, or very frequently together. Nevertheless, it starts with <code>-0.042</code> and is <code>-0.04</code> after 40 epochs (~400 batches/epoch). Obviously something's wrong here.</p>

<p>What could possibly be the reason for the situation when the model converges, but the distance between word embeddings doesn't change?    </p>
","language-model"
"56844","How to feed data for ngram model?","2019-08-02 14:39:40","56846","1","574","<nlp><dataset><language-model><ngrams>","<p>I want to train an ngram language model</p>

<p>Let's say I have the following corpus:</p>

<pre><code>The sliding cat is not able to dance
He is only able to slide
Because obviously he is the sliding cat
</code></pre>

<p>I am planning to use tf.data.Dataset to feed my model, which is fine</p>

<p>But I don't know if it is better to use a sliding window to iterate through my copus or simply feed my corpus n words at a time</p>

<p>Using a sliding window, my model (assuming a bigram) will see:</p>

<pre><code>The sliding
sliding cat
cat is
is not
...
</code></pre>

<p>Going n word at a time:</p>

<pre><code>The sliding
cat is
not able
...
</code></pre>

<p>I'd appreciate any recommandation, thanks</p>
","language-model"
"53875","What is whole word masking in the recent BERT model?","2019-06-15 23:13:57","53910","12","7755","<nlp><language-model><bert>","<p>I was checking <a href=""https://github.com/google-research/bert"" rel=""noreferrer"">BERT GitHub page</a> and noticed that there are new models built from a new training technique called ""whole word masking"". Here is a snippet describing it:</p>

<blockquote>
  <p>In the original pre-processing code, we randomly select WordPiece tokens to mask. For example:</p>
</blockquote>

<pre><code>Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head

Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head
</code></pre>

<blockquote>
  <p>The new technique is called Whole Word Masking. In this case, we always mask all of the the tokens corresponding to a word at once. The overall masking rate remains the same.</p>
</blockquote>

<pre><code>Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head
</code></pre>

<p>I can't understand ""<em>we always mask all of the the tokens corresponding to a word at once</em>"". ""jumped"", ""phil"", ""##am"", and ""##mon"" are masked and I am not sure how these tokens are related.</p>
","language-model"
"51141","How should I treat these non-English documents in the NLP task?","2019-04-29 21:43:59","51153","4","330","<machine-learning><nlp><language-model>","<p>So I have a small corpus of about 30k documents and about 50 documents in this corpus are in other languages (Persian, Chinese, Arabic, German, Spanish etc). I will be using this corpus for training a machine learning model.</p>

<p>Now the question is: How should these non-English documents be treated?</p>

<ol>
<li>Should I exclude them from the final corpus and from training the model?</li>
<li>or should I manually translate them (Requesting natives from each language to translate it for me) and include them in the final corpus?</li>
<li>or should I use Google translate/DeepL to translate these non-English documents into English and then include them in the final corpus?</li>
</ol>

<p>Each document in the corpus under question is not larger than 500 words each.</p>

<p>Any help or hint will be appreciated.</p>
","language-model"
"48969","Why Heaps' Law Equation looks so different in this NLP course?","2019-04-09 15:00:00","48983","1","237","<deep-learning><nlp><language-model>","<p>I'm actually not sure if this question is allowed on this community since it's more of a linguistics question than it is a data science question. I've searched extensively on the Web and have failed to find an answer and also the Linguistics Beta Stack Exchange community also doesn't seem to be able to help. If it's not allowed here please close it.</p>

<p><a href=""https://en.wikipedia.org/wiki/Heaps%27_law"" rel=""nofollow noreferrer"">Heaps' Law</a> basically is an empirical function that says <em><strong>the number of distinct words you'll find in a document grows as a function to the length of the document</strong></em>. The equation given in the Wikipedia link is</p>

<p><span class=""math-container"">$$V_R(n) = Kn^\beta$$</span></p>

<p>where <span class=""math-container"">$V_R$</span> is the number of distinct words in a document of size <span class=""math-container"">$n$</span>, and <span class=""math-container"">$K$</span> and <span class=""math-container"">$\beta$</span> are free parameters that are chosen empirically (usually <span class=""math-container"">$0 \le K \le 100$</span> and <span class=""math-container"">$0.4 \le \beta \le 0.6$</span>).</p>

<p>I'm currently following a course on Youtube called <em>Deep Learning for NLP</em> by Oxford University and DeepMind. There is a slide in a lecture that demonstrates Heaps' Law in a rather different way:</p>

<p><a href=""https://i.sstatic.net/QnAvH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QnAvH.png"" alt=""enter image description here""></a></p>

<p>The equation given with the logarithms apparently is also Heaps' Law. The fastest growing curve is a corpus for Twitter data and the slowest is for the Wall Street Journal. Tweets usually have less structure and more spelling errors, etc. compared to the WSJ which would explain the faster-growing curve.</p>

<p>The main question that I had is how Heaps' Law seems to have taken on the form that the author has given? It's a bit of a reach but the author didn't specify what any of these parameters are (i.e. <span class=""math-container"">$C$</span>, <span class=""math-container"">$\alpha$</span>, <span class=""math-container"">$r(w)$</span>, <span class=""math-container"">$b$</span>) and I was wondering if anybody might be familiar with Heaps' Law to give me some advise on how to solve my question.</p>
","language-model"
"48165","Why is MLP working similar to RNN for text generation","2019-03-28 17:46:13","","1","94","<neural-network><rnn><nlp><language-model>","<p>I was trying to perform text generation using only a character level feed-forward neural network after having followed <a href=""https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/"" rel=""nofollow noreferrer"">this tutorial</a> which uses LSTM. I one-hot encoded the characters of my corpus which gave a vector of length 45. Then I concatenated every 20 characters and fed this 20*45 length vector as input to an MLP with the 21st character's one hot as the output.</p>

<p>Thus my X (input data) shape is -> (144304, 900)
and my Y (output data) shape is -> (144304, 45)</p>

<p>Here's the output from my code:</p>

<blockquote>
  <p>alice was beginning very about a grible thing was and bet she with a
  great come and fill feel at and beck to the darcht repeat allice
  waited it, put this was not an encir to the white knew the mock turtle
  with a sigh. ‘i only took the regular cours was be crosd it to fits
  some to see it was and getting she dodn as the endge of the evence,
  and went on to love that you were no alway not--ohe f whow to the
  gryphon is, who denight to goover and even stried to the dormouse, and
  repeated her question. ‘why did they live at the bottom of a well?’</p>
  
  <p>‘tabl the without once it it howling it the duchess to herself it as
  eng, longing one of the door and wasting for the homend of the taits.’</p>
  
  <p>‘sthing i cancus croquet with the queen to-day?’</p>
  
  <p>‘i should like it very much,’ said the dryphon, ‘you first form into a
  line along-the sea-shore--’</p>
  
  <p>‘the right!’ cried the queen nother frowing tone. any the this her for
  some thing is and at like the look of it at all,’ said the king:
  ‘however, it may kiss my hand if it likes.’</p>
  
  <p>‘i’d really feeer that in a few this for some whele wish to get thing
  to eager to think thcapered twice, and shook note bill herself in a
  lell as expectant, and thowedd all have come fuconfuse it the march
  hare: she thought it must be the right way of speaking to a mouse: she
  had never done such a thing before, but she remembered having seen in
  her brother’s latin grammar, ‘a mouse--of a mouse--to a mouse--a
  mouse--o mister once to hin fent on the words with her fane with ale
  three king the said, and diffich a vage and so mane alice this cime.</p>
</blockquote>

<p>My Question is why is MLP working similar to an RNN/LSTM. What's the advantage of using RNN/LSTM for such tasks over MLP?</p>
","language-model"
"47817","How to prepare the data for text generation task","2019-03-23 00:43:54","","2","56","<deep-learning><nlp><language-model><text-generation><transformer>","<p>First, I'm not sure whether the model contains the encoder during training.</p>

<p>EOS means end-of-sentence. Encoder and decoder are part of transformer network.</p>

<p>If without-encoder, training time:</p>

<pre><code>target: [E, F, G, H, EOS]
decoder input: [0, E, F, G, H]
</code></pre>

<p>If without-encoder, testing time:</p>

<pre><code>decoder input: [0]
</code></pre>

<p>If with encoder, training time:</p>

<pre><code>encoder input: [A, B, C, B]
target: [E, F, G, H, EOS]
decoder input: [0, E, F, G, H]
</code></pre>

<p>If with-encoder, testing time:</p>

<pre><code>encoder input: [A, B, C, D]
decoder input: [0]
</code></pre>

<p>Am I exact right?</p>
","language-model"
"47773","The principle of LM deep model","2019-03-22 09:35:50","47780","1","65","<neural-network><deep-learning><nlp><language-model><transformer>","<p>Language model(LM) is the task of predicting the next word.</p>

<p>Does the deep model need the encoder? From the ptb code of tensor2tensor, I find the deep model do not contains the encoder.</p>

<p>Or both with-encoder and without-encoder can do the LM task?</p>
","language-model"
"46648","What does 'Linear regularities among words' mean?","2019-03-04 15:34:12","46650","5","736","<nlp><language-model><representation>","<p><strong>Context</strong>: In the paper <a href=""https://arxiv.org/abs/1301.3781"" rel=""noreferrer"">""Efficient Estimation of Word Representations in Vector Space""</a> by T. Mikolov et al., the authors make use of the phrase: 'Linear regularities among words'.</p>

<p>What does that mean in the context of the paper, or in a general context related to NLP? </p>

<p>Quoting the paragraph from the paper:</p>

<blockquote>
  <p>Somewhat surprisingly, it was found that similarity of word
  representations goes beyond simple syntactic regularities. Using a
  word offset technique where simple algebraic operations are performed
  on the word vectors, it was shown for example that vector(”King”) -
  vector(”Man”) + vector(”Woman”) results in a vector that is closest to
  the vector representation of the word Queen [20].</p>
  
  <p>In this paper, we try to maximize accuracy of these vector operations
  by developing new model architectures that preserve <strong>the linear
  regularities among words</strong>. We design a new comprehensive test set for
  measuring both syntactic and semantic regularities1 , and show that
  many such regularities can be learned with high accuracy. Moreover, we
  discuss how training time and accuracy depends on the dimensionality
  of the word vectors and on the amount of the training data.</p>
</blockquote>
","language-model"
"45190","How is maximizing L(lambda1, lamda2, lamda3) equivalent to minimizing perplexity?","2019-02-07 04:49:52","","1","78","<nlp><language-model>","<p>In language modeling, L(lambda1, lambda2, lambda3) is defined as:</p>

<pre><code>Sum(count of trigram(u,v,w) x q(w|u,v)) 
</code></pre>

<p>where u, v, w are words in the corpus
and 
perplexity is defined as:</p>

<pre><code>2^-l 
</code></pre>

<p>where </p>

<pre><code>l = (1/M)Sum(log(q(w|u,v)).
</code></pre>

<p>where M is the total no. of words in the corpus.</p>

<p>Also,</p>

<pre><code>q(w|u,v) = lambda1*q(w|u,v) + lambda2*q(w|v) + lamdba3*q(w)
</code></pre>

<p>The q-values on the right hand side are maximum likelihood estimates.
Some of the materials for Natural Language Processing state that maximizing L is same as minimizing perplexity. I don't see how that is true or can be mathematically proved.</p>
","language-model"
"43273","Build an Autocomplete model for document titles","2018-12-29 07:50:34","","2","79","<rnn><autoencoder><language-model>","<p>I want to build an autocomplete model using RNN where input is article names (documents title).</p>

<blockquote>
  <p>X: ['Billing', 'Loan status', 'Filling loan application', 'Contact Info', ...]</p>
</blockquote>

<p>The article name can be 1-5 word long and there is an even distribution of article names among word count i.e. no. of 1 word articles names is almost same to no. of 2 word article name.</p>

<p>I want my model to start predicting article names based on first 3 typed characters (as I do not want to miss one word article names) and then make predict call based on each next typed characters.</p>

<p>I am not able to decide what should be the target output (y) here while training the model? Should I right shift my input sequence by 3 characters and train the model like below?</p>

<p>PS: Please mind my drawing.</p>

<p><a href=""https://i.sstatic.net/u0Sza.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/u0Sza.png"" alt=""enter image description here""></a></p>
","language-model"
"38540","Are there any good out-of-the-box language models for python?","2018-09-20 13:34:22","121044","16","8158","<python><nlp><language-model>","<p>I'm prototyping an application and I need a language model to compute perplexity on some generated sentences.</p>

<p>Is there any trained language model in python I can readily use? Something simple like</p>

<pre><code>model = LanguageModel('en')
p1 = model.perplexity('This is a well constructed sentence')
p2 = model.perplexity('Bunny lamp robert junior pancake')
assert p1 &lt; p2
</code></pre>

<p>I've looked at some frameworks but couldn't find what I want. I know I can use something like:</p>

<pre><code>from nltk.model.ngram import NgramModel
lm = NgramModel(3, brown.words(categories='news'))
</code></pre>

<p>This uses a good turing probability distribution on Brown Corpus, but I was looking for some well-crafted model on some big dataset, like the 1b words dataset. Something that I can actually trust the results for a general domain (not only news)</p>
","language-model"
"36482","LSTM training/prediction with no starting sequence","2018-08-04 20:06:46","","3","364","<machine-learning><keras><lstm><sequence><language-model>","<p>ML newbie here. As an exercise, I'm trying to build a character based language model based on a simple 1 layer LSTM. Based on what I've learned about LSTMs, a common usage is to take in a sequence of characters and then predict the next character. What I don't fully understand is how I would go about predicting the very first character when there's no preceding sequence yet (or, by extension, predicting a character when there's not a long enough preceding sequence as input to the LSTM units).</p>

<p>The best solution I can thing of is to reserve a special character in the vocabulary to represent the abscence of any characters. A toy example:</p>

<pre><code>Full training corpus: ""foo""
LSTM unit count: 2
""Absent character"" symbol: ABSENT
""End of file"" symbol: EOF
Training inputs:
    sample: [ABSENT, ABSENT] label: 'f'
    sample: [ABSENT, 'f'] label: 'o'
    sample: ['f', 'o'] label: 'o'
    sample: ['o', 'o'] label: EOF
</code></pre>

<p>My question is: what's the best practice for doing this type of thing? Am I on the right track?</p>
","language-model"
"33019","NLP - extract sentence parts related to people","2018-06-12 11:57:36","33134","0","193","<machine-learning><nlp><text-mining><language-model>","<p>Thank you for your help, I appreciate your time.</p>
","language-model"
"30211","Fasttext exception error","2018-04-12 06:43:21","","2","1315","<machine-learning><language-model>","<p>I'm trying to run language detection using Facebook's fastText through a Python script but I get this error when I load the model :</p>

<blockquote>
  <p>Exception: fastText: Cannot load lid.176.bin due to C++ extension failed to allocate the memory</p>
</blockquote>

<p>Any ideas? I've already installed all prerequisites and still get this error.</p>
","language-model"
"28598","Word2Vec embeddings with TF-IDF","2018-03-04 12:07:33","","17","34038","<machine-learning><nlp><word2vec><language-model><tfidf>","<p>When you train the word2vec model (using for instance, gensim) you supply a list of words/sentences.  But there does not seem to be a way to specify weights for the words calculated for instance using TF-IDF.  </p>

<p>Is the usual practice to multiply the word vector embeddings with the associated TF-IDF weight? Or can word2vec organically take advantage of these somehow?</p>
","language-model"
"28538","Word2Vec, softmax function","2018-03-02 20:33:17","28560","1","2762","<machine-learning><nlp><word2vec><language-model>","<p>I was going term by term through the softmax function for the word2vec (SKIP-GRAM) model.  I found  most definition of these functions to be not 'clear'  so I modified the notation to make sure I understand it. </p>

<p><strong>Is the following  formulation correct?</strong></p>

<p>$$P(w_{-t} | w_{t} ; \theta) = softmax(score(w_{-t}, w_t))$$</p>

<p>$$P(w_{-t} | w_{t} ; \theta) = \frac{exp(score(w_{-t}, w_t))}{\sum_{w' \in \theta} exp(score(w', w_{t}))}$$</p>

<p>where:   </p>

<p>$w_{-t} =$ context</p>

<p>$w_{t} =$ target word</p>

<p>$score(A,B)$ a measure of similarity between vectors A and B.</p>

<p>$\theta = $ vector representation for all words in vocabulary </p>

<p>In the simplest case:</p>

<p>$$score(A ,B) = A \cdot B$$</p>

<hr>
","language-model"
"27939","How do we pass data to a RNN?","2018-02-18 01:26:48","","4","224","<deep-learning><rnn><language-model><nlp>","<p>Let's say we have A1, A2, ... , Am different articles in the corpus and each of them has W1, W2, ....., Ww words. We are training a language model on them. Do we:</p>

<p><strong>Scheme 1</strong></p>

<ol>
<li>Take the first batch of data as first S (Number of time steps) (S1, S2, .., Ss)words from each article (for the sake of simplicity let us assume batch size = m)</li>
<li>Set the initial hidden state H0 = $[0,0,..,0]$</li>
<li>Calculate loss and gradient on this batch and update the parameters</li>
<li>Move a word forward and get next S words from each article (hence S2, S3, ... , Ss are the same words as in the previous batch)</li>
<li>Use H1 calculated from previous step and use it as H0 in this iteration</li>
<li>Do this to the end</li>
</ol>

<p>*In this scheme we would have to use zero padding on the last batch (at the end of articles)</p>

<p><strong>Scheme 2</strong>
Same as Scheme 1 but in step 5 we reinitialize H0 to a vector of Zeroes</p>

<p><strong>Scheme 3</strong>
Same as scheme 1 but in step 4 we move s words forward to next non overlapping s words in each article and initialize H0 to Hs from last iteration</p>

<p><strong>Scheme 4, 5, 6</strong>
Same as Scheme 1, 2, 3 but instead of taking s consecutive words we take first sentence from each article and zero pad them to the length S</p>

<p>What is the right way to go through the data in feeding it to a RNN, Please give reasons as well. I think it should be Scheme 1 but it could be really slow</p>
","language-model"
"18057","Hidden Markov Models: Linking states to labels after EM training","2017-04-02 10:29:23","","1","95","<machine-learning><unsupervised-learning><probability><language-model><expectation-maximization>","<p>The tl;dr version first:</p>

<p>I have the following problem: I implemented Baum Welch for ergodic HMMs. I do it like this:</p>

<p>I pass the model two number <code>C1</code> and <code>C2</code>, it builds a fully connected state machine with <code>C1</code> states and <code>C2</code> emissions. I map all tokens from my training data onto the range <code>[0, C2)</code> and each label the HMM is supposed to assign a token during inference onto <code>[0, C1)</code>. Then the HMM goes ahead and does Baum Welch Learning. When it is done it has configured its state machine to maximize the likelihood of the training data (locally).</p>

<p>Now to my problem:</p>

<p>Assume I had two isomorphic initial state machines (isomorphic under consideration of all the probabilities, as structurally all the HMMs are isomorphic anyway, because ergodic). They vary only in their state IDs, so the IDs have been scrambled around from one machine to the other.
Now, after training both HMMs will be isomorphic again when trained with the same data. That means there is absolutely no connection between the IDs I map the labels from my label set to and the IDs of the states of the HMM. So how then can I interpret the HMM after training? How do I know which state corresponds to which POS-tag? Seems impossible, so I guess I am missing some crucial point.</p>

<hr>

<p>Now a little more detail if the above was unclear:</p>

<p>I take my training data (texts, like newspaper etc.) and count the number of different words (types).</p>

<p>Then I pass <code>count(types</code> and <code>count(labels)</code> the labels being a set of POS-tags. It then randomly constructs a probabilistic fully connected state machine with <code>pow(count(labels), order_of_model)</code> different states. <code>order_of_model</code> being the number of hidden variables (POS-tag ngrams) that get combined into an individual state. It also assigns each of these states an initial and an emission probability for all of the types.</p>

<p>The model assumes a mapping from <code>[0, pow(count(labels), order_of_model))</code> as state IDs onto external tuples of labels. And a mapping <code>[0,count(types)-1)</code> for the emissions onto words.</p>
","language-model"
"16603","What tools are available for programming language parsing for ML?","2017-01-29 11:34:11","","3","307","<nlp><language-model>","<p>I want to preform a machine learning task (e.g. supervised classification, clustering) on a corpus of programming language source code (lets say Python), and I'm looking for tools for parsing and constructing structures out of a document of Python (for example) code, similar in concept to NLP tokenization and higher level language processing, adjusted for programming languages.</p>

<p>While I'm able to find a lot of general NLP related material, it seems to me a better starting point would be previous research specifically focusing on programming languages.</p>

<p>I'm looking for tools, resources, academia articles and keywords to search for, basically any help is appreciated!</p>
","language-model"
"16348","Words to numbers faster lookup","2017-01-16 17:04:05","16356","5","3803","<python><language-model><sentiment-analysis><encoding>","<p>I'm training an LSTM for sentiment analysis on a review dataset downloaded from <a href=""http://www.cs.jhu.edu/~mdredze/datasets/sentiment/"" rel=""noreferrer"">here</a>. The music review dataset contains about 150K data points (reviews of varying length labelled pos or neg). After creating a dictionary, I'm running a script in Python to replace strings (words) with numbers that keras/theano will embed later. </p>

<p>The problem is that such a large dataset requires a lot of time for lookup. I would appreciate if anyone had suggestion on a tool for faster lookup or similar. Currently I just loop through every word in the corpus and replace it with the corresponding number from the dictionary (1-hot encoding essentially)</p>

<p>EDIT: </p>

<p>I'm doing roughly the following: each Python list is a sentence (before tokenization here):</p>

<p>['noble', 'interesting_superlatives',...,'the_idea']</p>

<p>which I want to conver to a list of integers, like:</p>

<p>[143599, 12387,...,7582]</p>

<p>I referred to it (probably incorrectly) as one-hot encoding because for each word there is exactly one number in the dictionary. </p>
","language-model"
"15085","Diminishing returns in language identification data set size?","2016-11-12 23:53:47","","0","149","<dataset><nlp><language-model>","<p>Most problems have a curve whereby the results improve as data are added but level off at some point.</p>

<p>Are there research papers or industry results that discuss the correlation between data set size and prediction accuracy for natural <a href=""https://en.wikipedia.org/wiki/Language_identification"" rel=""nofollow noreferrer"">language identification</a>?</p>
","language-model"
"14838","Given one language ngram model, how do I compare likelihoods of two texts of different length?","2016-10-30 22:28:13","","1","421","<nlp><nltk><language-model><sequence><text-generation>","<p>Let's say I have conditional probabilities estimates for N-grams and I want to find out which of the two sequences of different length 'looks more natural' in terms of the given model. </p>

<p>How does one do this? Is it something to do with perplexity?</p>

<p>Would also be glad if someone posted a reference to a reasonable Python implementation of the suggested method [a code snippet or a link to nltk/spacy/whatever method].</p>

<p>Thanks in advance.</p>
","language-model"
"14187","What is the difference between model hyperparameters and model parameters?","2016-09-24 11:24:50","14194","49","57347","<machine-learning><parameter><hyperparameter><language-model>","<p>I have noticed that such terms as model <strong>hyperparameter</strong> and model <strong>parameter</strong> have been used interchangeably on the web without prior clarification. I think this is incorrect and needs explanation. Consider a machine learning model, an SVM/NN/NB based classificator or image recognizer, just anything that first springs to mind. </p>

<p>What are the <strong>hyperparameters</strong> and <strong>parameters</strong> of the model?<br>
Give your examples please.</p>
","language-model"
"13030","how much text data is required for a meaningful use of word2vec","2016-07-27 15:38:08","","4","2884","<machine-learning><neural-network><word2vec><language-model>","<p>how much data does <a href=""https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html"" rel=""nofollow"">word2vec</a> require?  Are there any public data sets that are useful?</p>

<p>For example, could it be that 1000 newspaper articles are enough to use word2vec?  </p>

<p>Here is a word2vec tutorial from Kaggle that uses <a href=""https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors"" rel=""nofollow"">50,000 movie reviews</a>.  I am trying to understand the scale of the word2vec input.</p>
","language-model"
"12905","Can finite state machines be encoded as input/output for a neural network?","2016-07-21 09:48:54","","7","4283","<neural-network><classification><language-model>","<p>I want to encode finite state machines (specifically DFAs) as output (or input) of a neural network for a supervised learning task.</p>

<p>Are there any ways in the literature for doing this?</p>

<p>I've already found some algorithms being able to extract a DFA from a recurrent neural network, but nothing about DFAs either as input or output of ANN.</p>
","language-model"
"11754","In plain English, how to descibe i/o of the TensorFlow for language modelling?","2016-05-13 18:58:48","","2","154","<deep-learning><tensorflow><language-model>","<p>I have followed the tutorial <a href=""https://www.tensorflow.org/versions/r0.8/tutorials/recurrent/index.html#language_modeling"" rel=""nofollow"">here</a> about language modelling using Tensorflow to create LSTM and used PTB dataset. The code is <a href=""https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py"" rel=""nofollow"">here</a></p>

<p>I failed to understnad the exact specific input and the output of the model. </p>

<p>My question is: how many words do you feed to the model and how many words do you expect at the end? How the data processing happens?</p>

<p>For example: if I have the sentence ""The quick brown fox jumps over the lazy dog."". This will be turned into vectors using word2vec. Then how are these given to the NN?</p>
","language-model"
"10927","Stanford NER Training - Assign weight to each word","2016-03-28 13:43:29","","2","311","<nlp><language-model><stanford-nlp>","<p>I am using the Stanford NER to recognize each entity in a search text. Once I identify entities, I need to pass that entities to an algorithm which calculates score for each entity type (e.g. country, customer) using weights of each word. Currently my training data has word and answer like below:</p>
<pre><code>country_training.tsv
    brazil country
    japan country
    
customer_training.tsv
    hyundai customer
    apple customer
</code></pre>
<p>How can I associate weight to each above training data like below so when I can get weight of each word too?</p>
<pre><code>country_training.tsv
    brazil country 1.5
    japan country 4.0
    
customer_training.tsv
    hyundai customer 4.0
    apple customer 2.3
</code></pre>
<p>Please advise.</p>
<p>Weights are input to NER to annotate each entity wiith their weights.</p>
","language-model"
"10558","How does Alexa utterance parsing work?","2016-03-06 18:33:58","","3","1972","<nlp><language-model>","<p>What are the basic principles/tools necessary to make something like Alexa utterance parsing?</p>

<p>For reference, Alexa allows a designer to define phrases with ""placeholders"" that will be filled in. For example, the phrase ""what is the horoscope for gemini"" would match the underlying model (below), and return <code>Sign=gemini</code>.</p>

<pre><code>what's the horoscope for {Sign}
what is the horoscope for {Sign}
give me the horoscope for {Sign}
tell me the horoscope for {Sign}
</code></pre>

<p>To clarify: I am interested in the theory behind how the language model and parsing (with regard to theory and algorithms) work, so I can build my own version.</p>
","language-model"
"9558","Neural Networks for Predictive typing","2015-12-30 16:11:20","9566","2","799","<machine-learning><neural-network><nlp><language-model>","<p>I don't have a background in neural networks. But, various studies has been proved that neural networks (feed forward / Recurrent) outperformed n-gram language modeling for predicting words in a sequence. But, in an application to text messaging or any text-based conversation, where the language which is most likely used will be more informal or colloquial. Can still a neural networks perform well than n-gram LM? Considering the data to be fed are the text messages (colloquial phrases). If so, please enlighten me, thanks.</p>
","language-model"
"6153","Importance of Random initialisation VS number of hidden units","2015-06-19 07:37:44","","2","37","<neural-network><rnn><language-model>","<p>A question crossed my mind not so long ago: I am doing experiments on Language Model with RNN (always with the same network topology: 50 hidden units, and 10M &quot;directs connections&quot; that are emulating N_grams models) and different fraction of corpus (10,25,50,75,100%) (9M words).</p>
<p>I noticed that while perplexity seems to decrease when the training data become more abundant, certain times it does not.</p>
<p>Last example : 143 118 109 106 112</p>
<p>My first thought was network initialization, so I began testing with a smaller corpus and 20 hidden units (for technical reasons. Even with 10% corpus, learning can take up to 30h, which is problematic for me), and I found after 50 tries that all nets converged on values within 3% of each other.</p>
<p>But, I thought that maybe the importance of this initialization is a function of the number of hidden units? I mean the more hidden units the more parameters to tune.</p>
<p>Also, maybe my stop criterion is too sensitive (It stops if evolution of perplexity between two iterations is inferior to a certain number).
Do you think it would make an impact to allow it to run one of two iterations after the criterion was met to see if it was just a local thing?</p>
","language-model"
"5893","How to create a good list of stopwords","2015-05-24 21:45:02","5902","10","18933","<data-mining><nlp><information-retrieval><language-model>","<p>I am looking for some hints on how to curate a list of stopwords. Does someone know / can someone recommend a good method to extract stopword lists from the dataset itself for preprocessing and filtering?</p>

<p><strong>The Data:</strong></p>

<p>a huge amount of human text input of variable length (searchterms and whole sentences (up to 200 characters) ) over several years. The text contains a lot of spam (like machine input from bots, single words, stupid searches, product searches ... ) and only a few % of seems to be useful. I realised that sometimes (only very rarely) people search my side by asking really cool questions. These questions are so cool, that i think it is worth to have a deeper look into them to see how people search over time and what topics people have been interested in using my website.</p>

<p><strong>My problem:</strong></p>

<p>is that i am really struggling with the preprocessing (i.e. dropping the spam). I already tried some stopword list from the web (NLTK etc.), but these don't really help my needs regarding this dataset. </p>

<p>Thanks for your ideas and discussion folks!</p>
","language-model"
"1102","Improve CoreNLP POS tagger and NER tagger?","2014-09-11 17:09:52","1103","4","1314","<nlp><language-model>","<p>The CoreNLP parts of speech tagger and name entity recognition tagger are pretty good out of the box, but I'd like to improve the accuracy further so that the overall program runs better. To explain more about accuracy -- there are situations in which the POS/NER is wrongly tagged. For instance:</p>

<ul>
<li>""Oversaw car manufacturing"" gets tagged as NNP-NN-NN</li>
</ul>

<p>Rather than VB* or something similar, since it's a verb-like phrase (I'm not a linguist, so take this with a grain of salt).</p>

<p>So what's the best way to accomplish accuracy improvement?</p>

<ul>
<li>Are there better models out there for POS/NER that can be incorporated into CoreNLP?</li>
<li>Should I switch to other NLP tools?</li>
<li>Or create training models with exception rules?</li>
</ul>
","language-model"
"129","What is generative and discriminative model? How are they used in Natural Language Processing?","2014-05-18 06:17:37","166","10","1637","<nlp><language-model>","<p><a href=""https://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm"">This question</a> asks about generative vs. discriminative algorithm, but can someone give an example of the difference between these forms when applied to Natural Language Processing? <strong>How are generative and discriminative models used in NLP?</strong></p>
","language-model"