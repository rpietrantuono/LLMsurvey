Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"129881","Why Test MSE is better than Validation MSE?","2024-08-05 17:20:19","","0","55","<time-series><transformer>","<p>I am working on time series transformers.
I Have ran code of PatchTST(time series transformer) for etth1  multivariate data set but, I see test loss is always better than validation loss and I have also looked at Infomer (time series transformer)it was also doing same.</p>
<pre><code>EarlyStopping counter: 12 out of 100
Updating learning rate to 4.048376602284338e-09
Epoch: 100 cost time: 1.73805832862854
Epoch: 100, Steps: 63 | Train Loss: 0.4116055 Vali Loss: 0.9261471 Test Loss: 0.4135703
EarlyStopping counter: 13 out of 100
Updating learning rate to 3.643538942055904e-09
&gt;&gt;&gt;&gt;&gt;&gt;&gt;testing : 336_192_PatchTST_ETTh1_ftM_sl336_ll48_pl192_dm16_nh4_el3_dl1_df128_fc1_ebtimeF_dtTrue_Exp_0&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
test 2689
mse:0.41365155577659607, mae:0.4210711419582367, rse:0.6107578277587891
</code></pre>
","transformer"
"129784","Handling multiple variables in multivariate time series forecasting when one variable is of primary interest","2024-07-25 17:29:04","","-1","32","<time-series><transformer>","<p>I'm working on a multivariate time series forecasting problem where I have multiple input variables.</p>
<ol>
<li>I have 7 features in the dataset(etth1), When I am looking at some existing libraries that uses this for transformers (Reformer,autoformer)They were considering target variable as 'OT' which is one of the 7 features.</li>
<li>when they split data in batches it looks like these</li>
</ol>
<pre><code>train 8209
Train data shape: batch_x torch.Size([128, 336, 7]), batch_y torch.Size([128, 144, 7]), 
val 2785
Validation data shape: batch_x torch.Size([128, 336, 7]), batch_y torch.Size([128, 144, 7]), 
test 2785
Test data shape: batch_x torch.Size([128, 336, 7]), batch_y torch.Size([128, 144, 7])

</code></pre>
<p>This includes the target variable they have been mentioning</p>
<ol>
<li>Now I need to know in general when they say MSE and MAE are they calculating for all the variables together or they train for all variables and do MSE and MAE just for that target variable?</li>
<li>Is it generally better to predict all variables or should I modify the model to focus only on the primary target variable?</li>
</ol>
<p><a href=""https://github.com/google/trax/tree/master/trax/models/reformer"" rel=""nofollow noreferrer"">https://github.com/google/trax/tree/master/trax/models/reformer</a></p>
","transformer"
"129783","Keras Transformer regression model not predicting values beyond a threshold","2024-07-25 14:53:41","","0","16","<deep-learning><nlp><keras><tensorflow><transformer>","<p>I am working on a keras transformer model for regression and I am getting prediction values which are cut off to some specific threshold.</p>
<p>Code:</p>
<pre class=""lang-py prettyprint-override""><code>def transformer_block(self,inputs, embed_dim, num_heads, ff_dim, dropout_rate):
        
    x = inputs
    x = MultiHeadAttention(key_dim=embed_dim,num_heads=num_heads,dropout=dropout_rate)(x, x)
    #x = Dropout(dropout_rate)(x)
    res = x + inputs

    # Feed Forward Part
    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation=&quot;relu&quot;)(x)
    x = Dropout(dropout_rate)(x)
    x = Dense(inputs.shape[-1])(x)
    return x + res
# Build the model
def create_model(self, embed_dim = 64, num_heads = 6,
        ff_dim = 128, num_transformer_blocks = 3,
        dropout_rate = 0.1):
        
    vocab_size,input_length = self.vocab_size,self.input_length,
    inputs = tf.keras.Input(shape=(input_length,), dtype=tf.int32)
    embedding_layer = keras.layers.Embedding(input_dim=vocab_size,
    output_dim=embed_dim)(inputs)

    x = embedding_layer
    
    for _ in range(num_transformer_blocks):
        x = self.transformer_block(x, embed_dim, num_heads, ff_dim, dropout_rate)
        x = LayerNormalization(epsilon=1e-6)(x) 
        x = tf.keras.layers.GlobalAveragePooling1D()(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
        
    f = ff_dim
        
    while(f//2 &gt; 1):                
        x = tf.keras.layers.Dense(f//2, activation='relu')(x)
        x = tf.keras.layers.Dropout(dropout_rate)(x)
        f = f//2
            
    outputs = tf.keras.layers.Dense(1, activation = 'linear')(x)  # Regression output
    
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model
</code></pre>
<p>True vs Predicted Graph:
<a href=""https://i.sstatic.net/ew7LWGvI.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ew7LWGvI.png"" alt=""&quot;True vs Predicted Graph&quot;"" /></a></p>
<p>A previous question mentioned not to use tanh or sigmoid activation functions but I am only using relu (I also tried gelu and leaky relu). I tried adding more transformer layers but still get this problem. Can some one explain why I'm getting this and how to mitigate it?</p>
","transformer"
"129708","Implementing pytorch temporal fusion transformer on time series","2024-07-15 20:37:11","","0","16","<time-series><pytorch><transformer>","<p>I am trying to run the temporal fusion transformer from the pytorch package. I am trying to compare the output on like terms to the tensorflow output in this paper p. 15 <a href=""https://arxiv.org/pdf/1912.09363"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1912.09363</a> the 50th and 90th quantiles.</p>
<p>I run the following code, trying to get output using two different modes but I don't understand what the output is supposed to represent. Can you please let me know how to interpret it and how to get the output that I am looking for?</p>
<p>Sorry my formatting is a bit off. Any help is appreciated.</p>
<p>Thanks</p>
<pre><code>
df.categorical_time_on_day = df.categorical_time_on_day.astype(str)

df.categorical_day_of_week = df.categorical_day_of_week.astype(str)

df.sensor_day = df.sensor_day.astype(str)

df.categorical_id = df.categorical_id.astype(str)


# Set the date column as the index
#df.set_index('date', inplace=True)

train, test = train_test_split(df, test_size=0.2, shuffle=False)

max_prediction_length = 24

max_encoder_length = 168

#training_cutoff = data[&quot;time_idx&quot;].max() - max_prediction_length

training = TimeSeriesDataSet(

    df,

    time_idx=&quot;hours_from_start&quot;,

    target=&quot;values&quot;,

    group_ids=[&quot;categorical_id&quot;],

    min_encoder_length=max_encoder_length  // 2,  # keep encoder length long (as it is in the validation set)

    max_encoder_length=max_encoder_length,

    min_prediction_length=1,

    max_prediction_length=max_prediction_length,

    #static_categoricals=[&quot;categorical_id&quot;],

    #static_reals=[&quot;avg_population_2017&quot;, &quot;avg_yearly_household_income_2017&quot;],

    time_varying_known_categoricals=[&quot;categorical_time_on_day&quot;, &quot;categorical_day_of_week&quot;,&quot;sensor_day&quot;],

   # variable_groups={&quot;special_days&quot;: special_days},  # group of categorical variables can be treated as one variable

    time_varying_known_reals=[&quot;prev_values&quot;],

    #time_varying_unknown_categoricals=[],

    #time_varying_unknown_reals=[
    #    &quot;volume&quot;,
    #    &quot;log_volume&quot;,
    #    &quot;industry_volume&quot;,
    #    &quot;soda_volume&quot;,
    #    &quot;avg_max_temp&quot;,
    #    &quot;avg_volume_by_agency&quot;,
    #    &quot;avg_volume_by_sku&quot;,
    #],

    target_normalizer=GroupNormalizer(
        groups=[&quot;categorical_id&quot;], transformation=&quot;softplus&quot;
    ),  # use softplus and normalize by group

    add_relative_time_idx=False,

    add_target_scales=False,

    add_encoder_length=False,
)

# create validation set (predict=True) which means to predict the last max_prediction_length points in time

# for each series
test = TimeSeriesDataSet.from_dataset(training, df, predict=True, stop_randomization=True)

# create dataloaders for model

batch_size = 128  #128  # set this between 32 to 128

train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)

test_dataloader = test.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)

# configure network and trainer
pl.seed_everything(42)


trainer = pl.Trainer(
    accelerator=&quot;cpu&quot;,

    max_epochs=100,

    enable_model_summary=True,

    # clipping gradients is a hyperparameter and important to prevent divergance
    # of the gradient for recurrent neural networks
    gradient_clip_val=100 #from 0.1
)

hidden_size = 16


tft = TemporalFusionTransformer.from_dataset(
    training,

    # not meaningful for finding the learning rate but otherwise very important
    learning_rate=0.001,

    hidden_size=hidden_size,  # most important hyperparameter apart from learning rate
    # number of attention heads. Set to up to 4 for large datasets
    attention_head_size=4,

    dropout=0.3,  # between 0.1 and 0.3 are good values

    hidden_continuous_size=hidden_size,  # set to &lt;= hidden_size

    loss=QuantileLoss(),

    optimizer=&quot;adam&quot;

    # reduce learning rate if no improvement in validation loss after x epochs
    # reduce_on_plateau_patience=1000,
)

print(f&quot;Number of parameters in network: {tft.size()/1e3:.1f}k&quot;)

predictions = tft.predict(test_dataloader, mode = &quot;quantiles&quot;, return_y=True)

predictions```

which gives:

INFO: GPU available: False, used: False
INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
WARNING: Missing logger folder: /content/lightning_logs
WARNING:lightning.pytorch.loggers.tensorboard:Missing logger folder: /content/lightning_logs
Prediction(output=tensor([[[0.0330, 0.0346, 0.0266, 0.0362, 0.0375, 0.0689, 0.0364],
         [0.0761, 0.0426, 0.0341, 0.0446, 0.0522, 0.0253, 0.0662],
         [0.0307, 0.0386, 0.0391, 0.0653, 0.0517, 0.0517, 0.0469],
         [0.0371, 0.0421, 0.0346, 0.0317, 0.0463, 0.0788, 0.0365],
         [0.0256, 0.0466, 0.0452, 0.0526, 0.0898, 0.0388, 0.0578],
         [0.0425, 0.0272, 0.0325, 0.0922, 0.0923, 0.0455, 0.0418],
         [0.0381, 0.0326, 0.0362, 0.0779, 0.0669, 0.0574, 0.0438],
         [0.0468, 0.0299, 0.0340, 0.0567, 0.0409, 0.0568, 0.0470],
         [0.0320, 0.0359, 0.0277, 0.0580, 0.0631, 0.0613, 0.0467],
         [0.0741, 0.0375, 0.0369, 0.0467, 0.0379, 0.0307, 0.0513],
         [0.0483, 0.0328, 0.0356, 0.0672, 0.0500, 0.0530, 0.0455],
         [0.0304, 0.0296, 0.0501, 0.0340, 0.0580, 0.0575, 0.0743],
         [0.0451, 0.0250, 0.0379, 0.0563, 0.0381, 0.0484, 0.0425],
         [0.0673, 0.0318, 0.0561, 0.0323, 0.0388, 0.0343, 0.0729],
         [0.0458, 0.0311, 0.0341, 0.0680, 0.0522, 0.0557, 0.0461],
         [0.0377, 0.0232, 0.0357, 0.0614, 0.0481, 0.0731, 0.0454],
         [0.0348, 0.0280, 0.0349, 0.0633, 0.0542, 0.0671, 0.0458],
         [0.0480, 0.0312, 0.0366, 0.0769, 0.0614, 0.0491, 0.0456],
         [0.0427, 0.0302, 0.0336, 0.0680, 0.0525, 0.0616, 0.0456],
         [0.0388, 0.0306, 0.0283, 0.0855, 0.0629, 0.0661, 0.0379],
         [0.0467, 0.0463, 0.0365, 0.0756, 0.0919, 0.0376, 0.0466],
         [0.0288, 0.0301, 0.0322, 0.0827, 0.0968, 0.0543, 0.0427],
         [0.0416, 0.0608, 0.0332, 0.0338, 0.0371, 0.0683, 0.0340],
         [0.0352, 0.0335, 0.0307, 0.0950, 0.0811, 0.0558, 0.0361]],

        [[0.0194, 0.0207, 0.0135, 0.0225, 0.0234, 0.0619, 0.0226],
         [0.0739, 0.0291, 0.0200, 0.0308, 0.0395, 0.0124, 0.0594],
         [0.0171, 0.0248, 0.0250, 0.0572, 0.0387, 0.0392, 0.0343],
         [0.0235, 0.0282, 0.0202, 0.0183, 0.0332, 0.0785, 0.0225],
         [0.0128, 0.0335, 0.0314, 0.0407, 0.0954, 0.0246, 0.0475],
         [0.0285, 0.0138, 0.0187, 0.1018, 0.1017, 0.0324, 0.0281],
         [0.0270, 0.0170, 0.0209, 0.0646, 0.0445, 0.0507, 0.0332],
         [0.0344, 0.0162, 0.0201, 0.0440, 0.0258, 0.0451, 0.0349],
         [0.0316, 0.0138, 0.0181, 0.0321, 0.0197, 0.0420, 0.0464],
         [0.0542, 0.0156, 0.0196, 0.0350, 0.0183, 0.0310, 0.0502],
         [0.0379, 0.0182, 0.0210, 0.0510, 0.0309, 0.0407, 0.0350],
         [0.0143, 0.0164, 0.0405, 0.0197, 0.0563, 0.0460, 0.0740],
         [0.0305, 0.0114, 0.0246, 0.0487, 0.0253, 0.0321, 0.0262],
         [0.0614, 0.0173, 0.0497, 0.0193, 0.0278, 0.0194, 0.0703],
         [0.0304, 0.0183, 0.0203, 0.0693, 0.0465, 0.0436, 0.0310],
         [0.0233, 0.0106, 0.0215, 0.0546, 0.0365, 0.0704, 0.0316],
         [0.0161, 0.0147, 0.0213, 0.0662, 0.0604, 0.0597, 0.0292],
         [0.0355, 0.0182, 0.0235, 0.1025, 0.0758, 0.0262, 0.0298],
         [0.0243, 0.0194, 0.0207, 0.0816, 0.0617, 0.0465, 0.0268],
         [0.0229, 0.0179, 0.0126, 0.1265, 0.0750, 0.0587, 0.0194],
         [0.0342, 0.0349, 0.0214, 0.0760, 0.1028, 0.0220, 0.0328],
         [0.0154, 0.0176, 0.0177, 0.0921, 0.1138, 0.0400, 0.0276],
         [0.0278, 0.0517, 0.0190, 0.0199, 0.0227, 0.0622, 0.0200],
         [0.0211, 0.0195, 0.0171, 0.1059, 0.0813, 0.0448, 0.0222]],

        [[0.0781, 0.0383, 0.0384, 0.0767, 0.0456, 0.0623, 0.0667],
         [0.0855, 0.0636, 0.0529, 0.0739, 0.0949, 0.0350, 0.0862],
         [0.0366, 0.0318, 0.0470, 0.0706, 0.0951, 0.0597, 0.0834],
         [0.0609, 0.1179, 0.0466, 0.0197, 0.0283, 0.0774, 0.0499],
         [0.0421, 0.0414, 0.0483, 0.1358, 0.1277, 0.0716, 0.0526],
         [0.0701, 0.1052, 0.0321, 0.0556, 0.0622, 0.0707, 0.0316],
         [0.1205, 0.0823, 0.0520, 0.0741, 0.0896, 0.0201, 0.0812],
         [0.0708, 0.0906, 0.0612, 0.1050, 0.0872, 0.0300, 0.0536],
         [0.0756, 0.0838, 0.0589, 0.0753, 0.0873, 0.0397, 0.0454],
         [0.0627, 0.0833, 0.0629, 0.0969, 0.1090, 0.0279, 0.0610],
         [0.0718, 0.0559, 0.0531, 0.1245, 0.1142, 0.0323, 0.0517],
         [0.0701, 0.0725, 0.0601, 0.1100, 0.1037, 0.0286, 0.0558],
         [0.0659, 0.0613, 0.0538, 0.1447, 0.1049, 0.0356, 0.0451],
         [0.0589, 0.0890, 0.0455, 0.1189, 0.1162, 0.0328, 0.0494],
         [0.1026, 0.0700, 0.0565, 0.1031, 0.0814, 0.0241, 0.0531],
         [0.0815, 0.0676, 0.0560, 0.1157, 0.0919, 0.0293, 0.0512],
         [0.0632, 0.0711, 0.0768, 0.0742, 0.1056, 0.0290, 0.0787],
         [0.0862, 0.0618, 0.0576, 0.1152, 0.0873, 0.0287, 0.0487],
         [0.1240, 0.0520, 0.0925, 0.0486, 0.0633, 0.0221, 0.0928],
         [0.0849, 0.0688, 0.0552, 0.1114, 0.0931, 0.0282, 0.0530],
         [0.0754, 0.0464, 0.0557, 0.1264, 0.0801, 0.0450, 0.0518],
         [0.0782, 0.0673, 0.0561, 0.1118, 0.0915, 0.0314, 0.0519],
         [0.0851, 0.0652, 0.0565, 0.1111, 0.0927, 0.0283, 0.0530],
         [0.0911, 0.0935, 0.0428, 0.0950, 0.0721, 0.0256, 0.0382]]]), x=None, index=None, decoder_lengths=None, y=(tensor([[0.0103, 0.0083, 0.0082, 0.0110, 0.0209, 0.0499, 0.0844, 0.0694, 0.0664,
         0.0675, 0.0656, 0.0637, 0.0644, 0.0646, 0.0611, 0.0610, 0.0605, 0.0623,
         0.0511, 0.0381, 0.0336, 0.0278, 0.0195, 0.0142],
        [0.0060, 0.0043, 0.0042, 0.0087, 0.0233, 0.0597, 0.0882, 0.1922, 0.1318,
         0.0784, 0.0587, 0.0512, 0.0509, 0.0577, 0.0593, 0.0496, 0.0442, 0.0444,
         0.0364, 0.0301, 0.0266, 0.0260, 0.0205, 0.0131],
        [0.0765, 0.0648, 0.0575, 0.0474, 0.0348, 0.0228, 0.0164, 0.0153, 0.0116,
         0.0114, 0.0186, 0.0251, 0.0390, 0.0519, 0.0647, 0.0691, 0.0775, 0.0855,
         0.0845, 0.0934, 0.0910, 0.0942, 0.1637, 0.0794]]), None))

</code></pre>
<p>and</p>
<pre><code>predictions = tft.predict(test_dataloader, mode = &quot;prediction&quot;, return_y=False)
</code></pre>
<p>which gives:</p>
<pre><code>INFO: GPU available: False, used: False
INFO:lightning.pytorch.utilities.rank_zero:GPU available: False, used: False
INFO: TPU available: False, using: 0 TPU cores
INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores
INFO: HPU available: False, using: 0 HPUs
INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs
tensor([[0.0362, 0.0446, 0.0653, 0.0317, 0.0526, 0.0922, 0.0779, 0.0567, 0.0580,
         0.0467, 0.0672, 0.0340, 0.0563, 0.0323, 0.0680, 0.0614, 0.0633, 0.0769,
         0.0680, 0.0855, 0.0756, 0.0827, 0.0338, 0.0950],
        [0.0225, 0.0308, 0.0572, 0.0183, 0.0407, 0.1018, 0.0646, 0.0440, 0.0321,
         0.0350, 0.0510, 0.0197, 0.0487, 0.0193, 0.0693, 0.0546, 0.0662, 0.1025,
         0.0816, 0.1265, 0.0760, 0.0921, 0.0199, 0.1059],
        [0.0767, 0.0739, 0.0706, 0.0197, 0.1358, 0.0556, 0.0741, 0.1050, 0.0753,
         0.0969, 0.1245, 0.1100, 0.1447, 0.1189, 0.1031, 0.1157, 0.0742, 0.1152,
         0.0486, 0.1114, 0.1264, 0.1118, 0.1111, 0.0950]])```
</code></pre>
<p>This doesn't run but I think it doesn't make sense to predict on test when train hasn't been fit but I don't understand how this model works.</p>
<pre><code> model = trainer.fit(
    tft,

    train_dataloaders=train_dataloader,

    val_dataloaders=test_dataloader,
)
predictions = model.predict(test_dataloader, mode = &quot;quantiles&quot;, return_y=True)

</code></pre>
","transformer"
"129572","Does it common for LM (hundreds million parameters) beat LLM (billion parameters) for binary classification task?","2024-07-01 01:16:10","","0","19","<python><neural-network><nlp><transformer><huggingface>","<p><strong>Preface</strong></p>
<p>I am trying to fine-tune the transformer-based model (LM and LLM). The LM that I used is DEBERTA, and the LLM is LLaMA 3. The task is to classify whether a text contains condescending language (binary classification).</p>
<p>I use <code>AutoModelForSequenceClassification</code>, which adds a classification layer to the model's top layer for both LM and LLM.</p>
<p><strong>Implementation</strong></p>
<ol>
<li><p>Dataset:</p>
<ul>
<li>Amount: it has about 10.000 texts with each text labeled <code>0</code> (for not condescending) and <code>1</code> (condescending). The proportion is <code>1:10</code> (condescending : not condescending).</li>
</ul>
</li>
<li><p>Parameter</p>
</li>
</ol>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Parameter</th>
<th>LM</th>
<th>LLM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Batch size</td>
<td>32</td>
<td>16 (per_device_train_batch_size = 4,    gradient_accumulation_steps = 4)</td>
</tr>
<tr>
<td>Epoch / steps</td>
<td>2 epoch</td>
<td>1000 steps (20% used as validation set)</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>linear (2e-5)</td>
<td>constant (2e-5)</td>
</tr>
<tr>
<td>Optimizer</td>
<td>AdamW (lr = 2e-5, eps = 1e-8)</td>
<td>paged_adamw_32bit</td>
</tr>
<tr>
<td>Fine-tuning</td>
<td>Full fine-tuning</td>
<td>LoRA (rank=32, dropout=0.5, alpha=8) with 8-bit quantization</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>linear (2e-5)</td>
<td>constant (2e-5)</td>
</tr>
<tr>
<td>Precision</td>
<td>0,659</td>
<td>0,836</td>
</tr>
<tr>
<td>Recall</td>
<td>0,47</td>
<td>0,091</td>
</tr>
<tr>
<td>F1-score</td>
<td>0,549</td>
<td>0,164</td>
</tr>
</tbody>
</table></div>
<p><strong>Question and Issue</strong></p>
<p>Here is the log of the training sample. The validation f1-score is always <code>&gt;0.6</code>. But the validation loss is stuck at <code>0.24</code>. It is one of the samples of fine-tuned LLM.</p>
<p><a href=""https://i.sstatic.net/cWtxnVpgm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cWtxnVpgm.png"" alt=""enter image description here"" /></a></p>
<ol>
<li>Why does the test set f1-score only range from 0 - 0.2 for some parameter variation that I tuned when the f1-score for the validation set is always above 0.6, is it reasonable? why?</li>
<li>Is it common for LM to beat LLM for a particular task? If yes, what is the rationalization?</li>
</ol>
","transformer"
"129535","Training a transformer CNN for image output from scratch","2024-06-27 10:27:14","","0","16","<cnn><training><transformer><image-generation>","<p>I'm trying to train a Transformer-CNN model from scratch. The Transformer model is comparable to that of the ViViT model 2. The CNN is taking the output of the second (temporal) transformer and is generating an image output. The input are video files. The goal is to output the moving object in those video files which is partly hidden in those videos.<br />
My architecture (in Pytorch) is as follows (from input to output):</p>
<ul>
<li>TubletEmbedding</li>
<li>Convolution PatchEmbedding (1 Conv3D layer)</li>
<li>Adding of CLS Token</li>
<li>Positional Encoding</li>
<li>(Spatial) TransformerEncoder Layer (dim=512, heads=8, layers=1)</li>
<li>Adding of CLS Token</li>
<li>Global average Pooling</li>
<li>(Temporal) TransformerEncoder Layer (dim=512, heads=8, layers=1)</li>
<li>Decoder Layer
<ul>
<li>Linear</li>
<li>ReLU</li>
<li>Dropout</li>
<li>BatchNorm1d</li>
<li>Unflatten</li>
<li>2x
<ul>
<li>ConvT2d</li>
<li>ReLU</li>
<li>Dropout</li>
<li>BatchNorm2d</li>
</ul>
</li>
<li>ConvT2d</li>
<li>Tanh</li>
</ul>
</li>
</ul>
<p>I have got about 80.000 individual training videos. I use warm up learning for 20 epochs and then Cosine Annealing with Warm Restarts with AdamW optimizer. My Batch Size is 64. For training speed up (I only have one 4090) I use Mixed Precision Training and Gradient Accumulation. My criterion is MSE.</p>
<p><a href=""https://i.sstatic.net/zwg7fo5n.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zwg7fo5n.png"" alt=""Training Loss"" /></a></p>
<p>The model is actually converging pretty fast as can be seen above. The validation loss is very small in the end, but the output is not as desired. It is pretty much a mixture of all possible objects in one place:</p>
<p><a href=""https://i.sstatic.net/CQiZTLrk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/CQiZTLrk.png"" alt=""Output"" /></a></p>
<p>Any ideas?</p>
","transformer"
"129518","Do a transformer's embeddings self-organise the same way as word2vec embeddings?","2024-06-25 22:24:01","","0","14","<word-embeddings><transformer>","<p>Word2vec embeddings are well-known for being able to do vector arithmetic on them. So King - Queen ≈ Man - Woman. Or Germany - Berlin ≈ France - Paris.</p>
<p>When I first learned about transformers, one of the things that surprised me was they were randomly initialized, not taken from word2vec (or similar) embeddings. But, of course, it still works.</p>
<p>But I don't recall ever having seen any analysis of the embeddings at the end of transformer training. My assumption has always been they've stayed fairly random, and there is no (human-understandable) self-organisation going on. I.e. King - Queen ≈ Man - Woman. Or Germany - Berlin ≈ France - Paris would not work. Is this correct?</p>
<p>Note: if so is this a natural consequence of using SentencePiece (or equivalent)? So e.g. embedding #60 is &quot;is&quot;, #117 is &quot;man&quot;, but #118 is &quot;par&quot;, and #119 is &quot;wo&quot;.
Or can you average the embeddings for &quot;wo&quot; and &quot;man&quot; to get an embedding for &quot;woman&quot;, and similarly for &quot;is&quot; and &quot;par&quot; to get &quot;Paris&quot;, and then do useful vector arithmetic on those?</p>
<p>Background Aside: this came up in a discussion of using Maximum Mean Discrepancy to compare the embeddings for two sentences produced by a transformer model, and wondering if it is reasonable to say that a lower value equates to more similar sentences. Maybe this is still a valid assumption even if word2vec-style arithmetic is not possible?</p>
","transformer"
"129446","Transformer model conditional probability distribution of sub-sentences","2024-06-17 11:21:34","","2","45","<loss-function><transformer><probability><monte-carlo>","<p>I have a simple transformer model (decoder only) which is trained on some dataset containing sentences to do next-word prediction. The model captures a probability distribution <span class=""math-container"">$P_{\theta}(\mathbf{a})$</span> over a sentence <span class=""math-container"">$\mathbf{a}$</span> by using the chain rule for probabilities:</p>
<p><span class=""math-container"">$$P_\theta(\mathbf{a}) = P_\theta(a_1,\ a_2,\ \dots ,\ a_N) = \prod_{k}^NP_\theta(a_{k}|a_{&lt;k}),$$</span></p>
<p>where <span class=""math-container"">$a_i$</span> is the <span class=""math-container"">$i$</span>-th word and the <span class=""math-container"">$a_{&lt;k}$</span> notation indicates the sentence up to and not including the <span class=""math-container"">$k$</span>-th word. The conditional probability distributions <span class=""math-container"">$P_\theta(a_{k}|a_{&lt;k})$</span> are obtained auto-regressively, i.e. inputting <span class=""math-container"">$a_{&lt;k}$</span> for all <span class=""math-container"">$k$</span> (length of sentence) into the model and obtaining the (conditional) probability distribution output for the next word in the sentence.</p>
<h1>Goal:</h1>
<p>Extract the probability distribution of a set of <span class=""math-container"">$M$</span> words (sub-sentence) in the sentence of length <span class=""math-container"">$N$</span> given the context of the rest of the sentence: <span class=""math-container"">$P(a_{\{M\}} | a_{\{N\}/\{M\}})$</span>, where I indicated &quot;the rest of the sentence&quot; as <span class=""math-container"">$a_{\{N\}/\{M\}}$</span>.</p>
<p>Now, for sets <span class=""math-container"">$\{M\}$</span> that are &quot;not interrupted&quot; (containing all words up to a certain point in the sentence), it comes down to simply sampling up to that that word, i.e. <span class=""math-container"">$P(a_1, a_0) = P(a_1 | a_0)\cdot P(a_0)$</span>. However, for sets <span class=""math-container"">$\{M\}$</span> that <em>are</em> interrupted this is not the case, e.g. <span class=""math-container"">$P(a_0, a_2|a_1)$</span> for N = 3.</p>
<p>Since the vocabulary is too large, it is not possible to sample all possible combinations <span class=""math-container"">$\mathbf{a}$</span> as the cost is exponential in <span class=""math-container"">$N$</span>. For small systems however this is possible and this is what I am interested in. The goal is to obtain these probability distributions for &quot;sub-sentences&quot; and constrain them in a way (given by the context of the problem) by adding a term to the loss-function.</p>
<h1>My attempt:</h1>
<p>One can approximate the distribution using Monte Carlo. By obtaining a large enough set of samples and simply counting occurrences of the sub-sentences in <span class=""math-container"">$\{M\}$</span>. That way one indeed obtains <span class=""math-container"">$P(a_{\{M\}} | a_{\{N\}/\{M\}})$</span>.</p>
<p>The problem with this approach however is that this approach is not differentiable since sampling from a model is inherently discrete and that causes issues to calculate gradients through the model for back-propagation. I am aware of differentiable variants like the Gumbel-Softmax but it feels like there should be an easier solution that uses the conditional probability distributions.</p>
<h1>Update:</h1>
<p>We know that:
<span class=""math-container"">$$P(a_{\{M\}}|a_{\{N\}/\{M\}}) = \sum_{a_{\{N\}/\{M\}}}P(a_{N}|a_{\{N - 1\}}),$$</span>
which we sample sparsely from the model. This equality holds because by design of the model all conditional distributions are normalized.
Since we cannot sample all <span class=""math-container"">$a_{\{N - 1\}}$</span>), we are not guaranteed the distribution we obtain is normalized. After normalization this yields an estimation of the subsystem probability distribution.</p>
","transformer"
"129353","attentions not returned from transformers ViT model when using output_attentions=True","2024-06-09 19:13:34","129355","0","52","<visualization><transformer><attention-mechanism><huggingface>","<p>I'm using <a href=""https://huggingface.co/transformers/v4.9.1/model_doc/vit.html"" rel=""nofollow noreferrer"">this code snippet</a> from the docs of HuggingFace ViT classification model - with one addition: I'm using the <code>output_attentions=True</code> parameter. Nevertheless, no attentions are returned.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import ViTFeatureExtractor, ViTForImageClassification
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')
model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')

inputs = feature_extractor(images=image, return_tensors=&quot;pt&quot;)
outputs = model(**inputs, output_attentions=True)
logits = outputs.logits

# --&gt; this should print the attentions
print(output.attentions)

# model predicts one of the 1000 ImageNet classes
predicted_class_idx = logits.argmax(-1).item()
print(&quot;Predicted class:&quot;, model.config.id2label[predicted_class_idx])
</code></pre>
<p>The output of <code>print(output.attentions)</code> is:</p>
<pre><code>attentions=(None, None, None, None, None, None, None, None, None, None, None, None)
</code></pre>
<p>What am I doing wrong, and how can I get the attentions values?</p>
","transformer"
"129348","The real world implementations of RAG vs the methods explained in the paper","2024-06-09 08:08:21","","1","24","<nlp><transformer><llm>","<p>While building a RAG application we</p>
<ol>
<li>Encode the query</li>
<li>Retrieve k docs</li>
<li>Concatenate before the query</li>
<li>Pass the entire thing to a LLM and it completes it for you</li>
</ol>
<p>I do not think this is either of RAG-sequence or RAG-token explained in the <a href=""https://arxiv.org/abs/2005.11401"" rel=""nofollow noreferrer"">paper</a>. Or, am I missing something. It seems closer to RAG-sequence but there also there are k output sequences generated which are then marginalized over the retrieved documents and the output sequence with the highest probability is selected</p>
<p>From the paper:</p>
<blockquote>
<p>RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the complete sequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,</p>
</blockquote>
<blockquote>
<p>RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token</p>
</blockquote>
","transformer"
"129298","How to interpret the token embeddings from decoders?","2024-06-04 20:26:05","","0","23","<nlp><transformer>","<p>I am having trouble thinking about the token embeddings from masked attention compared to BERT.</p>
<p>Let's say we have 5 tokens. The embedding of the first token will be used to predict the second token, but we already know what the second token is. If we have only one decoder, then we can just use embedding of the 5th token to predict the next one (the other embeddings can be ignored). However, if we have a second decoder, then the embedding of the first token will be used by that second decoder (the intermediate embeddings from the first decoder are used).</p>
<p>After the first token, there are many possible next tokens. It seems to me that the embeddings for the first tokens won't be very informative (they have very little context). Why would the decoders after the first one pay attention to the intermediate results used to predict the previous tokens (1st, 2nd, 3rd, 4th) when they essentially have access to the final result (5th token)?</p>
<p>Can we think of the embedding of the 5th token as a sentence embedding? Can we do (5th token embedding - 4th token embedding) to obtain something similar to BERT and do token classification with GPTs?</p>
<p>(When I say 1st token, I was thinking of a word. I wonder if the embedding of the &lt;START_TOKEN&gt; is changed throughout the decoder blocks)</p>
","transformer"
"129259","Apply Swin transformer to 1d arrays","2024-06-01 04:51:21","","0","21","<keras><tensorflow><transformer>","<p>My input features are 1d arrays of shape (1000,)</p>
<p>I can tokenize the arrays using tf.extract_patches</p>
<pre><code>block_size = self.token_shape[0]
stride = self.token_shape[0]

tokens = extract_patches(images = reshaped, sizes=[1, 1, block_size, 1], 
                        strides=[1, 1, stride, 1], rates=[1, 1, 1, 1], padding='VALID')
</code></pre>
<p>However, I'm having trouble with using the tokens as input to the SWIN transformer.</p>
<p><a href=""https://keras.io/examples/vision/swin_transformers/"" rel=""nofollow noreferrer"">https://keras.io/examples/vision/swin_transformers/</a></p>
","transformer"
"129175","How contextual embeddings learned during training a transformer are applied to the input sequence at inference time","2024-05-24 13:49:52","","0","14","<machine-learning><deep-learning><word-embeddings><transformer>","<p>I'm trying to understand contextual word embeddings better, and how they are applied at inference time.</p>
<p>While training a transformer, embeddings are learned as parameters during training. Are the final embeddings one embedding per one word, but they encode information about the same word in different contexts? Or are there multiple embeddings for the same word in the embeddings matrix?</p>
<p>The latter seems implausible, because how at inference time could an embedding be applied to an input sequence if there are multiple choices.</p>
<p>The popular example of contextual embeddings is 'bank'. If contextual embeddings for the different contexts of this word are learned during training, how are these embeddings applied at inference time.</p>
","transformer"
"129069","In Swin-Transformer, Is each token (to-embedding) value an integer?","2024-05-14 02:40:45","","0","19","<computer-vision><transformer>","<p>Swin-Transformer transform the image to tokens to input to transformer.</p>
<p>Is each token (before-embedding) value an integer?</p>
<p>In practice, where is this done? <a href=""https://github.com/microsoft/Swin-Transformer"" rel=""nofollow noreferrer"">https://github.com/microsoft/Swin-Transformer</a></p>
<p>The <a href=""https://github.com/microsoft/Swin-Transformer/blob/main/models/swin_transformer_v2.py"" rel=""nofollow noreferrer"">code</a> (
<code>self.head = nn.Linear(self.num_features, num_classes)</code>)
seems not output integers?</p>
","transformer"
"128979","Why do we use similarity/cosine between Query and Key in attention?","2024-05-07 04:45:28","","0","135","<deep-learning><nlp><transformer><attention-mechanism>","<p>Let's take an example sentence for translation:</p>
<p><code>I am going to my home and play with toy house.</code></p>
<p>For translating 'home', as per my understanding, Query will be 'house's embedding vector, Key will be each of the token's vectors i.e size 11 (word based token).</p>
<p>Then we take cosine to find the similarity.</p>
<p>Why? Why similarity? '<strong>Home</strong>' and '<strong>House</strong>' shall be most similar, but that doesn't play a part in the translation. Rather, probably 'toy' is more important here from translation perspective.</p>
","transformer"
"128968","Instruction LLM - extract data from text wrongly continues","2024-05-06 09:04:32","","0","34","<transformer><huggingface><llm>","<p>I'm trying to fine-tune open sourced LLMs, for now let's stick with Mistral-7b-instruct model.</p>
<p>My task is a follow: I have emails, that represents &quot;price requests&quot; for shipments sends by our clients.
In the emails, the clients tells us the pickup address, the shipper, consignee ETC.</p>
<p>My initial idea was to train different adapters, using DORA, each of them is trained on extracting a different entity from the email.</p>
<p>My dataset was created as follow: I have the email, and the annotation which is &quot;Based on the email, I've found this [ENTITY]: entity_here</p>
<p>I've created a System message, and and chat_template to create the dataset in a way Mistral will accept, using this chat_template:</p>
<pre><code>&quot;{%- for message in messages %}&quot;
  &quot;{%- if message['role'] == 'system' -%}&quot;
      &quot;{{- '&lt;s&gt;' + message['content'] -}}&quot;
  &quot;{%- else -%}&quot;
      &quot;{%- if message['role'] == 'user' -%}&quot;
          &quot;{{-'[INST] ' + message['content'].rstrip() + ' [/INST]'-}}&quot;
      &quot;{%- else -%}&quot;
          &quot;{{-'' + message['content'] + '&lt;/s&gt;' -}}&quot;
      &quot;{%- endif -%}&quot;
  &quot;{%- endif -%}&quot;
&quot;{%- endfor -%}&quot;
&quot;{%- if add_generation_prompt -%}&quot;
    &quot;{{-''-}}&quot;
&quot;{%- endif -%}&quot;
</code></pre>
<p>Now to the problem. The model seems to learn what it needs to extract, it generates decent answers, with the same format as the assistant it was trained by, the problem is that after it generates the answer, it keeps on generating additional texts regarding the email that are irrelevant to the task, E.G. &quot;please contact us in....&quot;</p>
<p>When I fine tune GPT3.5 for example for the same task, the model is able to extract exactly what I need for it, which suggests to me that I'm doing something wrong.</p>
<p>Does anyone have suggestions as to where did I go wrong?</p>
","transformer"
"128895","Practical Experiments on Self-Attention Mechanisms: QQ^T vs. QK^T","2024-04-29 20:57:44","128973","2","42","<deep-learning><neural-network><transformer><attention-mechanism>","<p>I'm currently exploring the self-attention mechanism used in models like Transformers, and I have a question about the necessity of using a separate key matrix (K) instead of just using the query matrix (Q) twice, resulting in QQ^T instead of QK^T. Despite reading several materials and arguments, I'm not fully convinced why models cannot manage to use QQ^T for self-attention.</p>
<p>I'm asking to seek <strong>practical experiments or studies</strong> comparing the performance of QQ^T self-attention with the traditional QK^T approach. If such experiment exist, I'm particularly interested in any insights or results that could illustrate the impact of using QQ^T on model performance and learning dynamics.</p>
","transformer"
"128773","Use text embeddings to map job descriptions to ESCO occupations","2024-04-19 15:07:53","","2","41","<transformer><bert><embeddings><llm><semantic-similarity>","<p>I'm trying to build a model to map job descriptions to <a href=""https://esco.ec.europa.eu/en/classification/occupation_main"" rel=""nofollow noreferrer"">ESCO occupations</a> which is a taxonomy for job titles. Every ESCO occupations have a title, a description and some essential skills.
Ideally I would have built a classification model but since I don't have labelled data that's out of the question.</p>
<p>So my idea was to generate text embeddings from every ESCO occupation and then for an input job description, and using cosine similarity, find the most similar ESCO occupation to that job description. I'm using this <a href=""https://huggingface.co/jjzha/esco-xlm-roberta-large"" rel=""nofollow noreferrer"">model</a> to generate the embeddings, which is an XLM-roBERTa which was pre-trained on job market data. I use the mean of the embeddings for every token as the job description's final embedding.
However the results are very bad, it fails to find most relevant ESCO occupations.</p>
<p>Here's how I compute the embeddings:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;jjzha/esco-xlm-roberta-large&quot;)
model = AutoModel.from_pretrained(&quot;jjzha/esco-xlm-roberta-large&quot;)

sample_job_title = &quot;We are looking for a junior software developer with experience in React and Python.&quot;
encoded_input = tokenizer(sample_job_title, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
with torch.inference_mode():
    output = model(**encoded_input)
embedding = output.last_hidden_state.mean(dim=1)
</code></pre>
<p>From this output I retrieve output.last_hidden_state, which should correspond to the tokens embeddings, and then compute the mean embedding. This returns a pytorch tensor of shape (1, 1024). I have another tensor of shape (3007, 1024), <code>esco_embeddings</code> which corresponds to the embeddings for every 3007 ESCO occupation.
I then compute cosine similarity by doing:</p>
<pre class=""lang-py prettyprint-override""><code>similarities = torch.nn.functional.cosine_similarity(embedding, esco_embeddings, dim=1)
</code></pre>
<p>And find the k most similar ESCO occupations by computing</p>
<pre class=""lang-py prettyprint-override""><code>most_similar = torch.topk(similarities, k)
</code></pre>
<p>I thought that the problem might be in the embeddings itself, with XLM-roBERTa generating embeddings for every token and not one emebdding for the whole text.</p>
<p>Does anyone have an idea why that isn't working, and how it could be fixed? Maybe there's a better approach?</p>
<p>Thanks for the help.</p>
","transformer"
"128459","Why is positional encoding preferable over adding additional features for transformer models?","2024-03-26 22:00:29","","0","26","<transformer><encoding>","<p>Why is information about the position not added as an additional feature? I read in forums that the only reason would be length-based overfitting, but I couldn't find a reliable source for that. What if the transformer is used for applications where length-based overfitting is not an issue due to consistent sequence length?</p>
","transformer"
"128448","Insights about W0rd2Vec","2024-03-26 06:25:39","","0","22","<deep-learning><nlp><text-mining><word-embeddings><transformer>","<p>As per my knowledge, Word2Vec is belongs to non-contextual embedding technique. this have only semantic relationship between words.</p>
<p>We can implement Word2Vec, either in CBoW or skip-gram model. but i confused with below statements:</p>
<ol>
<li>The CBOW model is designed to predict a target word based on its surrounding context words.</li>
<li>Unlike the Skip-gram model, which predicts context words given a target word, CBOW focuses on predicting the target word itself.</li>
</ol>
<p>since word2vec is non-contextual. but in CBoW, it is considering the context to find the target.
can you please give more insights about these two(CBoW, skip-gram).</p>
","transformer"
"128378","Is the score function form of ALiBi, a positional encoding in Deep Learning, always lower triangular?","2024-03-20 12:37:27","","0","30","<deep-learning><transformer>","<p>I have a question about the score function of ALiBi (Attention with Linear Biases), which is a positional encoding method introduced by the following paper:</p>
<p><a href=""https://arxiv.org/pdf/2108.12409.pdf"" rel=""nofollow noreferrer"">TRAIN SHORT, TEST LONG: ATTENTION WITH LINEAR BIASES ENABLES INPUT LENGTH EXTRAPOLATION</a></p>
<p>According to Figure 3 in the paper, the score function form is a lower triangular matrix.</p>
<p>If in the masked multi-head attention in the Transformer model (e.g., in decoder layer), it is no problem.</p>
<p>On the contrary, I'd like to know about its use in a general format, in other words, in normal multi-head attention (e.g. encoder layer).</p>
<p>Is it also a lower triangular matrix? If not, what form does the linear bias term have?</p>
","transformer"
"128359","Do LSTM, GRU and Transformer models with less layers and units perform better than larger models when classifying short text sequences?","2024-03-19 09:35:47","","0","14","<classification><nlp><tensorflow><lstm><transformer>","<p>I am working with a Kaggle dataset with short Twitter messages as text input. I made a copy <a href=""https://www.kaggle.com/datasets/joachimrives/natural-language-processing-with-disaster-tweets"" rel=""nofollow noreferrer"">here</a>. When testing LSTMS, GRUs, bi-directional versions of the GRUs, and the Encoder layers of a Transformer model, the best models were shallow and had few units. The average text sequence length was 14-15 according to my code. Is it a good assumption that smaller models or models with fewer units usually do better when processing short text?</p>
<p>There are problems with my guess.</p>
<ol>
<li><p>The correlation between layer counts and unit counts vs. the length of meaningful words or tokens is not constant. For bi-directional LSTMs, the best models had one layer and 8 or 16 units. Other unit counts from 64 to 1 did not perform as well. Using the transformer encoder,1 layer with 2 heads and 4 inner units performed roughly as well as 6 heads and 8 inner dense units. For 1-D Convolutions, 32 and 64-filter convolutions did the best. I tested common multiples of 2 and some in-between values if the upper and lower limits performed well, e.g. 24 if 16 and 32 filters did well.</p>
</li>
<li><p>I did not sample a large range. I assumed that higher numbers of layers or units would make the model worse after reaching 128 filters for Conv-1D layers and 32 units for the LSTMs. This is because I did not have the computing power to test larger networks, not just that more units made performance worse. I also didn't give time to testing in-between values.</p>
</li>
<li><p>The performances of my models might depend on the data set, but they could also depend on the model architectures I set up, i.e. what layers came before or after. How do I tell if a trend might be generalizable to other text classification data sets vs. something specific to my model architecture?</p>
</li>
</ol>
<p>To summarize, I want to know how or what correlation there might be between model architectures or hyperparameters and the characteristics of the data set, like the sequence length and number of examples. If I test the same architectures on different data sets, would that help validate my guesses?</p>
<p>If you want, you can check the Notebook I <a href=""https://www.kaggle.com/code/joachimrives/layer-unit-count-binary-text-classification"" rel=""nofollow noreferrer"">linked</a>. Some of the models I tested have been turned into comments.</p>
","transformer"
"128328","Why Transformer applies Dropout after Positional Encoding?","2024-03-17 00:16:18","128330","1","272","<transformer><embeddings>","<p>Why Transformers applies Dropout after Positional Encoding?</p>
<p><a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention Is All You Need</a>
<a href=""https://i.sstatic.net/hL4Lz.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/hL4Lz.png"" alt=""enter image description here"" /></a></p>
<p>Not sure what is the benefit of removing 10% of tokens in a sequence by default. Read <a href=""https://discuss.pytorch.org/t/why-use-dropout-in-positional-encoding-layer/159923/5"" rel=""nofollow noreferrer"">Why use dropout in positional encoding layer</a> but not clear.</p>
<p><a href=""https://i.sstatic.net/EnjVxl.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EnjVxl.jpg"" alt=""enter image description here"" /></a></p>
<p>Also does the PyTorch implementation of PositionalEncoding with Dropout may drop <code>[CLS]</code> or <code>[SEP]</code>?</p>
<p>Pytorch <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">class PositionalEncoding(nn.Module)</a>:</p>
<pre><code>class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: Tensor) -&gt; Tensor:
        &quot;&quot;&quot;
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        &quot;&quot;&quot;
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)         # &lt;---- May drop [CLS] or [SEP]?
</code></pre>
","transformer"
"128265","What is the advantage of positional encoding over using additional features?","2024-03-12 21:20:22","","0","20","<neural-network><time-series><transformer><encoding>","<p>Popular models such as the transformer model use positional encoding on existing feature dimensions. Why is this preferred over adding more features to the feature dimension of the tensor which can hold the positional information?</p>
","transformer"
"128260","Character-wise accuracy for image-to-text models","2024-03-12 15:03:38","","0","9","<nlp><cnn><computer-vision><transformer><ocr>","<p>is it possible to enforce image-to-text models like ViT or a simple CNN+Transformer to achieve character-wise accuracy?</p>
<p>Here's the context of my project:
I am developing a model to extract some targeted phrases and numbers from individual pdf pages and I would like the numbers, especially, to be character wise accurate.</p>
<p>Correct me if I am wrong, but ViT or other CNN+Transformer models make context-based estimations which is not the same as character-wise estimation, am I right?</p>
<p>One idea that I have is to tokenize the ground truth string on a character level instead of word but I am not sure how feasible this is.</p>
","transformer"
"128257","What's the purpose of using MLM when pretraining?","2024-03-12 09:13:22","128258","0","39","<machine-learning><python><deep-learning><nlp><transformer>","<p>If BERT is a stack of transformer encoders, and the encoder already operates bidirectionally, understanding both left and right contexts and generating contextual embeddings, what is the purpose of pretraining BERT using MLM ? Does it aim to improve the contextual embeddings even better ? Could someone please provide an explanation on this ? Thanks.</p>
","transformer"
"128254","can decoder only large language model be fine tuned to perform well at semantic similarity search?","2024-03-12 07:08:36","","0","18","<deep-learning><nlp><transformer><llm>","<p>BERT based models are Encoder only which are well suited for text classification, and Semantic Text similarity search (If fine-tuned via sBERT). I want to know whether decoder only models like Llama2, GPT can be fine-tuned to do well on STS benchmark. If yes, does it perform better than fine-tuning encoder-only models?</p>
","transformer"
"128242","How do transformer-based architectures generate contextual embeddings?","2024-03-11 08:33:23","128243","0","39","<machine-learning><deep-learning><neural-network><nlp><transformer>","<p>How do transformer-based architectures, such as Roberta, etc., generate contextual embeddings? The issue is, I haven't found any articles that explain this process.</p>
","transformer"
"128234","Approach for Multi-class Classification of texts","2024-03-10 08:46:57","","0","26","<nlp><word-embeddings><transformer><bert><text-classification>","<p>I'm trying to do a project where I have paragraphs and I need to classify them into multiple labels. The dataset is around 40k rows with labels.
I understand there is no one right approach but should I consider typical ML classifiers like embeddings + Logistic regression, xgboost etc.
Or should I directly consider fine tuning transformers like BERT,DistilBERT etc.</p>
<p>My priority is on getting accurate predictions and I have a few weeks to complete this.</p>
","transformer"
"128203","what is the main difference between ROUGE and BLUE?","2024-03-07 11:58:40","","0","17","<nlp><transformer><model-evaluations><language-model><llm>","<p>Both (ROUGE, BLUE) are useful to find the similarity between machine generated summary and reference summary.</p>
<p>what is the main difference?</p>
","transformer"
"128201","Reducing language bias for text classification, transformer model","2024-03-07 09:46:36","","0","9","<machine-learning><classification><nlp><transformer><huggingface>","<p>I am working on a text classification model predicting classes for text. We have languages from many parts of the world and some of our classes are dominated by specific languages. The model we are using is:</p>
<pre><code>https://huggingface.co/distilbert/distilbert-base-multilingual-cased
</code></pre>
<p>Even though the model is multilingual it shows bias pushing certain languages towards specific classes. If I translate text from English to Thai I will receive different predictions. Given the dataset imbalance in classes/languages, this is understandable but I'd like to improve it.</p>
<p>I'm wondering if someone has a good solution for decreasing this bias? I'm thinking of simply translating training data between the languages to reduce it</p>
","transformer"
"128192","Could You Suggest Me Some Details of Realizing This LLM?","2024-03-06 21:46:05","","0","17","<nlp><transformer><llm>","<p>I mean this hypothetical LLM:
<a href=""https://twitter.com/RokoMijic/status/1663299142431432704"" rel=""nofollow noreferrer"">https://twitter.com/RokoMijic/status/1663299142431432704</a></p>
<p>I'm trying to figure out how the neural network (let's abstract from the data) can be realized. I understand that:</p>
<ol>
<li>It's a transformer;</li>
<li>It's sequence-to-sequence prediction (with a decoder, not with classification layers).</li>
</ol>
<p>I'd like to ask more experienced ML people for more details of the realization. &quot;Chronologically labelled data&quot; means here concatenation of the events with the dates (like in life2vec)? Am I missing something else that is critical?
Thanks a lot in advance!</p>
","transformer"
"128173","Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:3! in DPOTrainer with ec2 G5 12X Large","2024-03-06 05:49:18","","0","206","<python><deep-learning><pytorch><transformer><llm>","<pre><code>import os
import torch
from datasets import load_dataset, Dataset
from transformers import (
    BitsAndBytesConfig,
    AutoTokenizer,
    TrainingArguments,

)

from peft import AutoPeftModelForCausalLM
from trl import DPOTrainer
from peft import LoraConfig

hf_auth = &quot;&quot;
peft_model_path = 'test/'
dataset = load_dataset(
    &quot;test_classification&quot;,
)
print(&quot;Dataset loaded:&quot;, dataset)


def format_instruction(vignette: str):
    return f&quot;&quot;&quot;&lt;s&gt;[INST]{vignette.strip()} Generate given Vignette class and explain the reason for class.[/INST] &quot;&quot;&quot;.strip()


def generate_instruction_dataset(data_point):

    return {
        &quot;chosen&quot;: data_point[&quot;chosen&quot;],
        &quot;rejected&quot;: data_point[&quot;rejected&quot;],
        &quot;prompt&quot;: format_instruction(data_point[&quot;prompt&quot;])
    }


def process_dataset(data: Dataset):
    return (
        data.shuffle(seed=42)
        .map(generate_instruction_dataset)
    )


dataset = process_dataset(dataset)

print(&quot;Dataset processed:&quot;, dataset)

compute_dtype = getattr(torch, &quot;float16&quot;)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
)

print(&quot;Loading base model:&quot;)

model = AutoPeftModelForCausalLM.from_pretrained(
    peft_model_path,  # location of saved SFT model
    device_map=&quot;auto&quot;,
    quantization_config=bnb_config,
)

print(&quot;Loading reward model:&quot;)

model_ref = AutoPeftModelForCausalLM.from_pretrained(
    peft_model_path,  # same model as the main one
    device_map=&quot;auto&quot;,
    quantization_config=bnb_config,

)

print(&quot;Loading tokenizer:&quot;)

tokenizer = AutoTokenizer.from_pretrained(
    peft_model_path, use_auth_token=hf_auth, trust_remote_code=True, device_map=&quot;auto&quot;)


output_dir = &quot;dpo/output/&quot;
training_args = TrainingArguments(
    output_dir=output_dir,
    remove_unused_columns=True,
    per_device_train_batch_size=4,
)

print(&quot;Lora config added&quot;)

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias=&quot;none&quot;,
    task_type=&quot;CAUSAL_LM&quot;,
)

print(&quot;DPO trainer initialized:&quot;)

dpo_trainer = DPOTrainer(
    model,
    model_ref,
    args=training_args,
    beta=0.1,
    train_dataset=dataset['train'],
    # eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    peft_config=peft_config,
    max_length=1024,
    max_prompt_length=512,
)

torch.set_grad_enabled(True)

print(&quot;DPO trainer started:&quot;)

dpo_trainer.train()
print(&quot;Training done&quot;)
</code></pre>
<p>I am use G5 12X Large instance for this training it has following GPU's GPU 0: NVIDIA A10G GPU 1: NVIDIA A10G GPU 2: NVIDIA A10G GPU 3: NVIDIA A10G</p>
<p>But with start of dpo_trainer.train() following error will occur:</p>
<p>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:3!</p>
<hr />
<p>Moreover i used <code>ref_model=None</code> and <code>device_map={&quot;&quot;: PartialState().process_index}</code> in both model and tokenizer. Then it gives :</p>
<pre><code>output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 22.02 GiB of which 142.19 MiB is free. Including non-PyTorch memory, this process has 21.88 GiB memory in use. Of the allocated memory 19.27 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
</code></pre>
","transformer"
"128138","Effect of Sequential Data Quality Variation on Transformer Model Training: Seeking Insights and Experiences","2024-03-03 19:13:38","128139","2","41","<dataset><training><transformer><data-quality>","<p>I'm exploring the training efficiency of transformer models against the backdrop of data quality sequencing. Specifically, I ponder whether arranging unlabeled data by presumed quality affects training outcomes, akin to the structured progression observed in human learning paradigms. This inquiry stems from the hypothesis that a gradual escalation in data complexity or a focused emphasis on high-quality data during training (e.g., more epochs for data subsets) might optimize model performance.</p>
<p>I seek insights or empirical evidence from this community’s experiences or relevant literature that might illuminate the impact of data sequencing on transformer training. Any shared knowledge or references to studies exploring this facet would be invaluable.</p>
","transformer"
"128134","Training Models Directly with Transformer Attention Weights: A Viable Strategy?","2024-03-03 04:35:45","","1","39","<machine-learning><deep-learning><nlp><data-science-model><transformer>","<p>I'm currently using pre-trained transformers to extract embeddings for sequence analysis, which are then used in downstream tasks. My process involves using the extracted embeddings as features for training models tailored to specific applications. Recently, I came across a study <a href=""https://www.nature.com/articles/s41598-022-18205-9"" rel=""nofollow noreferrer"">1</a> that not only utilizes embeddings from an MSA transformer but also trains models directly on the extracted row attention weights independently.</p>
<p>This approach intrigues me since it's not commonly seen in the literature or practices I've encountered. Is it a practical approach to train downstream models directly on attention weights derived from transformers?</p>
","transformer"
"128132","Does Google DeepMind's Gemma 7B models specs have inconsistent dimensions?","2024-03-03 03:43:27","","1","44","<transformer><llm><google>","<p>In Google DeepMind's Gemma technical paper (<a href=""https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf"" rel=""nofollow noreferrer"">https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf</a>), the 7B Gemma model specs are given as d_model = 3072, num_heads = 16 and head_size = 256 for the 7B model. They don't seem consistent (16 * 256 != 3072).
Since the dimension is distributed across h heads, I think this should hold true -</p>
<pre><code>#heads * #head_size = d_model
</code></pre>
<p>This is also explained in the original Transformers paper, &quot;Attention Is All You Need&quot;.</p>
<p>This equation holds for the specs provided for Gemma 2B model in the same paper.
Am I missing something with the 7B Gemma specs? Or does this paper have an error?</p>
","transformer"
"128123","Where the term `cross-attention` is first used? (couldn't find the term in attention is all you need paper)","2024-03-02 10:52:17","128133","0","126","<transformer>","<p>I am looking for the paper that first used the term <code>cross-attention</code>.</p>
<p>I carefully read the paper <code>Attention is all you need (NeurIPS 2017)</code> but couldn't find the term <code>cross-attention</code>. I understand that cross-attention could be seen as a modification of self-attention in terms of query, key, and value, but I want to find the exact sentence that first used the term <code>cross-attention</code>.</p>
<p>Thank you very much.</p>
<p><a href=""https://vaclavkosar.com/ml/transformers-self-attention-mechanism-simplified"" rel=""nofollow noreferrer"">noted picture</a></p>
<p><a href=""https://i.sstatic.net/b0f4k.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/b0f4k.png"" alt=""enter image description here"" /></a></p>
","transformer"
"128113","How can I use contextual embeddings with BERT for sentiment analysis/classification","2024-03-01 22:34:09","128118","1","162","<nlp><word-embeddings><transformer><bert><embeddings>","<p>I have a BERT model which I want to use for sentiment analysis/classification. E.g. I have some tweets that need to get a POSITIVE,NEGATIVE or NEUTRAL label. I can't understand how contextual embeddings would help in a better model, practically.</p>
<p>I process the tweets and sentences to make them ready to be fed into the tokenizer. After I get every embedding as well as its mask, and feed it into a BERT model. From that BERT model I get some hidden states in return. As I understand it, now I have to also use a linear layer to take that 768 output from BERT and output a possibility for the 3 labels.</p>
<p><strong>How can contextual embeddings help me here?</strong> I get that we can use a combination of those hidden states/layers that we get for every sentence by the BERT model, and that helps us create better embeddings, which technically mean better models. But, after I follow some approach, e.g. summing the last four hidden states, or taking a mean of every token to create a token for each word, how do I proceed now? Do I need another model to take that embedding and output the labels that way (e.g. a linear layer but after the contextual embeddings are created)? Am I thinking of this the right way? Any input would be appreciated.</p>
","transformer"
"128105","Aside from trial and error, how do I select the number of layers and unit counts for LSTMS, GRUs, and Transformer units for text and time series?","2024-03-01 09:54:05","","1","36","<nlp><tensorflow><lstm><transformer><gru>","<p>When deciding on the number of units and layers for text processing or time-series prediction I rely heavily on trial and error. First, I look for a reference or paper on the topic such as the white paper on transformers: <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a>. Once I read why the standard is so-and-so, I code the standard. After that, I incrementally adjust the unit counts and number of layers one at a time. I am making wild guesses at that point. Maybe the number of meaningful or non-zero tokens could approximate the units required. If my sequence length is capped at 120 tokens, I'll see how long it takes to train a 128-unit LSTM, GRU, or Transformer model. I arbitrarily set the unit count to the lowest exponent value of 2 greater than the maximum number of tokens and then steadily reduce it. After that, I start adding layers. If the model has bad metrics, I keep adding layers. If the model takes more than a minute to train per layer, I reduce the number of units and layers. I tolerate long training times based on how much free time I think I have. Is there any way to search more systematically? My criteria are all arbitrary. I hope there is a way to calculate the layer and unit counts based on the input data or meta-data. I am trying this out with TensorFlow.</p>
","transformer"
"126951","Gradually increasing CPU load on using sentence embeddings model with kmeans","2024-02-20 16:06:25","","0","13","<machine-learning><neural-network><transformer>","<p>I am having a ML based production application, using flask, deployed on GCP server using gunicorn workers. In each incoming request, a text sentence is received.</p>
<p>It is using sentence transformers (All-MiniLM-L6-v2 model), which is loaded globally one time, to create embeddings of the incoming text and then use pre trained kmeans (also loaded globally) to predict/map it to a intent cluster. Basically, goal is to find intent of the sentence.</p>
<p>I have ample resources and the requests are also constant in number and texts are also similar, but still each day the CPU load is gradually increasing. Avg response time on 1st day was around 200 ms average, after 10 days now it is 400 ms.</p>
<p>I have tried deleting the embedding variable using 'del' command in the code itself, also forcing python garbage collector using 'gc.collect()' in a thread which executes after the main process execution is completed, but still the issue is coming.</p>
<p>One thing I have noticed is that if I dont use del and gc.collect(), the RAM starts to go down gradually. With both these, RAM is constant but now CPU usage is gradually going up day by day, hence the load and response time.</p>
<p>I have spent weeks on this issue trying to debug it but have got no solution, any help would be appreciated.</p>
","transformer"
"126942","Why do the Llama 2 weights have eight different files?","2024-02-19 23:09:51","","1","293","<machine-learning><deep-learning><neural-network><nlp><transformer>","<p>I downloaded the weights for <a href=""https://github.com/facebookresearch/llama/tree/main"" rel=""nofollow noreferrer"">Llama 2</a> (70B-chat). This process created a folder titled &quot;llama-2-70b-chat,&quot; which contained 8 files titled consolidated.00.pth, consolidated.01.pth, and so on until consolidated07.pth. Each file is about 16.84 GB. Here are the names and types of all the tensors in consolidated.00.pth:</p>
<ul>
<li>tok_embeddings.weight [32000, 1024]</li>
<li>norm.weight [8192]</li>
<li>output.weight [4000, 8192]</li>
<li>rope.freqs [64]</li>
<li>layers.0.attention.wq.weight [1024, 8192]</li>
<li>layers.0.attention.wk.weight [128, 8192]</li>
<li>layers.0.attention.wv.weight [128, 8192]</li>
<li>layers.0.attention.wo.weight [8192, 1024]</li>
<li>layers.0.feed_forward.w1.weight [3584, 8192]</li>
<li>layers.0.feed_forward.w2.weight [8192, 3584]</li>
<li>layers.0.feed_forward.w3.weight [3584, 8192]</li>
<li>layers.0.attention_norm.weight [8192]</li>
<li>layers.0.ffn_norm.weight [8192]</li>
<li>layers.1.attention.wq.weight [1024, 8192]</li>
<li>... and so on until</li>
<li>layers.79.ffn_norm.weight [8192]</li>
</ul>
<p><strong>But why are there 8 separate &quot;consolidated.0X.pt&quot; files?</strong> The other 7 files have tensors with the same names and the same shapes, but <em>different values</em>—even the token embeddings have different values!</p>
<p>In fact, if I multiply out the parameter dimensions above and sum them, I get approximately 8.6B parameters, which is far shy of 70B, but almost exactly an eighth of 70B.</p>
<p>I think the answer may relate to how the model uses <a href=""https://arxiv.org/pdf/2305.13245.pdf"" rel=""nofollow noreferrer"">grouped-query attention</a>; the <a href=""https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/"" rel=""nofollow noreferrer"">paper</a> mentions using 8 A100s with <a href=""https://huggingface.co/docs/text-generation-inference/en/conceptual/tensor_parallelism"" rel=""nofollow noreferrer"">tensor parallelism</a>. The params JSON file has this information:</p>
<pre><code>{&quot;dim&quot;: 8192, &quot;multiple_of&quot;: 4096, &quot;ffn_dim_multiplier&quot;: 1.3, &quot;n_heads&quot;: 64, &quot;n_kv_heads&quot;: 8, &quot;n_layers&quot;: 80, &quot;norm_eps&quot;: 1e-05, &quot;vocab_size&quot;: -1}
</code></pre>
<p>Everything else is available publicly in the <a href=""https://github.com/facebookresearch/llama/tree/main"" rel=""nofollow noreferrer"">Llama GitHub repo</a>.</p>
","transformer"
"126803","How can I use Time-GPT for pretraining my model","2024-02-10 08:15:49","","1","69","<deep-learning><time-series><transformer><transfer-learning><gpt>","<p>I am mentioning Time-GPT here as a placeholder example. It can be any pretrained model.</p>
<p>Suppose I have a dataset that requires some time series prediction. How can I leverage a well-trained model and transfer that learning to make a better prediction for my model?</p>
<p>Asking this because, in almost all cases the I/P and O/P dimensions of Time-GPT and my dataset would be different. Then how can I use it?</p>
<p>Could you share some resourceful git repo with similar examples?</p>
","transformer"
"126756","Using Transformers on a seq2seq task with sparse labels","2024-02-07 00:30:23","","0","23","<training><transformer><audio-recognition>","<p>I'm new to the ML world and would like to ask for architecture advice for a project I'm building. I want to detect a certain event throughout an audio. For example, if the audio is divided into 10 MFCC bins, then the labels look like: <code>[1, 0, 0, 0, 1, 0, 0, 0, 0, 1]</code>. The labels are very sparse (ratio of <code>1</code>s &gt; 0.97).</p>
<p>I currently have ~500 audio files with corresponding 0/1 labels. Each audio is 1-5 mins long; ground truth labels are provided at 30 fps, so I have to divide audio into 1800-9000 bins. Each bin contains 20 MFCC features.</p>
<p>I am currently using a very simple Transformer architecture:</p>
<ul>
<li>Embedding: I did not use an input embedding since the audio input cannot be tokenized.</li>
<li>Transformer: 2 heads, 2 encoder layers</li>
<li>Linear projection, then sigmoid to obtain a 0~1 label</li>
<li>Loss: I tried both BCELoss (for 0/1 labels) and MSELoss (for 0~1 smoothed labels)</li>
</ul>
<p>My questions are:</p>
<ol>
<li>The model converges to a local minimum very quickly which is to predict all <code>0</code>s for all frames. This is likely due to the sparsity of <code>1</code>s in the labels. How does one usually design the architecture to overcome this?</li>
</ol>
<p>(On a subset of the dataset (~25 audio) I was able to use focal loss to put emphasis on <code>1</code>s more and overfit on the data, but of course the validation accuracy is terrible, and this didn't work when there are more data.)</p>
<ol start=""2"">
<li><p>Is it appropriate to add decoders to this architecture? It seems like I can incorporate them if I reformulate the problem as &quot;predict whether the event will happen in the next frame based on audio this frame&quot;, but I am not sure if I should just stick with an encoder-only architecture.</p>
</li>
<li><p>My labels are rather human preferences other than &quot;ground truths&quot;, which I suspect could also be a reason why the model is not learning. Instead of &quot;a dog barks on this frame&quot;, my labels are more like &quot;this frame is a good moment to take a deep breath while singing this song&quot;. Would augmenting data such as generating more data using sliding window help the model generalize, or problems like this are fundamentally difficult for Transformer-based architecture due to a small dataset?</p>
</li>
</ol>
<p>Any other feedback would be really appreciated as well. Thanks!</p>
","transformer"
"126755","Strategies for Encoding Large Datasets in Symbolic Music Generation for BERT-type Model","2024-02-06 23:14:55","","0","7","<preprocessing><transformer><encoding><generative-models><tokenization>","<p>I am creating a BERT-type model for symbolic music generation. An observation of my database is a musical piece. Actually, is a <a href=""https://link.springer.com/chapter/10.1007/978-3-319-46282-0_13"" rel=""nofollow noreferrer"">&quot;viewpoint&quot;</a> of the piece: <code>(note spell, duration with respect to a quarter note)</code>: e.g. (&quot;A&quot;, 1.5), (&quot;B&quot;, 1)... So each piece is represented by a sequence of tuples. The process I am following is basically this: clean-up, pre-tokenization, tokenization, model training and generation. In the &quot;pre-tokenization&quot;, apart from other things, I am &quot;encoding&quot; each piece, meaning that I change each &quot;event&quot; (e.g. (&quot;A&quot;, 1.5)) by a simple character (e.g. &quot;a&quot;). I do this process manually, saving in a dictionary the mapping between events and characters, so that later I can revert it. After this encoding, I tokenize the database. I am using BPE tokenization, through <code>youtokentome</code> package.</p>
<p>I have an issue with the encoding done in the &quot;pre-tokenization&quot; stage: for big databases, I run out of symbols (i.e. I have already used all ASCII characters, but I would need more to account for all distinct elements in the database). What can I do? I could use two characters (e.g. &quot;1a&quot;, &quot;1b&quot;, ..., &quot;2a&quot;, &quot;2b&quot;, ...) but I fear that this may pose a problem because the tokenizer sees two characters (e.g. &quot;2a&quot;) and may tokenize as, for example, &lt;token for &quot;2&quot;&gt; and &lt;token for &quot;a&quot;&gt;, when in truth those two tokens do not make sense as they correspond to a single event. Can this happen? What alternatives do I have? and perhaps more importantly, Is the &quot;encoding&quot; I am doing really necessary? Or would the tokenizer processes well a string of the type &quot;('A', 1.5), ('B', 1), ...&quot;?</p>
","transformer"
"126728","Is vision transformer (ViT) always better than CNN?","2024-02-05 08:46:46","126890","1","325","<machine-learning><deep-learning><neural-network><computer-vision><transformer>","<p>The paper - <a href=""https://arxiv.org/abs/2010.11929"" rel=""nofollow noreferrer"">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</a> proposed vision transformer and outperformed CNN-based models in many cases.</p>
<p>When it comes to sequential data, we usually use transformer models instead of recurrent models such as RNN, LSTM. Is the same case with images using ViT instead of CNN?</p>
","transformer"
"126601","Why the standard deviation of the BERT weight initialization is 0.02 by default","2024-01-26 04:54:32","126616","0","79","<transformer><bert><variance><weight-initialization>","<p>The purpose of weight initialization in the neural network is to keep the variance of calculation output in the layers to 1.0, and it depends on the calculations involved in the layers.</p>
<p>Initializing weight <code>W</code> with <a href=""https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"" rel=""nofollow noreferrer"">Xavier initialization</a> for  Matrix Multiplication <code>X@W.T</code> at Self Attention in <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Transformer Architecture</a> will use the standard deviation <span class=""math-container"">$\frac{1}{\sqrt{D}}$</span> to sample values from <span class=""math-container"">$N(\mu=0,\sigma=\frac{1}{\sqrt{D}})$</span> so that the <a href=""https://stats.stackexchange.com/a/52699/105137"">product has variance 1.0</a>, providing the dimensions of X and W are both <code>D</code> and X follows the normal distribution.</p>
<p>The dimension <code>D</code> of Transformer based BERT is <code>768</code>, so <span class=""math-container"">$\sigma$</span> is expected to be <code>0.036</code>. But BertConfig says it is using <code>0.02</code>. Where is <code>0.02</code> coming from?</p>
<ul>
<li><a href=""https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertConfig"" rel=""nofollow noreferrer"">BertConfig</a></li>
</ul>
<blockquote>
<p>initializer_range (float, optional, <strong>defaults to 0.02</strong>) — The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>
</blockquote>
","transformer"
"126590","Getting rid of the warning ""The following columns ... have been ignored"" and ""ValueError: batch_size should be a positive integer value ...""","2024-01-25 16:50:12","126591","0","51","<transformer><python-3.x><huggingface>","<p>I train a fine-tuning model with the PyTorch Trainer class:</p>
<pre class=""lang-py prettyprint-override""><code>bln_truncation = False

dataset = load_dataset(&quot;text&quot;, data_files={&quot;train&quot;: file_path})

block_size = 512
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=bln_truncation)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir=&quot;./&quot; + model_name,
    overwrite_output_dir=True,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    save_steps=save_steps,
)
#     print(next(model.parameters()).device)

model = AutoModelForCausalLM.from_pretrained(model_name)
model = torch.nn.DataParallel(model)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets[&quot;train&quot;],
)
#     print(next(model.parameters()).device)

trainer.train()
</code></pre>
<p>I get the warning and error:</p>
<pre class=""lang-shell prettyprint-override""><code>PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `DataParallel.forward` and have been ignored: attention_mask, input_ids, text. If attention_mask, input_ids, text are not expected by `DataParallel.forward`,  you can safely ignore this message.

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In [18], line 36
     28 trainer = Trainer(
     29     model=model,
     30     args=training_args,
     31     data_collator=data_collator,
     32     train_dataset=tokenized_datasets[&quot;train&quot;],
     33 )
     35 # Start training
---&gt; 36 trainer.train()

File ~/.local/lib/python3.9/site-packages/transformers/trainer.py:1317, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1312     self.model_wrapped = self.model
   1314 inner_training_loop = find_executable_batch_size(
   1315     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1316 )
-&gt; 1317 return inner_training_loop(
   1318     args=args,
   1319     resume_from_checkpoint=resume_from_checkpoint,
   1320     trial=trial,
   1321     ignore_keys_for_eval=ignore_keys_for_eval,
   1322 )

File ~/.local/lib/python3.9/site-packages/transformers/trainer.py:1329, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1327 self._train_batch_size = batch_size
   1328 # Data loader and number of training steps
-&gt; 1329 train_dataloader = self.get_train_dataloader()
   1331 # Setting up training control variables:
   1332 # number of training epochs: num_train_epochs
   1333 # number of training steps per epoch: num_update_steps_per_epoch
   1334 # total number of training steps to execute: max_steps
   1335 total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size

File ~/.local/lib/python3.9/site-packages/transformers/trainer.py:769, in Trainer.get_train_dataloader(self)
    759     return DataLoader(
    760         train_dataset,
    761         batch_size=self.args.per_device_train_batch_size,
   (...)
    764         pin_memory=self.args.dataloader_pin_memory,
    765     )
    767 train_sampler = self._get_train_sampler()
--&gt; 769 return DataLoader(
    770     train_dataset,
    771     batch_size=self._train_batch_size,
    772     sampler=train_sampler,
    773     collate_fn=data_collator,
    774     drop_last=self.args.dataloader_drop_last,
    775     num_workers=self.args.dataloader_num_workers,
    776     pin_memory=self.args.dataloader_pin_memory,
    777     worker_init_fn=seed_worker,
    778 )

File /srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:357, in DataLoader.__init__(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)
    353             sampler = SequentialSampler(dataset)  # type: ignore[arg-type]
    355 if batch_size is not None and batch_sampler is None:
    356     # auto_collation without custom batch_sampler
--&gt; 357     batch_sampler = BatchSampler(sampler, batch_size, drop_last)
    359 self.batch_size = batch_size
    360 self.drop_last = drop_last

File /srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/utils/data/sampler.py:232, in BatchSampler.__init__(self, sampler, batch_size, drop_last)
    226 def __init__(self, sampler: Union[Sampler[int], Iterable[int]], batch_size: int, drop_last: bool) -&gt; None:
    227     # Since collections.abc.Iterable does not check for `__getitem__`, which
    228     # is one way for an object to be an iterable, we don't do an `isinstance`
    229     # check here.
    230     if not isinstance(batch_size, int) or isinstance(batch_size, bool) or \
    231             batch_size &lt;= 0:
--&gt; 232         raise ValueError(&quot;batch_size should be a positive integer value, &quot;
    233                          &quot;but got batch_size={}&quot;.format(batch_size))
    234     if not isinstance(drop_last, bool):
    235         raise ValueError(&quot;drop_last should be a boolean value, but got &quot;
    236                          &quot;drop_last={}&quot;.format(drop_last))

ValueError: batch_size should be a positive integer value, but got batch_size=11111111
</code></pre>
<p>I saw the warning at the beginning also in the remarks at <a href=""https://stackoverflow.com/a/70263850/11154841"">ValueError when pre-training BERT model using Trainer API</a>:</p>
<blockquote>
<p>The following columns in the training set don't have a corresponding argument in BertForMaskedLM.forward and have been ignored: Text, Sentiment.</p>
</blockquote>
<p>What can I do to get rid of the warning:</p>
<blockquote>
<p>The following columns in the training set don't have a corresponding argument in <code>DataParallel.forward</code> and have been ignored: attention_mask, input_ids, text. If attention_mask, input_ids, text are not expected by <code>DataParallel.forward</code>,  you can safely ignore this message.</p>
</blockquote>
<p>And the error:</p>
<blockquote>
<p>ValueError: batch_size should be a positive integer value, but got batch_size=11111111&quot;?</p>
</blockquote>
","transformer"
"126577","How to calibrate IMU for large scale deployments possibly using deep neural network","2024-01-24 22:22:18","","0","9","<deep-learning><computer-vision><transformer>","<p>We were testing our visual SLAM algorithm on robots. We were getting poor performance. Then we calculated wite noise and random walk parameters (<a href=""https://www.youtube.com/watch?v=BtzmsuJemgI&amp;ab_channel=PatrickGeneva"" rel=""nofollow noreferrer"">using kalibr</a>) for the IMU and used it in our algorithm and the performance included significantly.</p>
<p>Given that calculating random walk requires keeping IMU steady for longer duration (we kept it steady for 20 hours), I am guessing how we can feasibly calculate these values for different deployments. Our robots are meant to operate in broad spectrum of environment (from extreme coldest to extreme hottest etc). Also, we are planning deployment of hundreds of robots in distant future.</p>
<p><strong>Q1.</strong> Is it possible to calibrate IMU and obtain its white noise and random walk parameters using some software approach - kinda of &quot;online self calibration&quot; ?</p>
<p>I checked the literature, I came across <a href=""https://www.mdpi.com/1424-8220/23/5/2655"" rel=""nofollow noreferrer"">this paper</a>. It uses transformer based model to predict IMU noise (instead of predicting IMU intrinsics, white noise and random walk). Below are block diagram of the architecture and excerpt describing training strategy.</p>
<p><a href=""https://i.sstatic.net/AQTvM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/AQTvM.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/IbL98.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IbL98.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/mPLbT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mPLbT.png"" alt=""enter image description here"" /></a></p>
<p><strong>Q2.</strong> What this paper is exactly doing? Is it reducing error between artificially generated gaussian noise o and noise predicted by model ̂o, that is min L(o, ̂o)? Isnt it just equivalent to train NN to learn gaussian distribution? Isnt any untrained randomly initialied model already mimicking gaussian distribution?</p>
","transformer"
"126538","Annotated Transformer - Why x + DropOut(Sublayer(LayerNorm(x)))?","2024-01-23 07:53:38","126539","1","65","<transformer>","<p>Please clarify if the <a href=""https://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks"" rel=""nofollow noreferrer"">Annotated Transformer</a> Encoder LayerNorm implementation is correct.</p>
<p><a href=""https://i.sstatic.net/8rLZkm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8rLZkm.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Transformer paper</a> says the output of the sub layer is <code>LayerNorm(x + Dropout(SubLayer(x)))</code>.</p>
<p><a href=""https://i.sstatic.net/bl9MS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bl9MS.png"" alt=""enter image description here"" /></a></p>
<p><code>LayerNorm</code> should be applied <strong>after</strong> the <code>DropOut(SubLayer(x))</code> as per the paper:</p>
<p><a href=""https://i.sstatic.net/HrYPa.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HrYPa.png"" alt=""enter image description here"" /></a></p>
<p>However, the <a href=""https://nlp.seas.harvard.edu/annotated-transformer/#encoder-and-decoder-stacks"" rel=""nofollow noreferrer"">Annotated Transformer</a> implementation does <code>x + DropOut(SubLayer(LayerNorm(x)))</code> where <code>LayerNorm</code> is applied <strong>before</strong> <code>Sublayer</code>, which is the other way around.</p>
<pre><code>class SublayerConnection(nn.Module):
    &quot;&quot;&quot;
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    &quot;&quot;&quot;

    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        &quot;Apply residual connection to any sublayer with the same size.&quot;
        return x + self.dropout(sublayer(self.norm(x)))   # &lt;--- LayerNorm before SubLayer
</code></pre>
","transformer"
"126521","Getting a free and unknown answer to a question against a fine-tuned text generation model trained on many essays and their few questions and answers","2024-01-21 22:01:30","126751","0","53","<transformer><finetuning><text-generation><llm><question-answering>","<h4>Aim</h4>
<p>I want to fine-tune a text generation model with essays of changing size and then ask each of these input texts a few questions. I already have a wider range of question-answer pairs at hand for each essay, which should be enough to make a first prototype. Yet, my aim is not to get the answers that I fed during training. This is not to build a chat for a bank client who may need such a clear and learnt answer. This is more about a free speech opinion, judgement, understanding that I myself might have missed while I read the text.</p>
<h4>Example: do I need 50 models to get answers for each essay?</h4>
<p>Thus, if I have 50 essays and each of them has 5 questions with 5 answers during training, I cannot just train all of them with all question-answer pairs. If I ask one essay a question, I do not want to get the answer that it already knows. I want to get a new answer, as if the model had never seen the answer during training. I can train the model with all essays, but not with all question-answer pairs. I have to train the model with <strong>all question-answer pairs but the one that I want to ask questions to</strong> for which I can feed the model only with the questions, and <strong>not with the answers</strong>. Only then, I get free answers to those questions while it still generalizes from all the 49 other answers of the dataset. If the answers were known during training, it would just give me the answers that it knows from training.</p>
<p><strong>My aim is to get new answers from a model that does not know the answers but tries to find them from the <strong>given essay</strong> and the generalization of 49 other essays and their question-answer pairs.</strong></p>
<blockquote>
<p>Essay 1:</p>
<ul>
<li>Question 1 of 5: What is the plot of the main person.</li>
<li>Answer: Santa Claus is stressed and tries to skip Christmas. By chance, he brings the world the most comfortable Christmas ever.</li>
</ul>
<p>Essay 2:</p>
<ul>
<li>Question 1 of 5: What is the plot of the main person.</li>
<li>Answer: Frank Franklin is a wildlife activist who gets almost shot by a jungle company but survives and fights back.</li>
</ul>
<p>Essay 3:</p>
<ul>
<li>Question 1 of 5: What is the historical background of the story.</li>
<li>Answer: The story plays in the 19th century during the upcoming industrialization when there was a boom that made some people rich in
a short time through the first stocks markets and speculation.</li>
</ul>
<p>Essay 4:<br />
...</p>
<p>Essay 50:<br />
...</p>
</blockquote>
<p>Now When I train with essay 1 to 50, and I take essay 2 and want to get answers in free speech, I should train essay 1 and essay 3 - 50 <strong>with question-answer pairs</strong> while I would take essay 2 <strong>only with the 5 questions (without the 5 answers, so that they will be open answers if I ask!)</strong>:</p>
<blockquote>
<p>Essay 1:</p>
<ul>
<li>Question 1 of 5: What is the plot of the main person.</li>
<li>Answer: Santa Claus is stressed and tries to skip Christmas. By chance, he brings the world the most comfortable Christmas ever.</li>
</ul>
<p>Essay 2:</p>
<ul>
<li>Question 1 of 5: What is the plot of the main person.</li>
<li><strong>NO ANSWER HERE so that the model will answer it during fine-tuning</strong></li>
</ul>
<p>Essay 3:</p>
<ul>
<li>Question 1 of 5: What is the historical background of the story.</li>
<li>Answer: The story plays in the 19th century during the upcoming industrialization when there was a boom that made some people rich in
a short time through the first stocks markets and speculation.</li>
</ul>
<p>Essay 4:<br />
...</p>
<p>Essay 50:<br />
...</p>
</blockquote>
<p>But I wonder whether there is a way to train just one model that can answer everything in free speech and as if it had not seen its own answers but only the answers of all other essays' questions.</p>
<p>If I trained the whole model again with the rest of the essays with all question-answer pairs, and if I did not give the answers to the one essay that I want to ask questions to, then I would have to train the fine-tuning model each time I change the essay, which is quite a waste of energy and machine time.</p>
<h4>Tweaking the text generation model</h4>
<ul>
<li><p>I tried a text generation model (german-gpt2) and fed it with just one chosen essay and its 5 questions. This very small fine-tuning model had bad answers (trained without the answers). One essay is clearly not enough to have a generalizing text generation model.</p>
</li>
<li><p>I made the same text generation model, but then without the questions, and when I then asked it to write text after the prompt, the new text was not good enough, mostly too abstract or too far away from the essay, and a bit weird.</p>
</li>
<li><p>Should I add <code>eos_token</code> as argument of the <code>tokenizer.encode_plus()</code> and also &quot;end of sentence&quot; [EOS] tokens in the input text as well? Would that make the model any better? Does the model give better answers when there are padding [PAD] tokens as <code>pad_token</code> argument of the <code>tokenizer.encode_plus()</code>? Which other tweaks and tricks should give better answers?</p>
</li>
</ul>
<p>What could help the most to get a better text generation? Up to now, the text output of the text generation model is not good.</p>
<h4>Fine-tuning code with just one file as the text input</h4>
<p>I train the text generation model with the code that you find at <a href=""https://datascience.stackexchange.com/a/126389/97556"">How can you get a Huggingface fine-tuning model with the Trainer class from your own text where you can set the arguments for truncation and padding?</a>:</p>
<pre><code>from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments

from transformers import AutoTokenizer
from datasets import load_dataset

model_name = &quot;dbmdz/german-gpt2&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
file_path = './myfile.txt'
bln_truncation = False
num_train_epochs = 1
per_device_train_batch_size = 1
save_steps = 10_000

dataset = load_dataset(&quot;text&quot;, data_files={&quot;train&quot;: file_path})

block_size = 512
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(
        examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=bln_truncation)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
</code></pre>
<p>And here begins the fine-tuning with the transformer PyTorch Trainer class that seems to be first choice on Huggingface, see <a href=""https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer"" rel=""nofollow noreferrer"">Train with PyTorch Trainer</a>.</p>
<pre class=""lang-py prettyprint-override""><code>model_folder = f&quot;./{model_name}&quot;

training_args = TrainingArguments(
    output_dir=model_folder,
    overwrite_output_dir=True,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    save_steps=save_steps,
)

model = AutoModelForCausalLM.from_pretrained(model_name)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets[&quot;train&quot;],
)

trainer.train()
model.module.save_pretrained(model_folder)
tokenizer.save_pretrained(model_folder)
</code></pre>
<h4>Question</h4>
<p>How do I get a good answer, at best in free speech, to a question about an essay, with the generalized knowledge of all other essays and their question-answer pairs, but without the knowledge of the answers of that chosen essay?</p>
<p>How should I set up the training or model if I want to get answers for each essay, but always with the training knowledge of the whole input of all essays and all question-answer pairs but without the answers of the chosen essay that I want to ask questions about and only within the boundaries of that chosen essay? Do I have to train 50 models if I have 50 essays?</p>
<h4>Other models</h4>
<ul>
<li><p>There might be better models to reach this aim, I read of Retrieval-Augmented Generation (RAG) models at <a href=""https://datascience.stackexchange.com/q/123904/97556"">How does fine-tuning work in question answering for custom documents</a>. But that question is already asked, so I do not want to make a duplicate here.</p>
</li>
<li><p>I also tried a Question Answering model, but it answers with cut text from the essay, thus, not in free speech, at least if I train it with just one essay. It might generalize better with more input. But such a question is already asked at <a href=""https://datascience.stackexchange.com/q/121866/97556"">Fine-tuning a pre-trained LLM for question-answering</a>, and I do not want to make a duplicate here.</p>
</li>
</ul>
","transformer"
"126508","Outdated Transformers TextDataset class drops last block when text overlaps. Replace by datasets Dataset class as input of Trainer train_dataset?","2024-01-21 00:43:54","126509","0","196","<dataset><transformer><huggingface><finetuning><llm>","<h3>Why I try to replace the <code>transformers</code> TextDataset class with <code>datasets</code> Dataset class</h3>
<p>I stumbled upon this when I tried to make the <code>train_dataset</code> of the Transformers Trainer class from a text file, see <a href=""https://datascience.stackexchange.com/q/126382/97556"">How can you get a Huggingface fine-tuning model with the Trainer class from your own text where you can set the arguments for truncation and padding?</a>.</p>
<p>The TextDataset of the transformers package is</p>
<ul>
<li>buggy (next heading) and</li>
<li>outdated (overnext heading).</li>
</ul>
<h4>Transformers TextDataset drops the last block of the split text</h4>
<p>The TextDataset class drops the last block of the text that was split into blocks by means of the <code>block_size</code> parameter, in the following example, <code>512</code> tokens (~ words and other things) per block:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, TextDataset

model_name = &quot;dbmdz/german-gpt2&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
file_path = './myfile.txt'

train_dataset = TextDataset(
    tokenizer=tokenizer,
    file_path=file_path,
    block_size=512,
    overwrite_cache=True,
)
</code></pre>
<p>If I check the last block, I see that it cuts the very last block that has the tail of the text. This code shows only the second last block, the last block gets dropped by the TextDataset class:</p>
<p><code>tokenizer.decode(train_dataset['input_ids'][-1])</code></p>
<p>Instead, the Trainer class does not drop the last batch by default, but you see from this that there is such a parameter also for the Auto dataloader arguments of the Trainer class, see <a href=""https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.dataloader_drop_last"" rel=""nofollow noreferrer"">class transformers Training Arguments</a>:</p>
<blockquote>
<p>dataloader_drop_last (bool, optional, defaults to False) — Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not.</p>
</blockquote>
<h4>Transformers TextDataset is outdated</h4>
<p>When I change the setting of a tokenizer and build the TextDataset object another time, sometimes a warning shows that you should take the Transformers datasets Dataset class instead.</p>
<p>Here is the warning (there are two warnings in it):</p>
<p>Warning 1:</p>
<pre class=""lang-shell prettyprint-override""><code>&gt; /srv/home/my_user/.local/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:54:
&gt; FutureWarning: This dataset will be removed from the library soon,
&gt; preprocessing should be handled with the 🤗 Datasets library. You can
&gt; have a look at this example script for pointers:
&gt; https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py
</code></pre>
<p>Warning 2:</p>
<pre class=""lang-py prettyprint-override""><code>&gt; warnings.warn( Token indices sequence length is longer than the
&gt; specified maximum sequence length for this model (31482 &gt; 512).
&gt; Running this sequence through the model will result in indexing errors
</code></pre>
<p>Warning 2 is just from changing from one tokenizer to another, it comes from <a href=""https://github.com/huggingface/transformers/blob/3f69f415adcbdaedec154ba8eac220ef3276975d/examples/pytorch/language-modeling/run_mlm.py#L466C1-L470C14"" rel=""nofollow noreferrer"">this line in the given link of the warning</a>.</p>
<pre class=""lang-py prettyprint-override""><code>        if data_args.max_seq_length &gt; tokenizer.model_max_length:
            logger.warning(
                f&quot;The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the &quot;
                f&quot;model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.&quot;
            )
</code></pre>
<p>It is enough to run the code again to get rid of warning 2. This question is only about warning 1 (&quot;FutureWarning: This dataset will be removed...&quot;).</p>
<h3>Question</h3>
<p>How do I replace the <code>transformers</code> Textdataset class with the <code>datasets</code> Dataset class so that the output is a dataset that can be the argument of the <code>train_dataset</code> parameter of the <code>transformers</code> Trainer class?</p>
","transformer"
"126493","Transformers Trainer: ""RuntimeError: module must have its parameters ... on device cuda:6 (device_ids[0]) but found one of them on device: cuda:0""","2024-01-19 15:25:08","126494","0","677","<pytorch><transformer><gpu><nvidia><cuda>","<p>I ask this since I could not fix it with the help of:</p>
<ul>
<li>Stack Overflow <a href=""https://stackoverflow.com/q/59249563/11154841"">RuntimeError: module must have its parameters and buffers on device cuda:1 (device_ids[0]) but found one of them on device: cuda:2</a> or</li>
<li>Stack Overflow <a href=""https://stackoverflow.com/q/71278607/11154841"">Pytorch : Expected all tensors on same device</a>.</li>
</ul>
<h3>Juypter Notebook server</h3>
<p>I am on a Jupyter Notebook server, therefore, bash code starts with &quot;!&quot;.</p>
<p>You have to <em>begin</em> with the following line in the <em>same</em> Jupyter Notebook cell in which you build the model. Mind that it does not seem to work if you change the environment variable with
<code>import os</code> and then <code>os.environ['CUDA_VISIBLE_DEVICES'] = '6,3,7,2'</code>, and others ran into the same, see <a href=""https://github.com/pytorch/pytorch/issues/9158#issuecomment-665513010"" rel=""nofollow noreferrer"">setting CUDA_VISIBLE_DEVICES just has no effect #9158</a>.</p>
<pre class=""lang-py prettyprint-override""><code>!export CUDA_VISIBLE_DEVICES='6,3,7,2'
</code></pre>
<p>After changing the model to a DataParallel model, memory should then be spread among GPUs 4,5,6,7. If you ask why the code lists 6,3,7,2 even though it will then work on 4,5,6,7, see <a href=""https://datascience.stackexchange.com/q/126490/97556"">&quot;model.to('cuda:6')&quot; becomes (nvidia-smi) GPU 4, same with any other &quot;cuda:MY_GPU&quot;, only &quot;cuda:0&quot; becomes GPU 0. How do I get rid of this mapping?</a>.</p>
<p>*The outcome is also without GPU 7 (just 4,5,6), perhaps since it was not needed, and it is not the question since my main aim is to avoid the GPUs 1,2,3 since these are needed for another project.
I would also like to spare GPU 0 so that I have only 4,5,6,7, but that is not an urgent need. In short: if you do not face the same mapping problem (6,3,7,2 -&gt; 4,5,6,7), go on with your working setup, but if you have it, check the other link. The right mapping is not the question here.</p>
<h3>Code</h3>
<h4>Main model (run on some chosen GPUs)</h4>
<p>Here is how I build the model.</p>
<pre><code>!export CUDA_VISIBLE_DEVICES='6,3,7,2'

from transformers import (AutoTokenizer, AutoModelForCausalLM, AutoConfig, TextDataset, 
    DataCollatorForLanguageModeling, Trainer, TrainingArguments)

def get_model(model_name):

    #### from transformers import GPT2LMHeadModel, GPT2Tokenizer
    # Load pre-trained model and tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return tokenizer, model

import torch
device = torch.device('cuda:6')
print(torch.cuda.device_count())
torch.cuda.set_device(device)

model_name = &quot;dbmdz/german-gpt2&quot;
tokenizer, model = get_model(model_name)
config = model.config
print(next(model.parameters()).device)
device_ids = [6,3,7,2]
model = torch.nn.DataParallel(model, device_ids=device_ids)
print(model.device_ids)
print(next(model.parameters()).device)
</code></pre>
<p>Out:</p>
<pre class=""lang-py prettyprint-override""><code>8
cpu
[6, 3, 7, 2]
cuda:0
cpu
</code></pre>
<h4>Fine-tuning model with the Transformers Trainer class</h4>
<pre class=""lang-py prettyprint-override""><code>def make_finetuned_model(tokenizer, model, file_path='myfile.txt', model_name=&quot;fine-tuned-model&quot;, bln_truncation=True,
                        num_train_epochs=1, per_device_train_batch_size=1, save_steps=10_000):
        
    train_dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=512,
        overwrite_cache=True,
    )
    
    print(next(model.parameters()).device)
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    
    print(next(model.parameters()).device)

    model_folder = f&quot;./{model_name}&quot;
    
    # Define the Trainer
    trainer = Trainer(
        model=model.to('cuda:6'), # &quot;model=model&quot; should run through as well
        args=TrainingArguments(
            output_dir=model_folder,
            overwrite_output_dir=True,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            save_steps=save_steps,
        ),
        data_collator=data_collator,
        train_dataset=train_dataset,
    )
    model.to('cuda:6')
    print(next(model.parameters()).device)

    # Fine-tune the model
    trainer.train()
    
    # Save the model and tokenizer to the fine-tuned model directory
    # This is needed since the model config and tokenizer need to be loaded at any load
    # Since the fine-tuned model wrapped with DataParallel, save the underlying model with:
    model.module.save_pretrained(model_folder)
    tokenizer.save_pretrained(model_folder)

make_finetuned_model(tokenizer, model, file_path='myfile.txt', 
                     model_name=&quot;fine_tuned_model&quot;, bln_truncation=True,
                        num_train_epochs=1, per_device_train_batch_size=1, save_steps=10_000)
</code></pre>
<p>Out:</p>
<pre class=""lang-py prettyprint-override""><code>cuda:6
cuda:6
cuda:0
cuda:0
</code></pre>
<p>Thus, building the trainer object resets the device to &quot;cuda:0&quot; no matter what you wrote, see the third printout <code>cuda:0</code> after it has been <code>cuda:6</code> before. I checked the Huggingface thread <a href=""https://discuss.huggingface.co/t/setting-specific-device-for-trainer/784/22"" rel=""nofollow noreferrer"">Setting specific device for Trainer</a> which is open and busy since August 2022 (!).</p>
<p>Since I chose four other GPUs and GPU 0 comes on top, the error is thrown.</p>
<h3>Question</h3>
<p>The Transformers Trainer class will always set the device to &quot;cuda:0&quot;. How do I get rid of the errors:</p>
<blockquote>
<p>RuntimeError: module must have its parameters and buffers on device cuda:6 (device_ids[0]) but found one of them on device: cuda:0</p>
</blockquote>
<p>and during the same code work, but more seldomly, and I do not know how to get this error back, code is lost:</p>
<blockquote>
<p>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:4! (when checking argument for argument index in method wrapper_CUDA__index_select)</p>
</blockquote>
","transformer"
"126478","How to perform inference on a finetuned falcon 7b model fine tuned on open assistant dataset","2024-01-18 16:08:27","","0","17","<nlp><transformer><transfer-learning><huggingface><llm>","<p>I finetuned a falcon 7b model on the open assistant dataset using the official colab notebook provided by huggingface at <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing</a>
How do i perform inference on it?
A sample row from the dataset it has been finetuned on is</p>
<p>''### Human: Can you write a short introduction about the relevance of the term &quot;monopsony&quot; in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: &quot;Monopsony&quot; refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions. Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens &amp; Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions. Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue. References: Bivens, J., &amp; Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog''</p>
<p>What kind of prompt do i give this model for inference?
Should i use tags like Human and Assistant</p>
","transformer"
"126435","How are GPT2 token embedding vectors processed internally?","2024-01-15 16:44:52","","0","21","<transformer><embeddings><gpt>","<p>I am experimenting with the GPT2-XL model and trying to understand the internal structure. While I understand most of the components and how they affect the size of the activation tensors (such as multi-headed self-attention), I do not fully understand how the embeddings are processed.</p>
<p>When extracting the embeddings at a specific point of a forward pass, i.e. between two transformer layers, I get a vector of size token length x context length (so let's say 4 vectors of length 1600 for the first forward pass with input size of 4 tokens).</p>
<p>I understand that each token is represented by an embedding vector. But I do not understand, how these are then processed. Are they calculated one after one for each transformer layer before the new 4x1600 tensor is passed to the next step? Or is each embedding vector processed in an own forward pass? If so, how does the last forward pass play a role in the next one? Are they computed in parallel? If so, do they share weights? This is quite confusing to me at the moment.</p>
<p>Thanks!</p>
","transformer"
"126431","Falcon-7B llm giving random output","2024-01-15 14:21:16","126434","1","45","<nlp><transformer><huggingface><gpt><llm>","<p>I am using a falcon 7B model for a chatbot without any finetuning with the following code</p>
<pre><code>model_name = &quot;ybelkada/falcon-7b-sharded-bf16&quot;

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True
)
model.config.use_cache = False
from transformers import pipeline

generator = pipeline(
    &quot;text-generation&quot;,
    model=model,
    tokenizer=&quot;gpt2&quot;,
)

result = generator(&quot;Hi&quot;)
print(result)
</code></pre>
<p>the result isnt as expected and it outputs
[{'generated_text': 'Hi8\x10=:AHi8\x10&gt;Hi8\x10&gt;:AHi8\x10?'}].
How can i fix this and make it output a proper response</p>
","transformer"
"126410","Application of transformers to tabular data","2024-01-13 23:29:30","","0","32","<transformer><transfer-learning>","<p>Is anyone using transformer based models for tabular data in real data science jobs as of 2024</p>
<p>I mean models like</p>
<ul>
<li>Tabnet</li>
<li>Tabtransformer</li>
<li>ARM-net</li>
<li>SAINT</li>
<li>FT-Transformer</li>
<li>Non-Parametric Transformers</li>
</ul>
<p>I got these models from a blog post <a href=""https://levelup.gitconnected.com/tabula-rasa-could-we-have-a-transformer-for-tabular-data-9e4b238cde2c"" rel=""nofollow noreferrer"">https://levelup.gitconnected.com/tabula-rasa-could-we-have-a-transformer-for-tabular-data-9e4b238cde2c</a> and it also lists the paper &quot;Why do tree-based models still outperform deep learning on tabular data?&quot; Which claims tree-base models would still be superior.</p>
<p>I wonder if this still holds true in 2024. Maybe also somebody found a suitable transfer learning task where he profited from the deep learning methods a lot.</p>
","transformer"
"126324","Why does cross-attention in an NMT decoder use the encoder embeddings as values?","2024-01-08 15:02:29","","2","75","<nlp><transformer><embeddings><attention-mechanism><machine-translation>","<p>In the <a href=""https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">Vaswani 2017</a> paper introducing encoder-decoder transformers, the cross-attention step in the decoder is visualised as follows:</p>
<p><a href=""https://i.sstatic.net/8I3vo.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8I3vo.png"" alt=""cross-attention"" /></a></p>
<p>Because keys and values are always taken to be equal, this figure implies that the <em>final encoder embeddings</em> are used as keys and values, and the <em>intermediate decoder embeddings</em> are used as queries. Indeed, they write:</p>
<blockquote>
<p>In &quot;encoder-decoder attention&quot; layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder.</p>
</blockquote>
<p>What this means is that we have <span class=""math-container"">$m$</span> decoder embeddings coming in from the previous decoder block, yet counterintuitively, we don't have <span class=""math-container"">$m$</span> linear combinations of those <strong>decoder</strong> embeddings coming out of the current decoder block, but <span class=""math-container"">$m$</span> linear combinations of the <strong>encoder</strong> embeddings.</p>
<p>Although this is apparently not unheard of (<a href=""https://arxiv.org/pdf/1409.0473.pdf"" rel=""nofollow noreferrer"">Bahdanau 2015</a> sort of have the same thing by having the <em>context vector</em>, which is half of the recurrent input of the decoder, be a linear combination of the encoder embeddings), it has two very strange consequences:</p>
<ul>
<li><p>Assuming a distinct tokeniser for the target language (which isn't uncommon), the decoder has a separate embedding matrix which it indexes as its very first processing step. Its static embeddings pertain to tokens in the target language. These embeddings flow into the first decoder block. Yet, after that one decoder block, we have gone from these static <strong>target</strong>-language embeddings to a linear combination of contextualised <strong>source</strong>-language embeddings. The values in the decoder's embedding matrix are essentially thrown away after one decoder block, having contributed to nothing more than one set of dot products.</p>
</li>
<li><p>The decoder essentially <strong>regurgitates</strong> the same vectors in each block.</p>
<ul>
<li>In the encoder, a block starts out with <span class=""math-container"">$n$</span> embeddings. You then change these embeddings (with self-attention and an MLP). In the next block, you start with these changed embeddings, and change them again.</li>
<li>In the <em>decoder</em>, a block starts out with <span class=""math-container"">$m$</span> embeddings. Then you change them (with self-attention). Then you turn them into <span class=""math-container"">$m$</span> linear combinations of <span class=""math-container"">$n$</span> <em>encoder</em> embeddings. Then you change these (with an MLP). In the next block, you change these (with self-attention)... and then you go <em>back</em> to a linear combination of the <span class=""math-container"">$n$</span> <strong>old</strong> embeddings that you already transformed in the <em>previous block</em>. You keep circling back to replacing your work by (different linear combinations of) the <em>same</em> embeddings, rather than transforming them recursively.</li>
</ul>
</li>
</ul>
<p>User <em>noe</em> boiled this down to the &quot;black-boxiness&quot; of neural models in <a href=""https://datascience.stackexchange.com/a/122412/141432"">this thread</a>: if it doesn't make sense, just assume you're wrong and that the machine is right.</p>
<blockquote>
<p>[In a French-to-English model, the] keys, values and queries are not in an &quot;English representation space&quot; nor in a &quot;French representation space&quot;. Keys, vectors and queries are vectors in representation spaces that have been learned by the network during training. These representation spaces are not necessarily interpretable by a human, they were learned just to lower the loss at the task the model was trained in (i.e. to translate).</p>
</blockquote>
<p>This might handwave away the first concern (embeddings have &quot;no language&quot; even though we have different static embeddings for different languages), but the second concern requires an architectural motivation. Why do we regurgitate the encoder embeddings? <strong>Why don't we use the encoder vectors as queries to reweight a sequence of <span class=""math-container"">$m$</span> constantly evolving decoder embeddings used as keys and values?</strong> There must be a good motivation for doing it this way, right?</p>
<p>I have heard that some systems use deep encoders and shallow decoders. It isn't obvious to me that this isn't a side-effect of regurgitating the encoder embeddings. If you're not letting the decoder come up with vastly new embeddings in each block, there's no point in having a deep decoder.</p>
","transformer"
"126297","Why does scaled dot-product attention use softmax?","2024-01-06 16:53:24","","1","93","<transformer><activation-function><attention-mechanism><softmax><intuition>","<p>I am trying to understand the reasoning behind the Transformer architecture.</p>
<p>In <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">&quot;Attention is all you need&quot;</a>, the weights for the scaled dot-product attention is defined as the scaled dot-product of the keys and values, passed through a SoftMax:</p>
<p><a href=""https://i.sstatic.net/X4hnf.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/X4hnf.png"" alt=""Scaled dot-product attention diagram from the paper &quot;Attention is all you need&quot;"" /></a></p>
<p>Conceptually, I understand that these weights are used to &quot;select&quot; other tokens to &quot;take context&quot; from their values based on their relevance. Therefore, it makes sense that they should be between 0 and 1.</p>
<p>However, I don't understand why we don't simply use an activation function like the Sigmoid here.</p>
<p>I know that the outputs of Softmax add up to 1. And it seems like the authors try to preserve this property - even when the paper describes masking, they don't simply set the post-softmax weights to 0 (which seems like a more obvious solution, but would make the sum &lt;= 1). Instead, they subtract infinity from the Softmax inputs, which makes sure that the weights still sum up to 1.</p>
<p>Why is it so important that these weights add up to 1? In my understanding, this is usually used for values that represent probabilities. However, these weights don't represent probabilities, right? They are simply scores of how &quot;relevant&quot; the other tokens are to the context of the current tokens. Therefore, it would make sense to me if a similar model learned attention weights that sum up to more than 1.</p>
<p>Is my intuition for the &quot;meaning&quot; of these values correct? Is there a reason why it is beneficial to have the attention weights sum up to 1? Has there been any research on alternative options? Or is the use of Softmax here arbitrary? Would a simple activation function work here instead of Softmax?</p>
","transformer"
"126250","How to label a dataset of text pairs to use it as a universal one for calculating the precision@k metric for different models?","2024-01-02 19:33:12","","0","7","<neural-network><nlp><transformer><metric><search>","<p>I am facing a semantic search problem. I am fine tuning different NLU models and i want to use precision@k as my main metric. Is it possible to label a dataset of text pairs to use it as a universal one for calculating the precision@k metric for different NLU models? Or the only way is to label dataset after semantic search all over again for each model?</p>
","transformer"
"126248","Could someone help with fine-tuning dolphin-2.2.1?","2024-01-02 13:47:12","","1","75","<deep-learning><transformer><huggingface><finetuning>","<p><strong>Could someone help with fine-tuning dolphin-2.2.1?</strong></p>
<p><em>I have a problem with training: my train\loss - 0 and validation\loss - 0.000... after 800-1000 steps and this is overfitting</em></p>
<pre><code>Params:
dataset 250k, format &quot;text&quot; ### Human: ### Assistant: 
Prompt: ChatML
Trainer: optim - AdamW(model.parameters(), lr=6e-7, betas=(0.9, 0.95), eps=1e-05,), 
lr_scheduler_type=&quot;cosine&quot;,
warmup_steps=100,
per_device_train_batch_size=5,
per_device_eval_batch_size=5,
gradient_checkpointing=True,
gradient_accumulation_steps=4,
seed=42,
max_steps=10000,
learning_rate=6e-7,
logging_steps=100,
bf16=True,
logging_dir=&quot;./logs&quot;,
save_strategy=&quot;steps&quot;,
save_steps=100,
evaluation_strategy=&quot;steps&quot;,
eval_steps=100,
do_eval=True

Model with LORA: 
config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=[
        &quot;q_proj&quot;,
        &quot;k_proj&quot;,
        &quot;v_proj&quot;,
        &quot;o_proj&quot;,
        &quot;gate_proj&quot;,
        &quot;up_proj&quot;,
        &quot;down_proj&quot;,
        &quot;lm_head&quot;,
    ],
    bias=&quot;none&quot;,
    lora_dropout=0.05,
    task_type=&quot;CAUSAL_LM&quot;,
) 
</code></pre>
<p><strong>Not much experience I can't figure out what causes the model weights to be so memorized</strong></p>
<pre><code>More info:
fsdp_plugin = FullyShardedDataParallelPlugin(
    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),
    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),
)

accelerator = Accelerator(fsdp_plugin=fsdp_plugin)

base_model_id = &quot;cognitivecomputations/dolphin-2.2.1-mistral-7b&quot;
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = MistralForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)

tokenizer = LlamaTokenizer.from_pretrained(
    base_model_id,
    padding_side=&quot;left&quot;,
    add_eos_token=True)

tokenizer.pad_token = tokenizer.eos_token

def tokenize(prompt):
    result = tokenizer(
        prompt,
        truncation=True,
        max_length=2048,
        padding=&quot;max_length&quot;,
    )
    result[&quot;labels&quot;] = result[&quot;input_ids&quot;].copy()
    return result
</code></pre>
<p><em>libs: bitsandbytes, github.com/huggingface/transformers.git, github.com/huggingface/peft.git, github.com/huggingface/accelerate.git, datasets scipy ipywidgets</em>
latest</p>
<p>name\step\train|loss\eval|loss</p>
<pre><code>dolphin-2.2.1_new-2023-12-21-11-49  250 5.6574  5.372434139251709 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-21-17-19  500 4.3343  3.06201434135437 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-21-20-06  750 1.8981  0.4487628936767578
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-22-07-52  1000    0.2334  0.1926171183586121 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-22-13-18  1250    0.1687  0.1445329338312149 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-22-18-39  1500    0.1213  0.096932053565979 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-23-00-00  1750    0.0694  0.039184220135211945
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-23-06-34  2000    0.0169  0.0011213048128411174 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-23-19-08  2250    0.0027  0.00005872833207831718 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-24-01-06  2500    0.0009  0.000027619591492111795 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-24-10-11  2750    0.0002  0.000020941797629348 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-24-16-02  3000    0.0001  0.000017155833120341413 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-24-21-51  3250    0.0001  0.00001347742090729298 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-25-07-43  3500    0   0.000011896418072865345
</code></pre>
<pre><code>PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): MistralForCausalLM(
      (model): MistralModel(
        (embed_tokens): Embedding(32002, 4096)
        (layers): ModuleList(
          (0-31): 32 x MistralDecoderLayer(
            (self_attn): MistralSdpaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): MistralRotaryEmbedding()
            )
            (mlp): MistralMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): MistralRMSNorm()
            (post_attention_layernorm): MistralRMSNorm()
          )
        )
        (norm): MistralRMSNorm()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32002, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=4096, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=32002, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
    )
  )
)
</code></pre>
","transformer"
"126187","Cross-attention mask in Transformers","2023-12-27 15:44:28","126283","7","1482","<nlp><transformer><attention-mechanism><masking>","<p>I can't fully understand how we should create the mask for the decoder's cross-attention mask in the original Transformer model from <a href=""https://arxiv.org/abs/1706.03762"" rel=""noreferrer"">Attention Is All You Need</a>.
Here is my attempt at finding a solution:
Suppose we are training such Transformer model, and we are using different-length batches for the encoder and decoder, e.g. we are trying to train an <em>Italian-to-English</em> machine translation model and we have:</p>
<ol>
<li><p>The following input tokens for the Encoder:</p>
<p><code>[&lt;SOS&gt;, Mi, chiamo, Luke, &lt;EOS&gt;, &lt;PAD&gt;]</code></p>
<p><span class=""math-container"">$n_e=6$</span> (length of encoder's input)</p>
</li>
<li><p>The following input tokens for the Decoder:</p>
<p><code>[&lt;SOS&gt;, My, name, is, Luke, &lt;EOS&gt;, &lt;PAD&gt;, &lt;PAD&gt;]</code></p>
<p><span class=""math-container"">$n_d=8$</span> (length of decoder's input)</p>
</li>
</ol>
<p>We have three attentions masks:</p>
<ol>
<li><span class=""math-container"">$M_e \in \mathbb{R}^{n_e\times n_e}$</span>, Encoder's self attention mask.</li>
<li><span class=""math-container"">$M_d \in \mathbb{R}^{n_d\times n_d}$</span>, Decoder's self attention mask.</li>
<li><span class=""math-container"">$M_x \in \mathbb{R}^{n_d\times n_e}$</span>, Decoder's cross-attention.</li>
</ol>
<p>Note that for the cross-attention block, given a certain embedding dimension <span class=""math-container"">$d_m$</span>, we have that <span class=""math-container"">$Q\in\mathbb{R}^{n_d \times d_m}$</span>, <span class=""math-container"">$K\in\mathbb{R}^{n_e \times d_m}$</span>, <span class=""math-container"">$\frac{QK^T}{\sqrt{n_e}}\in\mathbb{R}^{n_d \times n_e} \to M_x \in \mathbb{R}^{n_d \times n_e}$</span></p>
<p><a href=""https://i.sstatic.net/J71IQl.pngz"" rel=""noreferrer""><img src=""https://i.sstatic.net/J71IQl.pngz"" alt=""enter image description here"" /></a></p>
<ol>
<li><p><span class=""math-container"">$M_e$</span> definition:</p>
<p>In this case we just have to apply the padding mask to the encoder's input</p>
<pre><code>mask = [       &lt;SOS&gt;     Mi Chiamo   Luke  &lt;EOS&gt;  &lt;PAD&gt;  
&lt;SOS&gt;        [     0,     0,     0,     0,     0,  -inf],
Mi           [     0,     0,     0,     0,     0,  -inf],
Chiamo       [     0,     0,     0,     0,     0,  -inf],
Luke         [     0,     0,     0,     0,     0,  -inf],
&lt;EOS&gt;        [     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,  -inf]
]

</code></pre>
<p>We zero-out all the elements belonging to the columns that correspond to the 
token, this way we are sure that the embeddings for the <code>&lt;PAD&gt;</code> token won't
contribute to the computation of the new <em>values</em> <span class=""math-container"">$V^{'}=\sigma(\frac{QK^T}{\sqrt{n_e}} + M_e)V$</span>. (Where <span class=""math-container"">$\sigma$</span> is the <em>softmax</em> function)</p>
</li>
<li><p><span class=""math-container"">$M_d$</span> definition:</p>
<p>In this case it should be enough to define the causal mask to the decoder's input</p>
<pre><code>mask = [       &lt;SOS&gt;     My   Name     is   Luke  &lt;EOS&gt;  &lt;PAD   &lt;PAD&gt;  
&lt;SOS&gt;        [     0,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf],
My           [     0,     0,  -inf,  -inf,  -inf,  -inf,  -inf,  -inf],
Name         [     0,     0,     0,  -inf,  -inf,  -inf,  -inf,  -inf],
is           [     0,     0,     0,     0,  -inf,  -inf,  -inf,  -inf],
Luke         [     0,     0,     0,     0,     0,  -inf,  -inf,  -inf],
&lt;EOS&gt;        [     0,     0,     0,     0,     0,     0,  -inf,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,     0,     0,     0]
]
</code></pre>
<p>We don't care about the padding mask because through the causal mask we implicitly ignore the values corresponding to the <code>&lt;PAD&gt;</code> tokens.</p>
</li>
<li><p><span class=""math-container"">$M_x$</span> definition:
I don't understand if we should combine the causal mask with the padding mask from the <em>encoder</em> output</p>
<pre><code>mask = [       &lt;SOS&gt;     Mi Chiamo   Luke  &lt;EOS&gt;  &lt;PAD&gt;  
&lt;SOS&gt;        [     0,  -inf,  -inf,  -inf,  -inf,  -inf],
My           [     0,     0,  -inf,  -inf,  -inf,  -inf],
Name         [     0,     0,     0,  -inf,  -inf,  -inf],
is           [     0,     0,     0,     0,  -inf,  -inf],
Luke         [     0,     0,     0,     0,     0,  -inf],
&lt;EOS&gt;        [     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,  -inf]
]  
</code></pre>
<p>or if we should just apply the padding mask (since the VALUES are coming from the encoder, and we should have full access over the whole encoder's input)</p>
<pre><code>mask = [       &lt;SOS&gt;     Mi Chiamo   Luke  &lt;EOS&gt;  &lt;PAD&gt;  
&lt;SOS&gt;        [     0,     0,     0,     0,     0,  -inf],
My           [     0,     0,     0,     0,     0,  -inf],
Name         [     0,     0,     0,     0,     0,  -inf],
is           [     0,     0,     0,     0,     0,  -inf],
Luke         [     0,     0,     0,     0,     0,  -inf],
&lt;EOS&gt;        [     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,  -inf],
&lt;PAD&gt;        [     0,     0,     0,     0,     0,  -inf]
]  
</code></pre>
<p>Is this the right way to implement the different attention masks? What's the right alternative for the cross-attention values and what's the rational behind it? Any valid and useful resource is welcome. Thank you!</p>
</li>
</ol>
<p>EDIT:
The rational behind the latter alternative, that personally makes a little more sense to me, is depicted here:</p>
<p><span class=""math-container"">$ 
   \text{Legend}\to 
   \color{orange}{\text{Decoder}} ,\  \color{green}{\text{Encoder}} \\
   \color{orange}{Q^{'}}=\sigma(\frac{\color{orange}{Q}\color{green}{K}^T}{\sqrt{n_e}} + M_x)\color{green}{V} = \\ 
\sigma\left(\color{orange}{
\tiny
\begin{bmatrix}
Q_{\text{&lt;SOS&gt;}_0} &amp; Q_{\text{&lt;SOS&gt;}_1} &amp; \dots &amp; Q_{\text{&lt;SOS&gt;}_{d_m}} \\
Q_{\text{My}_0} &amp; Q_{\text{My}_1} &amp; \dots &amp; Q_{\text{My}_{d_m}} \\
Q_{\text{Name}_0} &amp; Q_{\text{Name}_1} &amp; \dots &amp; Q_{\text{Name}_{d_m}} \\
Q_{\text{Is}_0} &amp; Q_{\text{Is}_1} &amp; \dots &amp; Q_{\text{Is}_{d_m}} \\
Q_{\text{Luke}_0} &amp; Q_{\text{Luke}_1} &amp; \dots &amp; Q_{\text{Luke}_{d_m}} \\
Q_{\text{&lt;EOS&gt;}_0} &amp; Q_{\text{&lt;EOS&gt;}_1} &amp; \dots &amp; Q_{\text{&lt;EOS&gt;}_{d_m}} \\
Q_{\text{&lt;PAD&gt;}_0} &amp; Q_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q_{\text{&lt;PAD&gt;}_{d_m}} \\
Q_{\text{&lt;PAD&gt;}_0} &amp; Q_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q_{\text{&lt;PAD&gt;}_{d_m}} 
\end{bmatrix}
}
\color{green}{
\tiny
\begin{bmatrix}
K_{\text{&lt;SOS&gt;}_0} &amp; K_{\text{Mi}_0} &amp; K_{\text{Chiamo}_0} &amp; K_{\text{Luke}_0} &amp; K_{\text{&lt;EOS&gt;}_0} &amp; K_{\text{&lt;PAD&gt;}_0} \\
K_{\text{&lt;SOS&gt;}_1} &amp; K_{\text{Mi}_1} &amp; K_{\text{Chiamo}_1} &amp; K_{\text{Luke}_1} &amp; K_{\text{&lt;EOS&gt;}_1} &amp; K_{\text{&lt;PAD&gt;}_1} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
K_{\text{&lt;SOS&gt;}_{d_m}} &amp; K_{\text{Mi}_{d_m}} &amp; K_{\text{Chiamo}_{d_m}} &amp; K_{\text{Luke}_{d_m}} &amp; K_{\text{&lt;EOS&gt;}_{d_m}} &amp; K_{\text{&lt;PAD&gt;}_{d_m}}
\end{bmatrix}
}\cdot\frac{1}{\sqrt{n_e}} + M_x\right)\color{green}{V} = 
    $</span>
<span class=""math-container"">$
\sigma\left(
{
\tiny
\begin{bmatrix}
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{My}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Name}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Is}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Luke}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;PAD&gt;}} 
\end{bmatrix}}\cdot\frac{1}{\sqrt{n_e}} + M_x
\right)\tiny{
\color{green}{
\begin{bmatrix}
V_{\text{&lt;SOS&gt;}_0} &amp; V_{\text{&lt;SOS&gt;}_1} &amp; \dots &amp; V_{\text{&lt;SOS&gt;}_{d_m}} \\
V_{\text{Mi}_0} &amp; V_{\text{Mi}_1} &amp; \dots &amp; V_{\text{Mi}_{d_m}} \\
V_{\text{Chiamo}_0} &amp; V_{\text{Chiamo}_1} &amp; \dots &amp; V_{\text{Chiamo}_{d_m}} \\
V_{\text{Luke}_0} &amp; V_{\text{Luke}_1} &amp; \dots &amp; V_{\text{Luke}_{d_m}} \\
V_{\text{&lt;EOS&gt;}_0} &amp; V_{\text{&lt;EOS&gt;}_1} &amp; \dots &amp; V_{\text{&lt;EOS&gt;}_{d_m}} \\
V_{\text{&lt;PAD&gt;}_0} &amp; V_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; V_{\text{&lt;PAD&gt;}_{d_m}}
\end{bmatrix}}
}=
\color{orange}{
\tiny
\begin{bmatrix}
Q^{'}_{\text{&lt;SOS&gt;}_0} &amp; Q^{'}_{\text{&lt;SOS&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;SOS&gt;}_{d_m}} \\
Q^{'}_{\text{My}_0} &amp; Q^{'}_{\text{My}_1} &amp; \dots &amp; Q^{'}_{\text{My}_{d_m}} \\
Q^{'}_{\text{Name}_0} &amp; Q^{'}_{\text{Name}_1} &amp; \dots &amp; Q^{'}_{\text{Name}_{d_m}} \\
Q^{'}_{\text{Is}_0} &amp; Q^{'}_{\text{Is}_1} &amp; \dots &amp; Q^{'}_{\text{Is}_{d_m}} \\
Q^{'}_{\text{Luke}_0} &amp; Q^{'}_{\text{Luke}_1} &amp; \dots &amp; Q^{'}_{\text{Luke}_{d_m}} \\
Q^{'}_{\text{&lt;EOS&gt;}_0} &amp; Q^{'}_{\text{&lt;EOS&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;EOS&gt;}_{d_m}} \\
Q^{'}_{\text{&lt;PAD&gt;}_0} &amp; Q^{'}_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;PAD&gt;}_{d_m}} \\
Q^{'}_{\text{&lt;PAD&gt;}_0} &amp; Q^{'}_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;PAD&gt;}_{d_m}} 
\end{bmatrix}
}
   $</span></p>
<p>Our constraint on the newly obtained <span class=""math-container"">$\color{orange}{Q^{'}}$</span> values is expressed below:</p>
<p><span class=""math-container"">$
\color{orange}{
\tiny
\begin{bmatrix}
Q^{'}_{\text{&lt;SOS&gt;}_0} &amp; Q^{'}_{\text{&lt;SOS&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;SOS&gt;}_{d_m}}\\
Q^{'}_{\text{My}_0} &amp; Q^{'}_{\text{My}_1} &amp; \dots &amp; Q^{'}_{\text{My}_{d_m}} \\
Q^{'}_{\text{Name}_0} &amp; Q^{'}_{\text{Name}_1} &amp; \dots &amp; Q^{'}_{\text{Name}_{d_m}} \\
Q^{'}_{\text{Is}_0} &amp; Q^{'}_{\text{Is}_1} &amp; \dots &amp; Q^{'}_{\text{Is}_{d_m}} \\
Q^{'}_{\text{Luke}_0} &amp; Q^{'}_{\text{Luke}_1} &amp; \dots &amp; Q^{'}_{\text{Luke}_{d_m}} \\
Q^{'}_{\text{&lt;EOS&gt;}_0} &amp; Q^{'}_{\text{&lt;EOS&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;EOS&gt;}_{d_m}} \\
Q^{'}_{\text{&lt;PAD&gt;}_0} &amp; Q^{'}_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;PAD&gt;}_{d_m}} \\
Q^{'}_{\text{&lt;PAD&gt;}_0} &amp; Q^{'}_{\text{&lt;PAD&gt;}_1} &amp; \dots &amp; Q^{'}_{\text{&lt;PAD&gt;}_{d_m}} 
\end{bmatrix}
}
\color{black}{
\tiny
\begin{matrix}
\to\text{Should only contain information from }\color{orange}{Q_\text{&lt;SOS&gt;}} \color{white}{,\text{My}} \color{white}{\text{Name,}} \color{white}{\text{Is,}} \color{white}{\text{Luke,}}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\ \ \ \ \ \ \ \ \ \ \ \\
\to\text{Should only contain information from }\color{orange}{Q_{\text{&lt;SOS&gt;}}}, \color{orange}{Q_\text{My}} \color{white}{\text{Name,,}} \color{white}{\text{Is,}} \color{white}{\text{Luke,}}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\ \ \ \ \ \ \ \\
\to\text{Should only contain information from }\color{orange}{Q_{\text{&lt;SOS&gt;}}}, \color{orange}{Q_{\text{My}}}, \color{orange}{Q_\text{Name}} \color{white}{\text{Is,,}} \color{white}{\text{Luke,}}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\ \ \ \ \ \ \\
\to\text{Should only contain information from }\color{orange}{Q_{\text{&lt;SOS&gt;}}}, \color{orange}{Q_{\text{My}}}, \color{orange}{Q_{\text{Name}}}, \color{orange}{Q_\text{Is}} \color{white}{\text{Luke,}}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\ \ \ \ \\
\to\text{Should only contain information from }\color{orange}{Q_{\text{&lt;SOS&gt;}}}, \color{orange}{Q_{\text{My}}}, \color{orange}{Q_{\text{Name}}}, \color{orange}{Q_{\text{Is}}}, \color{orange}{Q_\text{Luke}}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\ \ \\
\to\text{Should only contain information from }\color{orange}{Q_{\text{&lt;SOS&gt;}}}, \color{orange}{Q_{\text{My}}}, \color{orange}{Q_{\text{Name}}}, \color{orange}{Q_{\text{Is}}}, \color{orange}{Q_{\text{Luke}}}, \color{orange}{Q_{\text{&lt;EOS&gt;}}}\ \  \\
\to\text{We don't care}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}\\
\to\text{We don't care}\color{white}{Q^{'}_{\text{&lt;PAD&gt;}_{d_m}}}
\end{matrix}
}
   $</span></p>
<p>And I see no reason to define a causal mask given that each row in <span class=""math-container"">$\color{orange}{Q}\color{green}{K}^T$</span> contains information about the corresponding token in the decoder (i.e. the first row contains information about the first token <span class=""math-container"">$\color{orange}{\text{&lt;SOS&gt;}}$</span>, the second row contains information about the second token <span class=""math-container"">$\color{orange}{\text{My}}$</span>, and so on...)</p>
<p><span class=""math-container"">$
{
\tiny
\begin{bmatrix}
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;SOS&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{&lt;SOS&gt;}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{My}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{My}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{My}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Name}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Name}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{Name}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Is}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Is}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{Is}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{Luke}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{Luke}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{Luke}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;EOS&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{&lt;EOS&gt;}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{&lt;PAD&gt;}}\cdot\color{grey}{\text{&lt;PAD&gt;}} \\
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;SOS&gt;}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Mi}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Chiamo}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{Luke}} &amp; 
\color{orange}{\text{&lt;PAD&gt;}}\cdot\color{green}{\text{&lt;EOS&gt;}} &amp; 
\color{grey}{\text{&lt;PAD&gt;}}\cdot\color{grey}{\text{&lt;PAD&gt;}}
\end{bmatrix}}
$</span></p>
","transformer"
"126169","Using activations at a specific layer as an input for an LLM such as OPT-350m","2023-12-25 11:48:04","","0","74","<deep-learning><transformer><llm>","<p>I'm working with the OPT-350m model and aiming to utilize embeddings from different layers as inputs for generating data. I've encountered issues when trying to feed these embeddings back into the model using the provided methods.</p>
<pre><code># Import necessary libraries (e.g., PyTorch, Hugging Face Transformers)
import torch
from transformers import AutoTokenizer, OPTForCausalLM

model_name = &quot;facebook/opt-350m&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = OPTForCausalLM.from_pretrained(model_name)

# Encode the input text
inputs = tokenizer(&quot;hello&quot;, return_tensors=&quot;pt&quot;)

# Get model output including hidden states
outputs = model(**inputs, output_hidden_states=True)

# Extract embeddings from a specific layer (e.g., the second layer)
embeddings = outputs.hidden_states[1] 

output = model(inputs_embeds=embeddings)
</code></pre>
<p>However, the code above resulted in a shape error:</p>
<pre><code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x1024 and 512x1024)
</code></pre>
<p>Additionally, I attempted to use <strong>'get_input_embeddings()'</strong> to extract embeddings, but it only provided embeddings from the initial stage, lacking the layer diversity I intended to explore.</p>
<pre><code># Insert a dummy input sentence
input_sentence = &quot;London is the capital of&quot;

# Tokenize the input sentence and get embeddings of the first layer
inputs = tokenizer(input_sentence, return_tensors=&quot;pt&quot;, padding=True, truncation=True)
with torch.no_grad():
    embeddings = model.get_input_embeddings()(inputs.input_ids)

# Use the embeddings as input to the same model
with torch.no_grad():
    output = model.generate(inputs_embeds=embeddings, max_length=10)
</code></pre>
<p>Is there a reliable method to effectively utilize embeddings from different layers as input to the OPT-350m model without encountering shape errors or limitations in layer selection?</p>
","transformer"
"126142","The using of golden dataset in Augmented SBERT Training","2023-12-21 19:43:34","","0","16","<nlp><training><transformer><bert><autoencoder>","<p>I use the training strategy of <a href=""https://www.sbert.net/examples/training/data_augmentation/README.html"" rel=""nofollow noreferrer"">Augmented SBERT (Domain-Transfer)</a>. In the code example they use the golden-dataset (STSb) for the training evaluator. Here two code snippets of the <a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/data_augmentation/train_sts_indomain_semantic.py"" rel=""nofollow noreferrer"">example of sentence-transformers</a>:</p>
<p><em>Get data and split them</em></p>
<pre class=""lang-py prettyprint-override""><code>gold_samples = []
dev_samples = []
test_samples = []

with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:
    reader = csv.DictReader(fIn, delimiter='\t', quoting=csv.QUOTE_NONE)
    for row in reader:
        score = float(row['score']) / 5.0  # Normalize score to range 0 ... 1

        if row['split'] == 'dev':
            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))
        elif row['split'] == 'test':
            test_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))
        else:
            #As we want to get symmetric scores, i.e. CrossEncoder(A,B) = CrossEncoder(B,A), we pass both combinations to the train set
            gold_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))
            gold_samples.append(InputExample(texts=[row['sentence2'], row['sentence1']], label=score))
</code></pre>
<p><em>Initialize evaluator and fit model</em></p>
<pre class=""lang-py prettyprint-override""><code>logging.info(&quot;Read STSbenchmark dev dataset&quot;)
evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')

# Configure the training.
warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up
logging.info(&quot;Warmup-steps: {}&quot;.format(warmup_steps))

# Train the bi-encoder model
bi_encoder.fit(train_objectives=[(train_dataloader, train_loss)],
          evaluator=evaluator,
          epochs=num_epochs,
          evaluation_steps=1000,
          warmup_steps=warmup_steps,
          output_path=bi_encoder_path
          )
</code></pre>
<p><strong>First Question: Why is the golden-dataset used for the evaluation, if the model fits on the silver-dataset?</strong></p>
<p>Further, the <code>test_sample</code> from the golden dataset is used for the final analysis:</p>
<pre class=""lang-py prettyprint-override""><code># load the stored augmented-sbert model
bi_encoder = SentenceTransformer(bi_encoder_path)
test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')
test_evaluator(bi_encoder, output_path=bi_encoder_path)
</code></pre>
<p><strong>Second Question: Why is the <code>test_sample</code> based on the golden-dataset? Why is the <code>test_sample</code> not based on the silver dataset?</strong></p>
","transformer"
"126140","Interpretation of Evaluation Values of Augmented SBERT Training with EmbeddingSimilarityEvaluator()","2023-12-21 18:43:11","","1","38","<training><transformer><bert><similarity><domain-adaptation>","<p>I train a BI-Encoder to get an Augmented SBERT and I get a final training result.</p>
<p>How can I interpret the following output of the final training result?</p>
<pre><code>EmbeddingSimilarityEvaluator: Evaluating the model on test dataset:
Cosine-Similarity : Pearson: 0.8115 Spearman: 0.7777      
Manhattan-Distance: Pearson: 0.7318 Spearman: 0.6822      
Euclidean-Distance: Pearson: 0.7332 Spearman: 0.6835      
Dot-Product-Similarity: Pearson: 0.7780 Spearman: 0.7543

0.7777387754875323 # output of test_evaluator(...)
</code></pre>
<p>The output result out of the following code snipped:</p>
<pre class=""lang-py prettyprint-override""><code># load the stored augmented-sbert model
bi_encoder = SentenceTransformer(bi_encoder_path)
test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')
test_evaluator(bi_encoder, output_path=bi_encoder_path)
</code></pre>
<p>Is a high or low Person resp. Spearman better? They give information about the correlation.</p>
","transformer"
"126033","How does a spare mixture of experts route each token separately to an expert?","2023-12-13 16:51:40","","1","21","<transformer><mixture-of-experts>","<p>As far as I can understand the routing mechanism in the <a href=""https://arxiv.org/pdf/1701.06538.pdf"" rel=""nofollow noreferrer"">the sparsely-gated mixture-of-experts layer</a> routes each token to its own expert. Considering that the output of the attention layer is a three dimensional tensor(batch_size, seq_len, embd_dim), how does this routing happen especially considering that each batch is supposed to be processed independently?</p>
","transformer"
"126025","Do we really need a very large dataset to train GPTs?","2023-12-13 07:20:33","","0","108","<transformer><gpt><llm><chatgpt>","<p>Do we really need a very large dataset to train GPTs?
If this dataset is not big, won't GPT work well? Or will it still work better than conventional learning models in this situation?
And is it possible to quantitatively determine the minimum number of dataset samples suitable for this work? For example, if we talk about malware samples, we can say, for example, that the dataset suitable for GPTs should not be less than a certain number?</p>
","transformer"
"124998","RuntimeError: mat1 and mat2 shapes cannot be multiplied (25x7 and 1x512)","2023-12-10 22:58:28","","0","133","<deep-learning><nlp><time-series><transformer><huggingface>","<p>I am dealing with multivariate time series forecasting using Transformers.
below is my code step by step:</p>
<p>After some preprocessing and windowing time series dataset …</p>
<p>1- Creating Mask function</p>
<pre><code>input_sequence_length = 10 # incoder input sequence
target_sequence_length = 5 # decoder input sequence

tgt_mask = generate_square_subsequent_mask(
    dim1=target_sequence_length,
    dim2=target_sequence_length
   )
src_mask = generate_square_subsequent_mask(
    dim1=target_sequence_length,
    dim2=input_sequence_length
   )
</code></pre>
<p>2- Positional Encoding</p>
<pre><code>class PositionalEncoder(nn.Module):
    def __init__(self, dropout: float = 0.1, 
        max_seq_len: int = 5000, d_model: int = 512,device = device):

        super().__init__()

        self.d_model = d_model
        self.dropout = nn.Dropout(p=dropout)
        self.batch_first = True  # Assuming batch_first is always True

        position = torch.arange(max_seq_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        pe = torch.zeros(1, max_seq_len, d_model)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe)
        
    def forward(self, x: Tensor) -&gt; Tensor:
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
</code></pre>
<p>3 - Creating Transformers Encoder and Decoder with Pytorch</p>
<pre><code>class TimeSeriesTransformer(nn.Module):

    def __init__(self, 
        input_size: int,
        dec_seq_len: int,
        out_seq_len: int= 5, # target_sequence_length
        dim_val: int=512,  
        n_encoder_layers: int=2,
        n_decoder_layers: int=2,
        n_heads: int=4,
        dropout_encoder: float=0.2, 
        dropout_decoder: float=0.2,
        dropout_pos_enc: float=0.1,
        dim_feedforward_encoder: int=512,
        dim_feedforward_decoder: int=512,
        num_predicted_features: int=1
        ): 

        super().__init__() 

        self.dec_seq_len = dec_seq_len

        self.encoder_input_layer = nn.Linear(
            in_features=input_size, 
            out_features=dim_val 
            )

        self.decoder_input_layer = nn.Linear(
            in_features=num_predicted_features,
            out_features=dim_val
            )  
        
        self.linear_mapping = nn.Linear(
            in_features=dim_val, 
            out_features=num_predicted_features
            )

        # Create positional encoder
        self.positional_encoding_layer = PositionalEncoder(
            d_model=dim_val,
            dropout=dropout_pos_enc
            )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=dim_val, 
            nhead=n_heads,
            dim_feedforward=dim_feedforward_encoder,
            dropout=dropout_encoder,
            batch_first=True
            )

        self.encoder = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=n_encoder_layers, 
            norm=None
            )

        decoder_layer = nn.TransformerDecoderLayer(
            d_model=dim_val,
            nhead=n_heads,
            dim_feedforward=dim_feedforward_decoder,
            dropout=dropout_decoder,
            batch_first=True
            )

        self.decoder = nn.TransformerDecoder(
            decoder_layer=decoder_layer,
            num_layers=n_decoder_layers, 
            norm=None
            )

    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, 
                tgt_mask: Tensor=None) -&gt; Tensor:

        src = self.encoder_input_layer(src) 
      
        src = self.positional_encoding_layer(src) 
        src = self.encoder(src=src)
        
        decoder_output = self.decoder_input_layer(tgt)
        decoder_output = self.decoder(
            tgt=decoder_output,
            memory=src,
            tgt_mask=tgt_mask,
            memory_mask=src_mask
            )
        decoder_output = self.linear_mapping(decoder_output) 
        
        return decoder_output
</code></pre>
<p>4 - model</p>
<pre><code>model = TimeSeriesTransformer(
    input_size=7,
    dec_seq_len=5,
    num_predicted_features=1,
    ).to(device)
</code></pre>
<p>5 - creating loader # befor created in the preprocessing step</p>
<pre><code>i, batch = next(enumerate(train_loader))
src, trg, trg_y = batch
src = src.to(device) # shape [5 , 10 , 7] , batch size , encoder sequence len , number of feature
trg = trg.to(device) # shape [5 , 5 , 7], batch size , decoder sequence len , number of feature
</code></pre>
<p>6 - output of the model</p>
<pre><code> output = model(
        src=src,
        tgt=trg,
        src_mask=src_mask,
        tgt_mask=tgt_mask
        )
    trg_y = trg_y.to(device) # [5 , 5 , 1] , batch size , deocder or output sequence len , number predicted feature
</code></pre>
<p>7 - Finally the raised error is like below</p>
<pre><code>output = model(
    src=src,
    tgt=trg,
    src_mask=src_mask,
    tgt_mask=tgt_mask
    )
Traceback (most recent call last):

  Cell In[348], line 1
    output = model(

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1518 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1527 in _call_impl
    return forward_call(*args, **kwargs)

  Cell In[344], line 80 in forward
    decoder_output = self.decoder_input_layer(tgt)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1518 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1527 in _call_impl
    return forward_call(*args, **kwargs)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\linear.py:114 in forward
    return F.linear(input, self.weight, self.bias)

RuntimeError: mat1 and mat2 shapes cannot be multiplied (25x7 and 1x512)
</code></pre>
","transformer"
"124962","Higher level sentence similarity (meaning instead of 'just' embeddings)","2023-12-08 10:39:49","124963","4","250","<nlp><transformer><similarity><llm>","<p>I am looking for the correct model / approach for the task of checking if two sentences have the same <em>meaning</em></p>
<p>I know I can use embeddings to check similarity, but that is not what I am after. I suspect BERT style LLM have nice higher level vector that mights be useful, but I'm not sure how to apply that.</p>
<p>For example this sentence:</p>
<ul>
<li>I am very lazy</li>
</ul>
<p>Has a somewhat similar meaning as:</p>
<ul>
<li>I don't like to work hard</li>
</ul>
<p>But not</p>
<ul>
<li>A lazy horse is not very useful</li>
</ul>
<p>Using 'just' embeddings (for example HF: allMiniLM-L6-v2) gives results that are not useful.</p>
<p><a href=""https://i.sstatic.net/7pKtJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7pKtJ.png"" alt=""enter image description here"" /></a></p>
<p>What would be a good appoarch?</p>
","transformer"
"124902","Confused about Q and K in Attention Mechanism","2023-12-05 02:27:34","124912","0","117","<transformer><attention-mechanism>","<p>The following equation computes the attention scores:</p>
<p>A = softmax(QK / d)</p>
<p>I think Q and K are interchangeable, but why is one called Query and the other called Key?</p>
","transformer"
"124816","Prefix tuning in LLM uses learnable vectors to fine tune the model","2023-11-29 04:54:40","","0","45","<deep-learning><nlp><transformer>","<p>I would like to implement a new architecture for Transformer.</p>
<p>Below description is my thought.</p>
<p>Prefix tuning in LLM uses learnable vectors to fine tune the model.</p>
<p>Is there a way to use the output generated by the Neural network as prefix?</p>
<p>Thanks</p>
<p><a href=""https://i.sstatic.net/13Vmk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/13Vmk.png"" alt=""enter image description here"" /></a></p>
","transformer"
"124780","Assign layers and weights in BERT","2023-11-27 06:42:24","","0","38","<pytorch><transformer><bert><language-model>","<p>I print the weight names and shape of the BERT transformer. Now, I want to assign the printed weight to the layers in the transformers architecture:
<a href=""https://i.sstatic.net/Dtaun.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Dtaun.png"" alt=""enter image description here"" /></a></p>
<p>In the following, I can assign query, key and value:</p>
<p><a href=""https://i.sstatic.net/wfHeW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wfHeW.png"" alt=""enter image description here"" /></a></p>
<p>But, in the print there are attention.output.dense.weight and attention.output.LayerNorm.weight, where can I find it in the architecture of transformer/attention-head?</p>
<p>Further there is an intermediate.dense.weight and there output.dense.weight and output.LayerNorm.weight. Are they parts of &quot;Add &amp; Norm&quot; after the multi-head-attention blocks?</p>
","transformer"
"124772","Accuracy Drop in ViT with Patch Embedding: Investigating the Impact of Added Convolutional Layers","2023-11-26 18:21:58","","0","5","<neural-network><convolutional-neural-network><transformer>","<p>I'm currently working on incorporating a patch embedding layer into my Vision Transformer (ViT). I've defined this layer using four 2D convolutional and initialized it with a normal distribution. The remaining layers of the model use pre-trained weights. However, during inference without training, I noticed a significant accuracy drop to 0%. Initially, the model achieved 45% accuracy with only one 2D convolutional layer (ViT uses one conv as patch embedding). Any insights on why this accuracy drop occurred would be appreciated.</p>
","transformer"
"124742","Fine-tuning Hugging Face’s Llama Model with Unlabelled Data from PDFs from niche domain","2023-11-24 19:17:19","","1","127","<transformer><huggingface><finetuning><llm>","<p>I’m unsure about the next steps. Specifically, I have the following questions:</p>
<ul>
<li><p>How can I prepare my unlabelled data for the fine-tuning process?</p>
</li>
<li><p>What’s the best way to fine-tune the Llama model with my specific dataset?</p>
</li>
<li><p>Are there any specific considerations or best practices I should be aware of when fine-tuning a model with unlabelled data?</p>
</li>
</ul>
<p>I have a large amount of raw text data scraped from thousands of PDFs in a specific domain. I want to use this unlabelled data to fine-tune the Llama model from Hugging Face’s Transformers library.</p>
<p>Here’s what I’ve done so far:</p>
<p>I’ve successfully scraped the text from the PDFs and have it stored in a suitable format.</p>
","transformer"
"124726","What happens when I set is_decoder to True in the bert API from huggingface?","2023-11-23 18:14:29","","0","87","<transformer><bert><huggingface>","<p>Please help me understand the implications of initialising the bert model from huggingface with <code>is_decoder</code> parameter set to <code>True</code></p>
<p>According to the <a href=""https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bertmodel:%7E:text=The%20model%20can,the%20forward%20pass."" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of cross-attention is added between the self-attention layers, following the architecture described in Attention is all you need by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.
To behave as an decoder the model needs to be initialized with the is_decoder argument of the configuration set to True; an encoder_hidden_states is expected as an input to the forward pass.</p>
</blockquote>
<p>I know how Bert is just the encoder and there is no causal masking which is the hallmark of a decoder. Can you please me understand how <code>is_decoder</code> changes the implementation of the bert to make it usable as a decoder?</p>
","transformer"
"124642","Open-Source Large Language Models (LLM): Your experience and recommendation","2023-11-17 22:53:07","","0","178","<transformer><language-model><huggingface><llm>","<p>I’m looking for an open-source LLM for a new project. I want to use it for instructions and to fine-tune the model to a specific domain like legal and rights. Some LLMs are open-source, but they didn’t document, on which training data they trained their model. This makes it a bit complicated in my case.</p>
<p>I’m looking for models, that are open-source and the community knows on which datasets the model was trained.</p>
<p>Do you know open-source LLMs like that and do you have experience with them?</p>
<p>Thank you in advance.</p>
","transformer"
"124591","What is the input to an encoder-decoder transformer in next word prediction task?","2023-11-14 22:21:46","124593","0","225","<nlp><transformer><language-model>","<p>I'm trying to understand how encoder-decoder architectures are used, or if they are used at all, for generative tasks that do not require an explicit prompt (ie. machine translation, summarization, etc.).</p>
<p>From my understanding, decoder-only models autoregressively predict the next token in a sequence given its previous predictions. This makes sense, as we can simply keep feeding it tokens already predicted during inference. But how is this done when there is an encoder involved? For machine translation, we have the sequence in the source language to feed to the encoder. Similarly, we can feed it a passage to summarize for summarization. What would we feed the encoder if we simply wanted next word prediction? Do we feed it the sequence we want it to complete? I haven't found any examples of this task being performed. Does this mean that encoder-decoder models aren't needed for this task?</p>
","transformer"
"124577","How does Bert masked language modelling task make sense if half the time the next sentence is wrong context in the sequence passed through the encoder","2023-11-14 07:42:17","124578","2","115","<nlp><word-embeddings><transformer><bert>","<p>Bert has two types of tasks that it uses to learn contextual word embeddings:</p>
<ol>
<li>Masked word prediction</li>
<li>Next sentence prediction</li>
</ol>
<p>I have read the <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">paper</a> and even there the training details are a little fuzzy or, dont make sense to me</p>
<p>To quote the paper:</p>
<blockquote>
<p>To generate each training input sequence, we sample two spans of text from the corpus, which we
refer to as “sentences” even though they are typically much longer than single sentences (but can
be shorter also). The first sentence receives the A
embedding and the second receives the B embedding. 50% of the time B is the actual next sentence
that follows A and 50% of the time it is a random
sentence, which is done for the “next sentence prediction” task. They are sampled such that the combined length is ≤ 512 tokens. The LM masking is
applied after WordPiece tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces.</p>
</blockquote>
<p>Question:
If 50% of our training examples are sentence (in Bert sense) followed by another unrelated sentence and we run it as one sequence through the transformer and the objective is to find context-specific embeddings then how does the objective of masked language modelling make sense in these cases? Our second sentence is the wrong context here. Masked language modelling makes sense only if the second sentence is the accurate entailment which is the case only in 50% of the cases</p>
","transformer"
"124554","Pytorch Transformer only generating NaN when using mask","2023-11-12 22:57:04","","1","126","<pytorch><transformer><masking>","<p>When I generate a src_mask like this</p>
<pre><code>mask = torch.triu(
    torch.ones(batch_size, batch_size).bool(), 
    diagonal=0
)

&gt;&gt; tensor([[ True,  True,  True,  True,  True],
           [False,  True,  True,  True,  True],
           [False, False,  True,  True,  True],
           [False, False, False,  True,  True],
           [False, False, False, False,  True]])
</code></pre>
<p>then the transformer only generates NaN values. If I change diagonal=1 it works, but I don't really understand why. The goal of the mask is to prevent the transformer from paying attention to to any sample after the current sample (and I want to increase the amount of masked values later) for the model to predict further into the future.</p>
","transformer"
"124535","Can't overfit Transformer Encoder","2023-11-11 00:02:56","","0","138","<python><deep-learning><pytorch><transformer><overfitting>","<p>In the below code I am trying to train a very simple Transformer Encoder model to basically do nothing with its input. Giving some arbitrary input vector x, the aim of the model is then to output that exact same vector. As my loss function I am using the Mean Squared Error, between my input vector x and the output vector y. So the optimal solution would be located at y=x. However apart from very simple input vector shapes where for example the batch size, sequence length and feature dimension are all one, the model always seems to get stuck at some non marginal loss.</p>
<pre><code>import torch
import torch.nn as nn
import torch.optim as optim


class Transformer(nn.Module):
    def __init__(self, d_model, feedforward_expansion, num_layers):
        super().__init__()   
        
        layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=1, 
                                           dim_feedforward=d_model * feedforward_expansion,
                                           batch_first=True)
        
        self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)
        
    def forward(self, x):
        y = self.encoder(x)
        
        return y
    
    
n_batch, n_sequence, n_features = 10, 10, 10
lr = 1e-2
n_epochs = 1000
num_layers = 1
feedforward_expansion = 1
    
criterion = nn.MSELoss()
model = Transformer(n_features, feedforward_expansion, num_layers)
optimizer = optim.Adam(model.parameters(), lr=lr)

x = torch.randn((n_batch, n_sequence, n_features))

for i in range(n_epochs):    
    optimizer.zero_grad()
    
    y = model(x)
    loss = criterion(x, y)
    
    loss.backward()
    optimizer.step()
    
    print(f&quot;{i}: {loss.item():.4e}&quot;)
</code></pre>
<p>I assume this has something to do with the architecture of the Transformer. However I am wondering if someone could explain me in more detail why this model seems to fail at this rather simple task?</p>
","transformer"
"124488","Converting a Standard LSTM RNN over to a Transformer Model","2023-11-08 13:16:53","","0","160","<regression><cnn><lstm><rnn><transformer>","<p>I am looking for some advice on converting my existing CNN/LSTM RNN over to a Transformer type model.  This regression model takes a sliding window size of 240 rows with 33 features.  It aims to predict the percent difference of the next row of data we are working with.  <em>I have fully trained this model below with ~85% accuracy which is fantastic for its use case.</em></p>
<p><strong>Here is the existing model:</strong></p>
<pre><code>model = Sequential()

model.add(Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=(inp_history_size, len(features))))
model.add(BatchNormalization())

model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))
model.add(BatchNormalization())


model.add(Bidirectional(LSTM(256, return_sequences=True)))

model.add(Bidirectional(LSTM(128, return_sequences=True)))

model.add(Bidirectional(LSTM(64)))
model.add(Dropout(0.3))

model.add(Dense(64, activation='elu', kernel_regularizer=l1_l2(l1=l1_reg_strength, l2=l2_reg_strength)))
model.add(BatchNormalization())

model.add(Dense(1))
</code></pre>
<p>I have tried to craft a basic transformer model in a bid to try a different approach and possibly gain better accuracy.  However, <em>no matter what I seem to do, it stalls at ~55% accuracy</em> and losses stop falling.</p>
<p><strong>Here is the basic transformer code I have:</strong></p>
<pre><code>def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;elu&quot;)(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res
           
    
    
def build_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
):
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format=&quot;channels_first&quot;)(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;elu&quot;)(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(1)(x)
    return keras.Model(inputs, outputs)




input_shape=(inp_history_size, len(features))

  
    
model = build_model(
    input_shape,
    head_size=512,
    num_heads=6,
    ff_dim=8,
    num_transformer_blocks=8,
    mlp_units=[256],
    mlp_dropout=0.2,
    dropout=0.1,
)
</code></pre>
<p>I am curious as to why the transformer model stalls in learning.  I have run several tests tweaking the hyperparameters but all end up with the same result.</p>
<p>I run a learning rate finder before every test.  I hope I haven't missed anything but will update this question with further information if needed.</p>
<p>Can anyone lend any advice as to getting this up and running and learning well?  Thank you for your help, tips and code examples in advance.</p>
","transformer"
"124473","Are there other pre-trained time series transformers than TimeGPT?","2023-11-07 16:16:51","","0","181","<time-series><transformer>","<p>I am looking for pre-trained transformers trained on time-series data to use for transfer learning (for forecasting).</p>
<p>I found TimeGPT, and it does claim in <a href=""https://arxiv.org/pdf/2310.03589.pdf"" rel=""nofollow noreferrer"">this paper</a> to be the first foundation model. However, that is a claim I am skeptical of; The claim in the abstract is very broad.</p>
<p>Are there any others?</p>
","transformer"
"124449","For a fine-tuning a transformer to type like a specific person, should I use sentence semantic embeddings or word semantic embeddings","2023-11-06 20:27:34","","0","17","<word-embeddings><transformer><word2vec><embeddings><semantic-segmentation>","<p>I'm not clear on the pros and cons of each one for this particular task. Is there even a meaningful difference?</p>
<p>My guess is using semantic embeddings for words will be better in nearly all cases because if you use semantic embeddings for the entire sentence you will still need to tokenize every word anyway. Moreover, sentence semantic embeddings seem like they'd require a much larger vector to be useful given the wide variety of sentence combinations that exist. So even though each word needs its own embedding vs each sentence, if your sentences are all under like 20 words the word embeddings would need less computational power.</p>
<p>But that's just my intuition. Because sentence semantic embeddings really only got popular this year, I can't find a clear answer anywhere so I'm  asking.</p>
","transformer"
"124233","Understanding Multi-headed Attention from architecture details","2023-10-23 19:12:28","","0","49","<machine-learning><deep-learning><nlp><transformer><attention-mechanism>","<p>I've a conceptual question</p>
<p>BERT-base has a dimension of 768 for query, key and value and 12 heads (Hidden dimension=768, number of heads=12). The same is conveyed if we see the BERT-base architecture</p>
<pre><code>(self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
)
</code></pre>
<p>Now, my question is:</p>
<blockquote>
<p>Can I consider the first 64 neurons from the <em>out_features</em> as the
first-head, the next 64 neurons from the <em>out_features</em> as the 2nd
head and so on? (sec 3.2.2 from original paper; <a href=""https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"" rel=""nofollow noreferrer"">Link</a>)</p>
</blockquote>
<p>P.S: I referred to some of the previous posts (<a href=""https://datascience.stackexchange.com/questions/88330/how-do-the-linear-layers-in-the-attention-mechanism-work"">example</a>), but I would appreciate any validation on this thought-process as it's similar but not same.</p>
<p><strong>Update</strong></p>
<p>Here's a code which prunes a particular % in particular layer depending on <em>layer_index</em> and <em>prune_percentage</em></p>
<pre><code>model = AutoModelForMaskedLM.from_pretrained(checkpoint)

linear_layers_list = []
for name, layer in model.named_modules():
    if name in model_layers_list:
        linear_layers_list.append(layer)
print(f&quot;No of linear layers are: {len(linear_layers_list)}&quot;)

layer = linear_layers_list[layer_index]
if prune_type == 'ln_structured':
    # Ln structured with n=1 i.e L1 pruning
    prune.ln_structured(layer, name='weight', amount=prune_percentage, dim=0, n=n)
</code></pre>
<p>Here, I can understand that I can basically pass the Linear module and prune x% of weights.</p>
<p>Now, I would like to prune/remove one head in a similar fashion. Any help is appreciated!</p>
<p>Thanks</p>
","transformer"
"124224","In rotary positional embeddings (RoPE), why do we not rotate the values as well?","2023-10-23 10:53:39","","1","159","<nlp><transformer><encoding>","<p>Actually, the question is all there is</p>
<p>As per the <a href=""https://arxiv.org/pdf/2104.09864v4.pdf"" rel=""nofollow noreferrer"">paper</a> I see that the rotations are applied only to the keys and the queries. Why are the rotations not applied to the values as well?</p>
<p>The reasons for applying the rotations to the values as well:</p>
<ol>
<li>The sinusoidal embeddings are applied to all three: Keys, Queries and, the Values</li>
<li>Why would we not want the embedding values of the tokens to change depending on the relative positions of their occurrences?</li>
</ol>
","transformer"
"124200","What is the ""Extract"" token and how is the final Linear layer applied in GPT?","2023-10-22 04:08:28","124202","0","252","<nlp><transformer><transfer-learning><gpt>","<p>In the manuscript of GPT, the authors have given the following image:</p>
<p><a href=""https://i.sstatic.net/QYquD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/QYquD.png"" alt=""enter image description here"" /></a></p>
<p>Questions:</p>
<ol>
<li>What is the final &quot;Extract&quot; (token?)? Is it the &quot;END&quot; token?</li>
<li>How is the final linear layer applied? We would get a tensor of shape (N,L,D) out of the transformer (tranformer's final layer would itself be a pointwise feedforward) where N is the number of samples, L is the sequence length and D is the output dimension for every token. Taking just one sample it would be (1,L,D). How is the linear layer applied on top of this? The output of the final linear layer would have to be a vector with dimensions equal to the number of decisions at hand (n in case of n-way-classifier, vocabulary size in case of cloze task etc)</li>
</ol>
","transformer"
"124160","What is the prior mu in Heterogeneous Graph Transformer?","2023-10-17 22:26:53","","0","14","<class-imbalance><transformer><attention-mechanism><graph-neural-network><gnn>","<p>I am reading <a href=""https://arxiv.org/pdf/2003.01332.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2003.01332.pdf</a> and do not understand what the prior (\mu) is supposed to be. I also found their implementation on github, but it is still not clear to me. For context: The model operates on graphs where tau(s) phi(e) tau(t) is the &quot;meta&quot;-edgetype between the class of nodes s and t.</p>
<p>I am thinking it is some sort of edge count, e.g. the amount found in the training data, for each metaedge type. Is this correct?</p>
<p><a href=""https://i.sstatic.net/lk4ix.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lk4ix.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/81ZGR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/81ZGR.png"" alt=""enter image description here"" /></a></p>
","transformer"
"124127","A question about contextual embeddings in the decoder only transformer architecture (gpt)","2023-10-15 16:00:13","124133","1","312","<nlp><word-embeddings><transformer><gpt><context-vector>","<p>I am reading up on the <a href=""https://stanford-cs324.github.io/winter2022/lectures/training/#decoder-only-models"" rel=""nofollow noreferrer"">decoder only architecture</a></p>
<p>Relevant excerpts:</p>
<p>We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers):</p>
<p><span class=""math-container"">$$\phi : V^L \to R^{d \times L}$$</span></p>
<p>Recall that an autoregressive language model defines a conditional distribution:</p>
<p><span class=""math-container"">$$p(x_i∣x_{1:i−1})$$</span></p>
<p>We define it as follows:</p>
<ul>
<li>Map <span class=""math-container"">$x_{1:i-1}$</span> to contextual embeddings <span class=""math-container"">$\phi(x_{1:i-1})$</span></li>
<li>Apply an embedding matrix <span class=""math-container"">$E \in R^{V×d}$</span>to obtain scores for each token <span class=""math-container"">$E\phi(x_{1:i-1})_{i-1}$</span></li>
<li>Exponentiate and normalize it to produce the distribution over xi</li>
</ul>
<p>Succinctly:
<span class=""math-container"">$$p(x_{i+1} \mid x_{1:i}) = softmax(E \phi(x_{1:i})_i)$$</span></p>
<p>Questions:</p>
<ol>
<li>What is the meaning of the second subscript on <span class=""math-container"">$\phi$</span> in <span class=""math-container"">$ E \phi (x_{1:i-1})_{i-1}$</span></li>
<li>I think  <span class=""math-container"">$softmax(E \phi(x_{1:i})_i)$</span> just takes the dot product of the context embedding for the word at the position <span class=""math-container"">$i$</span> with the embeddings <span class=""math-container"">$E$</span> of the entire vocabulary. This means that for the word at <span class=""math-container"">$i$</span>, we are basically just trying to learn the context embeddings as something that would essentially be equal to the embedding of the next token (if the model learns perfectly) in <span class=""math-container"">$E$</span>. Why is that the case? Should there not be a feed-forward between the final context embeddings and the embedding <span class=""math-container"">$E$</span> for the next token and then the similarity should be checked? This way, in the best case scenario, the contextual embeddings learned would just be the vector that appeared for the word at <span class=""math-container"">$i$</span> in <span class=""math-container"">$E$</span>. Please help me understand how we are not just asking the contextual embedding to be equal to the embedding <span class=""math-container"">$E$</span> for the word at <span class=""math-container"">$i$</span>?</li>
</ol>
","transformer"
"124097","RetNet Paper Multi Scale Retention dimemsion question","2023-10-12 05:07:38","","0","34","<deep-learning><transformer><mathematics>","<p>From the paper:
<a href=""https://arxiv.org/pdf/2307.08621.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2307.08621.pdf</a>
<a href=""https://i.sstatic.net/I9KXx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/I9KXx.png"" alt=""From the paper"" /></a></p>
<p>But since X is of size n by <span class=""math-container"">$d_{model}$</span>. <a href=""https://i.sstatic.net/Dt2Q9.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Dt2Q9.png"" alt=""enter image description here"" /></a><a href=""https://i.sstatic.net/igWL0.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/igWL0.png"" alt=""Retention"" /></a></p>
<p>How can we compute <span class=""math-container"">$XW_Q$</span>? Since the row length of X which is <span class=""math-container"">$d_{model}$</span> is not the same as the column length of <span class=""math-container"">$W_Q$</span> which is <span class=""math-container"">$d$</span>.</p>
","transformer"
"124056","How to implement 2d Rotary Position Embedding in PyTorch?","2023-10-09 13:18:34","","1","1150","<pytorch><transformer>","<p>The original RoPE paper suggests that the Rotary Position Embedding it describes can easily be extended to two or more dimensions: 3.2.2 in <a href=""https://arxiv.org/abs/2104.09864"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2104.09864</a> . I'm trying to find a PyTorch implementation of this and I can't; I can only find implementations for a 1D embedding. Is anyone aware of any 2D implementations in PyTorch (or another framework, which I could translate to PyTorch)?</p>
","transformer"
"124052","Why are 1/n, 2/n, 3/n ... 2048/n not good positional encodings to be concatenated to the word vectors in transformers?","2023-10-09 06:45:48","","0","39","<transformer><encoding>","<p>The transformer architecture has no sense of the relative positions of the word and hence we need to pass that information apriori to the along with the word embeddings to the model</p>
<p>The positional encoding currently implemented in the transformers uses trig enccoding</p>
<p>Why is a simple positional encoding scheme like concatenating 1/n, 2/n, ... 2048/n (for gpt-4) to the input words not chosen? What are the arguments against using it?</p>
","transformer"
"124020","How can we use a transfomer model with new data if we still don't have the output?","2023-10-06 16:04:31","124104","2","131","<transformer><attention-mechanism>","<p>Transformer models are trained using inputs and outputs. They are both embedded and encoded and used to train multi-head attention mechanisms...</p>
<p>But how can we use a transformer model to predict new data?
We won't have any &quot;output&quot; to feed the model yet.</p>
<p>For example you use English and Spanish text to create a dictionary.
But when you want to translate new English text you don't know the translation yet.</p>
<p><a href=""https://i.sstatic.net/LIZ8i.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LIZ8i.jpg"" alt=""enter image description here"" /></a></p>
","transformer"
"123949","Deep Learning Methods for Video Classification","2023-10-02 15:25:20","","0","16","<deep-learning><classification><pytorch><computer-vision><transformer>","<p>I'm working on a dataset with ~300 videos that last from 9 to 13-minute interviews of each subject and it has all the personality-related metadata that was collected during initial surveys. Which Deep Learning architectures are best suitable for classifying high/low personality dimensions from videos?</p>
","transformer"
"123924","A good set of datasets/models for testing an NLP technique","2023-09-30 17:58:12","124090","1","332","<deep-learning><nlp><dataset><transformer>","<p>I am a machine learning researcher who up until this point has primarily worked on Computer Vision problems. However, I have an idea for an NLP technique involving a novel Transformer architecture, and I’d like to explore it.</p>
<p>What’s a good progression of datasets/models to explore? The technique I have in mind is pretty general and should apply to any decoder-only architecture. If it were, say, an image classification problem I might start with ResNet on an MNIST variant or CFAR, then move on to ImageNet. What's the NLP equivalent of that?</p>
<p>Unless there's a better way, I’d like to start by training from scratch on something small and comparing to a vanilla transformer. If things work I’ll want to try my ideas on more state-of-the art models and datasets, probably through fine-tuning. I don’t have a lot of resources, though I do plan to ultimately work up to a GPT-2 fine-tuning if I’m feeling confident.</p>
<p>Thank you in advance!</p>
","transformer"
"123914","Why is T5 often used in text-to-data for text prompt encoders?","2023-09-29 18:08:42","123956","1","183","<machine-learning><deep-learning><transformer><embeddings><generative-models>","<p>In the text-to-data(music, image, audio, etc.) generative AI field, one method of encoding text prompts is to use pre-trained language models. Such an approach was used in research on Moûsai [1] and Photorealistic [2] , for example. And in such recent text-to-data generative AI fields, T5 is often used for this pre-trained language model.</p>
<p>But why is T5 often used?</p>
<p>I also wonder if LLMs with a larger number of parameters than T5 could be used to better represent the complexity and composition of music. Therefore, I do not understand why such LLMs are not used.</p>
<h2>References:</h2>
<p>[1] Schneider, et al., ‘Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion’, Available: <a href=""https://arxiv.org/abs/2301.11757"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2301.11757</a></p>
<p>[2] Saharia et al., 'Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding', Available: <a href=""https://arxiv.org/abs/2205.11487"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2205.11487</a></p>
","transformer"
"123909","Neural regression predictions all around the mean of target","2023-09-29 13:22:51","","0","193","<deep-learning><regression><transformer><bias><torch>","<p>I have a transformer regression model and some data about last users transactions (categorical and numerical). My target has exponential distribution with mean aroud 10e4 and also zero-inflated, so I have written my own implementation of tweedie loss, which is actually improving over training</p>
<pre><code>#loss function is somethin like this
tweedie_loss = -1*(y*torch.pow(preds, 1-p)/(1-p)) + torch.pow(preds, 2-p)/(2-p)
</code></pre>
<p>Training is going smoothly and my loss function is improving (until it hits the &quot;plato&quot; but it`s okay)</p>
<p>But on evaluation model predicts pretty much same values around the mean (actually during training mean of predictions constanly growing up until it reaches mean of the target).
Even if I trying to overfit it on data Im getting same results.</p>
<p>I also tried to log-transform my data and fit it with different loss (rmse, smoothl1 etc...) but still getting nowhere.</p>
<p>Problem is definitely not in the model implementation because Im using exact same code for another task and it works flawlessly.</p>
<p>So I think my question is the following: do I have too complex target for the model or is there something wrong in my approach to solving the problem?</p>
","transformer"
"123908","How to get Llama-2 Rotary Embeddings?","2023-09-29 11:31:13","","0","319","<nlp><word-embeddings><transformer><language-model><huggingface>","<p>I want to get the Llama-2 rotary embeddings. I do <code>print(model)</code> and get the following output:
<a href=""https://i.sstatic.net/jFu2Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jFu2Z.png"" alt=""enter image description here"" /></a></p>
<p>In the picture I highlight the rotary embeddings.</p>
<p>How can get the rotary embeddings and how can I interpret the output? What means 32x LLamaDecoderLayer and in its round brakets are four layer plus LlamaRotaryEmbeddings?</p>
<p>It's possible to get the embeddings as the first hidden-state <code>hidden_state[0]</code> and I want to know, which hidden-state represents the rotary embeddings.
Am I right, that there are several rotary embeddings?</p>
<p>Thanks in forward.</p>
<p>Best regards.</p>
","transformer"
"123891","Errors in results after saving model vs using directly from memory","2023-09-28 09:52:51","","0","30","<pytorch><transformer><huggingface><llm><serialisation>","<p>I am trying to save a Fine Tuned model using <code>trainer.save_model()</code> but after I load the saved_model it just responds with the input back again and does not give any new output.</p>
<p>But, when I don't save the model and use the one with changed weights from FT from memory. It gives expected responses.  (Jupyter notebook)</p>
<p>Example Code:</p>
<pre><code>model_name = &quot;bigcode/starcoder&quot;

packing = False
base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', load_in_8bit =True)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

training_arguments = TrainingArguments(
    output_dir=&quot;/tmp/output&quot;,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=25
)

# Set supervised fine-tuning parameters
trainer = SFTTrainer(
    model=base_model,
    train_dataset=dataset[&quot;train&quot;],
    eval_dataset=dataset[&quot;test&quot;],
    dataset_text_field=&quot;text&quot;,
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

trainer.train()

trainer.save_model('xxx')

def infer(model, ip):
    inputs = tokenizer.encode(ip, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)
    outputs = model.generate(inputs, max_new_tokens=300, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(outputs[0])

</code></pre>
<p>Not sure what the issue is as not being able to save model is a bottleneck and it also does not release memory used within FT. (Jupyter)</p>
","transformer"
"123754","Different generated patches from original image using vision transformer (ViT)","2023-09-18 19:47:13","","0","86","<python><deep-learning><image-classification><transformer><transfer-learning>","<p>I am using ViT for image classification, I scaled images in range of [-1,1], and I also padded images. Then, I used the following code to see the original image and generated patches, but the output of generated patches is not what I expected. I added images too.        I have a couple of questions:</p>
<ol>
<li>Can I use pretrained vit for non-square images?</li>
<li>Does padding have negative effect on performance when the original size is (128,431) and I padded the data into (432,432) because I did not want to lose any information from my image.</li>
<li>How can I have a correct generated patches for vit ?</li>
</ol>
<p>I appreciate your help.</p>
<p><a href=""https://i.sstatic.net/8o40P.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8o40P.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/70cAX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/70cAX.png"" alt=""enter image description here"" /></a></p>
<pre><code>import numpy as np
import tensorflow as tf
import tensorflow.keras.layers as L
import matplotlib.pyplot as plt
import librosa.display


output_train = np.load('C:/..../ViT/output_train.npy')

class Patches(L.Layer):
    def __init__(self, patch_size):
        super(Patches, self).__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images = images,
            sizes = [1, self.patch_size, self.patch_size, 1],
            strides = [1, self.patch_size, self.patch_size, 1],
            rates = [1, 1, 1, 1],
            padding = 'VALID',
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches

def visualize_patches(image_size, patch_size):
    
    plt.figure(figsize=(4, 4))
    image = output_train[0][0]

    single_channel_spectrogram = np.mean(image, axis=-1)
    single_channel_spectrogram = np.squeeze(single_channel_spectrogram)
    librosa.display.specshow(single_channel_spectrogram,sr=44100,
                             x_axis='time',y_axis='mel',
                             fmax=5000)
    plt.axis('off')

    resized_image = tf.image.resize(
        tf.convert_to_tensor([image]), size = (image_size, image_size)
    )

    patches = Patches(patch_size)(resized_image)
    print(f'Image size: {image_size} X {image_size}')
    print(f'Patch size: {patch_size} X {patch_size}')
    print(f'Patches per image: {patches.shape[1]}')
    print(f'Elements per patch: {patches.shape[-1]}')
    
    n = int(np.sqrt(patches.shape[1]))
    plt.figure(figsize=(4, 4))
    
    for i, patch in enumerate(patches[0]):
        ax = plt.subplot(n, n, i + 1)
        patch_img = tf.reshape(patch, (patch_size, patch_size, 3))
        single_channel_patch_img = np.mean(patch_img, axis=-1)
        single_channel_patch_img = np.squeeze(single_channel_patch_img)
        librosa.display.specshow(single_channel_patch_img,sr=44100,
                                 x_axis='time',y_axis='mel',
                                 fmax=5000)
        
        plt.axis('off')
    plt.show()

if __name__ == '__main__':
    visualize_patches(224, 32)


</code></pre>
","transformer"
"123714","Some fundamental questions about Transformer","2023-09-15 12:32:28","","1","113","<word-embeddings><transformer>","<p>In the Transformer framework, a token as an input (time = <span class=""math-container"">$t$</span>) <span class=""math-container"">$y^t$</span> is given by a sum of the original embedding of the token <span class=""math-container"">$x^t$</span> plus, a position embedding factor <span class=""math-container"">$v^t$</span>, i.e.,
<span class=""math-container"">\begin{align}
y^t = x^t + v^t. 
\end{align}</span>
Here, each symbol denotes an <span class=""math-container"">$R^{d}$</span> vector, where <span class=""math-container"">$d$</span> is the size of the embedding.</p>
<p>The position embedding factor <span class=""math-container"">$v^t$</span> is fixed in the Transformer framework as
<span class=""math-container"">\begin{align}
v^t_{2i} &amp;=  \sin (t/T^{2i/d}), \\
v^t_{2i+1} &amp;=  \cos (t/T^{2i/d}).
\end{align}</span></p>
<p>Then, I have three questions:</p>
<ol>
<li><p>I guess that <strong>the original embeddings <span class=""math-container"">$x^t$</span> is usually normalized</strong> such that <span class=""math-container"">$|x^t_i|\leq 1.0$</span>. <strong>Then, the position embedding factor has a comparable size</strong> since the amplituide is one. Why the Transformer works well?
It is very differnt from my intuition. In my understanding, embeddings of the tokens <span class=""math-container"">$x^t$</span> correspond to the expansion by a basis of &quot;meaning&quot; space, if tokens are words. e.g, &quot;airplane&quot; = &quot;machine 80%&quot; and &quot;air&quot; 10% and ... .
Then, the comparable fluctuation by the position embedding factor can mess up completely that expansion. It can induce that &quot;airplane&quot; = &quot;human 80%&quot; and &quot;sea&quot; 15%... . How can we explain the reason it works?
If the expansion is not done on the meaning space, I still confuse because the position embedding factors can hide the original embeddings.</p>
</li>
<li><p><strong>Why the position embedding factor uses both sine and cosine?</strong>
I have read that, the existence of a linear transformation between different <span class=""math-container"">$t$</span> contributes to possibility of learning. It seems to be true but, <strong>the linear transformation again mixes up the expansion in the embedding space!</strong> Explicitly, the position embedding factor at <span class=""math-container"">$t + k$</span> can be expressed as
<span class=""math-container"">\begin{align}
\begin{pmatrix}
v^{t + k}_{2i}\\
v^{t + k}_{2i+1}  
\end{pmatrix}
=
\begin{pmatrix}
\cos(k/T^{2i/d}) &amp; \sin(k/T^{2i/d}) \\
-\sin(k/T^{2i/d}) &amp; \cos(k/T^{2i/d})
\end{pmatrix}
\begin{pmatrix}
v^{t}_{2i}\\
v^{t}_{2i+1}  
\end{pmatrix}.
\end{align}</span>
This can be understood as a basis transformation. It mixes up the two different directions in the embedding space. For me, it seems a disaster because each direction corresponds to a certain &quot;mean&quot; in the meaning space. Why this mixture is allowed?</p>
</li>
<li><p><strong>Of course, the actual input is the sum <span class=""math-container"">$y^t$</span>, not <span class=""math-container"">$v^t$</span>.</strong> In the above discussions, the appealing (which I do not understand yet) linear transformation is defined on <span class=""math-container"">$v^t$</span>. Nevertheless, the original paper and review papers of the Transformer claims that it is the attractive feature.  <strong>Why can we focus only on the position embedding factor?</strong></p>
</li>
</ol>
<p>I am new to ML (major in physics), so I will miss some fundamental concepts, sorry.</p>
<hr />
<p>(Edit)</p>
<p>What I said &quot;comparable&quot; means that <span class=""math-container"">$|x_i^t| \sim |v^t_{2i}|$</span>. Hence, if we add up them, the actual input <span class=""math-container"">$y^t$</span> and <span class=""math-container"">$x^t$</span> will be very different.</p>
<p>If there are some other approaches which change the amplitude as <span class=""math-container"">$v^t_{2i} = A \sin(t/T^{2i/d})$</span> with <span class=""math-container"">$A \ll 1$</span>, then it matches with my intuition.</p>
","transformer"
"123640","CLIP Visual Transformer image encoder","2023-09-10 17:49:53","","1","172","<deep-learning><computer-vision><transformer><embeddings><encoding>","<p>I was doing some experiments with the CLIP's visual transformer encoder output (<code>clip-ViT-B-32</code>). So basically given the same scene or image, it should output almost same image feature vector given it's a semantics model. But looks like it is very sensitive to illumination and lighting conditions which makes me wonder and the percentage of similarity between the images below are much lower than expected (surprisingly it says 89.45% similar)</p>
<p>Why is that? Is there any ways/models which are less sensitive to illumination changes and are more semantic based?</p>
<pre><code>from sentence_transformers import SentenceTransformer, util
#......
model = SentenceTransformer('clip-ViT-B-32')
encoded_image = model.encode(image, batch_size=128, convert_to_tensor=True, show_progress_bar=True)

# Now we run the clustering algorithm. This function compares images aganist 
# all other images and returns a list with the pairs that have the highest 
# cosine similarity score
processed_images = util.paraphrase_mining_embeddings(encoded_image)
</code></pre>
<p><a href=""https://i.sstatic.net/21a9Im.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/21a9Im.png"" alt=""enter image description here"" /></a> <a href=""https://i.sstatic.net/conHNm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/conHNm.png"" alt=""enter image description here"" /></a></p>
","transformer"
"123620","Swin Transformer Relative Position Biases","2023-09-08 17:23:31","","1","80","<machine-learning><pytorch><computer-vision><transformer><artificial-intelligence>","<p>I was reading the swin transformer paper and looking at the github implementation, i noticed that when calculating the relative position bias the input to the log function before the CPB MLP is scaled to a range 0 to 8. I couldn't see mention of this in the original paper my intuition is that this will give output in the range 0 to 1 I was wondering if this was the correct reasoning?</p>
<p>However, i also noticed that the output of the MLP is passed through a sigmoid function before being scaled by a factor of 16. I also couldn't find this being mentioned in the paper and was wondering what the underlying reasoning is?</p>
<p>Also I have just noticed that the logit scale parameter is intialised to ln(10) is there a reason for this?</p>
<p>Thankyou for any assistance.</p>
","transformer"
"123503","Tensorflow diagram for attention mechanism","2023-09-01 10:33:34","","0","56","<neural-network><transformer><attention-mechanism>","<p>I was reading the <a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">tutorial from tensorflow on the transformer model</a>, however, when they explain the transformer model, they display such a picture :</p>
<p><a href=""https://i.sstatic.net/p2CAK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/p2CAK.png"" alt=""enter image description here"" /></a> which I don't understand. What do the ingoing arrow and outgoing arrow mean? What do the bold vertical line and the normal horizontal lines mean? Why is there just one array for (key,value)?</p>
","transformer"
"123472","Deploying a model with GPU and pay-per-inference","2023-08-30 08:11:55","","1","78","<transformer><huggingface><aws><deployment><aws-lambda>","<p>I may have the wrong stack exchange. If that's the case, could someone point me to a stack that could help with this. Anyways...</p>
<p>My backend employs a sentence transformer model from HuggingFace. Since the number of requests per day is small, deploying a dedicated instance for serving inference requests is not cost-effective. Hence, I was thinking of AWS Lambda whereby I could pay per inference.</p>
<p>However, I need to serve each request fast, which necessitates using a GPU, which AWS Lambda does not offer.</p>
<p>Is there a deployment solution (not necessarily at AWS) whereby I would be able to use a GPU and pay per inference?</p>
","transformer"
"123454","Sum of vector sentence embeddings vs. paragraph embedding","2023-08-29 09:36:26","","0","192","<nlp><word-embeddings><transformer>","<p>I have been experimenting with the <a href=""https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"" rel=""nofollow noreferrer"">all-MiniLM-L6-v2 model</a> for computing 384-dimensional vector embeddings for text paragraphs. The following code compares the embedding computed for a paragraph with the sum of embeddings of the constituent sentences:</p>
<pre class=""lang-py prettyprint-override""><code>import re
def split_into_sentences(text):
    # Split using regular expression to preserve punctuation at the end of sentences
    sentences = re.split(r'(?&lt;=[.!?])\s+', text)
    return sentences

text = &quot;Tigers, the largest of all big cats, possess an undeniable allure that captivates the human imagination. Their iconic coat of burnt orange, embellished with bold ebony stripes, paints a portrait of both grace and potency, symbolizing the untamed beauty of the natural world. As stealthy hunters, they navigate their diverse habitats with an air of both confidence and mystery, often lurking within the dense undergrowth of jungles or prowling the open grasslands with unparalleled stealth. A tiger's sinuous movements reflect a balance between athleticism and elegance, a testament to their adaptability in various terrains. These felines are not merely ground-bound; their prowess extends to swimming, displaying an unexpected dexterity in the water, and climbing, as they ascend trees with remarkable agility. Intricately patterned, a tiger's stripes are akin to a fingerprint, unique to each individual, and play a vital role in their camouflage while stalking prey. It is the eyes of these creatures, however, that truly leave an indelible mark—an intense amber gaze that radiates an aura of fierce determination. Throughout history and across cultures, tigers have held a mythical status, embodying strength, wisdom, and courage. Yet, the same forces that once revered them now threaten their existence. The encroachment of human activity upon their habitats and the insidious specter of poaching pose grave challenges to their survival. In response, conservation efforts have emerged as a beacon of hope for these magnificent creatures. Collaborative initiatives, alongside advancements in technology and awareness campaigns, strive to protect and preserve their habitats, ensuring a future where tigers continue to roam the landscapes they have graced for millennia. The allure of the tiger, both as a symbol of nature's magnificence and as a reminder of our responsibility as stewards of the planet, remains as potent as ever—a reminder that the fate of these enigmatic creatures is intertwined with the destiny of our world.&quot;
sentences = split_into_sentences(text)
text_embedding = get_embeddings([text])[0]
sentence_embeddings = get_embeddings(sentences)

print([np.round(cosine_similarity(text_embedding, e), 3) for e in sentence_embeddings])
print(cosine_similarity(text_embedding, np.sum(sentence_embeddings, axis=0)))
</code></pre>
<p>I get the output:</p>
<pre><code>[0.677, 0.462, 0.526, 0.701, 0.586, 0.797, 0.544, 0.697, 0.163, 0.336, 0.32, 0.575, 0.697]
0.8497203877599846
</code></pre>
<p>The first line compares each of the sentences with the whole paragraph. The second line compares the sum of sentence embeddings with the embedding for the whole paragraph.</p>
<p>It strikes me as unexpected that the sum of all sentence embeddings (~0.85) does not do much better than the sixth sentence alone (~0.80). Why is that the case?</p>
","transformer"
"123367","How can BERT/Transformer models accept input batches of different sizes?","2023-08-24 00:05:04","","0","27","<neural-network><nlp><transformer><huggingface>","<p>I understand that all inputs in a batch need to be of the same size. However, it seems BERT/Transformers models can accept batches with different sizes as input.</p>
<p>How is that possible? I thought we needed to pad all examples in a batch to <code>model.max_input_size</code>, however, it seems HuggingFace does <code>Dynamic Padding</code> that allows sending batches of different lengths (till the time they are smaller than <code>max_input_size</code>)</p>
<p><a href=""https://i.sstatic.net/zC1WS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zC1WS.png"" alt=""enter image description here"" /></a></p>
<p>Link: <a href=""https://mccormickml.com/2020/07/29/smart-batching-tutorial/"" rel=""nofollow noreferrer"">https://mccormickml.com/2020/07/29/smart-batching-tutorial/</a><br />
Link2: <a href=""https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding</a></p>
","transformer"
"123325","how do we adapt LLM token embeddings with custom vocab","2023-08-20 16:14:35","","2","2832","<nlp><transformer><tokenization>","<p>Hi im just getting started with understanding transformer based models and I am not able to find how the token embeddings are arrived at?. there are multiple tokenization approaches and multiple vocabularies/documents llms are trained on. so my question is</p>
<ol>
<li>whether each llm also trains its own token embeddings?</li>
<li>how do those pre trained embeddings work for transfer learning or fine tuning, on
custom data sets where some OOV words may be present or we have some
special unique tokens we want to keep whole and not have tokenizer do subword tokens?</li>
</ol>
","transformer"
"123229","Understanding alpha parameter tuning in LORA paper","2023-08-14 08:52:24","","3","10057","<transformer><finetuning><llm>","<p>I was reading the LORA paper <a href=""https://arxiv.org/pdf/2106.09685.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2106.09685.pdf</a> a thing I don’t understand is section 4.1, where the updates are updated by alpha, where alpha is a constant in r. It is said that alpha is set to the first r tried. Then if I understand correctly, the authors say that this makes it unnecesary to tune the learning rate alpha. I would really appreciate if someone could explain this concept to me. To start with I don’t understand, why the need to scale the weight update by a constant. I mean, all the weights of the updates are optimized in the fine tuning process.</p>
<p>I also wanted to understand why is A initialized randomly and B to zero. Would it make a difference if it would be the other way around (A zero, B random?). Also, what would go wrong if both would be set to zero?</p>
","transformer"
"123216","Do transformers output probabilities depend only on previous tokens?","2023-08-12 22:09:29","123221","0","529","<machine-learning><transformer>","<p>In a transformer, the target sequence shifted right is placed in the input of the transformer decoder, and a upper triangular mask is provided so that the attention layer will not depend &quot;tokens from the future&quot;. The output of the encoder stage is not relevant for this question and it is assumed to be fixed.</p>
<p>The output of the transformer is a tensor of shape <code>(sequence_lenght, dictionary_lenght)</code> that contains the probabilities of each possible token, for each element of the sequence.</p>
<p>Are the probabilities predicted for the Nth element of the output sequence always the same for any two inputs sequences that have the same tokens for token 0 to token N of the sequences?</p>
<p>More formally:
Consider two target sequences <code>S1 = [&lt;begin&gt;, x0, ..., xN]</code> and <code>S2 = [&lt;begin&gt;, X0, ..., xM]</code> where <code>N &lt; M</code>, that is: two sequences that contains the exact same tokens up to token with index <code>N</code>, and then the second contains some more tokens. Assume <code>decoder: Tensor(sequence_length) -&gt; Tensor(sequence_length, dictionary_length)</code> is a function that implements a decoder that has already been bound to a mask and to a encoder output.</p>
<p>Is it true that <code>decoder(S1)[X][Y] == decoder(S2)[X][Y] for all X in [0..N], Y in [0..dictionary_lenght]</code>?</p>
<p>My understanding is that this is the entire point of a transformer, when the loss is backpropagated the transformer will learn as if it was only aware of the tokens it generated up to that point, but this is beyond my ability to prove, and i have not seen it being explicitly said anywhere.</p>
","transformer"
"123079","Unsupervised fine tuning of Code LLMs","2023-08-04 06:03:16","","0","930","<nlp><transformer><finetuning>","<p>How to prepare code data to fine tune a code LLM in an unsupervised way or is it even possible?</p>
<p>For example:
Task: Code summarization with custom code base (with no summaries)
Let's assume that this code base is unique, and a pre-trained model is giving unsatisfactory results. Now to fine tune there are three options,</p>
<ol>
<li>Manually prepare summaries for a portion of the code and fine tune</li>
<li>Find a similar code base which has the labels (docstring) and fine tune</li>
<li>Mask some portions of the code randomly and give as input and output will be the masked portions</li>
</ol>
<p>Options 1 and 2 don't seem feasible for a production environment.</p>
<p>The reasoning behind option 3 is that with no availability labels, the model will learn the patterns in the code base and provide a better summarization with its pre-trained knowledge.</p>
<p>I tried the option 3 with [CodeT5+ fine tuning] (<a href=""https://github.com/salesforce/CodeT5/blob/main/CodeT5%2B/tune_codet5p_seq2seq.py"" rel=""nofollow noreferrer"">https://github.com/salesforce/CodeT5/blob/main/CodeT5%2B/tune_codet5p_seq2seq.py</a>). The format of input and output was as follows
Input:</p>
<pre><code>    def __init__(self, text, font):
        self._text = text
        self._font = font

    def get_text(self):
        |&lt;mask&gt;|

    def set_text(self, value):
        self._text = value```


Output:
```return self._text```
</code></pre>
","transformer"
"123062","Why cant we use normalise position encodings instead of the cos and sine encodings used in the Transformer paper?","2023-08-03 09:14:56","123065","2","1072","<word-embeddings><transformer><embeddings><attention-mechanism><tokenization>","<p>I'm working with Transformer models for sequence-to-sequence tasks and I'm trying to fully understand the use of positional encodings in these models.</p>
<p>In the original &quot;Attention is All You Need&quot; paper by Vaswani et al., positional encodings are implemented using a mix of sine and cosine functions of different frequencies. I understand that these sinusoidal functions provide a unique encoding for each position and that they allow the model to learn to attend to relative positions in a way that is translation invariant.</p>
<p>While going through this <a href=""https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3"" rel=""nofollow noreferrer"">blog</a>.</p>
<p>However, I'm wondering why we couldn't simply use normalized positional encodings instead. For example, we could divide the position of each word in the sequence by the maximum sequence length to get a unique positional encoding for each word that falls within a range from 0 to 1. We could handle sequences shorter than the maximum length with padding.</p>
<p>This approach seems more straightforward and it might make training faster or more stable due to the normalized range of the encodings.</p>
<p>Mathematically, for a given position p in a sequence, and a maximum sequence length L, the normalized positional encoding E would be:</p>
<p>E(p) = p / L</p>
<p>The encodings would be unique for each index position and we can easily map the relation positions since the relationship would be linear.</p>
<p>One of the main arguments against this is that we wont be able to handle different sequence lengths since 4/5 and 16/20 would have same ratio but while training we set a max length and the ratios would be the same since padding is incorporated if the sequence is small so the ratios would be the same in all instances and I think that argument doesn't hold.</p>
","transformer"
"122955","What Can Prevent Time-Series Prediction Model From Learning Trend?","2023-07-26 13:29:53","","0","25","<deep-learning><time-series><prediction><transformer>","<p>I am building an encoder-decoder prediction model based on this paper:</p>
<p><a href=""https://www.sciencedirect.com/science/article/pii/S0952197623001483"" rel=""nofollow noreferrer"">https://www.sciencedirect.com/science/article/pii/S0952197623001483</a></p>
<p>It is made of a transformer encoder and a 1D CNN Decoder. The model takes in a input window of length L, and predicts a window of length L shifted by <span class=""math-container"">$delta$</span> time steps.</p>
<p>For now, I am using the model on 1D time series. When I train the model, it seems to be doing quite well as long as there is no trend in the data.</p>
<p>But when there is trend, the model only does well on the training data, and performance is less good on the validation data, and poor on the test data.</p>
<p>See below an example. The first red line shows the end of the training data, and the second the end of the validation data.</p>
<p><a href=""https://i.sstatic.net/CPPVQ.png"" rel=""nofollow noreferrer"">Example</a></p>
<p><strong>So my question is, what are the model parameters or elements of the model structure that might prevent the model from learning beyond data seen in training?</strong></p>
<p>Additional information:</p>
<ul>
<li><p>I fit a sklearn MinMaxScaler on the train data and apply that the the train, validation and test sets. I wonder if seing data outside the [0-1] range after training might be the issue? I have tried using StandardScaler as well but that did not help.</p>
</li>
<li><p>I have tried changind the lookback window, does not help.</p>
</li>
<li><p>I have added dropouts in several parts of the model, improved performance, but did not help in learning trend / learning on data outside the training range.</p>
</li>
</ul>
<p>Thanks you very much for your help!</p>
","transformer"
"122865","What does it mean order of input sequence does not matter for transformer self-attention head?","2023-07-21 16:02:20","","3","793","<neural-network><nlp><predictive-modeling><transformer><attention-mechanism>","<p>The need for positional encoding in transformer models is justified by permutation invariance of self-attention heads, because, without it, transformer wouldn't have any mechanism to take into account the order of the words.</p>
<p>Suppose we trained a simple trigram transformer model without positional encoding to predict C as the next token, if the input is AB. Because the self-attention head output has T (time) dimension, we actually predicted BC, in other words, <code>head.forward('AB')='BC'</code>. We use only the last token C as the prediction of our model.</p>
<p>Now, because of the head permutation invariance, <code>head.forward('BA')='CB'</code>. Thus, the next token prediction becomes B (last token in output of 'forward`). So why is it said that transformer models without positional encoding are position agnostic. In the above example, we permuted input tokens and obtained a different prediction (B instead of C)</p>
<p>UPDATE: made a colab notebook illustrating the concept:
<a href=""https://colab.research.google.com/drive/1ItIQTCg3sVGRUIGbrh1pxTNlfLU5C7kq?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1ItIQTCg3sVGRUIGbrh1pxTNlfLU5C7kq?usp=sharing</a></p>
","transformer"
"122863","How quickly can a transformer self-heal if you wipe out one of its layers?","2023-07-21 12:10:04","","0","52","<transformer><language-model>","<p>Say we have a fully-trained <code>N</code>-layer transformer model (encoder-only, decoder-only, or encoder-decoder), with embedding dimension <code>D</code>, trained for <code>E</code> epochs on <code>S</code> training strings.</p>
<p>Then we take one of those layers, layer <code>L</code>, and reset it back to random weights (all of it, the self-attention weight, the FFN weights, even the layer norm parameters). Assume weights in all other layers are frozen.</p>
<p>What I'm wondering is how quickly it can re-learn. Does it need to see all <code>S x E</code> training examples all over again? Or will it be much quicker, perhaps just needing to see 1% or 10% of the original training data? Or maybe it will go the other way, and refuse to train because the frozen weights around it stop it finding a minima??</p>
<p>I'm also curious if where <code>L</code> is in the stack makes much difference, and if <code>N</code> and <code>D</code> are factors, or the result is about the same however big the transformer is.</p>
<p>(By &quot;re-learn&quot; I mean get back to approximately the same loss/perplexity on validation data that the model had before; I do not mean recreate the exact same weights.)</p>
<p>I'm looking for answers that point to existing papers that have done these experiments, or equally your personal experiments. Or even a &quot;no-one has ever tried this, we don't know&quot;, if you think you can say that with confidence.</p>
<p>Aside: I did a test last year of swapping in a different encoder (of different dimension), in an encoder-decoder model, and wiping out the cross-attention weights, but otherwise not touching the rest of the decoder. It acted as if the whole model had been reset to random weights, and seemed to need retraining from scratch, even though 90+% of it was still &quot;trained&quot;. But that was only a quick test. Before trying again I'd like to have a better idea of how much retraining time might be needed. (And it struck me that wiping out a layer rather than all cross-attention weights might be a cleaner experiment to learn from.)</p>
","transformer"
"122835","Exploding loss in unstable model","2023-07-19 18:25:38","","0","200","<neural-network><transformer>","<p>I am training a classifier (based on transformer encoders), on top of some complex data. My data is extremely imbalanced (although I do undersample the higher concentration class somewhat) and rather limited (generally few training and validation examples). I am generally satisfied with the performance of the model, however models trained with the sample hyperparameters can vary significantly, and more concerning to me is that sometimes loss will explode, while yielding similar results. I am using a focal loss function, and I am having trouble understanding why these values are reaching the magnitudes they do. For example, here are the accuracy and loss I sometimes recieve: <a href=""https://i.sstatic.net/jqnWR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jqnWR.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/DeI7v.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DeI7v.png"" alt=""enter image description here"" /></a></p>
<p>While other times train loss explodes anywhere from magnitudes of -10^4 to -10^15 and higher. While not as pronounced with validation loss, sometimes it stays in a reasonable range as in the first example, and sometimes it can reach values in the -1000s</p>
<p><a href=""https://i.sstatic.net/Uvw5v.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Uvw5v.png"" alt=""enter image description here"" /></a>
<a href=""https://i.sstatic.net/5MeYx.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/5MeYx.png"" alt=""enter image description here"" /></a></p>
<p>I thought I understood how exploding / vanishing gradients work, yet model performance seems to be improving significantly, even while this problem occurs. While the figures below show negligible performance gains after loss drops significantly, some results I receive can have high magnitude loss on epoch one. The data between runs <em>is</em> somewhat different because of the samples that are chosen in the undersampling process, but everything else is seeded similarly, and I would not expect that to impact the model results in this way. How can I explain these results, and create more stability in my model &amp; results? I have attempted changing learning rates and such with no significant success.</p>
","transformer"
"122726","Transformers for time series - what is the role of the encoder?","2023-07-13 07:10:00","122803","2","768","<time-series><transformer>","<p>I am trying to become more familiar with transformers by applying them to simple sequences like uni-variate time series. For now I am reading the papers.</p>
<p>One of the important parts of transformers seems to be that attention mechanism compares every point in the prior series to every other prior point. That makes sense. What makes less sense is why one needs a decoder, in general? For example, here is the architecture of the Informer model (<a href=""https://arxiv.org/abs/2012.07436"" rel=""nofollow noreferrer"">https://arxiv.org/abs/2012.07436</a>)</p>
<p><a href=""https://i.sstatic.net/VjKm1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/VjKm1.png"" alt=""enter image description here"" /></a></p>
<p>Is the encoder-decoder needed here because the model is trained to be general enough to work for many different time-series? Thus input into the encoder is needed to <em>bias</em> the model to work for a specific type of time series during the application?</p>
<p>What if I wanted to train a model of only one type of time series? Could I not remove the encoder part and aim to get all the knowledge of the time series to be implanted into the decoder weights via training?</p>
<p>Thanks</p>
","transformer"
"122543","Time2Vec as positional encoding for Timeseries Transformer","2023-07-04 08:06:06","","1","676","<time-series><transformer>","<p>Although the transformer architecture was originally designed for NLP, there exists several articles and papers that attempt to apply the same architecture for numerical timeseries classification. These are some notable examples:</p>
<p><a href=""https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3"" rel=""nofollow noreferrer"">https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3</a></p>
<p><a href=""https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e"" rel=""nofollow noreferrer"">https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e</a></p>
<p><a href=""https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6"" rel=""nofollow noreferrer"">https://towardsdatascience.com/stock-predictions-with-state-of-the-art-transformer-and-time-embeddings-3a4485237de6</a></p>
<p>Based on the above, it seems that the usage of time2vec for positional encoding is prevalent. However, my concern is that all these articles use timeseries values as inputs to time2vec (e.g., prices or sales) rather than the actual temporal information (i.e., a time index t=0,1,2,3..). This seems problematic as:</p>
<ul>
<li><p>This is not a positional encoding, but an enrichment of feature representation</p>
</li>
<li><p>Even the word2vec paper (If I understood it correctly) mentions that it is designed to work on a time related feature.</p>
</li>
</ul>
<p>Several persons have asked similar questions in other sites, for example this is taken from the first article:</p>
<blockquote>
<p>Why is the feature being encoded in time2vec and not the actual time
index? How does encoding the feature provide positional encoding?</p>
</blockquote>
<p>Yet, I was not able to find any explanations. So, can someone please resolve this? Is this design approach a widespread misinterpretation of positional encoding?</p>
<p>Note: Even if time2vec is applied on a time index, it should probably be a global time index created before batching/windowing the data, because the local time index within the batch is arbitrary.</p>
","transformer"
"122409","query, key and value interpretation in transformers ( encoder - decoder framework )","2023-06-26 16:31:03","","0","365","<transformer><attention-mechanism>","<p>I am implementing a custom algo inspired by NMT architecture BUT in the decoder, if Query = target language then the &quot;value&quot; should also be the same thing right ? Only the &quot;key&quot; should be the encoder output ( encoded source language ). After much heartburn i have made peace with the fact that a &quot;query&quot; is what you are trying to find out and the &quot;key&quot; is some sort of index FOR the &quot;values&quot;, from which you choose your answer (based on the best score generated by the attention algo ). So if i want to convert French to English, and my encoder is encoding French, then my query is got to be in English and so the values must be in English right ? why does the TF code ( NMT tutorial ) take the French encoding as the KEY and the VALUE ??</p>
<p>OR is the interpretation that since the query ( masked input , English ) and the key ( encoded French ) are first &quot;dot producted&quot; ( sorry ) together, the &quot;values&quot; are in fact being learnt during the training based on the loss calculated by difference between input context ( English ), so far and the next predicted word ? and during inference the &quot;key-values&quot; are now in the form of a French-English dictionary ( a very smart dict at that which gives nearest word, based on context ) ?</p>
","transformer"
"122349","Effect of hyperparameters: the hidden size, layers, MLP size number of heads on Transformer","2023-06-23 20:23:07","122407","0","386","<deep-learning><tensorflow><pytorch><transformer>","<p>Is there any paper that explains the effect of hyperparameters:</p>
<ul>
<li>hidden size</li>
<li>Number of layers</li>
<li>MLP size</li>
<li>number of heads</li>
</ul>
<p>on Transformer performance. I found some explanation on the web but I need papers.</p>
","transformer"
"122281","Query on Attention Architecture","2023-06-21 07:58:58","122283","0","37","<deep-learning><nlp><tensorflow><transformer><attention-mechanism>","<p>As we most know that, Attention is focuses on specific parts of the input sequence those are most relevant in generating output sequence.</p>
<p>Ex: The driver could not drive the car fast because it had a problem.</p>
<ol>
<li><p>how attention finds the specific parts(here it is 'it') in input and how it will assign score for the token?</p>
</li>
<li><p>is attention context- based model?</p>
</li>
<li><p>how to obtain attention maps (query, key, value)?</p>
</li>
<li><p>On what basis attention assigns higher weights to input tokens?</p>
</li>
</ol>
","transformer"
"122236","what is the difference between word2vec and doc2vec","2023-06-19 01:58:32","","0","182","<machine-learning><deep-learning><nlp><transformer><word2vec>","<p>As we know Word2Vec is a non-contextual embedding, here it maps the words in global vocabulary and returns their corresponding vectors (at word level).</p>
<p>In case of Doc2Vec, hope this is also non-contextual embedding and it return the vectors at document level, that means internally document is a union of paragraphs, sentences (i.e words).</p>
<p>what is the implementation style of Doc2Vec?</p>
<p>is any difference between Doc2vec, sent2vec,word2vec ? (because for all these word/subword is basic)?</p>
<p>please share the more insights about them?</p>
","transformer"
"122220","what is the difference between NSP and text prediction","2023-06-17 17:02:37","","0","278","<machine-learning><deep-learning><nlp><transformer><bert>","<p>In BERT, NSP (Next Sentence Prediction) is for predicting next sentence based on context and Text prediction task is also for predicting next word or phrases.</p>
<p>So, both are for predicting next sentence or word/ phrase only and both are BERT NLP tasks, then why these two?</p>
","transformer"
"122212","What are the Bidirectional methodoly based pre-trained models in NLP","2023-06-17 10:57:57","","0","10","<machine-learning><deep-learning><nlp><data-science-model><transformer>","<p>BERT model is useful to solve many NLP tasks (11+) like Sentiment analysis, Q&amp;A, summarization and NER.</p>
<p>what are the other predefined model similar to BERT?
what is best way to find them in NLP?</p>
","transformer"
"122211","Should a Learning Rate Scheduler adjust the learning rate by optimization step (batch) or by epoch?","2023-06-17 08:54:56","","4","618","<deep-learning><tensorflow><pytorch><transformer><learning-rate>","<p>In <a href=""https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate"" rel=""nofollow noreferrer"">PyTorch doc</a>, it suggests</p>
<blockquote>
<p><code>torch.optim.lr_scheduler</code> provides several methods to adjust the learning rate based on the number of <strong>epochs</strong>.</p>
</blockquote>
<p>However, from other sources it looks like the learning rate should be adjusted in every <strong>optimization step (batch)</strong>:</p>
<ul>
<li><a href=""https://kikaben.com/transformers-training-details/"" rel=""nofollow noreferrer"">https://kikaben.com/transformers-training-details/</a></li>
<li><a href=""https://datascience.stackexchange.com/questions/103022/warmup-steps-in-deep-learning"">Warmup steps in deep learning</a></li>
<li><a href=""https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html"" rel=""nofollow noreferrer"">https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html</a></li>
<li><a href=""https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd"" rel=""nofollow noreferrer"">https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd</a></li>
</ul>
<p>So, <strong>Should the learning rate in a Learning Rate Scheduler be adjusted at each optimization step (batch) or at each epoch?</strong></p>
<p>Is there a definitive answer to this, or it depends on the model?
For transformer models, it looks like the learning rate is adjusted at every step (training batch). In the following example, there are a few thousands of steps so I think it cannot be epochs, is that right?
<a href=""https://nn.labml.ai/optimizers/noam.html"" rel=""nofollow noreferrer"">https://nn.labml.ai/optimizers/noam.html</a>
<a href=""https://i.sstatic.net/LFJnE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/LFJnE.png"" alt=""enter image description here"" /></a></p>
","transformer"
"122172","Why shouldn't the attention matrices $W^Q$, $W^K$, $W^V$ be the same?","2023-06-14 22:02:32","","0","41","<transformer><attention-mechanism><explainable-ai><semantic-similarity>","<p>My question is why the equally shaped attention head matrices <span class=""math-container"">$W^Q$</span>, <span class=""math-container"">$W^K$</span>, <span class=""math-container"">$W^V$</span> should not be the same <span class=""math-container"">$W = W^Q =W^K= W^V$</span>. In my understanding of transformer-based language models  <strong>one</strong> attention head is responsible for <strong>one</strong> syntactic or semantic relation between any two words in the context. One might think that such a relation is represented by <strong>one</strong> matrix <span class=""math-container"">$W$</span> that projects the full word embeddings <span class=""math-container"">$x_i$</span> from their full semantic space to a semantic subspace. Here we could - in principle - calculate scores <span class=""math-container"">$\sigma_{ij}$</span> as &quot;similiarities&quot; between two projected words <span class=""math-container"">$Wx_i$</span> and <span class=""math-container"">$Wx_j$</span> and then calculate the weighted sum of the projected tokens <span class=""math-container"">$Wx_k$</span>.</p>
<p>I wonder why this would not work, and why we need three different matrices.</p>
<p>The other way around: What does it mean to calculate the score as the dot-product of two vectors from two different semantic subspaces? Is this still some kind of similiarity (which lies at the heart of word embeddings)? And doesn't it sound like comparing apples and pears?</p>
","transformer"
"122164","LMM Fine Tuning - Supervised Fine Tuning Trainer (SFTTrainer) vs transformers Trainer","2023-06-14 15:54:10","123467","3","2352","<deep-learning><transformer><language-model><huggingface><finetuning>","<p>When should one opt for the Supervised Fine Tuning Trainer (SFTTrainer) instead of the regular Transformers Trainer when it comes to instruction fine-tuning for Language Models (LLMs)? From what I gather, the regular Transformers Trainer typically refers to unsupervised fine-tuning, often utilized for tasks such as Input-Output schema formatting after conducting supervised fine-tuning. There seem to be various examples of fine-tuning tasks with similar characteristics, but with some employing the SFTTrainer and others using the regular Trainer. Which factors should be considered in choosing between the two approaches?</p>
<p>I looking for Fine Tuning a LLM for generating json to json transformation (matching texts in json) using huggingface and trl libraries.</p>
","transformer"
"122142","Is sequence length and hidden size is the same in Transformer","2023-06-13 22:54:08","122144","1","2615","<lstm><pytorch><transformer>","<p>I'm confused about sequence length and hidden size if they are the same in Transformer.
I think they are different but not sure.</p>
<pre><code>class Embeddings(nn.Module):
    &quot;&quot;&quot;Construct the embeddings from patch, position embeddings.
    &quot;&quot;&quot;
    def __init__(self, config, img_size, in_channels=3):
        super(Embeddings, self).__init__()
        self.hybrid = None
        self.config = config
        img_size = _pair(img_size)
        b_size=16 
        if config.patches.get(&quot;grid&quot;) is not None:   # ResNet
            grid_size = config.patches[&quot;grid&quot;]
            patch_size = (img_size[0] // b_size // grid_size[0], img_size[1] // b_size // grid_size[1])
            patch_size_real = (patch_size[0] * b_size, patch_size[1] * b_size)
            n_patches = (img_size[0] // patch_size_real[0]) * (img_size[1] // patch_size_real[1])  
            self.hybrid = True
        else:
            patch_size = _pair(config.patches[&quot;size&quot;])
            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])
            self.hybrid = False

        if self.hybrid:
            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers, width_factor=config.resnet.width_factor)
            in_channels = self.hybrid_model.width * 16
        self.patch_embeddings = Conv2d(in_channels=in_channels,
                                       out_channels=config.hidden_size,
                                       kernel_size=patch_size,
                                       stride=patch_size)
        print(&quot;config.hidden_size&quot;,config.hidden_size)
        print(&quot;n_patches&quot;,n_patches)
        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches, config.hidden_size))

        self.dropout = Dropout(config.transformer[&quot;dropout_rate&quot;])


    def forward(self, x):
        
        if self.hybrid:
            x, features = self.hybrid_model(x)
        else:
            features = None
           
        x = self.patch_embeddings(x)  # (B, hidden. n_patches^(1/2), n_patches^(1/2))
        x = x.flatten(2)
        x = x.transpose(-1, -2)  # (B, n_patches, hidden)

        embeddings = x + self.position_embeddings
        embeddings = self.dropout(embeddings)
        return embeddings, features
</code></pre>
","transformer"
"122103","Further Training a pre-trained LLM","2023-06-12 09:57:03","","7","4579","<transformer><transfer-learning><language-model><pretraining>","<p>My goal is to use the general knowledge and language understanding of a pre-trained LLM and to continue training on a smaller domain specific corpus to improve the model's knowledge on the domain. What is the best practice approach here without running into issues (e.g. catastrophic forgetting)? Here are some points I consider, but not completely sure about them:</p>
<ul>
<li>use last checkpoint of pre-trained LLM and continue training on custom corpus</li>
<li>training policy and procedure is the same as used for pre-training (MLM etc.)</li>
<li>use a very small learning rate</li>
<li>is it possible to load the model in int8 (bitsandbytes) and continue training without breaking it?</li>
</ul>
<p>Does this approach make sense? Has anyone done this before and has some insights?</p>
<p>Any hints are highly appreciated!</p>
","transformer"
"122077","Confused about output shape of the transformer","2023-06-11 13:01:08","122078","1","1511","<pytorch><transformer>","<p>I have followed this tutorial <a href=""https://www.youtube.com/watch?v=U0s0f995w14"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=U0s0f995w14</a> to create a minified version of a transformer architecture but I am confused about the final shape of the output.</p>
<p>Heres the code:
<a href=""https://github.com/aladdinpersson/Machine-Learning-Collection/blob/558557c7989f0b10fee6e8d8f953d7269ae43d4f/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py#L2"" rel=""nofollow noreferrer"">https://github.com/aladdinpersson/Machine-Learning-Collection/blob/558557c7989f0b10fee6e8d8f953d7269ae43d4f/ML/Pytorch/more_advanced/transformer_from_scratch/transformer_from_scratch.py#L2</a></p>
<p>on the final lines (slightly modified):</p>
<pre><code>print(x.shape)
print(trg[:, :-1].shape)
out = model(x, trg[:, :-1])
print(out.shape)
</code></pre>
<p>The output shapes don't seem to make sense</p>
<pre><code>torch.Size([2, 9]) #input sentence (num_examples, num_tokens)
torch.Size([2, 7]) #generated sentence so far (num_examples, num_tokens_generated)
torch.Size([2, 7, 10]) # probabilities for next token (num_examples, ???, size_vocab)
</code></pre>
<p>The transformer is supposed to predict the next token across 2 training examples (which is why theres a two for the number of examples and a 10 for the size of the vocab), by generating probabilites for each token in the vocab. But I can't make sense of why theres a 7 there. The only explanation I can come up with is that it outputs all predictions simulatenously, but that would require feeding the outputs iteratively through the transformer, but that never happens (see lines 267-270).</p>
<p>So is there a mistake or am I not understanding something correctly? What is that output shape supposed to represent?</p>
<p>Can somebody make sense of this?</p>
","transformer"
"122004","Why is the target output in GPT (Andrej Karpathy's course) contains copies of tokens from the input?","2023-06-07 14:58:58","122005","1","102","<neural-network><pytorch><transformer><gpt>","<p>The question is based on Andrej Karpathy lecture on training a toy GPT model on Tiny Shakespeare dataset (<a href=""https://www.youtube.com/watch?v=kCc8FmEb1nY&amp;list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&amp;index=10&amp;t=5003s&amp;ab_channel=AndrejKarpathy"" rel=""nofollow noreferrer"">youtube link</a>). In this model, tokens are single characters with a dictionary of around 60 elements. He creates training and validation datasets as follows</p>
<pre><code>def get_batch(...):
    ...
    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i+block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size+1] for i in ix])
    ...
    return x, y
</code></pre>
<p>Why does he make the target output <code>y</code> a sequence of bytes <code>data[i+1:i+block_size+1]</code> instead of just a single byte <code>data[i+block_size+1]</code>? We are trying to predict the next <strong>single</strong> byte (token) after all.</p>
<p>It looks to me like gpt is trained to predict N (N is the <code>block_size</code>) characters of <code>Y</code> from N characters of <code>X</code>, but the first (N-1) characters in <code>Y</code> are just a copy of (N-1) characters in <code>X</code>. Surely the NN can be trained to do that, but isn't it a waste of weights and GPU cycles on essentially just copying (N-1) characters?</p>
<p>I ran the script in debugger to confirm that's what indeed happens in training. In the code, after the first breakpoint</p>
<pre><code>class GPTLanguageModel(nn.Module):

    ...
    def forward(self, idx, targets=None):
&gt;&gt;&gt;     B, T = idx.shape
</code></pre>
<p><code>idx</code> is something like <code>tensor([[ 1, 16, 24, 40, 26, 32, 43, 41],...])</code>, while <code>target</code> is <code>tensor([[16, 24, 40, 26, 32, 43, 41,  1],...])</code></p>
","transformer"
"121925","Can I add a new output class to a decoder and train only the final layer?","2023-06-02 14:25:31","","0","94","<classification><transformer><transfer-learning><finetuning>","<p>I am wondering how to approach a project, where I would like to increase the number of output classes of an already trained network. I have very good reason to believe that the model has already learnt the relevant information to be able to predict this new class, that is why I would aim only for fine-tuning (also, I have a lot less data for this class than for the rest and I do not have the hardware to train from scratch).</p>
<p>The model that I want to use is a transformer, where the decoder's final layers are 2 fully connected layers and a layernorm. To my understanding, new classes to a network can be added by freezing the model except for the final layer(s), increase the output dimension and fine-tune only this part of the network.</p>
<p>Is this a reasonable approach? If yes, do you usually take the weights for these layers and just increase the size of the weight matrix with some random weights (or just try both, and see which one gives better results)?</p>
","transformer"
"121866","Fine-tuning a pre-trained LLM for question-answering","2023-05-31 12:56:54","","2","1388","<transformer><language-model><huggingface><text-generation><finetuning>","<h3>Objective</h3>
<p>My goal is to fine-tune a pre-trained LLM on a dataset about Manchester United's (MU's) 2021/22 season (they had a poor season). I want to be able to prompt the fine-tuned model with questions such as &quot;How can MU improve?&quot;, or &quot;What are MU's biggest weaknesses?&quot;. The ideal responses would be insightful/logical and +100 words</p>
<h3>Data</h3>
<ul>
<li>I will simply use text from the relevant wiki page as my data: <a href=""https://en.wikipedia.org/wiki/2021%E2%80%9322_Manchester_United_F.C._season"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/2021%E2%80%9322_Manchester_United_F.C._season</a></li>
<li>How should I structure my data? Should it be a list dictionaries where the keys are the questions and the values are the answers (i.e. a list of question-answer pairs), or a long string containing all the text data (for context), or a combination of both?</li>
</ul>
<h3>Notes</h3>
<ul>
<li>I have mainly been experimenting with variations of Google's T5 (e.g.: <a href=""https://huggingface.co/t5-base"" rel=""nofollow noreferrer"">https://huggingface.co/t5-base</a>) which I have imported from the Hugging Face Transformers library</li>
<li>So far I have only fine-tuned the model on a list of 30 dictionaries (question-answer pairs), e.g.: {&quot;question&quot;: &quot;How could Manchester United improve their consistency in the Premier League next season?&quot;, &quot;answer&quot;: &quot; To improve consistency, Manchester United could focus on strengthening their squad depth to cope with injuries and fatigue throughout the season. Tactical adjustments could also be explored to deal with teams of different strengths and styles.&quot;}</li>
<li>Use of this small dataset (list of 30 dictionaries) has given poor results</li>
</ul>
<h3>Further Questions and Notes</h3>
<ul>
<li>Other than increasing the size of my dataset, is my approach sound?</li>
<li>What would you recommend as a minimum number of dictionaries to train/fine-tune the model on?</li>
<li>I am also aware that I can tune the hyperparameters to improve performance, but for now I am more concerned about my general approach being logical</li>
</ul>
","transformer"
"121818","About the last decoder layer in transformer architecture","2023-05-28 16:47:14","","1","179","<deep-learning><neural-network><nlp><transformer><linear-algebra>","<p>So, in the decoder layer of transfomer, suppose I have predicted 3 words till now, including the start token then the last decoder layer will produce 3 vectors of size d-model, and only the last vector will pass through embedding layer to form logits. Am I getting this right? Because its nowhere mentioned in the original paper and I'm having a hard time understanding it. What about the information that gets lost by discarding the two tokens before the last token. We could try to linearly project all the vectors into a single d-dimension vector but then the size of vectors would keep on increasing after we predict new word everytime and we'd need a new projection matrix everytime. This detail seems implicit and isnt mentioned anywhere. Can someone provide me what is actually done and the reason behind this or is this a random heuristic that seems to work (i.e. just take the final hidden state produced by the decoder)</p>
","transformer"
"121805","Sentiment Analysis on the first 100 words of a very large essay of 500/700 words","2023-05-27 21:44:24","","0","17","<deep-learning><transformer><sentiment-analysis>","<p>Are there any potential issues on performing sentiment analysis using the first 100 words of a very large essay that is of 500 to 700 words. I am having to do this because since most transformer models have a upper limit of 500 words.</p>
","transformer"
"121658","Library for Abstractive Summarization","2023-05-20 07:18:43","","0","87","<transformer><automatic-summarization>","<p>Is there a Python library that supports <strong>abstractive</strong> summarization? (Excluding cloud-based models like GPT or ChatGPT). We can perform <strong>extractive</strong> summarization easily using the code below:</p>
<pre><code>!pip3 install transformers==4.11.3
!pip3 install summarizer
!pip3 install bert-extractive-summarizer
!pip3 install keybert
!pip3 install keybert[flair]
!pip3 install keybert[gensim]
!pip3 install keybert[spacy]
!pip3 install keybert[use]
!pip3 install spacy
!pip3 install &quot;gensim==3.8.3&quot;

from summarizer import Summarizer,TransformerSummarizer
GPT2_model = TransformerSummarizer(transformer_type=&quot;GPT2&quot;,transformer_model_key=&quot;gpt2-medium&quot;)
GPT2_summary = ''.join(GPT2_model(body, ratio=0.1))
print(GPT2_summary)
</code></pre>
","transformer"
"121639","Load an LLM in multiple GPUs","2023-05-19 12:15:42","","2","7219","<python><nlp><pytorch><transformer><gpu>","<p>I am doing a POC on LLM text generation. I have one AWS p3.8x instance which has 4 GPUs each of 16 GB size. I am pretty new to use LLM and GPU. When I am trying load one LLM pertained model (WizardLM) in GPU, it is saying 16 GB is not sufficient for this. So my question is how can I load the model using all 64 GB?</p>
","transformer"
"121526","Transformers doubt","2023-05-14 11:29:25","","0","75","<deep-learning><neural-network><nlp><transformer><attention-mechanism>","<p><a href=""https://i.sstatic.net/jGTmJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jGTmJ.png"" alt=""Transformers encoder layer"" /></a></p>
<p>Basically here the <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> are passed through a linear layer to obtain the actual <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> for self attention mechanism and then we concatenate all of it.</p>
<p>My doubt is, I thought the <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> were obtained through the input embedding <span class=""math-container"">$X$</span>.</p>
<p><span class=""math-container"">$$Q=XW_q$$</span>
<span class=""math-container"">$$K=XW_k$$</span>
<span class=""math-container"">$$V=XW_v$$</span></p>
<p>How come we are using the <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> and linearly projecting them to again get back <span class=""math-container"">$Q$</span>,<span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span>.</p>
<p>Sorry if my doubt is stupid!</p>
","transformer"
"121412","Easy question on autoregressive LLM","2023-05-09 00:06:56","121414","0","73","<nlp><transformer><language-model>","<p>For LLM decoder, how exactly is the K, Q, V for each decoding step?</p>
<p>Say my input prompt is &quot;today is a&quot; (good day).</p>
<p>At t= 0 (generation step 0):
K, Q, V are the projections of the sequence (&quot;today is a&quot;)
Then say the next token generated is &quot;good&quot;</p>
<p><strong>At t= 1(generation step 1):
Which one is true:</strong></p>
<ul>
<li>K, Q, V are the projections of the sequence (&quot;today is a good&quot;)</li>
</ul>
<p>OR</p>
<ul>
<li>K, Q, are the projections of the sequence (&quot;today is a&quot;) , V is the projection of sequence (&quot;good&quot;)?</li>
</ul>
","transformer"
"121364","computer vision transformers: ViT does not have a decoder?","2023-05-06 04:21:05","121370","1","2585","<computer-vision><transformer><attention-mechanism>","<p><a href=""https://i.sstatic.net/iku6X.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/iku6X.png"" alt=""videopic"" /></a></p>
<p>from <a href=""https://www.youtube.com/watch?v=TrdevFK_am4"" rel=""nofollow noreferrer"">https://www.youtube.com/watch?v=TrdevFK_am4</a> that explains the paper titled, &quot;AN IMAGE IS WORTH 16X16 WORDS:
TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE&quot;</p>
<p>compare that to the architecture shown here <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">https://jalammar.github.io/illustrated-transformer/</a></p>
<p>So the ViT has a simpler architecture?  It seems like the output of the encoder is the input to the MLP for the classification tasks.</p>
<p>Also I was referred to this repo <a href=""https://github.com/lucidrains/vit-pytorch"" rel=""nofollow noreferrer"">https://github.com/lucidrains/vit-pytorch</a> for learning purposes.</p>
<p>Are there any other ones I should know about?</p>
<p>I took a computational photography class at GaTech OMSCS (my specialization says robotics and computational perception) but that was in 2019 so I need to do some catching up, not to mention computer vision is different from photography.</p>
<p>Please feel free to link to additional resources which I should be going through.</p>
","transformer"
"121334","How is the classification step performed in Swin Transformer?","2023-05-04 15:33:47","121376","1","23","<computer-vision><transformer>","<p>I've read the original paper of the <a href=""https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf"" rel=""nofollow noreferrer"">Swin Transformer</a>, and understand it quite well apart from the last step: the classification. In the paper, this is completely brushed over, and none of the blog posts talking about this architecture adress it as well.</p>
<p>In practice, how is this done? The transformer outputs a series of vectors (an array), so we can't just apply an FC layer to obtain a vector of probabilities. Are the different output vectors concatenated together (flattening) and fed to an FC, or are they fed to a decoder as in the standard transformer architecture?</p>
","transformer"
"121322","detect/generate possible tokens for the dataset (name,type/category,signatures)","2023-05-04 05:56:11","","0","67","<machine-learning><transformer><bert>","<p>I have a dataset in the following format:</p>
<pre><code>name, type, signature
Eg1 : A, 2, abc123
Eg2 : A, 2, ab3
Eg3 : A, 2, addc1
</code></pre>
<p>If we need to train the following dataset using roberta or any other model how can we do it? Or is there any other way to train this model to detect possible signatures for the name?</p>
","transformer"
"121305","Why do varied delimiters on text inputs help training stability?","2023-05-03 12:03:39","","0","27","<nlp><word-embeddings><transformer>","<p>In the preprint paper <a href=""https://arxiv.org/abs/2201.10005"" rel=""nofollow noreferrer"">Text and code embeddings by contrastive pre-training</a>, the authors describe a Transformer encoder which</p>
<blockquote>
<p>maps the input, x and y, to embeddings, vx and vy respectively and the similarity between two inputs is quantified by the cosine similarity between their embeddings, vx and vy</p>
</blockquote>
<p>And they state:</p>
<blockquote>
<p>We found that using different delimiters leads to more stable training. For x, we use ‘[’ as [SOS]x and ‘]’ as [EOS]x, while we use ‘{’ and ‘}’ as [SOS]y and [EOS]y respectively for y</p>
</blockquote>
<p>Is there an intuitive explanation for why using different delimiters is important for training stability?</p>
","transformer"
"121235","In the attention mechanism, why don't we normalize after multiplying values?","2023-04-29 22:58:54","","0","192","<deep-learning><neural-network><nlp><transformer><attention-mechanism>","<p>As this <a href=""https://ai.stackexchange.com/q/21237/23811"">question</a> says:</p>
<blockquote>
<p>In scaled dot product attention, we scale our outputs by dividing the
dot product by the square root of the dimensionality of the matrix:</p>
<p><a href=""https://i.sstatic.net/wLI4m.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wLI4m.png"" alt=""enter image description here"" /></a></p>
<p>The reason why is stated that this constrains the distribution of the weights of the output to have a standard deviation of 1.</p>
</blockquote>
<p>My question is why don't we do the same after multiplying to <span class=""math-container"">$V$</span>(values) for the same reason?</p>
","transformer"
"121213","Creating LLM chatbot using llama-index + langchain","2023-04-28 12:30:25","","1","747","<machine-learning><python><transformer><gpt><huggingface>","<p>As the title suggests: I'm trying to build a chatbot which his goal should be sort of like &quot;chatgpt&quot;.</p>
<p>The chatbot will be installed on Slack workspace and I'm struggling with <strong>which scope</strong> I should create the documents on for building the index.</p>
<p>At the moment the document scope is &quot;<strong>per channel</strong>&quot; (i.e. <strong>every channel is a whole document</strong>), but I'm not sure it's the right approach (maybe it should be far <strong>smaller</strong> - does it make any difference at all?)</p>
<p>I'm also using huggingface transformers library, but since I'm newbie to this whole new evolving technology, I'm not sure what <strong>type of model should I use</strong>:</p>
<ol>
<li>text2text</li>
<li>text-generation</li>
<li>summarization</li>
<li>quiestion-answering</li>
</ol>
<p>I want the bot to address all (maybe I'm a bit naive).</p>
<p>... so overall, 2 questions:</p>
<ol>
<li>What should the Document scope be?</li>
<li>Which type of model should I look at? Any specific recommended one?</li>
</ol>
","transformer"
"121121","Bert model for document sentiment classification","2023-04-24 16:43:35","121122","0","68","<deep-learning><nlp><transformer><bert><sentiment-analysis>","<p>I am trying to fine-tune a Bert model for sentiment analysis. Instead of one sentence, my inputs are documents (including several sentences) and I am not removing dots. I was wondering if is it okay to use just the embedding of the first token in such cases. If not, what should I do?</p>
","transformer"
"121015","What is purpose of stacking N=6 blocks of encoder and decoder in transformer?","2023-04-18 20:15:40","","2","1531","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.</p>
<p><a href=""https://i.sstatic.net/7p5lu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7p5lu.png"" alt=""enter image description here"" /></a></p>
<p>What is purpose of stacking <span class=""math-container"">$N=6$</span> blocks of encoder and decoder? Does higher blocks represent longer phrases and learns what longer phrases attend to? While bottommost block represent single word and its attention; something like how first layer of CNN represent pixel and deeper layers represent edges and further deeper layers represents shapes (like nose, hand etc.)?</p>
","transformer"
"121014","What does it exactly mean by ""different representation subspaces"" in transformer?","2023-04-18 20:14:34","","1","146","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.</p>
<p>The paper says:</p>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p>
</blockquote>
<p>What does it exactly mean by &quot;different representation subspaces&quot;. Can you give intuitive example in terms of natural language conversation example. For example, in sentence &quot;Jane went to Africa during summer&quot;, query matrix <span class=""math-container"">$Q$</span> correspoding to word &quot;Africa&quot; can comprise of different queries &quot;Who went to Africa?&quot;, &quot;When went to Africa?&quot;. What are &quot;different representation subspaces&quot; here? Or with any other example of your choice?</p>
","transformer"
"121013","How K and V are extracted from encoder output in transformer?","2023-04-18 20:12:33","","1","290","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.  The paper shows following transformer architecture:</p>
<p><a href=""https://i.sstatic.net/dtJ7Y.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dtJ7Y.png"" alt=""enter image description here"" /></a></p>
<p>How <span class=""math-container"">$K$</span> and <span class=""math-container"">$V$</span> is extracted from <span class=""math-container"">$512$</span> dimensional encoder output (which is then fed to second multi head attention in decoder)?</p>
","transformer"
"121012","Understanding dimensions of vectors at various places in transformer architecture","2023-04-18 20:08:22","","1","809","<machine-learning><nlp><transformer><language-model>","<p>I was trying to understand transformer architecture from &quot;Attention is all you need&quot; paper.  It says following regarding dimensions of different vectors:</p>
<blockquote>
<ul>
<li>The input consists of queries and keys of dimension <span class=""math-container"">$d_k$</span>, and values of dimension <span class=""math-container"">$d_v$</span>.</li>
<li><span class=""math-container"">$MultiHead(Q, K, V ) = Concat(head_1, ..., head_h) W^O$</span> where <span class=""math-container"">$head_i = Attention(QW_i^Q,KW^K_i,VW^V_i)$</span><br />
where <span class=""math-container"">$W_i^Q\in\mathbb{R}^{d_{model}\times d_k}$</span>, <span class=""math-container"">$W_i^K\in\mathbb{R}^{d_{model}\times d_k}$</span>, <span class=""math-container"">$W_i^V\in\mathbb{R}^{d_{model}\times d_v}$</span>, <span class=""math-container"">$W^O\in\mathbb{R}^{hd_{v}\times d_{model}}$</span></li>
<li><span class=""math-container"">$h=8$</span> parallel attention layers or heads</li>
<li><span class=""math-container"">$d_k=d_v=d_{model}/h=64$</span></li>
</ul>
</blockquote>
<p>From these I figured out dimensions of vectors at different position in the transformers model as follows (in red colored text):</p>
<p><a href=""https://i.sstatic.net/3sAvB.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3sAvB.png"" alt=""enter image description here"" /></a></p>
<p>I have following doubts:</p>
<p>Is dimension of <span class=""math-container"">$K$</span> (vector of multiple/all keys that current word-query needs to attend to)  <span class=""math-container"">$=d_k$</span>? Or <span class=""math-container"">$d_k$</span> is just for single key? If for single key, then what is the dimension for <span class=""math-container"">$K$</span>? Same is the doubt with <span class=""math-container"">$Q$</span> and <span class=""math-container"">$d_q$</span>. I feel <span class=""math-container"">$Q$</span> is set of all queries that can apply to single word. <span class=""math-container"">$K$</span> is the set of all keys that single word can attend to. If that is the case, then dimensions of <span class=""math-container"">$Q$</span> must be <span class=""math-container"">$d_q\times\text{number of queries to consider}$</span> and <span class=""math-container"">$K$</span> must be <span class=""math-container"">$d_k\times\text{number of keys to attend for each word-query}$</span> But then what is this number of queries and keys?</p>
","transformer"
"121004","Fine-tuned MLM based RoBERTa not improving performance","2023-04-18 12:42:45","","1","1239","<transformer><bert><attention-mechanism><language-model><huggingface>","<p>We have lots of domain-specific data (200M+ data points, each document having ~100 to ~500 words) and we wanted to have a domain-specific LM.</p>
<p>We took some sample data points (2M+) &amp; fine-tuned RoBERTa-base (using HF-Transformer) using the Mask Language Modelling (MLM) task.</p>
<p>So far,</p>
<ol>
<li>we did 4-5 epochs (512 sequence length, batch-size=48)</li>
<li>used cosine learning rate scheduler (2-3 cycles/epochs)</li>
<li>We used dynamin masking (masked 15% tokens)</li>
</ol>
<p>Since the RoBERTa model is finetuned on domain-specific data, we do expect this model to perform better than the pre-trained-RoBERTa which is trained on general texts (wiki data, books, etc)</p>
<p>We did perform some tasks like Named Entity Recognition (NER), Text Classification, and Embedding generation to perform cosine similarity tasks. We did this on both finetuned domain-specific RoBERTa and pre-trained-RoBERTa.</p>
<p>Surprisingly, the results are the same (very small difference) for both models. We did try Spacy models too, but the results are the same.</p>
<p>Perplexity scores indicate that finetuned MLM-based RoBERTa has a minimal loss.</p>
<ol>
<li>Can anyone please help us understand why MLM based model is NOT performing better?</li>
<li>Should we go for more data OR more epochs OR both, to see some effect?</li>
<li>are we doing anything wrong here? Let me know if any required details are missing. I will update</li>
</ol>
<p>any suggestions OR any valuable links addressing these concerns would be really helpful</p>
<p>Huggingface discussion page: <a href=""https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913</a></p>
","transformer"
"120997","Which model to use that can distinguish between names with the same words?","2023-04-18 09:42:04","","0","163","<machine-learning><deep-learning><nlp><transformer><semantic-similarity>","<p>For my task, I need a model that can distinguish between job titles that contain the same words. BERT model &quot;msmarco-MiniLM-L-12-v3&quot; shows high cosine similarity for positions: &quot;Data customer&quot; and &quot;Data provider&quot;. The meaning of these two positions are very different and I need my model to show a low cosine similarity for these two positions.</p>
<p>However, in this case cosine similarity must be high: &quot;Data customer&quot; &quot;Data consumer&quot;.</p>
<p>Which model should I use? Should I train classifier instead of nlu model? Why ChatGPT understands the difference between those texts, but BERT based models show high cosine similarity?</p>
","transformer"
"120952","Where can I see a summary of tools and techniques for most updated transformer developments?","2023-04-16 05:13:39","","0","20","<nlp><transformer><gpt>","<p>Since the invention of chatGPT, there are many tools and techniques and variants invented ever since. I want to keep track of these developments and tools but I find no avail. May I know which resources/sites that showcases all these? Thanks in advance.</p>
","transformer"
"120948","Dimensions of mel spectrogram","2023-04-16 02:48:37","120955","0","779","<deep-learning><neural-network><transformer><audio-recognition>","<p>Can someone explain me dimensions in ASR? For example, if I have an audio, convert it to mel spectrogram and now I have a tensor of dimension [1, 128, 850]. Am I understand right that 128 - number of channels and if i will apply CNN for input, input channels will be equal to 128? And what is 850? For example, if I will apply transformer for input mel spectrogram, number of embeddings that I will pass to encoder is 850? Thank you in advance</p>
","transformer"
"120816","Does the Transformer model has memory to store the state accross different data injection sequences(segments)?","2023-04-09 07:16:41","120827","1","258","<dataset><training><transformer><pretraining>","<p>I've trained a transformer model based on the pytorch tutorial: <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""nofollow noreferrer"">https://pytorch.org/tutorials/beginner/transformer_tutorial.html</a>,
But I found I've difficulties to understant this model's input and output connections. According to its training code, the model's input data is:</p>
<p>t[0], t[1], t[2],...,t[n], and its output target value should be t[1], t[2],...,t[n], t[n+1].</p>
<p>input: t[0], t[1], t[2],...,t[n-1], t[n]</p>
<p>output:t[1], t[2], t[3],...,t[n], t[n+1].</p>
<p>And based on my unerstanding, t[1] depends on t[0], t[2] depends on t[1], t[0], and so on, t[n+1] depends on t[n], t[n-1], ..., t[0].</p>
<p>My question is, since we need cut a long tokens list into multiple segments, and input these segments into the transformer model one by one, let's assume one segment has n tokens, <strong>is there any connection exists between two segments?</strong> e.g. does any state connection exist between the t[2n+2] and t[0]? Simply to say, <strong>is the 2nd segments target value t[2n+2] decided by t[0], t[1], ..., t[n], t[n+1], t[n+2]...,t[2n+1]?</strong></p>
","transformer"
"120792","Why does a decoder generate all hidden states during inference?","2023-04-07 16:57:56","","1","949","<transformer><attention-mechanism><sequence-to-sequence>","<p>Seems that in Vanilla transformers at least (a la AIAYN), during inference time, the hidden states are generated for all tokens in the input sequence, but only the last one is used to predict the next token.</p>
<p>My question - why are the other hidden states produced at all during inference? Wouldn't it be more efficient if only the last hidden state was produced?</p>
<p>For example, here : <a href=""https://nlp.seas.harvard.edu/annotated-transformer/#greedy-decoding"" rel=""nofollow noreferrer"">https://nlp.seas.harvard.edu/annotated-transformer/#greedy-decoding</a></p>
<pre><code>out = model.decode(
            memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)
        )
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]
</code></pre>
","transformer"
"120726","What should the numerical values of the <startofsentence> and <endofsentence> token vectors be?","2023-04-05 11:53:38","120979","1","154","<nlp><word-embeddings><transformer><gpt><tokenization>","<p>I'm trying to build GPT2 from scratch. I understand how to convert each word in a sentence to its respective token index and each token is then converted to its respective word embedding vector. I also understand there needs to be a fixed length for each input vector e.g. the max length of all sentences input into the transformer are 50 tokens, and for all sentences shorter than that padding token vectors consisting of nothing but zeroes fill the space where the additional word vectors would be.</p>
<p>I get that each input vector needs to have a start token at the beginning of the input vector, as well as a stop token after the last word and before the padding vectors. The integer values corresponding to the start and stop token indexes are somewhat arbitrary, but I still don't understand what the actual values of the start and stop token embeddings should be. Should they just also be vectors of zeroes? Are these values also arbitrary?</p>
","transformer"
"120601","Do transformers (e.g. BERT) have an unlimited input size?","2023-03-31 09:47:49","120602","8","2641","<machine-learning><nlp><transformer><bert><hyperparameter>","<p>There are various sources on the internet that claim that BERT has a fixed input size of 512 tokens (e.g. <a href=""https://datascience.stackexchange.com/q/89684/141432"">this</a>, <a href=""https://stackoverflow.com/q/58636587/9352077"">this</a>, <a href=""https://www.saltdatalabs.com/blog/bert-how-to-handle-long-documents"" rel=""noreferrer"">this</a>, <a href=""https://datascience.stackexchange.com/q/113489/141432"">this</a> ...). This magical number also appears in the BERT paper (<a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">Devlin et al. 2019</a>), the RoBERTa paper (<a href=""https://arxiv.org/pdf/1907.11692.pdf"" rel=""noreferrer"">Liu et al. 2019</a>) and the SpanBERT paper (<a href=""https://www.cs.princeton.edu/%7Edanqic/papers/tacl2020.pdf"" rel=""noreferrer"">Joshi et al. 2020</a>).</p>
<p>The going wisdom has always seemed <em>to me</em> that when NLP transitioned from recurrent models (RNN/LSTM Seq2Seq, Bahdanau ...) to transformers, we traded variable-length input for fixed-length input that required padding for shorter sequences and could not extend beyond 512 tokens (or whatever other magical number you want to assign your model).</p>
<p>However, come to think of it, all the parameters in a transformer (Vaswani et al. 2017) work on a token-by-token basis: the weight matrices in the attention heads and the FFNNs are applied tokenwise, and hence their parameters are independent of the input size. <strong>Am I correct that a transformer (encoder-decoder, BERT, GPT ...) can take in an arbitrary amount of tokens even with fixed parameters, i.e., the amount of parameters it needs to train is independent of the input size?</strong></p>
<p>I understand that memory and/or time will become an issue for large input lengths since attention is O(n²). This is, however, a limitation of our <em>machines</em> and not of our <em>models</em>. Compare this to an LSTM, which can be run on any sequence but compresses its information into a fixed hidden state and hence blurs all information eventually. <em>If</em> the above claim is correct, then I wonder: <strong>What role does input length play during pre-training of a transformer, given infinite time/memory?</strong></p>
<p>Intuitively, the learnt embedding matrix and weights must somehow be different if you were to train with extremely large contexts, and I wonder if this would have a positive or a negative impact. In an LSTM, it has negative impact, but a transformer doesn't have its information bottleneck.</p>
","transformer"
"120532","How to prepare my dataset for T5?","2023-03-28 04:42:22","","1","593","<python><deep-learning><transformer><t5>","<p>I want to use T5 to do sentiment analysis on IMDB dataset. My dataset is of the following format:</p>
<pre><code># train data
f = open(&quot;train.csv&quot;, &quot;r&quot;)
lines = f.readlines()
lines = [line.strip().split(&quot;,&quot;) for line in lines]
lines = [[line[0], line[1], &quot;,&quot;.join(line[2:])] for line in lines] 
train = pd.DataFrame(lines[1:])
train = train.drop(train.columns[0], axis=1) # drop first column
print(&quot;\ntrain set size:&quot;, train.shape)
print(&quot;\nNumber of positives: &quot;, train[1].astype(int).sum())
train = train.rename(columns={1: 'sentiment', 2: 'review'})
imdb_reviews = train[&quot;review&quot;]
sentiments = train[&quot;sentiment&quot;]
sentiments = [int(v) for v in sentiments]
sentiments=pd.DataFrame(sentiments)
sentiments=sentiments.rename(columns={0:'sentiment'})
sentiments = sentiments[&quot;sentiment&quot;].tolist()

# test data
f = open(&quot;test.csv&quot;, &quot;r&quot;)
lines = f.readlines()
lines = [line.strip().split(&quot;,&quot;) for line in lines]
lines = [[line[0], &quot;,&quot;.join(line[1:])] for line in lines] 
test = pd.DataFrame(lines[1:])
id_test = test[0]
print(&quot;\ntest set:&quot;, test.shape)
test = pd.DataFrame(test[1])
print(&quot;Number of test sentences: {:,}\n&quot;.format(test.shape[0]))
test = test.rename(columns={1:'review'})
</code></pre>
<p>I found <a href=""https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb"" rel=""nofollow noreferrer"">this code</a>, but I could not understand how to adapt it to my own data format. I would appreciate it if you could inform me how to do it. The train set includes 25000 observations, of which 10% should be used as validation set.</p>
","transformer"
"120469","Binary transformer classification model predicts everything as same value","2023-03-24 19:24:15","","0","384","<classification><keras><transformer><binary><numerical>","<p>I'm training a binary classifier using a transformer on structured numerical data (so the order of the columns in my spreadsheet matters). I have adapted the keras text classification model for IMDb reviews (<a href=""https://keras.io/examples/nlp/text_classification_with_transformer/"" rel=""nofollow noreferrer"">https://keras.io/examples/nlp/text_classification_with_transformer/</a>) but my model is predicting everything as the same class.</p>
<p>After some analysis of the outputs I've figured out that all the predictions are &lt; 0.5, so this is why they all get classed as the same thing, but I would like to learn how I can fix this. As I train the model, I notice that val-accuracy increases each epoch up to around 0.8, but running the predictions always results in everything being classed the same: <strong>why does this occur?</strong></p>
<p>I have tried experimenting with the hyperparameters but this has not fixed my problem. My dataset is small (around 500 samples) but balanced, and for each sample contains 846 sequential real numbers.</p>
<p>Any help or modifications to my code will be very appreciated!</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import pandas as pd
from keras import metrics

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.000001):
        super().__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential(
            [layers.Dense(ff_dim, activation=&quot;relu&quot;), layers.Dense(embed_dim),]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)
    
class TokenAndPositionEmbedding(layers.Layer):
    def __init__(self, maxlen, vocab_size, embed_dim):
        super().__init__()
        self.token_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)
        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)

    def call(self, x):
        maxlen = tf.shape(x)[-1]
        positions = tf.range(start=0, limit=maxlen, delta=1)
        positions = self.pos_emb(positions)
        x = self.token_emb(x)
        return x + positions

whole = pd.read_excel('bigset.xlsx', header=None)

wholedata = np.array((whole.drop(columns = [0]).values).tolist())
wholelabel = np.array((whole[0].values).tolist())

(x_train, y_train), (x_val, y_val) = (wholedata[0:500], wholelabel[0:500]), (wholedata[501:581], wholelabel[501:581])

# Define input shape and model hyperparameters
maxlen = x_train.shape[1]  # Maximum sequence length
embed_dim = maxlen  # Embedding size for each vector
num_heads = 2  # Number of attention heads
ff_dim = 4  # Hidden layer size in feed forward network inside transformer

# Define transformer model
inputs = layers.Input(shape=(maxlen,))
embedding_layer = TokenAndPositionEmbedding(maxlen, maxlen, embed_dim)
x = embedding_layer(inputs)
transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
x = transformer_block(x)
x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.1)(x)
x = layers.Dense(20, activation=&quot;sigmoid&quot;)(x)
x = layers.Dropout(0.1)(x)
outputs = layers.Dense(1, activation=&quot;sigmoid&quot;)(x)

model = keras.Model(inputs=inputs, outputs=outputs)

model.compile(optimizer=&quot;adam&quot;, loss=&quot;binary_crossentropy&quot;, metrics=[&quot;accuracy&quot;])

model.fit(
    x_train, y_train, batch_size=10, epochs=300, validation_data=(x_val, y_val)
)

predictions = model.predict(x_val)
print(predictions)
conf_matrix = tf.math.confusion_matrix(labels=y_val, predictions=predictions)
print(conf_matrix)

<span class=""math-container"">```</span>
</code></pre>
","transformer"
"120451","I have a question about Transformer's Q, K, V","2023-03-24 04:34:29","120454","1","61","<transformer><softmax>","<p>I think the cosine similarity of negative values has its own meaning.</p>
<p>If you softmax the cosine similarity of Q and K, wouldn't it prevent Transformer from using information with the opposite meaning?</p>
","transformer"
"120358","How many parameters does the vanilla Transformer have?","2023-03-20 09:33:01","120363","1","3260","<machine-learning><nlp><transformer><attention-mechanism>","<p>The original Transformer paper (<a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Vaswani et al; 2017 NeurIPS</a>) describes the model architecture and the hyperparameters in quite some detail, but it misses to provide the exact (or even rough) model size in terms of parameters (model weights).</p>
<p>I could not find a source with a definite answer on this. Table 3 also mentions a <code>base</code> and a <code>big</code> model, but for none of them model size is given.</p>
<p>How many parameters does a <code>base</code> or a <code>big</code> Transformer model, according to the original implementation by Vaswani et al., have?</p>
","transformer"
"120321","ChatGPT: How to use long texts in prompt?","2023-03-18 12:46:40","","4","17104","<transformer><gpt><tokenization><chatbot>","<p>I like the website <a href=""https://www.chatpdf.com"" rel=""nofollow noreferrer"">chatpdf.com</a> a lot. You can upload a PDF file and then discuss the textual content of the file with the file &quot;itself&quot;. It uses ChatGPT.</p>
<p>I would like to program something similar. But I wonder how to use the content of long PDF files in a ChatGPT prompt, as ChatGPT only accepts 4096 tokens per conversation.</p>
<p>How can I reduce the number of tokens needed?</p>
","transformer"
"120227","Using BERT to extract a list of words and phrases from documents","2023-03-15 16:26:56","","2","402","<nlp><transformer><bert><information-extraction>","<p>I have a list of words and phrases (~3k items). What are my options to extract them from documents (~3M of job descriptions) with NLP? I do not have labeled data.</p>
<p>For example my list of words and phrases look like,</p>
<pre><code>Leadership
Microsoft Office
AWS
.
.
.
Python Programing Language
</code></pre>
<p>The result I am looking for is a matrix(3K x 3M) with binary values inside.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Doc #</th>
<th>Leadership</th>
<th>Microsoft Office</th>
<th>AWS</th>
<th>...</th>
<th>Python Programing Language</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td></td>
<td>1</td>
</tr>
<tr>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td></td>
<td>.</td>
</tr>
<tr>
<td>3M</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td></td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><p>Regex -  This is the most straightforward solution comes my mind. However, this solutions is not robust and cannot capture different word/phrase forms. For example, people might write <code>MS Office</code> instead of <code>Microsoft Office</code>. Similarly, people might write <code>Amazon Web Service</code> rather than <code>AWS</code>.</p>
</li>
<li><p>Is there a solution to utilize a Large Language Model such as BERT?</p>
</li>
<li><p>If I create a labeled data using, for example, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html"" rel=""nofollow noreferrer"">AWS Ground Truth</a>, is there a way to utilize the results to build a model and extract the list of words/phrases?</p>
</li>
</ol>
","transformer"
"120215","Does high number of output labels affect the performance of BERT and how to handle the class imbalance issue while doing multi text classification?","2023-03-15 11:34:02","","1","366","<machine-learning><nlp><transformer><bert><text-classification>","<p>I am using BERT to do multiclass text classification. The number of output classes I have to predict from is: 116 and there is high degree of class imbalance that I see.<br>
We have the following kind of records available for each of the classes:<br>
{'Class A': 975 number of records,<br>
'Class B': 776 number of records,<br>
'Class C': 533 number of records,<br>
'Class D': 412 number of records,<br>
'Class E': 302 number of records,<br>
'Class F': 250 number of records,<br>
'Class G': 207 number of records,<br>
'Class H': 137 number of records,<br>
'Class I': 96 number of records,<br>
'Class J': 51 number of records,<br>
'Class K': 28 number of records,<br>
'Class L': 17 number of records,<br>
'Class M': 7 number of records,<br>
'Class N': 2 number of records}<br></p>
<p>So I have two questions here:<br>
Question1: As we have around 116 output classes to predict from, does that affect the performance of BERT due to the high number of output classes?</p>
<p>Question2: My original data has the similar type of class distribution that I have illustrated above. So how does this affect the performance of BERT and if it affects how do we handle this to get proper output?</p>
<p>Looking forward to get answer from the talented community we have here.</p>
<p>Much thanks in advance.</p>
","transformer"
"119991","A French version of Rebel","2023-03-06 11:01:33","","1","22","<nlp><transformer><knowledge-base><knowledge-graph>","<p>Is there an end-to-end trained transformer like Rebel for french data?
Rebel can extract entities and relations from text, yet as far as I know, it works only with english texts.
Is there any other alternatives for <strong>french</strong> data? I am working in building a knowledge graph of my french (after translating it) data using Rebel, but it didn't work well.</p>
<p>I tried to translate my french data to english. I already have a very noisy data, it becomes even noisier after translation, so I am looking for altenative of Rebel, an end-to-end architecture that deals with <strong>french</strong> data.</p>
","transformer"
"118898","Should I open abbreviations/acronyms in the text data, when training transformer model?","2023-03-01 08:10:58","","2","150","<machine-learning><deep-learning><nlp><transformer>","<p>I am currently training a transformer model on text data. Is it a good practise to open abbreviations/acronyms in the text data? I did not dins any tips or recommendations about it on internet.</p>
","transformer"
"118888","NANs, Infinities, and very large losses with normalizing flows","2023-02-28 17:13:48","","1","108","<loss-function><transformer><normalization><generative-models>","<p>I am new to normalizing flows and have been trying to use them with a high-dimensional dataset, and I have been running into very large numbers and errors with sampling that don't occur when I use a lower dimensional toy dataset. What are some possible causes/solutions to this? Could it be an issue with how the data is normalized? (My data is currently normalized to values between 0 and 1), or are there specific types of transforms/permutations/other layers that are commonly used to prevent this? I know normalizing flows have been relatively successful at a previous similar task to this one (with the same dimensionality), so I am assuming there is something wrong with my setup/hyperparameters.</p>
","transformer"
"118855","What does the output of an encoder in encoder-decoder model represent?","2023-02-27 19:40:59","118866","1","1655","<deep-learning><transformer><encoder>","<p>So in most blogs or books touching upon the topic of encoder-decoder architectures the authors usually say that the last hidden state(s) of the encoder is passed as input to the decoder and the encoder output is discarded. They skim over that topic only dropping that sentence about encoder outputs being discarded and that's it. It makes me confused as hell and even more so, because I'm also reading that in transformer models the encoder output is actually fed to the decoder, but since that's the only thing coming out of an non-rnn encoder, no surprise here.</p>
<p>How I understand it all is that in transformer architectures the encoder returns &quot;enriched features&quot;. If so, then in classical E-D architecture encoder returns just features. Why then is the output of the encoder model ignored in the non-transformer architecture? What does it represent?</p>
","transformer"
"118797","Self-attention in Transformers, are the component values of input vector trained or is it the set W_q, W_k, W_v?","2023-02-25 16:42:58","118798","1","89","<transformer><attention-mechanism>","<p>By far, I find this tutorial on self-attention the most digestible (<a href=""https://peterbloem.nl/blog/transformers"" rel=""nofollow noreferrer"">https://peterbloem.nl/blog/transformers</a>) </p>
<p>Still, I got a question from reading there, hopefully, you guys can help me out </p>
<ul>
<li>Are the component values of the input vectors updated throughout the training? (or the learned parts are the weight matrix Wq, Wk, Wv)? </li>
</ul>
<p>In the blog, the first part said:</p>
<p><em><strong>Since we are learning what the values in 𝐯t should be, how &quot;related&quot; two words are is entirely determined by the task. In most cases, the definite article the is not very relevant to the interpretation of the other words in the sentence; therefore, we will likely end up with an embedding 𝐯the that have a low or negative dot product with all other words</strong></em></p>
<p>So I assume the components of v_the will be learned. Say, if it has 4 component values, like 
v_the = [ 1, 2, 3, 4], then after certain epochs of training, it will become like v_the = [0, 0.1, 0.01, 0.001]</p>
<p>But then a bit further down,  when he started introducing W_q, W_k, and W_v, he said: </p>
<p><em><strong>This gives the self-attention layer some controllable parameters and allows it to modify the incoming vectors to suit the three roles they must play.</strong></em></p>
<p>So now it seems like we just keep the starting values of the input vectors in tact, and the training process will just update the corresponding W_q, W_k, W_v </p>
<p>Hence the question above.</p>
","transformer"
"118767","What are the advantages of autoregressive over seq2seq?","2023-02-24 09:43:33","118768","0","1168","<deep-learning><nlp><transformer><sequence-to-sequence><gpt>","<p>Why are recent dialog agents, such as ChatGPT, BlenderBot3, and Sparrow, based on the decoder architecture instead of the encoder-decoder architecture?</p>
<p>I know the difference between the attention of the encoder and the decoder, but in terms of dialogue, isn't the attention of the encoder-decoder better?</p>
","transformer"
"118736","Which algorithms are suitable for time series classification for this form of data?","2023-02-23 08:12:16","","0","21","<classification><time-series><lstm><transformer>","<p>Newbie here, I have a large list of csv files that contain a series of probability distributions (for 5 classes). I'm trying to train a binary classifier that classifies each file into either a positive or negative class (think medical data). A file would look something like this:</p>
<pre><code>5.716146182185134483e-01,1.570351925479201299e-02,7.879747111130347426e-03,2.257236373138224450e-03,4.025448790424259182e-01
9.282036396626092145e-01,1.350252924774404326e-02,1.202944234922503821e-03,1.077829667771727410e-04,5.698310388794716047e-02
7.827619863407961898e-01,1.238710984142047805e-01,4.863074012493366800e-03,1.025238277112187050e-04,8.840131740479445499e-02
9.164564167759865487e-01,5.029982363205267454e-02,1.889329141871147781e-03,3.470568579893339639e-05,3.131972476429063790e-02
8.438599468843770435e-01,1.249894809120763589e-01,4.892015882596660245e-03,6.318069857265228148e-05,2.619537562237720871e-02
7.927862899191280288e-01,1.836963229227321637e-01,8.236097888524881658e-03,7.672725903457503882e-05,1.520456201058032440e-02
6.938920913668409352e-01,1.154946186794907348e-01,4.775610289402269087e-03,6.998382596026862888e-05,1.857676958383056576e-01
5.260109666876285894e-01,5.373236552754552531e-02,2.351633924538396904e-03,5.219686669895120705e-05,4.178528369935886611e-01
3.048328823074770155e-01,6.639936048941325053e-02,2.493391932535035486e-03,3.446025521901748158e-05,6.262399050153556468e-01
4.090438902597091086e-01,8.938231982641510476e-02,3.129738246458822065e-03,4.481533923136918172e-05,4.983992363281855020e-01
3.390709739657606914e-01,7.605934289910418200e-02,2.752420617374790393e-03,4.097357351008719444e-05,5.820762889442502308e-01
6.149776057156663978e-01,7.070150093100460720e-02,2.396856449071802474e-03,3.329015183386138958e-05,3.118907467524231758e-01
5.370645930556066094e-01,1.300161777734864521e-01,3.121570148930582871e-03,2.977214131925493871e-05,3.297678868806570573e-01
4.034325013246508052e-01,1.843589776745651332e-01,4.607250776903500968e-03,3.698432440481392170e-05,4.075642858994759088e-01
4.864387774152492128e-01,2.113910367664585677e-01,4.652455876671098348e-03,2.834732864475179149e-05,2.974893826129763608e-01
2.994464950642671264e-01,3.608846139880280690e-01,7.909903942206513577e-03,2.764288414682062749e-05,3.317313441213514680e-01
3.384097307003371413e-01,3.577836317319948445e-01,9.840109811059946296e-03,3.679938546258189509e-05,2.939297283711455044e-01
.
.
.
.
</code></pre>
<p>While I understand I can take the argmax for each row and get a much simpler time series, in my particular use case (medical diagnosis) including the probabilities of different classes might be beneficial to the model. My initial plan was to either use transformer encoder -&gt; MLP -&gt; softmax (<a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">from this article</a>) or a simple LSTM, but I don't know how I can fit this form of data to those algorithms. I'd appreciate any help.</p>
","transformer"
"118481","What loss function to use for predicting discrete output sequence given a discrete input sequence?","2023-02-12 20:56:14","118491","0","434","<nlp><regression><multiclass-classification><transformer><sequence-to-sequence>","<p>I am working on sequence-to-sequence tasks where the input is an <code>n</code>-length sequence of discrete values from a finite set S (say <code>{x | x is a non-negative integer less than 10}</code>).
An example input sequence of length 5 is: <code>1 8 3 5 2</code>.</p>
<p>The output is supposed to be some length preserving transformation of the input sequence (say reverse, shift, etc.). To be explicit, the tokens of the output sequence also come from the same set as the input sequence. For example, for the input sequence above, the reverse transformation produces the output sequence: <code>2 5 3 8 1</code>.</p>
<p>I want the model to predict the output tokens exactly, so the task is closer to classification than regression. However, since the output is a sequence, we need to predict multiple classes (as many as the input length) for each input sequence.</p>
<p>I searched for references but could not find a similar setting. Please link some suitable references you are aware of that may be helpful. I have the following questions for my use case:</p>
<ol>
<li>What changes are needed such that the model works on discrete sequences as defined above?</li>
<li>What loss function would be appropriate for my use case?</li>
</ol>
<p>For 1), one might change the input sequence such that each token is replaced by an embedding vector (learned or fixed) and input that to the model. For the prediction, I was thinking of ensuring that the model produces a <code>n x k</code> length output (<code>n</code> = sequence length; <code>k</code> = <code>|S|</code> or the vocab size) and then using each of these <code>n</code> vectors to make a class prediction (from <code>k</code> classes).</p>
<p>For 2), the loss could be a sum of <code>n</code> cross-entropy losses corresponding to the <code>n</code> classifications.</p>
<p>Please help me with better answers to these two questions.</p>
<p>Thank you.</p>
<p>Edit: My setup is encoder-only (non-autoregressive prediction). Please account for this while answering the questions by suggesting approaches that are in line with the setup, if possible.</p>
","transformer"
"118417","Train question answering model with custom dataset","2023-02-09 14:25:06","118443","1","1211","<python><transformer><question-answering><chatbot>","<p>How can I train a question-answering ML model with a custom dataset?</p>
<p>I have gathered nearly 110GB of text data, containing documentation manuals for software products and I am looking into different ML algorithms for question-answering.</p>
<p>The problem is that I don't know how to process these files to make a dataset that will be later used to train the model. There are no labels or anything in the data.</p>
","transformer"
"118358","How much data and computation power do I need to train a machine translation model using Transformer architecture?","2023-02-07 19:27:53","","2","59","<machine-learning><dataset><transformer><machine-translation><cloud-computing>","<p>I am working right now on creating a dataset to use in creating a machine translation model to translate between two dialects. I have two questions that I am trying to find an answer for:</p>
<ol>
<li>How much data is enough? how many pairs of sentences would be sufficient to have a decent translation model that can prove the concept?</li>
<li>How much computation power would I need if I am going to use the transformer architecture? Would GPUs on Google colab be sufficient?</li>
</ol>
","transformer"
"118343","how can I translate Whisper encodings to SBERT embeddings?","2023-02-07 02:32:09","118348","0","251","<transformer><bert>","<p>I'm using the Whisper model to recognize speech, and then matching the output text against a list of known questions by generating SBERT embeddings from the text and ranking the known questions by cosine similarity with the SBERT embeddings of the text output from Whisper. It works pretty well, and I'm happy with the level of accuracy.</p>
<p>I'd like to streamline the process a bit, and I understand I can get the encoder embeddings from the whisper model output rather than just the transcribed text.</p>
<p>My question is: What's the best way to fuse these steps together? More generally, is there a good way to translate embeddings from one model vector space to another? What would that task even be called in terms of linear algebra?</p>
","transformer"
"118226","Vision Transformer ViT Parameter count","2023-02-01 20:52:00","","0","2817","<deep-learning><computer-vision><transformer><attention-mechanism>","<p>The Vision Transformer paper <em>An Image is with 16x16 words</em> by Dosovitskiy et al. (2021)
includes the following table:
<a href=""https://i.sstatic.net/4ytfc.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4ytfc.png"" alt=""enter image description here"" /></a></p>
<p>Can someone explain how they get the parameter counts or where my calculation is wrong?
Let's look at ViT-Base: Each attention layer requires three <span class=""math-container"">$768 \times 768$</span> matrixes to produces <span class=""math-container"">$Q, K, V$</span> from the input. Then the result of each attention layer is concatenated and transformed back to <span class=""math-container"">$D$</span> requiring another <span class=""math-container"">$(12 \cdot 768) \times 768$</span> matrix.</p>
<p>With 12 heads this adds up to <span class=""math-container"">$12 \cdot 768 \cdot 768 + 12 \cdot 768 \cdot 768 \approx 14M$</span> parameter per MSA head. And we add the parameters for the MLP (<span class=""math-container"">$2 * 768*3072 \approx 4.7M$</span>).</p>
<p>Using 12 layers this would imply <span class=""math-container"">$12 \cdot (14 + 4.7) \approx 224M$</span> parameter instead of the 86M specified?</p>
","transformer"
"118210","Predicting a next word from a sentence of a different lenght than seen in training","2023-02-01 11:56:12","","1","208","<deep-learning><nlp><transformer><language-model>","<p>I am building a custom Decoder-only transformer model, which is being trained on the task of Next Word Prediction. The training procedure is analogous to that of chat GPT models - the input to the model is a sentence of length K (say K=30) and the target is this sentence shifted one to the right, e.g.:</p>
<p>&quot;I would like a cup of&quot; - input</p>
<p>&quot;would like a cup of tea&quot; - output</p>
<p>If I train my model on sentences of a specified lenght, say K=30, how will it perform in inference mode when it is provided much shorter sentences, say of length 3?</p>
","transformer"
"118124","How to extract embeddings from an audio file using wav2vec along with context","2023-01-28 18:31:45","118305","0","2809","<nlp><feature-extraction><transformer><audio-recognition>","<p>I am trying to use wav2vec embeddings from the XLSR model for emotion recognition on the EMODB dataset. How can I extract embeddings using wav2vec?
I want to use the XLSR model pre-trained with wav2vec, but I am not sure how to extract embeddings from audio files to use for emotion recognition.</p>
<p>I have made attempt like following but they are not correct, this results in random mappings.</p>
<pre><code>feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('facebook/wav2vec2-large-xlsr-53') #XLSR is for SR, not specifically Emotion Rec. 
input_audio, sample_rate = librosa.load(emodb + file,  sr=16000)
extraction = feature_extractor(input_audio, sampling_rate=16000,  return_tensors=&quot;np&quot;, padding=&quot;max_length&quot;, max_length=max_len).input_values
</code></pre>
<p>Are there any series of steps to follow or libraries or methods I can use to extract the embeddings? Are there any examples or tutorials that I can follow to get started?</p>
","transformer"
"117949","Requirements for variable length output in transformer","2023-01-21 21:38:18","117959","1","1691","<nlp><pytorch><transformer><sequence-to-sequence>","<p>I have been working on modifying the transformer from the article <a href=""https://nlp.seas.harvard.edu/2018/04/03/attention.html#a-first--example"" rel=""nofollow noreferrer"">The Annotated Transformer</a>. One of the features I would like to include is the ability to pass a sequence of fixed length, and receive an output sequence of a shorter length, which is possible per <a href=""https://datascience.stackexchange.com/questions/45475/variable-input-output-length-for-transformer"">this reference</a>.</p>
<p>In my case, I am using a sequence of 10 randomly generated integers 0-9 for the input (just like the article) and trying to return a sequence of five 2s (this is the simplest attempt to get an output of a shorter length I could think of). The start of the sequence is denoted as 1, and the end of the sequence is not defined.</p>
<p>I am successfully able to send the encoder the &quot;source&quot; batch tensor, and the decoder the &quot;target&quot; batch tensor consisting of only 5 columns in the batch tensor. The transformer will train on this data, but it returns a sequence of length equal to the input.</p>
<p>What are the requirements of the transformer network to output a sequence of length that is not equal to the length of the input sequence?</p>
<p>Thanks in advance for any assistance</p>
","transformer"
"117832","What is the best approach to deploy N number of ML models as a scalable service in the Cloud?","2023-01-17 15:58:52","","2","41","<nlp><transformer><aws><deployment><google-cloud-platform>","<p>I've <strong>N (~50)</strong> number of sentiment models of different languages, which were fine tuned on HggingFace's transformer models. Each of the models as 2-3 GB in size approx. Now, how can I deploy all these sentiment models as a scalable service in a cloud platform like GCP, so that the bill is optimized and the service performance (low inference time, or latency) is maximized.</p>
<p>Currently we're deploying each of our models as a separate service. For each of the model we're following the below steps.</p>
<ol>
<li><strong>Develop the service using Flask</strong>: We write the code for our service, including routes and logic for handling requests.</li>
<li><strong>Create a Dockerfile</strong>: A docker file is created to build a Docker image of our service.</li>
<li><strong>Build the Docker image</strong>: We build the Docker image of our service.</li>
<li><strong>Push the Docker image to GCR</strong>: We create a new repository in GCR and
push the Docker image to it.</li>
<li><strong>Create a GKE Cluster</strong>: We go to the Kubernetes Engine console and create a new cluster. Select the appropriate number of nodes and configure the desired resources.</li>
<li><strong>Create a GKE Deployment</strong>: We create a new deployment and associate it
with the image from our GCR repository and configure the desired
number of replicas.</li>
<li><strong>Create a Cloud Load Balancer</strong>: We go to the Google Cloud Console and create a new Cloud Load Balancer. Select the GKE deployment we created in step 6 as the target for the Load Balancer.</li>
<li><strong>Update your DNS to point to the Load Balancer</strong>: Then we update our DNS settings to point to the IP address of the Load Balancer created in step 7.</li>
<li><strong>Monitor the service</strong>: We use Stackdriver to monitor the service and ensure that it is running smoothly and that the desired number of replicas are running.</li>
<li><strong>Scale the service</strong>: When necessary, we use the Auto Scaling feature of GKE to automatically scale the number of replicas running your micro-service based on incoming traffic or other metrics.</li>
</ol>
<p>We follow the same steps for each of our models and deploy the models as a dedicated service. However, this approach costs us a lot of money at the end of the month.</p>
<p>So, suggest me a better way to deploy such multiple models as a service in a scalable manner so that the cloud bill is optimized, but the performance is maximized.</p>
","transformer"
"117700","Memory Error when loading a txt file for an ML model","2023-01-11 16:02:33","","0","45","<transformer><python-3.x><pipelines><memory>","<p>I am trying to run the Python code below:</p>
<pre><code>from transformers import pipeline


question_answerer = pipeline(&quot;question-answering&quot;, model='distilbert-base-uncased-distilled-squad')

with open('output_file.txt', 'r', encoding='utf-8') as file:

    context = file.read()

    question = input(&quot;Type your question: &quot;)

    print(&quot;Calculating answer, please wait.&quot;)

    result = question_answerer(question=question, context=context)

    print(f&quot;Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}&quot;)
</code></pre>
<p>but I get <code>MemoryError</code>, since the input text file is too large (&gt;100GB).</p>
<p>Are there any techniques to read such files?</p>
<p>I can imagine there are others with the same issue. What is a standard approach in situations as such?</p>
","transformer"
"117681","Transformer model with same input fed into the encoder and decoder","2023-01-10 22:39:48","","0","137","<transformer>","<p>I need a model to process an list (Tx = T but variable across samples) of vectors to get another list of vecotrs (Ty = Tx = T so also different across different samples). I would like to use the transformer so that in the mapping the neighboring information could be extracted; the CNN method also works but for my application the transformer handles the variable length input better.</p>
<p>Specifically, it's a model to predict the system response given the control signal. For example, for a system I can have a control input of X and the system will quickly stablize with a response of Y. Both X and Y are vectors; they might be of different lengths, but given a X we are guaranteed to have a Y.</p>
<p>I am thinking about instead of feeding the decoder the shifted output, I can feed the input directly. In this case, the encoder encodes information in the K, Q, V matrices and the decoder can use it appropriately. Does this make sense?</p>
","transformer"
"117634","What does Embeddings Array Represent in BERT's Feature Extraction?","2023-01-09 09:08:20","","0","216","<nlp><word-embeddings><transformer><bert>","<p>I am new to academic NLP, and I had been tasked with to use BERT to extract features of a sentence.</p>
<pre><code>text_input = [
  &quot;Hello I'm a single sentence&quot;,
  &quot;And another sentence&quot;,
  &quot;And the very very last one&quot;,
  &quot;My name is Aun&quot;
  ]
</code></pre>
<p>I got embeddings using pipeline from huggingface:</p>
<pre><code>from transformers import pipeline
feature_extraction = pipeline('feature-extraction', model=&quot;distilroberta-base&quot;, tokenizer=&quot;distilroberta-base&quot;)
features = feature_extraction(text_input)
</code></pre>
<p>Embeddings were multi-dimension, which I flattened and then padded to match the array with highest size. Here <code>text_df.head()</code>:</p>
<pre><code>    text_input                  text_embeddings                                 text_em_flat                        text_em_flat_pad
0   Hello I'm a single sentence [[[-0.010155443102121353, 0.07965511828660965,...   [-0.010155443102121353, 0.07965511828660965, 0...   [-0.010155443102121353, 0.07965511828660965, 0...
1   And another sentence        [[[-0.010256338864564896, 0.0948348417878151, ...   [-0.010256338864564896, 0.0948348417878151, -0...   [-0.010256338864564896, 0.0948348417878151, -0...
2   And the very very last one  [[[-0.001137858722358942, 0.09048153460025787,...   [-0.001137858722358942, 0.09048153460025787, -...   [-0.001137858722358942, 0.09048153460025787, -...
3   My name is Aun              [[[-0.0018534815171733499, 0.08652304857969284...   [-0.0018534815171733499, 0.08652304857969284, ...   [-0.0018534815171733499, 0.08652304857969284, ...
</code></pre>
<p>But I don't understand what each value represents in the text_embeddings. I have gone through some explanation, but don't understand if they are token level or segment level or position level embeddings for a stack of all three. Please explain.
Following are the shapes for few instances:</p>
<pre><code>arr_dimen(text_df['text_embeddings'][0]):    [1, 8, 768]
arr_dimen(text_df['text_embeddings'][1]):    [1, 5, 768]
arr_dimen(text_df['text_embeddings'][2]):    [1, 8, 768]
arr_dimen(text_df['text_embeddings'][3]):    [1, 7, 768]
arlen(text_df['text_em_flat'][0]):   6144
arlen(text_df['text_em_flat'][1]):   3840
arlen(text_df['text_em_flat'][2]):   6144
arlen(text_df['text_em_flat'][3]):   5376
</code></pre>
<p>From original paper I understand that BERT divides input in three-layers and then uses them like shown in the figure from original paper. <a href=""https://i.sstatic.net/XPWhh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XPWhh.png"" alt=""enter image description here"" /></a></p>
<p>But I want to understand how BERT encodes <code>My name is Aun</code> to an array with shape <code>[1, 7, 768]</code>.</p>
","transformer"
"117583","Using transformers positional embedding","2023-01-06 21:07:56","","0","268","<deep-learning><transformer>","<p>Positional embeddings are introduced into a transformer in order to add positional information to a word embedding.</p>
<p>Now, suppose we have an existing data embedding that can be for any data domain word/image. We don't have the original text/image before being encoded but the final embedding. Is it possible to extract positional embedding from an embedding or do we have to run positional embedding on the raw input (text/image) please? Is there a way around it in case we don't have the original raw input but only its embedding?</p>
","transformer"
"117563","Are there any practical advantages of LSTMs over transformers?","2023-01-06 09:56:36","117565","2","415","<machine-learning><lstm><transformer>","<p>There are a number of <a href=""https://medium.com/analytics-vidhya/why-are-lstms-struggling-to-matchup-with-transformers-a1cc5b2557e3"" rel=""nofollow noreferrer"">articles noting that transformers have significant advantages</a> over &quot;traditional&quot; RNNs like LSTMs. And the industry as a whole have been moving away from LSTMs. My question is, in the domain of NLP, do LSTMs (or GRUs) have any practical benefits/advantages over transformers?</p>
","transformer"
"117458","Compound and Complex Sentence Tokenization","2023-01-02 19:45:24","","0","102","<nlp><transformer><sentiment-analysis>","<p>I am trying to tokenize sentences of a document for aspect-based sentiment analysis. There are some sentences that consist of more than one topic. A couple of examples:</p>
<ul>
<li>&quot;The touch screen is good but the battery is weak&quot;</li>
<li>&quot;Their smartphones are great and their TVs are perfect&quot;</li>
</ul>
<p>I want to tokenize sentences based on these conjunctions. Is there any pre-trained model for this task? Are there any other solutions?</p>
","transformer"
"117436","Predict the values of variable features over timestamps","2023-01-01 11:42:00","","0","99","<deep-learning><time-series><convolutional-neural-network><transformer>","<p>I have a dataset which contains timestamps and number of users at that timestamp. Each user has resource values which change per timestamp. How can I predict the number of users incoming at timestamp and the resource values of each user at that timestamp?</p>
<p>I found out that meta-learning model can be used for this proposal. Is there any conv Net or Deep neural networks can solve this proposal?</p>
","transformer"
"117395","What does the random seed influence in transformers?","2022-12-29 17:47:29","117396","0","1491","<transformer>","<p>I've been doing some experiments with setting different seeds on transformer training, and wanted to understand why I see so much variance.</p>
<p>Am I correct in thinking the random seed only influences two things when transformer training:</p>
<ul>
<li>The random sorting of the training data;</li>
<li>Which connections get removed by dropout.</li>
</ul>
<p>Oh, and the initial value of the randomly-initialized weights.</p>
<p>If so, that implies there is no stochastic element at all when using the model for inference?</p>
<p>Maybe my question would be better phrased as: what functions use randomness, so I can search a codebase for them, and confirm where they are used?</p>
<p>(I'm mainly looking at PyTorch implementations such as fairseq and huggingface; but I am assuming tensorflow implementations handle random numbers the same way.)</p>
","transformer"
"117285","Is normalization of word embeddings important?","2022-12-24 07:24:58","","1","117","<reinforcement-learning><word-embeddings><transformer><attention-mechanism><representation>","<p>I am doing actor-critic reinforcement learning for an environment that is best represented as a &quot;bag-of-words&quot;. For this reason, I have opted to use a single body, multi-head approach for the net architecture. I use a linear pre-processing layer to generate <code>n</code> word embeddings of dimension <code>d</code>. Then I run the <code>(batch,n,d)</code> words through a stack of (2) nn.TransformerEncoder layers for the body and each head is another encoder layer followed by a linear logit layer.</p>
<p>Since this is RL and I have limited compute, it is also difficult to evaluate training as its happening. I decided to try looking a the <em>mean cosine similarity</em> of the latent words after the encoder body. My intuition tells me if the net is learning a proper latent representation of the environment then dis-similar words should have low cosine similarity.</p>
<p>However even though the net is clearly improving somewhat the mean cosine sim. remains very high, &gt; .99</p>
<p>Thinking about it more, I don't think there's any reason to believe my first intuition, especially since I am not even normalizing the words after encoder body stack. But even if I did normalize, I'm not sure that would encourage lower cosine sim. as I am using 256 dimensions per word. All normalizing does is reduce the dimension of the output space by 1, which should hardly matter here.</p>
<p>Does this make sense? Also any general advice about my net is welcome</p>
","transformer"
"117015","Adding punctuation for a long text","2022-12-13 11:36:33","","2","898","<python><deep-learning><nlp><transformer>","<p>I want to add punctuation to a long text (youtube transcript) before using a Transformer pipeline for summarization.</p>
<p>I have found this answer here:
<a href=""https://datascience.stackexchange.com/questions/48575/is-there-any-nlp-library-or-package-which-can-help-in-adding-comma-punctuation?newreg=4c83717f50ec4a78943dd3eeae9511a1"">original answer</a></p>
<p>thus I have tried:</p>
<pre><code>from transformers import T5Tokenizer, TFT5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained('SJ-Ray/Re-Punctuate')
model = TFT5ForConditionalGeneration.from_pretrained('SJ-Ray/Re-Punctuate')

input_text = 'the story of this brave brilliant athlete whose very being was questioned so publicly is one that still captures the imagination'
inputs = tokenizer.encode(&quot;punctuate: &quot; + input_text, return_tensors=&quot;tf&quot;) 
result = model.generate(inputs)

decoded_output = tokenizer.decode(result[0], skip_special_tokens=True)
print(decoded_output)
</code></pre>
<p>While this is working fine with short sentences the token limit is 512 and I have text with lengths up to 13000 tokens.
This is not a problem for summarization I use longformer model that has limits of 16000, but I would like to know the best strategy to add punctuation for a long text.
How do you suggest splitting the text? other kinds of strategies? do you have a snippet of code?</p>
<p>Thank you very much for your help</p>
","transformer"
"116986","Possible NLP approaches to extract 'goals' from text","2022-12-12 14:35:25","","0","146","<nlp><transformer><spacy><association-rules><huggingface>","<p>I am planning to take up an interesting NLP project. I want to extract 'goal' statements from lengthy reports. For example, the goals can be <em>We would be reducing our carbon footprint by 50% by 2025</em> or <em>Our company aims to increase the diversity in the work-force in upcoming months</em>. Check below image for example text and highlighted goals.</p>
<p><a href=""https://i.sstatic.net/pU6sy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pU6sy.png"" alt=""Example with highlighted goals"" /></a></p>
<p>How can I go about the process of goal extraction, I would like to get some pointers on possible NLP approaches ?</p>
","transformer"
"116949","Solving video classification problem by taking EVA Large as backbone","2022-12-10 23:02:03","","3","112","<classification><cnn><pytorch><computer-vision><transformer>","<p>I am solving a video classification problem. There are 9 classes in total. At first I took ResNet as a feature extractor, this gave me 0.74 accuracy. Then I changed ResNet to EVA (I also tried Swin), hoping that this would increase accuracy. But this greatly worsened the result. That's my model:</p>
<pre class=""lang-py prettyprint-override""><code>class RSNAModel(nn.Module):
    def __init__(self, pretrained=True):
        super(RSNAModel, self).__init__()

        self.backbone = timm.create_model(model_name=Config['FEATURE_EXTRACTOR'], pretrained=True, num_classes=0)
        freeze_module(self.backbone)
        
        
        self.num_features = self.backbone.num_features
        self.backbone.classifier = Identity()
        self.dropout = nn.Dropout(p=Config['DR_RATE'])
        self.rnn = nn.LSTM(
            input_size=self.num_features, 
            hidden_size=self.num_features // 2, 
            num_layers=1,
            dropout=Config['RNN_DP'],
            bidirectional=True,
            batch_first=True, 
        )
        
        self.head = nn.Linear(in_features=self.num_features, out_features=Config['NUM_CLASSES'])
        
    def forward(self, inputs):
        b, f, c, h, w = inputs.shape
        inputs = inputs.reshape(b * f, c, h, w)
        embeddings = self.backbone(inputs)
        embeddings = embeddings.reshape(b, f, self.num_features)
        sequence_outputs, h_n = self.rnn(embeddings)
        features = sequence_outputs[:, -1]
        outputs = self.dropout(features)
        outputs = self.head(outputs)
        
        return outputs
</code></pre>
<pre><code>==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
RSNAModel                                [4, 9]                    --
├─Beit: 1-1                              [64, 1408]                363,264
│    └─PatchEmbed: 2-1                   [64, 256, 1408]           --
│    │    └─Conv2d: 3-1                  [64, 1408, 16, 16]        (829,312)
│    │    └─Identity: 3-2                [64, 256, 1408]           --
│    └─Dropout: 2-2                      [64, 257, 1408]           --
│    └─ModuleList: 2-3                   --                        --
│    │    └─Block: 3-3                   [64, 257, 1408]    
          ............
│    │    └─Block: 3-42                  [64, 257, 1408]           (25,248,768)
│    └─Identity: 2-4                     [64, 257, 1408]           --
│    └─LayerNorm: 2-5                    [64, 1408]                (2,816)
│    └─Identity: 2-6                     [64, 1408]                --
├─LSTM: 1-2                              [4, 16, 1408]             11,906,048
├─Dropout: 1-3                           [4, 1408]                 --
├─Linear: 1-4                            [4, 9]                    12,681
==========================================================================================
Total params: 1,023,064,841
Trainable params: 11,918,729
Non-trainable params: 1,011,146,112
Total mult-adds (G): 63.75
==========================================================================================
Input size (MB): 38.54
Forward/backward pass size (MB): 62167.32
Params size (MB): 3138.77
Estimated Total Size (MB): 65344.63
==========================================================================================
</code></pre>
<p>What should i do to improve results? What other tricks can be applied to improve accuracy?  I believe that a good vision transformer as backbone can improve accuracy.</p>
","transformer"
"116800","How is padding masking considered in the Attention Head of a Transformer?","2022-12-07 04:30:23","","3","291","<neural-network><nlp><pytorch><transformer>","<p>For purely educational purposes, my goal is to implement  basic Transformer architecture from scratch. So far I focused on the encoder for classification tasks and assumed that all samples in a batch have the same length. This means, I didn't care about any masking.</p>
<p>However, now I want to support masking. I like to think that I understand the the purpose of, e.g., the target mask so the order cannot &quot;peek into the future&quot;. I generate this mask as follows:</p>
<pre><code>source_batch = torch.LongTensor([
    [1, 2, 3, 0, 0, 0],
    [1, 2, 3, 4, 5, 6],
    [1, 2, 3, 4, 5, 0]
])

batch_size, seq_len = source_batch.shape

def generate_tgt_mask(size):
    return torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1)

print(generate_tgt_mask(seq_len))
</code></pre>
<p>yielding:</p>
<pre><code>tensor([[0., -inf, -inf, -inf, -inf, -inf],
        [0.,   0., -inf, -inf, -inf, -inf],
        [0.,   0.,   0., -inf, -inf, -inf],
        [0.,   0.,   0.,   0., -inf, -inf],
        [0.,   0.,   0.,   0.,   0., -inf],
        [0.,   0.,   0.,   0.,   0.,   0.]])
</code></pre>
<p>which should be the expected outcome when I check the PyTorch docs. This mask has a shape of <code>(L,L)</code> where <code>L</code> is the sequence length of the source or target sequence. Again, this matches the docs.</p>
<p>I use this mask in my implementation of the Scaled Dot Product Attention as follows -- which should be in line with many other implementations I've seen:</p>
<pre><code>class Attention(nn.Module):
    ### Implements Scaled Dot Product Attention
    
    def __init__(self):
        super().__init__()


    def forward(self, Q, K, V, mask=None, dropout=None):
        # All shapes: (batch_size, seq_len, hidden_size)
        
        # Perform Q*K^T (* is the dot product here)
        # We have to use torch.matmul since we work with batches!
        out = torch.matmul(Q, K.transpose(1, 2)) # =&gt; shape: (B, L, L)

        # Divide by scaling factor
        out = out / (Q.shape[-1] ** 0.5)

        # Optional: src_mask/tgt_mask (shape: (L, L); mask values are represented by -inf)
        if mask is not None:
            out += mask.unsqueeze(0) # Broadcast since it's the same mask for all samples in batch
        
        # Push throught softmax layer
        out = f.softmax(out, dim=-1)
        
        # Optional: Dropout
        if dropout is not None:
            out = nn.Dropout(out, dropout)
        
        # Multiply with values V
        out = torch.matmul(out, V)
        
        return out
</code></pre>
<p>So far so good...at least I like to think. However, my problem is now the mask to address the padding (e.g. <code>src_key_padding_mask</code>). From different tutorials using the <code>nn.Transformer</code>, this mask can be generated as follows:</p>
<pre><code>pad_token_index = 0

src_key_padding_mask = (source_batch != pad_token_index)

print(src_key_padding_mask)
</code></pre>
<p>yielding:</p>
<pre><code>tensor([[ True,  True,  True, False, False, False],
        [ True,  True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True, False]])
</code></pre>
<p>having shape of <code>(N,L)</code> which again matches the doc.</p>
<p>What I'm now missing is: How do I have to incorporate this matrix into my implementation of <code>Attention</code>?</p>
<p>Intuitively, I would assume that the masking matrix would contain <code>-inf</code> for each position associated the a padding. For example, looking at the first sequence in my example batch above, I would assume the masking matrix to look like:</p>
<pre><code>tensor([[0.,   0.,   0.,   -inf, -inf, -inf],
        [0.,   0.,   0.,   -inf, -inf, -inf],
        [0.,   0.,   0.,   -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf]])
</code></pre>
<p>And indeed, some -- but not all -- example code that implement the Transformer archictectur from scratch, create the masking matrix for the padding like this. Applying this matrix to the scores obviously also sets the scores to 0, that is, the last 3 rows are all 0.</p>
<p>However, once pushed throught Softmax, the last 3 rows now all contain the value <code>1/6</code>. For example, for the <code>source_batch</code> above I get</p>
<pre><code>tensor([[[0.1989, 0.4297, 0.3714, 0.0000, 0.0000, 0.0000],
         [0.4334, 0.2225, 0.3440, 0.0000, 0.0000, 0.0000],
         [0.2880, 0.2284, 0.4836, 0.0000, 0.0000, 0.0000],
         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],
         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667],
         [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667]],
       ...
       (the other 2 samples of the batch are not shown)
</code></pre>
<p>What am I missing here? I'm pretty sure it's something trivial, but I just can't see it right now.</p>
","transformer"
"116655","Is there bias in matrix multiplications for self attention","2022-12-02 06:45:40","","0","901","<transformer><bert><attention-mechanism>","<p>When the query matrix Q is computed as <span class=""math-container"">$XW_Q$</span>,  (<span class=""math-container"">$W_Q$</span> is the weight matrix for the queries), is it implemented as a linear layer without bias? I see some blogs saying there is are bias terms as well.</p>
<p>To paraphrase the question in a different way, are there bias terms when Q , K and V are computed using a linear layer in pytorch? If yes, why does it make sense since adding bias makes the transformation a non linear one</p>
","transformer"
"116518","NER - What advantage does IO Format have over BIO Format","2022-11-27 13:23:02","116523","3","458","<machine-learning><nlp><transformer><bert><named-entity-recognition>","<p>In <a href=""https://aclanthology.org/2021.acl-long.248.pdf"" rel=""nofollow noreferrer"">this</a> paper, the authors say that they used IO schema instead of BIO in their dataset, which, if I am not wrong, means they just tag the corresponding Entity Type or &quot;O&quot; in case the word is not a Named Entity. What advantage does this method have? I would imagine that it just takes away valuable information from the model and makes it harder to detect entities that span multiple words</p>
","transformer"
"116506","Transformer XL - understanding paper's illustration","2022-11-26 17:09:22","","2","104","<deep-learning><nlp><transformer><attention-mechanism>","<p>If I understand correctly, the <code>Key</code> hidden layer in the Transformer XL is of size <code>2L * d</code>,  where <code>L</code> is the segment length and <code>d</code> is the embedding dimension.</p>
<blockquote>
<p>concatenation of two hidden sequences along the length dimension</p>
</blockquote>
<p>Therefore, the size of the attention matrix would be <code>L X 2L</code>, where row <code>i</code> represents the attention <code>Query i</code> should apply to each of the <code>2L Keys</code>.</p>
<p><strong>That is, the self attention window length = 2 X segment length.</strong></p>
<p>However, in the following image from the paper, the segment length is 4 and there are only 4 lines linked to each node. Shouldn't there be 4 * 2 = 8 lines from each node?</p>
<p><a href=""https://i.sstatic.net/vJAyt.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/vJAyt.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://arxiv.org/pdf/1901.02860.pdf"" rel=""nofollow noreferrer"">Link to transformer XL paper</a></p>
","transformer"
"116488","How can I let Non-Transformed Values through Scikit-Learn's Column Transformer?","2022-11-25 12:25:42","116489","3","1165","<python><scikit-learn><data-cleaning><feature-engineering><transformer>","<p>I'm working with a heterogenous dataset that includes numerical and categorical columns. I want to use Scikit-Learn's <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer"" rel=""nofollow noreferrer""><code>ColumnTransformer</code></a> to <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"" rel=""nofollow noreferrer""><code>OneHotEncode</code></a> the categorical data, however, <code>ColumnTransformer</code> will only recombine the columns that I apply a transformer to.</p>
<p>I don't want to apply a transformer to the numerical columns, how can I let the non-transformed values through the function unchanged and included in the output?</p>
","transformer"
"116384","How to include information about labels in a multilabel classification task","2022-11-22 07:50:51","","0","107","<machine-learning><nlp><transformer><multilabel-classification><ai>","<p>Currently, I'm working on a multilabel classification problem for a shared task in NLP. I have quite a few labels, and with those labels, I have a little paragraph defining them. I was wondering if there is some way I can include that label information in a multilabel classification pipeline.</p>
<p>Up until now, I've tried prompt-learning, designing a prompt that includes that paragraph, but I haven't obtained good results. My best shot so far has been using a fine-tuned RoBERTa model, and I thought that if I could include that label definition somehow in the pipeline, I could obtain better results, as the LM beneath could extract more information about it.</p>
<p>Thanks in advance! Cheers.</p>
","transformer"
"116268","Does fine-tuning require retraining the entire model?","2022-11-17 18:50:48","116270","4","2426","<machine-learning><deep-learning><transformer><gpt>","<p>Would it be necessary to retrain the entire model if we were to perform fine-tuning?</p>
<p>Let's say we somehow got the GPT-3 model from OpenAI (I know GPT-3 is closed source).</p>
<p>Would anyone with access to a couple of RTX 3080 GPUs be able to fine tune it if they got the GPT-3 model weights?</p>
<p>Or would it need infrastructure like the big companies?</p>
","transformer"
"116233","Can I fine tune GPT-3?","2022-11-16 20:11:10","116236","2","1119","<machine-learning><deep-learning><transformer><gpt>","<p>Can anyone fine-tune the GPT-3 model on commodity hardware without GPU?</p>
<ul>
<li>What I meant is can we fine tune available GPT-3 equivalent models?</li>
<li>For example, we have only access to GPT-J.</li>
<li>Can we fine-tune GPT-J with commodity hardware or lets say with only basic GPU such as 1with one RTX 3080.</li>
</ul>
<p>Can we fine-tune these models (not training from scratch)?</p>
<p>Or will it need high-end infrastructure with GPUs?</p>
","transformer"
"115797","Overfitting problem with small model","2022-11-02 02:36:41","","0","675","<machine-learning><deep-learning><pytorch><transformer><overfitting>","<p><a href=""https://i.sstatic.net/N00OG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/N00OG.png"" alt=""enter image description here"" /></a></p>
<p>I have an encoder-decoder architecture where I have used top 3 layers of <a href=""https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py"" rel=""nofollow noreferrer"">Swin Transformer</a> and few convolutional layer. I tried different approach:</p>
<p>i. Training the Transformer layers as well, on doing so model contains approximately 304,086*2(encoder + decoder) trainable parameters.</p>
<p>ii. Freezing the transformer layers approx. 105 * 2 = 210 (encoder + decoder) total trainable parameters. This also shows I have very few layers of CNN.</p>
<p>On both the approach the validation loss is higher than the training loss. The above depicted curve is for approach (i).</p>
<p>I have 7K trainable data and have used 700 for validation. Also, used L2-Regularization but the results doesn't change.</p>
","transformer"
"115661","Keras NLP TransformerDecoder MultiHeadAttention Value Error","2022-10-28 02:10:04","","1","249","<machine-learning><neural-network><keras><transformer><attention-mechanism>","<p>Recently I have been working on a MIDI Music Generator using the TransformerEncoder &amp; TransformerDecoder layers found in the Keras NLP library. There is not much info/help on these layers which is why I am here. Everything was fine until eventually needed to batch my input dataset to get around GPU memory limitations. Now I encountered this error:</p>
<pre><code>Exception has occurred: ValueError (note: full exception trace is shown but execution is paused at: tf__call)

Exception encountered when calling layer &quot;multi_head_attention&quot; &quot; f&quot;(type MultiHeadAttention).

dim -7 not in the interval [-4, 3]. for '{{node TransDecPitch/multi_head_attention/ExpandDims}} = ExpandDims[T=DT_INT32, Tdim=DT_INT32](TransDecPitch/Tile, TransDecPitch/multi_head_attention/ExpandDims/dim)' with input shapes: [?,64,64], [] and with computed input tensors: input[1] = &lt;-7&gt;.

Call arguments received by layer &quot;multi_head_attention&quot; &quot;                 f&quot;(type MultiHeadAttention):
  • query=tf.Tensor(shape=(None, 64, 562, 149, 1007), dtype=float32)
  • value=tf.Tensor(shape=(None, 64, 562, 149, 1007), dtype=float32)
  • key=tf.Tensor(shape=(None, 64, 562, 149, 1007), dtype=float32)
  • attention_mask=tf.Tensor(shape=(None, 64, 64), dtype=int32)
  • return_attention_scores=False
  • training=False
  • use_causal_mask=False

</code></pre>
<p>To clarify some of the numbers above:</p>
<ul>
<li>Batch Size = 64</li>
<li>Sequences Per Batch = 562</li>
<li>Sequence Length = 149</li>
<li>Embedding Layer Output Dim = 1007</li>
</ul>
<p>This is the model architecture. I cannot paste the model summary because the error occurs befor reaching that line.</p>
<pre><code>PITCH_INPUT_NODES = len(self.unique_x_pitch) # Actual value of this is 1006
PITCH_OUTPUT_NODES = len(self.unique_y_pitch) # Actual value of this is 1006 
PITCH_OUTPUT_DIM = PITCH_OUTPUT_NODES +1  # Actual value of this is 1007
PITCH_ATTENTION = 6 
TRANS_DROPOUT = 0.4
            
opt_pitch = Adam(1e-4)
            
input_pitch = Input(self.pitch_input_shape)
x_pitch = TokenAndPositionEmbedding(PITCH_INPUT_NODES,self.seq_length,PITCH_OUTPUT_DIM, name='EmbeddingsPitch')(input_pitch)
x_pitch = TransformerEncoder(PITCH_OUTPUT_NODES, PITCH_ATTENTION, TRANS_DROPOUT, name='TransEncPitch')(x_pitch)
x_pitch = TransformerDecoder(PITCH_OUTPUT_NODES, PITCH_ATTENTION, TRANS_DROPOUT, name='TransDecPitch')(x_pitch)
pitch_output = Dense(PITCH_OUTPUT_NODES, activation='softmax', name='OutputPitch')(x_pitch) #Pitch Output Layer
self.model_pitch = Model(input_pitch, pitch_output, name=&quot;ModelPitch&quot;)
self.model_pitch.compile(opt_pitch, tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
                                     metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
                                     )

</code></pre>
<p>Any help would be fantastic and please let me know if any helpers need more info.
Thank you</p>
","transformer"
"115453","Do large pretrained language models already ""know"" about NLP tasks?","2022-10-21 14:09:46","","5","93","<nlp><transformer><language-model>","<p>Nowadays the state-of-the-art in NLP is to finetune a large pretrained language model such as BERT/GPT etc. on specfic tasks. These language models are pretrained on a huge amount of data and then basically evaluated on popular labeled datasets published for e.g. Question Answering, Machine Translation <a href=""http://nlpprogress.com/"" rel=""noreferrer"">etc.</a>. As those datasets became the de facto default of evaluating these model, those datasets have been published over and over again on various websites. Used and reused for people that are building their own small model etc. So basically these datasets (train and test data) including their e.g. label in classification tasks or the answer in Q/A tasks &quot;stray&quot; around in the internet. So now when training a <em>new</em> large language model (with a novel architecture) it is fed with text data that is often scraped from the internet as well. Wouldn't it be possible that in the training phase of these LMs, the networks have already seen this exact data (and learned on its co-occurence) which they are evaluated on later on? This would basically defeat the purpose of the evaluation as the test data already leaked into the process of pretraining the language models. Are there any prefiltering steps happening in pretraining these models such that this doesn't happen? And secondly, even if the network has seen the exact test data with e.g. test set question+answer among billions of other textual data, would it even pick up on that or is it just too much data anyways for the model to adjust the weights accordingly and &quot;remembering&quot; these exact datapoints.</p>
","transformer"
"115353","How does BERT produce CLS token? Internally does it do max-pooling or avarage pooling?","2022-10-18 19:00:55","115356","1","1733","<nlp><transformer><bert>","<p>I ran experiment to compare max-pooled word tokens vs CLS token for sentence classification and CLS clearly wins. Trying to understand how BERT generates CLS token embedding if its better than max or avg pooling.</p>
","transformer"
"115076","How exactly have T5 3B and 11B models been scaled?","2022-10-10 15:35:58","","2","177","<transformer>","<p>I've been trying to understand the difference between the large T5 model, and the 3B model. I thought it was a typo in a paper referencing it, but the original paper says the same thing. (For reference, I'm quoting from <a href=""https://arxiv.org/pdf/1910.10683.pdf#page=36"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1910.10683.pdf#page=36</a>)</p>
<p>Base uses <code>dmodel</code> of 768, 12 heads and <code>dkv</code> of 64. <code>12 x 64 = 768</code>, all good. 12 layers in each of encoder and decoder. <code>dff</code> is 3072, which means the FFN, in PyTorch terms would be:</p>
<pre><code>x = nn.Linear(768, 768 * 4)
x = F.relu(x)
x = nn.Linear(768 * 4, 768)
</code></pre>
<p>We then have Large size, which increases the model dimension, <code>dmodel</code>, to 1024, with 16 heads each of <code>dkv=64</code>, <code>16 x 64 = 1024</code>; it keeps the same x4 ratio for the FFN, so it is <code>1024 -&gt; 4096 -&gt; 1024</code>. It uses 24 layers in each of encoder and decoder, and has 770M params, compared to the base model 220M params.</p>
<blockquote>
<p>3B and 11B... we use <code>dmodel</code> = 1024, a 24 layer encoder and decoder, and <code>dkv</code> = 128. For the “3B” variant, we use <code>dff</code> = 16,384 with 32-headed attention</p>
</blockquote>
<p>Maybe it is just sloppy language, but I took 32-headed attention to mean there are 32 heads? But that is twice as many heads and twice the <code>dkv</code>, <code>32 x 128 = 4096</code> which is not equal to the <code>dmodel</code> of 1024. Though 4096 would then make sense, as then the standard <code>4x</code> in the FFN would match the <code>dff</code> they give us: <code>4096 -&gt; 16384 -&gt; 4096</code>.</p>
<p>However, I believe that would be a lot more than 3B parameters then? So did they mean to write &quot;with head dimension of 32&quot; ? And then their FFN is doing a fairly novel 16x ratio: <code>1024 -&gt; 16384 -&gt; 1024</code>.</p>
<p>The paper goes on to say:</p>
<blockquote>
<p>for “11B” we use <code>dff</code> = 65,536 with 128-headed attention...</p>
</blockquote>
<p>So does this mean their model dimension of 1024 is divided into 128 heads each of dimension <code>dkv</code> = 8? And then their FFN is <code>1024 -&gt; 65536 -&gt; 1024</code>?</p>
<p>If this is true, does anyone have any insight, papers, references to experiments, etc. on why if they were sticking with a model dimension of 1024, they didn't just stick with 16 heads of <code>dkv</code>=64?</p>
<p>By the way, not increasing model dimension as scaling up is unusual (e.g. GPT-3, <a href=""https://arxiv.org/pdf/2005.14165.pdf#page=8"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2005.14165.pdf#page=8</a>, keeps increasing it, and keeps head-dim almost the same; they also always use <code>dff = dmodel x 4</code>), so I've been surprised the paper doesn't draw more attention to it, only saying it is efficient on a TPU. The various blogs and articles about T5 that I've found don't seem to mention it at all.</p>
","transformer"
"114901","Extract the embedding from a specific layer of MarianModel","2022-10-04 14:23:59","","0","221","<transformer><machine-translation><huggingface>","<p>I am using using <a href=""https://huggingface.co/docs/transformers/model_doc/marian#transformers.MarianModel"" rel=""nofollow noreferrer"">MarianModel</a> from the hub of HuggingFace for a translation task. Now I want to extract the embedding from the output of the last <code>MarianEncoderLayer</code> layer. Specifically, given a text, I want to get the middle embedding of the text for another task. Here is what I did:</p>
<pre><code>text = 'hello how are you?'
inputs = tokenizer(text,max_length=512,truncation=True,return_tensors='pt',padding=True)
model.model.encoder.layers[5](**inputs)
</code></pre>
<p>But it throws the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\dohuut\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
TypeError: forward() got an unexpected keyword argument 'input_ids'
</code></pre>
<p>What is the proper way to extract this embedding?</p>
","transformer"
"114858","Should I annotate additional information besides the categories I already need in a text?","2022-10-03 05:54:27","114870","1","30","<neural-network><lstm><rnn><transformer><annotation>","<p>I have a dataset with bank transfer reasons. They vary a lot because humans wrote them.
From the reasons that are linked to invoice payments I need to extract several things:</p>
<ul>
<li><em>invoice number(s)</em></li>
<li><em>IBAN</em></li>
<li><em>counterparty</em></li>
</ul>
<p>Before I use any NN algorithm I need to annotate the data.</p>
<p>So, for example, I have these rows:</p>
<ol>
<li>&quot;Bank transfer for INV. <strong>00234</strong>, <strong>00435</strong>/<em>2022.01.13</em> [BIC] <strong>[IBAN]</strong> <strong>Company Ltd</strong>&quot;</li>
<li>&quot;Payment of invoice <strong>00034</strong>-<em>1120,34</em> on <em>02.17</em> [BIC] <strong>[IBAN]</strong> <strong>Company 2 inc</strong>.&quot;</li>
</ol>
<p>In case 1, I have:</p>
<ul>
<li>invoice numbers: 00234, 00435</li>
<li>IBAN - [IBAN]</li>
<li>counterparty - Company Ltd</li>
</ul>
<p>In case 2, I have:</p>
<ul>
<li>invoice number: 00034</li>
<li>IBAN - [IBAN]</li>
<li>counterparty - Company 2 inc</li>
</ul>
<p>I have also annotated invoice prefixes such as inv, INV, invoice, etc.</p>
<p>My question is, should I add additional annotations such as &quot;date&quot; (2022.01.13, 02.17) or &quot;sum paid&quot; (1120,34)? Could they be helpful for a transformer, for example, to find out what an invoice is?</p>
","transformer"
"114701","How do transformers differ from feature selection and regular machine learning?","2022-09-26 13:29:28","","2","570","<machine-learning><deep-learning><neural-network><transformer><attention-mechanism>","<p>This is perhaps a simplistic way of thinking, but to me transformers (attention based neural networks) focus on a subset of the input, learning what is important for the problem/prediction as the training goes on.</p>
<p>How does this differ from regular feature selection and neural network training on a subset of the input?</p>
","transformer"
"114670","One word changes everything NLP","2022-09-24 20:25:10","114672","0","65","<deep-learning><nlp><transformer><bert>","<p>I have a classification model (BERT) that classifies sentences as either question or normal sentences. But whenever a sentence has &quot;how&quot; word, the model chooses &quot;question&quot; class.
How can I solve this issue? (I have a very big dataset.)</p>
","transformer"
"114511","Inference Process in Autoregressive Transformer Architecture","2022-09-19 05:40:22","114519","1","509","<transformer><inference>","<p>I'm abit confused about how the inference/prediction process works in transformer.</p>
<p>For simplicity suppose it is a transformer for translation. My understanding is that when training, the whole input and output sentences are put into model. This is possible because of the causal mask in the decoder that prevents the model from cheating and looking ahead.</p>
<p><a href=""https://i.sstatic.net/PchHW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PchHW.png"" alt=""My understanding of Training Process"" /></a></p>
<p>Then, once the weights have been trained, the inference/prediction works by placing &lt;/s&gt; or start sentence tag in the beginning with padding. The predicted word is then concatenated until &lt;/s&gt; is the predicted word. My confusion arises from how the predicted word is acquired.</p>
<p>The causal mask ensures that the first predicted token (X_1 below) is only a function of the first token (i.e. is not affected by the padding we used in the other tokens. So our first predicted word/token should be taken from the first, and subsequently once we concatenated k words it should be taken from k+1 th output position. See the diagram below for clarity.</p>
<p><a href=""https://i.sstatic.net/ZZpuq.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZZpuq.png"" alt=""My understanding of Inference Process"" /></a></p>
<p>However, I've been using nlp.seas.harvard.edu/annotated-transformer/ as reference (and also checked another tensorflow tutorial), and they seem to take the predicted word/token as the last token (i.e. X_N).For example, under the inference section of the link above, we have:</p>
<pre><code>prob = test_model.generator(out[:, -1])         
_, next_word = torch.max(prob, dim=1)         
next_word = next_word.data[0]         
ys = torch.cat([ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1)
</code></pre>
<p>Thus, my question is whether I'm misunderstanding the model of misunderstanding the code?</p>
","transformer"
"114379","Threshold determination / prediction for cosine similarity scores","2022-09-13 07:41:14","114386","3","3646","<nlp><transformer><semantic-similarity>","<p>Given a query sentence, we search and find similar sentences in our corpus using transformer-based models for semantic textual similarity.</p>
<ul>
<li><p>For one query sentence, we might get 200 similar sentences with scores ranging from <strong>0.95 to 0.55</strong>.</p>
</li>
<li><p>For a second query sentence, we might get 200 similar sentences with scores ranging from <strong>0.44 to 0.27</strong>.</p>
</li>
<li><p>For a third query sentence, we might only get 100 similar sentences with scores ranging from <strong>0.71 to 0.11</strong>.</p>
</li>
</ul>
<p>In all those cases, is there a way to predict where our threshold should be without losing too many relevant sentences? Having a similarity score of <code>1.0</code> does not mean that two documents are 2X more similar than if the score was <code>0.5</code>. Is there a way to determine the <code>topk</code> (how many of the top scoring sentences we should return) parameter?</p>
","transformer"
"114346","Ignore or predict padding","2022-09-12 11:25:21","","1","238","<machine-learning><deep-learning><neural-network><transformer>","<p>I have a sequence to sequence classification model with two classes (similar to NER transformer) and because my data samples have different lengths I use padding. Is it better to use a custom loss function (with a masking layer/attention mask) that ignores padding, or use an extra class for padding and classify padding as well? Or it doesn't make any difference in terms of model quality?</p>
","transformer"
"114245","What could cause pre-trained Opus-MT models have wildly varying inference time when being used with transformers library?","2022-09-08 16:02:41","","1","82","<transformer><machine-translation><huggingface>","<p>I have been testing pre-trained Opus-MT models ported to transformers library for python implementation. Specifically, I am using <a href=""https://huggingface.co/Helsinki-NLP/opus-mt-en-fr"" rel=""nofollow noreferrer"">opus-mt-en-fr</a> for English to French translation. And the tokenizer and translation model is loaded via MarianTokenizer and MarianMTModels--similar to code examples shown <a href=""https://huggingface.co/docs/transformers/model_doc/marian"" rel=""nofollow noreferrer"">here</a> on huggingface.
Strangely, for the same pre-trained model translating the same English input on an identical machine, I have observed <em><strong>anywhere between 80+ ms and (whopping) 4 s per translation</strong></em> (example input = &quot;kiwi strawberry&quot;).</p>
<p>Wonder if anyone has observed similar behaviours, and what could cause such a wide variation?</p>
","transformer"
"114244","Multi head self attention output size for batches with different sequence length","2022-09-08 15:44:13","114258","0","1040","<transformer><attention-mechanism>","<p>I have a question regarding the self attention layer of transformers. When dealing with sequences of varying lengths in a mini-batch, we pad sequences so that all sequences in the batch have the same length.</p>
<p>Let's say that that most sequences in a dataset are &lt; 500 elements long, but there are a few very long sequences that can be 1000s of elements long. If I want to handle those very long sequences without truncating, will the multi-head self attention layer's size have to be tailored to the longest possible sequence even when input batches don't contain any of the long sequences?</p>
","transformer"
"114216","Is attention cache useful during transformer pretraining?","2022-09-07 17:06:01","","2","220","<transformer><nvidia>","<p>I am looking at the MegatronLM implementation, and the only thing that is cached are the results of <code>xK</code> and <code>xV</code> computation:</p>
<p><a href=""https://github.com/NVIDIA/Megatron-LM/blob/b44dca25727c294a7f825e74a3c4a53744cc8404/megatron/model/transformer.py#L339"" rel=""nofollow noreferrer"">https://github.com/NVIDIA/Megatron-LM/blob/b44dca25727c294a7f825e74a3c4a53744cc8404/megatron/model/transformer.py#L339</a></p>
<p>Which are then stacked with past values and still the full <code>QK</code> matrix is computed. The computation of keys and queries does not seem that expensive in comparison.</p>
","transformer"
"114169","How to further fine-tune a transformer NLP model on domain specific dataset, after general fine-tuning","2022-09-06 16:10:39","","1","263","<transformer><finetuning>","<p>I would like to fine-tune a pre-trained BERT-like model for a semantic similarity analysis task in the fashion of the SNLI/MNLI task (i.e. classify sentence pairs to &quot;entailment&quot; or &quot;contradiction&quot;). I have my own domain specific dataset but it is very small. For that reason I planned to fine-tune the model first on a large &quot;general&quot; dataset like the MNLI and then further fine-tune it on my own data. This way I want to take advantage of the large publicly available dataset and thus, hopefully improve results on my small dataset.</p>
<p>My questions here are:</p>
<ol>
<li>Does this approach make sense in general for language models? Or would phenomena like catastrophic forgetting break my plan anyway?</li>
<li>How do I design the training steps? Should I freeze some of the (additional) fine-tuned layers (apart from the embedding layers of course) or do I simply resume training from last checkpoint?</li>
</ol>
","transformer"
"114058","Dataset Format for fine tuning deepset/roberta-base-squad2 hugging face transformer model","2022-09-03 09:44:58","","1","380","<nlp><tensorflow><pytorch><transformer><huggingface>","<p>I have been trying to fine tune the roberta model for QnA to my specific domain (healthcare).
I am unable to find the correct way to provide the dataset format to the tokenizer in order to fine tune the model.</p>
<p>Sample Dataset format -&gt;</p>
<pre><code>train_data = (
{
    'context':'context for the training data',
    'answers':{'text':['answer 1'],'answer_start':[115],'answer_end':[138] },
    'question':'question1'
},
{
    'context':'context for the training data',
    'answers':{'text':['answer 2'],'answer_start':[115],'answer_end':[138] },
    'question':'question2'
},
{
    'context':'context for the training data',
    'answers':{'text':['answer 3'],'answer_start':[115],'answer_end':[138] },
    'question':'question3'
}
)
</code></pre>
<p>Can anyone help me with the correct format to provide to tokenizer?</p>
<p>Thanks in advance.</p>
","transformer"
"114008","Extend BERT or any transformer model using manual features","2022-09-01 08:11:37","","2","918","<transformer><bert><text-classification>","<p>I have been doing a thesis in my citation classifications. I just implemented Bert model for the classification of citations. I have 4 output classes and I give an input sentence and my model returns an output that tells the category of citation. Now my supervisor gave me another task.</p>
<p>You have to search that whether it is possible to extend BERT or any transformer model using manual features. e.g. You are currently giving a sentence as the only input followed by its class. What if you can give a sentence, and some other features as input; as we do in other classifiers?</p>
<p>I need some guidance about this problem. How can I add an extra feature in my Bert model and the feature would be categorical not numerical.</p>
<p>This is my code what I have done for implementation of BERT model
I want to add manual features in my code</p>
<p>I am using Bert Tokenizer</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
</code></pre>
<p>Then I am making dataset of inputs ids and attention masks of size 256</p>
<pre><code>X_input_ids = np.zeros((len(df), 256))
X_attn_masks = np.zeros((len(df), 256))
</code></pre>
<p>This is my function of tokenization of sentences</p>
<pre><code>def generate_training_data(df, ids, masks, tokenizer):
    for i, text in tqdm(enumerate(df['Citing Sentence'])):
        tokenized_text = tokenizer.encode_plus(
            text,
            max_length=256, 
            truncation=True, 
            padding='max_length', 
            add_special_tokens=True,
            return_tensors='tf'
        )
        ids[i, :] = tokenized_text.input_ids
        masks[i, :] = tokenized_text.attention_mask
    return ids, masks
</code></pre>
<p>Then I am generating input_ids and attention_masks</p>
<pre><code>from tqdm.auto import tqdm
X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)
</code></pre>
<p>Then I made set of size 4 because my output has 4 categories</p>
<ol>
<li>Related work</li>
<li>Comparison</li>
<li>Using the work</li>
<li>Extending the work</li>
</ol>
<p>One-hot encoded target tensor. Here follow-up is my output array array</p>
<pre><code>labels = np.zeros((len(df), 4))
labels[np.arange(len(df)), df['Follow-up'].values] = 1
dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))

def CitationDatasetMapFunction(input_ids, attn_masks, labels):
    return {
        'input_ids': input_ids,
        'attention_mask': attn_masks
    }, labels
</code></pre>
<p>converting to required format for tensorflow dataset</p>
<pre><code>dataset = dataset.map(CitationDatasetMapFunction)
</code></pre>
<p>batch size, drop any left out tensor</p>
<pre><code>dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)
</code></pre>
<p>for each 4 batch of data we will have len(df)//16 samples, take 80% of that for train.</p>
<pre><code>p = 0.8
train_size = int((len(df)/16)*p) 

train_dataset = dataset.take(train_size)
val_dataset = dataset.skip(train_size)
</code></pre>
<p>Summarising the model</p>
<pre><code>from transformers import TFBertModel
model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights

input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')
attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')

bert_embds = model(input_ids, attention_mask=attn_masks)[1] # 0 -&gt; activation layer (3D), 1 -&gt; pooled output layer (2D)
intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)
output_layer = tf.keras.layers.Dense(4, activation='softmax', name='output_layer')(intermediate_layer) # softmax -&gt; calcs probs of classes

citation_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)
citation_model.summary()
</code></pre>
","transformer"
"114007","Text cleaning when applying Sentence Similarity / Semantic Search","2022-09-01 08:01:24","114036","0","659","<nlp><data-cleaning><transformer><semantic-similarity>","<p>Do we need to apply text cleaning practices for the task of sentence similarity?</p>
<p>Most models are being used with whole sentences that even have punctuation. Here are two example sentences that we wish to compare using SentenceTransformer (<strong>all-MiniLM-L6-v2</strong>):</p>
<pre><code>sentences = [
    &quot;Oncogenic KRAS mutations are common in cancer.&quot;,
    &quot;Notably, c-Raf has recently been found essential for development of K-Ras-driven NSCLCs.&quot;] 

# yields that sentence 2 has a score of 0.191 when compared with sentence 1
</code></pre>
<p>Will cleaning those sentences change its semantic meaning?</p>
<pre><code>cleaned = ['oncogenic bras mutations common cancer', 
           'notably c-raf recently found essential development bras driven nsclcs.']

# yields that sentence 2 now has a score of 0.327 when compared to sentence 1
</code></pre>
<p>It seems the model works better when the text is cleaned. However, nowhere does it say that the input sentences are being / should be cleaned? Would love to know your takes on this.</p>
<hr />
","transformer"
"113810","How to use a `lr_scheduler` when you don't known how many training steps to do?","2022-08-24 11:45:27","","0","207","<bert><transformer><learning-rate>","<p>I am trying to fine-tune a BERT model, but instead of doing it a fix number of training step, I want to use a stalling policy and allow it to run until the model stalls for N evaluations. However, I was previously using the <code>transformers.get_linear_schedule_with_warmup</code>, which requires an explicit number of training steps.</p>
<p>Is there any other learning rate scheduler that I should use for this task?</p>
","transformer"
"113576","Bare minimum Transformer (Keras)","2022-08-16 12:27:41","","0","148","<keras><transformer>","<p>I'm experimenting with Transformers for a time-series prediction problem, where inputs are <span class=""math-container"">$k$</span>-length sequences and outputs are scalar values.</p>
<p>Looking at the <a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">Keras Transformer example</a>, my question is - what components of the example are actually required? Reproducing the Keras example below:</p>
<pre><code>def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;relu&quot;)(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res

def build_model(
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0,
    mlp_dropout=0,
):
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = layers.GlobalAveragePooling1D(data_format=&quot;channels_first&quot;)(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(n_classes, activation=&quot;softmax&quot;)(x)
    return keras.Model(inputs, outputs)
</code></pre>
<p>Looking at the code, what is the purpose of <code>LayerNormalization</code>?</p>
<p>[Edit] For anyone stumbling on this question, the version on Keras' <a href=""https://keras.io/examples/timeseries/timeseries_classification_transformer/"" rel=""nofollow noreferrer"">tutorial page</a> is different to if you google &quot;keras transformer tutorial&quot;. The example above was based on the google result. You will notice that LayerNormalization is before multi-head attention which doesn't make much sense to me.[\Edit]</p>
","transformer"
"113573","Running Model on both GPUs and CPUs","2022-08-16 11:41:48","113976","0","573","<deep-learning><pytorch><transformer><gpu><hpc>","<p>I have access to a hpc node, of 3 GPU and maximum of 38 CPU. I have a transformer model which I run of a single GPU at the moment, I want to utilize all the GPUs and CPUs.
I have seen couple of tutorial on Dataparrallel and DistributedDataParallel. They only mentioned how to  use multiple GPUs.</p>
<p>My questions are:</p>
<ol>
<li>Do I use  Dataparallel or DistributedDataParallel</li>
<li>How do I adapt my code run on the GPUs and CPUs simultaneously. Perhaps if I can get a tutorial link.</li>
<li>How to do I get the device ids.</li>
</ol>
","transformer"
"113530","how Can we add extra word embedding to the pytorch funnel transformer?","2022-08-15 10:30:55","","2","135","<nlp><pytorch><word-embeddings><transformer><text-classification>","<p>i was approaching NLP sequence classification problem (3 classes) using huggingface transformers (funnel-transformer/large)  and tensorflow.</p>
<p>first i created laserembedding like this :</p>
<pre><code>from laserembeddings import Laser
laser = Laser()
df = pd.read_csv(&quot;mycsv.csv&quot;)
embeds = laser.embed_sentences(df['text'].values, lang='en')
write_pickle_to_file('train.pkl', embeds )
</code></pre>
<h1>part 1  : Tensorflow version</h1>
<p>for data preparation i use code like below :</p>
<pre><code>
df['text']=temp['column1']+tokenizer.sep_token+temp['column2']+tokenizer.sep_token+temp['column3']

def encode_text(texts):
    enc_di = tokenizer.batch_encode_plus(
        texts, 
        padding='max_length',
        truncation=True,
        return_token_type_ids=True,
        pad_to_max_length=True,
        max_length=cfg.max_len
    )
    
    return [np.asarray(enc_di['input_ids'], dtype=np.int64), 
            np.asarray(enc_di['attention_mask'], dtype=np.int64), 
            np.asarray(enc_di['token_type_ids'], dtype=np.int64)]
</code></pre>
<p>then inside training function :</p>
<pre><code>
x_train = encode_text(df.text.to_list())
train_ds = (
      tf.data.Dataset
      .from_tensor_slices((
          {
              &quot;input_ids&quot;:      x_train[0], 
              &quot;input_masks&quot;:    x_train[1],
              &quot;input_segments&quot;: x_train[2], 
              &quot;lasers&quot;:         np.array( train[laser_columns].values, dtype=np.float32 ) #laser_columns contains all the laser embedded columns
          }, 
       
          tf.one_hot(df[&quot;label&quot;].to_list(), 3) #3 class
      ))
      .repeat()
      .shuffle(2048)
      .batch(cfg.batch_size)
      .prefetch(AUTO)
  )
</code></pre>
<p>i add laser embedding in my model like this :</p>
<pre><code>
def create_model():
    transformer = transformers.TFAutoModel.from_pretrained(cfg.pretrained,config=config,from_pt=True) 
    max_len=512
    # transformer
    input_ids      = Input(shape=(max_len,), dtype=&quot;int32&quot;, name=&quot;input_ids&quot;)
    input_masks    = Input(shape=(max_len,), dtype=&quot;int32&quot;, name=&quot;input_masks&quot;)
    input_segments = Input(shape=(max_len,), dtype=&quot;int32&quot;, name=&quot;input_segments&quot;)
    
    sequence_output = transformer(input_ids, attention_mask=input_masks, token_type_ids=input_segments)[0]

    cls_token = sequence_output[:, 0, :]
    
    # lasers
    lasers = Input(shape=(n_lasers,), dtype=tf.float32, name=&quot;lasers&quot;)  #n_lasers = 1024
    lasers_output = tf.keras.layers.Dense(n_lasers, activation='tanh')(lasers)

    x = tf.keras.layers.Concatenate()([cls_token, lasers_output])

    x = tf.keras.layers.Dropout(0.1)(x)
    x = tf.keras.layers.Dense(2048, activation='tanh')(x)
    x = tf.keras.layers.Dropout(0.1)(x)
    out = tf.keras.layers.Dense(3, activation='softmax')(x)
    
    model = Model(inputs=[input_ids, input_masks, input_segments, lasers], outputs=out)
    model.compile(Adam(lr=1e-5), loss=losses.CategoricalCrossentropy(), metrics=[&quot;acc&quot;, metrics.CategoricalCrossentropy(name='xentropy')])
    
    return model
</code></pre>
<p>now my question is, how do we do the same with pytorch for exact same problem and same dataset?</p>
<h1>part 2  : pytorch version</h1>
<pre><code>
df = pd.read_csv(&quot;mytrain.csv&quot;)
class myDataset(Dataset):
    def __init__(self,df, max_length, tokenizer, training=True):
        self.df = df
        self.max_len = max_length
        self.tokenizer = tokenizer
        self.column1 = self.df['column1'].values
        self.column2 = self.df['column2'].values
        self.column3= self.df['column3'].values
        self.column4= self.df['column4'].values
        self.training = training
        
        if self.training:
            self.targets = self.df['label'].values
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, index):
        column1 = self.column1[index]
        column2= self.column2[index]
        column3= self.column3[index]
        text0 = self.column4[index]
        text1 = column1  + ' ' + column2+ ' ' + column3

        
        inputs = self.tokenizer.encode_plus(
            text1 , 
            text0 ,
            truncation = True,
            add_special_tokens = True,
            return_token_type_ids = True,
            is_split_into_words=False,
            max_length = self.max_len
        )
        
        samples = {
            'input_ids': inputs['input_ids'],
            'attention_mask': inputs['attention_mask'],
        }
        
        if 'token_type_ids' in inputs:
            samples['token_type_ids'] = inputs['token_type_ids']
          
        if self.training:
            samples['target'] = self.targets[index]
        
        return samples
collate_fn = DataCollatorWithPadding(tokenizer=CONFIG['tokenizer'])

class myModel(nn.Module):
    def __init__(self, model_name):
        super(myModel, self).__init__()
        self.model = AutoModel.from_pretrained(model_name)
        if(True):
            print(&quot;using gradient_checkpoint...&quot;)
            self.model.gradient_checkpointing_enable()
        self.config = AutoConfig.from_pretrained(model_name)
       
        self.config.update(
            {
                &quot;output_hidden_states&quot;: True,
                &quot;hidden_dropout_prob&quot;: 0.0,
                &quot;layer_norm_eps&quot;: 1e-7,
                &quot;add_pooling_layer&quot;: False,
                &quot;attention_probs_dropout_prob&quot;:0.0,
            }
        )
        
        self.fc = nn.Linear(self.config.hidden_size, 3)
        
    def forward(self, ids, mask):        
        out = self.model(input_ids=ids,attention_mask=mask,output_hidden_states=False)
        out = out[0][:, 0, :]
        outputs = self.fc(out)
        return outputs
</code></pre>
<p>and in train and validation loop i have code like this :</p>
<pre><code>
bar = tqdm(enumerate(dataloader), total=len(dataloader))
for step, data in bar:
        ids = data['input_ids'].to(device, dtype = torch.long)
        mask = data['attention_mask'].to(device, dtype = torch.long)
        targets = data['target'].to(device, dtype=torch.long)
        
        batch_size = ids.size(0)
        optimizer.zero_grad()
        # forward pass with `autocast` context manager
        with autocast(enabled=True):
            outputs = model(ids, mask)
            loss = loss_fct(outputs, targets)
</code></pre>
<p>i would like to know where and how in my huggingface pytorch  pipeline i can use the laserembedding that i created earlier and used in tensorflow huggingface model?
i would like to concat laserembeddings with funnel transformer's simple CLS token output and train the transformers model with laser embed as extra feature in pytorch implementation exactly like i did in tensorflow example,do you know how to modify my pytorch code to make it working in pytorch? the tensorflow implementation with laserembedding concatenated above that i have posted here works good,i just wanted to do the same in pytorch implementation,,your help is highly appreciated,thanks in advance</p>
","transformer"
"113309","Creating class labels for custom DataSets efficiently (HuggingFace)","2022-08-07 19:33:44","113315","0","1409","<nlp><transformer><huggingface>","<p>I have pandas dataframes - test &amp; train,they both have <code>text</code> and <code>label</code> as columns as shown below -</p>
<pre><code> label       text
 fear        ignition problems will appear 
 joy         enjoying the ride

</code></pre>
<p>As usual, to run any Transformers model from the HuggingFace, I am converting these dataframes into <code>Dataset</code> class, and creating the classLabels (fear=0, joy=1) like this -</p>
<pre><code>from datasets import DatasetDict 

traindts = Dataset.from_pandas(traindf)
traindts = traindts.class_encode_column(&quot;label&quot;)

testdts = Dataset.from_pandas(testdf)
testdts = testdts.class_encode_column(&quot;label&quot;)
</code></pre>
<p>Finally these <code>Datasets</code> are put into <code>DatasetDict</code>like this-</p>
<pre><code>emotions = DatasetDict({
    &quot;train&quot; : traindts , 
    &quot;test&quot; : testdts 
})

</code></pre>
<p>Everything works well but as you see that the way I am doing it can be definitely improved. How can it be done more efficiently in less number of lines ?</p>
","transformer"
"113183","What Preprocessing is Needed for Semantic Search Using Pre-trained Hugging Face Transformers?","2022-08-02 12:08:18","113196","2","447","<nlp><dataset><preprocessing><transformer><huggingface>","<p>I am building a project for my bachelor thesis and am wondering how to prepare my raw data. The goal is to program some kind of semantic search for job postings. My data set consists of stored web pages in HTML format, each containing the detail page of a job posting. Via an interface I want to fill in predefined fields like skills, highest qualification, etc. with comma-separated sentences or words. These are then embedded via a Hugging Face Transformer and afterwards the similarity of the input is to be compared with the already embedded job postings and the &quot;best match&quot; is returned.</p>
<p>I have already found that intensive preprocessing such as stop word removal and lemmatization is not necessarily required for transformers. However, the data should be processed to resemble the data on which the pre-trained transformers learned. <strong>What would be the best way to prepare such a data set to fine-tune pre-trained Hugging Face Transformers?</strong></p>
<p>Additional info: 55,000 of the saved web pages contain an annotation scheme via which I could simply extract the respective sections &quot;Skills&quot; etc. from the HTML text. If that is not sufficient, I can use prodigy to further annotate the data, e.g. by span labeling texts within the text of the job postings.</p>
<p>Thank you very much in advance!</p>
","transformer"
"113177","Do I need training data in multiple languages for a multilingual transformer?","2022-08-02 09:36:49","","0","55","<machine-learning><nlp><transformer><language-model><huggingface>","<p>I am attempting to train a transformer which can categorize sentences into one of n categories. This model should be able to work with a number of different languages - English and Arabic in my case.</p>
<p>Do I need to have labelled training data in both English and Arabic to fine tune a pretrained transformer, such as <a href=""https://huggingface.co/docs/transformers/model_doc/bloom"" rel=""nofollow noreferrer"">BLOOM</a>, or can I fine tune the model using only English samples, and then the model should also work well on Arabic samples for my classificaiton task, since the fine tuning only trained the classification head?</p>
<p>My thoughts are that the pretraining of this model should allow it to transform the same input texts in English and Arabic to the same (or similar) embedding, which the classification head would have learned to then predict these embeddings accurately through the fine tuning.</p>
","transformer"
"113070","Transformers vs RNN basic doubt","2022-07-29 09:44:23","113075","0","123","<machine-learning><nlp><lstm><bert><transformer>","<p>I have a basic doubt. Kindly clarify this.</p>
<p>My doubt is, When we are using LSTM's, We pass the words sequentially and get some hidden representations.</p>
<p>Now transformers also does the same thing except non sequentially. But I have seen that the output of BERT based models can be used as word embeddings.</p>
<p>Why can't we use the output of LSTM also as a word embedding? I can find sentence similarity and all with LSTM also ?</p>
<p>For eg : If I have a sentence &quot; is it very hot out there&quot;</p>
<p>Now I will apply word2Vec and get dense representations and pass it to my LSTM model. The output of my LSTM can also be used as word embeddings as we do the same with BERT?</p>
<p>My understanding was that LSTM is used to identify dependencies between words and using that learned weights to perform classification/similar tasks.</p>
","transformer"
"112891","Smaller embedding size causes lower loss","2022-07-23 07:30:15","112892","0","204","<deep-learning><nlp><transformer><tokenization>","<p>When I convert my multilingual transformer model to a single lingual transformer model (got my languages embedding from the multilingual transformer and deleted other embeddings, decreased dimensions of embedding layers), the loss is much less. But I didn't understand why. What can be the reason for that?</p>
","transformer"
"112840","What to do with Transformer Encoder output?","2022-07-21 02:21:34","112846","3","3096","<neural-network><transformer><encoder><pooling>","<p>I'm in the middle of learning about Transformer layers, and I feel like I've got enough of the general idea behind them to be dangerous. I'm designing a neural network and my team would like to include them, but we're unsure how to proceed with the encoded sequences and what the right way to plug the them into the next layer in the model would be. We would like to process it such that we can plug the encoded sequence into a FC layer immediately after the Transformer Encoder.</p>
<p>If we just use a batch size of 1, for the sake of the argument, our encoded sequence output after being processed by the Transformer Encoder has shape tuple of (L,E), where L is the input sequence length and E is the embedded dimension size. I've seen some vague description of using some max/avg/conv1d pooling on the Encoded sequence, but nothing super clear about what that means. If I'm following this correctly, would I apply the max/avg/1conv1d pooling such that the pooling result gives me an resulting vector with shape tuple (E,), or would I pool along the other dimension?</p>
","transformer"
"112704","Transformers - Why Self Attention calculate dot product of q and k from of same word?","2022-07-16 08:59:33","112851","0","542","<transformer><attention-mechanism>","<p>As far as I understand and looked into <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a> and <a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""nofollow noreferrer"">Transformer model for language understanding</a>, the <strong>Self Attention</strong> at Scaled Dot-Product Attention is calculating <span class=""math-container"">$query$</span> and <span class=""math-container"">$key$</span> of the same word, which is the diagonal of the matrix in the diagram.</p>
<p>The <span class=""math-container"">$q \cdot k$</span> of the same word will generate the largest value, which means that <strong>a word attends to itself in a sentence</strong>. Is it correct or am I missing something?</p>
<p>Why Self Attention does not exclude the <span class=""math-container"">$q \cdot k$</span> of the same word itself?</p>
<p><a href=""https://i.sstatic.net/ZYcvL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ZYcvL.png"" alt=""enter image description here"" /></a></p>
","transformer"
"112626","SkLearn DecisionTree doesn't include numerical variables after one hot encoding pipeline","2022-07-13 21:24:06","","0","62","<scikit-learn><supervised-learning><transformer><pipelines>","<p>I'm trying to fit a dataframe with SkLearn DecisionTree with the following code.  But I get a error <code>Length of feature_names, 9 does not match number of features, 8</code>.  The DecisionTree seems to have only fitted categorical features after transformed by onehotencoding, not the numerical feature.  How can I include the numerical feature in the decisiontree model?</p>
<pre><code>import pandas as pd
import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn import tree
from matplotlib import pyplot as plt
import graphviz 
import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder,StandardScaler
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.linear_model import LinearRegression

df = pd.DataFrame({'brand'      : ['aaaa', 'asdfasdf', 'sadfds', 'NaN'],
                   'category'   : ['asdf','asfa','asdfas','as'], 
                   'num1'       : [1, 1, 0, 0] ,
                   'target'     : [1,0,0,1]})

df

dtarget=df['target']
dfeatures=df.drop('target', axis=1)


num = dfeatures.select_dtypes(include=[&quot;int64&quot;]).columns.tolist()
cat = dfeatures.select_dtypes(include=[&quot;object&quot;]).columns.tolist()


transformer = ColumnTransformer(
    transformers=[
        (&quot;cat&quot;, OneHotEncoder(),  cat),
    ]
)


clf= DecisionTreeClassifier(criterion=&quot;entropy&quot;, max_depth = 5)


pipe = Pipeline(steps=[
                ('onehotenc', transformer),
                ('decisiontree', clf)
                ])

#Fit the training data to the pipeline
pipe.fit(dfeatures, dtarget)



pipe.named_steps['onehotenc'].get_feature_names_out().tolist(), 

dot_data= tree.export_graphviz(clf,
                     out_file=None,
                     feature_names = num + pipe.named_steps['onehotenc'].get_feature_names_out().tolist(), 
                     class_names= ['1', '0'],
                     filled = True)
</code></pre>
","transformer"
"112438","How to get all 3 labels' sentiment from finbert instead of the most likely label's?","2022-07-06 07:23:47","112446","0","1381","<bert><transformer><sentiment-analysis><huggingface>","<p>I'm using bert to do sentiment analysis. I previous used cardiffnlp's twitter-roberta-base-sentiment, <a href=""https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment"" rel=""nofollow noreferrer"">https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment</a>.</p>
<p>It gives the the usage on its page.</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer
import numpy as np
from scipy.special import softmax
import csv
import urllib.request

# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []
 
 
    for t in text.split(&quot; &quot;):
        t = '@user' if t.startswith('@') and len(t) &gt; 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return &quot; &quot;.join(new_text)

# Tasks:
# emoji, emotion, hate, irony, offensive, sentiment
# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary

task='sentiment'
MODEL = f&quot;cardiffnlp/twitter-roberta-base-{task}&quot;

tokenizer = AutoTokenizer.from_pretrained(MODEL)

# download label mapping
labels=[]
mapping_link = f&quot;https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt&quot;
with urllib.request.urlopen(mapping_link) as f:
    html = f.read().decode('utf-8').split(&quot;\n&quot;)
    csvreader = csv.reader(html, delimiter='\t')
labels = [row[1] for row in csvreader if len(row) &gt; 1]

# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL)
model.save_pretrained(MODEL)

text = &quot;Good night 😊&quot;
text = preprocess(text)
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)

# # TF
# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)
# model.save_pretrained(MODEL)

# text = &quot;Good night 😊&quot;
# encoded_input = tokenizer(text, return_tensors='tf')
# output = model(encoded_input)
# scores = output[0][0].numpy()
# scores = softmax(scores)
</code></pre>
<p>It shows sentiments of all three labels, positive, neutral and negative.</p>
<p>However, I'm now trying to use Finbert from ProsusAI to do sentiment analysis <a href=""https://huggingface.co/ProsusAI/finbert"" rel=""nofollow noreferrer"">https://huggingface.co/ProsusAI/finbert</a>. It doesn't give me its usage on its page. So I'm following this tutorial <a href=""https://towardsdatascience.com/effortless-nlp-using-pre-trained-hugging-face-pipelines-with-just-3-lines-of-code-a4788d95754f"" rel=""nofollow noreferrer"">https://towardsdatascience.com/effortless-nlp-using-pre-trained-hugging-face-pipelines-with-just-3-lines-of-code-a4788d95754f</a>.</p>
<p>My code is</p>
<pre><code>from transformers import pipeline
classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert')
classifier('Stocks rallied and the British pound gained.')
</code></pre>
<p>However, the result is <code>[{'label': 'positive', 'score': 0.8983612656593323}]</code>. It only shows the sentiment of the most likely label's (positive). But I need all three labels' sentiment (positive, neutral and negative). How should I use it?</p>
","transformer"
"112402","What did Sentence-Bert return here?","2022-07-05 04:20:21","112405","0","305","<bert><transformer><information-retrieval><huggingface><information-extraction>","<p>I used sentence bert to embed sentences from this tutorial <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">https://www.sbert.net/docs/pretrained_models.html</a></p>
<pre><code>from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('all-mpnet-base-v2')
</code></pre>
<p>This is the event triples <code>t</code> I forgot to concat into sentences,</p>
<pre><code>[('U.S. stock index futures', 'points to', 'start'),
 ('U.S. stock index futures', 'points to', 'higher start')]
</code></pre>
<p><code>model.encode(t)</code> returns a 2d array of shape (2,768), with two idential 768-dimension vectors, and its value is different from both <code>model.encode('U.S. stock index futures')</code> and <code>model.encode('U.S. stock index futures points to start')</code>. What could possibly have it returned?</p>
<p>It is the same situation for other models on huggingface such as <a href=""https://huggingface.co/sentence-transformers/stsb-distilbert-base"" rel=""nofollow noreferrer"">https://huggingface.co/sentence-transformers/stsb-distilbert-base</a></p>
","transformer"
"112266","Is there practice to train language-to-code transformer (multi-modal transformer) using uni-modal pretrained models-transformers?","2022-06-30 08:46:55","","1","7","<machine-learning-model><transformer><pretraining>","<p>Language-to-code transformation/generation require multiple skills - language and reasoning skills to digest the core problem from the natural language specification. And programming language knowledge. There are separate pret-trained models for the language and code. And there are some multimodal language+code models (e.g. from stackoverflow, Github issues, etc.). My question is - is there traning of multimodal models that rely on the supervision by unimodal models solely?</p>
","transformer"
"112218","Visualize attention area","2022-06-28 22:27:58","112617","0","572","<visualization><computer-vision><transformer><attention-mechanism>","<p>I wonder how people draw a network's attention area on a single input.</p>
<p>Such as:</p>
<p><a href=""https://i.sstatic.net/khduJ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/khduJ.png"" alt=""enter image description here"" /></a></p>
<p>Any hint is much appreciated</p>
","transformer"
"112189","How does compute required scale with number of model parameters?","2022-06-28 06:48:23","","1","275","<deep-learning><transformer><gpu><gpt><scalability>","<p>GPT-3 has 175 billion parameters, required ~<span class=""math-container"">$3.114 * 10^{23}$</span> FLOPS, and took approximately one month to train on ~10k Tesla V100 GPUs. It seems commonly stated that the brain has the equivalent of ~100 trillion parameters. I was wondering what kind of compute would be required for training a transformer of this size. Would it simply be ~<span class=""math-container"">$10^3$</span> times more FLOPs?</p>
<p>In general, how does compute required scale with respect to model parameters for transformers, neural networks, CNNs, and other popular deep learning models?</p>
","transformer"
"112081","Using BERT embeddings as input for transformer architecture","2022-06-23 18:53:07","","0","331","<deep-learning><nlp><word-embeddings><bert><transformer>","<p>I will use BERT's embedding weights (as discussed <a href=""https://discuss.huggingface.co/t/how-to-get-embedding-matrix-of-bert-in-hugging-face/10261/4"" rel=""nofollow noreferrer"">here</a>) for embedding in embedding layers of the transformer model. But my question is: don't embeddings of BERT already go through the whole encoding layer and got that matrix? Why shouldn't I just remove-freeze the encoding layer and use BERT embedding vectors as input for the decoding layer? And also I will use BERT embeddings in the input of the decoding layer. Why should I not freeze attention layers in decoder layer too? Because embeddings of output text already have attention information?</p>
","transformer"
"112072","What are the inputs of encoder and decoder layers of transformer architecture?","2022-06-23 14:55:58","","1","497","<nlp><word-embeddings><bert><transformer><tokenization>","<p>In the paper (attention is all you need), it says &quot;embeddings&quot; are the input of the encoding layer. As I know embeddings are the numerical representation of words which is (for example) the output of bert model.</p>
<p>In the other hand, In BERT paper says the input of BERT is tokenized sentence <a href=""https://i.sstatic.net/b9IOH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/b9IOH.png"" alt=""here"" /></a>.</p>
<p>Since encoder part of the transformer is BERT, In transformer architecture(<a href=""https://i.sstatic.net/fpxwR.png"" rel=""nofollow noreferrer"">this</a>) is the input of the encoder &quot;tokenized sentence&quot; or &quot;embedded sentence&quot;?</p>
<p>In short: what is the input of the encoder and decoder layer in transformers? Please provide an example.</p>
<p>Thanks</p>
","transformer"
"112071","Mix of time-dependent and constant features for a transformer","2022-06-23 14:33:54","","0","86","<machine-learning><time-series><autoencoder><transformer>","<p>I'm using the transformer architecture to predict future time-points from previous time-points.<br />
Each item of the input sequence is a vector of <code>[ temperature, time, sunlight ]</code><br />
Each item of the output sequence is simply a <code>temperature</code>.</p>
<p>I want to also provide a contextual, non time-dependent feature input (e.g. <code>altitude</code>) to improve the prediction.</p>
<p>What's the best way to plug in that additional meta feature to the transformer? I've considered the following:</p>
<ol>
<li><p>Simply appending the constant feature to each item of the input sequence:<br />
<code>[ temperature, time, sunlight, altitude ]</code></p>
</li>
<li><p>Passing <code>altitude</code> through a couple of dense layers and using the output as a latent space representation to be inputted to one of the attention heads in every encoder layer. Basically replicating the decoder layers but with the <code>altitude</code> encoding as foreign input.</p>
</li>
<li><p>Passing <code>altitude</code> through a couple of dense layers and using the output to balance the output of the encoder layers.</p>
</li>
</ol>
<p>Is there an agreed upon best practice or available implementation for this use case?</p>
","transformer"
"112001","What is meant by averaging inhibits it in the paper 'Attention is All You Need'?","2022-06-21 09:38:56","","0","167","<machine-learning><deep-learning><transformer><machine-translation>","<p>Could anyone explain to me about the sentence below? What is meant by averaging inhibits it?</p>
<blockquote>
<p>Multi-head attention allows the model to jointly attend to information
from different representation subspaces at different positions. With a
single attention head, averaging inhibits this.</p>
</blockquote>
<p>Edit:
<a href=""https://i.sstatic.net/Q2Fce.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Q2Fce.jpg"" alt=""enter image description here"" /></a></p>
","transformer"
"111948","Model to implement Question Answering System over structured data","2022-06-19 13:03:11","","1","435","<machine-learning><nlp><bert><transformer><question-answering>","<p>I need to write a program(like a chatbot) that retrieves an answer from a CSV datafile based on a question user asks. So for example if the CSV stores list of products and its specifications in 5-10 columns, then if a user asks a question about specification Y for product X the program should return the correct answer based on CSV. I need to use NLP as the user can write synonyms of a particular word or ask a question a bit differently from the keywords in the dataset.</p>
<p>I think I am supposed to use BERT model using HuggingFace Transformer, but I'm not sure how to use NLP as this is over structured data. Additionally, I don't have a list of questions generated already.</p>
<p>Does anyone suggest how I should do this.</p>
<p>Also some of the specifications are values like prices. I was wondering if there is a way for the program to return the average or sum of two or more products if the user asks that question.</p>
","transformer"
"111747","Manipulating noise to get some data in right format and apply it to task using PPO","2022-06-12 18:21:19","112227","3","200","<machine-learning><python><nlp><reinforcement-learning><transformer>","<p><strong>Warning</strong>:<br>
I understand that my question may seem strange, stupid, and impossible, but let's just think about this interesting problem. I would not ask a question like: how to create an AGI in google colab. This is a real problem and I believe that it is possible to solve it. My question may seem strange, because I have little experience and maybe I have indicated something wrong. But I assure you, this is not complete nonsense. My actual task is much harder then task bellow, therefore to simplify question i have simplified problem<br><br>
<strong>I have RL task</strong>:<br>
My environment is python, agent is usual RL agent(it takes action like others RL agents), but i have no list of actions. Goal is writing the fastest python code for sorting.<br>Policy net(network which returns action) returns me sorting string(something like: &quot;[list1.pop(list1.index(min(list1))) for i in range(len(list1))]&quot;), i execute it through &quot;eval&quot;, get time of execution and use this time to form reward. But this task is easier, in my real task i have some variables and functions which model can use when produces sorting-strings. In our case it can be: &quot;list_1&quot;, &quot;some_function_which_helps_to_sort_list1_faster&quot;.<br><br>
<strong>That's how i'm going to get sorting-strings</strong>:<br>I know for sure i need code model. When i was looking for it i found <a href=""https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b"" rel=""nofollow noreferrer"">GPT-J</a>. GPT-J is usual transformer Decoder only model. First of all i create random initial(it's constant) noise. Policy net also produces noise. At the first time this(noise from policy net) is random noise, but over the time model will be trained better and the noise that policy net will produce will already be meaningful and will help to get normal sorting-strings. I add first initial noise to noise which i got from policy net, pass it through GPT-J and finally get sorting string. I gonna train model with many different initial noises, because logically if initial noises are different, model will: 1) be trained better 2)produce new &quot;the fastest&quot; results.
Entire approach looks like clip guided diffusion and i'm going to train it with PPO. As you remember, i have some variables that have to be in sorting strings. Therefore, there is a question: &quot;How to make policy net to add these variables into sorting strings?&quot;. I believe reward forming will help to solve it.
<br><br>
<strong>How reward will be formed</strong>:<br>
If policy net returns valid sorting string(which is valid python code and contains minimal set of variables i need(at least &quot;list1&quot;) to pass it through eval without errors) but it is more slower than previous best sorting-string, reward will be tiny(0.1). If policy net returns valid sorting string which is faster than previous best sorting string, reward will be huge(1). If policy net returns invalid sorting string(which is not valid python code or doesn't contain minimal set of variables), reward will be negative(-1).<br><br>
<strong>Thats how i'm going to train model. Bellow is how i'm going to use model at the inference time</strong>:<br>
First of all set initial noise. Then make the same like in training loop, but don't save weights(weights will be updated according PPO, all steps, which were in &quot;That's how i gonna get sorting-strings&quot; will be executed, but when i get result from final iteration, i won't save this new weights which i get in inference time and if i need to surpass previous the best result, i will run inference loop with new initial noise till i surpass this result.)<br><br></p>
<p><strong>What does here result from final iteration mean?</strong>:<br>
That's exactly like in clip guided diffusion. I set some variable n_steps. For example it will be equal to 1000. Here i make 1000 calls to policy net, 1000 times update policy weights(if it's training time, at the inference time i also update weights but keep them in RAM memory and don't save)... And when i get final result at 1000th iteration, that means for me result from final iteration.</p>
<p><strong>Question</strong>:<br>
Is my approach of implementing this problem right?
How would you implement my problem?
If you have some helpful tips for me(maybe you have some links which will help me, may be i wrong form reward...; here i meant anything which might be helpful for me), don't hesitate to share it with me.</p>
","transformer"
"111740","LSTM as learned positional encoding for vor variable sequence length input","2022-06-12 12:25:39","111758","1","848","<machine-learning><time-series><lstm><transformer><sequence>","<p>I'm solving a classification task on a time-series dataset. <br />
I use a Transformer encoder with learned positional encoding in the form of a matrix of shape
<span class=""math-container"">$\mathbb{R}^{seq \times  embedding}$</span>.<br />
Naturally, this leads to the fact that the sequence length that the model can process becomes fixed.
I had an idea to do learned positional encoding with LSTM. <br />
I.e., we project a sequence of tokens with a linear layer onto an embedding dimension, then feed the embeddings to LSTM layer and then add hidden states to the embedding. <br />
<span class=""math-container"">$x = MLP(x)$</span><br />
<span class=""math-container"">$x = x + LSTM(x)$</span></p>
<p>Do you think this will have the right effect?<br />
Are there any things to consider?</p>
","transformer"
"111322","Which model is better able to understand the difference that two sentences are talking about different things?","2022-05-26 10:11:25","","1","42","<deep-learning><nlp><word-embeddings><transformer><semantic-similarity>","<p>I'm currently working on the task of measuring semantic proximity between sentences. I use fasttext train _unsiupervised (skipgram) for this. I extract the sentence embeddings and then measure the cosine similarity between them. however, I ran into the following problem: cosine similarity between embeddings of these sentences:</p>
<p><code>&quot;Create a documentation of product A&quot;; &quot;he is creating a documentation of product B&quot;</code></p>
<p>is very high (&gt;0.9). obviously it because both of them is about creating a documentation. but however the first sentence is about product A and second is about product B and I would like my model to understand that and emphasise on those product names since they are key words in sentences. Which type of model would be more suitable for my case? Is BERT for example better for it and why?</p>
","transformer"
"111309","Could Attention_mask in T5 be a float in [0,1]?","2022-05-25 22:32:14","","1","193","<deep-learning><nlp><transformer><attention-mechanism><huggingface>","<p>I was inspecting T5 model from hf <a href=""https://huggingface.co/docs/transformers/model_doc/t5"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/t5</a> . attention_mask is presented as</p>
<pre><code>attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) — Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:
1 for tokens that are not masked,
0 for tokens that are masked. 
</code></pre>
<p>I was wondering whether it could be used something &quot;softer&quot; not only selecting the not-padding token but also selecting &quot;how much&quot; attention should be used on every token.</p>
<p>This question is related to the one proposed here <a href=""https://datascience.stackexchange.com/questions/94517/can-the-attention-mask-hold-values-between-0-and-1/111303#111303"">Can the attention mask hold values between 0 and 1?</a></p>
<p>Do you know if such attention_mask vector is used in any other ways where a non integer value could harm the model?</p>
<p>Thank you for your precious time and advices.</p>
","transformer"
"111013","ValueError: Mixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16) can only be used on CUDA devices","2022-05-17 08:24:56","","0","1770","<deep-learning><transformer><gpu><finetuning><cuda>","<p>i’m fine tuning the wav2vec-xlsr model. i’ve created a virtual env for that and i’ve installed cuda 11.0 and tensorflow-gpu==2.5.0 but it gives the following error : ValueError: Mixed precision training with AMP or APEX (--fp16 or --bf16) and half precision evaluation (--fp16_full_eval or --bf16_full_eval) can only be used on CUDA devices.
i want to fine tune the model on GPU ANY HELP ?<img src=""https://i.sstatic.net/YF7mH.png"" alt=""enter image description here"" /></p>
","transformer"
"110980","Pretrained vs. finetuned model","2022-05-16 09:05:51","111010","1","243","<transformer><transfer-learning><finetuning><pretraining>","<p>I have a doubt regarding terminology. When dealing with huggingface transformer models, I often read about &quot;using pretrained models for classification&quot; vs. &quot;fine-tuning a pretrained model for classification.&quot;</p>
<p>I fail to understand what the exact difference between these two is. As I understand, pretrained models by themselves cannot be used for classification, regression, or any relevant task, without attaching at least one more dense layer and one more output layer, and then training the model. In this case, we would keep all weights for the pretrained model, and only train the last couple of custom layers.</p>
<p>When task is about finetuning a model, how does it differ from the aforementioned case? Does finetuning also include reinitializing the weights for the pretrained model section, and retraining the entire model?</p>
","transformer"
"110877","Can i use Transformer-XL for text classification task?","2022-05-12 08:24:51","","0","179","<deep-learning><nlp><tensorflow><transformer><text-classification>","<p>I want to use transformer xl for text classification tasks. But I don't know the architect model for the text classification task. I use dense layers with activation softmax for logits output from the transformer xl model, but this doesn't seem right. when training I see accuracy is very low.</p>
<p>Output of my model:</p>
<p><a href=""https://i.sstatic.net/F7QFy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/F7QFy.png"" alt=""output, logits transformer-xl"" /></a></p>
<p>My training step:</p>
<p><a href=""https://i.sstatic.net/pgmk1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pgmk1.png"" alt=""dense layer"" /></a></p>
","transformer"
"110865","BERT base uncased required gpu ram","2022-05-11 15:28:37","110879","1","2157","<bert><transformer><gpu>","<p>I'm working on an NLP task, using BERT, and I have a little doubt about GPU memory.</p>
<p>I already made a model (using DistilBERT) since I had out-of-memory problems with tensorflow on a RTX3090 (24gb gpu's ram, but ~20.5gb usable) with BERT base model.</p>
<p>To make it working, I limited my data to 1.1 milion of sentences in training set (truncating sentences at 128 words), and like 300k in validation, but using an high batch size (256).</p>
<p>Now I have the possibility to retrain the model on a Nvidia A100 (with 40gb gpu's ram), so it's time to use BERT base, and not the distilled version.</p>
<p>My question is, if I reduce the batch size (e.g. from 256 to 64), will I have some possibilities to increase the size of my training data (e.g. from 1.1 to 2-3 milions), the lenght of sentences (e.g. from 128 to 256, or 198) and use the bert base (which has a lot of trainable params more than distilled version) on the 40gb of the A100, or it's probably that I will get an OOM error?</p>
<p>I ask this because I haven't unlimited tries on this cluster, since I'm not alone using it (plus I have to prepare data differently in each case, and it has a quite high size), so I would have an estimation on what could happen.</p>
","transformer"
"110576","Transformer time series classification using time2vec positional embedding","2022-05-02 21:55:26","111421","2","2734","<keras><transformer><embeddings>","<p>I want to use a transformer model to do classification of fixed-length time series. I was following along <a href=""https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3"" rel=""nofollow noreferrer"">this tutorial using keras</a> which uses time2vec as a positional embedding. According to the original <a href=""https://arxiv.org/pdf/1907.05321.pdf"" rel=""nofollow noreferrer"">time2vec paper</a> the representation is calculated as <span class=""math-container"">$$ \boldsymbol{t2v}(\tau)[i] = 
\begin{cases}
    \omega_i \tau + \phi_i,&amp; i = 0\\
    F(\omega_i \tau + \phi_i), &amp; 1 \leq i \leq k
\end{cases} $$</span></p>
<p>The mentioned tutorial simply concatenates this embedding with the input. Now, I understand the intention of the original time2vec paper, that you generate different possible periods and scales of a given time variable <span class=""math-container"">$ \tau $</span> and essentially put that into one vector. The mentioned tutorial, however, applies the given formula on the input directly. So <span class=""math-container"">$ \tau $</span> is not the time variable or index, but the input value at that time. I have found this way of implementing time2vec for transformer models in other projects on github as well. But by doing this, you are not really adding any positional information, or are you? Because the &quot;positional embedding&quot; only depends on the value at that time, not the time itself. Am I misunderstanding or missing something here?</p>
","transformer"
"110454","How do i generate text from ids in Torchtext's sentencepiece_numericalizer?","2022-04-28 14:05:11","110477","0","246","<python><nlp><pytorch><bert><transformer>","<p>The torchtext <code>sentencepiece_numericalizer()</code> outputs a generator with indices SentencePiece model corresponding to token in the input sentence. From the generator, I can get the ids.</p>
<p>My question is how do I get the text back after training?</p>
<p>For example</p>
<pre><code>&gt;&gt;&gt; sp_id_generator = sentencepiece_numericalizer(sp_model)
&gt;&gt;&gt; list_a = [&quot;sentencepiece encode as pieces&quot;, &quot;examples to   try!&quot;]
&gt;&gt;&gt; list(sp_id_generator(list_a))
    [[9858, 9249, 1629, 1305, 1809, 53, 842],
     [2347, 13, 9, 150, 37]]
</code></pre>
<p>How do I convert <code>list_a</code> back t(i.e <code>&quot;sentencepiece encode as pieces&quot;, &quot;examples to try!&quot;</code>)?</p>
","transformer"
"110310","Should I pretrain my BERT model on specific dataset if it has only one class of labels?","2022-04-24 12:48:40","","0","112","<machine-learning><deep-learning><nlp><bert><transformer>","<p>I want to use BERT model for sentences similarity measuring task. I know that BERT models were trained with natural language inference architecture with dataset with labels neutral, entailment, contradiction.</p>
<p>My data to which I want to apply BERT for sentences similarity task has very specific terms and jargon, so I want to pretrain model on it before. But in that data there are only cases of entailment labels (about 20k rows). Is it a good idea to pretrain model on that data? How could I handle my problem the best way?</p>
<p>Thanks in advance</p>
","transformer"
"110194","Guide to Natural language Prompt programming for few-shot learning of Pretrained Language Models","2022-04-19 18:43:36","","1","56","<deep-learning><nlp><transformer><text-generation><gpt>","<p>I'm currently working on a project with the goal of producing AI content in the space of a content generation like blog writing, Instagram caption generation etc. Found the in-context few-shot learning capabilities of the GPT-3 quite useful but I'm unable to generate creative content consistently. It becomes boring and repetitive in nature after a few iterations. I came across the concept of knowledge probing of language models and have come to this understanding that writing better prompts can actually solve my problem.</p>
<p>Can the community guide me to the right set of papers or other articles/blogs that expand on this idea further? so that I can make some progress on this interesting use case. Thanks, regards!.</p>
","transformer"
"110180","Why can't positions in transformers be simply appended to the input to preserve the positional information instead of using positional encodings?","2022-04-19 11:36:57","","1","894","<transformer>","<p>I saw in an intro to transformers in <a href=""https://youtu.be/dichIcUZfOw"" rel=""nofollow noreferrer"">this video</a> that positional encodings need to be used to preserve positional information, otherwise word order may not be understood by the neural network. They also explained why we cannot simply add positions to the original inputs or add other functions of positions to the input. However, what if we simply appended positions to the original input to preserve positional information? In fact, if directly  using positional numbers does not suffice, couldn't they also just append positional encodings (containing the sine and cosine functions of differing frequencies) to the input? This would also solve the problem of loss of information which happens when one adds the positional encodings to the original input.</p>
","transformer"
"109790","Comparison between applications of vanilla transformer and BERT","2022-04-08 03:14:40","109796","1","229","<bert><transformer>","<p>I try to identify applications of vanilla transformer in nlp, as well as those in BERT. But I don't seem to find good summaries for either of them. Thus my questions are:</p>
<ol>
<li>what are the applications of transformer and bert respectively?</li>
<li>in (1), why in some application vanilla transformer is used over BERT? (or vice versa?) What're the reasons?</li>
</ol>
<p>TIA.</p>
","transformer"
"109691","1D Sequence Classification","2022-04-05 14:38:50","","2","137","<lstm><rnn><multiclass-classification><transformer><sequence>","<p>Cross-post from <a href=""https://stackoverflow.com/questions/71752744/1d-sequence-classification"">https://stackoverflow.com/questions/71752744/1d-sequence-classification</a></p>
<p>I am working with a long sequence (~60 000 timesteps) classification task with continuous input domain. The input has the shape <code>(B, L, C)</code> where <code>B</code> is the batch size, <code>L</code> is the sequence length (i.e. timesteps) and <code>C</code> is the number of features where each feature is continuous (i.e. values like 0.6, 0.2, 0.5, 1.3, etc.).</p>
<p>Since the sequence is very long, I can't directly apply an RNN or Transformer Encoder layer without exceeding memory limits. Some proposed methods use several CNN layers to &quot;downsample&quot; the sequence length before feeding it into an RNN model. A successful example of this includes the CNN-LSTM model. By introducing several subsequent Convolutional blocks followed by max-pooling it is possible to &quot;downsample&quot; the sequence length by a given factor. The sampled sequence would instead have a sequence length of 60 timesteps for instance, which is much more manageable for an <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"" rel=""nofollow noreferrer"">LSTM</a> model.</p>
<p>Does it make sense to directly substitute the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"" rel=""nofollow noreferrer"">LSTM</a> model with a <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html"" rel=""nofollow noreferrer"">Transformer encoder</a>? I have read that the transformer attention mechanism can complement the LSTM layers and be used in succession.</p>
<p>There also exist many variants of Transformers and other architectures designed for handling long sequences. Latest examples include Performer, Linformer, Reformer, Nyströmformer, <a href=""https://arxiv.org/abs/2007.14062"" rel=""nofollow noreferrer"">BigBird</a>, FNet, <a href=""https://arxiv.org/abs/2111.00396"" rel=""nofollow noreferrer"">S4</a>, <a href=""https://arxiv.org/abs/2201.02143"" rel=""nofollow noreferrer"">CDIL-CNN</a>. Does there exist a library similar to <code>torchvision</code> for implementing these models in pytorch without copy-pasting large amounts of code from the respective repositories?</p>
","transformer"
"109689","Do the multiple heads in Multi head attention actually lead to more parameters or different outputs?","2022-04-05 12:51:01","","0","870","<pytorch><transformer><attention-mechanism>","<p>I am trying to understand Transformers. While I understand the concept of the encoder-decoder structure and the idea behind self-attention what I am stuck at is the &quot;multi head part&quot; of the &quot;MultiheadAttention-Layer&quot;.</p>
<p>Looking at this explanation <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">https://jalammar.github.io/illustrated-transformer/</a>, which I generally found very good, it appears that multiple weight matrices (one set of weight matrices per head) are used to transform the original input value into the <code>query</code>, <code>key</code> and <code>value</code>, which are then used to calculate the attention scores and the actual output of the MultiheadAttention layer. I also understand the idea of multiple heads to the individual attention heads can focus on different parts (as depicted in the link).</p>
<p>However, this seems to contradict other observations I have made:</p>
<ol>
<li>In the original paper <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1706.03762</a>, it is stated that the input is split into parts of equal size per attention head.</li>
</ol>
<p>So, for example I have:</p>
<pre><code>batch_size = 1
sequence_length = 12
embed_dim = 512 (I assume that the dimension for ```query```, ```key``` and ```value``` are equal)
Then the shape of my query, key and token would each be [1, 12, 512]
We assume we have two heads, so num_heads = 2
This results in a dimension per head of 512/2=256. According to my understanding this should result in the shape [1, 12, 256] for each attention head.
</code></pre>
<p>So, am I correct in assuming that this depiction <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">https://jalammar.github.io/illustrated-transformer/</a> just does not display this factor appropriately?</p>
<ol start=""2"">
<li>Does the splitting of the input into different heads actually lead to different calculations in the layer or is it just done to make computations faster?</li>
</ol>
<p>I have looked at the implementation in <code>torch.nn.MultiheadAttention</code> in <code>pytorch</code> and printed out the shapes at various stages during the forward pass through the layer. To me it appears that the operations are conducted in the following order:</p>
<ol>
<li>Use the <code>in_projection</code> weight matrices to get the <code>query</code>, <code>key</code> and <code>value</code> from the original inputs. After this the shape for <code>query</code>, <code>key</code> and <code>value</code> is [1, 12, 512]. From my understanding the weights in this step are the parameters that are actually learned in the layer during training.</li>
<li>Then the shape is modified for the multiple heads into [2, 12, 256].</li>
<li>After this the dot product between <code>query</code> and <code>key</code> is calculated, etc.. The output of this operation has the shape [2, 12, 256].</li>
<li>Then the output of the heads is concatenated which results in the shape [12, 512].</li>
<li>The attention_output is multiplied by the output projection weight matrices and we get [12, 1, 512] (The batch size and the sequence_length is sometimes switched around). Again here we have weights that are being trained inside the matrices.</li>
</ol>
<p>I printed the shape of the parameters in the layer for different <code>num_heads</code> and the amount of the parameters does not change:</p>
<ol>
<li>First parameter: [1536,512] (The input projection weight matrix, I assume, 1536=3*512)</li>
<li>Second parameter: [1536] (The input projection bias, I assume)</li>
<li>Third parameter: [512,512] (The output projection weight matrix, I assume)</li>
<li>Fourth parameter: [512] (The output projection bias, I assume)</li>
</ol>
<p>On this website <a href=""https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853"" rel=""nofollow noreferrer"">https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853</a>, it is stated that this is only a &quot;logical split&quot;. This seems to fit my own observations using the pytorch implementation.</p>
<p>So does the number of attention heads actually change the values that are outputted by the layer and the weights learned by the model? The way I see it, the weights are not influenced by the number of heads.
Then how can multiple heads focus on different parts (similar to the filters in convolutional layers)?</p>
<p>I also initialized a MultiheadAttention layer with weights which are all equal to one and the number of heads did not influence the result.</p>
<p>Thanks in advance for any advice!</p>
","transformer"
"109554","Incorporating structural information in a Transformer?","2022-03-31 21:55:33","","1","91","<machine-learning><deep-learning><transformer><graph-neural-network>","<p>For a Neural Machine Translation (NMT) task, my input data has relational information. This relation could be modelled using a graphical structure.</p>
<p>So one approach could be to use Graph Neural Network (GNN) and use a Graph2Seq model. But I can't find a good generational model for GNN.</p>
<p>Instead, I want to use Transformer. But then the challenge is how can I embed structural information there? Is there any open source artefact for Relational Transformer that I can use out of the box?</p>
","transformer"
"109200","Proof that multihead works better than single head in transformer","2022-03-20 03:15:34","109205","2","137","<nlp><transformer>","<p>According to <a href=""https://datascience.stackexchange.com/questions/96345/why-multiple-attention-heads-learn-differently"">this</a> post, the purpose of the multihead is to have 'gradient splitting' across heads, which is achieved by random initialization of weight matrices for Q, K and V in each head. But how can we prove this can solve the problems in using single head?</p>
<p>Specifically, how can the splitting of gradients ensures within each output attention vector for each word it wouldn't overemphasize (the attention) of itself?</p>
","transformer"
"109197","What makes differences in each head in the multiheaded attention in transformer?","2022-03-20 02:22:02","109204","0","320","<nlp><transformer>","<p>What makes differences in each head in the multiheaded attention in transformer?</p>
<p>As they are fed and trained in the exact same way, except the initialization of weights are different for each head to produce different sets of (Q,K,V) in each head. Such multi-headed design to me seems no difference than ensembling multiple models that are initialized differently.</p>
<p>Many sources claim that the multi-head attention 'can help capture meaning in different contextual subspace' without further substantiation or supporting proofs. Honestly I've been quite fed up with all those vague descriptions in the data science world that they make claim without mathematical rigor. I think I'm looking for more rigorous explanation as to why &quot;multi-head attention 'can help capture meaning in different contextual subspace'&quot; when they are simply an ensemble of identical models but weights randomly initialized?</p>
","transformer"
"109169","The separate of K and V is redundant in transformer?","2022-03-19 04:19:27","","0","31","<nlp><transformer>","<p>imho, I think the separate of K and V is redundant in transformer, as they are basically the same regardless in encoder self-attention, or decoder self-attention, or even the encoder-decoder attention. Can anyone counter-argue with me, based on the mathematical computation where K, V, Q are done in the training, to disprove my claim?</p>
<p>I would further argue that the preservation of Q, K, V in transformer is just a gimmick to match with the status quo of information retrieval, which adds little sense and brings much confusion to the learners of transformer.</p>
","transformer"
"109118","Add features to linear layer of transformer","2022-03-16 23:39:56","","2","119","<deep-learning><neural-network><transformer>","<p>I'm currently using a transformer for a binary text classification task. Besides the raw text I would also like to add some other handpicked features like text length, text date, text author etc.
I was thinking that I could just add these features to the vector just before the linear layer at the end of the transformer. So I would just append the handpicked features to the text embedding.</p>
<p>This seems quite logical to me but I can't really find any sources referencing this topic. Is the described method indeed the way to add additional features to a transformer? Or is there some other way?</p>
<p>Thanks in advance!</p>
","transformer"
"109012","ValueError: Exception encountered when calling layer ""transformer"" (type Transformer)","2022-03-13 15:57:13","","0","805","<python><keras><transformer>","<p>So I code a Transformers neural network that works as an ASR, it works, it trains good and saved the model as...</p>
<pre><code>model.save(&quot;savedmodel.model&quot;)
</code></pre>
<p>The problem is that when I want to predict, I do this..</p>
<pre><code>speech_model = load_model('D:\DOT\Speechrecognition\speechrecognitionE.model')

path = &quot;D:\DOT\Speechrecognition\Data\LJSpeech-1.1\wavs\LJ001-0001.wav&quot;

def path_to_audio(path):
    # spectrogram using stft
    audio = tf.io.read_file(path)
    audio, _ = tf.audio.decode_wav(audio, 1)
    audio = tf.squeeze(audio, axis=-1)
    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)
    x = tf.math.pow(tf.abs(stfts), 0.5)
    # normalisation
    means = tf.math.reduce_mean(x, 1, keepdims=True)
    stddevs = tf.math.reduce_std(x, 1, keepdims=True)
    x = (x - means) / stddevs
    audio_len = tf.shape(x)[0]
    # padding to 10 seconds
    pad_len = 2754
    paddings = tf.constant([[0, pad_len], [0, 0]])
    x = tf.pad(x, paddings, &quot;CONSTANT&quot;)[:pad_len, :]
    return x

x = path_to_audio(path)
#print(x)
speech_model.predict(x)
</code></pre>
<p>The path to audio function, converts the audio path to an spectrogram, in the training model it receive audio spectrograms as inputs, but it show this error..</p>
<pre><code>    Traceback (most recent call last):
File &quot;C:\Users\berna\Desktop\Programming\AI_ML_DL\Projects\DOT\DOT-alpha.py&quot;, line 72, in &lt;module&gt;
    speech_model.predict(x)
File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\tensorflow\python\framework\func_graph.py&quot;, line 1129, in autograph_handler
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1621, in predict_function  *
        return step_function(self, iterator)
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1611, in step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1604, in run_step  **
        outputs = model.predict_step(data)
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\engine\training.py&quot;, line 1572, in predict_step
        return self(x, training=False)
    File &quot;C:\Users\berna\AppData\Roaming\Python\Python39\site-packages\keras\utils\traceback_utils.py&quot;, line 67, in error_handler
        raise e.with_traceback(filtered_tb) from None

    ValueError: Exception encountered when calling layer &quot;transformer&quot; (type Transformer).

    Could not find matching concrete function to call loaded from the SavedModel. Got:
    Positional arguments (2 total):
        * Tensor(&quot;inputs:0&quot;, shape=(None, 129), dtype=float32)
        * False
    Keyword arguments: {}

    Expected these arguments to match one of the following 4 option(s):

    Option 1:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='inputs/1')]
        * False
    Keyword arguments: {}

    Option 2:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='inputs/1')]
        * True
    Keyword arguments: {}

    Option 3:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='input_2')]
        * False
    Keyword arguments: {}

    Option 4:
    Positional arguments (2 total):
        * [TensorSpec(shape=(None, None, 129), dtype=tf.float32, name='input_1'), TensorSpec(shape=(None, 199), dtype=tf.int32, name='input_2')]
        * True
    Keyword arguments: {}

    Call arguments received:
    • args=('tf.Tensor(shape=(None, 129), dtype=float32)',)
    • kwargs={'training': 'False'}
</code></pre>
<p>What does that means? what is wrong with the prediction?</p>
","transformer"
"108875","No feedback in Transformers","2022-03-08 12:15:19","","0","31","<transformer>","<p>Newbie question about transformers.</p>
<p>I am referring to the paper <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1706.03762.pdf</a> .</p>
<p>Figure 1 (bottom-right) says: &quot;Outputs (shifted right)&quot;.
To me, during generation (not training),
the n-th output of the network seems to be computed by consuming the n-1-th output,
and possibly earlier output entries as well.
Is my understanding correct?</p>
<p>If this is the case, that would be a feedback, no?
But then why transformers are said to be feed-forward?
Is it because the output is &quot;read-only&quot; during training?
Is it correct to say that transformers have feedback at generation time?</p>
","transformer"
"108595","Using KerasClassifier for training neural network","2022-02-27 20:39:43","","0","1783","<machine-learning><keras><nlp><bert><transformer>","<p>I created a simple neural network for binary spam/ham text classification using pretrained BERT transformer. The current pure-keras implementation works fine. I wanted however to plot certain metrics of the trained model, in particular the ROC curve. According to <a href=""https://www.dlology.com/blog/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier/"" rel=""nofollow noreferrer"">this blog post</a> I understood that this is only be possible using <code>KerasClassifier()</code> from the <code>keras.wrappers.scikit-learn</code> package which is now deprecated and has been replaced by the <code>scikeras</code> package.</p>
<p>Thus I created a <code>build_keras_nn()</code> function to build my custom BERT-based neural network. I then passed this custom function to <code>KerasClassifier()</code> as shown <a href=""https://www.adriangb.com/scikeras/stable/quickstart.html"" rel=""nofollow noreferrer"">in the documentation</a>, and fitted the model using train data.</p>
<p>At this point I got the following error message:</p>
<pre><code>ValueError: Expected 2D array, got 1D array instead: Reshape your data either using array.reshape(-1, 1) 
if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>
<p>Alright then, I thought a simple array reshape will work but I then ran into the following error:</p>
<pre><code>ValueError: could not convert string to float: 'awful bio obviously hatchet job press release totally 
biased bad grammar'
</code></pre>
<p>So for some reason the <code>KerasClassifier</code> implementation doesn't allow me to directly input text, even though my preprocessing steps are included within the custom function <code>build_keras_nn()</code>.</p>
<p>A full reproducible code is below:</p>
<pre><code>import tensorflow_hub as hub 
import tensorflow_text as text
import tensorflow as tf 
from tensorflow.keras.layers import Input, Dropout, Dense
from tensorflow.keras.metrics import BinaryAccuracy, AUC

bert_encoder_url = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;
bert_preprocessor_url = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;

bert_preprocessor_model = hub.KerasLayer(bert_preprocessor_url)
bert_encoder_model = hub.KerasLayer(bert_encoder_url)

df_ = pd.read_json(spam_ham_json)   # spam_ham_json: data in JSON file as a string

X_train_, X_test_, y_train_, y_test_ = train_test_split(df_['comment_text'], df_['label'])

def build_keras_nn():

    text_input = Input(shape=(), dtype=tf.string, name=&quot;text&quot;)
    preprocessed_text = bert_preprocessor_model(text_input)
    bert_output = bert_encoder_model(preprocessed_text)
    dropout = Dropout(0.1, name='dropout')(bert_output['pooled_output'])
    classification_output = Dense(1, activation='sigmoid', name='classification_output')(dropout)

    model = tf.keras.Model(inputs=[text_input], outputs=[classification_output])

    metrics_list = [AUC(name='auc'), BinaryAccuracy(name='accuracy')]
    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = metrics_list)
    return model 

# Following two rows show the pure-keras implementation: this one works.
# model = build_keras_nn()
# history = model.fit(X_train_, y_train_, epochs=5);

# Now let's see the KerasClassifier
model = KerasClassifier(build_fn=build_keras_nn)
# history = model.fit(X_train_, y_train_, epochs=5);     # &lt;-- Value Error 1
# history = model.fit(np.array(X_train_).reshape(-1,1), np.array(y_train_).reshape(-1,1), epochs=5);     # &lt;-- Value Error 2

</code></pre>
<p>Data is in json format:</p>
<pre><code>'{&quot;comment_text&quot;:{&quot;0&quot;:&quot;problem wanted say problem trying redirect event schedule pakistan NUMBERTAG NUMBERTAG pakistan mother fucker boy want married sister ohhhh love sister boob hmmmmm yummyy&quot;,&quot;1&quot;:&quot;get life fucking loser question ask ask katie goulet picture&quot;,&quot;2&quot;:&quot;cum drinker hey wat nigga thought u could ban took long cuz wa busy az hell recently ill keep cumming back take word cumdrinker&quot;,&quot;3&quot;:&quot;liar liar pant fire seriously looked contribution tennis portal page tennis page ha descussion ever please lie NUMBERTAG NUMBERTAG NUMBERTAG NUMBERTAG&quot;,&quot;4&quot;:&quot;stop writing p nothing discus given lack bsinc education diplomacy&quot;,&quot;5&quot;:&quot;wa fucking page one edit page&quot;,&quot;6&quot;:&quot;question mad gay&quot;,&quot;7&quot;:&quot;warning page nerd please leave one stay girl though pleeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaassssssssssssssssseeeeeeeeeeeee NUMBERTAG oneoneoneoneoneoneoneoneoneoneoenone&quot;,&quot;8&quot;:&quot;full shit&quot;,&quot;9&quot;:&quot;go fuck conrad black cheated thousand people pension anyone defends hm asshole apologist evil&quot;,&quot;10&quot;:&quot;list office bearer national union student australia wp userfy userfied page located&quot;,&quot;11&quot;:&quot;talk history scottish national party claim spying hi sentence someone belief npov claim mean someone belief npov claim&quot;,&quot;12&quot;:&quot;section meant vice review btw magazine website writer name attached also like richardwilson NUMBERTAG even know question ninjarobotpirate wa responding happy criticise answer \\u2026 \\u2026 btw NUMBERTAG far know none editor either albanian croatian maybe airplane vision quite good think take care&quot;,&quot;13&quot;:&quot;next time subtweet&quot;,&quot;14&quot;:&quot;physicsyo yo yo dog&quot;,&quot;15&quot;:&quot;self censorship tv show might might notable tv pre empted breaking news notable happens time&quot;,&quot;16&quot;:&quot;article contains information soursed huddersfield aa street street&quot;,&quot;17&quot;:&quot;utc onto something centrifugal force experienced mass exhibiting inertia result tiny little bullet hitting side ride merry go round rueda puthoff haisch described zero point field electronic lorenz equation coupling inertial frame reference give mass inertial reluctance rather resistance enable describe change velocity direction compare ac v dc tesla v edison NUMBERTAG NUMBERTAG NUMBERTAG june NUMBERTAG&quot;,&quot;18&quot;:&quot;meant wa meant state either unblock create new account rendering block useless simple&quot;,&quot;19&quot;:&quot;NUMBERTAG utc hi NUMBERTAG must mistakenly thought ian wa original member b c always viewed band definitive axeman NUMBERTAG yeah almost bought akai headrush looper year ago notorious role cab one guitarist recording settled bos loop station instead rather headrush boomerang due two reliability price issue respectively check hovercraft southpacific auburn lull kind hallucinitory guitar looping thought cab new lineup wa incredible saw NUMBERTAG skipped classic lineup NUMBERTAG compare two performance wise best NUMBERTAG NUMBERTAG NUMBERTAG may&quot;},&quot;label&quot;:{&quot;0&quot;:1,&quot;1&quot;:1,&quot;2&quot;:1,&quot;3&quot;:1,&quot;4&quot;:1,&quot;5&quot;:1,&quot;6&quot;:1,&quot;7&quot;:1,&quot;8&quot;:1,&quot;9&quot;:1,&quot;10&quot;:0,&quot;11&quot;:0,&quot;12&quot;:0,&quot;13&quot;:0,&quot;14&quot;:0,&quot;15&quot;:0,&quot;16&quot;:0,&quot;17&quot;:0,&quot;18&quot;:0,&quot;19&quot;:0}}'
</code></pre>
","transformer"
"108472","What happens when the length of input is shorter than length of output in transformer architecture?","2022-02-23 17:36:35","","1","652","<nlp><transformer><sequence-to-sequence>","<p>Given standard transformer architecture with encoder and decoder.</p>
<p>What happens when the input for the encoder is shorter than the expected output from the decoder?</p>
<p>The decoder is expecting to receive <strong>value</strong> and <strong>key</strong> tensors from the encoder which size is dependent on the amount of input token.</p>
<p>I could solve this problem during training by padding input and outputs to the same size.</p>
<p>But how about inference, when I don't know the size of the output?</p>
<p>Should I make a prediction and if the decoder doesn't output the <code>stop</code> token within range of available size, re-encode inputs with more padding and try again?</p>
<p>What are the common approaches to this problem?
Thanks in advance, have a great day :)</p>
","transformer"
"108259","Transformer similarity fine-tuned way too often predicts pairs as similar","2022-02-17 19:11:43","","0","26","<classification><transformer><similarity><huggingface><finetuning>","<p>I fine-tuned a transformer for classification to compute similarity between names. This is a toy example for the training data:</p>
<pre><code>name0 name1 label
Test  Test  y
Test  Hi    n
</code></pre>
<p>I fined-tuned the transformer using the label and feeding it with pairs of names as its tokenizer allows to feed 2 pieces of text.</p>
<p>I found a really weird behavior. At prediction times, there exist pairs that have very high chances to be predicted as similar just because they have repeated words. For example,</p>
<pre><code>name0        name1       label
Hi Hi Hi     dsfds       ?
</code></pre>
<p>has a high chance to be predicted as y!</p>
<p><strong>In general there exist some names that no matter what you pair them with, the pairs gets predicted as y.</strong></p>
<p>Did anyone notice this behavior? Is it because I am fine-tuning on about 1000 examples?</p>
<p>At the moment, I am trying to augment my data with:</p>
<ul>
<li>Empty names</li>
<li>Random chars (always the same)</li>
</ul>
<p>E.g.</p>
<pre><code>name0 name1 label
Test        n
      Test  n
Test  dsfsd n
dsfsd Test  n
</code></pre>
<p>Unfortunately, I still see the same behavior.</p>
","transformer"
"108178","How to prepare texts to BERT/RoBERTa models?","2022-02-15 12:15:41","","1","1184","<deep-learning><nlp><bert><transformer><huggingface>","<p>I have an artificial corpus I've built (not a real language) where each document is composed of multiple sentences which again aren't really natural language sentences.</p>
<p>I want to train a language model out of this corpus (to use it later for downstream tasks like classification or clustering with sentence BERT)</p>
<p><strong>How to tokenize the documents?</strong></p>
<p>Do I need to tokenize the input</p>
<p>like this:
<code>&lt;s&gt;sentence1&lt;/s&gt;&lt;s&gt;sentence2&lt;/s&gt;</code></p>
<p>or <code>&lt;s&gt;the whole document&lt;/s&gt;</code></p>
<p><strong>How to train?</strong></p>
<p>Do I need to train an MLM or an NSP or both?</p>
","transformer"
"107775","Positional encoding without input embedding","2022-02-02 19:23:57","107833","1","1045","<word-embeddings><transformer><attention-mechanism>","<p>Does it make sense to use a positional encoding in attention when the input tokens do not go through an embedding layer?</p>
<p>In NLP models, the embedding maps a word to real numbers. <code>&quot;hello&quot;</code> might map to some real numbers <code>[a, b, c]</code>. Then, after adding positional encoding <code>[e1, e2, e3]</code>, the attention layers will see <code>[a + e1, b + e2, c + e3]</code>. Since the network has seen this <code>&quot;hello&quot;</code> embedding before, it can separate out <code>[e1, e2, e3]</code> from <code>[a, b, c]</code>, understanding both the token itself and the token's position.</p>
<p>Now imagine we are doing something like detecting particles, where an embedding layer does not make sense. Rather than a set of possible words, the input to the attention layer is from some continuous domain (like the positions of said particles). Can the attention layer still effectively factor out <code>[e1, e2, e3]</code> when added to some vector in <span class=""math-container"">$\mathbb{R}^3$</span>? How does it know the value of <code>e1</code> if <code>a</code> could be any value in <span class=""math-container"">$\mathbb{R}$</span>?.</p>
<p>I know there are some papers that use transformers without embeddings, but do any show that the positional embedding becomes anything more than sinusoidal noise?</p>
","transformer"
"107759","Deep Learning: which kind of layer for {time_series + static} -> time_series problems?","2022-02-02 13:15:47","","0","24","<deep-learning><keras><time-series><transformer>","<p>as you can see in the image below, I need to bridge static data (on the left) and time series data (on the right) to create a time series output. I have looked at this <a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">example</a> on keras library which deals with <code>transformers</code> but to no avail. generally what are the layers that are used to solve:</p>
<p><strong>time_series + static -&gt; time_series</strong></p>
<p>would the &quot;core&quot; of the answer change if it were (=core maybe you keep the same framework and then you just pick the latest value or &quot;reduction&quot; of some kind):
<strong>time_series + static -&gt; static</strong></p>
<p><a href=""https://i.sstatic.net/exBjE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/exBjE.png"" alt=""enter image description here"" /></a></p>
<p>here the time series part is more &quot;features&quot; based on time that changes and I would avoid using RNN/LSTM or the sort as much as possible. it's just a matter of concatenating things without resulting in not exploding my features and therefore adding &quot;bias&quot; to the problem</p>
<p>p.s.: I know there are some <a href=""https://datascience.stackexchange.com/questions/99795/transforming-time-series-into-static-features"">custom solutions for this problem</a> but here I need to deal with transforming/concatenating things as I will add multiple sources like this (this is a simple example) and I need a general way to deal with this kind of aggregation problem (feeding everything to a single RNN will not do)</p>
","transformer"
"107587","Sequence-to-Sequence Transformer for Neural machine translation","2022-01-28 21:18:56","107609","0","84","<deep-learning><keras><nlp><transformer><language-model>","<p>I am using the tutorial in Keras documentation <a href=""https://keras.io/examples/nlp/neural_machine_translation_with_transformer/"" rel=""nofollow noreferrer"">here</a>. I am new to deep learning. On a different dataset <code>Menyo-20k</code> <a href=""https://zenodo.org/record/4297448"" rel=""nofollow noreferrer"">dataset</a>, of about 10071 total pairs, 7051 training pairs,1510 validation pairs,1510 test pairs. The highest validation accuracy and test accuracy I have gotten is approximately 0.26. I tried the list of things below:</p>
<ol>
<li>Using the following optimizers: <code>SGD, Adam, RMSprop</code></li>
<li>Tried different learning rate</li>
<li>Tried the dropout rate of <code>0.4 and 0.1</code></li>
<li>Tried using different embedding dimensions and feed-forward network dimension</li>
<li>Used <code>Early stopping and patience =3</code>, the model does not go past the <code>13th epoch</code>.
I tried the model itself without changing any parameters, the <code>validation accuracy never got to 0.3</code>, I tried to change the different parameters in order to know what I am doing wrong and I can't figure it out. Please what am I doing wrong? Thank you in advance for your guidance.</li>
</ol>
","transformer"
"107212","Get sentence embeddings of transformer-based models","2022-01-19 00:12:26","","0","1671","<nlp><bert><transformer><embeddings><huggingface>","<p>I want to get sentence embeddings of transformer-based models (Bert, Roberta, Albert, Electra...).</p>
<p>I plan on doing mean pooling on the hidden states of the second last layer just as what bert-as-service did.</p>
<p>So my questions is that when I do mean pooling, should I include the embeddings related to [PAD] tokens or [CLS] token or [SEP] token?</p>
<p>For example, my sequence is 300 tokens, and are padded into 512 tokens.</p>
<p>The output size is 512 (tokens) * 768 (embeddings).</p>
<p>So should I average the embeddings of first 300 tokens or the embeddings of whole 512 tokens?</p>
<p>Why the embeddings of the last 212 tokens are non-zero?</p>
","transformer"
"107043","Best practices to train a transformer text classifier to predict/handle unseen labels","2022-01-13 16:54:34","","2","65","<pytorch><transformer><text-classification>","<p>I fine-tuned a RoBERTa sequence classifier to classify paragraphs of certain documents using labeled paragraphs only (and skipping paragraphs with no label given). The model was validated and tested on labeled paragraphs only as well.
I am using an early stopping criterion to stop training if the cross validation loss is not improving for 5 epochs.</p>
<pre><code>Training accuracy: 0.97
Validation accuracy: 1.00
</code></pre>
<p>Now, during inference on the whole documents (labeled and unlabelled) paragraphs my model is predicting a label for each paragraph, which is expected behaviour of course.</p>
<p>I was hoping to handle the misclassifications with some sort of threshold (i.e. predict as <code>None</code>) if the model's confidence score is below 0.8 (or whatever). Unfortunately, the model predicts some of the unlabelled paragraphs with a pretty high confidence score (~0.99) which makes the use of any threshold impossible.</p>
<p>Unfortunately, my dataset only consists of 200 data points (I know its almost nothing but getting more data is really hard for my task).</p>
<p>Now my questions:</p>
<ol>
<li>Do you think my model is overfitting? (Validation and Test accuracy seems fine though)</li>
<li>Is there a best practice to train a model on a limited set of labels knowing that at inference time the model will see previously unseen and unlabelled texts?</li>
<li>I could try and train a model with the unlabelled paragraphs giving them an <code>other</code> label. But it seems like bad practice?</li>
<li>Any other suggestions?</li>
</ol>
","transformer"
"106956","Difference between Doc2Vec and BERT","2022-01-11 18:20:49","106975","0","2758","<machine-learning><nlp><bert><transformer><doc2vec>","<p>I am trying to understand the difference between Doc2Vec and BERT. I do understand that doc2vec uses a paragraph ID which also serves as a paragraph vector. I am not sure though if that paragraph ID serves in better able to understand the context in that vector?</p>
<p>Moreover, BERT definitely understands the context and attributes different vectors for words such as &quot;Bank&quot;. for instance,</p>
<ol>
<li>I robbed a bank</li>
<li>I was sitting by the bank of a river.</li>
</ol>
<p>BERT would allocate different vectors for the word BANK here. Trying to understand if doc2vec also gets this context since the paragraph id would be different here (for doc2vec). Can anyone please help with this?</p>
","transformer"
"106847","Variational Autoencoders VS Transformers","2022-01-08 14:08:58","","7","10947","<machine-learning><deep-learning><autoencoder><transformer>","<p>I'm relatively new to the field, but I'd like to know how do variational autoencoders fare compared to transformers?</p>
","transformer"
"106508","Algorithms for classification of very short text","2021-12-29 06:30:43","","0","36","<nlp><transformer>","<p>I am to create a classification model for texts that typically have 3 to 4 words in them. I thought of using BERT and XLNet but not sure if they are the right choice for texts that short.</p>
<p>Are there any models(pretrained or otherwise) that specialise for classification of short text?</p>
","transformer"
"106186","Variable batch size for inputs of different length","2021-12-16 12:49:19","","1","313","<machine-learning><nlp><transformer><language-model><learning-rate>","<p>We're fine-tuning a GPT-2 model (using the Adam optimizer) to some posts from a social network. The length of these posts varies quite dramatically, so while some are only two tokens long, others can span hundreds of tokens. We've defined a cutoff at 256, but creating batches randomly and then padding is quite costly in terms of training time. We are now sorting the posts by length and then sampling randomly in consecutive blocks of n posts, where n is the maximum number of posts of 256 tokens that we can fit in a batch without running out of memory. But for batches of smaller posts (like n posts of length 2), this is not utilizing the resources we have and our compute time is quite limited.</p>
<p>So now we're thinking we could pack sequences together in batches such that n*length(post) remains roughly constant across batches. So one batch would be 10 sequences of length 256, while another would be 1280 sequences of length 2. But we're not sure what impact this will have on the training. It seems the learning rate is now scaled to tokens rather than posts, which actually sounds desirable with this kind of variance in number of tokens, but maybe not? Does the learning rate need to be adjusted somehow?</p>
<p>I've seen others do something similar called &quot;tensor packing&quot;, where you would concatenate all the posts and then chop them into chunks of the same size. I don't like the idea of combining posts that have nothing to do with each other like that (so the model could learn predict one from the other), but other than that this seems to do roughly the same thing regarding the learning process and the learning rate, right?</p>
","transformer"
"105115","Get Hidden Layers in PyTorch TransformerEncoder","2021-12-14 17:54:40","105118","1","723","<python><pytorch><transformer><encoder>","<p>I am trying to access the hidden layers when using <code>TransformerEncoder</code> and <code>TransformerEncoderLayer</code>. I could not find anything like that in the source code for these classes.</p>
<p>I am not using hugging face but I know one can get <code>hidden_states</code> and <code>last_hidden_state</code>. I am looking for something similar.</p>
<p>Do you know how I can access them?</p>
","transformer"
"104680","mBART training ""CUDA out of memory""","2021-12-01 06:30:14","","0","461","<transformer><gpu><colab><cuda>","<p>I want to train a network with mBART model in google colab , but I got the message of</p>
<pre><code>RuntimeError: CUDA out of memory. Tried to allocate 886.00 MiB (GPU 0; 15.90 GiB total capacity; 13.32 GiB already allocated; 809.75 MiB free; 14.30 GiB reserved in total by PyTorch)
</code></pre>
<p>I subscribed with GPU in colab. I tried to use 128 or 64 for The maximum total input sequence length.</p>
<p>Kindly, What can I do to fix the problem?</p>
","transformer"
"104536","BERT vs GPT architectural, conceptual and implemetational differences","2021-11-26 21:22:18","","10","8948","<machine-learning><nlp><bert><transformer><gpt>","<p>In the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT paper</a>, I learnt that BERT is encoder-only model, that is it involves only transformer encoder blocks.</p>
<p>In the <a href=""https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"" rel=""noreferrer"">GPT paper</a>, I learnt that GPT is decoder-only model, that is it involves only transformer decoder blocks.</p>
<p>I was guessing whats the difference. I know following difference between encoder and decoder blocks: GPT Decoder looks only at previously generated tokens and learns from them and not in right side tokens. BERT Encoder gives attention to tokens on both sides.</p>
<p>But I have following doubts:</p>
<p><strong>Q1.</strong> GPT2,3 focuses on new/one/zero short learning. Cant we build new/one/zero short learning model with encoder-only architecture like BERT?</p>
<p><strong>Q2.</strong> Huggingface <code>Gpt2Model</code> contains <code>forward()</code> method. I guess, feeding single data instance to this method is like doing one shot learning?</p>
<p><strong>Q3.</strong> I have implemented neural network model which utilizes output from <code>BertModel</code> from hugging face. Can I simply swap <code>BertModel</code> class with <code>GPT2Model</code> with some class and will it. The return value of <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#transformers.GPT2Model.forward"" rel=""noreferrer""><code>Gpt2Model.forward</code></a> does indeed contain <code>last_hidden_state</code> similar to <a href=""https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel.forward"" rel=""noreferrer""><code>BertModel.forward</code></a>. So, I guess swapping out <code>BertModel</code> with <code>Gpt2Model</code> will indeed work, right?</p>
<p><strong>Q4.</strong> Apart from being decoder-only and encoder-only, auto-regressive vs non-auto-regressive and whether or not accepting tokens generated so far as input, what high level architectural / conceptual differences GPT and BERT have?</p>
","transformer"
"104403","Model doesn't know German well enough","2021-11-23 14:19:07","","1","37","<nlp><unsupervised-learning><transformer>","<p>I have a model that generates questions and answers based on input text. The texts are in German and based on observations it seems like the model doesn't know German well enough.</p>
<p>I need to pretrain the T5 transformer &quot;better&quot;, so I was thinking what possibilities I have: is there a better version of T5? A German pre-trained version? Isn't multilingual mT5 too much since I'm only interested in German?</p>
<p>And last but not least, can't I just use my current code for pretraining and use the c4 dataset in German to make it better?</p>
","transformer"
"104359","Any transformer model (NLP) for code classification?","2021-11-22 10:32:02","","1","19","<classification><nlp><transformer><text-classification><code>","<p>Does any Transformer (NLP) that is suitable for code classification tasks exist?</p>
<p>For example, I have a lot of source codes of various categories (driver, game, email client, etc.). I want to distinguish (classify) these types using such a model.</p>
<p>Many thanks for considering my question.</p>
","transformer"
"104179","Is the Transformer decoder an autoregressive model?","2021-11-15 18:36:18","","12","12406","<nlp><transformer>","<p>I have been trying to find an answer to these questions, but I only find conflicting information. Is the transformer as a whole autoregressive or not? And what about the decoder? I understand that the decoder during inference proceeds autoregressively, but I am not sure about during training time.</p>
<p>Here are posts saying that the Transformer is not autoregressive:</p>
<p><a href=""https://datascience.stackexchange.com/questions/93144/minimal-working-example-or-tutorial-showing-how-to-use-pytorchs-nn-transformerd"">Minimal working example or tutorial showing how to use Pytorch&#39;s nn.TransformerDecoder for batch text generation in training and inference modes?</a></p>
<p>Here are some saying that it is:</p>
<p><a href=""https://datascience.stackexchange.com/questions/81727/what-would-be-the-target-input-for-transformer-decoder-during-test-phase?rq=1"">What would be the target input for Transformer Decoder during test phase?</a></p>
<p><a href=""https://www.tensorflow.org/text/tutorials/transformer"" rel=""noreferrer"">https://www.tensorflow.org/text/tutorials/transformer</a></p>
<p><a href=""https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0"" rel=""noreferrer"">https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0</a></p>
<p><a href=""https://huggingface.co/transformers/summary.html#seq-to-seq-models"" rel=""noreferrer"">https://huggingface.co/transformers/summary.html#seq-to-seq-models</a></p>
","transformer"
"104140","Is PositionalEncoding needed for using Transformer models correctly?","2021-11-14 19:40:05","","0","25","<deep-learning><nlp><transformer>","<p>I am trying to make a model that uses a <code>Transformer</code> to see the relationship between several data vectors but the order of the data is not relevant in this case, so I am not using the <code>PositionalEncoding</code>.</p>
<p>Since the performance of models using <code>Transformers</code> is quite improved with the use of this part do you think that if I remove that part I am breaking the potential of <code>Transformers</code> or is it correct to do so?</p>
","transformer"
"103936","How to Fine Tune a BERT model for sentiment analysis to get the best f1 score","2021-11-08 12:42:25","","1","374","<loss-function><bert><transformer><sentiment-analysis><huggingface>","<p>I am building a multi-class sentiment analysis BERT model that's optimized to give the best f1 score.</p>
<p>More specifically, I train each epoch by optimizing binary cross entropy per class, taking the mean, and then run back propagation to optimize the parameters. At the end of each epoch, I then compute the soft-f1 score on the validation set, and then save that model only if the score has improved. The predicted class is chosen by taking an argmax, so there is no threshold.</p>
<p>There are a few observations here, some of which are troubling:</p>
<ol>
<li><p>If I simply use log-loss as the criteria for choosing the best model, then the model will typically only train for 3-4 epochs before the losses start to degrade. This is the standard approach, and gives descent f1 scores across the training/validation and test sets.</p>
</li>
<li><p>If I instead use the soft-f1 criteria as specified above, the model will typically train for <em>15</em> epochs (where I use a stopping condition of 5 epochs if there is no improvement). The f1 scores are significantly better on the training set, and marginally better on the validation and test sets. I'm guessing that the improvement on the training set is simply due to overfitting, as the model has run substantially longer. But then there are also improvements in the validation and test sets too. The log-loss for this second model is (naturally) worse than model #1.</p>
</li>
</ol>
<p>My question, which is the best model? Is it correct to use the f1 score to choose the best model while training, or am I just overfitting without realizing? Thanks!</p>
","transformer"
"103885","What does ""expansion layer"" mean?","2021-11-06 15:50:24","","1","1458","<transformer><mlp>","<p>Recently, I found &quot;expansion layer&quot; term in the next paper:</p>
<blockquote>
<p><a href=""https://arxiv.org/pdf/2103.14030.pdf"" rel=""nofollow noreferrer"">Liu, Ze, et al. &quot;Swin transformer: Hierarchical vision transformer using shifted windows.&quot; arXiv preprint arXiv:2103.14030 (2021).</a></p>
</blockquote>
<p>This term is mentioned in the context of Multilayer perceptron (MLP). So I have tried to figure out its meaning on my own, but I would not be able to find anything particular.</p>
<p>Also I found &quot;expansion ratio&quot; term (again in MLP context) in this paper:</p>
<blockquote>
<p><a href=""https://arxiv.org/pdf/2103.15808.pdf"" rel=""nofollow noreferrer"">Wu, Haiping, et al. &quot;Cvt: Introducing convolutions to vision transformers.&quot; arXiv preprint arXiv:2103.15808 (2021).</a></p>
</blockquote>
<p>So, what does &quot;expansion layer&quot; means? And what is &quot;expansion ratio&quot;? Thanks in advance.</p>
","transformer"
"103717","Machine translation transformer output - ""unknown"" tokens?","2021-11-02 08:46:45","107819","1","337","<nlp><transformer>","<p>Cross post from my original post in Stackoverflow: <a href=""https://stackoverflow.com/questions/69595863/machine-translation-transformer-output-unknown-tokens"">https://stackoverflow.com/questions/69595863/machine-translation-transformer-output-unknown-tokens</a></p>
<p>Based on the feedback , I have now updated my approach to use WordPiece from Huggingface's pretrained BERT tokenizers. However, I still run into &quot;unk&quot; tokens when translating. I am curious why that still happens? I thought that WordPiece would try to decode without outputting any &quot;unk&quot; tokens.</p>
<p>This is how I tokenized my data, I am using German to english for the translation task.</p>
<pre><code>from transformers import BertTokenizer
bert_tokenizer_en = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
bert_tokenizer_de = BertTokenizer.from_pretrained(&quot;bert-base-german-cased&quot;,do_lower_case=True)
</code></pre>
<p>Example out:</p>
<pre><code>ground truth = ['a', 'girl', 'in', 'a', 'jean', 'dress', 'is', 'walking', 'along', 'a', 'raised', 'balance', 'beam', '.']

predicted = ['a', 'girl', 'in', 'a', '&lt;unk&gt;', 'costume', 'is', 'jumping', 'on', 'a', 'clothesline', '.', '&lt;eos&gt;']
<span class=""math-container"">````</span>
</code></pre>
","transformer"
"103677","Is it possible to target a specific output length range with BART seq2seq?","2021-11-01 08:52:19","","1","441","<nlp><transformer><sequence-to-sequence>","<p>I'm currently working on an extractive summary model based on Facebook's BART model. Consistent absolute length output would be highly desirable. The problem is that input length may vary wildly. That is to say, creating the training data, the instructions look like this:</p>
<ol>
<li>Take the input text (a news article) and start (recursively) deleting examples, excess details, unnecessary background information, quotes, etc.</li>
<li>Once your summary has less than 90 words, stop deleting stuff.</li>
<li>Fix up the text format to match the style guide.</li>
</ol>
<p>The large BART model available on Huggingface was fine-tuned on 200 samples. All 200 samples had 60-88 words as the output sequence length. However, the model predicted outputs with lengths varying from 50 to 105 words, with some outliers as high as 120 words.</p>
<p>Now I'm questioning whether just throwing more samples at the problem will actually fix it. Since the model is doing very well following the style guide, I don't want to give up on this approach. The outputs which are too long can be eliminated by cranking up the length penalty. But that would make the &quot;too short&quot; case even more prevalent.</p>
<p>Can fine-tuning achieve a tighter range of output lengths just by specifying more examples? Or is there perhaps a more &quot;hacky&quot; solution to penalize output lengths not in the range?</p>
","transformer"
"103655","ResNet50 + Transformer","2021-10-31 15:27:31","","4","273","<tensorflow><cnn><image-classification><feature-extraction><transformer>","<p>In many papers people extract features from image using ResNet and than pass them through transformer. I want to implement the same. I want to get features and than classify them using transformer.  That's what I have done:</p>
<ol>
<li>Downloaded CIFAR100</li>
<li>From each image extracted features( shape is (3, 3, 2048)) and added these features to training  dataset</li>
<li>Defined  model:</li>
</ol>
<pre><code>Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_3 (InputLayer)            [(None, 3, 3, 2048)] 0                                            
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 18432)        0           input_3[0][0]                    
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 4608)         84939264    flatten_2[0][0]                  
__________________________________________________________________________________________________
dropout_20 (Dropout)            (None, 4608)         0           dense_20[0][0]                   
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 3, 3, 512)    0           dropout_20[0][0]                 
__________________________________________________________________________________________________
patch_encoder_1 (PatchEncoder)  (None, 3, 3, 512)    264192      reshape_1[0][0]                  
__________________________________________________________________________________________________
layer_normalization_17 (LayerNo (None, 3, 3, 512)    1024        patch_encoder_1[0][0]            
__________________________________________________________________________________________________
multi_head_attention_8 (MultiHe (None, 3, 3, 512)    4200960     layer_normalization_17[0][0]     
                                                                 layer_normalization_17[0][0]     
__________________________________________________________________________________________________
add_16 (Add)                    (None, 3, 3, 512)    0           multi_head_attention_8[0][0]     
                                                                 patch_encoder_1[0][0]            
__________________________________________________________________________________________________
layer_normalization_18 (LayerNo (None, 3, 3, 512)    1024        add_16[0][0]                     
__________________________________________________________________________________________________
dense_22 (Dense)                (None, 3, 3, 1024)   525312      layer_normalization_18[0][0]     
__________________________________________________________________________________________________
dropout_21 (Dropout)            (None, 3, 3, 1024)   0           dense_22[0][0]                   
__________________________________________________________________________________________________
dense_23 (Dense)                (None, 3, 3, 512)    524800      dropout_21[0][0]                 
__________________________________________________________________________________________________
dropout_22 (Dropout)            (None, 3, 3, 512)    0           dense_23[0][0]                   
__________________________________________________________________________________________________
add_17 (Add)                    (None, 3, 3, 512)    0           dropout_22[0][0]                 
                                                                 add_16[0][0]                     
__________________________________________________________________________________________________
layer_normalization_19 (LayerNo (None, 3, 3, 512)    1024        add_17[0][0]                     
__________________________________________________________________________________________________
multi_head_attention_9 (MultiHe (None, 3, 3, 512)    4200960     layer_normalization_19[0][0]     
                                                                 layer_normalization_19[0][0]     
__________________________________________________________________________________________________
add_18 (Add)                    (None, 3, 3, 512)    0           multi_head_attention_9[0][0]     
                                                                 add_17[0][0]                     
__________________________________________________________________________________________________
layer_normalization_20 (LayerNo (None, 3, 3, 512)    1024        add_18[0][0]                     
__________________________________________________________________________________________________
dense_24 (Dense)                (None, 3, 3, 1024)   525312      layer_normalization_20[0][0]     
__________________________________________________________________________________________________
dropout_23 (Dropout)            (None, 3, 3, 1024)   0           dense_24[0][0]                   
__________________________________________________________________________________________________
dense_25 (Dense)                (None, 3, 3, 512)    524800      dropout_23[0][0]                 
__________________________________________________________________________________________________
dropout_24 (Dropout)            (None, 3, 3, 512)    0           dense_25[0][0]                   
__________________________________________________________________________________________________
add_19 (Add)                    (None, 3, 3, 512)    0           dropout_24[0][0]                 
                                                                 add_18[0][0]                     
__________________________________________________________________________________________________
layer_normalization_21 (LayerNo (None, 3, 3, 512)    1024        add_19[0][0]                     
__________________________________________________________________________________________________
multi_head_attention_10 (MultiH (None, 3, 3, 512)    4200960     layer_normalization_21[0][0]     
                                                                 layer_normalization_21[0][0]     
__________________________________________________________________________________________________
add_20 (Add)                    (None, 3, 3, 512)    0           multi_head_attention_10[0][0]    
                                                                 add_19[0][0]                     
__________________________________________________________________________________________________
layer_normalization_22 (LayerNo (None, 3, 3, 512)    1024        add_20[0][0]                     
__________________________________________________________________________________________________
dense_26 (Dense)                (None, 3, 3, 1024)   525312      layer_normalization_22[0][0]     
__________________________________________________________________________________________________
dropout_25 (Dropout)            (None, 3, 3, 1024)   0           dense_26[0][0]                   
__________________________________________________________________________________________________
dense_27 (Dense)                (None, 3, 3, 512)    524800      dropout_25[0][0]                 
__________________________________________________________________________________________________
dropout_26 (Dropout)            (None, 3, 3, 512)    0           dense_27[0][0]                   
__________________________________________________________________________________________________
add_21 (Add)                    (None, 3, 3, 512)    0           dropout_26[0][0]                 
                                                                 add_20[0][0]                     
__________________________________________________________________________________________________
layer_normalization_23 (LayerNo (None, 3, 3, 512)    1024        add_21[0][0]                     
__________________________________________________________________________________________________
multi_head_attention_11 (MultiH (None, 3, 3, 512)    4200960     layer_normalization_23[0][0]     
                                                                 layer_normalization_23[0][0]     
__________________________________________________________________________________________________
add_22 (Add)                    (None, 3, 3, 512)    0           multi_head_attention_11[0][0]    
                                                                 add_21[0][0]                     
__________________________________________________________________________________________________
layer_normalization_24 (LayerNo (None, 3, 3, 512)    1024        add_22[0][0]                     
__________________________________________________________________________________________________
dense_28 (Dense)                (None, 3, 3, 1024)   525312      layer_normalization_24[0][0]     
__________________________________________________________________________________________________
dropout_27 (Dropout)            (None, 3, 3, 1024)   0           dense_28[0][0]                   
__________________________________________________________________________________________________
dense_29 (Dense)                (None, 3, 3, 512)    524800      dropout_27[0][0]                 
__________________________________________________________________________________________________
dropout_28 (Dropout)            (None, 3, 3, 512)    0           dense_29[0][0]                   
__________________________________________________________________________________________________
add_23 (Add)                    (None, 3, 3, 512)    0           dropout_28[0][0]                 
                                                                 add_22[0][0]                     
__________________________________________________________________________________________________
layer_normalization_25 (LayerNo (None, 3, 3, 512)    1024        add_23[0][0]                     
__________________________________________________________________________________________________
multi_head_attention_12 (MultiH (None, 3, 3, 512)    4200960     layer_normalization_25[0][0]     
                                                                 layer_normalization_25[0][0]     
__________________________________________________________________________________________________
add_24 (Add)                    (None, 3, 3, 512)    0           multi_head_attention_12[0][0]    
                                                                 add_23[0][0]                     
__________________________________________________________________________________________________
layer_normalization_26 (LayerNo (None, 3, 3, 512)    1024        add_24[0][0]                     
__________________________________________________________________________________________________
dense_30 (Dense)                (None, 3, 3, 1024)   525312      layer_normalization_26[0][0]     
__________________________________________________________________________________________________
dropout_29 (Dropout)            (None, 3, 3, 1024)   0           dense_30[0][0]                   
__________________________________________________________________________________________________
dense_31 (Dense)                (None, 3, 3, 512)    524800      dropout_29[0][0]                 
__________________________________________________________________________________________________
dropout_30 (Dropout)            (None, 3, 3, 512)    0           dense_31[0][0]                   
__________________________________________________________________________________________________
add_25 (Add)                    (None, 3, 3, 512)    0           dropout_30[0][0]                 
                                                                 add_24[0][0]                     
__________________________________________________________________________________________________
layer_normalization_27 (LayerNo (None, 3, 3, 512)    1024        add_25[0][0]                     
__________________________________________________________________________________________________
multi_head_attention_13 (MultiH (None, 3, 3, 512)    4200960     layer_normalization_27[0][0]     
                                                                 layer_normalization_27[0][0]     
__________________________________________________________________________________________________
add_26 (Add)                    (None, 3, 3, 512)    0           multi_head_attention_13[0][0]    
                                                                 add_25[0][0]                     
__________________________________________________________________________________________________
layer_normalization_28 (LayerNo (None, 3, 3, 512)    1024        add_26[0][0]                     
__________________________________________________________________________________________________
dense_32 (Dense)                (None, 3, 3, 1024)   525312      layer_normalization_28[0][0]     
__________________________________________________________________________________________________
dropout_31 (Dropout)            (None, 3, 3, 1024)   0           dense_32[0][0]                   
__________________________________________________________________________________________________
dense_33 (Dense)                (None, 3, 3, 512)    524800      dropout_31[0][0]                 
__________________________________________________________________________________________________
dropout_32 (Dropout)            (None, 3, 3, 512)    0           dense_33[0][0]                   
__________________________________________________________________________________________________
add_27 (Add)                    (None, 3, 3, 512)    0           dropout_32[0][0]                 
                                                                 add_26[0][0]                     
__________________________________________________________________________________________________
layer_normalization_29 (LayerNo (None, 3, 3, 512)    1024        add_27[0][0]                     
__________________________________________________________________________________________________
multi_head_attention_14 (MultiH (None, 3, 3, 512)    4200960     layer_normalization_29[0][0]     
                                                                 layer_normalization_29[0][0]     
__________________________________________________________________________________________________
add_28 (Add)                    (None, 3, 3, 512)    0           multi_head_attention_14[0][0]    
                                                                 add_27[0][0]                     
__________________________________________________________________________________________________
layer_normalization_30 (LayerNo (None, 3, 3, 512)    1024        add_28[0][0]                     
__________________________________________________________________________________________________
dense_34 (Dense)                (None, 3, 3, 1024)   525312      layer_normalization_30[0][0]     
__________________________________________________________________________________________________
dropout_33 (Dropout)            (None, 3, 3, 1024)   0           dense_34[0][0]                   
__________________________________________________________________________________________________
dense_35 (Dense)                (None, 3, 3, 512)    524800      dropout_33[0][0]                 
__________________________________________________________________________________________________
dropout_34 (Dropout)            (None, 3, 3, 512)    0           dense_35[0][0]                   
__________________________________________________________________________________________________
add_29 (Add)                    (None, 3, 3, 512)    0           dropout_34[0][0]                 
                                                                 add_28[0][0]                     
__________________________________________________________________________________________________
layer_normalization_31 (LayerNo (None, 3, 3, 512)    1024        add_29[0][0]                     
__________________________________________________________________________________________________
multi_head_attention_15 (MultiH (None, 3, 3, 512)    4200960     layer_normalization_31[0][0]     
                                                                 layer_normalization_31[0][0]     
__________________________________________________________________________________________________
add_30 (Add)                    (None, 3, 3, 512)    0           multi_head_attention_15[0][0]    
                                                                 add_29[0][0]                     
__________________________________________________________________________________________________
layer_normalization_32 (LayerNo (None, 3, 3, 512)    1024        add_30[0][0]                     
__________________________________________________________________________________________________
dense_36 (Dense)                (None, 3, 3, 1024)   525312      layer_normalization_32[0][0]     
__________________________________________________________________________________________________
dropout_35 (Dropout)            (None, 3, 3, 1024)   0           dense_36[0][0]                   
__________________________________________________________________________________________________
dense_37 (Dense)                (None, 3, 3, 512)    524800      dropout_35[0][0]                 
__________________________________________________________________________________________________
dropout_36 (Dropout)            (None, 3, 3, 512)    0           dense_37[0][0]                   
__________________________________________________________________________________________________
add_31 (Add)                    (None, 3, 3, 512)    0           dropout_36[0][0]                 
                                                                 add_30[0][0]                     
__________________________________________________________________________________________________
layer_normalization_33 (LayerNo (None, 3, 3, 512)    1024        add_31[0][0]                     
__________________________________________________________________________________________________
flatten_3 (Flatten)             (None, 4608)         0           layer_normalization_33[0][0]     
__________________________________________________________________________________________________
dropout_37 (Dropout)            (None, 4608)         0           flatten_3[0][0]                  
__________________________________________________________________________________________________
dense_38 (Dense)                (None, 2048)         9439232     dropout_37[0][0]                 
__________________________________________________________________________________________________
dropout_38 (Dropout)            (None, 2048)         0           dense_38[0][0]                   
__________________________________________________________________________________________________
dense_39 (Dense)                (None, 1024)         2098176     dropout_38[0][0]                 
__________________________________________________________________________________________________
dropout_39 (Dropout)            (None, 1024)         0           dense_39[0][0]                   
__________________________________________________________________________________________________
dense_40 (Dense)                (None, 100)          102500      dropout_39[0][0]                 
==================================================================================================
Total params: 138,869,348
Trainable params: 138,869,348
Non-trainable params: 0
</code></pre>
<p>You can find PatchEncoder layer here <a href=""https://keras.io/examples/vision/image_classification_with_vision_transformer/"" rel=""nofollow noreferrer"">https://keras.io/examples/vision/image_classification_with_vision_transformer/</a></p>
<p>Than I started to train model. I stopped training because got bad metrics:
<a href=""https://pastebin.com/cHJjj9Lt"" rel=""nofollow noreferrer"">https://pastebin.com/cHJjj9Lt</a></p>
<p>Should I train this model for more epochs?</p>
<p>After that I defined ResNet50 with max pooling. Now extracted features have shape (2048,)
I started to create dataset for this shape, but I don't know how to define model.</p>
<p><strong>I have a couple of questions:</strong></p>
<ol>
<li>Which features should I pass to transformer model? Features with shape (3, 3, 2048) or features with shape (2048,)?</li>
<li>If I should use features with shape (2048,), than help me to create model.
I don't know how to create first layers. Deeper layers I will copy from link above.
3)If I should use features with shape (3, 3, 2048), tell me what I did wrong.</li>
</ol>
<p>Please, help me to implement ResNet50 + Transformer.
I really appreciate any kind of help.</p>
<p>I used code from here: <a href=""https://keras.io/examples/vision/image_classification_with_vision_transformer/"" rel=""nofollow noreferrer"">https://keras.io/examples/vision/image_classification_with_vision_transformer/</a></p>
","transformer"
"103344","How does T5 model work on input and target data while transfer learning?","2021-10-21 03:41:11","","0","699","<nlp><transformer><transfer-learning><huggingface><nlg>","<p>I am working on a project where I want the model to generate job description based on <code>Role, Industry, Skills</code>. I have trained my data and got the resultant output.</p>
<p>I am aware that the <code>T5 model</code> was trained on C4 data in an unsupervised manner. The various different techniques applied including denoising, corrupted span, etc. But I am not able to understand how it works for classification problem.</p>
<p>My concern is if I pass my input and target variables for training how the model is going to train. Also give me a brief idea as how the model is going to react to input and output data.</p>
<p>Any links, resources is appreciated. Thankyou.</p>
","transformer"
"103279","Combining models trained on a multilingual multi-source corpus","2021-10-19 09:47:40","","1","39","<bert><transformer>","<p>Consider the following training corpora:</p>
<ul>
<li><p><em>dataset1</em>: composed of French instances</p>
</li>
<li><p><em>dataset2</em>: <em>dataset1</em> + Arabic instances</p>
</li>
<li><p><em>test_dataset</em> (for both scenarios): composed of French instances</p>
</li>
</ul>
<p>(the same annotation guidelines were used for both languages).</p>
<p>After analyzing the results of our preliminary experimental setup, we chose BERT as our baseline system.</p>
<p>Considering the different languages involved, we experimented with different models capable of handling them: FlauBERT and CamemBERT (for French), AraBERT (for Arabic) as well as BERT multilingual. Generally, for both languages, the results obtained by BERT multilingual are lower than those obtained by the language specific models.</p>
<p>Is it theoretically possible to <em>merge</em> multiple models together into one model, effectively combining all the data learnt so far? For example, combining CamemBERT trained only on the French part of <em>dataset2</em> and AraBERT trained only on the Arabic part?</p>
","transformer"
"103224","Why do Transformers need positional encodings?","2021-10-17 17:40:21","","7","2964","<machine-learning><deep-learning><neural-network><nlp><transformer>","<p>At least in the first self-attention layer in the encoder, inputs have a correspondence with outputs, I have the following questions.</p>
<ul>
<li>Isn't ordering already implicitly captured by the query vectors, which themselves are just transformations of the inputs?</li>
<li>What do the sinusoidal positional encodings capture that the ordering of the query vectors don't already do?</li>
<li>Am I perhaps mistaken in thinking that transformers take in the entire input at once?</li>
<li>How are words fed in?</li>
<li>If we feed in the entire sentence at once, shouldn't the ordering be preserved?</li>
</ul>
<p><a href=""https://i.sstatic.net/Ezu0V.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/Ezu0V.png"" alt=""source: https://web.eecs.umich.edu/~justincj/slides/eecs498/FA2020/598_FA2020_lecture13.pdf"" /></a></p>
","transformer"
"103103","What is the difference between batch_encode_plus() and encode_plus()","2021-10-13 11:04:44","103106","1","8956","<nlp><transformer><transfer-learning><nlg>","<p>I am doing a project using T5 Transformer. I have read documentations related to T5 Transformer model. While using T5Tokenizer I am kind of confused with tokenizing my sentences.</p>
<p>Can someone please help me understand the difference between batch_encode_plus() and encode_plus() and when should I use either of the tokenizers.</p>
","transformer"
"103102","Huggingface - TypeError: 'TensorSliceDataset' object is not subscriptable","2021-10-13 10:58:15","","1","1581","<deep-learning><dataset><transformer><huggingface>","<p>I'm trying to make my own model for translate a language to another with <a href=""https://huggingface.co/transformers/model_doc/t5.html#t5forconditionalgeneration"" rel=""nofollow noreferrer"">T5ForConditionalGeneration</a> and Huggingface using no pretrained model (I need to use my own dataset and tokenizer because no pretrained model exists for the languages I use). When running the train() method, I get the following error: TypeError: 'TensorSliceDataset' object is not subscriptable</p>
<p>I suppose it is due to the type of the dataset given to the Trainer (TensorSliceDataset), but I can't figure out what should be the structure and type of the dataset that I should use for language translation. Can someone give me an example of dataset structure? I do not understand how the model can find the x and y labels if we don't tell it explicitly which column should be x and y.
<a href=""https://gist.github.com/TristanBilot/4cca4733faaa171f0475cb6cff4c634a"" rel=""nofollow noreferrer"">Source code of the example.</a></p>
","transformer"
"103016","NER prections with distilbert transformer model","2021-10-11 02:45:37","","1","339","<bert><transformer><text-classification><named-entity-recognition>","<p>I am trying to extract 'agreement date' label from a corpus of legal contracts. In the train dataset, I used pytorch-transformer model to train.</p>
<pre><code>model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))
</code></pre>
<p>Here label_list is the IOB format which gives ['B-Date', 'I-Date', 'O'] and model_checkpoint is &quot;distilbert-base-uncased&quot;
I train the dataset after defining TrainingArguements, datacollator and matrics computaitons from predictions</p>
<pre><code>model_output_dir = 'C:/Python/model_output_dir'

args = TrainingArguments(
    output_dir = model_output_dir,
    evaluation_strategy = &quot;epoch&quot;,
    logging_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,    
    run_name = model_checkpoint,
    metric_for_best_model=&quot;f1&quot;,
    load_best_model_at_end = True,
    weight_decay=0.01,
    )

from transformers import DataCollatorForTokenClassification
data_collator = DataCollatorForTokenClassification(tokenizer)
from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)]

    # Define the metric parameters
    overall_precision = precision_score(true_labels, true_predictions, zero_division=1)
    overall_recall = recall_score(true_labels, true_predictions, zero_division=1)
    overall_f1 = f1_score(true_labels, true_predictions, zero_division=1)
    overall_accuracy = accuracy_score(true_labels, true_predictions)
    
    # Return a dictionary with the calculated metrics
    return {
        &quot;precision&quot;: overall_precision,
        &quot;recall&quot;: overall_recall,
        &quot;f1&quot;: overall_f1,
        &quot;accuracy&quot;: overall_accuracy,}

trainer = Trainer(
                model= model,
                args = args,
                train_dataset=tokenized_datasets[&quot;train&quot;],
                eval_dataset=tokenized_datasets[&quot;test&quot;],
                data_collator=data_collator,
                tokenizer=tokenizer,
                compute_metrics=compute_metrics,
                )
</code></pre>
<p>My input training data after preprocessing look like this
<a href=""https://i.sstatic.net/tPxRT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tPxRT.jpg"" alt=""enter image description here"" /></a></p>
<p>My test data has agreement text only in tokenized form. When I predict I get the big tensor like below
<a href=""https://i.sstatic.net/qBySx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qBySx.jpg"" alt=""enter image description here"" /></a></p>
<p>How I get the required date label from this output ?</p>
","transformer"
"103005","Can a reformer model really handle long-range dependency?","2021-10-10 16:15:16","","1","38","<deep-learning><nlp><transformer><attention-mechanism><huggingface>","<p>I read this <a href=""https://huggingface.co/blog/reformer"" rel=""nofollow noreferrer"">article</a> about new attention model called <a href=""https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb"" rel=""nofollow noreferrer"">Reformer</a>. Here is the main strength of this model:</p>
<blockquote>
<p>The Reformer pushes the limit of longe sequence modeling by its ability to process up to half a million tokens at once as shown in this demo.</p>
</blockquote>
<p>Is it actually true? Because later in jupyter notebook I see the following in a config:</p>
<pre><code>config = {
    &quot;attention_head_size&quot;: 64,
    &quot;attn_layers&quot;: [&quot;local&quot;, &quot;lsh&quot;, &quot;local&quot;, &quot;lsh&quot;, &quot;local&quot;, &quot;lsh&quot;],
    &quot;lsh_attn_chunk_length&quot;: 64,
    &quot;local_attn_chunk_length&quot;: 64,

    &quot;lsh_num_chunks_before&quot;: 1,
    &quot;lsh_num_chunks_after&quot;: 0,
    &quot;local_num_chunks_before&quot;: 1,
    &quot;local_num_chunks_after&quot;: 0,
}
</code></pre>
<p>This config tells that they use chunks, so attention can span only <code>64</code> tokens and 1 chunk before it.</p>
<p>Does it mean that there is no any long-range dependency can be learned by this model?</p>
","transformer"
"102607","Is it possible to fine-tuning BERT by training it on multiple datasets? (Each dataset having it's own purpose)","2021-09-29 11:54:47","","1","816","<nlp><bert><transformer><transfer-learning><finetuning>","<p>BERT can be fine-tuned on a dataset for a specific task. Is it possible to fine-tune it on all these datasets for different tasks and then be utilized for these tasks instead of fine-tuning a BERT model specific to each task?</p>
","transformer"
"102558","Training NMT models for noisy social media roman text","2021-09-28 03:26:12","","0","16","<transformer><machine-translation><tokenization>","<p>I am trying to train an NMT model where the source side is roman text of Asian languages from social media, and target side is English. Note that since roman text is not native to Asia, the romanizations done by people to type on the Internet are very personal and hence a bit noisy, but easily intelligible to native speakers.</p>
<p>The following is an example for writing a Hindi sentence in different ways:</p>
<ul>
<li>Vaise bhi mere paas jo bhi hai maine aap ko sab kuch dey diyaa bhaai</li>
<li>wesebi mr pas jobi h, mene apko sbkch dedia bhai</li>
</ul>
<p>So I think sub-word tokenizers might not help much here (for the source side), and will also not be robust to different variations of noise. (Note that the target side could be sub-word tokenizer.)</p>
<p>What models and tokenizer for the source side is generally suggested and works for such cases? Would character-level models would be the best suited?</p>
","transformer"
"102460","AttributeError: 'numpy.ndarray' object has no attribute 'fit'","2021-09-24 20:53:11","","0","8989","<scikit-learn><transformer><pipelines>","
<p>I am relatively new to ML and in the process of learning pipelines.</p>
<p>I am creating a pipeline of custom transformers and get this error:
AttributeError: 'numpy.ndarray' object has no attribute 'fit'. Below is the code.</p>
<p>I am not entirely sure where the error is occurring. Any help is appreciated</p>
<p>Note: I am using the King county housing data
<a href=""https://www.kaggle.com/harlfoxem/housesalesprediction"" rel=""nofollow noreferrer"">https://www.kaggle.com/harlfoxem/housesalesprediction</a></p>
<pre class=""lang-ipyhton prettyprint-override""><code>housing_df =pd.read_csv(&quot;kc_house_data.csv&quot;)
housing_df


class FeatureSelector(BaseEstimator,TransformerMixin):
    def __init__(self,feature_names):
    self._feature_names = feature_names
    
    def fit(self,X, y= None):
        #We will not do anything here and just return the object
        print(f'\n Fit method - Feature Selector\n')
        return self

    def transform(self,X, y= None):
        print(f'\n Transform method - Feature Selector\n')
        #we will return only the columns mentioned in the feature names
        return X[self._feature_names]

class CategoricalTansformer(BaseEstimator,TransformerMixin):
    def __init__(self, use_dates = ['year','month','day']):
        self._use_dates = use_dates
    
    def fit(self,X,y=None):
        #nothing to do here. Return the object
        print(f'\n Fit method - Categorical Transformer\n')
        return self

    # Helper functions to extract year from column 'dates'
    def get_year(self, data):
        return str(data)[:4]

    def get_month(self,data):
        return str(data)[4:6]

    def get_day(self,data):
        return str(data)[6:8]

    #Helper function thta converts values to Binary
    def create_binary(self,data):
        if data ==0:
            return 'No'
        else:
            return 'Yes'
 
    def transform(self,X,y=None):
        print(f'\n Transform method - Categorical Transformer\n')
        #Depending on the costructor argument break dates column to specified units
        for spec in self._use_dates:
            exec(&quot;X.loc[:,'{}']= X['date'].apply(self.get_{})&quot;.format(spec,spec))
        #now drop the date column
        X = X.drop('date',axis =1)
             
       #Convert the columns to binary for one hot encoding later
    
        X.loc[:,'waterfront']=X['waterfront'].apply(self.create_binary)
        X.loc[:,'view']= X['view'].apply(self.create_binary)
        X.loc[:,'yr_renovated']= X['yr_renovated'].apply(self.create_binary)
    
        # returns numpy array
        return X.values

class NumericalTransformer(BaseEstimator,TransformerMixin):

    def __init__(self,bath_per_bed =True, years_old = True):
        self._bath_per_bed = bath_per_bed
        self._years_old = years_old
    
    def fit(self, X,y=None):
        # No computations here, return object
        print(f'\n Fit method - Numerical Transformer\n')
        return self

    def transform(self,X,y=None):
        print(f'\n Transform method - Numerical Transformer\n')
        if self._bath_per_bed:
            #create a new column
            X.loc[:,'bath_per_bed'] = X['bathrooms']/X['bedrooms']
            #drop redundant column
            X.drop('bathrooms',axis =1)
        
        if self._years_old:
            #create a new column
            X.loc[:,'years_old']= 2019 - X['yr_built']
            # drop redundant column
            X.drop('yr_built',axis =1)
      

        #Converting any infinity value in the data set to NaN
        X =X.replace([np.inf,-np.inf],np.nan)
        #print(X.values)
        #returns a numpy array
        return X.values


#Categorical features to pass down the Categorical pipeline
 categorical_features =['date','waterfront','view','yr_renovated']

 #Numerical features to pass down the Numerical pipeline
 numerical_features =    ['bedrooms','bathrooms','sqft_living','sqft_lot','floors','condition',
                'grade','sqft_basement','yr_built']

 #Defining the Categorical Pipeline
 categorical_pipeline = Pipeline(steps=[
                    ('cat_selector',    FeatureSelector(categorical_features)),
                    ('cat_transformer',CategoricalTansformer()),
                    ('one_hot_encoder', OneHotEncoder(sparse=False))
                    ])
  #Defining the Numerical Pipeline
  numerical_pipeline = Pipeline(steps =[
                        ('num_selector',FeatureSelector(numerical_features)),
                        ('num_transformer', NumericalTransformer()),
                        ('imputer', SimpleImputer(strategy ='median')),
                        ('std_scaler',StandardScaler)
                    ])


  #Combining numerical and categorical pipelines using FeatureUnion
   full_pipeline = FeatureUnion(transformer_list =[
                        ('categorical_pipeline',categorical_pipeline),
                        ('numerical_pipeline',numerical_pipeline)
                    ])


#Let us add an estimator to the pipeline that was built

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

data_X= housing_df.drop('price',axis =1)

# Note the values of y are converted to numpy values
data_y= housing_df['price'].values

X_train,X_test,y_train,y_test = train_test_split(data_X,data_y,test_size =0.2, random_state = 42)


#Now let us build the final pipeline
full_model_pipeline = Pipeline(steps =[
                        ('full_pipeline',full_pipeline),
                        ('model',LinearRegression())
                    ])

full_model_pipeline.fit(X_train,y_train)
y_pred =full_model_pipeline.predict(X_test)
</code></pre>
<p>Here is the full stack trace:</p>
<pre class=""lang-ipyhton prettyprint-override""><code>AttributeError                            Traceback (most recent call last)
&lt;ipython-input-192-64f513a54376&gt; in &lt;module&gt;
     16                         ])
     17 
---&gt; 18 full_model_pipeline.fit(X_train,y_train)
     19 y_pred =full_model_pipeline.predict(X_test)

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    339         &quot;&quot;&quot;
    340         fit_params_steps = self._check_fit_params(**fit_params)
--&gt; 341         Xt = self._fit(X, y, **fit_params_steps)
    342         with _print_elapsed_time('Pipeline',
    343                                  self._log_message(len(self.steps) - 1)):

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/sklearn/pipeline.py in _fit(self, X, y, **fit_params_steps)
    301                 cloned_transformer = clone(transformer)
    302             # Fit or load from cache the current transformer
--&gt; 303             X, fitted_transformer = fit_transform_one_cached(
    304                 cloned_transformer, X, y, None,
    305                 message_clsname='Pipeline',

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/joblib/memory.py in __call__(self, *args, **kwargs)
    350 
    351     def __call__(self, *args, **kwargs):
--&gt; 352         return self.func(*args, **kwargs)
    353 
    354     def call_and_shelve(self, *args, **kwargs):

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    752     with _print_elapsed_time(message_clsname, message):
    753         if hasattr(transformer, 'fit_transform'):
--&gt; 754             res = transformer.fit_transform(X, y, **fit_params)
    755         else:
    756             res = transformer.fit(X, y, **fit_params).transform(X)

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    978             sum of n_components (output dimension) over transformers.
    979         &quot;&quot;&quot;
--&gt; 980         results = self._parallel_func(X, y, fit_params, _fit_transform_one)
    981         if not results:
    982             # All transformers are None

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/sklearn/pipeline.py in _parallel_func(self, X, y, fit_params, func)
   1000         transformers = list(self._iter())
   1001 
-&gt; 1002         return Parallel(n_jobs=self.n_jobs)(delayed(func)(
   1003             transformer, X, y, weight,
   1004             message_clsname='FeatureUnion',

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/joblib/parallel.py in __call__(self, iterable)
   1042                 self._iterating = self._original_iterator is not None
   1043 
-&gt; 1044             while self.dispatch_one_batch(iterator):
   1045                 pass
   1046 

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    857                 return False
    858             else:
--&gt; 859                 self._dispatch(tasks)
    860                 return True
    861 

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/joblib/parallel.py in _dispatch(self, batch)
    775         with self._lock:
    776             job_idx = len(self._jobs)
--&gt; 777             job = self._backend.apply_async(batch, callback=cb)
    778             # A job can complete so quickly than its callback is
    779             # called before we get here, causing self._jobs to

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/joblib/_parallel_backends.py in apply_async(self, func, callback)
    206     def apply_async(self, func, callback=None):
    207         &quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;
--&gt; 208         result = ImmediateResult(func)
    209         if callback:
    210             callback(result)

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/joblib/_parallel_backends.py in __init__(self, batch)
    570         # Don't delay the application, to avoid keeping the input
    571         # arguments in memory
--&gt; 572         self.results = batch()
    573 
    574     def get(self):

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/joblib/parallel.py in __call__(self)
    260         # change the default number of processes to -1
    261         with parallel_backend(self._backend, n_jobs=self._n_jobs):
--&gt; 262             return [func(*args, **kwargs)
    263                     for func, args, kwargs in self.items]
    264 

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/joblib/parallel.py in &lt;listcomp&gt;(.0)
    260         # change the default number of processes to -1
    261         with parallel_backend(self._backend, n_jobs=self._n_jobs):
--&gt; 262             return [func(*args, **kwargs)
    263                     for func, args, kwargs in self.items]
    264 

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/sklearn/utils/fixes.py in __call__(self, *args, **kwargs)
    220     def __call__(self, *args, **kwargs):
    221         with config_context(**self.config):
--&gt; 222             return self.function(*args, **kwargs)

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    752     with _print_elapsed_time(message_clsname, message):
    753         if hasattr(transformer, 'fit_transform'):
--&gt; 754             res = transformer.fit_transform(X, y, **fit_params)
    755         else:
    756             res = transformer.fit(X, y, **fit_params).transform(X)

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    385             fit_params_last_step = fit_params_steps[self.steps[-1][0]]
    386             if hasattr(last_step, 'fit_transform'):
--&gt; 387                 return last_step.fit_transform(Xt, y, **fit_params_last_step)
    388             else:
    389                 return last_step.fit(Xt, y,

~/ML_Projects/Base_ML_env/env/lib/python3.9/site-packages/sklearn/base.py in fit_transform(self, X, y, **fit_params)
    697         if y is None:
    698             # fit method of arity 1 (unsupervised transformation)
--&gt; 699             return self.fit(X, **fit_params).transform(X)
    700         else:
    701             # fit method of arity 2 (supervised transformation)

AttributeError: 'numpy.ndarray' object has no attribute 'fit'
</code></pre>
","transformer"
"102437","Where do Q vectors come from in Attention-based Sequence-to-Sequence Transformers?","2021-09-24 13:26:59","102447","0","72","<nlp><transformer><attention-mechanism>","<p>I'm taking a course on Attention-based NLP but I'm not understanding the calculation and application of Attention, based on the use of Q, K, and V vectors.  My understanding is that the K and V vectors are derived from the encoder input and the Q vector is derived from the decoder input.  This makes sense to me in the context of <em>training</em>, where the entire input sequence is presented to the encoder and the entire output sequence is presented to the decoder.  What does not make sense, however, is how this applies in the context of <em>inference</em>.  In that case, it would seem like there is no input to the decoder, so where does the Q vector come from?</p>
","transformer"
"102126","Developing a deep learning hybrid architecture for a particular problem is a highly complicated task","2021-09-16 10:02:40","","1","34","<deep-learning><time-series><computer-vision><transformer><research>","<p>I am currently conducting research on application of deep learning (sensor signal recognition). I spent about a year and a half sifting through the literature and discovered some research patterns. To begin, I noted the emergence of Convolutional Networks (CNNs). Individuals applied CNN to their problems and reported state-of-the-art outcomes. Then LSTM was proposed; it was quickly adopted and declared state-of-the-art. Then the trend shifted; people began to use hybrid architectures and reported cutting-edge results. The current trend is to focus on transformers and simple attention everywhere, plus work with hybrid deep learning architectures and publish their work with new baseline results.</p>
<p>In my experience, proposing a hybrid architecture to outperform the current state-of-the-art result is quite difficult. Specifically, I am puzzled about several issues:</p>
<p>1- At this point, I feel compelled to investigate a different domain, such as computer vision, and observe how specific architectures were applied. Because it is somewhat related to my domain, and because deep learning was initially developed for the natural language or computer vision domains, people began to consider how they could apply these new deep learning architectures to their domain and solve problems. So , I decided to look at other domains as well. Perhaps I'll discover a more effective approach that I can incorporate into my domain's current base architecture to improve the results. To begin, I fully understand the research paper and then consider how I might apply the xyz approach to my scenario. The issue is that even in 2021, hundreds of architectures (using transformers and attentions) are published. It is not trivial to comprehend other domain papers. Thus, the question is how to narrow down these papers and decide on which architecture, layer, or module to try.
2- Consider the case where you obtain an architecture based on intuition. You are about to conduct the experiments. It is nearly impossible to run the deep learning code once and achieve acceptable results. This is almost certainly going to require a lot of luck. At the very least, to validate your architecture and determine whether it can produce better results than the state-of-the-art, you should run your code at least 200 times and up to 3000 or 4000 times. Then, you can select the best result from all iterations and compare it to the baseline. Otherwise, you will need to tweak your code slightly and re-run the experiment. It is a lengthy process, and there is a 70% chance that you will not achieve the best results possible using the then-current baseline.</p>
<p>3- I wrote every line of code from scratch. However, I am not 100% sure that I implemented the exact idea that I had in mind. Due to the fact that we use multiple libraries to implement deep learning architectures, we cannot be 100% sure that either I made a mistake in my implementation code or that I have a problem with my proposed method.</p>
<p>So can we have a better way to at least minimize these problems as much as possible?</p>
","transformer"
"102098","How to use is_split_into_words with Huggingface NER pipeline","2021-09-15 06:44:03","122836","4","1676","<transformer><named-entity-recognition><huggingface>","<p>I am using Huggingface transformers for NER, following this excellent guide: <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">https://huggingface.co/blog/how-to-train</a>.</p>
<p>My incoming text has already been split into words. When tokenizing during training/fine-tuning I can use <code>tokenizer(text,is_split_into_words=True)</code> to tokenize the incoming text. However, I can't figure out how to do the same in a <code>pipeline</code> for predictions.</p>
<p>For example, the following works (but requires incoming text to be a string):</p>
<pre><code>s1 = &quot;Here is a sentence&quot;
p1 = pipeline(&quot;ner&quot;,model=model,tokenizer=tokenizer)
p1(s1)
</code></pre>
<p>But the following raises the following error: <code>Exception: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.</code></p>
<pre><code>s2 = &quot;Here is a sentence&quot;.split()
toks = tokenizer(s2,is_split_into_words=True)
p2 = pipeline(&quot;ner&quot;,model=model)
p2(toks)
</code></pre>
<p>I don't want to join the incoming text into one sentence because whitespace is significant in my use case. Post-processing the outputs of the pipeline will be complicated if I just pass in one string rather than a list of words.</p>
<p>Any advice on how I can use <code>is_split_into_words=True</code> functionality in the pipeline?</p>
","transformer"
"102069","What is the principial difference between zero-shot learning and k-NN and clusterization based methods?","2021-09-14 11:24:06","","2","31","<clustering><transformer><k-nn>","<p>One can consider clustering and k-NN to be a zero-shot, too?</p>
<p>I think there is no much principal difference, except using some neural network architecture (usually it is a transformer) which is used to build embedding space, i.e. simplify nearest neighbours search.</p>
<p>Am I wrong? What am I missing?</p>
","transformer"
"101783","Please explain Transformer vs LSTM using a sequence prediction example","2021-09-04 03:55:12","","1","1658","<nlp><lstm><rnn><transformer>","<p>I don't understand the difference in mechanics of a transformer vs LSTM for a sequence prediction problem. Here is what I have gathered so far:</p>
<p><strong>LSTM:</strong></p>
<p><a href=""https://i.sstatic.net/bmL98.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bmL98.png"" alt=""LSTM"" /></a></p>
<p>suppose we want to predict the remaining tokens in the word 'deep' given the first token 'd'. Then the first input will be 'd', and the predicted output is 'e'. Now at the next time step, the previously predicted output 'e' is fed along with the previous hidden state which contains information on 'd'. This is done till the predicted output is </p>
<p><strong>Transformer:</strong></p>
<p>In the same example, how would the transformer work, and avoid sequential inputs? Would we be giving the entire word 'deep' as input and leave it to the network to get the characters in correct sequence, or do we only input 'd' (which is what we did in LSTM)?</p>
<p>I'm really confused and I think I am missing out on some very fundamental concepts here. Would be really thankful for your help. Thanks!</p>
","transformer"
"100515","Transformer model is very slow and doesn't predict well","2021-08-25 19:35:33","","1","363","<machine-learning><python><deep-learning><keras><transformer>","<p>I created my first transformer model, after having worked so far with LSTMs. I created it for multivariate time series predictions - I have 10 different meteorological features (temperature, humidity, windspeed, pollution concentration a.o.) and with them I am trying to predict time sequences (24 consecutive values/hours) of air pollution. So my input has the shape <code>X.shape = (75575, 168, 10)</code> - 75575 time sequences, each sequence contains 168 hourly entries/vectors and each vector contains 10 meteo features. My output has the shape <code>y.shape = (75575, 24)</code> - 75575 sequences each containing 24 consecutive hourly values of the air pollution concentration.</p>
<p>I took as a model an <a href=""https://keras.io/examples/timeseries/timeseries_transformer_classification/"" rel=""nofollow noreferrer"">example</a> from the official keras site. It is created for classification problems, I only took out the <code>softmax</code> activation and in the last dense layer I set the number of neurons to 24 and I hoped it would work. I runs and trains, but it doesn't do a better job than the LSTMs I have used on the same problem and more importantly - it is very slow - 4 min/epoch. Below I attach the model and I would like to know:</p>
<p>I) Have I done something wrong in the model? can the accuracy or speed be improved? Are there maybe some other parts of the code I need to change for it to work on regression, not classification problems?</p>
<p>II) Also, can a transformer at all work on multivariate problems of my kind (10 features input, 1 feature output) or do transformers only work on univariate problems? Tnx</p>
<pre><code>def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):

    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):

        # Normalization and Attention
        x = layers.LayerNormalization(epsilon=1e-6)(x)
        x = layers.MultiHeadAttention(
            key_dim=head_size, num_heads=num_heads, dropout=dropout
        )(x, x)
        x = layers.Dropout(dropout)(x)
        res = x + inputs

        # Feed Forward Part
        x = layers.LayerNormalization(epsilon=1e-6)(res)
        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(dropout)(x)
        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
        x = x + res

    x = layers.GlobalAveragePooling1D(data_format=&quot;channels_first&quot;)(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=&quot;relu&quot;)(x)
        x = layers.Dropout(mlp_dropout)(x)
    x = layers.Dense(24)(x)
    return keras.Model(inputs, x)

model_tr = build_transformer_model(input_shape=(window_size, X_train.shape[2]), head_size=256, num_heads=4, ff_dim=4, num_transformer_blocks=4, mlp_units=[128], mlp_dropout=0.4, dropout=0.25)
model_tr.compile(loss=&quot;mse&quot;,optimizer='adam') 
m_tr_history = model_tr.fit(x=X_train, y=y_train, validation_split=0.25, batch_size=64, epochs=10, callbacks=[modelsave_cb])
</code></pre>
","transformer"
"100456","VQ-GAN understanding","2021-08-24 12:40:17","","3","388","<deep-learning><computer-vision><transformer><gan>","<p>I tried to understand how VQ-GAN works, but unfortunately I have not understood it. I tried to read some articles about it and watch a video. I believe a good and simple article will help me. You helped me with <a href=""https://datascience.stackexchange.com/questions/88823/transformers-understanding"">transformers</a>. You gave me wonderful link. I know what CNNs, GANs, TRANSFORMERSs, and CLIP are. I somehow understood a bit how VQ-VAE works, but still do not understand some things about it. But I cannot understand how VQ-GAN works. Please, help me to understand it. Thanks.</p>
","transformer"
"100274","Not clear about relative position bias","2021-08-19 11:39:15","","1","2605","<nlp><computer-vision><word-embeddings><transformer><embeddings>","<p>I've been reading the <a href=""https://arxiv.org/abs/2103.14030"" rel=""nofollow noreferrer"">Swin Transformer</a> paper and came across <strong>relative position bias</strong> concept. I'm not able to figure out how is it more effective than positional embeddings. I hope someone can explain it intuitively. Thanks in advance!</p>
","transformer"
"100160","How is attention different from linear MLPs?","2021-08-16 18:40:53","","4","2028","<deep-learning><nlp><transformer><attention-mechanism>","<p>Each output for both the attention layer (as in transformers) and MLPs or feedforward layer(linear-activation) are weighted sums of previous layer. So how they are different?</p>
","transformer"
"100021","Row embedding as output of a transformer - how are they defined?","2021-08-13 07:25:24","100026","1","186","<deep-learning><time-series><transformer>","<p>I am reading the paper <a href=""https://arxiv.org/abs/2011.01843"" rel=""nofollow noreferrer""><em>Tabular transformers for modeling multivariate time series</em></a> and am having issues understanding the structure in Fig. 2. In Sec. 2.2, the authors say that the field transformer processes rows individually, creating row embeddings. What exactly is a row embedding?</p>
<p><a href=""https://i.sstatic.net/wgj2F.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wgj2F.png"" alt=""enter image description here"" /></a></p>
","transformer"
"100001","Paraphrasing a sentence and changing the tone of it","2021-08-12 18:39:11","","1","295","<machine-learning><nlp><transformer><machine-translation>","<p>I am trying to make a model that is capable of translating a sentence into a new and a better form. I would like the model to change the tone and also give it some character. I am using this in my web app UI, simply allowing the users to witness new description as they refresh the page. For example, &quot;You are logged out&quot; -&gt; &quot;Looks like you have logged out&quot;. Something of such sort, any idea on this?</p>
","transformer"
"99755","An issue for sub-word tokenization preprocessing transformer","2021-08-05 22:47:38","99765","0","121","<python><nlp><bert><transformer><python-3.x>","<p>I'm stacked with executing <strong>the sub-word tokenization preprocessing</strong> to use transformer.</p>
<p>According to the tutorial on <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">the article</a>, I have executed the sample code.</p>
<p>However, one function was not defined properly and no hint to fix it on the article.</p>
<p>If you have any ideas to fix the code, could you help me?</p>
<h2>Error</h2>
<pre><code>     22  return X, y
     23 
---&gt; 24 X_train, y_train = preprocess(train_samples)
     25 X_val, y_val = preprocess(val_samples)

     12 def preprocess(samples):
     13  tag_index = {tag: i for i, tag in enumerate(schema)}
---&gt; 14  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
     15  max_len = max(map(len, tokenized_samples))
     16  X = np.zeros((len(samples), max_len), dtype=np.int32)

TypeError: 'module' object is not callable
</code></pre>
<h2>Code</h2>
<p>This code is from <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">the article</a> to build a model for named entity recognition using transformer.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tqdm
 
def tokenize_sample(sample):
  seq = [
         (subtoken, tag)
         for token, tag in sample
         for subtoken in tokenizer(token)['input_ids'][1:-1]
         ]
  return [(3, 'O')] + seq + [(4, 'O')]

def preprocess(samples):
  tag_index = {tag: i for i, tag in enumerate(schema)}
  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
  max_len = max(map(len, tokenized_samples))
  X = np.zeros((len(samples), max_len), dtype=np.int32)
  y = np.zeros((len(samples), max_len), dtype=np.int32)
  for i, sentence in enumerate(tokenized_samples):
    for j, (subtoken_id, tag) in enumerate(sentence):
      X[i, j] = subtoken_id
      y[i,j] = tag_index[tag]
  return X, y

X_train, y_train = preprocess(train_samples)
X_val, y_val = preprocess(val_samples)
</code></pre>
<h2>What I tried</h2>
<p>I checked that the function, tokenize_sample is executable with the below code.</p>
<p>However, I'm not sure how to insert it to the original code.</p>
<pre><code>for sample in samples:
  print(tokenize_sample(sample))
</code></pre>
","transformer"
"99741","Sub-word tokenization preprocessing to use transformer","2021-08-05 14:44:31","99767","0","90","<python><nlp><numpy><transformer><python-3.x>","<p>I'm stacked with executing <strong>the sub-word tokenization preprocessing</strong> to use transformer.</p>
<p>According to the tutorial on <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">the article</a>, I have executed the sample code.</p>
<p>However, one function was not defined properly and no hint to fix it on the article.</p>
<p>If you have any ideas to fix the code, could you help me?</p>
<h2>Error</h2>
<pre><code>     22  return X, y
     23 
---&gt; 24 X_train, y_train = preprocess(train_samples)
     25 X_val, y_val = preprocess(val_samples)

     12 def preprocess(samples):
     13  tag_index = {tag: i for i, tag in enumerate(schema)}
---&gt; 14  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
     15  max_len = max(map(len, tokenized_samples))
     16  X = np.zeros((len(samples), max_len), dtype=np.int32)

TypeError: 'module' object is not callable
</code></pre>
<h2>Code</h2>
<p>This code is from <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">the article</a> to build a model for named entity recognition using transformer.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tqdm
 
def tokenize_sample(sample):
  seq = [
         (subtoken, tag)
         for token, tag in sample
         for subtoken in tokenizer(token)['input_ids'][1:-1]
         ]
  return [(3, 'O')] + seq + [(4, 'O')]

def preprocess(samples):
  tag_index = {tag: i for i, tag in enumerate(schema)}
  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
  max_len = max(map(len, tokenized_samples))
  X = np.zeros((len(samples), max_len), dtype=np.int32)
  y = np.zeros((len(samples), max_len), dtype=np.int32)
  for i, sentence in enumerate(tokenized_samples):
    for j, (subtoken_id, tag) in enumerate(sentence):
      X[i, j] = subtoken_id
      y[i,j] = tag_index[tag]
  return X, y

X_train, y_train = preprocess(train_samples)
X_val, y_val = preprocess(val_samples)
</code></pre>
<h2>What I tried</h2>
<p>I checked that the function, tokenize_sample is executable with the below code.</p>
<p>However, I'm not sure how to insert it to the original code.</p>
<pre><code>for sample in samples:
  print(tokenize_sample(sample))
</code></pre>
","transformer"
"99688","Error to load a pre-trained BERT model","2021-08-04 13:39:27","99736","0","566","<python><nlp><bert><transformer>","<h2>Background</h2>
<p>I'm reading <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">this article</a> about a natural language task, named entity recognition and trying to load a pre-trained BERT model on Google colaboratory.</p>
<p><strong>How can I fix an error to load a pre-trained BERT model?</strong></p>
<h2>Code</h2>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoConfig, TFAutoModelForTokenClassification
MODEL_NAME = 'bert-base-german-cased' 
config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))
model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)
model.summary()
</code></pre>
<h2>Error</h2>
<p>I can understand that schema is not defined before the line, but I cannot find a clew on the article to fix it.</p>
<pre><code>      1 from transformers import AutoConfig, TFAutoModelForTokenClassification
      2 MODEL_NAME = 'bert-base-german-cased'
----&gt; 3 config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))
      4 model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)
      5 model.summary()

NameError: name 'schema' is not defined
</code></pre>
<h2>What I tried</h2>
<p>I checked <a href=""https://blog.codecentric.de/en/2020/11/take-control-of-named-entity-recognition-with-you-own-keras-model/"" rel=""nofollow noreferrer"">previous blogpost</a> following the advice from a comment, and found one description.</p>
<p>However, I'm not sure where to insert it to the original code.</p>
<pre><code>For simplicity, we’ll truncate the sentences to a maximum length and pad shorter input sequences. But first, let us determine the set of all tags in the data and add an extra tag for the padding:

#code
schema = ['_'] + sorted({tag for sentence in samples for _, tag in sentence})
</code></pre>
<p>Is it correct understanding?</p>
<pre><code>def load_data(filename: str):
   with open(filename, 'r') as file:
     lines = [line[:-1].split() for line in file]
     samples, start = [], 0
     for end, parts in enumerate(lines):
       if not parts:
         sample = [(token, tag.split('-')[-1]) for token, tag in lines[start:end]]
         samples.append(sample)
         start = end + 1
     if start &lt; end:
       samples.append(lines[start:end])
     
     return samples

samples = load_data('data/01_raw/bag.conll')
train_samples = load_data('data/01_raw/bag.conll')
val_samples = load_data('data/01_raw/bgh.conll')
all_samples = train_samples + val_samples

schema = ['_'] + sorted({tag for sentence in samples for _, tag in sentence})
</code></pre>
<p>I checked the output.</p>
<pre><code>print(schema)
#result
['_', 'AN', 'EUN', 'GRT', 'GS', 'INN', 'LD', 'LDS', 'LIT', 'MRK', 'O', 'ORG', 'PER', 'RR', 'RS', 'ST', 'STR', 'UN', 'VO', 'VS', 'VT']
</code></pre>
","transformer"
"98380","time series anomaly detection","2021-07-27 19:01:08","","0","99","<neural-network><time-series><autoencoder><transformer><attention-mechanism>","<p>I want to ask for time series anomaly detection we can apply tnn on multiple features or not?</p>
<p>I used transformer for sentiment analysis where I have to provide a sentence and it predicts its output as positive or negative. In another case I provided a single word and model predicts its language.
This is how it works that it takes single column input where as in time series dataset there are more than one columns as input.</p>
<p>I implemented transformer neural network  i am confused how can i add more layers in transformer like other neural network architecture ?</p>
","transformer"
"98002","Self-Attention Summation and Loss of Information","2021-07-17 11:02:25","","0","361","<deep-learning><transformer><attention-mechanism><information-theory>","<p>In self-attention, the attention for a word is calculated as:</p>
<p><span class=""math-container"">$$
A(q, K, V) = \sum_{i} \frac{exp(q.k^{&lt;i&gt;})}{\sum_{j} exp(q.k^{&lt;j&gt;})}v^{&lt;i&gt;}
$$</span></p>
<p>My question is why we sum over the Softmax*Value vectors.  Doesn't this lose information about which other words in particular are important to the word under consideration?</p>
<p>In other words, how does this summed vector point to which words are relevant?</p>
<p>For example, consider two extreme scenarios where practically the entire output depends on the attention vector of word <span class=""math-container"">$x^{&lt;t&gt;}$</span>, and one where it depends on the vector of word <span class=""math-container"">$x^{&lt;t+1&gt;}$</span>.  It's possible that <span class=""math-container"">$A(q, K, V)$</span> has the exact same values in both scenarios.</p>
","transformer"
"97628","How do the linear + softmax layers give out word probabilities in Transformer network?","2021-07-08 17:30:20","97629","0","1202","<machine-learning><nlp><pytorch><transformer>","<p>I am trying to implement a transformer network from scratch in pytorch to understand it. I am using <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">The illustrated transformer</a> for guidance. The part where I am stuck is about how do we go from the output of the final decoder layer to linear + softmax.</p>
<p>From what I have understood, if we have a batch of size B, max output seq length M, embedding dimension D, and vocab size V, then the output of the last decoder layer would be BxMxD which we have to turn into a vector of probabilities of size BxV so that we can apply softmax and get next predicted word. But how do we go from a variable size MxD matrix to a fixed-length V vector?</p>
<p><a href=""https://datascience.stackexchange.com/questions/74525/transformer-decoder-output-how-is-it-linear"">This post</a> says we apply the linear layer to all M vectors sequentially:</p>
<blockquote>
<p>That's the thing. It isn't flattened into a single vector. The linear
transformation is applied to all M vectors in the sequence
individually. These vectors have a fixed dimension, which is why it
works.</p>
</blockquote>
<p>But how do we coalesce those transformed vectors into just one single vector? Do we sum them up?</p>
","transformer"
"97617","BERT Optimization for Production","2021-07-08 13:18:05","97652","1","111","<nlp><bert><transformer><semantic-similarity>","<p>I'm  using BERT to transform text into 768 dim vector, It's multilingual :</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') 
</code></pre>
<p>Now i want to put the model into production but the embedding time is too much and i want to reduce and optimize the model <strong>to reduce the embedding time</strong> What are the libraries that enable me to do this ?</p>
","transformer"
"97310","What is the difference between BERT and Roberta","2021-07-01 11:02:12","97340","11","21066","<bert><transformer>","<p>I want to understand the difference between BERT and Roberta. I saw the article below.</p>
<p><a href=""https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8"" rel=""noreferrer"">https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8</a></p>
<p>It mentions that Roberta was trained on 10x more data but I don't understand the dynamic masking part. It says masked tokens change between epochs.
Shouldn't this flatten the learning curve?</p>
","transformer"
"97067","Using Subsequent Mask in Transformer Leads to NaN Outputs","2021-06-24 21:53:34","","2","1815","<deep-learning><neural-network><nlp><pytorch><transformer>","<p>I am trying to implement an autoregressive transformer model similar to the paper <em>attention is all you need</em>. From what I have understood, in order to replicate the architecture fully, I need to give the transformer decoder 3 masks.</p>
<ol>
<li><p>Target subsequent mask: this is for causality.</p>
</li>
<li><p>Target padding indexes: just to look at non-padded indices.</p>
</li>
<li><p>Encoder padding indices: just to look at non-padded inputs from the encoder.</p>
</li>
</ol>
<p>The snippet is here:</p>
<pre><code>y = self.decoder(y, x,
                         tgt_mask=tgt_causal_mask,
                         tgt_key_padding_mask=tgt_padding_mask,
                         memory_key_padding_mask=src_padding_mask)
</code></pre>
<hr />
<p>With masks being generated like this:</p>
<pre><code>def generate_no_peek_mask(self, sz):
    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
    mask = mask.float().masked_fill(mask == 0, float(&quot;-inf&quot;)).masked_fill(mask == 1, float(0.0))
    mask = mask.to(self.device)
    return mask

def generate_padding_mask(self, seq, pad_idx):
    return (seq != pad_idx).to(self.device)
</code></pre>
<p>The problem is that using these masks leads to issues with the Softmax function because of NaN values. Without these masks, the model does not generate any NaN value. I have tried toying with various input lengths and seeing what happens when I make sure my inputs are moderately big, but it still does not work. The only thing that works is not giving the decoder the masks.</p>
","transformer"
"96904","How to write a generator to fine-tune transformer based models (Tensorflow)","2021-06-21 06:00:20","","0","382","<keras><tensorflow><bert><transformer>","<p>I have been trying to write a generator for DistillBertFast model</p>
<pre><code>## Generator
def _generator(text=train_texts, label=Y_oh_train, batch_size=1):
# label = tf.ragged.constant(label)
while True:
    for i in range(0,len(text),batch_size):
        yield dict(tokenizer(text[i:i+batch_size], truncation=True, padding=True, return_tensors='tf')), label[i:i+batch_size]

## tf Dataset
train_dataset = tf.data.Dataset.from_generator(_generator, output_types=({'input_ids':tf.int32,
                                                                      'attention_mask':tf.int32}, tf.float32))


## model compile

    loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=True)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss=loss_fn,
    metrics=[tf.keras.metrics.categorical_accuracy])

## sample data
train_texts = ['This gif kills me Death is literally gushing towards you and you really gon do a whole 3point turn', 'LOVE TEST Raw Real JaDine', 'We would like to wish everyone a very Happy New Year and all the best in 2018']

Y_oh_train=array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0.],
   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0.],
   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0.]])
</code></pre>
<p>But when I try to fit the model it gives error:</p>
<pre><code>    ValueError                                Traceback (most recent call last)
&lt;ipython-input-195-05df82e86e2e&gt; in &lt;module&gt;()
      4     loss=loss_fn,
      5     metrics=[tf.keras.metrics.categorical_accuracy])
----&gt; 6 model.fit(t)

9 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1181                 _r=1):
   1182               callbacks.on_train_batch_begin(step)
-&gt; 1183               tmp_logs = self.train_function(iterator)
   1184               if data_handler.should_sync:
   1185                 context.async_wait()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--&gt; 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    931       # This is the first call of __call__, so we have to initialize.
    932       initializers = []
--&gt; 933       self._initialize(args, kwds, add_initializers_to=initializers)
    934     finally:
    935       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    762     self._concrete_stateful_fn = (
    763         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 764             *args, **kwds))
    765 
    766     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   3048       args, kwargs = None, None
   3049     with self._lock:
-&gt; 3050       graph_function, _ = self._maybe_define_function(args, kwargs)
   3051     return graph_function
   3052 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3442 
   3443           self._function_cache.missed.add(call_context_key)
-&gt; 3444           graph_function = self._create_graph_function(args, kwargs)
   3445           self._function_cache.primary[cache_key] = graph_function
   3446 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3287             arg_names=arg_names,
   3288             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3289             capture_by_value=self._capture_by_value),
   3290         self._function_attributes,
   3291         function_spec=self.function_spec,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    997         _, original_func = tf_decorator.unwrap(python_func)
    998 
--&gt; 999       func_outputs = python_func(*func_args, **func_kwargs)
   1000 
   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    670         # the function a weak reference to itself to avoid a reference cycle.
    671         with OptionalXlaContext(compile_with_xla):
--&gt; 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    673         return out
    674 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    984           except Exception as e:  # pylint:disable=broad-except
    985             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 986               raise e.ag_error_metadata.to_exception(e)
    987             else:
    988               raise

ValueError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:855 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py:800 call  *
        distilbert_output = self.distilbert(
    /usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py:415 call  *
        embedding_output = self.embeddings(
    /usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py:122 call  *
        final_embeddings = self.LayerNorm(inputs=final_embeddings)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1030 __call__  **
        outputs = call_fn(inputs, *args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization.py:1218 call
        ndims = len(input_shape)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:855 __len__
        raise ValueError(&quot;Cannot take the length of shape with unknown rank.&quot;)

ValueError: Cannot take the length of shape with unknown rank.
</code></pre>
<p>I have been trying to find a work around, I can't put in a fixed tensor shape in generator because I can't control the shape of output from generator, it'd be based on the max length on each call, I can't load all the data at once, since the data is too huge to be loaded in memory</p>
","transformer"
"96801","Attention transformation - matrices","2021-06-18 09:54:06","96803","0","89","<deep-learning><transformer><matrix><softmax>","<p>Could somebody explain which matrix dimension should be found here - K? and if it is for example 3X3, should I use just 9?</p>
<p><a href=""https://i.sstatic.net/kJpJ1.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kJpJ1.png"" alt=""enter image description here"" /></a></p>
","transformer"
"96595","Ways to build Abstractive summarisation and what are it's challenges","2021-06-14 07:30:21","","0","21","<deep-learning><nlp><bert><transformer>","<p>What are state of art techniques to build Abstractive summarisation on some paragraphs or articles and what kind of hurdles or challenges are there to approach this problem?</p>
","transformer"
"96511","Spatial positional encodings Vs Learned positional encodings(Object queries)","2021-06-11 12:08:54","","1","398","<deep-learning><object-detection><transformer>","<p>I have been trying to understand facebook's <a href=""https://arxiv.org/abs/2005.12872v1"" rel=""nofollow noreferrer"">Detection transformer</a>(DeTr) paper.<br></p>
<h1>Architecture</h1>
<p><a href=""https://i.sstatic.net/lphy3.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lphy3.png"" alt=""enter image description here"" /></a>
Most of the explanation about the architecture is straightforward. <br>
I don't especially understand the concept of <strong>object queries</strong>.</p>
<h1>Details</h1>
<p>In the paper,</p>
<blockquote>
<p>A transformer decoder then takes as input a small fixed number of
learned positional embeddings, which we call object queries, and
additionally attends to the encoder output.</p>
</blockquote>
<blockquote>
<p>There are 2 kinds of positional encodings in our model:
spatial positional encodings(fixed) and output positional encodings(Object queries).</p>
</blockquote>
<p><strong>FYI</strong>: <br>
<strong>Learned</strong> positional encodings and <strong>output</strong> positional encodings <strong>are same</strong>.</p>
<h1>Questions</h1>
<ol>
<li>What are these <strong>&quot;learned&quot;</strong> positional encodings(object queries)?</li>
<li>how are they different from spatial positional encodings?</li>
<li>When are they exactly <strong>learned</strong>?</li>
</ol>
","transformer"
"96345","why multiple attention heads learn differently","2021-06-06 18:46:19","","1","932","<bert><transformer><attention-mechanism>","<p>In transformer architecture multi head attention blocks are used. While visualizing their output it can be seen that every layer has learnt different relations of words. e.g., layer 5 has learnt that &quot;It&quot; is more related to &quot;animal&quot;.</p>
<p><a href=""https://i.sstatic.net/uTExM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uTExM.png"" alt=""sample attention layer 5"" /></a></p>
<p>Question here is, when all attention layers are running in parallel, what is different fed to different layer so that they learn different things?<br />
Note: this answer is not clear - <a href=""https://datascience.stackexchange.com/questions/42985/why-and-how-bert-can-learn-different-attentions-for-each-head"">why-and-how-bert-can-learn-different-attentions-for-each-head</a></p>
","transformer"
"96342","Attention weights - change during learning and prediction","2021-06-06 17:59:43","","0","914","<transformer><attention-mechanism><sequence-to-sequence>","<p>Assume a simple LSTM Followed by Attention layer or a full transformer architecture. The attention weights are learnt during training, which get multiplied with keys, queries and values.<br />
Please correct if my above understanding is wrong or below question.<br />
The question is, when these weights of attention layer gets changed and when not.</p>
<ol>
<li>Do attention layer weights change for each input in sequence? (I assume no, but please confirm)</li>
<li>Do attention layer weights get frozen during prediction (inference)? Or these keep on changing?</li>
<li>In transformers or Bert, were these weights supplied as part of pretrained model?</li>
</ol>
","transformer"
"96285","Struggling to understand/implement Transformer Decoder","2021-06-05 00:06:47","","3","251","<machine-learning><python><pytorch><transformer><attention-mechanism>","<p>I'm struggling to understand the decoder in a Transformer model, specifically with regards to some aspects of its architecture as well as how it actually handles the data during training.</p>
<p>What I have right now is a working implementation of the encoder which outputs some calculated self-attention scores (using scaled dot-product for my attention mechanism).</p>
<p>In my decoder, I understand that the correct &quot;target&quot; output is first embedded &amp; positionally encoded (which I have implemented). I then added a layer of self-attention to the decoder.</p>
<p>From what I understand, what follows is the encoder-decoder attention blocks.</p>
<p><a href=""https://i.sstatic.net/kOs4Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kOs4Z.png"" alt=""Transformer Decoder Block"" /></a></p>
<p>Firstly, should the self-attention layer be included in the decoder block to be repeated alongside the encoder-decoder attention layer, or should this just occur once after the target is passed into the decoder?</p>
<p>Next, for the actual encoder-decoder attention, I am a bit uncertain as to how the query, key, and value vectors are calculated. From my understanding, the output of the encoder is passed through a feedforward layer to calculate the key and value vectors, whereas the output of the previous block of the decoder is passed through a feedforward layer to calculate the query vector. Thus, my implementation is as follows:</p>
<pre><code>x = self.encoder(x)
y = self.decoder_self_attention(y)
for decoder_block in self.decoder_blocks:
    y = decoder_block(x, y)
</code></pre>
<p>This makes mathematical sense to me (the desired output should have the same embedding dimensionality as the input to the decoder block, so the query vector <em>has</em> to be from the decoder as it is the leftmost matrix in the dot-product attention calculation). However, I fail to understand how this allows it to handle recurrent data. This is especially confusing as the final layer is a feedforward layer, and I fail to understand how this would handle sequences rather than say, an LSTM.</p>
<p>In addition, I have read very conflicting information on when the masking operation occurs. Some papers perform masking on the embedding directly, whereas some perform it after positional encoding, whereas some implement it in the attention mechanism. My model simply adds the mask to the cross product of the query and key vectors during the cross product calculation (before softmax), but I'm unsure how to verify this is correct.</p>
<pre><code>z = (q @ k.transpose(-2, -1)) * scale # Q K^T / sqrt(d)
z += self.mask # before softmax, set everything above diagonal to -inf
</code></pre>
<p>Lastly, although I am certain I'm failing to understand some aspect of the decoder causing a bug, I am unable to get my model to fit onto data of different modalities. My input embedding is of shape <span class=""math-container"">$(T_a, E_a)$</span> where <span class=""math-container"">$T_a$</span> is the number of tokens and <span class=""math-container"">$E_a$</span> is the embedding dimension. However, my output dimension is of shape <span class=""math-container"">$(T_b, E_b)$</span> where <span class=""math-container"">$T_b \neq T_a$</span> and <span class=""math-container"">$E_a \neq E_b$</span>.</p>
<p>While mathematically this should be fine with respect to the attention mechanism, using the default PyTorch Encoder and Decoder layers throws a multitude of errors unless <span class=""math-container"">$T_a = T_b$</span> and <span class=""math-container"">$E_a = E_b$</span>, which leads me to believe there may be issues when the input and output sequence are of different modalities. Although my model does converge loss-wise, when I pass in training data and translate the output back into tokens, the output is absolutely nonsensical. I have checked the exact same dataset against a more traditional LSTM auto-encoder and gotten sensible results, which leads me to believe I may simply have misunderstood some pieces of the transformer architecture.</p>
","transformer"
"95164","Transformer removes heading labels","2021-06-02 08:23:14","","1","11","<transformer><labels>","<p><strong>Question: Is there a way to retain/include headers once they go through a make_column_transformer?</strong></p>
<p>My dataframe has the usual columns and rows, but a header is included.</p>
<p>Example:</p>
<p><a href=""https://i.sstatic.net/zn4vZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zn4vZ.png"" alt=""enter image description here"" /></a></p>
<p>When I take the data through a transformer, the column labels are removed and the data is displayed in a scientific format.</p>
<p>Example of the first row:</p>
<p><a href=""https://i.sstatic.net/HEwLg.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HEwLg.png"" alt=""enter image description here"" /></a></p>
<p>This obviously makes it difficult to interpret the features. Is there a way to keep or add labels?</p>
","transformer"
"95040","1D Data for NLP models","2021-05-29 20:40:42","","0","15","<machine-learning><deep-learning><nlp><transformer>","<p>My dataset( Network traffic dataset where we do binary classification)-</p>
<p><a href=""https://i.sstatic.net/ehEw4.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ehEw4.png"" alt=""enter image description here"" /></a></p>
<p>Number of features in data is 25</p>
<p>Can we use this kind of dataset in NLP models like transformers? If yes what should be the transformations to be done on this to feed it into models like transformers for binary classification?</p>
","transformer"
"94987","An algorithm to extract the purpose of a document","2021-05-27 17:33:50","","0","35","<nlp><lstm><bert><transformer><attention-mechanism>","<p>I want to build an algorithm to extract the <strong>purpose of the document</strong> (scientific papers for example) by extracting the sentences that state the purpose. I don't have many annotated data so I might use a semi-supervised learning algorithm. I was thinking of training a Q/A algorithm (using Bert for example..) but I want something to be specific to the purpose task..</p>
<p>Any idea or keywords that might help ?
Thanks!</p>
","transformer"
"94973","Transformer: English -> Source Code Training Accuracy stuck 60% and validation 40%","2021-05-27 13:26:27","","0","161","<machine-learning><deep-learning><nlp><tensorflow><transformer>","<p>I'm working on my final year project which is to write a model that takes as an input an english sentence and generate source code (currently testing on english-&gt;javascript dataset provided by CodeSearchNet) as output.</p>
<p>So, I decided to use Transformers as my model because I read many articles and it seemed that it is a good seq2seq model with good result.</p>
<p>I have used the transformer code provided by the Tensorflow documentation and I have applied few changes of course like now I'm using a pre-trained tokenizer called CodeBert which is pretrained on nl --&gt; 6 programming language (java/javascript/python/....)</p>
<p>The problem I'm facing that when I run my training(50k training data,5k validation), after 20epochs my training accuracy gets stuck around 60-65% and my validation accuarcy around 40-45% I even tried to train for 30 more epochs but it is the same and result of generation is no where close to something correct. Here what it looks like :</p>
<p>English: generate a number from 1 to 10
Result :
function ( ) { var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0 ; var i = 0x0 = 0x0 &amp; 0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0x0000000000000000000000000000000000000000000000000000000000000000000000</p>
<p>I don't really know what I'm doing wrong, I have tried to play around with hyper-parameter but no result. I'm stuck I really need your advice if someone could help me and thanks in advance. here is my code:</p>
<p>Positional Encoding:</p>
<pre><code>import numpy as np
import tensorflow as tf
from tensorflow.keras import layers
class PositionalEncoding(layers.Layer):
def __init__(self):
    super(PositionalEncoding, self).__init__()

def get_angles(self, pos, i, d_model):
    angles = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
    return pos * angles

def call(self, inputs):
    seq_length = inputs.shape.as_list()[-2]
    d_model = inputs.shape.as_list()[-1]
    angles = self.get_angles(np.arange(seq_length)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)
    angles[:, 0::2] = np.sin(angles[:, 0::2])
    angles[:, 1::2] = np.cos(angles[:, 1::2])
    pos_encoding = angles[np.newaxis, ...]
    return inputs + tf.cast(pos_encoding, tf.float32)
</code></pre>
<p>Multi-head Attention</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers

def scaled_dot_product_attention(queries,keys,values,mask):
    product = tf.matmul(queries,keys,transpose_b=True)
    keys_dim = tf.cast(tf.shape(keys)[-1],tf.float32)
    scaled_product = product/tf.math.sqrt(keys_dim)
    if mask is not None:
        scaled_product += (mask * -1e9)
    attention = tf.matmul(tf.nn.softmax(scaled_product,axis=-1),values)
    return attention


class MultiHeadAttention(layers.Layer):
    def __init__(self,nb_proj):
        super(MultiHeadAttention, self).__init__()
        self.nb_proj = nb_proj

    def build(self, input_shape):
        self.d_model = input_shape[-1]
        assert self.d_model % self.nb_proj == 0

        self.d_proj = self.d_model // self.nb_proj

        self.query_lin = layers.Dense(units=self.d_model)
        self.key_lin = layers.Dense(units=self.d_model)
        self.value_lin = layers.Dense(units=self.d_model)

        self.final_lin = layers.Dense(units=self.d_model)

    def split_proj(self,inputs,batch_size):
        shape = (batch_size,-1,self.nb_proj,self.d_proj)
        splited_inputs = tf.reshape(inputs,shape=shape)
        return tf.transpose(splited_inputs,perm=[0,2,1,3])

    def call(self,queries,keys,values,mask):
        batch_size = tf.shape(queries)[0]

        queries = self.query_lin(queries)
        keys = self.key_lin(keys)
        values = self.value_lin(values)

        queries = self.split_proj(queries,batch_size)
        keys = self.split_proj(keys,batch_size)
        values = self.split_proj(values,batch_size)

        attention = scaled_dot_product_attention(queries,keys,values,mask)

        attention = tf.transpose(attention,perm=[0,2,1,3])

        concat_attention = tf.reshape(attention,shape=(batch_size,-1,self.d_model))

        output = self.final_lin(concat_attention)

        return output
</code></pre>
<p>Encoder Layer</p>
<pre><code>from tensorflow.keras import layers
from TransformerArch import MultiHeadAttention


class EncoderLayer(layers.Layer):
    def __init__(self, FFN_units, nb_proj, dropout):
        super(EncoderLayer, self).__init__()
        self.FFN_units = FFN_units
        self.nb_proj = nb_proj
        self.dropout = dropout

    def build(self, input_shape):
        self.d_model = input_shape[-1]
        self.multi_head_attention = MultiHeadAttention.MultiHeadAttention(self.nb_proj)
        self.dropout_1 = layers.Dropout(rate=self.dropout)
        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)
        self.dense_1 = layers.Dense(units=self.FFN_units, activation=&quot;relu&quot;)
        self.dense_2 = layers.Dense(units=self.d_model)
        self.dropout_2 = layers.Dropout(rate=self.dropout)
        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)

    def call(self, inputs, mask, training):

        attention = self.multi_head_attention(inputs, inputs, inputs, mask)

        attention = self.dropout_1(attention, training=training)

        attention = self.norm_1(attention + inputs)

        outputs = self.dense_1(attention)

        outputs = self.dense_2(outputs)

        outputs = self.dropout_2(outputs)

        outputs = self.norm_2(outputs + attention)

        return outputs
</code></pre>
<p>Encoder:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers
from TransformerArch import PositionalEncoding
from TransformerArch import EncoderLayer


class Encoder(layers.Layer):
    def __init__(self, nb_layers, FFN_units, nb_proj, dropout, vocab_size, d_model, name=&quot;encoder&quot;):
        super(Encoder, self).__init__(name=name)
        self.nb_layers = nb_layers
        self.FFN_units = FFN_units
        self.nb_proj = nb_proj
        self.dropout = dropout
        self.d_model = d_model

        self.embedding = layers.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding.PositionalEncoding()
        self.dropout = layers.Dropout(rate=dropout)
        self.enc_layers = [EncoderLayer.EncoderLayer(FFN_units, nb_proj, dropout)
                           for _ in range(nb_layers)]

    def call(self, inputs, mask, training):
        outputs = self.embedding(inputs)
        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        outputs = self.pos_encoding(outputs)
        outputs = self.dropout(outputs, training)
        for i in range(self.nb_layers):
            outputs = self.enc_layers[i](outputs, mask, training)
        return outputs
</code></pre>
<p>Decoder Layer:</p>
<pre><code>from tensorflow.keras import layers
from TransformerArch import MultiHeadAttention


class DecoderLayer(layers.Layer):
    def __init__(self, FFN_units, nb_proj, dropout):
        super(DecoderLayer, self).__init__()
        self.FFN_units = FFN_units
        self.nb_proj = nb_proj
        self.dropout = dropout

    def build(self, input_shape):

        self.d_model = input_shape[-1]
        self.multi_head_attention_1 = MultiHeadAttention.MultiHeadAttention(self.nb_proj)
        self.dropout_1 = layers.Dropout(rate=self.dropout)
        self.norm_1 = layers.LayerNormalization(epsilon=1e-3)

        self.multi_head_attention_2 = MultiHeadAttention.MultiHeadAttention(self.nb_proj)
        self.dropout_2 = layers.Dropout(rate=self.dropout)
        self.norm_2 = layers.LayerNormalization(epsilon=1e-3)

        self.dense_1 = layers.Dense(units=self.FFN_units, activation=&quot;relu&quot;)
        self.dense_2 = layers.Dense(units=self.d_model)
        self.dropout_3 = layers.Dropout(rate=self.dropout)
        self.norm_3 = layers.LayerNormalization(epsilon=1e-3)

    def call(self, inputs, enc_outputs, mask_1, mask_2, training):
        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)

        attention = self.dropout_1(attention, training)
        attention = self.norm_1(attention + inputs)

        attention_2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2)
        attention_2 = self.dropout_2(attention_2, training)
        attention_2 = self.norm_2(attention_2 + attention)


        outputs = self.dense_1(attention_2)
        outputs = self.dense_2(outputs)
        outputs = self.dropout_3(outputs, training)
        outputs = self.norm_3(outputs + attention_2)


        return outputs
</code></pre>
<p>Decoder:</p>
<pre><code>import tensorflow as tf
from tensorflow.keras import layers
from TransformerArch import DecoderLayer
from TransformerArch import PositionalEncoding


class Decoder(layers.Layer):
    def __init__(self, nb_layers, FFN_units, nb_proj, dropout, vocab_size, d_model, name=&quot;decoder&quot;):
        super(Decoder, self).__init__(name=name)
        self.nb_layers = nb_layers
        self.d_model = d_model

        self.embedding = layers.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding.PositionalEncoding()
        self.dropout = layers.Dropout(rate=dropout)
        self.dec_layers = [DecoderLayer.DecoderLayer(FFN_units, nb_proj, dropout) for _ in range(nb_layers)]

    def call(self, inputs, enc_outputs, mask_1, mask_2, training):
        outputs = self.embedding(inputs)

        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))

        outputs = self.pos_encoding(outputs)

        outputs = self.dropout(outputs, training)

        for i in range(self.nb_layers):
            outputs = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training)

        return outputs
</code></pre>
<p>Transformer:</p>
<pre><code>import tensorflow as tf
from TransformerArch import Encoder
from TransformerArch import Decoder
from tensorflow.keras import layers


class Transformer(tf.keras.Model):
    def __init__(self, vocab_size_enc, vocab_size_dec, d_model, nb_layers, FFN_units, nb_proj, dropout,
                 name=&quot;transfomer&quot;):
        super(Transformer, self).__init__(name=name)
        self.encoder = Encoder.Encoder(nb_layers, FFN_units, nb_proj, dropout, vocab_size_enc, d_model)
        self.decoder = Decoder.Decoder(nb_layers, FFN_units, nb_proj, dropout, vocab_size_dec, d_model)
        self.last_linear = layers.Dense(units=vocab_size_dec)

    def create_padding_mask(self, seq):
        mask = tf.cast(tf.math.equal(seq, 1), tf.float32)
        return mask[:, tf.newaxis, tf.newaxis, :]

    def create_look_ahead_mask(self, seq):
        seq_len = tf.shape(seq)[1]
        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)
        return look_ahead_mask

    #def call(self, enc_inputs, dec_inputs, training):
    def call(self, inputs):
        enc_mask = self.create_padding_mask(inputs[0])
        dec_mask_1 = tf.maximum(self.create_padding_mask(inputs[1]), self.create_look_ahead_mask(inputs[1]))
        dec_mask_2 = self.create_padding_mask(inputs[0])
        enc_outputs = self.encoder(inputs[0], enc_mask, inputs[2])
        dec_outputs = self.decoder(inputs[1], enc_outputs, dec_mask_1, dec_mask_2, inputs[2])
        outputs = self.last_linear(dec_outputs)
        return outputs
</code></pre>
<p>Training Code:</p>
<pre><code>from TransformerArch import Transformer
import tensorflow as tf
import time
import os
import DPPreLN, ValidationS


TF_CONFIG_ = tf.compat.v1.ConfigProto()
TF_CONFIG_.gpu_options.allow_growth = True
sess = tf.compat.v1.Session(config=TF_CONFIG_)

DIRNAME = os.path.dirname(__file__)
with tf.device('/GPU:0'):
    tf.keras.backend.clear_session()

    # Hyper-parameters
    D_MODEL = 128  # 512 #128
    NB_LAYERS = 4  # 6 #4
    FFN_UNITS = 256  # 2048 #512
    NB_PROJ = 8  # 8
    DROPOUT = 0.1  # 0.1

    transformer = Transformer.Transformer(vocab_size_enc=50265, vocab_size_dec=50265, d_model=D_MODEL,
                                          nb_layers=NB_LAYERS,
                                          FFN_units=FFN_UNITS, nb_proj=NB_PROJ, dropout=DROPOUT)

    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=&quot;none&quot;)


    def loss_function(target, pred):
        mask = tf.math.logical_not(tf.math.equal(target, 1)) #CodeBert pad Token ID is 1 not 0
        loss = loss_object(target, pred)
        mask = tf.cast(mask, dtype=loss.dtype)
        loss *= mask
        return tf.reduce_sum(loss) / tf.reduce_sum(mask)


    def accuracy_function(real, pred):
        accuracies = tf.equal(real, tf.cast(tf.argmax(pred, axis=2), tf.int32))

        mask = tf.math.logical_not(tf.math.equal(real, 1))
        accuracies = tf.math.logical_and(mask, accuracies)

        accuracies = tf.cast(accuracies, dtype=tf.int32)
        mask = tf.cast(mask, dtype=accuracies.dtype)
        return tf.reduce_sum(accuracies) / tf.reduce_sum(mask)


    train_loss = tf.keras.metrics.Mean(name=&quot;train_loss&quot;)
    train_accuracy = tf.keras.metrics.Mean(name=&quot;train_accuracy&quot;)

    valid_loss = tf.keras.metrics.Mean(name=&quot;valid_loss&quot;)
    valid_accuracy = tf.keras.metrics.Mean(name=&quot;valid_accuracy&quot;)


    class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
        def __init__(self, d_model, warmup_steps=4000):
            super(CustomSchedule, self).__init__()
            self.d_model = tf.cast(d_model, tf.float32)
            self.warmup_steps = warmup_steps

        def __call__(self, step):
            arg1 = tf.math.rsqrt(step)
            arg2 = step * (self.warmup_steps ** -1.5)
            self.lr = tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)
            return self.lr

        def get_config(self):
            config = {
                'd_model': self.d_model,
                'warmup_steps': self.warmup_steps,
                'lr': self.lr
            }
            return config


    learning_rate = CustomSchedule(D_MODEL)

    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate ,beta_1=0.9, beta_2=0.98, epsilon=1e-9)

    checkpoint_path = os.path.join(DIRNAME, &quot;Checkpoint&quot;)  # path to pc

    ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)

    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)

    if ckpt_manager.latest_checkpoint:
        ckpt.restore(ckpt_manager.latest_checkpoint)
        print(&quot;Latest Checkpoint restored !&quot;)

    EPOCHS = 50
    dataset = DPPreLN.prepareData()
    valid_set = ValidationS.prepareData()
    
    for epoch in range(EPOCHS):
        print(&quot;Start of epoch {}&quot;.format(epoch + 1))
        start = time.time()
        train_loss.reset_states()
        train_accuracy.reset_states()
        for (batch, (enc_inputs, targets)) in enumerate(dataset):
            dec_inputs = targets[:, :-1]
            dec_outputs_real = targets[:, 1:]
            with tf.GradientTape() as tape:
                predictions = transformer([enc_inputs, dec_inputs, True])
                loss = loss_function(dec_outputs_real, predictions)
            gradients = tape.gradient(loss, transformer.trainable_variables)
            optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))
            train_loss(loss)
            train_accuracy(accuracy_function(dec_outputs_real, predictions))
            
            if batch % 50 == 0:
                print(
                    &quot;Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}&quot;.format(epoch + 1, batch,
                                                                           train_loss.result(),
                                                                           train_accuracy.result() * 100,
                                                                           ))

        # VALIDATION TEST
        print(&quot;***** VALIDATION PART *****&quot;)
        valid_loss.reset_states()
        valid_accuracy.reset_states()
        for (batch, (enc_inputs, targets)) in enumerate(valid_set):
            dec_inputs = targets[:, :-1]
            dec_outputs_real = targets[:, 1:]
            predictions = transformer([enc_inputs, dec_inputs, False])
            loss = loss_function(dec_outputs_real, predictions)
            valid_loss(loss)
            valid_accuracy(accuracy_function(dec_outputs_real, predictions))
            if batch % 50 == 0:
                print(
                    &quot;Validation :Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}&quot;.format(epoch + 1, batch,
                                                                                       valid_loss.result(),
                                                                                       valid_accuracy.result() * 100,
                                                                                       ))

        # SAVE CHECKPOINT/MODEL
        ckpt_save_path = ckpt_manager.save()
        print(&quot;Saving checkpoint for epoch {} at {} &quot;.format(epoch + 1, ckpt_save_path))
        print(&quot;Time taken for 1 epoch : {} secs\n&quot;.format(time.time() - start))
        transformer.save_weights(os.path.join(DIRNAME, &quot;Models&quot;, &quot;Weight&quot;))
        print(&quot;*****************************************&quot;)
        print(&quot;*   Model Has Been Successfully Saved   *&quot;)
        print(&quot;*****************************************&quot;)


    print(transformer.summary())
</code></pre>
","transformer"
"94890","Why do we need dot product as part of the Transformer's training process?","2021-05-25 15:05:54","","1","46","<machine-learning><transformer><attention-mechanism>","<p>I do understand that dot product conveys the meaning of similarity in a vector space. At the same time it looks like during the training process we are learning the weights( or how much attention) each token in a sequence should put into other tokens. So the question is why is that important to have scalar value from the dot product when we could just learn a corresponding(a bigger for example) weight during training.</p>
<p>One of the reasons behind my confusion comes from a common example/explanation of how attention/self-attention works: model is able to 'understand' which other word token <code>it</code> is connected to.</p>
<blockquote>
<p>it(animal) is too tired, or it(road) is too wide</p>
</blockquote>
<p>In that scenario  dot product between <code>it</code> and <code>animal/road</code> should be almost the same( given reasonable initial words' embeddings). And we are still able to learn proper weights.</p>
<p>Questions:</p>
<ol>
<li>If and in what way dot product helps to learn attention weights?</li>
<li>maybe there are other benefits that dot product brings us?</li>
</ol>
","transformer"
"94886","In Transformer's multi-headed attention, how attending ""different representation subspaces at different positions"" is achieved?","2021-05-25 14:14:57","94893","2","1088","<machine-learning><deep-learning><neural-network><transformer><attention-mechanism>","<p>Question partially inspired by <a href=""https://datascience.stackexchange.com/questions/55647/multi-head-attention-mechanism-in-transformer-and-need-of-feed-forward-neural-ne"">this post</a> about the need of multi-head attention mechanism.</p>
<p>For me though it is still not clear how we will be able to initialise those attention heads in a diverse way(so that they potentially can - as stated in the Attention is all you need paper - attend to information from different representation subspaces at different positions) and most importantly preserve this diversity during the training process.</p>
","transformer"
"94685","What exactly is the linear layer in the transformer model?","2021-05-20 13:52:21","94687","3","3683","<deep-learning><neural-network><transformer><attention-mechanism>","<p>Please see this image:
<a href=""https://i.sstatic.net/PSqZ7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/PSqZ7.png"" alt="""" /></a></p>
<p>There are linear layers to modify the Query, key and value matrices and one linear layer after the multi head attention as they mention also from here:</p>
<p><a href=""https://i.sstatic.net/KhFxR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/KhFxR.png"" alt="""" /></a></p>
<p>Are these linear layers simply dense or fully connected layers? Let's consider the weight matrix W<sub>i</sub> <sup>Q</sup>. Does this represent a dense layer with &quot;Q&quot; nodes? As they are using matrices as input rather than 1D vectors, I am getting a little confused.</p>
","transformer"
"94406","Transformer: where is the output of the last FF sub-layer of the encoder used?","2021-05-13 20:11:02","94423","0","131","<transformer>","<p>In the &quot;Attention Is All You Need&quot; paper, the decoder consists of two attention sub-layers in each layer followed by a FF sub-layer.
The first is a masked self attention which gets as an input the output of the decoder in the previous step (and the first input is a special start token).
The second, 'encoder-decoder', attention sub-layer gets as an input queries from the lower self-attention sub-layer and keys &amp; values from the encoder.
I do not see the use of the output of the FF sub-layer in the encoder; can someone explain where is it used?
Thanks</p>
","transformer"
"93803","Verifying the implementation of Multihead Attention in Transformer","2021-05-01 04:48:22","","0","41","<deep-learning><neural-network><keras><tensorflow><transformer>","<p>I have implemented the <code>MultiAttention head</code> in <code>Transformers</code>. There are so many implementations around so its confusing. Can someone please verify if my implementation is correct:</p>
<pre><code>import tensorflow as tf

def scaled_dot_product(q,k,v):
    
    #calculates Q . K(transpose)
    qkt = tf.matmul(q,k,transpose_b=True)
    #caculates scaling factor
    dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))
    scaled_qkt = qkt/dk
    softmax = tf.nn.softmax(scaled_qkt,axis=-1)
    
    z = tf.matmul(softmax,v)
    #shape: (m,Tx,depth), same shape as q,k,v
    return z

class MultiAttention(tf.keras.layers.Layer):
    
    def __init__(self,d_model,num_of_heads):
        
        super(MultiAttention,self).__init__()
        self.d_model = d_model
        self.num_of_heads = num_of_heads
        self.depth = d_model//num_of_heads
        self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]
        self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]
        self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]
        self.wo = tf.keras.layers.Dense(d_model)
        
    def call(self,x):
        
        multi_attn = []
        for i in range(self.num_of_heads):
            Q = self.wq[i](x)
            K = self.wk[i](x)
            V = self.wv[i](x)
            multi_attn.append(scaled_dot_product(Q,K,V))
            
        multi_head = tf.concat(multi_attn,axis=-1)
        multi_head_attention = self.wo(multi_head)
        return multi_head_attention

#Calling the attention 
multi = MultiAttention(d_model=512,num_of_heads=8)
m = 5; sequence_length = 4; word_embedding_dim = 512
sample_ip = tf.constant(tf.random.normal(shape=(m,sequence_length,word_embedding_dim)))
attn =multi(sample_ip)
#shape of op (attn): (5,4,512)
        
        
</code></pre>
","transformer"
"93768","Dimensions of Transformer - dmodel and depth","2021-04-30 08:05:31","93775","8","11097","<deep-learning><neural-network><keras><tensorflow><transformer>","<p>Trying to understand the dimensions of the <code>Multihead Attention</code> component in <code>Transformer</code> referring the following tutorial <a href=""https://www.tensorflow.org/tutorials/text/transformer#setup"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/text/transformer#setup</a></p>
<p>There are 2 unknown dimensions - <code>depth and d_model</code> which I dont understand.</p>
<p>For example, if I fix the dimensions of the <code>Q,K,V</code> as <code>64</code> and the <code>number_of_attention_heads</code> as <code>8</code>, and <code>input_embedding</code> as <code>512</code> , can anyone please explain what is <code>depth and d_model</code>?</p>
","transformer"
"93638","Is positional encoding (in transformers) an estimation of the relative positions of words in the training corpus texts?","2021-04-27 19:23:31","93655","1","338","<bert><transformer>","<p>Is this some kind of estimation of the relative positions of words in the training texts? are they creating some kind of statistical &quot;distribution&quot; of words? is &quot;cat&quot; usually 2 or 3 words away from &quot;milk&quot; in English language? things have to have a meaning, havent they?  Is BERT just adding some aditional dimensions to the vector space to include info on the relative positions of words?</p>
","transformer"
"93535","Pytorch: understanding the purpose of each argument in the forward function of nn.TransformerDecoder","2021-04-25 18:33:48","93539","3","3496","<nlp><pytorch><transformer><sequence-to-sequence><text-generation>","<p>According to <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html"" rel=""nofollow noreferrer"">https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html</a>, the <a href=""https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html#torch.nn.TransformerDecoder.forward"" rel=""nofollow noreferrer"">forward function</a> of nn.TransformerDecoder contemplates the following arguments:</p>
<ul>
<li><strong>tgt</strong> – the sequence to the decoder (required).</li>
<li><strong>memory</strong> – the sequence from the last layer of the encoder (required).</li>
<li><strong>tgt_mask</strong> – the mask for the tgt sequence (optional).</li>
<li><strong>memory_mask</strong> – the mask for the memory sequence (optional).</li>
<li><strong>tgt_key_padding_mask</strong> – the mask for the tgt keys per batch (optional).</li>
<li><strong>memory_key_padding_mask</strong> – the mask for the memory keys per batch (optional).</li>
</ul>
<p>Unfortunately, Pytorch's official documentation on the function isn't exactly very thorough at this point (April 2021), in terms of the expected dimensions of each tensor and when it does or doesn't make sense to use each of the optional arguments.</p>
<p>For example, <a href=""https://datascience.stackexchange.com/a/93146/46305"">in previous conversations</a> it was explained to me that <strong>tgt_mask</strong> is usually a square matrix used for self attention masking to prevent future tokens from leaking into the prediction of past tokens. Similarly, <strong>tgt_key_padding_mask</strong> is used for masking padding tokens (which happens when you pad a batch of sequences of different lengths so that they can fit into a single tensor). In light of this, it makes total sense to use tgt_mask in the decoder, but I wouldn't be so sure about tgt_key_padding_mask. What would be the point of masking target padding tokens? Isn't it enough to simply ignore the predictions associated to padding tokens during training (say, you could do something like <code>nn.CrossEntropyLoss(ignore_index=PADDING_INDEX)</code> and that's it)?</p>
<p>More generally, and considering that the current documentation is not as thorough as one would like it to be, I would like to know what the purpose is of each argument of nn.TransformerDecoder's forward function, when it makes sense to use each of the optional arguments, and if there are nuances in the usage one should keep in mind when switching between training and inference modes.</p>
","transformer"
"93487","Masked Language Modeling on Domain-specific Data","2021-04-24 08:53:23","","1","475","<deep-learning><nlp><transformer><huggingface>","<p>My goal is to have a language model that understands the relationships between words and can fill the masks in a sentence related to a specific domain. At first, I thought about pretraining or even training a language model(like BERT) from scratch, but unfortunately, my data isn't that big to help the previous model learn new connections, let alone learn the embeddings from scratch.</p>
<p>Now what I have in mind is creating a transformer model with my own vocabulary which consists of words in my domain-specific data (after separating them with spaces and not using transformer tokenizers). This way the vocab size would be smaller and the positions and relations would be learned faster and more easily. Although I'm a bit confused about implementation.</p>
<p>Can I use <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">this architecture (that is for NMT)</a> and give plain text for both the input and output? or should I mask some tokens in the input and give the complete sentence as the label?</p>
<p>Any other suggestions?</p>
","transformer"
"93419","Embedding from Transformer-based model from paragraph or documnet (like Doc2Vec)","2021-04-22 18:47:47","","1","1119","<nlp><bert><transformer><embeddings><doc2vec>","<p>I have a set of data that contains the different lengths of sequences. On average the sequence length is 600. The dataset is like this:</p>
<pre><code>S1 = ['Walk','Eat','Going school','Eat','Watching movie','Walk'......,'Sleep']
S2 = ['Eat','Eat','Going school','Walk','Walk','Watching movie'.......,'Eat']
.........................................
.........................................
S50 = ['Walk','Going school','Eat','Eat','Watching movie','Sleep',.......,'Walk']
</code></pre>
<p>The number of unique actions in the dataset are fixed. That means some sentences may not contain all of the actions.</p>
<p>By using Doc2Vec (Gensim library particularly), I was able to extract embedding for each of the sequences and used that for later task (i.e., clustering or similarity measure)</p>
<p>As transformer is the state-of-the-art method for NLP task. I am thinking if Transformer-based model can be used for similar task. While searching for this technique I came across the <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">sentence-Transformer</a>. But it uses a pretrained BERT model (which is probably for language but my case is not related to language) to encode the sentences. Is there any way I can get embedding from my dataset using Transformer-based model?</p>
","transformer"
"93377","Is time series forecasting possible with a transformer?","2021-04-21 19:49:19","","7","7311","<time-series><pytorch><forecasting><transformer>","<p>For my bachelor project I've been tasked with making a transformer that can forecast time series data, specifically powergrid data. I need to take a univariate time series of length N, that can then predict another univariate time series M steps into the future.</p>
<p>I started out by following the &quot;<a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention is all you need</a>&quot; paper but since this paper is meant for NLP I had to make some changes. Instead of embedding each word to a random point in d_model-dimensional space I use a linear layer embed the data. I've also tried using a nn.Conv1d layer with a kernel size of 1 to embed, but these approaches fail to make a non-linear prediction of the data and instead only predict a straight line through the average of the data.</p>
<p>First I though that the problem was my implementation of the transformer, but even when I use Pytorch' build in nn.Transformer module I get the same results. I then tried different types of positional encoding like the <a href=""https://arxiv.org/abs/1907.05321"" rel=""nofollow noreferrer"">&quot;Time2Vec&quot;</a> paper that approximates the data by using different sinus functions.</p>
<p>I feel like I've tried a lot of different things to make this transformer work but to no avail. So my question is, do transformers alone work for multistep forecasting with univariate data. And if so are there any articles, papers, repositories etc. that forecasts time series data with succes? If not which approach should I take going forwards to see if I can get my transformer to work.</p>
<p><strong>Edit: I figured out the problem, apparently the only issue was that I had set my learning rate too high :)</strong></p>
","transformer"
"93144","Minimal working example or tutorial showing how to use Pytorch's nn.TransformerDecoder for batch text generation in training and inference modes?","2021-04-16 16:11:45","93146","7","10099","<pytorch><transformer><sequence-to-sequence><text-generation><huggingface>","<p>I want to solve a sequence-to-sequence text generation task (e.g. question answering, language translation, etc.).</p>
<p>For the purposes of this question, you may assume that I already have the input part already handled. (I already have a tensor of dimensions batch_size x num_input_tokens x input_dim representing the input sequences. Also, all input sequences in my problem are of the same length, so no masking is required on the input side of things).</p>
<p>Now, I want to generate the output sequences using nn.TransformerDecoder. I'm aware of Pytorch's official tutorial <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""noreferrer"">SEQUENCE-TO-SEQUENCE MODELING WITH NN.TRANSFORMER AND TORCHTEXT</a>. Unfortunately, the official tutorial doesn't meet my needs, for the following reasons:</p>
<ul>
<li>nn.TransformerDecoder is not used in the example.</li>
<li>The example is about language modeling, not text generation. There is no forward loop that generates text word by word.</li>
</ul>
<p>I've searched around the web and I've found a few things, but nothing like a simple and minimal working example that directly applies to my problem setting. Concretely, on the output side of things I need the following:</p>
<ul>
<li>I want to generate output sequences in batch. I've found codes on GitHub where people appear to be doing text generation, but they do it for a single sequence at a time, not a batch of multiple sequences.</li>
<li>The output sequences may have different lengths.</li>
<li>I want to train my model with the teacher-forcing strategy and batches of multiple sequences. Given that in training I know the lengths of the sequences in advance, you may assume that I already have my batches padded with zeroes. However, I still need to figure out how to implement the forward function of my model, with a generation loop that uses nn.TransformerDecoder. Basically, I need to figure out how to iterate word-wise over my batch of output sequences, masking out the future words in each step (so that the model doesn't cheat by trivially predicting the next words).</li>
<li>Then, I need a similar forward function for inference mode. I need to figure out how to implement the generation loop to do basically the same as in training mode, except that instead of teacher-forcing I want to implement greedy search (i.e. use the tokens with highest predicted probability at iteration i as the next input for iteration i+1).</li>
</ul>
<p>I already know how to do all this using LSTMs. Below you can see the forward function of a model that I implemented in the past to do exactly what I just said with an LSTM. The same forward function is used for both training and inference, depending on the value of the variable 'mode':</p>
<pre><code>  def forward(
      self,
      image_local_features,
      question_vectors,
      answers=None,
      max_answer_length=None,
      mode='train',
  ):
    if mode == 'train':
      batch_size, max_answer_length = answers.shape
      assert answers is not None
    else:
      batch_size = image_local_features.size(0)
      assert max_answer_length is not None
    
    y = self.embedding_table(self.start_idx).expand(batch_size, -1)
    o = torch.zeros(batch_size, self.hidden_size).to(DEVICE)
    h = self.W_h(question_vectors)
    c = self.W_c(question_vectors)

    if mode == 'train':
      answer_embeddings = self.embedding_table(answers.permute(1,0))
      assert answer_embeddings.shape == (max_answer_length, batch_size, self.embed_size)

    output = []

    for t in range(max_answer_length):
      y_bar = torch.cat((y,o),1)
      assert y_bar.shape == (batch_size, self.embed_size + self.hidden_size)
      assert h.shape == (batch_size, self.hidden_size)
      assert c.shape == (batch_size, self.hidden_size)
      h, c = self.lstm_cell(y_bar, (h, c))
      e = (self.W_attn(image_local_features) * h.unsqueeze(1)).sum(-1)
      att = torch.softmax(e,-1)
      a = (image_local_features * att.unsqueeze(2)).sum(1)
      assert a.shape == (batch_size, self.image_local_feat_size)
      u = torch.cat((a,h),1)
      assert u.shape == (batch_size, self.hidden_size + self.image_local_feat_size)
      v = self.W_u(u)
      o = self.dropout(torch.tanh(v))
      assert o.shape == (batch_size, self.hidden_size)
      output.append(self.W_vocab(o))
      if mode == 'train':
        y = answer_embeddings[t] # teacher-forcing
      else:
        y = self.embedding_table(torch.argmax(output[t], 1)) # greedy search
      assert y.shape == (batch_size, self.embed_size)

    output = torch.stack(output, 1)
    assert output.shape == (batch_size, max_answer_length, self.vocab_size)
    return output
</code></pre>
<p>Another way to phrase my question would be: how can I reimplement what I did with LSTMs using nn.TransformerDecoder instead?</p>
<p>Any minimal working / <em>hello world</em> example that shows how to do batch training and batch inference with nn.TransformerDecoder for text generation will be very appreciated.</p>
<hr />
<p><em>Note</em>: alternatively, if there is a straightforward way of accomplishing the same with an out-of-the-box solution from <a href=""https://huggingface.co/transformers/"" rel=""noreferrer"">hugginface</a>, that would be awesome too.</p>
","transformer"
"93038","Why are convolutions still used in some Transformer networks for speech enhancement?","2021-04-14 01:47:43","93061","1","133","<machine-learning><transformer><attention-mechanism>","<p>So I’ve read in <a href=""https://arxiv.org/pdf/1706.03762v5.pdf"" rel=""nofollow noreferrer"">Attention is All You Need</a> that Transformers remove the need for recurrence and convolutions entirely. However, I’ve seen some TNNs (such as <a href=""https://arxiv.org/pdf/2010.13154.pdf"" rel=""nofollow noreferrer"">SepFormer</a>, <a href=""https://arxiv.org/pdf/2007.13975.pdf"" rel=""nofollow noreferrer"">DPTNet</a>, and <a href=""https://arxiv.org/pdf/2103.09963v1.pdf"" rel=""nofollow noreferrer"">TSTNN</a>) that still utilize convolutions. Is there any particular reason for this? Doesn’t that defeat the purpose of Transformers?</p>
","transformer"
"92953","Does multi-head attention remove the need for self-attention?","2021-04-12 08:52:18","92980","5","2948","<deep-learning><transformer><attention-mechanism>","<p>The title may be confusing but suppose I were to build Transformer Neural Network with a masking network that utilizes multi-head attention (like that in <a href=""https://arxiv.org/pdf/2010.13154.pdf"" rel=""noreferrer"">SepFormer</a>), would adding self-attention in the encoder and decoder still be necessary?</p>
","transformer"
"92630","Save and Load Simple Transformer Model","2021-04-06 10:49:03","","1","7098","<python><transformer><torch>","<p>I have trained Text classifier using simpleTranformer.ai I am struggling to save  and load the model in docker container. Please let me know how can I save the trained model and then load it into different environment smoothly.
I am using this library to : <a href=""https://simpletransformers.ai/"" rel=""nofollow noreferrer"">https://simpletransformers.ai/</a> to train a text model using these commands</p>
<pre><code> model = ClassificationModel('xlmroberta', 'xlm-roberta-base',use_cuda=cuda_available, num_labels=78, args={'learning_rate':1e-5, 'num_train_epochs': 1,'train_batch_size':256,'eval_batch_size':1048, 'n_gpu':4, 'reprocess_input_data': True, 
'overwrite_output_dir':True, 'overwrite_output_dir': True})

model.train_model(train_df)
</code></pre>
<p>I am saving the trained model using pytorch function:</p>
<pre><code>torch.save(model, 'classifier')
</code></pre>
<p>But its showing error of some missing files when I tried to load this model from different virtual machine. So, I am looking for best alternative to save and load the simpleTransformer model.</p>
","transformer"
"92602","Intuition of ""Head"" in Attention models (Transformer)?","2021-04-05 23:49:53","","0","149","<transformer><attention-mechanism>","<p>I keep seeing the &quot;head&quot; in attention models (transformers). Aside from the mathematical formula, could anyone please share the intuition behind the idea &quot;head&quot;?</p>
","transformer"
"92568","learn information from text and resolve problem using transformers","2021-04-05 09:42:23","103696","0","51","<tensorflow><transformer><question-answering>","<p>Let's imagine that we have some question, like this: &quot;x multiplied by x equals 9. What is x?&quot;
For this easy question answer is +-3. I want to make AI model answer on questions like that.
To train model we have only corpus<br>
Ex:
&quot;If some variable in power one multiplied by itself and equals to some digit, then we have to get root square from this digit in order to find x&quot;. Model has to learn some data from text. Model can't just look for answers in text.</p>
<p>How to do model which could answer on this kind of questions?
I'm waiting for some tips.</p>
<p><strong>UPD:</strong>
I heard that someone had made AI to resolve schrodinger equation. I want to reproduce their work, but for my own tasks ,which are more easy.</p>
","transformer"
"92269","BERT Masked Language Model question","2021-03-28 15:04:26","92384","0","85","<bert><transformer>","<p>I have been reading about BERT from the internet, and from what I understand the point of masked language modelling for BERT pretraining is so that BERT will learn to guess a &quot;masked&quot; word from the context given. The loss function will be the lowest for output embeddings which are closest to the original masked word embedding. Wouldn't it be that using this loss funtion does not guarantee that BERT will output word embeddings with context and instead could just output an embedding closest to the original masked word embedding without the relevant context?</p>
<p>Thanks.</p>
","transformer"
"90908","How to do NER predictions with Huggingface BERT transformer","2021-03-20 01:59:04","","1","2445","<machine-learning><tensorflow><transformer><named-entity-recognition><huggingface>","<p>I am trying to do a prediction on a test data set without any labels for an NER problem.</p>
<p>Here is some background. I am doing named entity recognition using tensorflow and Keras. I am using huggingface transformers.</p>
<p>I have two datasets. A train dataset and a test dataset. The training set has labels, the tests does not.
Below you will see what a tokenized sentence looks like, what it's labels look like, and what it looks like after encoding</p>
<pre><code>['The', 'pope', &quot;isn't&quot;, 'really', 'making', 'much', 'of', 'an', 'effort', '.', 'He', &quot;'s&quot;, 'wearing', 'the', 'same', 'clothes', 'as', 'yesterday', '.']
['O', 'B-person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
[101, 1109, 17460, 2762, 112, 189, 1541, 1543, 1277, 1104, 1126, 3098, 119, 1124, 112, 188, 3351, 1103, 1269, 3459, 1112, 8128, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</code></pre>
<p>Here is the code on how I tokenized my text and encoded my labels</p>
<pre><code>from transformers import DistilBertTokenizerFast

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')
train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)
val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)

def encode_tags(tags, encodings):
    labels = [[tag2id[tag] for tag in doc] for doc in tags]
    encoded_labels = []
    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):
        # create an empty array of -100
        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100
        arr_offset = np.array(doc_offset)

        # set labels whose first offset position is 0 and the second is not 0
        doc_enc_labels[(arr_offset[:,0] == 0) &amp; (arr_offset[:,1] != 0)] = doc_labels
        encoded_labels.append(doc_enc_labels.tolist())

    return encoded_labels

train_labels = encode_tags(train_tags, train_encodings)
val_labels = encode_tags(val_tags, val_encodings)

</code></pre>
<p>I have gotten my model to train and work. I'm getting pretty goods numbers when validating. Here is how that was done</p>
<pre><code>from transformers import TFDistilBertForTokenClassification, TFTrainer, TFTrainingArguments

training_args = TFTrainingArguments(
    output_dir='./results',
    num_train_epochs=5,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=16,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    evaluation_strategy = &quot;epoch&quot;,
    learning_rate = 2e-5
)

with training_args.strategy.scope():
    model = TFDistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))

trainer = TFTrainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,            # evaluation dataset
    compute_metrics=compute_metrics
)

trainer.train()

trainer.evaluate()
</code></pre>
<p>My main issue is that I don't know how to predict with this. I'm not familiar with the library and the documentation has not been helping much.</p>
<p>I can apparently use <code>trainer.predict(*param*)</code>, but I can't figure out what to actually input as the <em>param</em>.</p>
<p>On the other hand, when I do <code>model.predict(param)</code> where the param is the encoded sentence example I show above, I get this result</p>
<pre><code>TFTokenClassifierOutput(loss=None, logits=array([[[-0.3232851 ,  0.12578554, -0.47193137, ...,  0.16509804,
          0.19799986, -0.3560003 ]],

       [[-1.8808482 , -1.07631   , -0.49765658, ..., -0.7443374 ,
         -1.2379731 , -0.5022731 ]],

       [[-1.4291595 , -1.8587289 , -1.5842767 , ..., -1.1863587 ,
         -0.21151644, -0.52205306]],

       ...,

       [[-1.6405941 , -1.2474233 , -1.0701559 , ..., -1.1816512 ,
          0.323739  , -0.45317683]],

       [[-1.6405947 , -1.247423  , -1.0701554 , ..., -1.1816509 ,
          0.3237388 , -0.45317668]],

       [[-1.6405947 , -1.247423  , -1.0701554 , ..., -1.1816509 ,
          0.3237388 , -0.45317668]]], dtype=float32), hidden_states=None, attentions=None)
</code></pre>
<p>I don't know how I'm supposed to take that result and decode it back into labels. What am I supposed to do with the logits array? How am I supposed to predict this?</p>
","transformer"
"90783","There could be a problem with the linear layer after the attention inside a transformer?","2021-03-17 14:48:26","90787","0","198","<deep-learning><neural-network><transformer><attention-mechanism><masking>","<p>My question regards this image:</p>
<p><a href=""https://i.sstatic.net/eZNUO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/eZNUO.png"" alt=""enter image description here"" /></a></p>
<p>It seems that after the multi head attention there is a linear layer as they mention also from here:
<a href=""https://i.sstatic.net/lF3XT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/lF3XT.png"" alt=""enter image description here"" /></a></p>
<p>the linearity is given by the weights W^{o}. my quesion is: for the decoder, doesn't this linear layer mess up with the masking of the attention? I mean, if each token from the attention layer should not depend from the next tokens then these weights seem to mess up the whole thing or they don't? Indeed these linear weights will learn the dependencies among all the tokens and during inference there could be a problem, maybe(?). Thanks for any clarification.</p>
","transformer"
"90441","How does the Transformer predict n steps into the future?","2021-03-09 14:08:13","90443","4","2669","<tensorflow><pytorch><transformer><sequence-to-sequence>","<p>I have barely been able to find an implementation of the Transformer (that is not bloated nor confusing), and the one that I've used as reference was the PyTorch implementation. However, the Pytorch implementation requires you to pass the input (<em>src</em>) and the target (<em>tgt</em>) tensors for every step, rather than encoding the input once and keep on iterating for <strong>n</strong> steps to generate the full output. Am I missing something here?</p>
<p>My first guesses were that the <strong>Transformer</strong> isn't <em>technically</em> a seq2seq model, that I have not understood how I'm supposed to implement it, or that I've just been implementing seq2seq models incorrectly for the last few years :)</p>
","transformer"
"90290","Decoder Transformer feedforward","2021-03-05 10:40:52","90292","2","640","<neural-network><deep-learning><transformer><attention-mechanism><masking>","<p>I have a question about the decoder transformer feed forward during training.</p>
<p>Let's pick an example: input data <code>&quot;i love the sun&quot;</code> traduction i want to predict (italian traduction) <code>&quot;io amo il sole&quot;</code>.</p>
<p>Now i feed the encoder with the input &quot;i love the sun&quot; and i get the hidden states.
Now i have to do multiple feed forwards on the decoder with the input &quot;BOS io amo il&quot;
where BOS is a token that stands for beginning of sentence.
So i have this feedforward i assume</p>
<ul>
<li>[BOS, IO, AMO, IL] -&gt; decoder -&gt; IO</li>
<li>[BOS, IO, AMO, IL] -&gt; decoder -&gt; AMO</li>
<li>[BOS, IO, AMO, IL] -&gt; decoder -&gt; IL</li>
<li>[BOS, IO, AMO, IL] -&gt; decoder -&gt; SOLE</li>
</ul>
<p>I think this is the correct way. And what should be applied to differentiate the training i think is the masked attention mechanism maybe(?)
is it right to assume that the masking will be</p>
<pre><code>[1 0 0 0,
 0 0 0 0 ,
 0 0 0 0,
 0 0 0 0]   for the first feed forward

[1 0 0 0,
 1 1 0 0 ,
 0 0 0 0,
 0 0 0 0]   for the second feed forward

[1 0 0 0,
 1 1 0 0 ,
 1 1 1 0,
 0 0 0 0]   for the third feed forward

[1 0 0 0,
 1 1 0 0 ,
 1 1 1 0,
 1 1 1 1]   for the fourth feed forward
</code></pre>
<p>is it the correct way? or what should be different?
If you can provide me also a python implementation could be useful, thanks in advance.</p>
","transformer"
"89915","Where can I find documentation or paper mentioning pre-trained distilbert-base-nli-mean-tokens model?","2021-02-25 14:42:20","","0","1110","<nlp><word-embeddings><bert><transformer><embeddings>","<p>I am trying to find more information about pre-trained model <code>distilbert-base-nli-mean-tokens</code>. Can someone please point me to it's paper or documentation? Is it based on <a href=""https://arxiv.org/abs/1910.01108"" rel=""nofollow noreferrer"">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> paper? This is published in March 2020. I am looking for links between this paper and Sentence-BERT (<a href=""https://www.sbert.net/index.html"" rel=""nofollow noreferrer"">sentence-transformers</a>). Original <a href=""https://arxiv.org/pdf/1908.10084.pdf"" rel=""nofollow noreferrer"">sentence-bert</a> paper is published in Aug 2019. I wanted to try pre-trained model using S-BERT model and hence tried <code>distilbert-base-nli-mean-tokens</code> <a href=""https://www.sbert.net/examples/training/sts/README.html"" rel=""nofollow noreferrer"">model</a>. After implementation I found out that it's much faster than other pre-trained models available on sentence-transformer <a href=""https://www.sbert.net/examples/training/sts/README.html"" rel=""nofollow noreferrer"">website</a>. While studying it's paper I realised original paper do not mention this pre-trained model.</p>
<p>I found <a href=""https://arxiv.org/pdf/2004.09813.pdf"" rel=""nofollow noreferrer"">Making Monolingual Sentence Embeddings Multilingual using
Knowledge Distillation</a> this paper published by same author which do mention <code>DistilmBERT</code> but not <code>DistilBert</code> Can someone please help me solve this mystery?</p>
","transformer"
"89630","Why does my manual derivative of Layer Normalization imply no gradient flow?","2021-02-19 21:43:34","89659","2","987","<normalization><transformer><gradient>","<p>I recently tried computing the derivative of the layer norm function (<a href=""https://arxiv.org/abs/1607.06450"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1607.06450</a>), an essential component of transformers, but the result suggests that no gradient flows through the operation, which can't be true.</p>
<p>Here's my calculations:</p>
<p><span class=""math-container"">$\textrm{Given a vector of real numbers $X$ of length $N$, indexed as $x_i$,}\\
\textrm{we define the following operations:}\\
\mu =\frac{\sum_{k=1}^{N}{x_k}}{N}\\
\sigma = \sqrt{\frac{\sum_{k=1}^{N}{(x_k-\mu)^2}}{N}}\\
y_i=\frac{(x_i-\mu)}{\sigma}\\
\textrm{We seek to calculate the derivative of $y_i$ w.r.t $X$. That is,}\\
\frac{dy_i}{dX} = \sum^{N}_{k=1}\frac{dy_i}{dx_k}\\
\textrm{By the quotient rule:}\\
\frac{dy_i}{dx_j}=\frac{(x_i-\mu)'\sigma-(x_i-\mu)\sigma'}{\sigma^2}\\
(x_i-\mu)'=\delta_{ij}-\mu'\\
\mu'=\frac{1}{N}\\
\implies(x_i-\mu)' = \delta_{ij}-\frac{1}{N}\\
\sigma'=\frac{1}{2}(\frac{\sum_{k=1}^{N}{(x_k-\mu)^2}}{N})^{-\frac{1}{2}}*[\frac{\sum_{k=1}^{N}{(x_k-\mu)^2}}{N}]'\\
[\frac{\sum_{k=1}^{N}{(x_k-\mu)^2}}{N}]'=\frac{1}{N}\sum_{k=1}^{N}2*(x_k-\mu)(\delta_{kj}-\frac{1}{N})\\
\qquad =\frac{2}{N}\sum_{k=1}^{N}(x_k-\mu)\delta_{ij}-(x_k-\mu)\frac{1}{N}\\
\textrm{Note that $\delta_{kj}$ is only 1 when when $k=j$ and 0 otherwise, so we can further reduce:}\\
\qquad =\frac{2}{N}((x_j-\mu)-\sum_{k=1}^{N}(x_k-\mu)\frac{1}{N})\\
\qquad =\frac{2}{N}((x_j-\mu)-\frac{1}{N}\sum_{k=1}^{N}(x_k)+\frac{1}{N}\sum_{k=1}^{N}\mu)\\
\qquad =\frac{2}{N}((x_j-\mu)-\mu-\frac{1}{N}N\mu)\\
\qquad =\frac{2}{N}(x_j-\mu)\\
\textrm{Thus plugging that back into $\sigma'$ we get:}\\
\sigma'=\frac{1}{2}(\frac{\sum_{k=1}^{N}{(x_k-\mu)^2}}{N})^{-\frac{1}{2}}*\frac{2}{N}(x_j-\mu)\\
\quad=\frac{1}{N}(\frac{1}{\sigma})*(x_j-\mu)\\
\quad=\frac{(x_j-\mu)}{N\sigma}\\
\textrm{Now that we have all the components we can return to the derivative $\frac{dy_i}{dx_j}$:}\\
\frac{dy_i}{dx_j}=\frac{(x_i-\mu)'\sigma-(x_i)\sigma'}{\sigma^2}\\
\qquad=\frac{(x_i-\mu)'\sigma}{\sigma^2}-\frac{(x_i-\mu)\sigma'}{\sigma^2}\\
\qquad=\frac{\delta_{ij}-\frac{1}{N}}{\sigma}-\frac{(x_i-\mu)\frac{(x_j-\mu)}{N\sigma}}{\sigma^2}\\
\qquad=\frac{\delta_{ij}-\frac{1}{N}}{\sigma}-\frac{(x_i-\mu)(x_j-\mu)}{N\sigma^3}\\
\qquad=\frac{1}{N\sigma}(N\delta_{ij}-1-\frac{(x_i-\mu)(x_j-\mu)}{\sigma^2})\\
\qquad=\frac{1}{N\sigma}(N\delta_{ij}-1-\frac{(x_i-\mu)}{\sigma}\frac{(x_j-\mu)}{\sigma})\\
\qquad=\frac{1}{N\sigma}(N\delta_{ij}-1-y_iy_j)\\
\textrm{Finally, returning to $\frac{dy_i}{dX}$:}\\
\frac{dy_i}{dX}=\sum^{N}_{j=1}\frac{1}{N\sigma}(N\delta_{ij}-1-y_iy_j)\\
\textrm{Note that we are adding $N$ once (when $i=j$) and $(-1)$ $N$ times, so we can simplify to:}\\
\frac{dy_i}{dX}=\frac{1}{N\sigma}(N+(-1)N-\sum^{N}_{j=1}y_iy_j)\\
\quad=\frac{1}{N\sigma}(-\sum^{N}_{j=1}y_iy_j)\\
\quad=\frac{1}{N\sigma}(-y_i\sum^{N}_{j=1}y_j)\\
\quad=\frac{-y_i}{\sigma}\frac{(\sum^{N}_{j=1}y_j)}{N}\\
\quad=\frac{-y_i}{\sigma}\mu_y\\
\textrm{BUT by properties of data following a standard normal distribution $\mu_y=0$, so}\\
\frac{dy_i}{dX}=\frac{-y_i}{\sigma}0\\
\quad=0\\
\textrm{Which means no gradient flows through a layer normalization}\\\\$</span></p>
<p>I'm almost certain I've simply made a mistake somewhere, so if someone could point it out I'd greatly appreciate it. Thanks!</p>
","transformer"
"89357","How to use BERT in seq2seq model?","2021-02-14 11:40:02","89375","1","2629","<neural-network><bert><transformer><transfer-learning><sequence-to-sequence>","<p>I would like to use pretrained BERT as encoder of transformer model. The decoder has the same vocabulary as encoder and I am going to use shared embeddings. But I need <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code> tokens which are not trained with BERT. How should I get them ? Can I use <code>&lt;CLS&gt;</code> token as <code>&lt;SOS&gt;</code> and <code>&lt;SEP&gt;</code> as <code>&lt;EOS&gt;</code> ? Or I have to create these two embeddings as trainable Variables and concatenate them to the decoder input / labels ?</p>
","transformer"
"89014","Are all 110 million parameter in bert are trainable","2021-02-06 05:38:05","89018","1","853","<deep-learning><neural-network><nlp><bert><transformer>","<p>I am trying to understand are all these 110 million parameters trainable of bert uncased model. Is there any non trainable parameters in this image below?</p>
<p>By trainable I understand they are initialized with random weight and during pretraining these weights are backpropagated and updated.</p>
<p><a href=""https://i.sstatic.net/tMIhL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tMIhL.png"" alt=""Bert 110 Million Parameters"" /></a></p>
","transformer"
"88981","What are the inputs to the first decoder layer in a Transformer model during the training phase?","2021-02-05 14:05:19","88983","9","6676","<deep-learning><transformer>","<p>I am trying to wrap my head around how the Transformer architecture works. I think I have a decent top-level understanding of the encoder part, sort of how the Key, Query, and Value tensors work in the MultiHead attention layers. What I am struggling with is the decoder part, specifically the inputs to the very first decoder layer.</p>
<p>I understand that there are two things. The output of the final encoder layer, but before that an embedded (positional encoding + embedding) version of... well something.</p>
<p>In the original <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">paper</a> in Figure 1, they mention that the first decoder layer input is the Outputs (shifted right). I am a little confused on what they mean by &quot;shifted right&quot;, but if I had to guess I would say the following is happening</p>
<p>Input: &lt;Start&gt; How are you &lt;EOS&gt;
Output: &lt;Start&gt; I am fine &lt;EOS&gt;</p>
<p>and so the input to the first decoder layer will be [&lt;Start&gt; I am fine].</p>
<p>What is the need for shifting the sequence? Why would we not just input the target sequence itself? I was thinking maybe it's because of the auto-regressive nature of the decoder part, but then the only difference between the sequences would be the &lt;EOS&gt; token if I am seeing this correctly.</p>
<p>As you can probably tell I am a little bit confused by how some of the parts work, so any help to get to a better understanding would be much appreciated.</p>
","transformer"
"88977","Backpropagation of a transformer","2021-02-05 13:22:11","88978","3","5928","<deep-learning><neural-network><nlp><bert><transformer>","<p>when a transformer model is trained there is linear layer in the end of decoder which i understand is a fully connected neural network. During training of a transformer model when a loss is obtained it will backpropagate to adjust the weights.</p>
<p>My question is how deep the backpropagation is?</p>
<ul>
<li>does it happen only till linear layer weights(fully connected neural net) ?</li>
<li>OR does it extend to all the decoder layer weight matrices(Q,K,V) and Feed forward layers weights?</li>
<li>OR does it extend to the even the encoder + decoder weights ?</li>
</ul>
<p>Please help me with the answer.</p>
","transformer"
"88824","Unigram tokenizer: how does it work?","2021-02-02 13:28:18","88831","5","4302","<nlp><transformer><tokenization>","<p>I have been trying to understand how the unigram tokenizer works since it is used in the sentencePiece tokenizer that I am planning on using, but I cannot wrap my head around it.</p>
<p>I tried to read the original paper, which contains so little details that it feels like it's been written explicitely not to be understood. I also read several blog posts about it but none really clarified it (one straight up admitted not undertanding it completely).</p>
<p>Can somebody explain it to me? I am familiar with the EM algorithm but I cannot see how it related to the loss function in order to find the subwords probabilities...</p>
","transformer"
"88823","Transformers understanding","2021-02-02 12:45:33","88884","-2","163","<keras><nlp><transformer>","<p>i have i a big trouble. I don't understand transformers. I understand embedding, rnn's, GAN's, even Attention. But i don't understand transformers. Approximately 2 months ago i decided to avoid usage of transformers, because i found them hard. But i can't anymore avoid transformers. Please, help me. I want to use and understand work of transformers. How can i start to work with them?Past the fact that i want to understand their idea in general, i also want to can write/implement them using keras/tensorflow
Of course i tied to read some tutorials.  But i don't understand them anyway.</p>
","transformer"
"88552","Layer normalization details in GPT-2","2021-01-27 14:24:12","","6","2518","<deep-learning><normalization><transformer>","<p>I've read that GPT-2 and other transformers use layer normalization before the self-attention and feedforward blocks, but I am still unsure exactly how the normalization works.</p>
<p><a href=""https://i.sstatic.net/oDmqd.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/oDmqd.png"" alt=""Transformer diagram from http://jalammar.github.io/illustrated-transformer/"" /></a></p>
<p>Let's say that our context size is 1024 tokens, the embedding size is 768 (so that each token and its subsequent hidden states are represented by vectors of size 768), and we are using 12 multi-attention heads. So in the diagram above, there are 1024 r's and each r has dimensionality 768.</p>
<p>For a given layer in the transformer, how many normalization statistics (sample mean and stdev) are computed? Do we do one normalization per token, for 12x1024 normalizations so that the feature values <em>within</em> each token have mean 0 and std 1? Or do we normalize the values for each feature <em>across</em> tokens, for 12x768 normalizations? Or do we normalize all the feature values for all the tokens together, for 12 normalizations? Do we compute separate normalizations for each context in the minibatch?</p>
<p>I'm also keen to understand intuitively why this normalization is desirable. Assuming that the scheme is to normalize the feature values <em>within</em> each token: let's say one of our tokens is a bland word like &quot;ok&quot; whereas another token is the word &quot;hatred&quot;. I would expect that the representation of &quot;hatred&quot; would be spikier, with higher variance among the different feature values. Why is it useful to throw away this information and force the representation for &quot;ok&quot; to be just as spiky? On the other hand, if the normalization scheme is to normalize across feature values, so that if you take feature 1 from all of the tokens in our context, they will have zero mean and stdev 1, doesn't this throw away information when all of the words in our context are very negative, for example in the context &quot;war violence hate fear&quot;?</p>
<p>Separately, with layer normalization it seems like it is optional to re-scale the normalized values through learned bias and gain parameters. Does GPT-2 do this, or does it keep the values normalized to mean 0 and std 1?</p>
","transformer"
"88504","Transformer architecture question","2021-01-26 15:01:45","88521","0","66","<neural-network><deep-learning><nlp><transformer><attention-mechanism>","<p>I am hand-coding a transformer (<a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1706.03762.pdf</a>) based primarily on the instructions I found at this blog: <a href=""http://jalammar.github.io/illustrated-transformer/"" rel=""nofollow noreferrer"">http://jalammar.github.io/illustrated-transformer/</a>.</p>
<p>The first attention block takes matrix input of the shape [words, input dimension] and multiplies by the attention weight matrices of shape [input dimension, model dimension]. The model dimension is chosen to be less than the input dimension and is the dimension used as output in all subsequent steps.</p>
<p>There is a residual connection around the attention block and the input is meant to be added to the output of the attention block. However the output of the attention block is shape [words, model dimension] and the input is form [words, input dimension]. Should I interpolate the input down to the model dimension as is done in ResNet? Or maybe add another weight matrix to transform the input?</p>
<p><a href=""https://i.sstatic.net/dI4Al.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dI4Al.png"" alt=""enter image description here"" /></a></p>
","transformer"
"88500","Train a final model with the full data","2021-01-26 13:38:31","","1","69","<pytorch><transformer>","<p>I have trained a few NLP models, measured their performances and now I want to create a final model for production trained with all the data I have available.</p>
<p>I'm working in text classification and I'm using <a href=""https://huggingface.co/"" rel=""nofollow noreferrer"">transformers</a> with the library <a href=""https://pytorch-lightning.readthedocs.io/en/latest/"" rel=""nofollow noreferrer"">PyTorch Lightning</a>. The issue here is that I can't find an example of how to turn off the validation step (with pytorch lightning) and after several google searches I was not able to find discussions about that.</p>
<p>So, now I'm having second thoughts and questioning if I'm thinking correctly. Is it common practice to train the production model with the full dataset without the validation? Or should I just use the best model I have already trained? If so, why?</p>
","transformer"
"88393","How to evaluate the quality of speech-to-text data without access to the true labels?","2021-01-24 01:12:46","","2","261","<nlp><text-mining><transformer><speech-to-text>","<p>I am dealing with a data set of transcribed call center data, where customers are being recorded when interacting with the agent. This is then automatically transcribed by an external transcription system. I want to automatically assess the quality of these transcriptions.</p>
<p>Sadly, the quality seems to be disastrous. In some cases it's little more than gibberish, often due to different dialects the machine is not able to handle. We have no access to the original recordings (data privacy), so there is no way whatsoever to get or create the true labels. The system cannot be replaced as we are committed to it.</p>
<p>Again to the question: <strong>is there any way to automatically assess the quality of transcriptions with NLP methods?</strong> We want to quantify and compare transcription quality to filter out the best samples for semantic inference of our customers' input in a downstream task. I am thinking about something like a coherence measure in order to find the sentences which make the most sense, grammatically or semantically. Sadly, things as BLEU, WER or Rouge do not work in this case.</p>
<p>I'd be grateful for anything pointing in the right direction. Most importantly again, we have no labels and it needs to be scalable.</p>
<p>Thanks a lot!</p>
","transformer"
"88330","How do the linear layers in the attention mechanism work?","2021-01-22 11:45:58","88332","1","3058","<machine-learning><nlp><transformer><attention-mechanism>","<p>I think I now the answer to my question but I dont really get confirmation.
When taking a look at the multi-head-attention block as presented in &quot;Attention Is All You Need&quot; we can see that there are three linear layers applied on the key, query and value matrix. And then one layer at the end, which is applied on the output of the matrix multiplication of the score matrix an the value.</p>
<p>The three linear layers at the beginnging: When the key/query/value with shape (seq-len x emb-dim) enter the linear layer the output is still (seq-len x emb-dim). Does that mean, the same linear layer is applied on every &quot;index&quot; of the input matrix. Like this (pseudo-code):</p>
<pre class=""lang-py prettyprint-override""><code>fc = linear(emb-dim, emb-dim) # in-features and out-features have the shape of emb-dim
output_matrix = []

for x in key/query/value:
    # x is one row the input matrix with shape (emb-dim x 1)
    x = fc(x)
    # x after the linear layer has still the shape of (emb-dim x 1)
    output_matrix.append(x)

# output_matrix has now the shape (seq-len x emb-dim); the same as the input-matrix
</code></pre>
<p>So is this indeed what happens? I couldn't explain why the output is the same as the input otherwise.</p>
<p>The linear layer before the output: So the output of the matrix multiplication of the score matrix an the value is also (seq-len x emb-dim) and therefore the output of the linear layer is too. So the output of the whole attention block has the same shape as the input.</p>
<p>So Im just asking for comfirmation if the explaination I wrote is correct. And if not: What am I understanding wrong?</p>
<p>Extra question: When I want to further use the output of the attention block further for classification, I would have to take the mean along the seq axis in order to get a vector of fixed shape (emb-dim x 1) so I can feed it into a classification layer. But I guess that valueable information is getting lost in that process.
My question: Could I replace the last linear layer with an RNN to get the desired output shape and without losing information?</p>
","transformer"
"88097","why do transformers mask at every layer instead of just at the input layer?","2021-01-17 20:04:01","88128","1","1009","<neural-network><nlp><transformer>","<p>working thru the <a href=""http://nlp.seas.harvard.edu/2018/04/03/attention"" rel=""nofollow noreferrer"">annotated transformer</a>, I see that every layer in both the encoder (mask paddings) and decoder (mask padding + future positions) get masked. Why couldn't it be simplified to just one mask at the first layers of encoder and decoder?</p>
","transformer"
"88072","how is the linear relation between positional encoding helping attention?","2021-01-16 21:00:52","","1","101","<nlp><transformer><attention-mechanism>","<p>I'm reading <a href=""http://nlp.seas.harvard.edu/2018/04/03/attention"" rel=""nofollow noreferrer"">the annotated transformer</a>, and interested in the mechanics behind the <a href=""https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model"">positional encoding</a>. I understand the <a href=""https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/"" rel=""nofollow noreferrer"">linear relation</a> between position <span class=""math-container"">$t$</span> and position <span class=""math-container"">$t+\phi$</span>, and understand that it is a function only of <span class=""math-container"">$\phi$</span>, and not of <span class=""math-container"">$t$</span>.</p>
<p>However, I'm still not clear on how exactly is this helping the model to attend to phrases such as &quot;they are&quot;, no matter if they appear in position 3 or position 7 in the sentence. The attention layer gets as an input [<span class=""math-container"">$E(they)+PE(3)$</span>, <span class=""math-container"">$E(are)+PE(4)$</span>] where <span class=""math-container"">$PE(4)=T_1PE(3)$</span> and <span class=""math-container"">$T_1$</span> is a nice diagonal linear transformation that depends only on the position diff which is 1. How is it using those nice properties for learning and generalizing for the case where the same phrase &quot;they are&quot; appears in position 7? how is the fact that the linear transformation is nice and diagonal helping the model?</p>
","transformer"
"87906","Transformer model: Why are word embeddings scaled before adding positional encodings?","2021-01-13 10:10:24","87909","12","7554","<tensorflow><nlp><transformer><attention-mechanism>","<p>While going over a <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""noreferrer"">Tensorflow tutorial for the Transformer model</a> I realized that their implementation of the Encoder layer (and the Decoder) scales word embeddings by sqrt of embedding dimension <strong>before</strong> adding positional encodings. Notice that this is different from scaling the dot product attention.</p>
<p>I'm referring to the 3rd line of the call method of the Encoder class here: <a href=""https://www.tensorflow.org/tutorials/text/transformer#encoder"" rel=""noreferrer"">https://www.tensorflow.org/tutorials/text/transformer#encoder</a></p>
<pre class=""lang-py prettyprint-override""><code>def call(self, x, training, mask):

  seq_len = tf.shape(x)[1]
  
  # adding embedding and position encoding.
  x = self.embedding(x) # (batch_size, input_seq_len, d_model)
  x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
  x += self.pos_encoding[:, :seq_len, :]
  
  x = self.dropout(x, training=training)

  for i in range(self.num_layers):
    x = self.enc_layers[i](x, training, mask)

  return x # (batch_size, input_seq_len, d_model)
</code></pre>
<p>I could not find any mention of this scaling in the papers I've read so far. People always show the input to the encoder as WE + PE, that is word embedding plus positional encoding. But this implementation seems to use sqrt(d_model) * WE + PE.</p>
<p>My questions:</p>
<ol>
<li>Have you ever seen this extra scaling step mentioned in a paper? I didn't find it in &quot;Attention is all you need&quot; (Vaswani et. al.).</li>
<li>What is this additional scaling trying to achieve?</li>
</ol>
","transformer"
"87898","How to i get word embeddings for out of vocabulary words using a transformer model?","2021-01-13 07:02:51","","3","1313","<nlp><transformer><stanford-nlp><tokenization><huggingface>","<p>When i tried to get word embeddings of a sentence using bio_clinical bert, for a sentence of 8 words i am getting 11 token ids(+start and end) because &quot;embeddings&quot; is an out of vocabulary word/token, that is being split into em,bed,ding,s.</p>
<p>I would like to know if there is any aggregation strategies available that make sense apart from doing a mean of these vectors.</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
# download and load model
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

sentences = ['This framework generates embeddings for each input sentence']


#Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')


#Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)

print(encoded_input['input_ids'].shape)
</code></pre>
<p>Output :
torch.Size([1, 13])</p>
<pre><code>for token in encoded_input['input_ids'][0]:
  print(tokenizer.decode([token]))
</code></pre>
<p>Output:</p>
<pre><code>[CLS]
this
framework
generates
em
##bed
##ding
##s
for
each
input
sentence
[SEP]
</code></pre>
","transformer"
"87717","Using numpy.ndarray in machine learning sklearn.preprocessing model","2021-01-09 17:23:18","","1","113","<machine-learning><image-classification><numpy><dataframe><transformer>","<p>I'm having a problem storing and using an array on one model that I'm building in sklearn, Here is what I'm doing:</p>
<ol>
<li>I'm converting an image to numpy and storing as numpy.ndarray in my dataframe (there is an example bellow)</li>
</ol>
<pre><code>     [6.19926266e-02 4.74323332e-02 1.22703509e-02 1.02023669e-02
         1.45243444e-02 2.16958560e-02 3.42147425e-02 9.73318636e-01
         7.03395926e-05 1.93824657e-04 5.42396388e-04 9.58181568e-04
         1.64594641e-03 2.38841982e-03 3.74831865e-03 8.31570302e-04
         7.34657951e-05 2.03203264e-04 6.39308710e-04 1.13481213e-03
         9.09725379e-04 1.56779133e-03 1.78818870e-03 3.70455178e-04
         6.25240791e-05 1.71941225e-04 3.67328990e-04 4.62678203e-04
         4.90814040e-04 4.39231662e-04 3.72018287e-04 8.12813087e-05
         1.40679185e-04 8.59706124e-05 1.65688820e-04 2.53222534e-04
         3.04804911e-04 3.09494208e-04 5.93978766e-05 1.56310205e-06
         ....]
</code></pre>
<ol start=""2"">
<li>I stored my dataframe as pickle because when I was storing as csv, the column was converting into string. My columns stored in the dataframe:</li>
</ol>
<pre><code>0       [0.0026778684, 0.003117677, 0.00040434036, 0.0...
1       [0.061992627, 0.047432333, 0.012270351, 0.0102...
2       [0.0, 0.0, 0.0, 4.3830705e-06, 1.3149212e-05, ...
3       [0.30314153, 0.04477268, 0.01840577, 0.0319251...
4       [0.2563626, 0.03259786, 0.018686974, 0.0198365...
                              ...                        
1287    [0.11471527, 0.032394826, 0.012400794, 0.01131...
1288    [0.002138354, 0.001044489, 0.0007786191, 0.001...
1289    [0.056204572, 0.026556363, 0.02082041, 0.01966...
1290    [0.051759016, 0.0058623934, 0.0054726205, 0.00...
1291    [0.0, 5.4140626e-05, 4.4114586e-05, 4.8125003e...
Name: F3, Length: 1292, dtype: object
</code></pre>
<ol start=""3"">
<li>I can use this column if I create a list and use np.hstack in each row (see the code bellow) and then I can apply the MinMaxScaler on the list I create and use in my machine learning model and this works:</li>
</ol>
<pre><code>mli = []    
    for i in range(0,len(data)):

       mli.append(np.hstack([data['F3'][i],data['F5'][i],data['F6'][i],data['F7'][i]]))

    X = scaler.fit_transform(mli)
</code></pre>
<p>PS: 'F5', 'F6', 'F7' have the same structure as F3</p>
<p>The problem:</p>
<p>I need to apply this column to a machine learning model, which I'm using make_column_transformer because I have text, number and array data to apply transformations on it, so I need to use the column as an input data, and when I try to fit_transform (MinMaxScaler) I have the error:</p>
<blockquote>
<p>TypeError Traceback (most recent call last)
TypeError: only size-1 arrays can be converted to Python scalars
ValueError: setting an array element with a sequence.</p>
</blockquote>
<p>I need to use at least one column numpy.ndarray as input to the MinMaxScaler, has anyone tried to use an array like this, or convert into something that would work?</p>
<p>One example of exactly what I want to do:</p>
<pre><code>features = make_column_transformer(
                            (transf,'timagem'),(transf,'legenda'),
                            (scaler, 'F3'),remainder ='drop')
</code></pre>
<p>transf =  Tdidf Transformer
timagem, legenda = text input
scaler = MinMax
F3 =  Exemple of code (1) above</p>
","transformer"
"87637","What is the difference between GPT blocks and BERT blocks","2021-01-07 15:43:05","87679","3","2977","<bert><transformer><gpt>","<p>Nowadays many applications only use the Encoder and Decoder part of the Transformer respectively. I am having trouble understanding the difference though.</p>
<p>If GPT uses Decoder only and BERT uses Encoder only does this mean that the only difference between the two is basically in the masking part?</p>
<p>The cross attention layer in the Decoder is omitted since there is no Encoder within GPT right?</p>
","transformer"
"87634","List of Google T5 possible operations","2021-01-07 15:21:18","","0","153","<nlp><transfer-learning><transformer><huggingface>","<p>I am trying to use the huggingface.co pre-trained model of Google T5 (<a href=""https://huggingface.co/t5-base"" rel=""nofollow noreferrer"">https://huggingface.co/t5-base</a>) for a variety of tasks. But I can`t find a list of many tasks it really supports and how to address them. I found <code>summarize: </code> + the text to summarize. I also try to find an overview in the paper (<a href=""https://arxiv.org/abs/1910.10683"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1910.10683</a>) there are for instance examples of question-answering in the appendix but without instructions how to tell T5 to answer a specific question. The huggingface.co documentation did not provide any further information besides the <code>summarize: </code>declaration.</p>
","transformer"
"87606","What is the number of neurons for the input layer of the BERT?","2021-01-06 20:42:18","87612","1","730","<bert><transformer>","<p>I think it is the vocab size. However I am not sure and I appreciate your help.</p>
","transformer"
"87518","What does attention weights output from Transformer network do?","2021-01-05 06:35:18","","1","22","<neural-network><transformer>","<p>I'm trying to understand transformer networks. I want to know that are the attention weights, which are the outputs from forward/predict method where we get final output and attention weights as return, important after we done predicting?</p>
","transformer"
"87389","Inference order in BERT masking task","2020-12-31 20:33:17","87390","1","150","<neural-network><nlp><bert><transformer><language-model>","<p>In BERT, multiple words in a single sentence can be masked at once. Does the model infer all of those words at once or iterate over them in either left to right or some other order?</p>
<p>For example:</p>
<blockquote>
<p>The dog walked in the park.</p>
</blockquote>
<p>Can be masked as</p>
<blockquote>
<p>The [mask] walked in [mask] park.</p>
</blockquote>
<p>In what order (if any) are these tokens predicted?</p>
<p>If you have any further reading on the topic, I'd appreciate it as well.</p>
","transformer"
"87382","How do I handle class imbalance for text data when using pretrained models like BERT?","2020-12-31 14:09:09","","1","561","<class-imbalance><multiclass-classification><bert><transformer><smote>","<p>I have a skewed dataset consisting of samples of the form:</p>
<pre><code>Category 1 10000
Category 2  2000
Category 3   400
Category 4   300 
Category 5   100
</code></pre>
<p>The dataset consists of text with data labeled into one of the five categories. I am trying to use the pretrained models like BERT for the classification task but the model fails to identify the
categories 3-5 .I have tried to apply class weights in the loss criterion however it doesn't help much although it gives better performance as compared to simple fine tuning of the pretrained models. I have came to know about SMOTE and other methods in order to handle the class imbalance issues . But since most of the transformer models expect the inputs as text which are later tokenized by their respective tokenizers I am not able to do any kind of oversampling . If there is a workaround for this thing I would be interested to know about it.</p>
","transformer"
"87188","Can Transformer Models be used for Training Chatbots?","2020-12-27 03:29:55","","0","149","<machine-learning><deep-learning><nlp><transformer><chatbot>","<p>Can Transformer Models be used for Training Chatbots?</p>
<p><strong>Note  - I am talking about the transformer model google released on the paper 'Attention is all you need'</strong></p>
","transformer"
"86869","Many questions training unbalanced and duplicated data","2020-12-18 15:10:58","","1","25","<classification><training><class-imbalance><transformer>","<p>I'm a DS student. I have like 30.000 of bank statements, all labeled with a specific category(cat1, cat2, ...). With that data I'm trying to train a classification model but I found several problems:</p>
<ul>
<li>The category that has more statements has 10k, the one with less 100 (unbalanced dataset)</li>
<li>Many rows are repeated with the same text ( for example &quot;Buy for a total of X$ in the supermarket Y&quot;, but the repeated rows appears in each label)</li>
</ul>
<ul>
<li>Should I train with repeated statements? if I do so the model will be trained for some specific cases and the predicted results will not be real because for example if I have repeated statements with the same label:</li>
</ul>
<p>A, B, B, B, B, B (labels)</p>
<p>and I take only one row with label A and B, since the statement will be repeated the accuracy predicting the others B's will be 100%.</p>
<ul>
<li>If I do not train with repeated statements am I not deleting important information about my data and can affect the prediction later?</li>
<li>Should I undersampling or oversampling? It's correct for this kind of data? I will have more repeated rows in the case of oversampling, and undersampling I'll lose a lot of information.</li>
</ul>
<p>I do not know if what I'm thinking has some sense. I really need some tips in order to train my data.</p>
","transformer"
"86572","BERT uses WordPiece, RoBERTa uses BPE","2020-12-11 19:10:22","86573","0","2335","<bert><transfer-learning><transformer><language-model><tokenization>","<p>In the original <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer""><strong>BERT</strong></a> paper, section <em>'A.2 Pre-training Procedure'</em>, it is mentioned:</p>
<blockquote>
<p>The LM masking is applied after <strong>WordPiece</strong> tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces.</p>
</blockquote>
<p>And in the <a href=""https://arxiv.org/pdf/1907.11692.pdf"" rel=""nofollow noreferrer""><strong>RoBERTa</strong></a> paper, section <em>'4.4 Text Encoding'</em> it is mentioned:</p>
<blockquote>
<p>The original BERT implementation (Devlin et al., 2019) uses a
character-level <strong>BPE</strong> vocabulary of size 30K, which is learned after
preprocessing the input with heuristic tokenization rules.</p>
</blockquote>
<p>I appreciate if someone can clarify why in the <a href=""https://arxiv.org/pdf/1907.11692.pdf"" rel=""nofollow noreferrer"">RoBERTa</a> paper it is said that <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">BERT</a> uses BPE?</p>
","transformer"
"86566","What's the right input for gpt-2 in NLP","2020-12-11 17:29:15","","4","9665","<nlp><data-science-model><transformer><gpt>","<p>I'm fine-tuning pre-trained gpt-2 for text summarization. The dataset contains 'text' and 'reference summary'. So my question is how to add special tokens to get the right input format. Currently I'm thinking doing like this:</p>
<p>example1  &lt;BOS&gt; text  &lt;SEP&gt; reference summary &lt;EOS&gt; ,<br />
example2 &lt;BOS&gt; text &lt;SEP&gt; reference summary &lt;EOS&gt; ,
.....</p>
<p>Is this correct? If so, a follow-up question would be whether the max-token-length(i.e. 1024 for gpt-2) means also the concatenate length of text and reference summary?</p>
<p>Any comment would be very much appreciated!</p>
","transformer"
"86548","Trained BERT models perform unpredictably on test set","2020-12-11 10:23:51","86557","4","418","<nlp><bert><transformer>","<p>We are training a BERT model (using the Huggingface library) for a sequence labeling task with six labels: five labels indicate that a token belongs to a class that is interesting to us, and one label indicates that the token does not belong to any class.</p>
<p>Generally speaking, this works well: loss decreases with each epoch, and we get good enough results. However, if we compute precision, recall and f-score after each epoch on a test set, we see that they oscillate quite a bit. We train for 1,000 epochs. After 100 epochs performance seems to have plateaued. During the last 900 epochs, precision jumps constantly to seemingly random values between 0.677 and 0.709; recall between 0.729 and 0.798. The model does not seem to stabilize.
To mitigate the problem, we already tried the following:</p>
<ul>
<li>We increase the size of our test data set.</li>
<li>We experimented with different learning rates and batch sizes.</li>
<li>We used different transformer models from the Huggingface library, e.g. RoBERTa, GPT-2 etc.
Nothing of this has helped.</li>
</ul>
<p>Does anyone have any recommendations on what we could do here? How can we pick the “best model”? Currently, we pick the one that performs best on the test set, but we are unsure about this approach.</p>
","transformer"
"86410","Understanding the XLNet model for a concrete case","2020-12-08 14:03:11","86567","1","33","<machine-learning><neural-network><classification><transformer><text-classification>","<p>I'm a data science student, recently I reviewed the XLNet paper and I have a doubt about it:</p>
<p>Imagine we are using many categories, let's say 200, can this model has problems reaching a good accuracy (only applying what the paper says), if not, why? I thought that if we combine a hierarchical classification and then apply the XLNet model it can lead to better results for many categories but I'm not sure because many examples in Kaggle and other websites use XLNet directly. Has some sense what am I saying or I didn't properly understand what XLNet does since I didn't see anyone applying this proposal for many categories?</p>
","transformer"
"86304","In transformers, do you understand why are the Value (V) vectors comes from the encoder? And than normalize with the query (Q) vector?","2020-12-05 17:20:44","","0","431","<nlp><transformer><spatial-transformer>","<p>In transformers, there is a phase for rasidual connection, where the queries and the output from the attention are add and normalize.
Can one please give some advise to the motivation of it? Or maybe I get it wrong?
It seems to me that the values shouldn't come from the encoder, the values are the vector that we want to have attention on.
And if so. We should have add and normalize the values from the previous state and not the queries... I'm confused..</p>
","transformer"
"86288","Predict customer behaviour with Transformer(attention is all you need)","2020-12-05 11:24:21","","0","441","<transformer><sequence-to-sequence><attention-mechanism>","<p>Please advice, am I thinking correctly: is it possible to represent customer behavior data from an online store as a sequence data? Because it is describing interactions of the customer with the shop through time.</p>
<p>So in this case N would be the number of users (or number of user sessions), T / time window I could set myself and D I could set also myself taking only event type (purchase, view, etc.) or something else like price, brand etc.(please see screenshot below)
<a href=""https://i.sstatic.net/WJLje.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/WJLje.png"" alt=""enter image description here"" /></a></p>
<p>Please share your opinion
Many thanks in advance</p>
","transformer"
"86252","Effect of Stop-Word Removal on Transformers for Text Classification","2020-12-03 20:24:23","87548","9","5166","<nlp><preprocessing><transfer-learning><transformer><text-classification>","<p>The domain here is essentially topic classification, so not necessarily a problem where stop-words have an impact on the analysis (as opposed to, say, sentiment analysis where structure can affect meaning).</p>
<p>With respect to the positional encoding mechanism in transformer language models, <strong>when using a pretrained LM</strong> is stop-word removal as a preprocessing step actively harmful if the LM was trained on a corpus where they were left in? I'm still working on fully understanding the mechanism but I feel like removing stop-words would affect which wavelength is used to construct the context between any given pair of words with stop words between them, which in turn would impact the encoding.</p>
<p>Or, would this not matter because the regression when trained figures it out from consistently processed input? I feel like it should matter but haven't been able to find anything on the topic.</p>
","transformer"
"86104","What is the difference between BERT architecture and vanilla Transformer architecture","2020-11-30 03:34:44","86108","3","2386","<nlp><bert><transformer><encoder>","<p>I'm doing some research for the summarization task and found out BERT is derived from the Transformer model. In every blog about BERT that I have read, they focus on explaining what is a bidirectional encoder, So, I think this is what made BERT different from the vanilla Transformer model. But as far as I know, the Transformer reads the entire sequence of words at once, therefore it is considered bidirectional too. Can someone point out what I'm missing?</p>
","transformer"
"85972","BERT data cleaning","2020-11-26 10:35:07","","0","207","<nlp><preprocessing><bert><transformer>","<p>I am wondering which data cleaning steps should be performed if you want to re-fine a BERT model on custom text data.</p>
<p>Which steps should be performed?</p>
<p>Does it make sense to perform a stemming or lemmatization if it has not been applied to the initial training of the BERT Base/Large model?</p>
","transformer"
"85855","Why transform embedding dimension in sin-cos positional encoding?","2020-11-24 00:12:56","85858","3","4103","<transformer><encoder>","<p>Positional encoding using sine-cosine functions is often used in transformer models.</p>
<p>Assume that <span class=""math-container"">$X \in R^{l\times d}$</span> is the embedding of an example, where <span class=""math-container"">$l$</span> is the sequence length and <span class=""math-container"">$d$</span> is the embedding size. This positional encoding layer encodes <span class=""math-container"">$X$</span>’s position <span class=""math-container"">$P \in R^{l\times d}$</span> and outputs <span class=""math-container"">$P + X$</span></p>
<p>The position <span class=""math-container"">$P$</span> is a 2-D matrix, where <span class=""math-container"">$i$</span> refers to the order in the sentence, and <span class=""math-container"">$j$</span> refers to the position along the embedding vector dimension. In this way, each value in the origin sequence is then maintained using the equations below:</p>
<p><span class=""math-container"">$${P_{i, 2j} = \sin \bigg( \frac{i}{10000^{2j/d}}} \bigg) $$</span></p>
<p><span class=""math-container"">$${P_{i, 2j+1} = \cos \bigg( \frac{i}{10000^{2j/d}}} \bigg)$$</span></p>
<p>for <span class=""math-container"">$i = 0,..., l-1$</span> and <span class=""math-container"">$j=0,...[(d-1)/2]$</span></p>
<p>I understand the transormation across the time dimension <span class=""math-container"">$i$</span> but why do we need the transformation across the embedding size dimension <span class=""math-container"">$j$</span>? Since we are adding the position, wouldn't sin-cos just on time dimension be sufficient to encode the position?</p>
<p><em>EDIT</em></p>
<p>Answer 1 -  <em>Making the embedding vector independent from the &quot;embedding size dimension&quot; would lead to having the same value in all positions, and this would reduce the effective embedding dimensionality to 1.</em></p>
<p>I still don't understand how the embedding dimensionality will be reduced to 1 if the same positional vector is added. Say we have an input <span class=""math-container"">$X$</span> of zeros with 4 dimensions - <span class=""math-container"">$d_0, d_1, d_2, d_3$</span> and 3 time steps - <span class=""math-container"">$t_0, t_1, t_2$</span></p>
<p><span class=""math-container"">$$
\begin{matrix}
&amp; d_0 &amp; d_1 &amp; d_2 &amp; d_3\\
t_0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
t_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
t_2 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
\end{matrix}
$$</span></p>
<p>If <span class=""math-container"">$d_0$</span> and <span class=""math-container"">$d_2$</span> are the same vectors <span class=""math-container"">$[0, 0, 0]$</span>, and the meaning of position i.e time step is the same, why do they need to have different positional vectors? Why can't <span class=""math-container"">$d_0$</span> and <span class=""math-container"">$d_2$</span> be the same after positional encoding if the input <span class=""math-container"">$d_0$</span> and <span class=""math-container"">$d_2$</span> are the same?</p>
<p>As for the embedding dimensionality reducing to 1, I don't see why that would happen. Isn't the embedding dimensionality dependent on the input matrix <span class=""math-container"">$X$</span>. If I add constants to it, the dimensionality will not change, no?</p>
<p>I may be missing something more fundamental here and would like to know where am I going wrong.</p>
","transformer"
"85847","Role of decoder in Transformer?","2020-11-23 20:29:43","85849","2","2176","<transformer><attention-mechanism><encoder>","<p>I understand the mechanics of Encoder-Decoder architecture used in the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a> paper. My question is more high level about the role of the decoder. Say we have a sentence translation task: <em>Je suis ètudiant</em> -&gt; <em>I am a student</em></p>
<p>The encoder receives <em>Je suis ètudiant</em> as the input and generates encoder output which ideally should embed the context/meaning of the sentence.</p>
<p>The decoder receives this encoder output and an input query (<em>I</em>, <em>am</em>, <em>a</em>, <em>student</em>) as its inputs and outputs the next word (<em>am</em>, <em>a</em>, <em>student</em>, <em>EOS</em>). This is done step by step for every word.</p>
<p>Now, do I understand this correctly that the decoder is doing two things?</p>
<ol>
<li>Figuring out relationship between the input query and encoder embedding i.e how is the query related to the input sentence <em>Je suis ètudiant</em></li>
<li>Figuring out how is the current query related to previous queries through the masked attention mechanism. So when the query is <em>student</em>, the decoder would attend to relevant words which have already occurred (<em>I am a</em>).</li>
</ol>
<p>If this is not the right way to think about it, can someone give a better explanation?</p>
<p>Also, if I have a task of classification or regression for a time series, do I need the decoder? I would think just the encoder would suffice as there is no context in the output of the model.</p>
","transformer"
"85790","Why this TensorFlow Transformer model has Linear output instead of Softmax?","2020-11-22 15:08:08","85793","0","470","<deep-learning><tensorflow><nlp><transformer><attention-mechanism>","<p>I am checking this official TensorFlow tutorial on a <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">Transformer model for Portuguese-English translation</a>.</p>
<p>I am quite surprised that <a href=""https://www.tensorflow.org/tutorials/text/transformer#create_the_transformer"" rel=""nofollow noreferrer"">when the Transformer is created</a>, their final output is a Dense layer with <strong>linear activation</strong>, instead of <strong>Softmax</strong>. Why is that the case? In the original paper <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention is All You Need</a> the image is pretty clear, there is a Softmax layer just at the end (Fig.1, p. 3).</p>
<p>How can you justify this difference, when your task involves building a language model and your Loss is based on sparse categorical crossentropy?</p>
","transformer"
"85486","What is the difference between GPT blocks and Transformer Decoder blocks?","2020-11-16 09:54:05","85489","8","8768","<deep-learning><transformer><language-model>","<p>I know GPT is a Transformer-based Neural Network, composed of several blocks. These blocks are based on the original Transformer's Decoder blocks, but are they exactly the same?</p>
<p>In the original Transformer model, Decoder blocks have two attention mechanisms: the first is pure Multi Head Self-Attention, the second is Self-Attention with respect to Encoder's output. In GPT there is no Encoder, therefore I assume its blocks only have one attention mechanism. That's the main difference I found.</p>
<p>At the same time, since GPT is used to generate language, its blocks must be masked, so that Self-Attention can only attend previous tokens. (Just like in Transformer Decoders.)</p>
<p>Is that it? Is there anything else to add to the difference between GPT (1,2,3,...) and the original Transformer?</p>
","transformer"
"85385","How to use paraphrase_mining using sentence transformers pre-trained model","2020-11-13 20:56:43","","2","324","<word-embeddings><python-3.x><transformer>","<p>I am trying to find similarity between sentences using a pre-trained sentence-transformers model. I am trying to follow the code here - <a href=""https://www.sbert.net/docs/usage/paraphrase_mining.html"" rel=""nofollow noreferrer"">https://www.sbert.net/docs/usage/paraphrase_mining.html</a></p>
<p>In trial one I run 2 for-loops where in I try to find similarity of given sentence with every other sentence. Here is the code for that -</p>
<pre><code>from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')


# Single list of sentences
sentences = ['The cat sits outside',
             'A man is playing guitar',
             'The new movie is awesome',
             'Do you like pizza?']

#Compute embeddings
embeddings = model.encode(sentences, convert_to_tensor=True)

#Compute cosine-similarities for each sentence with each other sentence
cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)

#Find the pairs with the highest cosine similarity scores
pairs = []
for i in range(len(cosine_scores)-1):
    for j in range(i+1, len(cosine_scores)):
        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})

#Sort scores in decreasing order
pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)

print(len(pairs))
6

for pair in pairs[0:10]:
    i, j = pair['index']
    print(&quot;{} \t\t {} \t\t Score: {:.4f}&quot;.format(sentences[i], sentences[j], pair['score']))

A man is playing guitar          Do you like pizza?          Score: 0.1080
The new movie is awesome         Do you like pizza?          Score: 0.0829
A man is playing guitar          The new movie is awesome        Score: 0.0652
The cat sits outside         Do you like pizza?          Score: 0.0523
The cat sits outside         The new movie is awesome        Score: -0.0270
The cat sits outside         A man is playing guitar         Score: -0.0530
</code></pre>
<p>This works as expected because there can be 6 combinations of the similarity scores between the combinations of 4 sentences. On their documentation page, they mention that this does not scale well because of quadratic complexity and hence they recommend using paraphrase_mining() method.</p>
<p>But when I try to use that method, I do not get 6 combinations but instead only get 5. Why is that the case?</p>
<p>Here is the sample code that I try using the paraphrase_mining() method -</p>
<pre><code># Single list of sentences
sentences = ['The cat sits outside',
             'A man is playing guitar',
             'The new movie is awesome',
             'Do you like pizza?']


paraphrases = util.paraphrase_mining(model, sentences)
print(len(paraphrases))
5

k = 0
for paraphrase in paraphrases:
    print(k)
    score, i, j = paraphrase
    print(&quot;{} \t\t {} \t\t Score: {:.4f}&quot;.format(sentences[i], sentences[j], score))
    print()
    k = k + 1

0
A man is playing guitar          Do you like pizza?          Score: 0.1080

1
The new movie is awesome         Do you like pizza?          Score: 0.0829

2
A man is playing guitar          The new movie is awesome        Score: 0.0652

3
The cat sits outside         Do you like pizza?          Score: 0.0523

4
The cat sits outside         The new movie is awesome        Score: -0.0270
</code></pre>
<p>Is there a difference between how the <code>paraphrase_mining()</code> works?</p>
","transformer"
"85301","Why does an attention layer in a transformer learn context?","2020-11-12 15:31:37","","1","688","<neural-network><nlp><transformer><sequence-to-sequence><attention-mechanism>","<p>I understand the transformer architecture (from &quot;Attention is All You Need&quot;), as well as how the attention is computed in the multi-headed attention layers.</p>
<p>What I'm confused on is <em>why</em> the output of an attention layer is a context vector. That is to say: what is it about the way that a transformer is trained causes the attention layers to learn context? What I would expect to see in the paper is a justification along the lines of &quot;when you train a transformer using attention on sequence-to-sequence tasks, the attention layers learn context <em>and here's why...</em>&quot;. I believe it because I've seen the heatmaps that show that attention between related words, but I want to understand why that is necessarily the result of training a transformer.</p>
<p>Why couldn't it be the case that the attention layers learn some other features that happen to also be beneficial in sequence to sequence tasks? How do we know that they learn context, other than that's what we observe?</p>
<p>Again, I get the math and I know there are several posts about it. What I want to know is <em>what about the math or the training process implies that the attention layers learn context</em>.</p>
","transformer"
"85150","Understanding Transformer's Self attention calculations","2020-11-09 13:00:18","","0","734","<transformer><attention-mechanism>","<p>I was going through this link: <a href=""https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&amp;utm_medium=demystifying-bert-groundbreaking-nlp-framework#comment-160771"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&amp;utm_medium=demystifying-bert-groundbreaking-nlp-framework#comment-160771</a></p>
<p><a href=""https://i.sstatic.net/tiWfw.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tiWfw.png"" alt=""enter image description here"" /></a></p>
<p>What is the value of <strong>Key, Value</strong> in the self attention calculation of Transformer model?</p>
<p><strong>Query vector</strong> is embedding vector for the word that is queried, is that right?</p>
<p>Is attention calculated in RNN is different from self attention in Transformer?</p>
","transformer"
"84930","Weights shared by different parts of a transformer model","2020-11-04 04:04:37","86363","4","4007","<machine-learning><neural-network><deep-learning><nlp><transformer>","<p><a href=""https://i.sstatic.net/R2IDD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/R2IDD.png"" alt=""enter image description here"" /></a></p>
<p>Which parts of a transformer share weights, like, do all the encoders share the same weight or all the decoders share the same weights?</p>
","transformer"
"84703","How to train a model on top of a transformer to output a sequence?","2020-10-30 11:37:22","84720","2","454","<pytorch><transformer><sequence><sequence-to-sequence>","<p>I am using huggingface to build a model that is capable of identifying mistakes in a given sentence.
Say I have a given sentence and a corresponding label as follows -&gt;</p>
<pre><code>correct_sentence = &quot;we used to play together.&quot;
correct_label = [1, 1, 1, 1, 1]

changed_sentence = &quot;we use play to together.&quot;
changed_label = [1, 2, 2, 2, 1]
</code></pre>
<p>These labels are further padded with 0s to an equal length of <code>512</code>. The sentences are also tokenized and are padded up(or down) to this length.
The model is as follows:</p>
<pre><code>class Camembert(torch.nn.Module):
    &quot;&quot;&quot;
    The definition of the custom model, last 15 layers of Camembert will be retrained
    and then a fcn to 512 (the size of every label).
    &quot;&quot;&quot;
    def __init__(self, cam_model):
        super(Camembert, self).__init__()
        self.l1 = cam_model
        total_layers = 199
        for i, param in enumerate(cam_model.parameters()):
            if total_layers - i &gt; hparams[&quot;retrain_layers&quot;]:
                param.requires_grad = False
            else:
                pass
        self.l2 = torch.nn.Dropout(hparams[&quot;dropout_rate&quot;])
        self.l3 = torch.nn.Linear(768, 512)

    def forward(self, ids, mask):
        _, output = self.l1(ids, attention_mask=mask)
        output = self.l2(output)
        output = self.l3(output)
        return output
</code></pre>
<p>Say, <code>batch_size=2</code>, the output layer will therefore be <code>(2, 512)</code> which is same as the target_label.
To the best of my knowledge, this method is like saying there are <code>512</code> classes that are to be classified which is not what I want, the problem arises when I try to calculate loss using <code>torch.nn.CrossEntropyLoss()</code> which gives me the following error (truncated):</p>
<pre><code> File &quot;D:\Anaconda\lib\site-packages\torch\nn\functional.py&quot;, line 1838, in nll_loss
    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), igno
re_index)
RuntimeError: multi-target not supported at C:/w/1/s/tmp_conda_3.7_100118/conda/conda-bld/p
ytorch_1579082551706/work/aten/src\THCUNN/generic/ClassNLLCriterion.cu:15
</code></pre>
<p>How am I supposed to solve this issue, are there any tutorials for similar kinds of models?</p>
","transformer"
"84692","Can I fine-tune the BERT on a dissimilar/unrelated task?","2020-10-30 07:20:30","84695","1","466","<bert><transformer><language-model><tokenization>","<p>In the original BERT paper, section 3 (arXiv:1810.04805) it is mentioned:</p>
<p>&quot;During pre-training, the model is trained on unlabeled data over <strong>different</strong> pre-training tasks.&quot;</p>
<p>I am not sure if I correctly understood the meaning of the word <strong>&quot;different&quot;</strong> here. different means a different <strong>dataset</strong> or a different <strong>prediction task</strong>?</p>
<p>For example if we pre-train the BERT on a &quot;sentence-classification-task&quot; with a big dataset. Then, should I fine-tune it again on the <strong>same</strong> &quot;sentence-classification-task&quot; task on a smaller and task-specific data-set or I can use the trained model for some other tasks such as &quot;sentence-tagging&quot;?</p>
","transformer"
"84673","SVM on BERT-Embeddings with very small dataset does not converge","2020-10-29 17:52:03","","1","685","<scikit-learn><svm><embeddings><text-classification><transformer>","<p>I am trying to reproduce the results from this <a href=""https://arxiv.org/pdf/2004.13138.pdf"" rel=""nofollow noreferrer"">paper</a> where they use a linear SVM on top of BERT-Embeddings for text-classification. They use the average of the token-embeddings which results in a 768 dimensional vector for each sample.</p>
<p>As this is active-learning, the number of samples used is very small (between 40 and 500).</p>
<p>So: <code>nb_samples &lt; nb_features</code>.</p>
<p>I thought that a SVM converges most of the times (see <a href=""https://stackoverflow.com/questions/40453793/does-svm-always-converge"">this question</a>), and surely in the cases where <code>nb_samples &lt; nb_features</code>. However, this is not the case, even with only 40 samples, the SVM (LinearSVC from sklearn) does not converge.</p>
<p>I am quite sure my embeddings are somehow meaningful, they look also ok (values between between -1 and 1) no need for normalization etc.</p>
<p><strong>What could be my problem?</strong></p>
<p>I tried <code>dual=True and dual=False, increasing max_iter to 10000</code> etc.</p>
","transformer"
"82451","Why is 10000 used as the denominator in Positional Encodings in the Transformer Model?","2020-10-01 21:27:52","","9","5830","<machine-learning><word-embeddings><transformer>","<p>I was working through the <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""noreferrer"">All you need is Attention</a> paper, and while the motivation of positional encodings makes sense and the other stackexchange answers filled me in on the motivations of the structure of it, I still don't understand why <span class=""math-container"">$1/10000$</span> was used as the scaling factor for the <span class=""math-container"">$pos$</span> of a word. Why was this number chosen?</p>
<p><a href=""https://i.sstatic.net/4fIP9.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/4fIP9.png"" alt=""enter image description here"" /></a></p>
","transformer"
"81727","What would be the target input for Transformer Decoder during test phase?","2020-09-15 10:23:15","82206","1","3831","<nlp><transformer><attention-mechanism>","<p>The Transformer Decoder takes in two inputs, the encoder's output, and the target sequence. How the target is fed into the decoder has been provided in this <a href=""https://datascience.stackexchange.com/questions/51785/what-is-the-first-input-to-the-decoder-in-a-transformer-model"" >answer </a></p>
<p>I am having confusion about what the target sequence will be when the trained model is evaluated?.</p>
<p>Is it that we start with a <code>&lt;SOS&gt;</code> tag for the first timestep and loop through the transformer decoder for each timestep like in RNN's?</p>
<p>It would be helpful if someone can clarify this for me.</p>
","transformer"
"81681","Bert for QuestionAnswering input exceeds 512","2020-09-14 12:59:36","81689","4","1063","<bert><transformer><question-answering><huggingface>","<p>I'm training Bert on question answering (in Spanish) and i have a large context, only the context exceeds 512, the total question + context is 10k, i found that longformer is bert like for long document, but there's no pretrained in spanish so, is there any idea get around bert.</p>
<p>What i tried is:</p>
<pre><code>from transformers import BertConfig
config=BertConfig.from_pretrained(BERT_MODEL_PATH)
config.max_length=4000 
config.max_position_embeddings=4000
config.output_hidden_states=True
model = MyBertModel(config)    

</code></pre>
<p>but still gives me an error mismatch</p>
<blockquote>
<p>RuntimeError: Error(s) in loading state_dict for BertModel:
size mismatch for bert.embeddings.position_embeddings.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([4000, 768]).</p>
</blockquote>
","transformer"
"81651","How to treat data transformation choices as hyperparemeters?","2020-09-14 04:25:32","","2","98","<scikit-learn><hyperparameter-tuning><transformer>","<p>While reading the book hands-on ML by Aurelien Geron, I came across this line-</p>
<blockquote>
<p>Treat your data transformation choices as hyperparameters, especially
when you are not sure about them (e.g., if you’re not sure whether to
replace missing values with zeros or with the median value, or to just
drop the rows).</p>
</blockquote>
<p>How exactly do I do that? Is there a way to do it via sklearn or do I have to manually keep several datasets (each with a different transformation) and then fit models onto all of them?</p>
","transformer"
"81508","Question about BERT embeddings with high cosine similarity","2020-09-10 15:13:03","85325","2","341","<nlp><bert><transformer><cosine-distance>","<p>Under what circumstances would BERT assign two occurrences of the same word similar embeddings? If those occurrences are contained within similar syntactic relations with their co-occurrents?</p>
","transformer"
"81248","Does finetuning BERT involving updating all of the parameters or just the final classification layer?","2020-09-04 20:54:25","","2","2013","<nlp><bert><transformer><finetuning><pretraining>","<p>Currently learning and reading about transformer models, I get that during the pretraining stage the BERT model is trained on a large corpus via MLM and NSP.  But during finetuning, for example trying to classify sentiment based on another text, are all of the BERT parameters (110M+ parameters + final classification layer) updated or just only final classification layers?  Couldn't find a concrete answer to this in the resources I've been looking at.</p>
<p>Thank you in advance.</p>
","transformer"
"80965","Loss first decreases and then increases","2020-08-29 09:30:52","","2","216","<nlp><bert><transformer>","<p>I am using pre-trained <code>xlnet-base-cased</code> model and training it further on real vs fake news detection dataset. I noticed a trend in accuracy for first epoch. Accuracy increases till some point (approximately half) of first epoch and then decreases. Loss also first decreases and then increases rapidly (it is not Nan). What can be reason of this trend ?<br />
I noticed same trend when I tried to run it with <code>roberta-base</code>. But any such trend was not noticed when I trained with <code>distilbert</code>, accuracy goes on increasing in this case.</p>
<p>Here is graph for accuracy VS steps:
<a href=""https://i.sstatic.net/bIY8E.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bIY8E.jpg"" alt=""enter image description here"" /></a></p>
<p>[EDIT]
File containing running output of the model:
<a href=""https://drive.google.com/file/d/1r5AWftyHTLf5sqtgWnQm_4lqB84UrJex/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1r5AWftyHTLf5sqtgWnQm_4lqB84UrJex/view?usp=sharing</a></p>
","transformer"
"80826","Transformer masking during training or inference?","2020-08-26 17:35:55","81492","6","3738","<nlp><training><generative-models><transformer><attention-mechanism>","<p>I'm working through <a href=""http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""noreferrer"">Attention is All you Need</a>, and I have a question about masking in the decoder. It's stated that masking is used to ensure the model doesn't attend to any tokens in the future (not yet predicted), so it can be used autoregressively during inference.</p>
<p>I don't understand how masking is used during inference. When the encoder is given an unseen sample with no ground truth output or prediction, it seems to me that there is nothing to mask, since there aren't any output tokens beyond what the decoder has already produced. Is my understanding of masking correct?</p>
<p>Thanks!</p>
","transformer"
"80682","Why does the non autoregresive transfomer model in fairseq require the prev_output_tokens input?","2020-08-23 08:35:23","","0","221","<pytorch><transformer><sequence-to-sequence>","<p><a href=""https://github.com/pytorch/fairseq"" rel=""nofollow noreferrer"">fairseq</a> includes an implementation of a non autoregressive transformer - which (as much as I understand) means that the whole output sequence is generated in a single forward run (in contrast to autoregresive models where each forward run predicts the next token from the input and the previous predicted tokens)</p>
<p>However, from the code it appears that the models still expects the previous tokens as input:</p>
<pre><code>def forward(self, src_tokens, src_lengths, prev_output_tokens, tgt_tokens, **kwargs)
</code></pre>
<p><a href=""https://github.com/pytorch/fairseq/blob/master/fairseq/models/nat/nonautoregressive_transformer.py#L78"" rel=""nofollow noreferrer"">https://github.com/pytorch/fairseq/blob/master/fairseq/models/nat/nonautoregressive_transformer.py#L78</a></p>
","transformer"
"80660","Splitting into multiple heads -- multihead self attention","2020-08-22 16:19:20","","1","2151","<tensorflow><bert><transformer><attention-mechanism>","<p>So, I have a doubt in <strong>Attention is all you need</strong>:</p>
<p>The implementation of transformers on tensorflow's official documentation <a href=""https://www.tensorflow.org/tutorials/text/transformer#multi-head_attention"" rel=""nofollow noreferrer"">says</a>:</p>
<blockquote>
<p>Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads.</p>
</blockquote>
<p>However, The paper mentions:</p>
<blockquote>
<p>Instead of performing a single attention function with dmodel-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values h times with different, learned
linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional
output values.</p>
</blockquote>
<p>There is no mention of <em>splitting</em> Q,K and V to obtain heads. Instead, the paper says that they are passed through 'h' different dense layers to convert d-model dimensional vectors to 'h' different dk,dk and dv dimensional vectors respectively. So basically the pseudocode, from what I understood, should look something like this:</p>
<pre><code>Q,K &amp; V are d-model dimensional vectors.

for i in range(h):
   Qi = Dense(dk)(Q)
   Ki = Dense(dk)(K)
   Vi = Dense(dv)(V)
   Ai = Attention(Qi, Ki, Vi)

A0, A1, A2, ..., Ah are then concatenated.
</code></pre>
<p>Is this right? or am I missing something here?</p>
","transformer"
"80537","What are the hidden states in the Transformer-XL? Also, how does the recurrence wiring look like?","2020-08-19 21:00:18","80553","0","1589","<deep-learning><nlp><rnn><transformer><attention-mechanism>","<p>After exhaustively reading the many blogs and papers on Transformers-XL, I still have some questions before I can say that I understand Transformer-XL (and by extension XLNet). Any help in this regard is hugely appreciated.</p>
<ol>
<li>when we say <strong>hidden states</strong> are transferred from one segment to another, what exactly is included in these hidden states? Are the weights of the networks implementing the attention mechanism (i.e. calculating the Q, K and V) included? Are the weights involved in calculating the input word embedding included in the hidden state?</li>
<li>When the <strong>hidden states</strong> are transferred during recurrence, is this transfer from the encoder of one segment to the encoder of the next segment? Or is it from the decoder of the current segment to the encoder of the next segment? Is the decoder involved at all in the hidden state transfer?</li>
<li>I see images like the following in the following in the papers and blogs. what do the dots represent? encoders? decoders? or an entire unit? I guess the answer to my second question will shed a light on this one too.
<a href=""https://i.sstatic.net/fULu8.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fULu8.png"" alt=""From Transformer-XL Google page"" /></a></li>
</ol>
<p>Thank you</p>
","transformer"
"80483","Based on transformer, how to improve the text generation results?","2020-08-19 04:09:42","80496","1","490","<deep-learning><nlp><transformer><text-generation>","<p>If I do not pretrain the text generation model like BART, how to improve the result based on transformer like tensor2tensor?</p>
<p>What are the improvement ideas for transformer in text generation task?</p>
","transformer"
"80191","Overfitting while fine-tuning pre-trained transformer","2020-08-12 18:03:26","","4","11232","<nlp><transfer-learning><transformer>","<p>Pretrained transformers (GPT2, Bert, XLNET) are popular and useful because of their transfer learning capabilities.</p>
<p>Just as a reminder: The goal of Transfer learning is is to transfer knowledge gained from one domain/task and use that transfer/use that knowledge to solve some related tasks. This is done by training a model on a huge amount of labelled data (which we already have and is probably easy to get), then remove the last few layers and fine-tune the model for the new related task with task-related dataset.</p>
<p>I took a recent pretrained transformer published by Google, called XLNET, and just added the classification layer from the top of it and fine-tuned the whole network. (Which is the main intention of this kind of model, correct me if I am wrong).</p>
<p>The problem is that the model is largely overfitting. I have 1200 examples to train and each has 350 words on average.</p>
<p>To overcome the overfitting, I set the dropout of each layer of the transformer from 0.1 to 0.5. This did not work. So I decreased the number of trainable parameters (since the transformer has a huge number of parameters), by freezing first 10 layers (11 layers + 1 classification layer in total). Even that does not work. So I counted the number of trainable parameters in the last layer. There are 7680000 parameters which are very high compared to my dataset (around 1200*350= 420000 words). So, this high number of tunable parameters is the most possible reason for overfitting.</p>
<p>Here is the loss graph:</p>
<p><a href=""https://i.sstatic.net/EXOXZ.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/EXOXZ.png"" alt=""enter image description here"" /></a></p>
<p>My questions are:
Do you see any flaw in my analysis?
Is there anything I can do to decrease overfitting? (tried with low learning rate and large batch size)
If my analysis is correct, then the claim that &quot;fine-tune pre-trained transformers with small dataset&quot; is bit misleading and datasets should not be so small. Am I correct?</p>
","transformer"
"79995","Explanation about i//2 in positional encoding in tensorflow tutorial about transformers","2020-08-08 22:29:26","","2","159","<tensorflow><nlp><encoding><transformer><attention-mechanism>","<p>I was implementing the transformer architecture in tensorflow.</p>
<p>I was following the tutorial : <a href=""https://www.tensorflow.org/tutorials/text/transformer#setup_input_pipeline"" rel=""nofollow noreferrer"">https://www.tensorflow.org/tutorials/text/transformer#setup_input_pipeline</a></p>
<p>They implement the positional encoding in this way:</p>
<pre><code>angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))
</code></pre>
<p>However in the paper i is not divided by 2 (i//2), is this a bug? , or why is the reason to make this operation?</p>
<p><a href=""https://i.sstatic.net/fL0cp.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fL0cp.jpg"" alt=""enter image description here"" /></a></p>
<p>thanks</p>
","transformer"
"78556","For NLP, is GPT-3 better than RoBERTa?","2020-07-30 17:51:12","78562","0","2626","<nlp><bert><transformer><gpt>","<p>I am learning deep learning and I want to get into NLP. I have done LSTM, and now I am learning about vectorisation and transformers. Can you please tell me, which algorithm is more effective and accurate?</p>
","transformer"
"78460","Question of pretraining text-generation task, it seems that pretraining is not work for a small model?","2020-07-29 02:22:37","80484","0","26","<transformer><machine-translation><text-generation><bart><pretraining>","<p>My task is to generate keywords from sentences.</p>
<p>I pretrain a text-generation model. I mask the sentences' tokens and predict the whole sentences' tokens.</p>
<p>Pretraining batch_size = 8 and step = 1000000</p>
<p>I haven't observed improvement from pretraining. BLEU score is 10.5 for not pretraining, BLEU score is 9.5 for pretraining.</p>
<h2>Code</h2>
<p>I take the python code from</p>
<p><a href=""https://github.com/google-research/pegasus/blob/master/pegasus/models/transformer.py#L38"" rel=""nofollow noreferrer"">https://github.com/google-research/pegasus/blob/master/pegasus/models/transformer.py#L38</a></p>
<p>hidden_size = 512
num_encoder_layers = 3
num_decoder_layers = 3</p>
<h2>Discussion</h2>
<p>The task is to generate keyword from sentences.
The keyword may not appear in the sentences.
So input masked sentences to predict whole sentences, it is not benefit the keywords generation task.
Input masked sentences to predict whole sentences, it do not have relation to the keywords generation task.
Am I right? Is it the reason that pretraining do not improve the BLEU score?</p>
<p>Thank you very much.</p>
","transformer"
"77257","BERT reasoning capabilities","2020-07-06 18:00:56","","0","73","<bert><transformer>","<p>I'm working on a Twitter classification task and while analyzing the errors I found quite a few strange predictions. I'm searching for a tool (preferably open-source) similar to <a href=""https://towardsdatascience.com/how-does-bert-reason-54feb363211"" rel=""nofollow noreferrer"">https://towardsdatascience.com/how-does-bert-reason-54feb363211</a> that is able to compute the highest positive/negative attribution given to the words (the reason why I'm not able to use the approach presented in the aforementioned article is the price). In this way, I hope that I'll be able to better understand (and possibly correct) these misclassifications. I tried looking at the attention heads but I don't feel that I'm able to fully understand and draw conclusions based on this information.</p>
<p>Any help would be greatly appreciated!</p>
","transformer"
"77093","what is the difference between positional vector and attention vector used in transformer model?","2020-07-03 19:43:11","77121","1","123","<deep-learning><rnn><transformer><attention-mechanism><vector-space-models>","<p>what is the difference between positional vector and attention vector used in transformer model ? , i saw a video in youtue and the defintion for positional vector was give as :* &quot;vector that gives context based on postion of word in sentence &quot;*
defintion for attention vector was give as &quot;For ever word we can have attention vector generated which captures contextual relationship between words in sentence&quot;</p>
<p>Capturing context information based on distance(postional vector) and attention (attention vector ) sounds same right? or is it different ?</p>
","transformer"
"77044","Bert-Transformer : Why Bert transformer uses [CLS] token for classification instead of average over all tokens?","2020-07-02 21:25:51","","8","6323","<machine-learning><deep-learning><tensorflow><bert><transformer>","<p>I am doing experiments on bert architecture and found out that most of the fine-tuning task takes the final hidden layer as text representation and later they pass it to other models for the further downstream task.</p>
<p>Bert's last layer looks like this :</p>
<p><a href=""https://i.sstatic.net/m0jrg.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/m0jrg.png"" alt=""enter image description here"" /></a></p>
<p>Where we take the [CLS] token of each sentence :</p>
<p><a href=""https://i.sstatic.net/1OklZ.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/1OklZ.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"" rel=""noreferrer"">Image source</a></p>
<p>I went through many discussion on this <a href=""https://github.com/huggingface/transformers/issues/1950"" rel=""noreferrer"">huggingface issue</a>,  <a href=""https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-its-encoding-output-is-important"">datascience forum question</a>,  <a href=""https://github.com/google-research/bert/issues/196"" rel=""noreferrer"">github issue</a> Most of the data scientist gives this explanation :</p>
<blockquote>
<p>BERT is bidirectional, the [CLS] is encoded including all
representative information of all tokens through the multi-layer
encoding procedure. The representation of [CLS] is individual in
different sentences.</p>
</blockquote>
<p>My question is, Why the author ignored the other information ( each token's vector ) and taking the average, max_pool or other methods to make use of all information rather than using [CLS] token for classification?</p>
<p>How does this [CLS] token help compare to the average of all token vectors?</p>
","transformer"
"76872","Next sentence prediction in RoBERTa","2020-06-29 20:55:34","","2","2865","<nlp><bert><transformer>","<p>I'm trying to wrap my head around the way next sentence prediction works in RoBERTa. Based on their paper, in section 4.2, I understand that in the original BERT they used a pair of text segments which may contain multiple sentences and the task is to predict whether the second segment is the direct successor of the first one. RoBERTa's authors proceed to examine 3 more types of predictions - the first one is basically the same as BERT, only using two sentences insted of two segments, and you still predict whether the second sentence is the direct successor of the first one. But I can't understand what the goal is in the other 2. I will cite their explanation below:</p>
<p>• FULL-SENTENCES: Each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens. Inputs may cross document boundaries. When we reach the end of one document, we begin sampling sentences from the next document and add an extra separator token between documents. We remove the NSP loss.</p>
<p>• DOC-SENTENCES: Inputs are constructed similarly to FULL-SENTENCES, except that they
may not cross document boundaries. Inputs sampled near the end of a document may be
shorter than 512 tokens, so we dynamically increase the batch size in these cases to achieve a similar number of total tokens as FULL-SENTENCES. We remove the NSP loss.</p>
<p>So from what I understand in these two training strategies they already sample consecutive sentences, or at least consecutive sentences from neighbouring documents,  and I can't see what they are trying to predict - it can't be whether they're consecutive text blocks, because to me it seems that all of their training examples have already been sampled contiguously, thus making such a task redundant. It would be of enormous help if someone were to shed some light on the issue, thanks in advance!</p>
","transformer"
"76787","What is the difference between register_buffer() and parameter.detach() in PyTorch?","2020-06-28 03:25:17","","2","605","<neural-network><pytorch><transformer><machine-translation>","<p>I am writing a <code>PositionalEmbedding()</code> module which is an implementation based on &quot;<a href=""https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf"" rel=""nofollow noreferrer"">Attention Is All You Need</a>&quot; using PyTorch. According to the paper, there should be no learnable attribute in the module <code>PositionalEmbedding()</code>.</p>
<p>The initialization of the Embedding weights is as follows:</p>
<pre><code>    import math
    import torch
    
    class TrigonometricPositionalEmbedding(torch.nn.Module):
        def __init__(self, embedding_number, dimension, padding_idx):
            position = torch.arange(0, embedding_number).unsqueeze(1)
            sin_multiplicator = torch.exp(-(math.log(10000) / dimension) * 2 * torch.arange(0, dimension, 2))
            cos_multiplicator = torch.exp(-(math.log(10000) / dimension) * 2 * torch.arange(1, dimension, 2))
            sin_weight = torch.sin(position * sin_multiplicator)
            cos_weight = torch.cos(position * cos_multiplicator)
    
            weight = torch.zeros(embedding_number, dimension)
            weight[:, 0::2] = sin_weight
            weight[:, 1::2] = cos_weight
</code></pre>
<p>Next up, because the <code>weight</code> in the module should not be learnable, <code>detach()</code> the <code>weight</code> came into my mind. I think it may not be the most elegant way, so I made a further investigation. I found that there are two kinds of implementation method like what <a href=""https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/modules/embeddings.py#L34"" rel=""nofollow noreferrer"">OpenNMT</a> and <a href=""https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sinusoidal_positional_embedding.py#L104"" rel=""nofollow noreferrer"">FAIRSeq</a> have done respectively.</p>
<p>I follow the method of <strong>OpenNMT</strong> and my implementation is as follows:</p>
<pre><code>    class TrigonometricPositionalEmbedding(torch.nn.Module):
        def __init__(self, embedding_number, dimension, padding_idx):
            ...
            weight[:, 1::2] = cos_weight
            self.register_buffer('weight', weight)
    
        def forward(self, position):
            torch.index_select(self.weight, 0, position)
</code></pre>
<p>I follow the method of <strong>FAIRSeq</strong> and my implementation is as follows:</p>
<pre><code>    class TrigonometricPositionalEmbedding(torch.nn.Module):
        def __init__(self, embedding_number, dimension, padding_idx):
            ...
            weight[:, 1::2] = cos_weight
            self.weight = weight
    
        def forward(self, position):
            torch.index_select(self.weight, 0, position).detach()
</code></pre>
<p>So I am curious about the difference between the two method, or what is the difference between <strong>registered buffer</strong> and <strong>detached parameter</strong>?</p>
<p>(The question was also posted in <a href=""https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-parameter-detach-in-pytorch/87163"" rel=""nofollow noreferrer"">PyTorch Forums</a> but no answer yet.)</p>
","transformer"
"76359","Calculating key and value vector in the Transformer's decoder block","2020-06-20 18:13:17","","1","348","<pytorch><transformer>","<p>I am implementing the transformer model in Pytorch by following Jay Alammar's <a href=""http://jalammar.github.io/illustrated-transformer"" rel=""nofollow noreferrer"">post</a>
and the implementation <a href=""https://github.com/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"" rel=""nofollow noreferrer"">here</a>.</p>
<p><a href=""https://i.sstatic.net/SPNEP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/SPNEP.png"" alt=""from https://jaammar.github.io/illustrated-transformer/"" /></a></p>
<p>My question is regarding the input to the decoder layer.</p>
<p>As shown in the diagram above, the <i>encoder_decoder_attention</i> layer in the decoder get the <i>queries</i> vector from the self-attention layer below, and <i>keys</i> and <i>values</i> vector from the encoder.</p>
<p>Here is the implementation of the decoder from above post:</p>
<pre><code>class DecoderLayer(nn.Module):
        # ...
        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)
        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)
        #...

    def forward(self, trg, enc_src, trg_mask, src_mask):
        
        #trg = [batch size, trg len, hid dim]
        #enc_src = [batch size, src len, hid dim]
        #trg_mask = [batch size, trg len]
        #src_mask = [batch size, src len]
        
        #self attention
        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)
        
        #encoder decoder attention
        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)

        
</code></pre>
<p>And the <code>MultiHeadAttentionLayer</code> looks like this:</p>
<pre><code>class MultiHeadAttentionLayer(nn.Module):
    def __init__(self, hid_dim, n_heads, dropout, device):
        
        # ...

        self.fc_q = nn.Linear(hid_dim, hid_dim)
        self.fc_k = nn.Linear(hid_dim, hid_dim)
        self.fc_v = nn.Linear(hid_dim, hid_dim)
        
        
    def forward(self, query, key, value, mask = None):
        
        batch_size = query.shape[0]
        
        #query = [batch size, query len, hid dim]
        #key = [batch size, key len, hid dim]
        #value = [batch size, value len, hid dim]
                
        Q = self.fc_q(query)
        K = self.fc_k(key)
        V = self.fc_v(value)
        
        #Q = [batch size, query len, hid dim]
        #K = [batch size, key len, hid dim]
        #V = [batch size, value len, hid dim]
                
        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)
        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)
        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)

        # ...
</code></pre>
<p>What got me confused is as Jay mentions in his post:</p>
<blockquote>
<p>we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices(Wq, Wk and Wv) that we trained during the training process.</p>
</blockquote>
<p>Visually from the diagram, it looks like the <i>key</i> and the <i>value</i> vector themselves from the encoder is passed to the decoder. If so should't we be using the same weight matrices (Wq, Wk and Wv) from the encoder, in the decoder's <i>encoder_decoder_attention</i> layer as well ?</p>
<p>The code implementation has it's own Linear layers (with own weights) to calculate these vectors, and obtains the vectors by multiplying the encoder_outputs.</p>
","transformer"
"75343","What are the simplest methods for the label noise problem?","2020-06-03 02:16:33","88116","0","51","<machine-learning><neural-network><deep-learning><bert><transformer>","<p>If I have enough low quality data from unsupervised methods or rule-based methods.</p>

<p>I read from <a href=""https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise"" rel=""nofollow noreferrer"">https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise</a> ,but these methods are a little complex for me.</p>

<p>In detail, I deal with a multi-label classification task. First I crawl web page such as wiki and use regex-based rule to mark the label. The model input is the wiki title and the model output is the rule-matched labels from wiki content. My task is to predict the labels for the wiki title.</p>

<p>Do you think <strong>removing the wrong data predicted by trained model</strong> is a simple but effective method?</p>
","transformer"
"75314","How to understand Inconsistent and ambiguous dimensions of matrices used in the Attention layer?","2020-06-02 16:51:58","75494","0","238","<deep-learning><rnn><transformer><attention-mechanism>","<p>Attention-scoring mechanism seems to be a commonly-used component in various seq2seq models, and I was reading about the original ""Location-based Attention"" in Bahadanau well-known paper at <a href=""https://arxiv.org/pdf/1506.07503.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1506.07503.pdf</a>. (it seems this attention is used in various forms of GNMT and text-to-speech sythesizers like tacotron-2 <a href=""https://github.com/Rayhane-mamah/Tacotron-2"" rel=""nofollow noreferrer"">https://github.com/Rayhane-mamah/Tacotron-2</a>).</p>

<p>Even after repeated readings of this paper and other articles about Attention-mechanism, I'm confused about the dimensions of the matrices used, as the paper doesn't seem to describe it. My understanding is:</p>

<ul>
<li><p>If I have decoder hidden dim 1024, that means (<span class=""math-container"">$s_{i-1}$</span>} vector is 1024 length.</p></li>
<li><p>If I have encoder output dim 512, that means <span class=""math-container"">$h_{j}$</span> vector is 512 length.</p></li>
<li><p>If the maximum number of inputs to the encoder is 256, then the number of <span class=""math-container"">$j$</span> can be from 1 to 256.</p></li>
<li><p>Since <span class=""math-container"">$W \times S_{i-1}$</span> is a matrix multiply, it seems <span class=""math-container"">$\text{cols}(W)$</span> should match <span class=""math-container"">$\text{rows}(S_{i-1})$</span>, but <span class=""math-container"">$\text{rows}(W)$</span> still remain undefined. The same seems true for matrices <span class=""math-container"">$V$</span>, <span class=""math-container"">$U$</span>, <span class=""math-container"">$w$</span>, <span class=""math-container"">$b$</span>.</p></li>
</ul>

<p>This is page-3/4 from the paper above that describes Attention-layer:</p>

<p><a href=""https://i.sstatic.net/xKD2U.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/xKD2U.png"" alt=""enter image description here""></a></p>

<p>I'm unsure how to make sense of this. Am I missing something, or can someone explain this?</p>

<p>What I don't understand is:</p>

<ul>
<li><p>What is the dimension of previous alignment (denoted by <span class=""math-container"">$\alpha_{i-1}$</span>)? Shouldn't it be total values of <span class=""math-container"">$j$</span> in <span class=""math-container"">$h_{j}$</span> (which is 256 and means total different encoder output states)?</p></li>
<li><p>What is the dimension of <span class=""math-container"">$f_{i,j}$</span> and convolution filter <span class=""math-container"">$F$</span>? (the paper says F belongs to <span class=""math-container"">$k\times r$</span> shape but doesn't define <span class=""math-container"">$r$</span> anywhere). What is <span class=""math-container"">$r$</span> and what does <span class=""math-container"">$k \times r$</span> mean here?</p></li>
<li><p>How are these unknown dimensions for matrices <span class=""math-container"">$V$</span>, <span class=""math-container"">$U$</span>, <span class=""math-container"">$w$</span>, <span class=""math-container"">$b$</span> described above determined in this model?</p></li>
</ul>
","transformer"
"75304","BPE vs WordPiece Tokenization - when to use / which?","2020-06-02 14:21:40","","10","7567","<machine-learning><nlp><sentiment-analysis><transformer><machine-translation>","<p>What's the general tradeoff between choosing BPE vs WordPiece Tokenization? When is one preferable to the other? Are there any differences in model performance between the two? I'm looking for a general overall answer, backed up with specific examples.</p>
","transformer"
"75201","TensorFlow1.15, multi-GPU-1-machine, how to set batch_size?","2020-06-01 05:23:51","88118","1","632","<deep-learning><tensorflow><bert><transformer>","<p>The input function code:</p>

<pre><code>    def input_fn(params):
        """"""The actual input function.""""""
        batch_size = FLAGS.train_batch_size

        name_to_features = {
            ""input_ids"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""input_mask"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""segment_ids"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""masked_lm_positions"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
            ""masked_lm_ids"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
            ""masked_lm_weights"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.float32),
            ""next_sentence_labels"":
                tf.FixedLenFeature([1], tf.int64),
        }

        # For training, we want a lot of parallel reading and shuffling.
        # For eval, we want no shuffling and parallel reading doesn't matter.
        if is_training:
            d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))
            d = d.repeat()
            d = d.shuffle(buffer_size=len(input_files))

            # `cycle_length` is the number of parallel files that get read.
            cycle_length = min(num_cpu_threads, len(input_files))

            # `sloppy` mode means that the interleaving is not exact. This adds
            # even more randomness to the training pipeline.
            d = d.apply(
                tf.contrib.data.parallel_interleave(
                    tf.data.TFRecordDataset,
                    sloppy=is_training,
                    cycle_length=cycle_length))
            d = d.shuffle(buffer_size=100)
        else:
            d = tf.data.TFRecordDataset(input_files)
            # Since we evaluate for a fixed number of steps we don't want to encounter
            # out-of-range exceptions.
            d = d.repeat()

        # We must `drop_remainder` on training because the TPU requires fixed
        # size dimensions. For eval, we assume we are evaluating on the CPU or GPU
        # and we *don't* want to drop the remainder, otherwise we wont cover
        # every sample.
        d = d.apply(
            tf.contrib.data.map_and_batch(
                lambda record: _decode_record(record, name_to_features),
                batch_size=batch_size,
                num_parallel_batches=num_cpu_threads,
                drop_remainder=True))
        d = d.prefetch(10)
        return d

</code></pre>

<p>The mirrow strategy code:</p>

<pre><code>    distribution = tf.contrib.distribute.MirroredStrategy(
        devices=[""device:GPU:%d"" % i for i in range(FLAGS.n_gpus)],
        # num_gpus=4,
        cross_tower_ops=tf.distribute.HierarchicalCopyAllReduce())
    run_config = RunConfig(
        train_distribute=distribution,
        # eval_distribute=dist_strategy,
        log_step_count_steps=log_every_n_steps,
        model_dir=FLAGS.output_dir,
        save_checkpoints_steps=FLAGS.save_checkpoints_steps)

    model_fn = model_fn_builder(
        bert_config=bert_config,
        init_checkpoint=FLAGS.init_checkpoint,
        learning_rate=FLAGS.learning_rate,
        num_train_steps=FLAGS.num_train_steps,
        num_warmup_steps=FLAGS.num_warmup_steps,
        use_tpu=FLAGS.use_tpu,
        use_one_hot_embeddings=FLAGS.use_tpu)

    # If TPU is not available, this will fall back to normal Estimator on CPU
    # or GPU.
    estimator = Estimator(
        model_fn=model_fn,
        params={},
        config=run_config)
</code></pre>

<p>The problem is that if I have 4 GPU. Each GPU could run 8 batchsize. I set <code>batch_size = 8</code> not 32. <code>batch_size = 32</code> will OOM.</p>

<p>Am I right? Will the data be distributed to 4 GPU with different batches?</p>
","transformer"
"75108","German Chatbot or conversational AI","2020-05-30 10:55:41","","3","859","<dataset><nlp><bert><transformer>","<p>I want to build a chatbot mostly BERT(Transformer) based in the German Language. But I do not find any German chatbot data set!</p>

<p>So does it make sense to use google translator API to translate the English dataset to German and then train the model on it?</p>

<p>Any idea where I can find German datasets or solve this issue?</p>
","transformer"
"74893","Transformer-based architectures for regression tasks","2020-05-26 18:03:35","","7","5590","<regression><autoencoder><transformer><attention-mechanism>","<p>As far as I've seen, transformer-based architectures are always trained with classification tasks (one-hot text tokens for example). Are you aware of any architectures using attention and solving regression tasks? Could one build a regressive auto-encoder for example? How would normalization fit into this (as LayerNorm destroys some of the information from the input)?</p>
","transformer"
"74525","Transformer decoder output - how is it linear?","2020-05-20 13:43:21","74529","6","2088","<deep-learning><transformer><attention-mechanism>","<p>I'm not quite sure how's the decoder output is flattened into a single vector.
As from my understanding, if we input the encoder with a length N sentence, it's output is N x units (e.g. N x 1000), and we input the decoder with a length M sentence, the output of the decoder will give us M x units output. M is not fixed (M should be the length of the decoder's raw input) and will change during the different steps of inference.
How do we go from here to a single vector?
<a href=""https://i.sstatic.net/rtdK5.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/rtdK5.png"" alt=""enter image description here""></a>
Screen shot from ""Attention is all you need""</p>
","transformer"
"74444","sklearn ColumnTransformer creates new columns in output when there are overlapping columns between steps","2020-05-19 09:19:27","","1","1310","<python><scikit-learn><transformer>","<p>I need to process some dataframe columns in different steps using <a href=""https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html"" rel=""nofollow noreferrer"">ColumnTransformer</a>. 
The first step process the date columns (timestamp) imputing missing values and the second step applies scaling to all the numeric columns (including the dates columns). In output I get a number of columns which is the sum of the numeric columns and the dates columns, but the dates columns are a subset of the numeric columns so this is not correct.</p>

<pre class=""lang-py prettyprint-override""><code>from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

dates_columns = ['ts_1', 'ts_2']
numeric_columns = ['ts_1', 'ts_2', 'n_1', 'n_2']

column_transformer = ColumnTransformer([
    ('imputer_dates', SimpleImputer(strategy='median', missing_values=0), date_columns),
    ('scaler', StandardScaler(), numeric_columns)
])

X_transformed = column_transformer.fit_transform(X)
print(X_transformed.shape) # Got 6 columns, but it should be 4
</code></pre>

<p>How to fix this?</p>
","transformer"
"74115","Is BERT a language model?","2020-05-13 12:22:22","74119","9","3853","<nlp><bert><transformer><language-model>","<p>Is BERT a language model in the sense of a function that gets a sentence and returns a probability?
I know its main usage is sentence embedding, but can it also provide this functionality?</p>
","transformer"
"73189","Does BERT use GLoVE?","2020-04-28 21:23:47","73276","7","5281","<nlp><bert><transformer><attention-mechanism>","<p>From all the docs I read, people push this way and that way on how BERT uses or generates embedding. I GET that there is a key and a query and a value and those are all generated.</p>

<p>What I don't know is if the original embedding - the original thing you put into BERT - could or should be a vector. People wax poetic about how BERT or ALBERT can't be used for word to word comparisons, but nobody says explicitly what bert is consuming. Is it a vector? If so is it just a one-hot vector? Why is it not a GLoVE vector? (ignore the positional encoding discussion for now please)</p>
","transformer"
"72857","Overfitting with text classification using Transformers","2020-04-23 12:43:10","","3","9291","<classification><nlp><transformer><text-classification>","<p>I am trying to make a binary text classification model by using the encoder part of the transformer and then using its output to feed into an LSTM network. However, I am not able to achieve good accuracy on both the training set (92%) and the validation set (72%). Is my approach correct? Please tell me a better way to design the model and improve accuracy.</p>
","transformer"
"72420","Can BERT be used for predicting words?","2020-04-16 09:03:52","72449","1","900","<neural-network><deep-learning><bert><transformer><attention-mechanism>","<p>I have a question regarding the pre-training section (in particular, the Masked Language Model).</p>

<p>In the example <em>Let's stick to improvisation in this skit</em>, by masking the word <em>improvisation</em>, after applying BERT (followed by the FFNN and Softmax), by looking at the probabilities of each of the possible English words,  we are able to correctly predict that the masked word was <em>improvisation</em>.</p>

<p>Is it possible to actually play with this by using my own examples? I was wondering if I can input a sentence in a different language (from the multilingual model) and have a sorted list of the most probable words that were masked in the original sentence. If it's possible, what needs to be tweaked?</p>

<p>Any help would be greatly appreciated.</p>
","transformer"
"72025","What are good toy problems for testing Transformer architectures?","2020-04-09 12:15:28","","5","1066","<dataset><transformer><attention-mechanism>","<p>I am testing various variants for Transformers and Transformer architectures. But training on full language tasks is a rather time-consuming affair. What are good toy problems to test if a transformer (or alternative thereof) is working at all? I am looking for simple problems that can preferably be synthetically created and can be trained with really small setups (few layers, small embedding sizes, etc.) in a short time. Ideally, these should be problems that play to the strengths of transformers but would be hard to solve for, say, a fully connected feed-forward network. Tasks that can be applied to just an Attention Layer would be useful, too.</p>
","transformer"
"71856","Custom functions and pipelines","2020-04-06 16:21:06","","2","2546","<machine-learning><python><cross-validation><transformer><pipelines>","<p>I'm not really used to working with pipelines, so I'm wondering how can I use custom functions and pipelines.</p>

<p>Situation: I want to fill some missing values with the mean but using groups based on other feature. That's why I'm using this custom function:</p>

<pre><code>def replaceNullFromGroup(From, To, variable, by):

    # 1. Create aggregation from train dataset
    From_grp = From.groupby(by)[variable].median().reset_index()

    # 2. Merge dataframes
    To_merged = To.merge(From_grp, on=by, suffixes=['_test', '_train'], how = ""left"")

    # 3. Create dictionaries
    to_cols = [col for col in To_merged.columns if 'test' in col]
    from_cols = [col for col in To_merged.columns if 'train' in col]
    dict_cols =dict(zip(to_cols, from_cols))

    # 4. Replace null values
    for to_col, from_col  in dict_cols.items():
        To_merged[to_col] = np.where(To_merged[to_col].isnull(), 
                                     To_merged[from_col], 
                                     To_merged[to_col])

    # 5. Clean up dataframe    
    To_merged.drop(from_col, axis=1, inplace=True)
    To_merged.columns = To_merged.columns.str.replace('_test', '')
    return To_merged
</code></pre>

<p>Variables meaning:</p>

<ul>
<li>From: Dataframe where I'm taking the information (Train dataset)</li>
<li>To: Dataframe where I will fill the missing values (Train and test dataset)</li>
<li>variable: variable with missing values</li>
<li>by: Variables I'm using to make groups</li>
</ul>

<p><strong>Can I use this function in a pipeline so I can use cross validation avoiding data leakage?</strong></p>

<p>Thank you very much</p>
","transformer"
"71683","What is ""position"" in CNN (im2latex) for Positional Encoding?","2020-04-03 15:49:15","","0","502","<cnn><encoding><transformer><mathematics>","<p>I'm trying to build a model that maps images of math
formulas into LaTeX markup.</p>
<p>I found an acticle (<a href=""https://arxiv.org/ftp/arxiv/papers/1908/1908.11415.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/ftp/arxiv/papers/1908/1908.11415.pdf</a>) that proposes an encoder-decoder architecture that solves this problem.</p>
<p>In encoder after the work of CNN it's proposed (page 3) to add following signals to generated feature maps &quot;in order to retain the spatial locality information&quot;:</p>
<p><span class=""math-container"">$$PE(x,y,2i)=sin(x/10000^{4i/D})$$</span>
<span class=""math-container"">$$PE(x,y,2i+1)=cos(x/10000^{4i/D})$$</span>
<span class=""math-container"">$$PE(x,y,2j+D/2)=sin(y/10000^{4j/D})$$</span>
<span class=""math-container"">$$PE(x,y,2j+1+D/2)=cos(y/10000^{4j/D})$$</span></p>
<p>Here the authors refer to positional encoding part of Transformer model &quot;tailoring the 1-D positional encoding technique to 2-D&quot;.</p>
<p>In original Transformer model (from article &quot;<a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Attention Is All You Nee</a>d&quot;) the position encoding is calculated like this:</p>
<p><a href=""https://i.sstatic.net/B2l60.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/B2l60.png"" alt=""enter image description here"" /></a></p>
<p>Where &quot;pos&quot; is just an index number of word in given sentence (correct me if I'm wrong).</p>
<p>And here comes the question: <strong>what is &quot;x&quot; and &quot;y&quot; in first formula?</strong></p>
","transformer"
"71658","In ""Attention Is All You Need"", why are the FFNs in (2) the same as two convolutions with kernel size 1?","2020-04-03 03:15:50","73539","1","365","<convolution><transformer><attention-mechanism><research>","<p>In addition, why do we need a FFN in each layer when we already have attention?</p>

<p>Here's a screenshot of the relevant section from Vaswani et al. (2017):</p>

<p><a href=""https://i.sstatic.net/T9qeu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/T9qeu.png"" alt=""eScreenshot of relevant section from Vaswani et al. (2017)""></a></p>
","transformer"
"70222","Does the transformer decoder reuse previous tokens' intermediate states like GPT2?","2020-03-25 15:44:14","71654","3","2153","<nlp><transformer><gpt>","<p>I recently read Jay Alammar's blogpost about GPT-2 (<a href=""http://jalammar.github.io/illustrated-gpt2/"" rel=""nofollow noreferrer"">http://jalammar.github.io/illustrated-gpt2/</a>) which I found quite clear appart from one point :
He explains that the decoder of GPT-2 processes input tokens one at a time, only actively processing the last input token, the past tokens being saved in memory already and ""passively"" reused without reevaluation.</p>

<p>From my understanding of the transformer architecture, I had the impression that the decoder reevaluates every token generated at each generation. Is this then a difference between the decoder from GPT-2 or does the decoder from a ""classical' transformer also work this way ?</p>

<p>Intuitively I would think that it would make more sense the reevaluate everything at each iteration since new dependencies between words can appear that weren't there at the beginning and would then not be taken into account if past processed words are passively reused.</p>

<p>I hope I am making sense, can someone with knowledge about the GPT2 architecture help me clarify this ?</p>
","transformer"
"69844","Transformer-XL architecture","2020-03-17 17:05:17","","0","184","<neural-network><transformer>","<p>I am a bit perplex from the transformer-XL architecture that is claimed to solve the issue of context fragmantation. I probably understood it wrong but it looks like all the transformer-XL is doing, is being 2 times larger than the normal one, and that you translate at each iterations. Plus a fancy new way of doing positional encoding.</p>

<p>Am I interpreting it wrong ?</p>
","transformer"
"69546","Transformer seq2seq model and loading embeddings from XLM-RoBERTa","2020-03-11 18:01:15","","0","927","<neural-network><pytorch><sequence-to-sequence><machine-translation><transformer>","<p>Is it possible to feed embeddings from XLM- RoBERTa to transformer seq2seq model? I'm working on NMT that translates verbal language sentences to sign language sentences (e.g Input: He sells food. Output (sign language sentence): Food he sells). But I have a very small dataset of sentence pairs - around 1000. And the language is a low resource language.</p>

<p>I am a new researcher on the field of deep learning. Please help with your valuable advice.</p>
","transformer"
"69358","Why does vanilla transformer has fixed-length input?","2020-03-08 16:28:59","69481","8","2490","<nlp><transformer>","<p>I know that in the math on which the transformer is based there is no restriction on the length of input. But I still can’t understand why we should fix it in the frameworks (PyTorch). Because of this problem Transformer-XL has been created.</p>

<p>Can you explain to me where this problem is hiding, please?</p>
","transformer"
"69041","How do Bahdanau - Luong Attentions use Query, Value, Key vectors?","2020-03-03 08:56:55","85643","2","1949","<deep-learning><tensorflow><rnn><transformer><attention-mechanism>","<p>In the latest <strong>TensorFlow 2.1</strong>, the <code>tensorflow.keras.layers</code> submodule contains <code>AdditiveAttention()</code> and <code>Attention()</code> layers, implementing Bahdanau and Luong's attentions, respectively. (docs <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/AdditiveAttention"" rel=""nofollow noreferrer"">here</a> and <a href=""https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention"" rel=""nofollow noreferrer"">here</a>.)</p>

<p>These new type of layers require <code>query</code>, <code>value</code> and <code>key</code> inputs (the latest is optional though). However, Query, Value, Key vectors are something I've always read referred to Transformer architectures.</p>

<p>What do these vectors represent, when it comes to Bahdanau and Luong attention? For example, if I want to train an RNN model for a common task (let's say time series forecast), what would these inputs represent?</p>

<hr>

<p>EDIT:
I'm thinking about a seq2seq to make forecasts. The input would be a series of given length, and a series of external variables. The output would be the series shifted forward of n steps.</p>
","transformer"
"68641","Should weight distribution change more when fine-tuning transformers-based classifier?","2020-02-24 20:08:45","","1","284","<pytorch><transformer><huggingface><weight-initialization><histogram>","<p>I'm using pre-trained DistilBERT model from <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Huggingface</a> with custom classification head, which is almost the same as in the <a href=""https://github.com/huggingface/transformers/blob/fb560dcb075497f61880010245192e7e1fdbeca4/src/transformers/modeling_distilbert.py#L579"" rel=""nofollow noreferrer"">reference implementation</a>: </p>

<pre class=""lang-py prettyprint-override""><code>class PretrainedTransformer(nn.Module):
    def __init__(
        self, target_classes):
        super().__init__()
        base_model_output_shape=768
        self.base_model = DistilBertModel.from_pretrained(""distilbert-base-uncased"")
        self.classifier = nn.Sequential(
            nn.Linear(base_model_output_shape, out_features=base_model_output_shape),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(base_model_output_shape, out_features=target_classes),
        )

        for layer in self.classifier:
            if isinstance(layer, nn.Linear):
                layer.weight.data.normal_(mean=0.0, std=0.02)
                if layer.bias is not None:
                    layer.bias.data.zero_()

    def forward(self, input_, y=None):
        X, length, attention_mask = input_
        base_output = self.base_model(X, attention_mask=attention_mask)[0]
        base_model_last_layer = base_output[:, 0]
        cls = self.classifier(base_model_last_layer)
        return cls
</code></pre>

<p>During training, I use linear LR warmup schedule with max LR=<code>5-e5</code> and <em>cross entropy loss</em>.
In general, the model is able to learn on my dataset and reach high precision/recall metrics.</p>

<p><strong>My question is:</strong></p>

<p>Should weights distributions and biases in <em>classification</em> layers change more during training? It seems like the weights almost do not change at all, even when I do not initialize them as in the code (to mean=<code>0.0</code> and std=<code>0.02</code>). Is this an indication that something is wrong with my model or it's just because the layers I've added are redundant and model does not learn nothing new?</p>

<p>Take look at the image of weight from the tensorboard:
<a href=""https://i.sstatic.net/JRDQ7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JRDQ7.png"" alt=""weights of classification layers""></a></p>
","transformer"
"68553","Why does the transformer positional encoding use both sine and cosine?","2020-02-23 12:54:49","","11","7153","<machine-learning><nlp><transformer><attention-mechanism>","<p>In the transformer architecture they use positional encoding (explained in <a href=""https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model"">this answer</a> and I get how it is constructed. </p>

<p>I am wondering why it needs to use both sine and cosine though instead of just one or the other?</p>
","transformer"
"68220","How are Q, K, and V Vectors Trained in a Transformer Self-Attention?","2020-02-17 09:55:54","","9","10290","<machine-learning><nlp><sequence-to-sequence><transformer><attention-mechanism>","<p>I am new to transformers, so this may be a silly question, but I was reading about transformers and how they use attention, and it involves the usage of three special vectors. Most articles say that one will understand their purpose after reading about how they are used for attention. I believe I understand what they do, but I'm unsure about how they are created.</p>

<p>I'm aware that they come from the multiplication of the input vector by three corresponding weights, but I'm not sure how these weights are derived. Are they chosen at random and trained like a standard neural network, and if so how if there's no predefined attention data in the training corpus? </p>

<p>I'm very new to this, so I hope everything I'm saying makes sense. If I've got something completely wrong, please tell me! </p>
","transformer"
"68020","What is the feedforward network in a transformer trained on?","2020-02-13 14:09:33","68067","1","1893","<neural-network><nlp><autoencoder><transformer><attention-mechanism>","<p>After reading the 'Attention is all you need' article, I understand the general architecture of a transformer. However, it is unclear to me how the feed forward neural network learns. </p>

<p>What I learned about neural nets is that they learn based on a target variable, through back propagation according to a particular loss function. </p>

<p><a href=""https://i.sstatic.net/ofQsr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/ofQsr.png"" alt=""Feed forward neural net""></a></p>

<p>Looking at the architecture of a Transformer, it is unclear to me what the target variables are in these feed forward nets. Can someone explain this to me?</p>

<p><a href=""https://i.sstatic.net/BcfGr.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/BcfGr.png"" alt=""The transformer architecture""></a></p>
","transformer"
"67984","Pretrained Models for Keyword-Based Text Generation","2020-02-12 16:12:59","","1","73","<nlp><transformer><text-generation><gpt>","<p>I'm looking for an implementation that allows me to generate text based on a pre-trained model (e.g. GPT-2).</p>
<p>An example would be <a href=""https://github.com/minimaxir/gpt-2-keyword-generation"" rel=""nofollow noreferrer"">gpt-2-keyword-generation</a> (<a href=""https://minimaxir.com/apps/gpt2-reddit/"" rel=""nofollow noreferrer"">click here for demo</a>). As the author notes, there is</p>
<blockquote>
<p>[...] no explicit mathematical/theoetical basis behind the keywords
aside from the typical debiasing of the text [...]</p>
</blockquote>
<p>Hence my question: <strong>Are there more sophisticated ways of keyword-based text generation</strong> or at least any other <strong>alternatives</strong>?</p>
<p>Thank you</p>
","transformer"
"65671","Why do BERT classification do worse with longer sequence length?","2019-12-31 18:08:12","108967","6","1008","<deep-learning><bert><transformer><hyperparameter-tuning><hyperparameter>","<p>I've been experimenting using transformer networks like BERT for some simple classification tasks. My tasks are binary assignment, the datasets are relatively balanced, and the corpus are abstracts from <a href=""https://www.ncbi.nlm.nih.gov/pubmed/"" rel=""nofollow noreferrer"">PUBMED</a>. The median number of tokens from pre-processing is about 350 but I'm finding a strange result as I vary the sequence length. While using too few tokens hampers BERT in a predictable way, BERT doesn't do better with more tokens. It looks like the optimal number of tokens is about 128 and consistently performs worse as I give it more of the abstract.</p>

<p>What could be causing this, and how can I investigate it further?<a href=""https://i.sstatic.net/9b1Vi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9b1Vi.png"" alt=""enter image description here""></a></p>
","transformer"
"65619","How do I implement Dual-encoder model in Pytorch?","2019-12-30 10:04:03","","1","388","<transformer><encoder>","<p>I am trying to implement the paper titled Learning Cross-lingual Sentence Representations via a Multi-task Dual-Encoder Model.</p>

<p>Here the encoder and decoder share the same weights but I am unable to put it in code. Any links ?</p>
","transformer"
"65286","Why Decision Tree Classifier is not working with categorical value?","2019-12-22 18:54:00","65288","4","998","<python><decision-trees><categorical-data><feature-scaling><transformer>","<p>I am learning my way through this, so please be easy on me if you find any mistakes, I could really use a professional opinion here. Thx.</p>

<p>I am trying to model a Decision Tree Classifier as part of an ensemble (soft voting system). The problem is that I have the categorical features already converted to Integer values (i.e. if blood pressure is within the medium range, then blood pressure =2 in the dataset).</p>

<p>The categories for the feature variables are as follows</p>

<p>feature#1 (total cholesterol) = 1, 2, 3</p>

<p>feature#2 (Systolic Blood Pressure) = 1, 2, 3</p>

<p>feature#3 (diastolic Blood Pressure) = 1, 2, 3</p>

<p>feature#4 (smoking rate) = 1, 2, 3</p>

<p>The target/class variable (stroke) = 1, 2 (yes and no, respectively).</p>

<p>The problem is that when I draw the DT (please check the picture), I expect to get a splitting decision where a certain feature equals one of the above-mentioned values (i.e. 1, 2, 3). However, it is giving me other values like 1.5, 0.5, etc.,</p>

<p>I have included the code and a sample dataset for anyone who can help me out. btw, I need to keep the current dataset in the number format as I am using it in conjunction with other classifiers for the soft voting classifier.</p>

<p><a href=""https://www.dropbox.com/s/lj3kqbra2z5n7bj/FDALL_balanced8_convert.csv?dl=0"" rel=""nofollow noreferrer"">Here</a> is the dataset and <a href=""https://www.dropbox.com/s/s5zj7usgrr1kylo/DT.py?dl=0"" rel=""nofollow noreferrer"">this is the code</a> for reference, I am using Spyder with Python 3.7
<a href=""https://i.sstatic.net/fsqMf.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/fsqMf.jpg"" alt=""enter image description here""></a></p>
","transformer"
"65235","Measuring quality of answers from QnA systems","2019-12-21 15:21:19","","1","99","<bert><transformer><search-engine><question-answering>","<p>I am having a question answering system which is using Seq2Seq kind of architecture. Actually it is a transformer architecture. When a question is asked it gives startposition and endposition of answer along with their logits.<br>
The answer is formed by choosing the best logits span and final probability is calculated by summing the start and end logits.  </p>

<p>Now the problem is, I have multiple answer and many times the good answer is at 2nd or 3rd place (after sorting on the result of sum of start and end probability).  Is there any metric in search engine science using which I can rank the best answers?  </p>

<p>Followings have been tried:</p>

<ul>
<li>cosine similarity between question words and answers  -  This works many times but  fails when question semantic meaning is complex</li>
<li>TFIDF - gives good score but fails when there is synonym in answers rather than matching word.</li>
<li>gensim semantic similarity - fails badly.</li>
<li>BLUE score and new BERTF1Score also tried</li>
</ul>

<p>Few terms I heard of but I doubt if these work, like Mean Reciprocal Rank which I think gives search quality rather than answer quality and also the correct response is required to calculate MRR (Please correct if I am wrong). Or the PageRank which is not valid in my case as the answer semantic meaning is preferred in QnA rather than the document popularity.  </p>

<p>Kindly suggest other metrics which search engines generally use to rank the answers.</p>
","transformer"
"65067","Proper masking in the transformer model","2019-12-18 11:18:32","65070","9","7582","<nlp><word-embeddings><transformer>","<p>Concerning the transformer model, a mask is used to mask out attention scores (replace with 1e-9) prior to the matrix multiplication with the value tensor. Regarding the masking, I have 3 short questions and would appreciate if you could clarify those:</p>

<ol>
<li>Are the attention scores the only place (besides the loss) where masks are needed or should the input be masked out as well? </li>
</ol>

<p>I am asking because is see implementations where a linear layer for the query, key and values is with <code>bias=False</code> is used. </p>

<ol start=""2"">
<li><p>Is the reason for setting <code>bias=False</code> to have zeros preserved in the output of the layers or is there a different explanation?</p></li>
<li><p>Should a padding_idx be used when learning word embeddings in order to zero out the padded tokens? </p></li>
</ol>
","transformer"
"64659","the library 'transformers' works also with older version of Tensorflow?","2019-12-11 16:22:26","64688","0","158","<python><tensorflow><transformer>","<p>i am working with Tensorflow version 1.14 and i would like to use the bert embedding. 
In order to do so, i was thinking to use the transformers library( <a href=""https://pypi.org/project/transformers/"" rel=""nofollow noreferrer"">https://pypi.org/project/transformers/</a>)  but i am not sure if that will work with my tensorflow version.</p>
","transformer"
"64254","Weight matrices in transformers","2019-12-05 10:34:50","","2","535","<sequence-to-sequence><bert><machine-translation><transformer><attention-mechanism>","<p>I am trying to understand the transformer architecture.</p>

<p>I am aware that the encoder/decoder contains multiple stacked self attention layers. Further each layer contains multiple heads. For example take 8 heads.</p>

<p>Now for a particular layer we will have 8 <strong>different</strong> sets of (Wq, Wk, Wv), the weight matrices used to calculate the query, key and value.</p>

<p>Now what I want to know is whether these weight matrices are <strong>shared</strong> between the different layers i.e are the (Wq, Wk, Wv) matrices of head#1 in layer 1 same for head#1 of layers 2, 3, ....?</p>

<p>And if they are shared, doesn't it affect in parallelization?</p>
","transformer"
"64200","In Deep Learning, how many kinds of Attention exist? And what is the history of Attention models?","2019-12-04 11:47:44","","1","117","<machine-learning><deep-learning><rnn><transformer><attention-mechanism>","<p>How many definitions of attention are commonly employed for Deep Learning tasks?</p>

<p>That's what I've encountered up to now:</p>

<ul>
<li>Self-attention</li>
<li>Bahdanau</li>
<li>Luong</li>
<li>Multi-Head (used in Transformers)</li>
</ul>

<p>Could you provide formal explanations of each of these (and others in case the list is incomplete) and tips on when to prefer one to the other?</p>

<p>And what is the history of Attention models? How did they develop through time, and how did they improve previous formulations?</p>
","transformer"
"63997","Why seq2seq models are superior to simple LSTMs?","2019-11-29 14:24:55","","1","1991","<machine-learning><deep-learning><transformer>","<p>It is common knowledge in the field of Deep Learning that the most powerful Recurrent architecture is the sequence-to-sequence, or <strong>seq2seq</strong>, for pretty much any task (to time series forecasts, to machine translation, to text generation).</p>

<p>Why? What are the underlying <em>mathematical</em> reasons for which an LSTM encoder-decoder architecture would outperform more canonical RNNs? Is it in the generation of dense latent representations? Is it about the comparatively higher number of parameters? Any hint is appreciated.</p>
","transformer"
"63140","BERT for non-textual sequence data","2019-11-14 08:55:22","","2","724","<neural-network><categorical-data><embeddings><bert><transformer>","<p>I'm working on a deep learning solution for classifying sequence data that isn't raw text but rather entities (which have already been extracted from the text). I am currently using word2vec-style embeddings to feed the entities to a CNN, but I was wondering if a Transformer (à la BERT) would be a better alternative &amp; provide a better way of capturing the semantics of the entities involved. I can't seem to find any articles (let alone libraries) to apply sth like BERT to non-textual sequence data. Does anybody know any papers about this angle? I've thought about training a BERT model from scratch and treating the entities as if they were text. The issue with that though is that apparently BERT is slow when dealing with long sequences (sentences). In my data I often have sequences that have a length of 1000+  so I'm worried BERT won't cut it. Any help, insights or references are very much appreciated! Thanks</p>
","transformer"
"62862","Preprocessing for Text Classification in Transformer Models (BERT variants)","2019-11-08 06:28:48","","14","8598","<python><nlp><preprocessing><bert><transformer>","<p>This might be silly to ask, but I am wondering if one should carry out the conventional text preprocessing steps for training one of the transformer models?</p>
<p>I remember for training a Word2Vec or Glove, we needed to perform an extensive text cleaning like: tokenize, remove stopwords, remove punctuations, stemming or lemmatization and more. However, during last few days I have had a quick jump into transformer models (fascinated btw), and what I have noticed that most of these models have a built-in tokenizer (cool), but none of the demos, examples, or tutorials are performing any of the these text preprocessing steps. You may take <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""noreferrer"">fast-bert</a> for instance, there are no text preprocessing involved for the demos (maybe it is just a demo), but at <a href=""https://github.com/kaushaltrivedi/fast-bert#5-model-inference"" rel=""noreferrer"">inference</a> the whole sentences are passed without any cleaning:</p>
<pre><code>texts = ['I really love the Netflix original movies',
         'this movie is not worth watching']
predictions = learner.predict_batch(texts)
</code></pre>
<p>The same is true for the <a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">original transformer</a> by HuggingFace. Or many tutorials that I have looked at (take <a href=""https://github.com/kaushaltrivedi/bert-toxic-comments-multilabel/blob/master/toxic-bert-multilabel-classification.ipynb"" rel=""noreferrer"">this</a> or <a href=""https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d"" rel=""noreferrer"">another one</a>). I can imagine that depending on the task this might not be required, e.g. next work prediction or machine translation and more. More importantly I think this is part of the contextual-based approach that these models offer (that is the innovation so to say) that are meant to keep most of the text and we may obtain a minimum but still good representation of the each token (out of vocabulary word). Borrowed from <a href=""https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d"" rel=""noreferrer"">medium article</a> by HuggingFace:</p>
<blockquote>
<p><strong>Tokenisation</strong>
BERT-Base, uncased uses a vocabulary of 30,522 words. The
processes of tokenisation involves splitting the input text into list
of tokens that are available in the vocabulary. In order to deal with
the words not available in the vocabulary, BERT uses a technique
called BPE based WordPiece tokenisation. In this approach an out of
vocabulary word is progressively split into subwords and the word is
then represented by a group of subwords. Since the subwords are part
of the vocabulary, we have learned representations an context for
these subwords and the context of the word is simply the combination
of the context of the subwords.</p>
</blockquote>
<p>But does that hold true for tasks like multi-label text classification? In my use case the text is full of not useful stopwords, punctuation, characters and abbreviations and it is multi-label text classification as mentioned earlier. And in fact the prediction accuracy is not good (after a few rounds of training using <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""noreferrer"">fast-bert</a>). What do I miss here?</p>
","transformer"
"61954","When do you use FunctionTransformer instead of .apply()?","2019-10-19 09:06:26","61970","2","1670","<transformer>","<p>I'm watching a <a href=""https://www.youtube.com/watch?v=BFaadIqWlAg&amp;t=702s"" rel=""nofollow noreferrer"">PyData talk</a> from 2017 in which the speaker provides this example for how to use FunctionTransformer for sklearn.preprocessing</p>

<pre><code>from sklearn.preprocessing import FunctionTransformer
logger = FunctionTransformer(np.log1p)
X_log = logger.transform(X)
</code></pre>

<p>In other words, she's applying a function over the rows of a column.  I assumed this could be done more simply using .apply().  I feel that there must be something more to the reason why a data analyst would import FunctionTransformer.  Could someone help me understand what differentiates the .apply() method from FunctionTransformer?</p>
","transformer"
"61024","What is auxiliary loss in Character-level Transformer model?","2019-09-30 04:00:46","61056","3","886","<deep-learning><nlp><loss-function><transformer>","<p>I am reading <a href=""https://arxiv.org/abs/1808.04444"" rel=""nofollow noreferrer"">Character-Level Language Modeling with Deeper Self-Attention</a> from Rami Al-Rfou. In the second page, they had mentioned about Auxiliary Losses which can speed-up the model convergence and as an additional regularizer. They said they had 3 kinds of auxiliary losses:</p>

<ol>
<li>Auxiliary losses at intermediate sequence positions</li>
<li>Auxiliary losses from intermediate hidden representations</li>
<li>Auxiliary losses at target positions multiple steps.</li>
</ol>

<p>However, I cannot find any information or reference explaining auxiliary losses. I would like to know:</p>

<ul>
<li>What is auxiliary losses? Is it an <a href=""https://arxiv.org/abs/1803.00144"" rel=""nofollow noreferrer"">unsupervised prediction model using part of the data</a>?</li>
<li>How can I calculate the auxiliary losses?</li>
</ul>
","transformer"
"60258","NMT, What if we do not pass input for decoder?","2019-09-16 06:52:32","60259","1","306","<deep-learning><nlp><machine-translation><transformer>","<p>For transformer-based neural machine translation (NMT), take English-Chinese for example, we pass English for encoder and use decoder input(Chinese) attend to encoder output, then final output.</p>

<p>What if we do not pass input for decoder and consider it as a 'memory' model for translation.
Is it possible and what will happen?</p>

<p>It seems decoder could be removed and there only exist encoder.</p>

<p>Could I do translation task like text generation?
See:</p>

<p><a href=""https://github.com/salesforce/ctrl/blob/master/generation.py"" rel=""nofollow noreferrer"">https://github.com/salesforce/ctrl/blob/master/generation.py</a></p>

<p><a href=""https://einstein.ai/presentations/ctrl.pdf"" rel=""nofollow noreferrer"">https://einstein.ai/presentations/ctrl.pdf</a></p>
","transformer"
"58794","How does Byte Pair Encoding work on the byte sequence?","2019-09-06 13:51:02","","1","89","<nlp><transformer>","<p>I am reading a paper on OpenAI GPT-2, and in the paper the authors are mentioning that they have performed Byte Pair Encoding (BPE) on the byte sequence themselves, and I am not sure what they meant by that.</p>

<p>How is BPE on the byte sequence different from the regular BPE?</p>

<p>Could you give me some example?</p>

<p>Thank you,</p>
","transformer"
"58336","Does it make sense to use Transformer encoders on top of a pretrained Word2Vec embedding for a classification task?","2019-08-28 14:45:43","","2","76","<neural-network><classification><transformer>","<p>As the title says. I am dealing with a text classification task, but I do not have the resources to train a BERT word embedding from scratch. 
I was thinking of using an existing Word2Vec embedding and placing a stack of Transformer encoder layers on top of it, with a final Dense layer for the classification. 
From my limited understanding of the Attention paper, this seems like it should work, but my model ends up performing very poorly (predicting all 0). Before I start looking for errors in my code, is there a conceptual reason for this architecture to fail?</p>

<p>Edited to add: 
I am performing multilabel classification, where each label has two classes. The data I have is from legal contracts. Each sample is a paragraph, and the different labels correspond to the presence or absence of legal concepts. I have a Word2Vec model trained on legal text, and the embedding is not part of the model - I tokenize, normalize and vectorize as part of the generator.</p>
","transformer"
"57000","Bi-directionality in BERT model","2019-08-05 17:14:17","57019","4","634","<machine-learning><deep-learning><machine-learning-model><transformer>","<p>I am reading the paper <em>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em> that can be found <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">here</a>. </p>

<p>It looks to me that the crux of the paper is using masked inputs to achieve bidirectionally.</p>

<p>This is an excerpt from the Google AI blog <a href=""https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html"" rel=""nofollow noreferrer"">here</a> which states:</p>

<blockquote>
  <p>""However, it is not possible to train bidirectional models by simply
  conditioning each word on its previous and next words, since this
  would allow the word that’s being predicted to indirectly “see itself”
  in a multi-layer model.  To solve this problem, we use the
  straightforward technique of masking out some of the words in the
  input and then condition each word bidirectionally to predict the
  masked words.""</p>
</blockquote>

<p>Can someone please help me understand how does bidirectionally allow the words to see themselves and how masking solves this problem?</p>

<p>Thanks.</p>
","transformer"
"56145","What is Bit Per Character?","2019-07-22 10:11:13","","5","4168","<neural-network><metric><machine-translation><transformer>","<p>What is <code>Bits per Character (bpc)</code> metric which has been used to measure the model accuracy with reference to <code>text8</code> and <code>enwiki8</code> datasets. I encountered the term bpc in <code>transformer -XL paper</code> <a href=""https://arxiv.org/pdf/1901.02860.pdf"" rel=""noreferrer"">here</a>. How different is it from the <code>perplexity</code> as a metric?</p>
","transformer"
"55901","In a Transformer model, why does one sum positional encoding to the embedding rather than concatenate it?","2019-07-18 08:34:46","76291","39","11569","<nlp><encoding><transformer><attention-mechanism>","<p>While reviewing the Transformer architecture, I realized something I didn't expect, which is that :</p>

<ul>
<li>the positional encoding is summed to the word embeddings </li>
<li>rather than concatenated to it.</li>
</ul>

<blockquote>
  <p><a href=""https://i.sstatic.net/bFPI9.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/bFPI9.png"" alt=""positional encoding summed to word embedding""></a></p>
  
  <p><a href=""http://jalammar.github.io/images/t/transformer_positional_encoding_example.png"" rel=""noreferrer"">http://jalammar.github.io/images/t/transformer_positional_encoding_example.png</a></p>
</blockquote>

<p>Based on the graphs I have seen wrt what the encoding looks like, that means that :</p>

<ul>
<li>the first few bits of the embedding are completely unusable by the network because the position encoding will distort them a lot, </li>
<li>while there is also a large amount of positions in the embedding that are only slightly affected by the positional encoding (when you move further towards the end).</li>
</ul>

<blockquote>
  <p><a href=""https://i.sstatic.net/XLT9V.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/XLT9V.png"" alt=""graph shows positional encoding affects firsts logits a lot, last logits hardly not""></a></p>
  
  <p><a href=""https://www.tensorflow.org/beta/tutorials/text/transformer_files/output_1kLCla68EloE_1.png"" rel=""noreferrer"">https://www.tensorflow.org/beta/tutorials/text/transformer_files/output_1kLCla68EloE_1.png</a></p>
</blockquote>

<p>So, why not instead have smaller word embeddings (reduce memory usage) and a smaller positional encoding retaining only the most important bits of the encoding, and instead of summing the positional encoding of words keep it concatenated to word embeddings?</p>
","transformer"
"54764","Transformer for neural machine translation: is it possible to predict each word in the target sentence in a single forward pass?","2019-06-30 02:44:25","","0","496","<nlp><training><pytorch><transformer>","<p>I want to replicate the Transformer from the paper <a href=""https://arxiv.org/abs/1706.03762"" rel=""nofollow noreferrer"">Attention Is All You Need</a> in PyTorch. My question is about the decoder branch of the Transformer. If I understand correctly, given a sentence in the source language and partial/incomplete translation in the target language, the Transformer is tasked with predicting the next token in the translation. For example:</p>

<p>English (source): <em>I love eating chocolate</em></p>

<p>Spanish (target): <em>Yo amo comer</em> ...</p>

<p>The next token in the translation should be <em>chocolate</em> (thus, the full translation would be <em>""Yo amo comer chocolate""</em>). So, in this example, the encoder would process the sentence <em>""I love eating chocolate""</em>, the decoder would process the partial translation <em>""Yo amo comer""</em> and the final output would be a softmax over the whole vocabulary (hopefully with <em>chocolate</em> being the token with the highest score).</p>

<p>The issue is, during training we want the Transformer to learn the full translation. This means that if the target sentence has length N, we want the transformer to predict the first word, the second word, the third word, and so on all the way up to the N-th word of the target sentence. One way to do that is by generating N training instances (one for each word of a target sentence of length-N). However this approach is computationally quite expensive, because instead of having a single training instance per source-target pair, we now have as many training instances as there are words in all the target sentences. So I was wondering if it's possible make all predictions for all words in the target sentence in a single forward pass. Is that possible?</p>
","transformer"
"53814","Pytorch: How to implement nested transformers: a character-level transformer for words and a word-level transformer for sentences?","2019-06-14 18:44:26","","2","308","<nlp><pytorch><transformer>","<p>I have a model in mind, but I'm having a hard time figuring out how to actually code it in Pytorch, especially when it comes to training the model (e.g. how to define mini-batches, etc.). First of all let me quickly introduce the context:</p>

<p>I'm working on VQA (visual question answering), in which the task is to answer questions about images, for example:</p>

<p><a href=""https://i.sstatic.net/UW8hE.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UW8hE.png"" alt=""enter image description here""></a></p>

<p>So, letting aside many details, I just want to focus here on the NLP aspect/branch of the model. In order to process the natural language question, I want to  use <em>character-level</em> embeddings (instead of traditional <em>word-level</em> embeddings) because they are more robust in the sense that they can easily accommodate for morphological variations in words (e.g. prefixes, suffixes, plurals, verb conjugations, hyphens, etc.). But at the same time I don't want to lose the inductive bias of reasoning at the word level. Therefore, I came up with the following design:</p>

<p><a href=""https://i.sstatic.net/Tl7bu.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Tl7bu.png"" alt=""enter image description here""></a></p>

<p>As you can see in the picture above, I want to use <a href=""http://papers.nips.cc/paper/7181-attention-is-all-you-need"" rel=""nofollow noreferrer"">transformers</a> (or even better, <a href=""https://arxiv.org/abs/1807.03819"" rel=""nofollow noreferrer"">universal transformers</a>), but with a little twist. I want to use 2 transformers: the first one will process each word characters in isolation (<em>character-level</em> transformer) to produce an initial word-level embedding for each word in the question. Once we have all these initial word-level embeddings, a second <em>word-level</em> transformer will refine these embeddings to enrich their representation with context, thus obtaining <em>context-aware word-level</em> embeddings.</p>

<p>The full model for the whole VQA task obviously is more complex, but I just want to focus here on this NLP part. So my question is basically about which Pytorch functions should I pay attention to when implementing this. For example, since I'll be using <em>character-level</em> embeddings I have to define a <em>character-level</em> embedding matrix, but then I have to perform lookups on this matrix to generate the inputs for the <em>character-level</em> transformer, repeat this for each word in the question and then feed all these vectors into the <em>word-level</em> transformer. Moreover, words in a single question can have different lengths, and questions within a single mini-batch can have different lengths too. So in my code I have to somehow account for different lengths at both word and question levels simultaneously within a single mini-batch (during training), and I've got no idea how to do that in Pytorch or whether it's even possible at all.</p>

<p>Any tips on how to go about implementing this in Pytorch that could lead me in the right direction will be deeply appreciated.</p>
","transformer"
"52744","Test dataset with categorical variable value not present in train dataset & transformer","2019-05-28 04:53:34","52756","4","351","<categorical-data><transformer>","<p>I want to replace values of a categorical variable ( named 'six' ) by the mean of my target variable ( named 'target' ). </p>

<p>I am fitting a transformer doing just that on a train dataset df and then transform the test dataset df2.</p>

<p>How do I deal with a value appearing solely in the test dataset ? </p>

<p>When fitted on the train dataset the transformer received no mean value of the target variable on that value.</p>

<p>For example :</p>

<pre><code>myarray = np.array([ [ 1 , 1 , 3 , 'v' , 'a' , 'x' , 0 ] , 
                   [ 2 , 2 , 2 , 'v' , 'b' , 'y' , 1 ] ,
                   [ 4 , 5 , 1 , 'w' , 'c' , 'z' , 1 ] ,
                   [ 2 , 1 , 9 , 'w' , 'c' , 'x' , 1 ] , 
                   [ 1 , 0 , 4 , 'w' , 'b' , 'y' , 1 ] ,
                   [ 2 , 2 , 3 , 'v' , 'b' , 'z' , 0 ] ] )

colnames = [ 'one', 'two', 'three' , 'four' , 'five' , 'six' , 'target' ]

df = pd.DataFrame( myarray , columns = colnames )

myarray2 = np.array([ [ 2 , 7 , 3 , 'v' , 'a' , 'x' , 0 ] , 
                      [ 9 , 2 , 2 , 'v' , 'a' , 'y' , 0 ] ,
                      [ 4 , 5 , 1 , 'w' , 'c' , 'k' , 1 ] ]  )

colnames2 = [ 'one', 'two', 'three' , 'four' , 'five' , 'six' , 'target' ]

df2 = pd.DataFrame( myarray2 , columns = colnames2 )
</code></pre>

<p>df is my train dataset, df2 my test dataset.</p>

<p>We can see variable 'six' has the k value not existing in the train dataset.</p>

<p>Next :</p>

<pre><code>df[ 'target' ] = df[ 'target' ].astype( 'float64' )
</code></pre>

<p>Next ( my homemade transformer ) :</p>

<pre><code>class Cat2TargetMean( BaseEstimator , TransformerMixin ) :

    def __init__( self , col2trans , tgt_col ) :
        self._col2trans = col2trans
        self._tgt_col = tgt_col

    def fit( self, X, y = None ) :
        self._dic_col_p = {}

        for col in self._col2trans :
            p = X.groupby( col ).mean()[ self._tgt_col ]
            self._dic_col_p.update( { col : p.to_dict() } )
        return self 

    def transform( self , X , y = None ) :
        for col , dic_p in self._dic_col_p.items() : 
            X.replace( { col : dic_p } , inplace = True )  
        return X
</code></pre>

<p>Then :</p>

<pre><code> tsf = Cat2TargetMean( [ 'four' , 'five' , 'six' ] , 'target' )

 tsf.fit( df )

 tsf.transform( df )

 tsf.transform( df2 )
</code></pre>

<p>Result :</p>

<pre><code>    one two three   four    five    six target
    0   2.0 7.0 3.0 0.333333    0.0 0.5 0.0
    1   9.0 2.0 2.0 0.333333    0.0 1   0.0
    2   4.0 5.0 1.0 1.000000    1.0 k   1.0
</code></pre>

<p>'k' value of column 'six' has not been transformed.</p>
","transformer"
"52118","Pyspark Pipeline Custom Transformer","2019-05-17 13:02:53","","3","1013","<pyspark><transformer>","<p>I'm having some trouble understanding the creation of custom transformers for Pyspark pipelines.</p>

<p>I am writing a custom transformer that will take the dataframe column Company and remove stray commas:</p>

<pre><code>from pyspark.sql.functions import *

class DFCommaDropper(Transformer):

    def__init__(self, *args, **kwargs):
        self.name = CommaDropper

    def transform(self,df):
        df = df.withColumn('Company', regexp_replace('Company',',','')
        return df
</code></pre>

<p>The above code is obviously wrong. I'm unsure what/how to initialize this and then how to use the initialized class instance in the transform function.</p>

<p>Thanks in advance for your help.</p>
","transformer"
"51785","what is the first input to the decoder in a transformer model?","2019-05-11 08:36:07","51798","12","15292","<nlp><sequence><bert><transformer>","<p><a href=""https://i.sstatic.net/SPNEP.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/SPNEP.png"" alt=""from https://jalammar.github.io/illustrated-transformer/""></a></p>

<p>The image is from url: <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">Jay Alammar on transformers</a></p>

<p>K_encdec and V_encdec are calculated in a matrix multiplication with the encoder outputs and sent to the encoder-decoder attention layer of each decoder layer in the decoder.<br>
The previous output is the input to the decoder from step 2 but what is the input to the decoder in step 1?  Just the K_encdec and V_encdec or is it necessary to prompt the decoder by inputting the vectorized output (from the encoder) for the first word?  </p>
","transformer"
"51522","What is the use of [SEP] in paper BERT?","2019-05-07 04:53:18","","18","25003","<machine-learning><nlp><transformer><bert>","<p>I know that [CLS] means the start of a sentence and [SEP] makes BERT know the second sentence has begun. </p>

<p>However, I have a question.</p>

<p>If I have 2 sentences, which are s1 and s2, and our fine-tuning task is the same. </p>

<p>In one way, I add special tokens and the input looks like [CLS]+s1+[SEP] + s2 + [SEP]. </p>

<p>In another, I make the input look like [CLS] + s1 + s2 + [SEP]. </p>

<p>When I input them to BERT respectively, what is the difference between them? </p>

<ul>
<li><p>Will the s1 in second one integrate more information from s2 than the s1 in first one does?</p></li>
<li><p>Will the token embeddings change a lot between the 2 methods?</p></li>
</ul>

<p>Thanks for any help!</p>
","transformer"
"51444","Problem trying to build my own sklean transformer","2019-05-05 17:39:44","51454","3","112","<scikit-learn><transformer>","<p>I build the following sklearn transformer :</p>

<pre><code>class Cat2Rat( BaseEstimator , TransformerMixin ) :
   def __init__( self , col2trans ) :
      self._col2trans = col2trans

   def fit( self, X, y = None ):
      return self 

   def transform( self , X , y = None ) :
      for col in self._col2trans : 
         p = X[ col ].value_counts() / X.shape[0]
         dic = dict( [ ( i , p[i] ) for i in ( X[ col ].value_counts() ).index ] )
         X.replace( { col : dic } , inplace = True )
      return X
</code></pre>

<p>This transformer is replacing catagorical values by their rates.</p>

<p>For example :</p>

<pre><code>myarray = np.array([ [ 1 , 1 , 3 , 'v' , 0 ] , 
                 [ 2 , 2 , 2 , 'v' , 1 ] ,
                 [ 4 , 5 , 1 , 'w' , 1 ] ,
                 [ 2 , 1 , 9 , 'w' , 1 ] , 
                 [ 1 , 0 , 4 , 'w' , 1 ] ] )

colnames = [ 'one', 'two', 'three' , 'four' , 'target' ]

df = pd.DataFrame( myarray , columns = colnames )
</code></pre>

<p>Value 'v' ( 'w' ) for column 'four' is replaced by 2/5 ( 3/5 ).</p>

<p>My purpose is to fit the transformer on df and apply it to another dataframe df2 :</p>

<pre><code>myarray2 = np.array([ [ 2 , 7 , 3 , 'v' , 0 ] , 
                    [ 9 , 2 , 2 , 'v' , 0 ] ,
                    [ 4 , 5 , 1 , 'w' , 1 ] ]  )

colnames2 = [ 'one', 'two', 'three' , 'four' , 'target' ]

df2 = pd.DataFrame( myarray2 , columns = colnames2 )
</code></pre>

<p>I am doing this that way :</p>

<pre><code># Transformer instance
trsf = Cat2Rat( [ 'four' ] )

# Fitting
trsf.fit( df )

# Then applying
trsf.transform( df2 )
</code></pre>

<p>But the rates are those of column 'four' values of df2 not df ( on which the transformer was fitted ).</p>

<p>I must have been missing something on the way to properly build such a transformer. </p>

<p>Could someone give some clue on how to fix the transformer so that it gives proper result?</p>

<p>Thanks.</p>
","transformer"
"51065","What is the positional encoding in the transformer model?","2019-04-28 14:43:17","90038","106","115861","<nlp><encoding><attention-mechanism><transformer>","<p>I'm trying to read and understand the paper <a href=""https://arxiv.org/abs/1706.03762"" rel=""noreferrer"">Attention is all you need</a> and in it, there is a picture:</p>

<p><a href=""https://i.sstatic.net/BpxYv.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/BpxYv.png"" alt=""enter image description here""></a></p>

<p>I don't know what <strong>positional encoding</strong> is. by listening to some youtube videos I've found out that it is an embedding having both meaning and position of a word in it and has something to do with <span class=""math-container"">$sin(x)$</span> or <span class=""math-container"">$cos(x)$</span></p>

<p>but I couldn't understand what exactly it is and how exactly it is doing that. so I'm here for some help. thanks in advance.</p>
","transformer"
"48005","Which is better: GPT or RelGAN for text generation?","2019-03-26 08:39:57","48006","1","1566","<deep-learning><gan><nlp><text-generation><transformer>","<p>Based on my understanding, gpt or gpt-2 are using language model loss to train and generate text, which do not contains GAN.</p>

<p>So which is better: GPT vs RelGAN/LeakGAN/SeqGAN/TextGAN </p>

<p>I am so confused about this question.</p>
","transformer"
"47817","How to prepare the data for text generation task","2019-03-23 00:43:54","","2","56","<deep-learning><nlp><language-model><text-generation><transformer>","<p>First, I'm not sure whether the model contains the encoder during training.</p>

<p>EOS means end-of-sentence. Encoder and decoder are part of transformer network.</p>

<p>If without-encoder, training time:</p>

<pre><code>target: [E, F, G, H, EOS]
decoder input: [0, E, F, G, H]
</code></pre>

<p>If without-encoder, testing time:</p>

<pre><code>decoder input: [0]
</code></pre>

<p>If with encoder, training time:</p>

<pre><code>encoder input: [A, B, C, B]
target: [E, F, G, H, EOS]
decoder input: [0, E, F, G, H]
</code></pre>

<p>If with-encoder, testing time:</p>

<pre><code>encoder input: [A, B, C, D]
decoder input: [0]
</code></pre>

<p>Am I exact right?</p>
","transformer"
"47773","The principle of LM deep model","2019-03-22 09:35:50","47780","1","65","<neural-network><deep-learning><nlp><language-model><transformer>","<p>Language model(LM) is the task of predicting the next word.</p>

<p>Does the deep model need the encoder? From the ptb code of tensor2tensor, I find the deep model do not contains the encoder.</p>

<p>Or both with-encoder and without-encoder can do the LM task?</p>
","transformer"
"47406","Incrementally Train BERT with minimum QnA records - to get improved results","2019-03-16 10:30:21","","2","1886","<machine-learning><training><transformer><bert>","<p>We are using Google BERT for Question and Answering. We have fine tuned BERT with SQUAD QnA release train data set (<a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a> , <a href=""https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json"" rel=""nofollow noreferrer"">https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json</a>)   </p>

<p>It generated new checkpoints and BERT is giving good answers for most of questions we asked on our text documents. However, there are some questions which it is answering wrong, so we are trying to further fine tune with our Question and known answer on our text document. We further trained based on last generated checkpoint and got new checkpoint.   </p>

<p><strong>With new checkpoint when we are asking the same question</strong>, the answer did not got corrected! Previously BERT was giving wrong answer with 99% confidence and now also <strong>giving same wrong answer with 95% confidence</strong>.  </p>

<p>Can someone suggest, if they have same or similar experience, and suggest please.<br>
Following are questions in BERT github Issues, and are unanswered for quite some time:  </p>

<ul>
<li>BERT accuracy reduced after providing custom training..The answer is
also not correct : <a href=""https://github.com/google-research/bert/issues/492"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/issues/492</a></li>
<li>Unable to incrementally train BERT with custom training: <a href=""https://github.com/google-research/bert/issues/482"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/issues/482</a></li>
<li>Little training has no impact:    <a href=""https://github.com/google-research/bert/issues/481"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/issues/481</a>  </li>
<li>Custom Domain Training: <a href=""https://github.com/google-research/bert/issues/498"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/issues/498</a></li>
</ul>
","transformer"
"47397","How Transformer is Bidirectional - Machine Learning","2019-03-16 07:12:36","","3","1721","<machine-learning><transformer><bert>","<p>Asking question in datascience forum, as this forum seems well suited for data science related questions: <a href=""https://stackoverflow.com/questions/55158554/how-transformer-is-bidirectional-machine-learning/55158766?noredirect=1#comment97066160_55158766"">https://stackoverflow.com/questions/55158554/how-transformer-is-bidirectional-machine-learning/55158766?noredirect=1#comment97066160_55158766</a></p>

<p>I am coming from Google BERT context (Bidirectional Encoder representations from Transformers). I have gone through architecture and codes. People say this is <strong>bidirectional by nature</strong>. To make it unidirectional attention some mask is to be applied.</p>

<p>Basically a transformer takes key, values and queries as input; uses encoder decoder architecture; and applies attention to these keys, queries and values. What I understood is we need to pass tokens explicitly rather than transformer understanding this by nature.</p>

<p>Can someone please explain <strong>what makes transformer bidirectional by nature</strong></p>

<p>Answer received so far:<br>
1. People confirmed that Transformer has Bidirectional nature, rather than an external code making it bidirectional.<br>
2. 
My doubt: We are passing Q K V embeddings to transformer, to which it applies N layers of self attention using ScaledDotMatrix attention. Same thing can be done by unidirection approach as well. May I know what part I am missing in my understanding. If someone can point to code where it is getting bidirectional, it would be a great help.</p>
","transformer"
"46377","Can BERT do the next-word-predict task?","2019-02-28 08:37:42","46382","21","11856","<neural-network><deep-learning><attention-mechanism><transformer><bert>","<p>As BERT is bidirectional (uses bi-directional transformer), is it possible to use it for the next-word-predict task? If yes, what needs to be tweaked?</p>
","transformer"
"46171","What is the reason for the speedup of transformer-xl?","2019-02-25 02:24:10","46291","0","383","<deep-learning><nlp><attention-mechanism><transformer>","<p>The inference speed of transformer-xl is faster than transformer.</p>

<p>Why?</p>

<p>If state reuse is the reason, so it is compared by two 32seq_len + state-reuse vs one 64seq_len + no-state-reuse?</p>
","transformer"