Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"129660","Hugging Face Real Time Object Detection Deployment","2024-07-10 11:13:49","","0","18","<yolo><huggingface><mlops><deployment><streamlit>","<p>I'm developing a live object detection app using Streamlit and the YOLOv8 model. The app runs smoothly with real-time inference on my local machine. However, when I deploy it to Hugging Face Spaces, only the first and last frames of the video appear on the client side.</p>
<p>Here is what I did</p>
<pre><code>import streamlit as st
import cv2
import tempfile
import os
from ultralytics import YOLO

model_pk = YOLO('models/model_pk.pt')

# Initialize session state
if 'started' not in st.session_state:
    st.session_state.started = False

st.title(&quot;Video Streamer&quot;)

if not st.session_state.started:
    uploaded_file = st.file_uploader(&quot;Upload a video file&quot;, type=[&quot;mp4&quot;, &quot;avi&quot;, &quot;mov&quot;, &quot;mkv&quot;])

    if uploaded_file is not None:
        # Save the uploaded video to a temporary file
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(uploaded_file.read())
        video_path = tfile.name

        # Stream the video
        st.video(video_path)

        if st.button(&quot;Start Streaming&quot;):
            st.session_state.video_path = video_path
            st.session_state.started = True
else:
    video_path = st.session_state.video_path
    # OpenCV video capture
    cap = cv2.VideoCapture(video_path)
    stframe = st.empty()

    while cap.isOpened():
        ret, frame = cap.read()
        frame = cv2.resize(frame, (640, 640))

        height, width, _ = frame.shape

        results_pk = model_pk(frame, conf=0.5)

        annotated_frame = results_pk[0].plot()

        annotated_frame = cv2.resize(annotated_frame, (height, width))

        if not ret:
            break

        stframe.image(annotated_frame, channels=&quot;BGR&quot;)

    cap.release()
    os.remove(video_path)
    st.session_state.started = False

</code></pre>
<p>I would appreciate any help in resolving this issue.</p>
","huggingface"
"129580","How can I make my Hugging Face fine-tuned model's config.json file reference a specific revision/commit from the original pretrained model?","2024-07-01 15:46:05","","0","18","<nlp><bert><huggingface><finetuning>","<p>I uploaded this model: <a href=""https://huggingface.co/pamessina/CXRFE"" rel=""nofollow noreferrer"">https://huggingface.co/pamessina/CXRFE</a>, which is a fine-tuned version of this model: <a href=""https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized</a></p>
<p>Unfortunately, CXR-BERT-specialized has this issue: <a href=""https://github.com/huggingface/transformers/issues/30412"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/30412</a></p>
<p>I fixed the issue with this pull request: <a href=""https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized/discussions/5"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized/discussions/5</a></p>
<p>However, when I save my fine-tuned model, the config.json file doesn't point to that specific pull request, pointing instead to the main branch by default, but the main branch of CXR-BERT-specialized has the aforementioned issue. As a consequence, when I try to use my model, it triggers the bug from the main branch of the underlying model, which shouldn't happen it were using the version from my pull request.</p>
<p>I've tried explicitly enforcing the revision I want like this:</p>
<pre><code>model = AutoModel.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized', revision=&quot;6cfc310817fb7d86762d888ced1e3709c57ac578&quot;, trust_remote_code=True)
...
model.save_pretrained(&quot;/home/pamessina/huggingface_models/CXRFE/&quot;)
</code></pre>
<p>But the config file that gets saved doesn't reference the desired revision:</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized&quot;,
  &quot;architectures&quot;: [
    &quot;CXRBertModel&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.25,
  &quot;auto_map&quot;: {
    &quot;AutoConfig&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized--configuration_cxrbert.CXRBertConfig&quot;,
    &quot;AutoModel&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized--modeling_cxrbert.CXRBertModel&quot;
  },
  &quot;classifier_dropout&quot;: null,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.25,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;cxr-bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 0,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;projection_size&quot;: 128,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.41.2&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 30522
}
</code></pre>
<p>And when I try to use my fine-tuned model from Hugging Face on Google Colab, I get this error:</p>
<p><a href=""https://i.sstatic.net/rUuXJTyk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rUuXJTyk.png"" alt=""enter image description here"" /></a></p>
<p>As  you can see, it's invoking the version associated with the commit id &quot;b59c09e51ab2410b24f4be214bbb49043fe63fc2&quot;, when instead it should be using the commit id &quot;6cfc310817fb7d86762d888ced1e3709c57ac578&quot; with my pull request that fixes the bug.</p>
<p>What can I do?</p>
<p>Thanks in advance.</p>
","huggingface"
"129572","Does it common for LM (hundreds million parameters) beat LLM (billion parameters) for binary classification task?","2024-07-01 01:16:10","","0","19","<python><neural-network><nlp><transformer><huggingface>","<p><strong>Preface</strong></p>
<p>I am trying to fine-tune the transformer-based model (LM and LLM). The LM that I used is DEBERTA, and the LLM is LLaMA 3. The task is to classify whether a text contains condescending language (binary classification).</p>
<p>I use <code>AutoModelForSequenceClassification</code>, which adds a classification layer to the model's top layer for both LM and LLM.</p>
<p><strong>Implementation</strong></p>
<ol>
<li><p>Dataset:</p>
<ul>
<li>Amount: it has about 10.000 texts with each text labeled <code>0</code> (for not condescending) and <code>1</code> (condescending). The proportion is <code>1:10</code> (condescending : not condescending).</li>
</ul>
</li>
<li><p>Parameter</p>
</li>
</ol>
<div class=""s-table-container""><table class=""s-table"">
<thead>
<tr>
<th>Parameter</th>
<th>LM</th>
<th>LLM</th>
</tr>
</thead>
<tbody>
<tr>
<td>Batch size</td>
<td>32</td>
<td>16 (per_device_train_batch_size = 4,    gradient_accumulation_steps = 4)</td>
</tr>
<tr>
<td>Epoch / steps</td>
<td>2 epoch</td>
<td>1000 steps (20% used as validation set)</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>linear (2e-5)</td>
<td>constant (2e-5)</td>
</tr>
<tr>
<td>Optimizer</td>
<td>AdamW (lr = 2e-5, eps = 1e-8)</td>
<td>paged_adamw_32bit</td>
</tr>
<tr>
<td>Fine-tuning</td>
<td>Full fine-tuning</td>
<td>LoRA (rank=32, dropout=0.5, alpha=8) with 8-bit quantization</td>
</tr>
<tr>
<td>Learning Rate</td>
<td>linear (2e-5)</td>
<td>constant (2e-5)</td>
</tr>
<tr>
<td>Precision</td>
<td>0,659</td>
<td>0,836</td>
</tr>
<tr>
<td>Recall</td>
<td>0,47</td>
<td>0,091</td>
</tr>
<tr>
<td>F1-score</td>
<td>0,549</td>
<td>0,164</td>
</tr>
</tbody>
</table></div>
<p><strong>Question and Issue</strong></p>
<p>Here is the log of the training sample. The validation f1-score is always <code>&gt;0.6</code>. But the validation loss is stuck at <code>0.24</code>. It is one of the samples of fine-tuned LLM.</p>
<p><a href=""https://i.sstatic.net/cWtxnVpgm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/cWtxnVpgm.png"" alt=""enter image description here"" /></a></p>
<ol>
<li>Why does the test set f1-score only range from 0 - 0.2 for some parameter variation that I tuned when the f1-score for the validation set is always above 0.6, is it reasonable? why?</li>
<li>Is it common for LM to beat LLM for a particular task? If yes, what is the rationalization?</li>
</ol>
","huggingface"
"129390","NLP: how to handle bad tokenization","2024-06-12 03:50:46","","0","21","<machine-learning><nlp><huggingface>","<p>I get nonsense when trying to translate the following german sentence to swedish using google/madlad400-3b-mt:</p>
<blockquote>
<p>a. Nat√ºrliche Personen: BundID mit ELSTER-Zertifikat oder nPA/eID/eAT-Authentifizierung
b. Juristische Personen: Unternehmenskonto BUND mit ELSTER-Zertifik</p>
</blockquote>
<p>-&gt;</p>
<blockquote>
<p>. Personen mit Behinderung: BundesID mit ELSTER-Zertifikat oder nPA/eID/eAT-Authentifizierung c. Personen mit Behinderung: BundesID mit ELSTER-Zertifikat oder nPA db. Personen mit Behinderung: BundesID mit ELSTER-Zertifikat oder nPA/e</p>
</blockquote>
<p>Code:</p>
<pre><code>pipe = pipeline(&quot;translation&quot;, model=&quot;google/madlad400-3b-mt&quot;)
pipe('&lt;2sv&gt;'+input, max_length = n_words*5)

</code></pre>
<p>This is likely due to the abundance of abbreviations and special words.</p>
<p><em>Is there a per sentence metric I can use to measure bad tokenizations? A naive one would be to calulate the percentage of unknown tokens. In my case the problem seems to be that it falsely attends to abbreviations rather than unknown confusion.</em></p>
","huggingface"
"129353","attentions not returned from transformers ViT model when using output_attentions=True","2024-06-09 19:13:34","129355","0","52","<visualization><transformer><attention-mechanism><huggingface>","<p>I'm using <a href=""https://huggingface.co/transformers/v4.9.1/model_doc/vit.html"" rel=""nofollow noreferrer"">this code snippet</a> from the docs of HuggingFace ViT classification model - with one addition: I'm using the <code>output_attentions=True</code> parameter. Nevertheless, no attentions are returned.</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import ViTFeatureExtractor, ViTForImageClassification
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')
model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')

inputs = feature_extractor(images=image, return_tensors=&quot;pt&quot;)
outputs = model(**inputs, output_attentions=True)
logits = outputs.logits

# --&gt; this should print the attentions
print(output.attentions)

# model predicts one of the 1000 ImageNet classes
predicted_class_idx = logits.argmax(-1).item()
print(&quot;Predicted class:&quot;, model.config.id2label[predicted_class_idx])
</code></pre>
<p>The output of <code>print(output.attentions)</code> is:</p>
<pre><code>attentions=(None, None, None, None, None, None, None, None, None, None, None, None)
</code></pre>
<p>What am I doing wrong, and how can I get the attentions values?</p>
","huggingface"
"128968","Instruction LLM - extract data from text wrongly continues","2024-05-06 09:04:32","","0","34","<transformer><huggingface><llm>","<p>I'm trying to fine-tune open sourced LLMs, for now let's stick with Mistral-7b-instruct model.</p>
<p>My task is a follow: I have emails, that represents &quot;price requests&quot; for shipments sends by our clients.
In the emails, the clients tells us the pickup address, the shipper, consignee ETC.</p>
<p>My initial idea was to train different adapters, using DORA, each of them is trained on extracting a different entity from the email.</p>
<p>My dataset was created as follow: I have the email, and the annotation which is &quot;Based on the email, I've found this [ENTITY]: entity_here</p>
<p>I've created a System message, and and chat_template to create the dataset in a way Mistral will accept, using this chat_template:</p>
<pre><code>&quot;{%- for message in messages %}&quot;
  &quot;{%- if message['role'] == 'system' -%}&quot;
      &quot;{{- '&lt;s&gt;' + message['content'] -}}&quot;
  &quot;{%- else -%}&quot;
      &quot;{%- if message['role'] == 'user' -%}&quot;
          &quot;{{-'[INST] ' + message['content'].rstrip() + ' [/INST]'-}}&quot;
      &quot;{%- else -%}&quot;
          &quot;{{-'' + message['content'] + '&lt;/s&gt;' -}}&quot;
      &quot;{%- endif -%}&quot;
  &quot;{%- endif -%}&quot;
&quot;{%- endfor -%}&quot;
&quot;{%- if add_generation_prompt -%}&quot;
    &quot;{{-''-}}&quot;
&quot;{%- endif -%}&quot;
</code></pre>
<p>Now to the problem. The model seems to learn what it needs to extract, it generates decent answers, with the same format as the assistant it was trained by, the problem is that after it generates the answer, it keeps on generating additional texts regarding the email that are irrelevant to the task, E.G. &quot;please contact us in....&quot;</p>
<p>When I fine tune GPT3.5 for example for the same task, the model is able to extract exactly what I need for it, which suggests to me that I'm doing something wrong.</p>
<p>Does anyone have suggestions as to where did I go wrong?</p>
","huggingface"
"128904","Since LoRA parameters are randomly initialized, shouldn't that mean that initially breaks a models output?","2024-04-30 09:46:03","128907","2","52","<huggingface><finetuning>","<p>I have just tried using LoRA on Llama 3 8B and I found without doing any fine tuning it performed pretty well on my dataset. But then I realized that surely the LoRA parameters are randomly initialized right? So if that's the case, shouldn't that mean the model outputs are initially detrimented by the LoRA parameters? Since they're just adding random values to the regular parameters?</p>
<p>I also have a somewhat related question if you don't mind answering it as well. I keep reading that the alpha parameter in LoRA is the scaling factor e.g. y = Wx + alpha * L1L2, but I often see alpha values of 256 for example which seems way to large because that would be setting a ratio of 1 : 256 for the influence share between the regular parameters and the LoRA parameters on the output.</p>
","huggingface"
"128875","""No sentence-transformers model found with name"" on huggingface even though it exists","2024-04-28 10:00:02","","1","498","<nlp><word-embeddings><huggingface>","<p>I am trying to use <a href=""https://huggingface.co/infgrad/stella-base-en-v2"" rel=""nofollow noreferrer"">infgrad/stella-base-en-v2</a> on hugging to generate embeddings using langchain</p>
<ol>
<li>The model exists on the huggingface hub</li>
<li>The model is listed on the MTEB leaderboard</li>
<li>The model has sentence transformer tag on it</li>
</ol>
<p>Still when I try to use it like so:</p>
<pre><code>from langchain_community.embeddings import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name = &quot;infgrad/stella-base-en-v2&quot;)
</code></pre>
<p>I get the warning:</p>
<pre><code>No sentence-transformers model found with name infgrad/stella-base-en-v2. Creating a new one with MEAN pooling.
</code></pre>
<p>Why is that the case?</p>
","huggingface"
"128870","Not able to use huggingface inference API to get text embeddings","2024-04-27 19:31:55","","0","36","<huggingface>","<p>From the tutorials I am using the <a href=""https://python.langchain.com/docs/integrations/text_embedding/huggingfacehub/#hugging-face-inference-api"" rel=""nofollow noreferrer"">example</a> that is provided</p>
<pre><code>from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings

embeddings = HuggingFaceInferenceAPIEmbeddings(
    api_key=inference_api_key, model_name=&quot;sentence-transformers/all-MiniLM-l6-v2&quot;
)
text = &quot;this is a sample text&quot;
query_result = embeddings.embed_query(text)
query_result[:3]
</code></pre>
<p>But this results in the error:</p>
<pre><code>File ~/miniconda3/envs/rag/lib/python3.12/site-packages/langchain_community/embeddings/huggingface.py:373, in HuggingFaceInferenceAPIEmbeddings.embed_query(self, text)
    364 def embed_query(self, text: str) -&gt; List[float]:
    365     &quot;&quot;&quot;Compute query embeddings using a HuggingFace transformer model.
    366 
    367     Args:
   (...)
    371         Embeddings for the text.
    372     &quot;&quot;&quot;
--&gt; 373     return self.embed_documents([text])[0]

KeyError: 0

</code></pre>
<p>I can generate the text completions from LLMs hosted at the huggingface hub using the same inference_api_key so I guess maybe that is fine.(?)</p>
<p>Can you please help me understand how to use the inference API to get the embeddings from any embeddings model hosted on the huggingfacehub</p>
","huggingface"
"128857","How do I get model.generate() to omit the input sequence from the generation?","2024-04-26 15:06:02","128858","1","208","<nlp><pytorch><huggingface>","<p>I'm using Huggingface to do inference on llama-3-B. Here is my model:</p>
<pre><code>model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = &quot;unsloth/llama-3-8b-Instruct-bnb-4bit&quot;,
    max_seq_length = 2048,
    dtype = torch.float16,
    load_in_4bit = True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16,
    target_modules = [&quot;q_proj&quot;, &quot;k_proj&quot;, &quot;v_proj&quot;, &quot;o_proj&quot;, &quot;gate_proj&quot;, &quot;up_proj&quot;, &quot;down_proj&quot;],
    lora_alpha = 16,
    lora_dropout = 0,
    bias = &quot;none&quot;,
    use_gradient_checkpointing = True,
    random_state = 42,
    use_rslora = False,
    use_dora = False,
    loftq_config = None,
)
</code></pre>
<p>My issue is that when I prompt the model, it outputs &quot;prompt + generation&quot; rather than just outputing the generation. Here is where I prompt it:</p>
<pre><code>inputs = tokenizer(prompts[:2], return_tensors = &quot;pt&quot;, padding=True).to(&quot;cuda&quot;)
outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)
predictions = tokenizer.batch_decode(outputs, skip_special_tokens = True)
print(predictions[0])
</code></pre>
<p>E.g. if the input prompt i.e. <code>prompt[0]</code> is &quot;What color is grass?&quot; the output is like &quot;What color is grass? Green.&quot; but I want it to just be &quot;Green.&quot;.</p>
","huggingface"
"128703","Does Fine Tuning with Custom Label Build Upon the Capability of Zero Shot Classification or Does It Train from Scratch?","2024-04-15 09:30:38","","0","41","<nlp><bert><text-classification><huggingface><zero-shot-learning>","<p>The task is to classify email text bodies into exclusive categories like feedback, complaint etc. I have a labelled dataset available having about 350 samples.</p>
<p>I have tried the <code>facebook/bart-large-mnli</code> zero shot classification model where I passed the class names as possible label. It is already giving a decent performance.</p>
<p>Now, if I want to improve by using the existing labelled dataset and some model like <code>distilbert-base-uncased</code>, then will I lose the capability offered by the zero shot model altogether, and the new model will be trained entirely based on the labelled data?</p>
<p>I am afraid to go down the route because the number of labelled samples is so small, I feel it will fail to update the huge models having more weights than number of samples (we know the larger the model, the more samples you need).</p>
<p>So how do you guys address this concern, and how best to use the capabilities of the zero shot model on huggingface, while also using the labelled sample somehow?</p>
<p>I feel if I could just nudge the zero shot model a bit with the training samples, that would be the best possibility, but how to achieve it with huggingface?</p>
","huggingface"
"128658","Public Email Classification Dataset but not Spam vs Ham","2024-04-12 04:46:27","","1","31","<classification><nlp><dataset><huggingface>","<h5>Context</h5>
<p>Working to deliver a POC on automated email classification (in customer service context) to tag emails as related to <em>feedback</em>, <em>complain</em>, <em>lost and found</em> etc. The tags are not entirely exclusive, but the goal of the model is to assign a weight to each of these tags for a specific email. Like based on the email body, it is 20% related to feedback, 70% complaint and 10% lost and found.</p>
<p>Now, ideally, I would start with my client company's real email inbox. But it is not a mature data company (for systematic consumption of their inbox data), and there are privacy issues yet to be resolved.</p>
<h5>Question 1</h5>
<p>Is there any publicly available email/feedback related dataset (with plain texts, and other optional features like timestamp etc.) that can be used to show some quick POC? Most email data I see are obviously spam-ham type, not in the domain I want.</p>
<h5>Question 2</h5>
<p>Any idea which model (best if pre-trained with well documented interface, like from HuggingFace) will be suitable for the task, with some scope for fine-tuning? I am personally more a software engineer with ML experience, not an NLP expert, but picking up as I go.</p>
","huggingface"
"128471","Generate VTT file from speech to text","2024-03-27 19:35:30","128472","0","65","<audio-recognition><huggingface>","<p>I was able to generate text from an audio file using huggingface, using this code</p>
<pre><code>transcriber = pipeline(
  &quot;automatic-speech-recognition&quot;, 
  model=&quot;fsicoli/whisper-large-v3-pt-mzcv16&quot;
)

transcriber.model.config.forced_decoder_ids = (
  transcriber.tokenizer.get_decoder_prompt_ids(
    language=&quot;pt&quot;, 
    task=&quot;transcribe&quot;
  )
)

transcription = transcriber(&quot;half0.mp3&quot;)
</code></pre>
<p>Is there a way to output a vtt file? I get the transcription but not the timestamps</p>
","huggingface"
"128371","Training split generation - Extremely Slow","2024-03-19 20:25:38","","0","16","<machine-learning><dataset><huggingface>","<p>I am fine-tuning mbert on wikipedia dataset, loaded with Datasets (Hugging face)</p>
<pre><code>if data_args.dataset_lang:
    raw_datasets = load_dataset('wikipedia', language = data_args.dataset_lang, date = '20240201', cache_dir=model_args.cache_dir)
    if &quot;validation&quot; not in raw_datasets.keys():
        raw_datasets[&quot;validation&quot;] = load_dataset('wikipedia', language = data_args.dataset_lang,
            date = '20240201',
            split=f&quot;train[:{data_args.validation_split_percentage}%]&quot;,
            cache_dir=model_args.cache_dir,
            trust_remote_code=True, 
        )

        raw_datasets[&quot;train&quot;] = load_dataset('wikipedia', language = data_args.dataset_lang,
            date = '20240201',
            split=f&quot;train[{data_args.validation_split_percentage}%:]&quot;,
            cache_dir=model_args.cache_dir,
            trust_remote_code=True, 
        )
</code></pre>
<p>This is the bash script submitted to Slurm (irrelevant lines removed):</p>
<pre><code>#!/bin/bash
#SBATCH -o .../examples/language-modeling/slogs/sl_ka1_%A.out
#SBATCH -e .../examples/language-modeling//slogs/sl_ka1_%A.out
#SBATCH -N 1      # nodes requested
#SBATCH -n 1      # tasks requested
#SBATCH --gres=gpu:8  # use 1 GPU
#SBATCH --mem=60000  # memory in Mb
#SBATCH --partition=PGR-Standard
#SBATCH -t 24:00:00  # time requested in hour:minute:seconds
#SBATCH --cpus-per-task=16  # number of cpus to use - there are 32 on each node

torchrun --nproc_per_node 8 run_mlm.py \
--model_name_or_path bert-base-multilingual-cased \
--cache_dir **<span class=""math-container"">${CACHE_HOME}**2\
--dataset_lang $</span>{LANG} \
--dataset_name Wikipedia \
--output_dir ${OUTPUT_DIR} \
--do_train \
--do_eval \
--per_device_train_batch_size 4 \
--per_device_eval_batch_size 4 \
--gradient_accumulation_steps 2 \
--max_seq_length 256 \
--overwrite_output_dir \
--ft_params_num 7667712 \
--evaluation_strategy steps \
--eval_steps 1000 \
--dataloader_num_workers 32 \
--preprocessing_num_workers 32 \
--validation_split_percentage 5 \
--load_best_model_at_end \
--save_total_limit 2
</code></pre>
<p>Here is the report of the speed, <strong>very slow</strong>.</p>
<pre><code>    Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14.1k/14.1k [00:00&lt;00:00, 14.0MB/s]
Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 205M/205M [00:44&lt;00:00, 4.59MB/s] 
Generating train split: 0 examples [00:00, ? examples/s]Extracting content from /.../language-modeling/cache_directory22/downloads/f797c17d35d578a4c1a3f251847095789ec04ae453f10623aeb8366ff4797a07
Generating train split: 170787 examples [17:57, 158.45 examples/s]
</code></pre>
<p>Thank you all in advance!</p>
","huggingface"
"128346","Will hypermeters tuned on sampled dataset work for the whole dataset?","2024-03-18 12:53:15","","0","9","<nlp><bert><text-classification><hyperparameter><huggingface>","<p>I'm doing multi-label classification on text data using BERT model. Since the dataset is huge, around  50 thousand rows, I was thinking to use stratify sampling on dataset to reduce it to around 2-4 thousand to hyperparameter tune on.
I'm confused between trading off number of trails with size of sample set. Example: Would training 3000 rows with 5 trials will be better than training 1500 rows with 10 trials?
Moreover, thinking if I should drop epoch from tuning and focus on learning rate and weight decay.</p>
","huggingface"
"128345","dimensions conflict when trying to use a pretrained wav2vec2-xls-r-300m model","2024-03-18 12:07:10","","0","8","<deep-learning><huggingface>","<p>i am trying to use a pretrained model facebook/wav2vec2-xls-r-300m and fine tune it for audio classification and more specifically emotion recognition. i am using an audio labeled dataset ( 12 labels )</p>
<p>model conf:</p>
<p>MAX_DURATION = 5
SAMPLING_RATE = 16000
BATCH_SIZE = 32<br />
NUM_CLASSES = 12<br />
HIDDEN_DIM = 768<br />
MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE  # Maximum length of the input audio file.</p>
<p>MAX_FRAMES = 49
MAX_EPOCHS = 2</p>
<p>MODEL_CHECKPOINT = &quot;facebook/wav2vec2-xls-r-300m&quot;  # Name of pretrained model from Hugging Face Model</p>
<pre><code>
from transformers import Wav2Vec2FeatureExtractor

config = {
    &quot;do_normalize&quot;: True,
    &quot;feature_extractor_type&quot;: &quot;Wav2Vec2FeatureExtractor&quot;,
    &quot;feature_size&quot;: 1,
    &quot;padding_side&quot;: &quot;right&quot;,
    &quot;padding_value&quot;: 0,
    &quot;return_attention_mask&quot;: True,
    &quot;sampling_rate&quot;: 16000
}


# Initialize the feature extractor
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_CHECKPOINT, **config)
 
# Define your preprocess function
def preprocess_function(examples):
    audio_arrays = [x[&quot;array&quot;] for x in examples[&quot;audio&quot;]]
    inputs = feature_extractor(
        audio_arrays,
        max_length=MAX_SEQ_LENGTH,
        truncation=True,
        padding=True,
    )
    return inputs
classification head :


from transformers import TFWav2Vec2Model


def mean_pool(hidden_states, feature_lengths):
    attenion_mask = tf.sequence_mask(
        feature_lengths, maxlen=MAX_FRAMES, dtype=tf.dtypes.int64
    )
    padding_mask = tf.cast(
        tf.reverse(tf.cumsum(tf.reverse(attenion_mask, [-1]), -1), [-1]),
        dtype=tf.dtypes.bool,
    )
    hidden_states = tf.where(
        tf.broadcast_to(
            tf.expand_dims(~padding_mask, -1), (BATCH_SIZE, MAX_FRAMES, HIDDEN_DIM)
        ),
        0.0,
        hidden_states,
    )
    pooled_state = tf.math.reduce_sum(hidden_states, axis=1) / tf.reshape(
        tf.math.reduce_sum(tf.cast(padding_mask, dtype=tf.dtypes.float32), axis=1),
        [-1, 1],
    )
    return pooled_state


class TFWav2Vec2ForAudioClassification(layers.Layer):
    &quot;&quot;&quot;Combines the encoder and decoder into an end-to-end model for training.&quot;&quot;&quot;

    def __init__(self, model_checkpoint, num_classes):
        super().__init__()
        # Instantiate the Wav2Vec 2.0 model without the Classification-Head
        self.wav2vec2 = TFWav2Vec2Model.from_pretrained(
            model_checkpoint, apply_spec_augment=False, from_pt=True
        )
        self.pooling = layers.GlobalAveragePooling1D()
        # Drop-out layer before the final Classification-Head
        self.intermediate_layer_dropout = layers.Dropout(0.5)
        # Classification-Head
        self.final_layer = layers.Dense(num_classes, activation=&quot;softmax&quot;)

    def call(self, inputs):
        # We take only the first output in the returned dictionary corresponding to the
        # output of the last layer of Wav2vec 2.0
        hidden_states = self.wav2vec2(inputs[&quot;input_values&quot;])[0]

        # If attention mask does exist then mean-pool only un-masked output frames
        if tf.is_tensor(inputs[&quot;attention_mask&quot;]):
            # Get the length of each audio input by summing up the attention_mask
            # (attention_mask = (BATCH_SIZE x MAX_SEQ_LENGTH) ‚àà {1,0})
            audio_lengths = tf.cumsum(inputs[&quot;attention_mask&quot;], -1)[:, -1]
            # Get the number of Wav2Vec 2.0 output frames for each corresponding audio input
            # length
            feature_lengths = self.wav2vec2.wav2vec2._get_feat_extract_output_lengths(
                audio_lengths
            )
            pooled_state = mean_pool(hidden_states, feature_lengths)
        # If attention mask does not exist then mean-pool only all output frames
        else:
            pooled_state = self.pooling(hidden_states)

        intermediate_state = self.intermediate_layer_dropout(pooled_state)
        final_state = self.final_layer(intermediate_state)

        return final_state

**model**

def build_model():
    # Model's input
    inputs = {
        &quot;input_values&quot;: tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=&quot;float32&quot;),
        &quot;attention_mask&quot;: tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=&quot;int32&quot;),
    }
    # Instantiate the Wav2Vec 2.0 model with Classification-Head using the desired
    # pre-trained checkpoint
    wav2vec2_model = TFWav2Vec2ForAudioClassification(MODEL_CHECKPOINT, NUM_CLASSES)(
        inputs
    )
    # Model
    model = tf.keras.Model(inputs, wav2vec2_model)
    # Loss
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)
    # Optimizer
    optimizer = keras.optimizers.Adam(learning_rate=1e-5)
    # Compile and return
    model.compile(loss=loss, optimizer=optimizer, metrics=[&quot;accuracy&quot;])
    return model


model = build_model()

</code></pre>
<p>i got this error</p>
<pre><code>ValueError                                Traceback (most recent call last)
&lt;ipython-input-32-b7f377675faf&gt; in &lt;cell line: 23&gt;()
     21 
     22 
---&gt; 23 model = build_model()

4 frames
/tmp/__autograph_generated_filejjrdimiv.py in tf__mean_pool(hidden_states, feature_lengths)
     10                 attenion_mask = ag__.converted_call(ag__.ld(tf).sequence_mask, (ag__.ld(feature_lengths),), dict(maxlen=ag__.ld(MAX_FRAMES), dtype=ag__.ld(tf).dtypes.int64), fscope)
     11                 padding_mask = ag__.converted_call(ag__.ld(tf).cast, (ag__.converted_call(ag__.ld(tf).reverse, (ag__.converted_call(ag__.ld(tf).cumsum, (ag__.converted_call(ag__.ld(tf).reverse, (ag__.ld(attenion_mask), [-1]), None, fscope), -1), None, fscope), [-1]), None, fscope),), dict(dtype=ag__.ld(tf).dtypes.bool), fscope...
---&gt; 12                 hidden_states = ag__.converted_call(ag__.ld(tf).where, (ag__.converted_call(ag__.ld(tf).broadcast_to, (ag__.converted_call(ag__.ld(tf).expand_dims, (~ag__.ld(padding_mask), -1), None, fscope), (ag__.ld(BATCH_SIZE), ag__.ld(MAX_FRAMES), ag__.ld(HIDDEN_DIM))), None, fscope), 0.0, ag__.ld(hidden_states)), None, fscope)
     13                 pooled_state = ag__.converted_call(ag__.ld(tf).math.reduce_sum, (ag__.ld(hidden_states),), dict(axis=1), fscope) / ag__.converted_call(ag__.ld(tf).reshape, (ag__.converted_call(ag__.ld(tf).math.reduce_sum, (ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(padding_mask),), dict(dtype=ag__.ld(tf).dtypes.float32), fscope),), dict(axis=0...
     14                 try:

ValueError: Exception encountered when calling layer &quot;tf_wav2_vec2_for_audio_classification&quot; (type TFWav2Vec2ForAudioClassification).

in user code:

    File &quot;&lt;ipython-input-31-474bb7233250&gt;&quot;, line 56, in call  *
        pooled_state = mean_pool(hidden_states, feature_lengths)
    File &quot;&lt;ipython-input-31-474bb7233250&gt;&quot;, line 12, in mean_pool  *
        hidden_states = tf.where(

    ValueError: Dimensions must be equal, but are 49 and 249 for '{{node tf_wav2_vec2_for_audio_classification/SelectV2}} = SelectV2[T=DT_FLOAT](tf_wav2_vec2_for_audio_classification/BroadcastTo, tf_wav2_vec2_for_audio_classification/SelectV2/t, tf_wav2_vec2_for_audio_classification/tf_wav2_vec2_model/wav2vec2/encoder/layer_norm/batchnorm/add_1)' with input shapes: [32,49,768], [], [?,249,1024].


Call arguments received by layer &quot;tf_wav2_vec2_for_audio_classification&quot; (type TFWav2Vec2ForAudioClassification):
  ‚Ä¢ inputs={'input_values': 'tf.Tensor(shape=(None, 80000), dtype=float32)', 'attention_mask': 'tf.Tensor(shape=(None, 80000), dtype=int32)'}


</code></pre>
<p>i am using the EdwardLin2023/ASVP_ESD from hugging face , where i found this exact tutorial on how to finetune a facebook/wav2vec2-base on audio classification and since the model i am using facebook/wav2vec2-xls-r-300m is based on wav2vec2 i didnt change any parameters</p>
<p>heres the config of the model i am using as instructed in huggingface</p>
<pre><code>{
  &quot;do_normalize&quot;: true,
  &quot;feature_extractor_type&quot;: &quot;Wav2Vec2FeatureExtractor&quot;,
  &quot;feature_size&quot;: 1,
  &quot;padding_side&quot;: &quot;right&quot;,
  &quot;padding_value&quot;: 0,
  &quot;return_attention_mask&quot;: true,
  &quot;sampling_rate&quot;: 16000
}

{
  &quot;activation_dropout&quot;: 0.0,
  &quot;apply_spec_augment&quot;: true,
  &quot;architectures&quot;: [
    &quot;Wav2Vec2ForPreTraining&quot;
  ],
  &quot;attention_dropout&quot;: 0.1,
  &quot;bos_token_id&quot;: 1,
  &quot;codevector_dim&quot;: 768,
  &quot;contrastive_logits_temperature&quot;: 0.1,
  &quot;conv_bias&quot;: true,
  &quot;conv_dim&quot;: [
    512,
    512,
    512,
    512,
    512,
    512,
    512
  ],
  &quot;conv_kernel&quot;: [
    10,
    3,
    3,
    3,
    3,
    2,
    2
  ],
  &quot;conv_stride&quot;: [
    5,
    2,
    2,
    2,
    2,
    2,
    2
  ],
  &quot;ctc_loss_reduction&quot;: &quot;sum&quot;,
  &quot;ctc_zero_infinity&quot;: false,
  &quot;diversity_loss_weight&quot;: 0.1,
  &quot;do_stable_layer_norm&quot;: true,
  &quot;eos_token_id&quot;: 2,
  &quot;feat_extract_activation&quot;: &quot;gelu&quot;,
  &quot;feat_extract_dropout&quot;: 0.0,
  &quot;feat_extract_norm&quot;: &quot;layer&quot;,
  &quot;feat_proj_dropout&quot;: 0.1,
  &quot;feat_quantizer_dropout&quot;: 0.0,
  &quot;final_dropout&quot;: 0.0,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout&quot;: 0.1,
  &quot;hidden_size&quot;: 1024,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 4096,
  &quot;layer_norm_eps&quot;: 1e-05,
  &quot;layerdrop&quot;: 0.1,
  &quot;mask_feature_length&quot;: 10,
  &quot;mask_feature_prob&quot;: 0.0,
  &quot;mask_time_length&quot;: 10,
  &quot;mask_time_prob&quot;: 0.075,
  &quot;model_type&quot;: &quot;wav2vec2&quot;,
  &quot;num_attention_heads&quot;: 16,
  &quot;num_codevector_groups&quot;: 2,
  &quot;num_codevectors_per_group&quot;: 320,
  &quot;num_conv_pos_embedding_groups&quot;: 16,
  &quot;num_conv_pos_embeddings&quot;: 128,
  &quot;num_feat_extract_layers&quot;: 7,
  &quot;num_hidden_layers&quot;: 24,
  &quot;num_negatives&quot;: 100,
  &quot;pad_token_id&quot;: 0,
  &quot;proj_codevector_dim&quot;: 768,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.12.0.dev0&quot;,
  &quot;use_weighted_layer_sum&quot;: false
}

<span class=""math-container"">```</span>
</code></pre>
","huggingface"
"128252","How to find LLM that is best at STS task?","2024-03-12 06:01:18","","0","54","<deep-learning><nlp><huggingface><llm>","<p>I'm trying to find large language models that maps an embedding vector in proximity if they are semantically similar, in Korean. I tried looking at bunch of leaderboard such as MTEB_ko-ko STS, AI Hub benchmark(Korean LLM benchmark), etc... However not all models that I want to compare are within one benchmark therefore hard to compare which one is the best.</p>
<p>So I'm reading about each LLM from its base model, how it is continuously pre-trained to see how its objective function looks like. After shortlisting LLMs I'm planning to create my own dataset to compare all LLMs in shortlists.</p>
<p>As this method seem tidious, wanted to here some ideas on how others will tackle such problem.</p>
","huggingface"
"128201","Reducing language bias for text classification, transformer model","2024-03-07 09:46:36","","0","9","<machine-learning><classification><nlp><transformer><huggingface>","<p>I am working on a text classification model predicting classes for text. We have languages from many parts of the world and some of our classes are dominated by specific languages. The model we are using is:</p>
<pre><code>https://huggingface.co/distilbert/distilbert-base-multilingual-cased
</code></pre>
<p>Even though the model is multilingual it shows bias pushing certain languages towards specific classes. If I translate text from English to Thai I will receive different predictions. Given the dataset imbalance in classes/languages, this is understandable but I'd like to improve it.</p>
<p>I'm wondering if someone has a good solution for decreasing this bias? I'm thinking of simply translating training data between the languages to reduce it</p>
","huggingface"
"128058","Safetensors are slower than torch.load. Why?","2024-02-27 15:21:13","","0","180","<pytorch><huggingface>","<p>Lately, I have been testing safetensors. It claims that the safetensors are faster and safer than pickle. So I did my own comparison with the code below:</p>
<pre><code>    start_time = time.time()
    with open(&quot;./pytorch_model.bin&quot;, &quot;rb&quot;) as f:
        tensors = torch.load(f, map_location=&quot;cpu&quot;)
    print(time.time() - start_time)
    start_time = time.time()
    model.load_state_dict(tensors)
    print(time.time() - start_time)

    start_time = time.time()
    tensors = load_file(&quot;./model.safetensors&quot;, device=&quot;cpu&quot;)
    print(time.time() - start_time)
    start_time = time.time()
    model.load_state_dict(tensors)
    print(time.time() - start_time)
</code></pre>
<p>The <strong>torch load</strong> takes 0.9035871028900146 seconds.</p>
<p><strong>Loading state dict with tensors loaded using torch load</strong> takes 0.14872288703918457 seconds.</p>
<p><strong>Safetensors</strong> load takes 0.14471077919006348 seconds.</p>
<p><strong>Loading state dict with tensors loaded using safetensors load</strong> takes 5.597403526306152 seconds.</p>
<p>When it comes to loading tensors safetensors load function clearly wins and is faster but why the same tensors take 5 seconds to instantiate the model?
Torch loaded and safentesors loaded tensors are the same. Why does it then take way more time?</p>
","huggingface"
"126590","Getting rid of the warning ""The following columns ... have been ignored"" and ""ValueError: batch_size should be a positive integer value ...""","2024-01-25 16:50:12","126591","0","51","<transformer><python-3.x><huggingface>","<p>I train a fine-tuning model with the PyTorch Trainer class:</p>
<pre class=""lang-py prettyprint-override""><code>bln_truncation = False

dataset = load_dataset(&quot;text&quot;, data_files={&quot;train&quot;: file_path})

block_size = 512
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize_function(examples):
    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=bln_truncation)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir=&quot;./&quot; + model_name,
    overwrite_output_dir=True,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    save_steps=save_steps,
)
#     print(next(model.parameters()).device)

model = AutoModelForCausalLM.from_pretrained(model_name)
model = torch.nn.DataParallel(model)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets[&quot;train&quot;],
)
#     print(next(model.parameters()).device)

trainer.train()
</code></pre>
<p>I get the warning and error:</p>
<pre class=""lang-shell prettyprint-override""><code>PyTorch: setting up devices
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The following columns in the training set don't have a corresponding argument in `DataParallel.forward` and have been ignored: attention_mask, input_ids, text. If attention_mask, input_ids, text are not expected by `DataParallel.forward`,  you can safely ignore this message.

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In [18], line 36
     28 trainer = Trainer(
     29     model=model,
     30     args=training_args,
     31     data_collator=data_collator,
     32     train_dataset=tokenized_datasets[&quot;train&quot;],
     33 )
     35 # Start training
---&gt; 36 trainer.train()

File ~/.local/lib/python3.9/site-packages/transformers/trainer.py:1317, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)
   1312     self.model_wrapped = self.model
   1314 inner_training_loop = find_executable_batch_size(
   1315     self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size
   1316 )
-&gt; 1317 return inner_training_loop(
   1318     args=args,
   1319     resume_from_checkpoint=resume_from_checkpoint,
   1320     trial=trial,
   1321     ignore_keys_for_eval=ignore_keys_for_eval,
   1322 )

File ~/.local/lib/python3.9/site-packages/transformers/trainer.py:1329, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)
   1327 self._train_batch_size = batch_size
   1328 # Data loader and number of training steps
-&gt; 1329 train_dataloader = self.get_train_dataloader()
   1331 # Setting up training control variables:
   1332 # number of training epochs: num_train_epochs
   1333 # number of training steps per epoch: num_update_steps_per_epoch
   1334 # total number of training steps to execute: max_steps
   1335 total_train_batch_size = args.train_batch_size * args.gradient_accumulation_steps * args.world_size

File ~/.local/lib/python3.9/site-packages/transformers/trainer.py:769, in Trainer.get_train_dataloader(self)
    759     return DataLoader(
    760         train_dataset,
    761         batch_size=self.args.per_device_train_batch_size,
   (...)
    764         pin_memory=self.args.dataloader_pin_memory,
    765     )
    767 train_sampler = self._get_train_sampler()
--&gt; 769 return DataLoader(
    770     train_dataset,
    771     batch_size=self._train_batch_size,
    772     sampler=train_sampler,
    773     collate_fn=data_collator,
    774     drop_last=self.args.dataloader_drop_last,
    775     num_workers=self.args.dataloader_num_workers,
    776     pin_memory=self.args.dataloader_pin_memory,
    777     worker_init_fn=seed_worker,
    778 )

File /srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py:357, in DataLoader.__init__(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)
    353             sampler = SequentialSampler(dataset)  # type: ignore[arg-type]
    355 if batch_size is not None and batch_sampler is None:
    356     # auto_collation without custom batch_sampler
--&gt; 357     batch_sampler = BatchSampler(sampler, batch_size, drop_last)
    359 self.batch_size = batch_size
    360 self.drop_last = drop_last

File /srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/utils/data/sampler.py:232, in BatchSampler.__init__(self, sampler, batch_size, drop_last)
    226 def __init__(self, sampler: Union[Sampler[int], Iterable[int]], batch_size: int, drop_last: bool) -&gt; None:
    227     # Since collections.abc.Iterable does not check for `__getitem__`, which
    228     # is one way for an object to be an iterable, we don't do an `isinstance`
    229     # check here.
    230     if not isinstance(batch_size, int) or isinstance(batch_size, bool) or \
    231             batch_size &lt;= 0:
--&gt; 232         raise ValueError(&quot;batch_size should be a positive integer value, &quot;
    233                          &quot;but got batch_size={}&quot;.format(batch_size))
    234     if not isinstance(drop_last, bool):
    235         raise ValueError(&quot;drop_last should be a boolean value, but got &quot;
    236                          &quot;drop_last={}&quot;.format(drop_last))

ValueError: batch_size should be a positive integer value, but got batch_size=11111111
</code></pre>
<p>I saw the warning at the beginning also in the remarks at <a href=""https://stackoverflow.com/a/70263850/11154841"">ValueError when pre-training BERT model using Trainer API</a>:</p>
<blockquote>
<p>The following columns in the training set don't have a corresponding argument in BertForMaskedLM.forward and have been ignored: Text, Sentiment.</p>
</blockquote>
<p>What can I do to get rid of the warning:</p>
<blockquote>
<p>The following columns in the training set don't have a corresponding argument in <code>DataParallel.forward</code> and have been ignored: attention_mask, input_ids, text. If attention_mask, input_ids, text are not expected by <code>DataParallel.forward</code>,  you can safely ignore this message.</p>
</blockquote>
<p>And the error:</p>
<blockquote>
<p>ValueError: batch_size should be a positive integer value, but got batch_size=11111111&quot;?</p>
</blockquote>
","huggingface"
"126508","Outdated Transformers TextDataset class drops last block when text overlaps. Replace by datasets Dataset class as input of Trainer train_dataset?","2024-01-21 00:43:54","126509","0","196","<dataset><transformer><huggingface><finetuning><llm>","<h3>Why I try to replace the <code>transformers</code> TextDataset class with <code>datasets</code> Dataset class</h3>
<p>I stumbled upon this when I tried to make the <code>train_dataset</code> of the Transformers Trainer class from a text file, see <a href=""https://datascience.stackexchange.com/q/126382/97556"">How can you get a Huggingface fine-tuning model with the Trainer class from your own text where you can set the arguments for truncation and padding?</a>.</p>
<p>The TextDataset of the transformers package is</p>
<ul>
<li>buggy (next heading) and</li>
<li>outdated (overnext heading).</li>
</ul>
<h4>Transformers TextDataset drops the last block of the split text</h4>
<p>The TextDataset class drops the last block of the text that was split into blocks by means of the <code>block_size</code> parameter, in the following example, <code>512</code> tokens (~ words and other things) per block:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, TextDataset

model_name = &quot;dbmdz/german-gpt2&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
file_path = './myfile.txt'

train_dataset = TextDataset(
    tokenizer=tokenizer,
    file_path=file_path,
    block_size=512,
    overwrite_cache=True,
)
</code></pre>
<p>If I check the last block, I see that it cuts the very last block that has the tail of the text. This code shows only the second last block, the last block gets dropped by the TextDataset class:</p>
<p><code>tokenizer.decode(train_dataset['input_ids'][-1])</code></p>
<p>Instead, the Trainer class does not drop the last batch by default, but you see from this that there is such a parameter also for the Auto dataloader arguments of the Trainer class, see <a href=""https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments.dataloader_drop_last"" rel=""nofollow noreferrer"">class transformers Training Arguments</a>:</p>
<blockquote>
<p>dataloader_drop_last (bool, optional, defaults to False) ‚Äî Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not.</p>
</blockquote>
<h4>Transformers TextDataset is outdated</h4>
<p>When I change the setting of a tokenizer and build the TextDataset object another time, sometimes a warning shows that you should take the Transformers datasets Dataset class instead.</p>
<p>Here is the warning (there are two warnings in it):</p>
<p>Warning 1:</p>
<pre class=""lang-shell prettyprint-override""><code>&gt; /srv/home/my_user/.local/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:54:
&gt; FutureWarning: This dataset will be removed from the library soon,
&gt; preprocessing should be handled with the ü§ó Datasets library. You can
&gt; have a look at this example script for pointers:
&gt; https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py
</code></pre>
<p>Warning 2:</p>
<pre class=""lang-py prettyprint-override""><code>&gt; warnings.warn( Token indices sequence length is longer than the
&gt; specified maximum sequence length for this model (31482 &gt; 512).
&gt; Running this sequence through the model will result in indexing errors
</code></pre>
<p>Warning 2 is just from changing from one tokenizer to another, it comes from <a href=""https://github.com/huggingface/transformers/blob/3f69f415adcbdaedec154ba8eac220ef3276975d/examples/pytorch/language-modeling/run_mlm.py#L466C1-L470C14"" rel=""nofollow noreferrer"">this line in the given link of the warning</a>.</p>
<pre class=""lang-py prettyprint-override""><code>        if data_args.max_seq_length &gt; tokenizer.model_max_length:
            logger.warning(
                f&quot;The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the &quot;
                f&quot;model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.&quot;
            )
</code></pre>
<p>It is enough to run the code again to get rid of warning 2. This question is only about warning 1 (&quot;FutureWarning: This dataset will be removed...&quot;).</p>
<h3>Question</h3>
<p>How do I replace the <code>transformers</code> Textdataset class with the <code>datasets</code> Dataset class so that the output is a dataset that can be the argument of the <code>train_dataset</code> parameter of the <code>transformers</code> Trainer class?</p>
","huggingface"
"126478","How to perform inference on a finetuned falcon 7b model fine tuned on open assistant dataset","2024-01-18 16:08:27","","0","17","<nlp><transformer><transfer-learning><huggingface><llm>","<p>I finetuned a falcon 7b model on the open assistant dataset using the official colab notebook provided by huggingface at <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing</a>
How do i perform inference on it?
A sample row from the dataset it has been finetuned on is</p>
<p>''### Human: Can you write a short introduction about the relevance of the term &quot;monopsony&quot; in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: &quot;Monopsony&quot; refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions. Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens &amp; Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions. Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue. References: Bivens, J., &amp; Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.### Human: Now explain it to a dog''</p>
<p>What kind of prompt do i give this model for inference?
Should i use tags like Human and Assistant</p>
","huggingface"
"126431","Falcon-7B llm giving random output","2024-01-15 14:21:16","126434","1","45","<nlp><transformer><huggingface><gpt><llm>","<p>I am using a falcon 7B model for a chatbot without any finetuning with the following code</p>
<pre><code>model_name = &quot;ybelkada/falcon-7b-sharded-bf16&quot;

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    trust_remote_code=True
)
model.config.use_cache = False
from transformers import pipeline

generator = pipeline(
    &quot;text-generation&quot;,
    model=model,
    tokenizer=&quot;gpt2&quot;,
)

result = generator(&quot;Hi&quot;)
print(result)
</code></pre>
<p>the result isnt as expected and it outputs
[{'generated_text': 'Hi8\x10=:AHi8\x10&gt;Hi8\x10&gt;:AHi8\x10?'}].
How can i fix this and make it output a proper response</p>
","huggingface"
"126382","How can you get a Huggingface fine-tuning model with the Trainer class from your own text where you can set the arguments for truncation and padding?","2024-01-12 11:35:42","126389","0","669","<huggingface><finetuning><llm><parameter>","<p>I want to find out the role of truncation and padding in Huggingface Transformers pretrained models and any fine-tuning model on top of that. Therefore I played around with these parameters, but I could not find a way to set them for my own text input for the fine-tuning model.</p>
<p>To see a default guide, the code that can help understand what this question is about is at <a href=""https://huggingface.co/docs/transformers/training"" rel=""nofollow noreferrer"">Huggingface - Transformers - Fine-tune a pretrained mode</a>. Here, you can pass the truncation parameter to a function that takes care of each example. If a dataset has 10 examples (10 quoted sentences, each in a new row), you will apply this function tokenize_function to each example.</p>
<pre class=""lang-py prettyprint-override""><code>from datasets import load_dataset
from transformers import AutoTokenizer

dataset = load_dataset(&quot;yelp_review_full&quot;)

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)


def tokenize_function(examples):

    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)


tokenized_datasets = dataset.map(tokenize_function, batched=True)
</code></pre>
<p>Afterwards, you can go on, the Trainer class takes care of the rest:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()
</code></pre>
<p>Now how would that be done if I did not load the dataset from the datasets module but if I had just an input text like &quot;A dog jumps over the wall&quot;? I want to take the German GPT2 and run a text generation model as a fine-tuning model on top of that.</p>
<p>How could I tell the tokenizer that it should or should not truncate, and then take that output and run the fine-tuning model on it so that it runs through?</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer

text = &quot;Wie ist das Wetter heute?&quot;

tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/german-gpt2&quot;)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Unclear why, I had to add these two lines of code:
# Check if the tokenizer has a padding token
if tokenizer.pad_token is None:
    # If not, assign the eos_token as the padding token
    tokenizer.pad_token = tokenizer.eos_token

###
#   HERE: --&gt; make the text a dataset that can be tokenized with or without truncation
#   THIS BLOCK NEEDS TO BE REPLACED BY THE NEEDED CODE
#   output object: dataset
### 

# And here is again the function of the guide above that is called by the map function 
# of the dataset object. This is likely not the only way to get the text above 
# tokenized with or without truncation.
def tokenize_function(examples):

    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)

tokenized_datasets = dataset.map(tokenize_function, batched=True)

training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
)

trainer.train()
</code></pre>
<p>I could make a dataset object by loading the text from a text file instead, like this:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling, TextDataset, Trainer, TrainingArguments

model_name = &quot;dbmdz/german-gpt2&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
file_path = './myfile.txt'

train_dataset = TextDataset( #LineByLineTextDataset(
    tokenizer=tokenizer,
    file_path=file_path,
    block_size=512,
    overwrite_cache=True,
    # truncation=True, # cannot be done in this object
    # padding=True, # cannot be done in this object
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir=&quot;./output&quot;,
    overwrite_output_dir=True,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    save_steps=save_steps,
)

model = AutoModelForCausalLM.from_pretrained(model_name)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
)
</code></pre>
<p>But then I do not know how to set the truncation or padding arguments, uncommenting <code>truncation=True</code>
leads to <code>TypeError: __init__() got an unexpected keyword argument 'truncation'</code>, same with padding.</p>
<p>And when I try the truncation afterwards:</p>
<pre class=""lang-py prettyprint-override""><code>    train_dataset = dataset.map(
        lambda examples: tokenize_function(examples, tokenizer, bln_truncation),
        batched=True,
    )
</code></pre>
<p>It throws an error:</p>
<pre class=""lang-shell prettyprint-override""><code>AttributeError: 'TextDataset' object has no attribute 'map'
</code></pre>
<p>The same with</p>
<pre class=""lang-py prettyprint-override""><code>    dataset = LineByLineTextDataset( #LineByLineTextDataset( #TextDataset(
        tokenizer=tokenizer,
        file_path='myfile.txt',
        block_size=128,
#         truncation=True, # cannot be done in this object
#         padding=True, # cannot be done in this object
    )
</code></pre>
<p>Out:</p>
<pre class=""lang-shell prettyprint-override""><code>AttributeError: 'LineByLineTextDataset' object has no attribute 'map'
</code></pre>
<p>Therefore, I tried it with the load_dataset module of the datasets package to get the <code>map()</code> method of the class:</p>
<pre class=""lang-py prettyprint-override""><code>    from datasets import load_dataset

    dataset = load_dataset(&quot;text&quot;, data_files=file_path)

    # with this `['train']` key as a first test
    train_dataset = dataset['train']
</code></pre>
<p>But the fine-tuning at <code>trainer.train()</code> throws:</p>
<pre class=""lang-shell prettyprint-override""><code>File ~/.local/lib/python3.9/site-packages/datasets/dataset_dict.py:48, in DatasetDict.__getitem__(self, k)
     44 available_suggested_splits = [
     45     str(split) for split in (Split.TRAIN, Split.TEST, Split.VALIDATION) if split in self
     46 ]
     47 suggested_split = available_suggested_splits[0] if available_suggested_splits else list(self)[0]
---&gt; 48 raise KeyError(
     49     f&quot;Invalid key: {k}. Please first select a split. For example: &quot;
     50     f&quot;`my_dataset_dictionary['{suggested_split}'][{k}]`. &quot;
     51     f&quot;Available splits: {sorted(self)}&quot;
     52 )

KeyError: &quot;Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']&quot;
</code></pre>
<p>And if I code it without this <code>['train']</code> key since I want to have the full data and not a slice:</p>
<pre class=""lang-py prettyprint-override""><code>    from datasets import load_dataset

    dataset = load_dataset(&quot;text&quot;, data_files=file_path)

    train_dataset = dataset
</code></pre>
<p>The fine-tuning at <code>trainer.train()</code> throws:</p>
<pre class=""lang-shell prettyprint-override""><code>File /srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/_utils.py:644, in ExceptionWrapper.reraise(self)
    640 except TypeError:
    641     # If the exception takes multiple arguments, don't try to
    642     # instantiate since we don't know how to
    643     raise RuntimeError(msg) from None
--&gt; 644 raise exception

RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File &quot;/srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py&quot;, line 64, in _worker
    output = module(*input, **kwargs)
  File &quot;/srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/srv/home/tester/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py&quot;, line 1046, in forward
    transformer_outputs = self.transformer(
  File &quot;/srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/srv/home/tester/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py&quot;, line 889, in forward
    outputs = block(
  File &quot;/srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/srv/home/tester/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py&quot;, line 390, in forward
    attn_outputs = self.attn(
  File &quot;/srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/srv/home/tester/.local/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py&quot;, line 312, in forward
    query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)
  File &quot;/srv/home/seid/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py&quot;, line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File &quot;/srv/home/tester/.local/lib/python3.9/site-packages/transformers/pytorch_utils.py&quot;, line 107, in forward
    x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
</code></pre>
<p>All of this coding runs in circles when I ask ChatGPT 3.5 for help (I did not test a higher version). It will begin to subclass classes like <code>class MyTrainer(Trainer)</code> or <code>class MyDataCollator(DataCollatorForLanguageModeling)</code> and tries new functions like <code>collate_function(batch, tokenizer)</code> or overrides or overloads methods. After trying this for hours, I do not get to any code where I can just set the truncation, to begin with, let alone the padding, <em>and</em> get a working fine-tuned text generation model from this tokenized short input text.</p>
<p>How should I code a Huggingface fine-tuning model with the Trainer class where I can set the arguments for the truncation and padding parameters to tokenize my own short input text and where afterwards <code>trainer.train()</code> runs through?</p>
<h4>PS: why I ask</h4>
<p>I want to know this since I feed a text generation fine-tuning model with a short essay and ask it questions (even if it is not a Question Answering model). I do this since a Question Answering model is only cutting pieces from the text, there is no free speech, and even though the answers are not bad either, they are not as if a human being would make judgements, it is more a fishing for some keywords and their embedding. In short, the output is not good, I already tried a lot of hyperparameters. I do not split the text into sentences but load the whole text as one so-called &quot;example&quot; of the dataset. I do not know whether this is good, I just hope that the model understands the text better if it is one full text in one &quot;example&quot;.</p>
<p>I ask it questions regarding the whole text, not just some split sentences. Therefore, I want to make sure that the tokenizer works such that the whole text is read and tokenized without dropping any text. To check this, I want to change truncation from the default <code>False</code> to <code>True</code> only to see whether the output is worse. If it is not worse, I would know that it gets truncated to the max_length of the model. In short, I want to find out with this question here whether I need to change the code and split the text into many examples or not, but I also want to play around with the parameters and want to know whether truncation plays any role for the output at all.</p>
","huggingface"
"126380","Should you care about truncation and padding in an LLM even if it has a very large tokenizer.max_length so that truncation will never happen?","2024-01-12 10:27:01","126381","2","1466","<huggingface><finetuning><llm><parameter>","<p>I want to find out the role of truncation and padding in Huggingface Transformers pretrained models and/or any fine-tuning models on top. Taking a large language model like the German GPT2 shows that the <code>max_length</code> is very large so that truncation should not play a role in the code - if I am not mistaken:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = AutoTokenizer.from_pretrained(&quot;dbmdz/german-gpt2&quot;)
tokenizer.truncate_sequences
</code></pre>
<p>Out:</p>
<pre class=""lang-shell prettyprint-override""><code>&gt; &lt;bound method PreTrainedTokenizerBase.truncate_sequences of
&gt; PreTrainedTokenizerFast(name_or_path='dbmdz/german-gpt2',
&gt; vocab_size=50265, model_max_len=1000000000000000019884624838656,
&gt; is_fast=True, padding_side='right', truncation_side='right',
&gt; special_tokens={'bos_token': '&lt;|endoftext|&gt;', 'eos_token':
&gt; '&lt;|endoftext|&gt;', 'unk_token': '&lt;|endoftext|&gt;'})&gt;
</code></pre>
<p>Checking only the <code>max_length</code>:</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer.model_max_length
</code></pre>
<p>Out:</p>
<pre class=""lang-shell prettyprint-override""><code>1000000000000000019884624838656
</code></pre>
<p>We can see that the <code>max_length</code> is so utterly large that I doubt any full document will ever reach it - and this is just the length of each example, row by row, in the dataset. I guess that truncation does not play a role at all anymore if such a large <code>max_length</code> is set. &quot;Set the truncation parameter to True to truncate a sequence to the maximum length accepted by the model&quot; is what the Huggingface guide says about truncation, see <a href=""https://huggingface.co/docs/transformers/preprocessing#truncation"" rel=""nofollow noreferrer"">Truncation</a>.</p>
<p>That does not sound as if I ever can truncate the text or ever wanted to since that would lead to a worse understanding of the text. And that seems right since <a href=""https://huggingface.co/docs/transformers/pad_truncation#padding-and-truncation"" rel=""nofollow noreferrer"">Padding and truncation</a> shows that:</p>
<blockquote>
<p><code>False</code> or 'do_not_truncate': no truncation is applied. This is the default behavior.<br />
...<br />
<code>False</code> or 'do_not_pad': no padding is applied. This is the default behavior.</p>
</blockquote>
<p>The default for truncation is <code>False</code> anyway, why should I care about truncation? Should I care at all? If the <code>tokenizer.max_length</code> is so very very large, is truncation not just never happening anyway? Will the change to <code>True</code> change the output of the model or not?</p>
","huggingface"
"126248","Could someone help with fine-tuning dolphin-2.2.1?","2024-01-02 13:47:12","","1","75","<deep-learning><transformer><huggingface><finetuning>","<p><strong>Could someone help with fine-tuning dolphin-2.2.1?</strong></p>
<p><em>I have a problem with training: my train\loss - 0 and validation\loss - 0.000... after 800-1000 steps and this is overfitting</em></p>
<pre><code>Params:
dataset 250k, format &quot;text&quot; ### Human: ### Assistant: 
Prompt: ChatML
Trainer: optim - AdamW(model.parameters(), lr=6e-7, betas=(0.9, 0.95), eps=1e-05,), 
lr_scheduler_type=&quot;cosine&quot;,
warmup_steps=100,
per_device_train_batch_size=5,
per_device_eval_batch_size=5,
gradient_checkpointing=True,
gradient_accumulation_steps=4,
seed=42,
max_steps=10000,
learning_rate=6e-7,
logging_steps=100,
bf16=True,
logging_dir=&quot;./logs&quot;,
save_strategy=&quot;steps&quot;,
save_steps=100,
evaluation_strategy=&quot;steps&quot;,
eval_steps=100,
do_eval=True

Model with LORA: 
config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=[
        &quot;q_proj&quot;,
        &quot;k_proj&quot;,
        &quot;v_proj&quot;,
        &quot;o_proj&quot;,
        &quot;gate_proj&quot;,
        &quot;up_proj&quot;,
        &quot;down_proj&quot;,
        &quot;lm_head&quot;,
    ],
    bias=&quot;none&quot;,
    lora_dropout=0.05,
    task_type=&quot;CAUSAL_LM&quot;,
) 
</code></pre>
<p><strong>Not much experience I can't figure out what causes the model weights to be so memorized</strong></p>
<pre><code>More info:
fsdp_plugin = FullyShardedDataParallelPlugin(
    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),
    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),
)

accelerator = Accelerator(fsdp_plugin=fsdp_plugin)

base_model_id = &quot;cognitivecomputations/dolphin-2.2.1-mistral-7b&quot;
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=&quot;nf4&quot;,
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = MistralForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)

tokenizer = LlamaTokenizer.from_pretrained(
    base_model_id,
    padding_side=&quot;left&quot;,
    add_eos_token=True)

tokenizer.pad_token = tokenizer.eos_token

def tokenize(prompt):
    result = tokenizer(
        prompt,
        truncation=True,
        max_length=2048,
        padding=&quot;max_length&quot;,
    )
    result[&quot;labels&quot;] = result[&quot;input_ids&quot;].copy()
    return result
</code></pre>
<p><em>libs: bitsandbytes, github.com/huggingface/transformers.git, github.com/huggingface/peft.git, github.com/huggingface/accelerate.git, datasets scipy ipywidgets</em>
latest</p>
<p>name\step\train|loss\eval|loss</p>
<pre><code>dolphin-2.2.1_new-2023-12-21-11-49  250 5.6574  5.372434139251709 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-21-17-19  500 4.3343  3.06201434135437 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-21-20-06  750 1.8981  0.4487628936767578
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-22-07-52  1000    0.2334  0.1926171183586121 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-22-13-18  1250    0.1687  0.1445329338312149 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-22-18-39  1500    0.1213  0.096932053565979 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-23-00-00  1750    0.0694  0.039184220135211945
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-23-06-34  2000    0.0169  0.0011213048128411174 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-23-19-08  2250    0.0027  0.00005872833207831718 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-24-01-06  2500    0.0009  0.000027619591492111795 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-24-10-11  2750    0.0002  0.000020941797629348 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-24-16-02  3000    0.0001  0.000017155833120341413 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-24-21-51  3250    0.0001  0.00001347742090729298 
</code></pre>
<pre><code>dolphin-2.2.1_new-2023-12-25-07-43  3500    0   0.000011896418072865345
</code></pre>
<pre><code>PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): MistralForCausalLM(
      (model): MistralModel(
        (embed_tokens): Embedding(32002, 4096)
        (layers): ModuleList(
          (0-31): 32 x MistralDecoderLayer(
            (self_attn): MistralSdpaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (rotary_emb): MistralRotaryEmbedding()
            )
            (mlp): MistralMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.05, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=8, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=8, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): MistralRMSNorm()
            (post_attention_layernorm): MistralRMSNorm()
          )
        )
        (norm): MistralRMSNorm()
      )
      (lm_head): lora.Linear(
        (base_layer): Linear(in_features=4096, out_features=32002, bias=False)
        (lora_dropout): ModuleDict(
          (default): Dropout(p=0.05, inplace=False)
        )
        (lora_A): ModuleDict(
          (default): Linear(in_features=4096, out_features=8, bias=False)
        )
        (lora_B): ModuleDict(
          (default): Linear(in_features=8, out_features=32002, bias=False)
        )
        (lora_embedding_A): ParameterDict()
        (lora_embedding_B): ParameterDict()
      )
    )
  )
)
</code></pre>
","huggingface"
"126197","Multilingual sentence generation with Hugging Face","2023-12-28 12:08:54","126217","0","38","<huggingface><text-generation><llm><t5>","<p>For an application I need to generate some random sentences, i.e. I don't need the output sentences to have any specific link to the prompt other than using the same language. If possible I need this process to be multilingual, i.e. to accept and generate in as many languages as possible. I also need to be able to run this process as many times as I want, this is why a dataset isn't suitable (fixed size).</p>
<p>I tried using <a href=""https://huggingface.co/docs/transformers/model_doc/mt5"" rel=""nofollow noreferrer"">mT5</a> which seems to be the most suitable model for my requirements, right? but I can't get anything out of it. Is it because it needs to be fine-tuned, as I read in different places? If so, any advice how I do this in my case?</p>
<p>Ideally I would also need this not to require too much computation, even at the cost of grammaticality/meaning of sentences.</p>
","huggingface"
"124998","RuntimeError: mat1 and mat2 shapes cannot be multiplied (25x7 and 1x512)","2023-12-10 22:58:28","","0","133","<deep-learning><nlp><time-series><transformer><huggingface>","<p>I am dealing with multivariate time series forecasting using Transformers.
below is my code step by step:</p>
<p>After some preprocessing and windowing time series dataset ‚Ä¶</p>
<p>1- Creating Mask function</p>
<pre><code>input_sequence_length = 10 # incoder input sequence
target_sequence_length = 5 # decoder input sequence

tgt_mask = generate_square_subsequent_mask(
    dim1=target_sequence_length,
    dim2=target_sequence_length
   )
src_mask = generate_square_subsequent_mask(
    dim1=target_sequence_length,
    dim2=input_sequence_length
   )
</code></pre>
<p>2- Positional Encoding</p>
<pre><code>class PositionalEncoder(nn.Module):
    def __init__(self, dropout: float = 0.1, 
        max_seq_len: int = 5000, d_model: int = 512,device = device):

        super().__init__()

        self.d_model = d_model
        self.dropout = nn.Dropout(p=dropout)
        self.batch_first = True  # Assuming batch_first is always True

        position = torch.arange(max_seq_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        pe = torch.zeros(1, max_seq_len, d_model)
        pe[0, :, 0::2] = torch.sin(position * div_term)
        pe[0, :, 1::2] = torch.cos(position * div_term)

        self.register_buffer('pe', pe)
        
    def forward(self, x: Tensor) -&gt; Tensor:
        x = x + self.pe[:, :x.size(1)]
        return self.dropout(x)
</code></pre>
<p>3 - Creating Transformers Encoder and Decoder with Pytorch</p>
<pre><code>class TimeSeriesTransformer(nn.Module):

    def __init__(self, 
        input_size: int,
        dec_seq_len: int,
        out_seq_len: int= 5, # target_sequence_length
        dim_val: int=512,  
        n_encoder_layers: int=2,
        n_decoder_layers: int=2,
        n_heads: int=4,
        dropout_encoder: float=0.2, 
        dropout_decoder: float=0.2,
        dropout_pos_enc: float=0.1,
        dim_feedforward_encoder: int=512,
        dim_feedforward_decoder: int=512,
        num_predicted_features: int=1
        ): 

        super().__init__() 

        self.dec_seq_len = dec_seq_len

        self.encoder_input_layer = nn.Linear(
            in_features=input_size, 
            out_features=dim_val 
            )

        self.decoder_input_layer = nn.Linear(
            in_features=num_predicted_features,
            out_features=dim_val
            )  
        
        self.linear_mapping = nn.Linear(
            in_features=dim_val, 
            out_features=num_predicted_features
            )

        # Create positional encoder
        self.positional_encoding_layer = PositionalEncoder(
            d_model=dim_val,
            dropout=dropout_pos_enc
            )

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=dim_val, 
            nhead=n_heads,
            dim_feedforward=dim_feedforward_encoder,
            dropout=dropout_encoder,
            batch_first=True
            )

        self.encoder = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=n_encoder_layers, 
            norm=None
            )

        decoder_layer = nn.TransformerDecoderLayer(
            d_model=dim_val,
            nhead=n_heads,
            dim_feedforward=dim_feedforward_decoder,
            dropout=dropout_decoder,
            batch_first=True
            )

        self.decoder = nn.TransformerDecoder(
            decoder_layer=decoder_layer,
            num_layers=n_decoder_layers, 
            norm=None
            )

    def forward(self, src: Tensor, tgt: Tensor, src_mask: Tensor=None, 
                tgt_mask: Tensor=None) -&gt; Tensor:

        src = self.encoder_input_layer(src) 
      
        src = self.positional_encoding_layer(src) 
        src = self.encoder(src=src)
        
        decoder_output = self.decoder_input_layer(tgt)
        decoder_output = self.decoder(
            tgt=decoder_output,
            memory=src,
            tgt_mask=tgt_mask,
            memory_mask=src_mask
            )
        decoder_output = self.linear_mapping(decoder_output) 
        
        return decoder_output
</code></pre>
<p>4 - model</p>
<pre><code>model = TimeSeriesTransformer(
    input_size=7,
    dec_seq_len=5,
    num_predicted_features=1,
    ).to(device)
</code></pre>
<p>5 - creating loader # befor created in the preprocessing step</p>
<pre><code>i, batch = next(enumerate(train_loader))
src, trg, trg_y = batch
src = src.to(device) # shape [5 , 10 , 7] , batch size , encoder sequence len , number of feature
trg = trg.to(device) # shape [5 , 5 , 7], batch size , decoder sequence len , number of feature
</code></pre>
<p>6 - output of the model</p>
<pre><code> output = model(
        src=src,
        tgt=trg,
        src_mask=src_mask,
        tgt_mask=tgt_mask
        )
    trg_y = trg_y.to(device) # [5 , 5 , 1] , batch size , deocder or output sequence len , number predicted feature
</code></pre>
<p>7 - Finally the raised error is like below</p>
<pre><code>output = model(
    src=src,
    tgt=trg,
    src_mask=src_mask,
    tgt_mask=tgt_mask
    )
Traceback (most recent call last):

  Cell In[348], line 1
    output = model(

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1518 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1527 in _call_impl
    return forward_call(*args, **kwargs)

  Cell In[344], line 80 in forward
    decoder_output = self.decoder_input_layer(tgt)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1518 in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\module.py:1527 in _call_impl
    return forward_call(*args, **kwargs)

  File C:\ProgramData\anaconda3\Lib\site-packages\torch\nn\modules\linear.py:114 in forward
    return F.linear(input, self.weight, self.bias)

RuntimeError: mat1 and mat2 shapes cannot be multiplied (25x7 and 1x512)
</code></pre>
","huggingface"
"124995","Understanding processors in huggingface tokenizer library","2023-12-10 20:59:51","125008","1","50","<huggingface><tokenization>","<p>tl;dr
What are the <code>:0</code> and <code>:1</code> in the following huggingface processors reference usage given on their <a href=""https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt#:%7E:text=and%20overflowing.-,The,-last%20step%20in"" rel=""nofollow noreferrer"">page</a>:</p>
<pre><code>tokenizer.post_processor = processors.TemplateProcessing(
    single=f&quot;[CLS]:0 $A:0 [SEP]:0&quot;,
    pair=f&quot;[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1&quot;,
    special_tokens=[(&quot;[CLS]&quot;, cls_token_id), (&quot;[SEP]&quot;, sep_token_id)],
)
</code></pre>
<p>Here the <code>cls_token_id</code> is 2 and <code>sep_token_id</code> is 3</p>
<p>Description:
Post you train a tokenizer using the huggingface library you have to then add a postprocessr for things like adding the [CLS] token at the beginning of every sentence.</p>
<p>The following is an example from their documentation</p>
<p><a href=""https://i.sstatic.net/7KxGT.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/7KxGT.png"" alt=""enter image description here"" /></a></p>
<p>Can you help me understand what purpose are the <code>:0</code> and <code>:1</code> serving?</p>
","huggingface"
"124742","Fine-tuning Hugging Face‚Äôs Llama Model with Unlabelled Data from PDFs from niche domain","2023-11-24 19:17:19","","1","127","<transformer><huggingface><finetuning><llm>","<p>I‚Äôm unsure about the next steps. Specifically, I have the following questions:</p>
<ul>
<li><p>How can I prepare my unlabelled data for the fine-tuning process?</p>
</li>
<li><p>What‚Äôs the best way to fine-tune the Llama model with my specific dataset?</p>
</li>
<li><p>Are there any specific considerations or best practices I should be aware of when fine-tuning a model with unlabelled data?</p>
</li>
</ul>
<p>I have a large amount of raw text data scraped from thousands of PDFs in a specific domain. I want to use this unlabelled data to fine-tune the Llama model from Hugging Face‚Äôs Transformers library.</p>
<p>Here‚Äôs what I‚Äôve done so far:</p>
<p>I‚Äôve successfully scraped the text from the PDFs and have it stored in a suitable format.</p>
","huggingface"
"124726","What happens when I set is_decoder to True in the bert API from huggingface?","2023-11-23 18:14:29","","0","87","<transformer><bert><huggingface>","<p>Please help me understand the implications of initialising the bert model from huggingface with <code>is_decoder</code> parameter set to <code>True</code></p>
<p>According to the <a href=""https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bertmodel:%7E:text=The%20model%20can,the%20forward%20pass."" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of cross-attention is added between the self-attention layers, following the architecture described in Attention is all you need by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.
To behave as an decoder the model needs to be initialized with the is_decoder argument of the configuration set to True; an encoder_hidden_states is expected as an input to the forward pass.</p>
</blockquote>
<p>I know how Bert is just the encoder and there is no causal masking which is the hallmark of a decoder. Can you please me understand how <code>is_decoder</code> changes the implementation of the bert to make it usable as a decoder?</p>
","huggingface"
"124642","Open-Source Large Language Models (LLM): Your experience and recommendation","2023-11-17 22:53:07","","0","178","<transformer><language-model><huggingface><llm>","<p>I‚Äôm looking for an open-source LLM for a new project. I want to use it for instructions and to fine-tune the model to a specific domain like legal and rights. Some LLMs are open-source, but they didn‚Äôt document, on which training data they trained their model. This makes it a bit complicated in my case.</p>
<p>I‚Äôm looking for models, that are open-source and the community knows on which datasets the model was trained.</p>
<p>Do you know open-source LLMs like that and do you have experience with them?</p>
<p>Thank you in advance.</p>
","huggingface"
"124530","F1 and Exact-Match (EM) Score in Extractive QA NLP","2023-11-10 15:51:39","","0","59","<nlp><bert><huggingface><llm><question-answering>","<p>I have a question as to how the F1 should be calculated in NLP and whether the text normalization is optional or not.</p>
<p>So I have been working on a project where we created a closed-domain extractive QA dataset from scratch, and we are trying to finetune and assess the performance of several LLMs in this new dataset. I came across different definitions of the F1 score, sometimes with text normalization (<a href=""https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#Exact-Match"" rel=""nofollow noreferrer"">like in here</a>) and sometimes not. I have run all my experiments without the normalization step for both the EM and F1 scores. Should I rerun all experiments?</p>
<p>Is the</p>
","huggingface"
"124155","How to use Bertweet model for topic modeling","2023-10-17 13:28:48","","0","70","<python><pytorch><bert><topic-model><huggingface>","<p>The problem is implementation of Bertweet in a topic-modeling project with understandable output like <a href=""https://maartengr.github.io/BERTopic/api/bertopic.html#bertopic._bertopic.BERTopic.fit_transform"" rel=""nofollow noreferrer"">BERTopic</a>, i want to use it on a relatively large (20k tweets) unlabelled dataset to segment it into topics, number of which is either user-specified or pre-defined by the model.</p>
<p>I've read a <a href=""https://huggingface.co/docs/transformers/model_doc/bertweet"" rel=""nofollow noreferrer"">documentation</a> of Bertweet and it's too short to be cohesive for a someone like me without previous experience with neural networks and transformers.
(For safety here's the whole example code from the link above)</p>
<pre><code>import torch
from transformers import AutoModel, AutoTokenizer

bertweet = AutoModel.from_pretrained(&quot;vinai/bertweet-base&quot;)

# For transformers v4.x+:
tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/bertweet-base&quot;, use_fast=False)

# For transformers v3.x:
# tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/bertweet-base&quot;)

# INPUT TWEET IS ALREADY NORMALIZED!
line = &quot;SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:&quot;

input_ids = torch.tensor([tokenizer.encode(line)])

with torch.no_grad():
    features = bertweet(input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
# from transformers import TFAutoModel
# bertweet = TFAutoModel.from_pretrained(&quot;vinai/bertweet-base&quot;)
</code></pre>
<p>From documentation example the output is:</p>
<pre><code>with torch.no_grad():
    features = bertweet(input_ids)
</code></pre>
<p>which is <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</code>
class, that contains attributes and information i don't understand.</p>
<p>And few not less important questions:</p>
<ol>
<li>Is this suitable for unsupervised learning tasks like
topic-modeling?</li>
<li>How to unpack this class to a format similar to
<strong>document - assigned_topic</strong>?</li>
</ol>
","huggingface"
"123908","How to get Llama-2 Rotary Embeddings?","2023-09-29 11:31:13","","0","319","<nlp><word-embeddings><transformer><language-model><huggingface>","<p>I want to get the Llama-2 rotary embeddings. I do <code>print(model)</code> and get the following output:
<a href=""https://i.sstatic.net/jFu2Z.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/jFu2Z.png"" alt=""enter image description here"" /></a></p>
<p>In the picture I highlight the rotary embeddings.</p>
<p>How can get the rotary embeddings and how can I interpret the output? What means 32x LLamaDecoderLayer and in its round brakets are four layer plus LlamaRotaryEmbeddings?</p>
<p>It's possible to get the embeddings as the first hidden-state <code>hidden_state[0]</code> and I want to know, which hidden-state represents the rotary embeddings.
Am I right, that there are several rotary embeddings?</p>
<p>Thanks in forward.</p>
<p>Best regards.</p>
","huggingface"
"123891","Errors in results after saving model vs using directly from memory","2023-09-28 09:52:51","","0","30","<pytorch><transformer><huggingface><llm><serialisation>","<p>I am trying to save a Fine Tuned model using <code>trainer.save_model()</code> but after I load the saved_model it just responds with the input back again and does not give any new output.</p>
<p>But, when I don't save the model and use the one with changed weights from FT from memory. It gives expected responses.  (Jupyter notebook)</p>
<p>Example Code:</p>
<pre><code>model_name = &quot;bigcode/starcoder&quot;

packing = False
base_model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', load_in_8bit =True)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = &quot;right&quot;

training_arguments = TrainingArguments(
    output_dir=&quot;/tmp/output&quot;,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    optim=optim,
    save_steps=save_steps,
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    weight_decay=weight_decay,
    fp16=fp16,
    bf16=bf16,
    max_grad_norm=max_grad_norm,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    lr_scheduler_type=lr_scheduler_type,
    evaluation_strategy=&quot;steps&quot;,
    eval_steps=25
)

# Set supervised fine-tuning parameters
trainer = SFTTrainer(
    model=base_model,
    train_dataset=dataset[&quot;train&quot;],
    eval_dataset=dataset[&quot;test&quot;],
    dataset_text_field=&quot;text&quot;,
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
    packing=packing,
)

trainer.train()

trainer.save_model('xxx')

def infer(model, ip):
    inputs = tokenizer.encode(ip, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)
    outputs = model.generate(inputs, max_new_tokens=300, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(outputs[0])

</code></pre>
<p>Not sure what the issue is as not being able to save model is a bottleneck and it also does not release memory used within FT. (Jupyter)</p>
","huggingface"
"123889","Specifying arguments of HuggingFaceHub","2023-09-28 07:48:29","","0","266","<python><nlp><language-model><huggingface>","<p>In this <a href=""https://python.langchain.com/docs/integrations/llms/huggingface_hub"" rel=""nofollow noreferrer"">tutorial</a>, when specifying</p>
<pre><code>llm = HuggingFaceHub(
    repo_id=repo_id, model_kwargs={&quot;temperature&quot;: 0.5, &quot;max_length&quot;: 64}) 
</code></pre>
<p>only repository id is mentioned, without referring to the task that is to be performed (for instance summarization or text generation). However, one may specify something like</p>
<pre><code>dotaks = transformers.pipeline(
    model=repo_id, 
    tokenizer=tokenizer,
    task='text-generation',
    return_full_text=True,
    temperature=0.5,
    max_new_tokens=124,)
</code></pre>
<p>If I try to remove the task it would not work. What is the difference among the above two approaches? How will the top code decide on the appropriate task if not specified?</p>
","huggingface"
"123826","How to Use Multiple Adapters with a Pretrained Model in Hugging Face Transformers for Inference?","2023-09-23 22:02:38","","1","587","<deep-learning><nlp><pytorch><huggingface><inference>","<p>I have a pretrained Llama-2 model in the <code>models_hf</code> directory and two fine-tuned adapters: a summarization adapter in <code>./tmp/llama-output</code> and a chat adapter in <code>./tmp/chat_adapter</code>. The details of the code are in <a href=""https://stackoverflow.com/questions/77164963/how-to-merge-fine-tuned-adapter-and-pretrained-model-in-hugging-face-transformer"">another question</a>.</p>
<p>For inference, I'd like to use all three components: the pretrained model, summarization adapter, and chat adapter. However, I'm unsure about the memory requirements and the best approach.</p>
<p>Do I need to:</p>
<p><strong>Option 1</strong>: Load one instance of the pretrained model (e.g., 10GB GPU memory), and then load the two separate instances of adapters (e.g., 100MB each), resulting in a total memory usage of 10GB + 200MB?</p>
<p>OR</p>
<p><strong>Option 2</strong>: Load two separate instances of the 10GB pretrained model and stack the summarization adapter on one and the chat adapter on the other, resulting in a total memory usage of 20GB + 200MB?</p>
<p>Additionally, could you provide a code example or steps on how to load and use these components for inference effectively? I'm looking for guidance on memory management and loading processes to ensure smooth and efficient inference with this setup.</p>
","huggingface"
"123472","Deploying a model with GPU and pay-per-inference","2023-08-30 08:11:55","","1","78","<transformer><huggingface><aws><deployment><aws-lambda>","<p>I may have the wrong stack exchange. If that's the case, could someone point me to a stack that could help with this. Anyways...</p>
<p>My backend employs a sentence transformer model from HuggingFace. Since the number of requests per day is small, deploying a dedicated instance for serving inference requests is not cost-effective. Hence, I was thinking of AWS Lambda whereby I could pay per inference.</p>
<p>However, I need to serve each request fast, which necessitates using a GPU, which AWS Lambda does not offer.</p>
<p>Is there a deployment solution (not necessarily at AWS) whereby I would be able to use a GPU and pay per inference?</p>
","huggingface"
"123367","How can BERT/Transformer models accept input batches of different sizes?","2023-08-24 00:05:04","","0","27","<neural-network><nlp><transformer><huggingface>","<p>I understand that all inputs in a batch need to be of the same size. However, it seems BERT/Transformers models can accept batches with different sizes as input.</p>
<p>How is that possible? I thought we needed to pad all examples in a batch to <code>model.max_input_size</code>, however, it seems HuggingFace does <code>Dynamic Padding</code> that allows sending batches of different lengths (till the time they are smaller than <code>max_input_size</code>)</p>
<p><a href=""https://i.sstatic.net/zC1WS.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zC1WS.png"" alt=""enter image description here"" /></a></p>
<p>Link: <a href=""https://mccormickml.com/2020/07/29/smart-batching-tutorial/"" rel=""nofollow noreferrer"">https://mccormickml.com/2020/07/29/smart-batching-tutorial/</a><br />
Link2: <a href=""https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding"" rel=""nofollow noreferrer"">https://huggingface.co/learn/nlp-course/en/chapter3/2?fw=pt#dynamic-padding</a></p>
","huggingface"
"122718","Can I run falcon-7b on a free google colab?","2023-07-13 03:24:13","122720","0","845","<huggingface><google><llm><artificial-intelligence>","<p>I'm a beginner at ML and AI.</p>
<p><strong>Background:</strong></p>
<p>I wanted to try out falcon-7b, the example I'm trying out: <a href=""https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing"" rel=""nofollow noreferrer"">https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing</a> (Falcon-Guanaco example by hugging face team) . But I'm not able to use this due to disk space restrictions. Colab is offering 75GB of disk space and when running the final code block of <code>trainer.train()</code>, the colab's VM runs out of all the disk space it has and fails.</p>
<p><strong>Questions:</strong></p>
<ol>
<li>Is there anything I can change in that example's code and make it work on a free google colab. I can't get the paid one.</li>
<li>If that's not possible are there any other open source models that I can run on a free google colab that are close to falcon-7b's abilities?</li>
</ol>
","huggingface"
"122315","How to load Hugginface model on CPU?","2023-06-22 11:09:59","","1","250","<nlp><huggingface>","<p>My system runs out of memory on GPU. But I want just to test if it works and want to load it on CPU. How do I do that?
Adding <code>device = torch.device(‚Äòcpu‚Äô)</code> before loading model doesn‚Äôt help
It crashes here:</p>
<pre><code>model = AutoModelForCausalLM.from_pretrained(
    config.base_model_name_or_path,
    quantization_config=bnb_config,
    device_map=&quot;auto&quot;,
    trust_remote_code=True,
)
</code></pre>
","huggingface"
"122252","How can someone evaluate llama index model?","2023-06-19 19:24:53","","2","175","<python><nlp><huggingface><openai-gym><chatbot>","<p>I have built a openAI llama index based model which takes multiple pdf and able to give chatbot based response. I want to evaluate the llm for accuracy. I already know method such as Rouge and Bleu. Is there any other way to evaluate model ?</p>
","huggingface"
"122164","LMM Fine Tuning - Supervised Fine Tuning Trainer (SFTTrainer) vs transformers Trainer","2023-06-14 15:54:10","123467","3","2352","<deep-learning><transformer><language-model><huggingface><finetuning>","<p>When should one opt for the Supervised Fine Tuning Trainer (SFTTrainer) instead of the regular Transformers Trainer when it comes to instruction fine-tuning for Language Models (LLMs)? From what I gather, the regular Transformers Trainer typically refers to unsupervised fine-tuning, often utilized for tasks such as Input-Output schema formatting after conducting supervised fine-tuning. There seem to be various examples of fine-tuning tasks with similar characteristics, but with some employing the SFTTrainer and others using the regular Trainer. Which factors should be considered in choosing between the two approaches?</p>
<p>I looking for Fine Tuning a LLM for generating json to json transformation (matching texts in json) using huggingface and trl libraries.</p>
","huggingface"
"121866","Fine-tuning a pre-trained LLM for question-answering","2023-05-31 12:56:54","","2","1388","<transformer><language-model><huggingface><text-generation><finetuning>","<h3>Objective</h3>
<p>My goal is to fine-tune a pre-trained LLM on a dataset about Manchester United's (MU's) 2021/22 season (they had a poor season). I want to be able to prompt the fine-tuned model with questions such as &quot;How can MU improve?&quot;, or &quot;What are MU's biggest weaknesses?&quot;. The ideal responses would be insightful/logical and +100 words</p>
<h3>Data</h3>
<ul>
<li>I will simply use text from the relevant wiki page as my data: <a href=""https://en.wikipedia.org/wiki/2021%E2%80%9322_Manchester_United_F.C._season"" rel=""nofollow noreferrer"">https://en.wikipedia.org/wiki/2021%E2%80%9322_Manchester_United_F.C._season</a></li>
<li>How should I structure my data? Should it be a list dictionaries where the keys are the questions and the values are the answers (i.e. a list of question-answer pairs), or a long string containing all the text data (for context), or a combination of both?</li>
</ul>
<h3>Notes</h3>
<ul>
<li>I have mainly been experimenting with variations of Google's T5 (e.g.: <a href=""https://huggingface.co/t5-base"" rel=""nofollow noreferrer"">https://huggingface.co/t5-base</a>) which I have imported from the Hugging Face Transformers library</li>
<li>So far I have only fine-tuned the model on a list of 30 dictionaries (question-answer pairs), e.g.: {&quot;question&quot;: &quot;How could Manchester United improve their consistency in the Premier League next season?&quot;, &quot;answer&quot;: &quot; To improve consistency, Manchester United could focus on strengthening their squad depth to cope with injuries and fatigue throughout the season. Tactical adjustments could also be explored to deal with teams of different strengths and styles.&quot;}</li>
<li>Use of this small dataset (list of 30 dictionaries) has given poor results</li>
</ul>
<h3>Further Questions and Notes</h3>
<ul>
<li>Other than increasing the size of my dataset, is my approach sound?</li>
<li>What would you recommend as a minimum number of dictionaries to train/fine-tune the model on?</li>
<li>I am also aware that I can tune the hyperparameters to improve performance, but for now I am more concerned about my general approach being logical</li>
</ul>
","huggingface"
"121756","How to monitor training of text generation models?","2023-05-25 16:46:55","","1","238","<deep-learning><nlp><text-generation><huggingface><nlg>","<p>I'm finetuning a pretrained Huggingface model based on Transformers for a downstream <strong>Text Generation</strong> task, but I have doubts on how the fine-tuning process should be monitored:</p>
<p>In classification, I usually calculate the loss and other metrics (e.g   accuracy) on a validation set to for early stopping and to save the checkpoint of the best epoch, but for Text Generation tasks I see these additional issues:</p>
<ul>
<li><p>(Causal) text generation is <em>slow</em>, while training can be more parallelized with teacher forcing. <em>Does it make sense to perform validation and generate text after every training epoch?</em></p>
</li>
<li><p>Text generation depends on an additional set of hyperparameters which greatly condition the quality of generated text, such as the <a href=""https://huggingface.co/blog/how-to-generate"" rel=""nofollow noreferrer"">decoding technique</a> (greedy vs sampling-based or other techniques), number of beams, temperature, maximum length, etc. All these parameters are not actually used during training. <em>I suppose  also find the best combination <em>after</em> training, but how can I monitor the training?</em></p>
</li>
<li><p>HuggingFace <a href=""https://huggingface.co/docs/transformers/main_classes/text_generation"" rel=""nofollow noreferrer"">generation API</a> does not provide the loss during prediction, i.e I cannot generate text and calculate the cross-entropy loss (at least out-of-the-box) during validation. To calculate loss I could either</p>
<ol>
<li>Create a custom generation procedure which includes loss.</li>
<li>Perform two passes on all data during validation (one with <code>model.generate</code> and one with <code>model.forward</code> )</li>
</ol>
<p><em>Both these alternatives are suboptimal and this made me think that it is not common to calculate validation loss in text generation tasks, is it true?</em></p>
</li>
</ul>
<p><strong>What is the common way to monitor training/fine-tuning of text generation models?</strong></p>
","huggingface"
"121446","Can we train the Dolly v-2 model on a large general purpose unlabelled text?","2023-05-11 07:21:52","","0","160","<nlp><language-model><huggingface>","<p>I am familiar with ML and Deep Learning concepts and have had a look at Dolly and even got the pretrained model running on a Jupyter lab notebook on Databricks.</p>
<p>However when I take a look at their training dataset format, they are all in instruction and response format.</p>
<p>My specific question is that if I have a super large dump of general text that is not labelled in form of instruction and response, can I just train Dolly as an autoregressive language model that will take a piece of text as an input to the generate function later once trained, and just generate text ?</p>
<p>Suggestions would be really appreciated. Thanks</p>
","huggingface"
"121213","Creating LLM chatbot using llama-index + langchain","2023-04-28 12:30:25","","1","747","<machine-learning><python><transformer><gpt><huggingface>","<p>As the title suggests: I'm trying to build a chatbot which his goal should be sort of like &quot;chatgpt&quot;.</p>
<p>The chatbot will be installed on Slack workspace and I'm struggling with <strong>which scope</strong> I should create the documents on for building the index.</p>
<p>At the moment the document scope is &quot;<strong>per channel</strong>&quot; (i.e. <strong>every channel is a whole document</strong>), but I'm not sure it's the right approach (maybe it should be far <strong>smaller</strong> - does it make any difference at all?)</p>
<p>I'm also using huggingface transformers library, but since I'm newbie to this whole new evolving technology, I'm not sure what <strong>type of model should I use</strong>:</p>
<ol>
<li>text2text</li>
<li>text-generation</li>
<li>summarization</li>
<li>quiestion-answering</li>
</ol>
<p>I want the bot to address all (maybe I'm a bit naive).</p>
<p>... so overall, 2 questions:</p>
<ol>
<li>What should the Document scope be?</li>
<li>Which type of model should I look at? Any specific recommended one?</li>
</ol>
","huggingface"
"121208","trainable weights in automodel and comparison with lora","2023-04-28 06:57:42","","1","86","<huggingface>","<p>if i use RobertaForSequenceClassification or AutoModelForSequenceClassification,
are all the weights trained or only the new classification head trained</p>
<p>also ,</p>
<p>i am also noticing for &quot;roberta-large&quot; ,peft lora is under performing AutoModelForSequenceClassification
for glue data and benchmark.</p>
<p>peft lora lr=2e-4 , epoch =10,{'accuracy': 0.8897058823529411, 'f1': 0.9168207024029574}</p>
<p>AutoModelForSequenceClassification, lr=2e-5,,epoch =10 {'accuracy': 0.9044117647058824, 'f1': 0.9312169312169313}</p>
<p>is something wrong , or is lora only useful for larger models</p>
","huggingface"
"121200","Passing target text to gpt2 and T5 for fine tuning to learn text generation task","2023-04-27 20:33:02","121201","0","266","<nlp><language-model><gpt><huggingface><t5>","<p>I have text with each line in following format:</p>
<pre><code>&lt;text-1&gt; some text-1 &lt;text-2&gt; some text-2 &lt;text-3&gt; some text-3
</code></pre>
<p>I want fine tune model to learn generate <code>some text-3</code> after reading <code>some text-1</code> and <code>some text-2</code>. In GPT2 and T5 text generation tutorials, we do specify <code>input-ids</code> for target text i.e. labels, but for GPT2 we dont.</p>
<p>For example in <a href=""https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887"" rel=""nofollow noreferrer"">this T5 text generation tutorial</a>, we can find line:</p>
<pre><code>model_inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
</code></pre>
<p>But I could not find any such line in these GPT2 text generation examples:</p>
<ul>
<li><p><a href=""https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=6O-8Kr_m8AHE"" rel=""nofollow noreferrer"">huggingtweets demo</a>,</p>
</li>
<li><p><a href=""https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb#scrollTo=L8kjz49JEa-5"" rel=""nofollow noreferrer"">huggingartists demo</a></p>
</li>
<li><p><a href=""https://towardsdatascience.com/how-to-fine-tune-gpt-2-for-text-generation-ae2ea53bc272"" rel=""nofollow noreferrer"">Finetune GPT2 for text generation</a></p>
</li>
</ul>
","huggingface"
"121004","Fine-tuned MLM based RoBERTa not improving performance","2023-04-18 12:42:45","","1","1239","<transformer><bert><attention-mechanism><language-model><huggingface>","<p>We have lots of domain-specific data (200M+ data points, each document having ~100 to ~500 words) and we wanted to have a domain-specific LM.</p>
<p>We took some sample data points (2M+) &amp; fine-tuned RoBERTa-base (using HF-Transformer) using the Mask Language Modelling (MLM) task.</p>
<p>So far,</p>
<ol>
<li>we did 4-5 epochs (512 sequence length, batch-size=48)</li>
<li>used cosine learning rate scheduler (2-3 cycles/epochs)</li>
<li>We used dynamin masking (masked 15% tokens)</li>
</ol>
<p>Since the RoBERTa model is finetuned on domain-specific data, we do expect this model to perform better than the pre-trained-RoBERTa which is trained on general texts (wiki data, books, etc)</p>
<p>We did perform some tasks like Named Entity Recognition (NER), Text Classification, and Embedding generation to perform cosine similarity tasks. We did this on both finetuned domain-specific RoBERTa and pre-trained-RoBERTa.</p>
<p>Surprisingly, the results are the same (very small difference) for both models. We did try Spacy models too, but the results are the same.</p>
<p>Perplexity scores indicate that finetuned MLM-based RoBERTa has a minimal loss.</p>
<ol>
<li>Can anyone please help us understand why MLM based model is NOT performing better?</li>
<li>Should we go for more data OR more epochs OR both, to see some effect?</li>
<li>are we doing anything wrong here? Let me know if any required details are missing. I will update</li>
</ol>
<p>any suggestions OR any valuable links addressing these concerns would be really helpful</p>
<p>Huggingface discussion page: <a href=""https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913</a></p>
","huggingface"
"120781","Dynamic batching and padding batches for NLP in deep learning libraries","2023-04-07 12:05:10","","2","2731","<nlp><pytorch><huggingface><dynamic-batching>","<p>This is the usual way we train modern deep learning models for NLP, e.g. with Huggingface libraries where we have a fix length for the input no. of tokens/subwoords unit. <a href=""https://huggingface.co/docs/transformers/pad_truncation"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/pad_truncation</a></p>
<p>In the follow example, we have 5 sentences of various length and all of them are padded to the max length set at 1024.</p>
<p>The first part of my question is with regards to GPU memory usage and pad, when we train a model with batches of data with padded inputs, <strong>would the padded tokens hog up the GPU RAM</strong>? Even if the model don't compute them since they will return zeros, it's still rather wasteful.</p>
<p><strong>Or does PyTorch / Tensorflow or other lower-level tensor libraries reoptimize the batch such that the pads don't take up memory? If so, any pointers to code/docs on this?</strong></p>
<p><a href=""https://i.sstatic.net/UOo7qm.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UOo7qm.png"" alt=""enter image description here"" /></a></p>
<p>There are instances where the batches can be ordered in a way to arrange batches with similar length to go together, esp. at the start of model training, e.g. <a href=""https://discuss.huggingface.co/t/are-dynamic-padding-and-smart-batching-in-the-library/10404/15"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/are-dynamic-padding-and-smart-batching-in-the-library/10404/15</a></p>
<p>Instead of doing padding, are there existing code for some sort of dynamic batching without sorting, <strong>is there a way to keep an offset of all the input sentences EOS token and pack the batch into something that looks like this</strong>:</p>
<p><a href=""https://i.sstatic.net/04ggX.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/04ggX.png"" alt=""enter image description here"" /></a></p>
<p><strong>Are there examples of the above batch packing in other deep learning libraries? Or in native Pytorch/Tensorflow/JAX?</strong></p>
","huggingface"
"120681","RuntimeError: It looks like your LightningModule has parameters that were not used in producing the loss returned by training_step","2023-04-03 20:06:57","","0","1177","<pytorch><huggingface><pytorch-lightning>","<p>I'm trying to use <a href=""https://github.com/clovaai/donut"" rel=""nofollow noreferrer"">donut</a>, which is a transformer model with a huggingface implementation, and pre-train it on a language it hasn't been yet on my desktop. Unfortunately the version of the stack provided on the original repo doesn't support my GPU, so I had to port it to a newer PyTorch/PyTorch Lightning version.</p>
<p>Upon the first run, I got the following error:</p>
<pre><code>RuntimeError: It looks like your LightningModule has parameters that were not used in 
producing the loss returned by training_step. If this is intentional, you must enable 
the detection of unused parameters in DDP, either by setting the string value 
`strategy='ddp_find_unused_parameters_true'` or by setting the flag in the strategy with 
`strategy=DDPStrategy(find_unused_parameters=True)`.
</code></pre>
<p>Since I haven't really used Lightning before, I'm unsure of what this means. I've managed to get it run by setting said string value to True, but I don't know if I did something wrong while porting or if this is by design.</p>
<p>I've checked the <a href=""https://lightning.ai/docs/pytorch/stable/notebooks/lightning_examples/finetuning-scheduler.html?highlight=find_unused_parameters"" rel=""nofollow noreferrer"">documentation</a>, but there's very limited information. Setting this to <code>True</code> comes with a performance impact, so I'd like to know if I'm doing something wrong or if it's needed.</p>
<p>The training step is defined as follows:</p>
<pre><code>def training_step(self, batch, batch_idx):
    image_tensors, decoder_input_ids, decoder_labels = list(), list(), list()
    for batch_data in batch:
        image_tensors.append(batch_data[0])
        decoder_input_ids.append(batch_data[1][:, :-1])
        decoder_labels.append(batch_data[2][:, 1:])
    image_tensors = torch.cat(image_tensors)
    decoder_input_ids = torch.cat(decoder_input_ids)
    decoder_labels = torch.cat(decoder_labels)
    loss = self.model(image_tensors, decoder_input_ids, decoder_labels)[0]
    self.log_dict({&quot;train_loss&quot;: loss}, sync_dist=True)
    return loss
</code></pre>
<p>I'll gladly share more code as I'm not sure where the parameters are being checked for this error message. I'd be thankful for any help.</p>
","huggingface"
"120630","Fine-tune GPT on sketch data (stroke-3)","2023-04-01 17:44:43","","1","87","<data><preprocessing><bert><huggingface><finetuning>","<p>These past days I have started a personal project where I would like to build a model that, given an uncompleted sketch, it can finish it.
I was planning on using some pretrained models that are available in HuggingFace and fine-tune them with my sketch data for my task. The sketch data I have is in stoke-3 format, like the following example:
<br>
[ <br>
[10, 20, 1], <br>
[20, 30, 1], <br>
[30, 40, 1], <br>
[40, 50, 0], <br>
[50, 60, 1], <br>
[60, 70, 0]<br>
]
<br>
The first value of each triple is the X-coordinate, the second value the Y-coordinate and the last value is a binary value indicating whether the pen is down (1) or up (0).
I was wondering if you guys could give me some instruction/tips about how should I approach this problem? How should I prepare/preprocess the data so I can fit it into the pre-trained models like BERT, GPT, etc. Since it's stroke-3 data and not text or a sequence of numbers, I don't really know how should I treat/process the data.</p>
<p>Thanks a lot! :)</p>
","huggingface"
"119912","How to finetune a closed generative huggingface model?","2023-03-02 14:52:20","","0","299","<nlp><huggingface><finetuning>","<p>I want to finetune a huggingface pretrained model on our internal documentation in a way it stats answering related questions. I could not find the adequate tutorial.</p>
","huggingface"
"118248","How to deal with DataCollator and DataLoaders in Huggingface?","2023-02-02 18:53:16","","0","927","<python><pytorch><text-classification><huggingface>","<p>I have issues combining a DataLoader and DataCollator. The following code with DataCollatorWithPadding results in a <code>ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.</code> when I want to iterate through the batches.</p>
<pre><code>from torch.utils.data.dataloader import DataLoader
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer)
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16, 
collate_fn=data_collator)
eval_dataloader = DataLoader(eval_dataset, batch_size=16, collate_fn=data_collator)
for epoch in range(2):
    model.train()
    for step, batch in enumerate(train_dataloader):          
          outputs = model(**batch)
          loss = outputs.loss
</code></pre>
<p>However, I found annother approach where I changed the DataCollator to <code>lambda x: x</code> Then it  gives me a <code>TypeError: DistilBertForSequenceClassification object argument after ** must be a mapping, not list</code></p>
<pre><code>from torch.utils.data.dataloader import DataLoader
train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=16, collate_fn=lambda x: x  )
eval_dataloader = DataLoader(eval_dataset, batch_size=16, collate_fn=lambda x: x)
for epoch in range(2):
    model.train()
    for step, batch in enumerate(train_dataloader):          
          outputs = model(**batch)
          loss = outputs.loss
</code></pre>
<p>For reproducability and for the rest of the code I provide you a Jupyter Notebook on Google Colab. You find the errors at the bottom of the notebook.
<a href=""https://colab.research.google.com/drive/1UboXyiL8Iovg-5ikRoSC-at7fWlFtPIW?usp=sharing"" rel=""nofollow noreferrer"">Link to Colab Notebook</a></p>
","huggingface"
"117689","error useing soft max gives outputs greater than 1","2023-01-11 10:19:35","","0","73","<multiclass-classification><softmax><huggingface>","<p>I am using Hugging Face AutoModelForSequenceClassification, model is roberta, using it for text classification.</p>
<p>There are 3 classes.</p>
<p>The output is: <code>[-3.7550,-4.4172,7.8079]</code></p>
<p>I need to convert this to probabilities
should I apply soft max to this to get the probabilities , if i do that i am getting outputs greater than one</p>
<pre><code>[9.51,4.90,0.99]
</code></pre>
","huggingface"
"117293","Combining sentence embeddings of two different models (sBERT and mBERT)","2022-12-25 02:49:52","117311","0","985","<nlp><bert><huggingface>","<br>
I am working on a chatbot that helps students. <br>
So, I wanted to make use of bert model which has better performance on mathematics, which lead to me to math-bert, but the paper on it said that it was trained only on mathematical corpus, which means it wont have great performance on general sentences (example in image), so is there a method to combine sentence-bert and math-bert?
<br>[![enter image description here][1]][1]
<br>Or, the only way is to train bert model from scratch using corpus used for sentence-bert and math-bert.
","huggingface"
"116986","Possible NLP approaches to extract 'goals' from text","2022-12-12 14:35:25","","0","146","<nlp><transformer><spacy><association-rules><huggingface>","<p>I am planning to take up an interesting NLP project. I want to extract 'goal' statements from lengthy reports. For example, the goals can be <em>We would be reducing our carbon footprint by 50% by 2025</em> or <em>Our company aims to increase the diversity in the work-force in upcoming months</em>. Check below image for example text and highlighted goals.</p>
<p><a href=""https://i.sstatic.net/pU6sy.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/pU6sy.png"" alt=""Example with highlighted goals"" /></a></p>
<p>How can I go about the process of goal extraction, I would like to get some pointers on possible NLP approaches ?</p>
","huggingface"
"114901","Extract the embedding from a specific layer of MarianModel","2022-10-04 14:23:59","","0","221","<transformer><machine-translation><huggingface>","<p>I am using using <a href=""https://huggingface.co/docs/transformers/model_doc/marian#transformers.MarianModel"" rel=""nofollow noreferrer"">MarianModel</a> from the hub of HuggingFace for a translation task. Now I want to extract the embedding from the output of the last <code>MarianEncoderLayer</code> layer. Specifically, given a text, I want to get the middle embedding of the text for another task. Here is what I did:</p>
<pre><code>text = 'hello how are you?'
inputs = tokenizer(text,max_length=512,truncation=True,return_tensors='pt',padding=True)
model.model.encoder.layers[5](**inputs)
</code></pre>
<p>But it throws the following error:</p>
<pre><code>Traceback (most recent call last):
  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;
  File &quot;C:\Users\dohuut\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\torch\nn\modules\module.py&quot;, line 1102, in _call_impl
    return forward_call(*input, **kwargs)
TypeError: forward() got an unexpected keyword argument 'input_ids'
</code></pre>
<p>What is the proper way to extract this embedding?</p>
","huggingface"
"114245","What could cause pre-trained Opus-MT models have wildly varying inference time when being used with transformers library?","2022-09-08 16:02:41","","1","82","<transformer><machine-translation><huggingface>","<p>I have been testing pre-trained Opus-MT models ported to transformers library for python implementation. Specifically, I am using <a href=""https://huggingface.co/Helsinki-NLP/opus-mt-en-fr"" rel=""nofollow noreferrer"">opus-mt-en-fr</a> for English to French translation. And the tokenizer and translation model is loaded via MarianTokenizer and MarianMTModels--similar to code examples shown <a href=""https://huggingface.co/docs/transformers/model_doc/marian"" rel=""nofollow noreferrer"">here</a> on huggingface.
Strangely, for the same pre-trained model translating the same English input on an identical machine, I have observed <em><strong>anywhere between 80+ ms and (whopping) 4 s per translation</strong></em> (example input = &quot;kiwi strawberry&quot;).</p>
<p>Wonder if anyone has observed similar behaviours, and what could cause such a wide variation?</p>
","huggingface"
"114058","Dataset Format for fine tuning deepset/roberta-base-squad2 hugging face transformer model","2022-09-03 09:44:58","","1","380","<nlp><tensorflow><pytorch><transformer><huggingface>","<p>I have been trying to fine tune the roberta model for QnA to my specific domain (healthcare).
I am unable to find the correct way to provide the dataset format to the tokenizer in order to fine tune the model.</p>
<p>Sample Dataset format -&gt;</p>
<pre><code>train_data = (
{
    'context':'context for the training data',
    'answers':{'text':['answer 1'],'answer_start':[115],'answer_end':[138] },
    'question':'question1'
},
{
    'context':'context for the training data',
    'answers':{'text':['answer 2'],'answer_start':[115],'answer_end':[138] },
    'question':'question2'
},
{
    'context':'context for the training data',
    'answers':{'text':['answer 3'],'answer_start':[115],'answer_end':[138] },
    'question':'question3'
}
)
</code></pre>
<p>Can anyone help me with the correct format to provide to tokenizer?</p>
<p>Thanks in advance.</p>
","huggingface"
"113309","Creating class labels for custom DataSets efficiently (HuggingFace)","2022-08-07 19:33:44","113315","0","1409","<nlp><transformer><huggingface>","<p>I have pandas dataframes - test &amp; train,they both have <code>text</code> and <code>label</code> as columns as shown below -</p>
<pre><code> label       text
 fear        ignition problems will appear 
 joy         enjoying the ride

</code></pre>
<p>As usual, to run any Transformers model from the HuggingFace, I am converting these dataframes into <code>Dataset</code> class, and creating the classLabels (fear=0, joy=1) like this -</p>
<pre><code>from datasets import DatasetDict 

traindts = Dataset.from_pandas(traindf)
traindts = traindts.class_encode_column(&quot;label&quot;)

testdts = Dataset.from_pandas(testdf)
testdts = testdts.class_encode_column(&quot;label&quot;)
</code></pre>
<p>Finally these <code>Datasets</code> are put into <code>DatasetDict</code>like this-</p>
<pre><code>emotions = DatasetDict({
    &quot;train&quot; : traindts , 
    &quot;test&quot; : testdts 
})

</code></pre>
<p>Everything works well but as you see that the way I am doing it can be definitely improved. How can it be done more efficiently in less number of lines ?</p>
","huggingface"
"113183","What Preprocessing is Needed for Semantic Search Using Pre-trained Hugging Face Transformers?","2022-08-02 12:08:18","113196","2","447","<nlp><dataset><preprocessing><transformer><huggingface>","<p>I am building a project for my bachelor thesis and am wondering how to prepare my raw data. The goal is to program some kind of semantic search for job postings. My data set consists of stored web pages in HTML format, each containing the detail page of a job posting. Via an interface I want to fill in predefined fields like skills, highest qualification, etc. with comma-separated sentences or words. These are then embedded via a Hugging Face Transformer and afterwards the similarity of the input is to be compared with the already embedded job postings and the &quot;best match&quot; is returned.</p>
<p>I have already found that intensive preprocessing such as stop word removal and lemmatization is not necessarily required for transformers. However, the data should be processed to resemble the data on which the pre-trained transformers learned. <strong>What would be the best way to prepare such a data set to fine-tune pre-trained Hugging Face Transformers?</strong></p>
<p>Additional info: 55,000 of the saved web pages contain an annotation scheme via which I could simply extract the respective sections &quot;Skills&quot; etc. from the HTML text. If that is not sufficient, I can use prodigy to further annotate the data, e.g. by span labeling texts within the text of the job postings.</p>
<p>Thank you very much in advance!</p>
","huggingface"
"113177","Do I need training data in multiple languages for a multilingual transformer?","2022-08-02 09:36:49","","0","55","<machine-learning><nlp><transformer><language-model><huggingface>","<p>I am attempting to train a transformer which can categorize sentences into one of n categories. This model should be able to work with a number of different languages - English and Arabic in my case.</p>
<p>Do I need to have labelled training data in both English and Arabic to fine tune a pretrained transformer, such as <a href=""https://huggingface.co/docs/transformers/model_doc/bloom"" rel=""nofollow noreferrer"">BLOOM</a>, or can I fine tune the model using only English samples, and then the model should also work well on Arabic samples for my classificaiton task, since the fine tuning only trained the classification head?</p>
<p>My thoughts are that the pretraining of this model should allow it to transform the same input texts in English and Arabic to the same (or similar) embedding, which the classification head would have learned to then predict these embeddings accurately through the fine tuning.</p>
","huggingface"
"112438","How to get all 3 labels' sentiment from finbert instead of the most likely label's?","2022-07-06 07:23:47","112446","0","1381","<bert><transformer><sentiment-analysis><huggingface>","<p>I'm using bert to do sentiment analysis. I previous used cardiffnlp's twitter-roberta-base-sentiment, <a href=""https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment"" rel=""nofollow noreferrer"">https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment</a>.</p>
<p>It gives the the usage on its page.</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer
import numpy as np
from scipy.special import softmax
import csv
import urllib.request

# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []
 
 
    for t in text.split(&quot; &quot;):
        t = '@user' if t.startswith('@') and len(t) &gt; 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return &quot; &quot;.join(new_text)

# Tasks:
# emoji, emotion, hate, irony, offensive, sentiment
# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary

task='sentiment'
MODEL = f&quot;cardiffnlp/twitter-roberta-base-{task}&quot;

tokenizer = AutoTokenizer.from_pretrained(MODEL)

# download label mapping
labels=[]
mapping_link = f&quot;https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt&quot;
with urllib.request.urlopen(mapping_link) as f:
    html = f.read().decode('utf-8').split(&quot;\n&quot;)
    csvreader = csv.reader(html, delimiter='\t')
labels = [row[1] for row in csvreader if len(row) &gt; 1]

# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL)
model.save_pretrained(MODEL)

text = &quot;Good night üòä&quot;
text = preprocess(text)
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)

# # TF
# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)
# model.save_pretrained(MODEL)

# text = &quot;Good night üòä&quot;
# encoded_input = tokenizer(text, return_tensors='tf')
# output = model(encoded_input)
# scores = output[0][0].numpy()
# scores = softmax(scores)
</code></pre>
<p>It shows sentiments of all three labels, positive, neutral and negative.</p>
<p>However, I'm now trying to use Finbert from ProsusAI to do sentiment analysis <a href=""https://huggingface.co/ProsusAI/finbert"" rel=""nofollow noreferrer"">https://huggingface.co/ProsusAI/finbert</a>. It doesn't give me its usage on its page. So I'm following this tutorial <a href=""https://towardsdatascience.com/effortless-nlp-using-pre-trained-hugging-face-pipelines-with-just-3-lines-of-code-a4788d95754f"" rel=""nofollow noreferrer"">https://towardsdatascience.com/effortless-nlp-using-pre-trained-hugging-face-pipelines-with-just-3-lines-of-code-a4788d95754f</a>.</p>
<p>My code is</p>
<pre><code>from transformers import pipeline
classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert')
classifier('Stocks rallied and the British pound gained.')
</code></pre>
<p>However, the result is <code>[{'label': 'positive', 'score': 0.8983612656593323}]</code>. It only shows the sentiment of the most likely label's (positive). But I need all three labels' sentiment (positive, neutral and negative). How should I use it?</p>
","huggingface"
"112402","What did Sentence-Bert return here?","2022-07-05 04:20:21","112405","0","305","<bert><transformer><information-retrieval><huggingface><information-extraction>","<p>I used sentence bert to embed sentences from this tutorial <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">https://www.sbert.net/docs/pretrained_models.html</a></p>
<pre><code>from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('all-mpnet-base-v2')
</code></pre>
<p>This is the event triples <code>t</code> I forgot to concat into sentences,</p>
<pre><code>[('U.S. stock index futures', 'points to', 'start'),
 ('U.S. stock index futures', 'points to', 'higher start')]
</code></pre>
<p><code>model.encode(t)</code> returns a 2d array of shape (2,768), with two idential 768-dimension vectors, and its value is different from both <code>model.encode('U.S. stock index futures')</code> and <code>model.encode('U.S. stock index futures points to start')</code>. What could possibly have it returned?</p>
<p>It is the same situation for other models on huggingface such as <a href=""https://huggingface.co/sentence-transformers/stsb-distilbert-base"" rel=""nofollow noreferrer"">https://huggingface.co/sentence-transformers/stsb-distilbert-base</a></p>
","huggingface"
"111309","Could Attention_mask in T5 be a float in [0,1]?","2022-05-25 22:32:14","","1","193","<deep-learning><nlp><transformer><attention-mechanism><huggingface>","<p>I was inspecting T5 model from hf <a href=""https://huggingface.co/docs/transformers/model_doc/t5"" rel=""nofollow noreferrer"">https://huggingface.co/docs/transformers/model_doc/t5</a> . attention_mask is presented as</p>
<pre><code>attention_mask (torch.FloatTensor of shape (batch_size, sequence_length), optional) ‚Äî Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]:
1 for tokens that are not masked,
0 for tokens that are masked. 
</code></pre>
<p>I was wondering whether it could be used something &quot;softer&quot; not only selecting the not-padding token but also selecting &quot;how much&quot; attention should be used on every token.</p>
<p>This question is related to the one proposed here <a href=""https://datascience.stackexchange.com/questions/94517/can-the-attention-mask-hold-values-between-0-and-1/111303#111303"">Can the attention mask hold values between 0 and 1?</a></p>
<p>Do you know if such attention_mask vector is used in any other ways where a non integer value could harm the model?</p>
<p>Thank you for your precious time and advices.</p>
","huggingface"
"109978","How to train a Task Specific Knowledge Distillation model using Hugging face model","2022-04-13 14:23:07","","2","291","<deep-learning><pytorch><python-3.x><huggingface>","<p>I was referring to this code:</p>
<p><a href=""https://github.com/philschmid/knowledge-distillation-transformers-pytorch-sagemaker/blob/master/knowledge-distillation.ipynb"" rel=""nofollow noreferrer"">https://github.com/philschmid/knowledge-distillation-transformers-pytorch-sagemaker/blob/master/knowledge-distillation.ipynb</a></p>
<p>From @philschmid</p>
<p>I could follow most of the code, but had few doubts. Please help me to clarify these doubts.</p>
<p>In this code below:</p>
<pre><code>class DistillationTrainer(Trainer):
    def __init__(self, *args, teacher_model=None, **kwargs):
        super().__init__(*args, **kwargs)
        
        self.teacher = teacher_model
        
        # place teacher on same device as student
        self._move_model_to_device(self.teacher,self.model.device)
        
        self.teacher.eval()
</code></pre>
<p>When I take fine-tuned <code>teacher model</code> it is never fine-tuned in the process of Task Specific Distillation training, as in line <code> self.teacher.eval()</code> mentioned in the code.? Only the output of <code>teacher model</code> is considered for loss calculations.</p>
<p>I couldn't follow this line <code>self._move_model_to_device(self.teacher,self.model.device)</code>. What it is actually doing?</p>
<p>In Task Specific Distillation training, I am fine tuning my student model, but in the <code>DistillationTrainer</code> I pass both models. <strong>Where it's making sure that only student model weights are learned and not the teacher?</strong></p>
<pre><code>trainer = DistillationTrainer(
    student_model,
    training_args,
    teacher_model=teacher_model,
    train_dataset=train_data,
    eval_dataset=val_data,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    
)
<span class=""math-container"">```</span>
</code></pre>
","huggingface"
"109911","How to save hugging face fine tuned model using pytorch and distributed training","2022-04-12 03:25:35","","1","1495","<pytorch><python-3.x><huggingface><distributed><google-cloud>","<p>I am fine tuning masked language model from XLM Roberta large on google machine specs.
When I copy the model using <code>gsutil and subprocess</code> from container to GCP bucket it gives me error.</p>
<h3>Versions</h3>
<pre><code>  Versions torch==1.11.0+cu113 
  torchvision==0.12.0+cu113  
  torchaudio==0.11.0+cu113 
  transformers==4.17.0
</code></pre>
<p>I am using pre-trained Hugging face model.</p>
<p><code>I launch it as train.py file which I copy inside docker image and use vertex-ai ( GCP) to launch it using Containerspec</code></p>
<p><code>machineSpec = MachineSpec(machine_type=&quot;a2-highgpu-4g&quot;,accelerator_count=4,accelerator_type=&quot;NVIDIA_TESLA_A100&quot;)</code></p>
<pre><code>python -m torch.distributed.launch --nproc_per_node 4 train.py --bf16 

</code></pre>
<p>I am using</p>
<p><a href=""https://huggingface.co/xlm-roberta-large"" rel=""nofollow noreferrer"">https://huggingface.co/xlm-roberta-large</a></p>
<pre><code>tokenizer = tr.XLMRobertaTokenizer.from_pretrained(&quot;xlm-roberta-large&quot;,local_files_only=True)
model = tr.XLMRobertaForMaskedLM.from_pretrained(&quot;xlm-roberta-large&quot;, return_dict=True,local_files_only=True)

</code></pre>
<p><strong>Training Code</strong></p>
<pre><code>training_args = tr.TrainingArguments(

     output_dir='****'
    ,logging_dir='****'        # directory for storing logs
    ,save_strategy=&quot;epoch&quot;
    ,run_name=&quot;****&quot;
    ,learning_rate=2e-5
    ,logging_steps=1000
    ,overwrite_output_dir=True
    ,num_train_epochs=10
    ,per_device_train_batch_size=4
    ,prediction_loss_only=True
    ,gradient_accumulation_steps=2
#     ,gradient_checkpointing=True
    ,bf16=True #57100 
,optim=&quot;adafactor&quot;

)


trainer = tr.Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_data
)

</code></pre>
<p><strong>Train.py</strong></p>
<pre><code>import torch
import numpy as np
import pandas as pd
from transformers import BertTokenizer, BertForSequenceClassification
import transformers as tr
from sentence_transformers import SentenceTransformer
from transformers import XLMRobertaTokenizer, XLMRobertaForMaskedLM
from transformers import AdamW
from transformers import AutoTokenizer
from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup,BertForMaskedLM
from transformers import DataCollatorForLanguageModeling
from scipy.special import softmax
import scipy
import random
import pickle
import os
import time

import subprocess as sp


# torch.cuda.empty_cache()




start=time.time()


device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
print(&quot;Using&quot;, device)
torch.backends.cudnn.deterministic = True  

tr.trainer_utils.set_seed(0)

print(&quot;here&quot;)

tokenizer = tr.XLMRobertaTokenizer.from_pretrained(&quot;xlm-roberta-large&quot;,local_files_only=True)
model = tr.XLMRobertaForMaskedLM.from_pretrained(&quot;xlm-roberta-large&quot;, return_dict=True,local_files_only=True)
model.gradient_checkpointing_enable() #included as new line
print(&quot;included gradient checkpoint&quot;)

model.to(device)
print(&quot;Model loaded successfully&quot;)

df=pd.read_csv(&quot;data.csv&quot;) 
train_df=df.text.tolist()
print(len(train_df))

train_df=list(set(train_df))
train_df = [x for x in train_df if str(x) != 'nan']

print(&quot;Length of training data is \n &quot;,len(train_df))
print(&quot;DATA LOADED successfully&quot;)


train_encodings = tokenizer(train_df, truncation=True, padding=True, max_length=512, return_tensors=&quot;pt&quot;)
print(&quot;encoding done&quot;)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)
print(&quot;data collector done&quot;)

class SEDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
        
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        return item

    def __len__(self):
        return len(self.encodings[&quot;attention_mask&quot;])

train_data = SEDataset(train_encodings)

print(&quot;train data created&quot;)

training_args = tr.TrainingArguments(

     output_dir='results_mlm_exp1'
    ,logging_dir='logs_mlm_exp1'        # directory for storing logs
    ,save_strategy=&quot;epoch&quot;
    ,learning_rate=2e-5
    ,logging_steps=500
    ,overwrite_output_dir=True
    ,num_train_epochs=20
    ,per_device_train_batch_size=4
    ,prediction_loss_only=True
    ,gradient_accumulation_steps=2

    ,bf16=True #Ampere GPU
)


trainer = tr.Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_data
)

trainer.train()
print(&quot;model training finished&quot;)
trainer.save_model(&quot;model_mlm_exp1&quot;)

print(&quot;training finished&quot;)

end=time.time()

print(&quot;total time taken in hours is&quot;, (end-start)/3600)
</code></pre>
<p><strong>Error</strong></p>
<pre><code>trainer.save_model(&quot;model_mlm_exp1&quot;)
subprocess.call('gsutil cp -r /pythonPackage/trainer/model_mlm_exp1 gs://******/model_mlm_exp1', shell=True, stdout=subprocess.PIPE)

ERROR   ResumableUploadAbortException: 409 The object has already been created in an earlier attempt and was overwritten, possibly due to a race condition.
</code></pre>
","huggingface"
"109658","Hugging face Model Output 'last_hidden_state'","2022-04-04 10:59:53","","0","1601","<pytorch><bert><huggingface><bart>","<p>I am using the Huggingface BERTModel, The model gives <strong>Seq2SeqModelOutput</strong> as output. The output contains the past hidden states and the last hidden state.
These are my questions</p>
<ol>
<li>What is the use of the hidden states?</li>
<li>How do I pass my hidden states to my output layer?</li>
<li>What I actually want is the output tokens, from the model how do I get the prediction tokens?</li>
</ol>
","huggingface"
"108259","Transformer similarity fine-tuned way too often predicts pairs as similar","2022-02-17 19:11:43","","0","26","<classification><transformer><similarity><huggingface><finetuning>","<p>I fine-tuned a transformer for classification to compute similarity between names. This is a toy example for the training data:</p>
<pre><code>name0 name1 label
Test  Test  y
Test  Hi    n
</code></pre>
<p>I fined-tuned the transformer using the label and feeding it with pairs of names as its tokenizer allows to feed 2 pieces of text.</p>
<p>I found a really weird behavior. At prediction times, there exist pairs that have very high chances to be predicted as similar just because they have repeated words. For example,</p>
<pre><code>name0        name1       label
Hi Hi Hi     dsfds       ?
</code></pre>
<p>has a high chance to be predicted as y!</p>
<p><strong>In general there exist some names that no matter what you pair them with, the pairs gets predicted as y.</strong></p>
<p>Did anyone notice this behavior? Is it because I am fine-tuning on about 1000 examples?</p>
<p>At the moment, I am trying to augment my data with:</p>
<ul>
<li>Empty names</li>
<li>Random chars (always the same)</li>
</ul>
<p>E.g.</p>
<pre><code>name0 name1 label
Test        n
      Test  n
Test  dsfsd n
dsfsd Test  n
</code></pre>
<p>Unfortunately, I still see the same behavior.</p>
","huggingface"
"108178","How to prepare texts to BERT/RoBERTa models?","2022-02-15 12:15:41","","1","1184","<deep-learning><nlp><bert><transformer><huggingface>","<p>I have an artificial corpus I've built (not a real language) where each document is composed of multiple sentences which again aren't really natural language sentences.</p>
<p>I want to train a language model out of this corpus (to use it later for downstream tasks like classification or clustering with sentence BERT)</p>
<p><strong>How to tokenize the documents?</strong></p>
<p>Do I need to tokenize the input</p>
<p>like this:
<code>&lt;s&gt;sentence1&lt;/s&gt;&lt;s&gt;sentence2&lt;/s&gt;</code></p>
<p>or <code>&lt;s&gt;the whole document&lt;/s&gt;</code></p>
<p><strong>How to train?</strong></p>
<p>Do I need to train an MLM or an NSP or both?</p>
","huggingface"
"107212","Get sentence embeddings of transformer-based models","2022-01-19 00:12:26","","0","1671","<nlp><bert><transformer><embeddings><huggingface>","<p>I want to get sentence embeddings of transformer-based models (Bert, Roberta, Albert, Electra...).</p>
<p>I plan on doing mean pooling on the hidden states of the second last layer just as what bert-as-service did.</p>
<p>So my questions is that when I do mean pooling, should I include the embeddings related to [PAD] tokens or [CLS] token or [SEP] token?</p>
<p>For example, my sequence is 300 tokens, and are padded into 512 tokens.</p>
<p>The output size is 512 (tokens) * 768 (embeddings).</p>
<p>So should I average the embeddings of first 300 tokens or the embeddings of whole 512 tokens?</p>
<p>Why the embeddings of the last 212 tokens are non-zero?</p>
","huggingface"
"106341","How to improve language model ex: BERT on unseen text in training?","2021-12-22 10:50:35","106342","2","291","<classification><nlp><text-mining><bert><huggingface>","<p>I am using pre-trained language model for binary classification. I fine-tune the model by training on data my downstream task. The results are good almost 98% F-measure.</p>
<p>However, when I remove a specific similar sentence from the training data and add it to my test data, the classifier fails to predict the class of that sentence. For example, the sentiment analysis task</p>
<blockquote>
<p>&quot;I love the movie more specifically the acting was great&quot;</p>
</blockquote>
<p>I removed from training all sentences containing the words <strong>&quot; more specifically&quot;</strong> and surprisingly in the test set they were all misclassified, so the precision decreased by a huge amount.</p>
<p>Any ideas on how can I further fine-tune/improve my model to work better on unseen text in training to avoid the problem I described above? (of course without feeding the model on sentences containing the words <strong>&quot;more specifically&quot;</strong>)</p>
<p>Note: I observed the same performance regardless of the language model in use (BERT, RoBERTa etc).</p>
","huggingface"
"104842","Adding a new token to a transformer model without breaking tokenization of subwords","2021-12-07 03:25:48","","1","4256","<tokenization><huggingface>","<p>I'm running an experiment investigating the internal structure of large pre-trained models (BERT and RoBERTa, to be specific). Part of this experiment involves fine-tuning the models on a made-up new word in a specific sentential context and observing its predictions for that novel word in other contexts post-tuning. Because I am just trying to teach it a new word, we freeze the embeddings for the other words during fine-tuning so that only the weights for the new word are updated. This means that I would like for everything to be treated as if it were &quot;normal,&quot; except for adding the new word to the model's vocabulary.</p>
<p>I've added the new word to the model and tokenizer like in this MWE (in the case of BERT):</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer, BertForMaskedLM
new_words = ['myword1', 'myword2']
model = BertForMaskedLM.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_basic_tokenize = False)
tokenizer.tokenize('myword1 myword2') 
# verify the words do not already exist in the vocabulary
# result: ['my', '##word', '##1', 'my', '##word', '##2']
    
tokenizer.add_tokens(new_words)
model.resize_token_embeddings(len(tokenizer))
    
tokenizer.tokenize('myword1 myword2')
# result: ['myword1', 'myword2']
   
new_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_basic_tokenize = False)
    
new_tokenizer.tokenize('the period is a subword.')
# result: ['the', 'period', 'is', 'a', 'sub', '##word', '##.']
    
tokenizer.tokenize('but not when it follows myword1.')
# result: ['but', 'not', 'when', 'it', 'follows', 'myword1', '.']
</code></pre>
<p>How can I add a new token and have it behave correctly (i.e., by preserving the correct subword tokenization of adjacent strings)? Similar issues happen with RoBERTa, where the following word does not appear to be tokenized correctly (it is tokenized without the 'ƒ†' that indicates a preceding space, which is present when the new word is replaced with an existing token). (This ƒ† is also not present on the added token, but I assume that as long as the added token never occurs at the beginning of a string, it wouldn't matter.)</p>
<p>Edit:
After poking around a bit more, I've found that this is related to my setting <code>do_basic_tokenize=False</code>. If it is not set to false, the results come out as expected. Nevertheless, I'd prefer to keep that set to false if there's a way to fix this.</p>
","huggingface"
"103936","How to Fine Tune a BERT model for sentiment analysis to get the best f1 score","2021-11-08 12:42:25","","1","374","<loss-function><bert><transformer><sentiment-analysis><huggingface>","<p>I am building a multi-class sentiment analysis BERT model that's optimized to give the best f1 score.</p>
<p>More specifically, I train each epoch by optimizing binary cross entropy per class, taking the mean, and then run back propagation to optimize the parameters. At the end of each epoch, I then compute the soft-f1 score on the validation set, and then save that model only if the score has improved. The predicted class is chosen by taking an argmax, so there is no threshold.</p>
<p>There are a few observations here, some of which are troubling:</p>
<ol>
<li><p>If I simply use log-loss as the criteria for choosing the best model, then the model will typically only train for 3-4 epochs before the losses start to degrade. This is the standard approach, and gives descent f1 scores across the training/validation and test sets.</p>
</li>
<li><p>If I instead use the soft-f1 criteria as specified above, the model will typically train for <em>15</em> epochs (where I use a stopping condition of 5 epochs if there is no improvement). The f1 scores are significantly better on the training set, and marginally better on the validation and test sets. I'm guessing that the improvement on the training set is simply due to overfitting, as the model has run substantially longer. But then there are also improvements in the validation and test sets too. The log-loss for this second model is (naturally) worse than model #1.</p>
</li>
</ol>
<p>My question, which is the best model? Is it correct to use the f1 score to choose the best model while training, or am I just overfitting without realizing? Thanks!</p>
","huggingface"
"103389","HuggingFace hate detection model","2021-10-22 08:20:44","103466","1","52","<nlp><text-classification><huggingface>","<p>I am trying to train and evaluate a hate detection model using the <a href=""https://huggingface.co/transformers/"" rel=""nofollow noreferrer"">HuggingFace Transformers library</a> and this <a href=""https://github.com/t-davidson/hate-speech-and-offensive-language/tree/master/data"" rel=""nofollow noreferrer"">dataset</a>. Model performance is secondary, just trying to get it going. I have preprocessed the data and tokenised it as shown below:</p>
<pre class=""lang-py prettyprint-override""><code>import pandas as pd
import numpy as np
from numpy.random import RandomState
import re
import preprocessor as p
from transformers import AutoTokenizer

# Loading raw data
original_data = pd.read_csv('../data/data.csv')

# Make a random test and train split
rng = RandomState()
train = original_data.sample(frac=0.7, random_state=rng)
test = original_data.loc[~df.index.isin(train.index)]

# Preprocessing: remove special characters using RegEx
REPLACE_NO_SPACE = re.compile(&quot;(\.)|(\;)|(\:)|(\!)|(\')|(\?)|(\,)|(\&quot;)|(\|)|(\()|(\))|(\[)|(\])|(\%)|(\\\$)|(\&gt;)|(\&lt;)|(\{)|(\})&quot;)
REPLACE_WITH_SPACE = re.compile(&quot;(&lt;br\s/&gt;&lt;br\s/?)|(-)|(/)|(:).&quot;)

# Custum function to clean the datasets
def clean_tweets(df):
  tempArr = []
  for line in df:
    # send to tweet_processor
    tmpL = p.clean(line)
    # remove puctuation
    tmpL = REPLACE_NO_SPACE.sub(&quot;&quot;, tmpL.lower()) # convert all tweets to lower cases
    tmpL = REPLACE_WITH_SPACE.sub(&quot; &quot;, tmpL)
    tempArr.append(tmpL)
  return tempArr

# clean training data
train_tweet = clean_tweets(train[&quot;tweet&quot;])
train_tweet = pd.DataFrame(train_tweet)
# append cleaned tweets to the testing data
train[&quot;clean_tweet&quot;] = train_tweet

# clean the test data 
test_tweet = clean_tweets(test[&quot;tweet&quot;])
test_tweet = pd.DataFrame(test_tweet)
# append cleaned tweets to the training data
test[&quot;clean_tweet&quot;] = test_tweet

# Tokenisation so the inputs are ready for the model
tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)
</code></pre>
<p>The above code generates a table for the test data as:
<a href=""https://i.sstatic.net/dMzOi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dMzOi.png"" alt=""enter image description here"" /></a></p>
<p>As I understand, the next part should be the model training part and extracting the % of hate tweets. Any suggestions on implementation?</p>
","huggingface"
"103344","How does T5 model work on input and target data while transfer learning?","2021-10-21 03:41:11","","0","699","<nlp><transformer><transfer-learning><huggingface><nlg>","<p>I am working on a project where I want the model to generate job description based on <code>Role, Industry, Skills</code>. I have trained my data and got the resultant output.</p>
<p>I am aware that the <code>T5 model</code> was trained on C4 data in an unsupervised manner. The various different techniques applied including denoising, corrupted span, etc. But I am not able to understand how it works for classification problem.</p>
<p>My concern is if I pass my input and target variables for training how the model is going to train. Also give me a brief idea as how the model is going to react to input and output data.</p>
<p>Any links, resources is appreciated. Thankyou.</p>
","huggingface"
"103102","Huggingface - TypeError: 'TensorSliceDataset' object is not subscriptable","2021-10-13 10:58:15","","1","1581","<deep-learning><dataset><transformer><huggingface>","<p>I'm trying to make my own model for translate a language to another with <a href=""https://huggingface.co/transformers/model_doc/t5.html#t5forconditionalgeneration"" rel=""nofollow noreferrer"">T5ForConditionalGeneration</a> and Huggingface using no pretrained model (I need to use my own dataset and tokenizer because no pretrained model exists for the languages I use). When running the train() method, I get the following error: TypeError: 'TensorSliceDataset' object is not subscriptable</p>
<p>I suppose it is due to the type of the dataset given to the Trainer (TensorSliceDataset), but I can't figure out what should be the structure and type of the dataset that I should use for language translation. Can someone give me an example of dataset structure? I do not understand how the model can find the x and y labels if we don't tell it explicitly which column should be x and y.
<a href=""https://gist.github.com/TristanBilot/4cca4733faaa171f0475cb6cff4c634a"" rel=""nofollow noreferrer"">Source code of the example.</a></p>
","huggingface"
"103005","Can a reformer model really handle long-range dependency?","2021-10-10 16:15:16","","1","38","<deep-learning><nlp><transformer><attention-mechanism><huggingface>","<p>I read this <a href=""https://huggingface.co/blog/reformer"" rel=""nofollow noreferrer"">article</a> about new attention model called <a href=""https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb"" rel=""nofollow noreferrer"">Reformer</a>. Here is the main strength of this model:</p>
<blockquote>
<p>The Reformer pushes the limit of longe sequence modeling by its ability to process up to half a million tokens at once as shown in this demo.</p>
</blockquote>
<p>Is it actually true? Because later in jupyter notebook I see the following in a config:</p>
<pre><code>config = {
    &quot;attention_head_size&quot;: 64,
    &quot;attn_layers&quot;: [&quot;local&quot;, &quot;lsh&quot;, &quot;local&quot;, &quot;lsh&quot;, &quot;local&quot;, &quot;lsh&quot;],
    &quot;lsh_attn_chunk_length&quot;: 64,
    &quot;local_attn_chunk_length&quot;: 64,

    &quot;lsh_num_chunks_before&quot;: 1,
    &quot;lsh_num_chunks_after&quot;: 0,
    &quot;local_num_chunks_before&quot;: 1,
    &quot;local_num_chunks_after&quot;: 0,
}
</code></pre>
<p>This config tells that they use chunks, so attention can span only <code>64</code> tokens and 1 chunk before it.</p>
<p>Does it mean that there is no any long-range dependency can be learned by this model?</p>
","huggingface"
"102514","Is it possible to fine-tune a (Spanish RoBERTa) model for a different task?","2021-09-26 21:02:21","","1","370","<bert><sentiment-analysis><text-classification><huggingface>","<p>I'm doing sentiment analysis of Spanish tweets.</p>
<p>After reviewing some of the recent literature, I've seen that there's been a most recent effort to train a <a href=""https://huggingface.co/BSC-TeMU/roberta-large-bne"" rel=""nofollow noreferrer"">RoBERTa model</a> exclusively on Spanish text. It seems to perform better than the current state-of-the-art model for Spanish language modeling so far, <a href=""https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased"" rel=""nofollow noreferrer"">BETO</a>.</p>
<p>The initial BETO model has been trained with the Whole Word Masking technique for a variety of tasks, but <strong>does not include text classification</strong>.</p>
<p>However, I've come across a <a href=""https://huggingface.co/finiteautomata/beto-sentiment-analysis#beto-sentiment-analysis"" rel=""nofollow noreferrer"">model</a> that used the BETO model to create a model that is able to do text classification - more precisely, sentiment analysis.</p>
<p>It seems therefore possible to have a baseline model that cannot perform a certain task and fine-tune (or is it train?) for a different task.</p>
<p><strong>QUESTION:</strong><br>
Would it, therefore, be possible to take the afore-mentioned RoBERTa model - which in its initial form cannot perform text classification - and fine-tune it to a different task (sentiment analysis in tweets)? <br>
If so, how would I go about this? Can someone name any helpful resources?</p>
<p>Note: I have a labelled dataset with Spanish Tweets that I could use for fine-tuning.</p>
","huggingface"
"102098","How to use is_split_into_words with Huggingface NER pipeline","2021-09-15 06:44:03","122836","4","1676","<transformer><named-entity-recognition><huggingface>","<p>I am using Huggingface transformers for NER, following this excellent guide: <a href=""https://huggingface.co/blog/how-to-train"" rel=""nofollow noreferrer"">https://huggingface.co/blog/how-to-train</a>.</p>
<p>My incoming text has already been split into words. When tokenizing during training/fine-tuning I can use <code>tokenizer(text,is_split_into_words=True)</code> to tokenize the incoming text. However, I can't figure out how to do the same in a <code>pipeline</code> for predictions.</p>
<p>For example, the following works (but requires incoming text to be a string):</p>
<pre><code>s1 = &quot;Here is a sentence&quot;
p1 = pipeline(&quot;ner&quot;,model=model,tokenizer=tokenizer)
p1(s1)
</code></pre>
<p>But the following raises the following error: <code>Exception: Impossible to guess which tokenizer to use. Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.</code></p>
<pre><code>s2 = &quot;Here is a sentence&quot;.split()
toks = tokenizer(s2,is_split_into_words=True)
p2 = pipeline(&quot;ner&quot;,model=model)
p2(toks)
</code></pre>
<p>I don't want to join the incoming text into one sentence because whitespace is significant in my use case. Post-processing the outputs of the pipeline will be complicated if I just pass in one string rather than a list of words.</p>
<p>Any advice on how I can use <code>is_split_into_words=True</code> functionality in the pipeline?</p>
","huggingface"
"101974","Which steps are involved in sentiment analysis with Huggingface Transformers?","2021-09-11 13:20:53","","0","37","<keras><tensorflow><sentiment-analysis><huggingface>","<p>I want to perform a sentiment analysis of a dataset of (Spanish) tweets about COVID-19 vaccines.</p>
<p>I've already scraped the tweets and identified a <a href=""https://huggingface.co/finiteautomata/beto-sentiment-analysis"" rel=""nofollow noreferrer"">pretrained model</a> I can use for Spanish.</p>
<p>What I don't really understand - and don't seem to find the right resources either - is <br> <em>what steps are involved in the whole process?</em></p>
<p>I know I have to fine-tune the model which I'm not exactly sure how to do. <br>
<em>Do I necessarily need a labeled dataset for this or can I just train the model on my scraped tweets (that don't have a label yet)?</em></p>
","huggingface"
"101784","How to measure the accuracy of an NLP paraphrasing model?","2021-09-04 04:15:03","101790","4","424","<deep-learning><nlp><model-evaluations><huggingface>","<p>I using the HuggingFace library to do sentence paraphrasing (given an input sentence, the model outputs a paraphrase). How am I supposed to compare the results of two separate models (one trained with t5-base, the other with t5-small) for this task? Can I just compare the validation loss or do I need to use a metric (if so, what metric)?</p>
","huggingface"
"100508","Personal Project Classifying Bank Account Data - NLP Noob","2021-08-25 17:28:13","","1","28","<nlp><multiclass-classification><spacy><huggingface>","<p>Background:</p>
<p>I'm looking to get up to speed with some of the newer ML classification techniques and keep my ML python fresh in my spare time/learn some new skills, so as a challenge I'm trying to see if I can beat some of the budgeting apps with their classification (and make the tagging a bit more personal). I use Mint and have easy access to my own banking data so have a good way to benchmark this little project. Data is &lt; 100 characters, unstructured (I'm not going to share my personal data as thats all I have at the moment, but to give an idea of the structure I'm using have a look at your csv bank statement!).</p>
<p>Ideally I'd love to have way to tag a couple of &quot;levels&quot; e.g. Restaurants and Eating Out : Coffee shop with the merchant name e.g. Starbucks cleansed too.</p>
<p>Context:</p>
<p>I work in Python, familiar with Tensorflow (but use more for gradient descent type optimizations in my day to day work). I haven't done any NLP in over 10 years. Back then I used Naive Bayes classifier which took some heavy lift... things have come on A LOT since then.</p>
<p>So few questions:</p>
<ul>
<li><p>What are the current tools people use for this type of exercise. I've read about spaCy, huggingface - are these the main tools used? What are the pros cons to each if so? (without sounding too old... there is so much off the shelf nowadays since I last looked at this space... props to the open source community!!!)</p>
</li>
<li><p>Any other tools I should be considering?</p>
</li>
<li><p>Any good articles people have read where people have tried to implement this problem? Any good online resources tackling this?</p>
</li>
<li><p>Any gotchas I should watch out for?</p>
</li>
<li><p>Any public large datasets anyone is aware of I could use?</p>
</li>
<li><p>And also I plan to do this in Python, because that's what I'm familiar with, but if people are tackling this in a different language I'd love to hear which and why (and maybe I'll give it a spin).</p>
</li>
</ul>
","huggingface"
"99796","HuggingFace Transformers is giving loss: nan - accuracy: 0.0000e+00","2021-08-06 19:23:11","","2","2756","<nlp><bert><huggingface><loss>","<p>I am a HuggingFace Newbie and I am fine-tuning a BERT model (<code>distilbert-base-cased</code>) using the Transformers library but the training loss is not going down, instead I am getting <code>loss: nan - accuracy: 0.0000e+00</code>.</p>
<p>My code is largely per the boiler plate on the [HuggingFace course][1]:-</p>
<pre><code>model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)

opt = Adam(learning_rate=lr_scheduler)

model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])

model.fit(
    encoded_train.data,
    np.array(y_train),
    validation_data=(encoded_val.data, np.array(y_val)),
    batch_size=8,
    epochs=3
)
</code></pre>
<p>Where my loss function is:-</p>
<p><code>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</code></p>
<p>The learning rate is calculated like so:-</p>
<pre><code>lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=0.,
    decay_steps=num_train_steps
    )
</code></pre>
<p>The number of training steps is computed thus:-</p>
<pre><code>batch_size = 8
num_epochs = 3

num_train_steps = (len(encoded_train['input_ids']) // batch_size) * num_epochs
</code></pre>
<p>So far then all very much like the boiler plate code.</p>
<p>My data looks like this:-</p>
<pre><code>{'input_ids': &lt;tf.Tensor: shape=(1040, 512), dtype=int32, numpy=
array([[  101,   155,  1942, ...,     0,     0,     0],
       [  101, 27900,  7641, ...,     0,     0,     0],
       [  101,   155,  1942, ...,     0,     0,     0],
       ...,
       [  101,   109,  7414, ...,     0,     0,     0],
       [  101,  2809,  1141, ...,     0,     0,     0],
       [  101,  1448,  1111, ...,     0,     0,     0]], dtype=int32)&gt;, 'attention_mask': &lt;tf.Tensor: shape=(1040, 512), dtype=int32, numpy=
array([[1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       ...,
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)&gt;}
</code></pre>
<p>And like this:-</p>
<pre><code>10     2
147    1
342    1
999    3
811    3
Name: sentiment, dtype: int64
</code></pre>
<p>I have studied the forums and made the most obvious checks:-</p>
<p>Here I check if there are any NaN in the data:-</p>
<pre><code>print(&quot;Any NaN in y_train? &quot;,np.isnan(np.array(y_train)).any())

print(&quot;Any NaN in y_val? &quot;,np.isnan(np.array(y_val)).any())
</code></pre>
<p>Which gives:-</p>
<pre><code>Any NaN in y_train?  False
Any NaN in y_val?  False
</code></pre>
<p>I also tried:-</p>
<pre><code>print(&quot;Any NaN in encoded_train['input_ids']? &quot;,np.isnan(np.array(encoded_train['input_ids'])).any())
print(&quot;Any NaN 'encoded_train['attention_mask']'? &quot;,np.isnan(np.array(encoded_train['attention_mask'])).any())
</code></pre>
<p>but only got:-</p>
<pre><code>Any NaN in encoded_train['input_ids']?  False
Any NaN 'encoded_train['attention_mask']'?  False
</code></pre>
<p>I am struggling to know where to go next with this code.</p>
<p>The full error trace looks like this, you can see the accuracy and loss on each epoch and this model is obviously not training at all:-</p>
<pre><code>Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']
- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_59']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/3
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
130/130 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0019WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
130/130 [==============================] - 63s 452ms/step - loss: nan - accuracy: 0.0019 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 2/3
130/130 [==============================] - 57s 438ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 3/3
130/130 [==============================] - 57s 441ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
&lt;tensorflow.python.keras.callbacks.History at 0x7f304f714fd0&gt;
</code></pre>
<p>I would be happy to post more details if anyone is able to tell me what it would be useful to see.</p>
","huggingface"
"99744","Question answering bot: EM>F1, does it make sense?","2021-08-05 15:25:16","","0","1290","<nlp><f1score><huggingface><question-answering>","<p>I am fine-tuning a Question Answering bot starting from a pre-trained model from HuggingFace repo.</p>
<p>The dataset I am using for the fine-tuning has a lot of empty answers. So, after the fine tuning, when I'm evaluating the dataset by using the model just created, I find that the EM score is (much) higher than the F1 score. (I know that I must not use the same dataset for training and evaluation, it was just a quick test to see that everything is running)</p>
<p>I assume that this happens because every question with no real answer is a match when the model cannot predict an answer, but as a non-expert of NLP I wonder does this makes sense? is it theoretically possible or am I missing something big?</p>
","huggingface"
"96882","How to do batch inference on Hugging face pretrained models?","2021-06-20 16:10:22","","1","281","<deep-learning><nlp><machine-translation><huggingface>","<p>I want to do batch inference on MarianMT model. Here's the code:</p>
<pre><code>from transformers import MarianTokenizer
tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')
src_texts = [ &quot;I am a small frog.&quot;, &quot;Tom asked his teacher for advice.&quot;]
tgt_texts = [&quot;Ich bin ein kleiner Frosch.&quot;, &quot;Tom bat seinen Lehrer um Rat.&quot;]  # optional
inputs = tokenizer(src_texts, return_tensors=&quot;pt&quot;, padding=True)
with tokenizer.as_target_tokenizer():
    labels = tokenizer(tgt_texts, return_tensors=&quot;pt&quot;, padding=True)
inputs[&quot;labels&quot;] = labels[&quot;input_ids&quot;]
outputs = model(**inputs) 
</code></pre>
<p>How do I do batch inference?</p>
","huggingface"
"94753","Train and validation sets splits using load_data","2021-05-21 23:05:29","","1","28","<python><dataset><huggingface>","<p>I'm using the package &quot;datasets&quot;. The code I have:</p>
<pre><code>squad_v2 = True
model_checkpoint = &quot;roberta-base&quot;
batch_size = 10

from datasets import load_dataset, load_metric
datasets = load_dataset(&quot;squad_v2&quot;)
datasets
</code></pre>
<p>and output:</p>
<p><a href=""https://i.sstatic.net/Qp43N.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Qp43N.png"" alt=""enter image description here"" /></a></p>
<p>It's working fine, but as the dataset really huge is a lack of memory on the gpu during the training model. Is there any way how to using &quot;load_dataset&quot; make train and validations splits smaller? But still to have them both on &quot;datasets&quot;?</p>
","huggingface"
"94116","Is it correct to load weights from task Masked Language Modeling to train Causal Language Modeling","2021-05-07 06:02:23","","1","17","<huggingface>","<blockquote>
<p>I intend to use 2 tasks of modelling including (a) Causal language modelling &amp;<br />
(b) Mask language modelling for training my new added tokens</p>
</blockquote>
<p>My pseudo-code is below</p>
<pre><code>##add new tokenizer
model_name = &quot;vinai/phobert-large&quot;
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
with open(&quot;tokenizer_vocab.txt&quot;,&quot;r&quot;) as wr_file:
   new_tokens=wr_file.read().splitlines()
_________________
## Train the (a) task - Casual language modeling
added_tokens = tokenizer.add_tokens([tokens for i in new_tokens])
model.resize_token_embeddings(len(tokenizer))

## train the model following the hugging face released 
https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/language_modeling.ipynb#scrollTo=JAscNNUD3l-P

##save_the_model
model.save_pretrained('weights_tokenizer/')
tokenizer.save_pretrained('weights_tokenizer/')
___________________
## Train the (b) task - Masked language modeling
model = AutoModelForMaskedLM.from_pretrained(&quot;weights_tokenizer/&quot;)
tokenizer= AutoTokenizer.from_pretrained(&quot;weights_tokenizer/&quot;)

</code></pre>
<p><em>My question here is it correct when I want to train the (b) task with weights from the (a) (because I think I can somehow more enrich the tokenizer).</em></p>
<p>Or there is any solution that I can train my tokenizer in both these two tasks. And I then can use the weights (from training on both two tasks above) to train my model with the task (C)</p>
<p>I do appreciate your time and sharing.</p>
","huggingface"
"93487","Masked Language Modeling on Domain-specific Data","2021-04-24 08:53:23","","1","475","<deep-learning><nlp><transformer><huggingface>","<p>My goal is to have a language model that understands the relationships between words and can fill the masks in a sentence related to a specific domain. At first, I thought about pretraining or even training a language model(like BERT) from scratch, but unfortunately, my data isn't that big to help the previous model learn new connections, let alone learn the embeddings from scratch.</p>
<p>Now what I have in mind is creating a transformer model with my own vocabulary which consists of words in my domain-specific data (after separating them with spaces and not using transformer tokenizers). This way the vocab size would be smaller and the positions and relations would be learned faster and more easily. Although I'm a bit confused about implementation.</p>
<p>Can I use <a href=""https://www.tensorflow.org/tutorials/text/transformer"" rel=""nofollow noreferrer"">this architecture (that is for NMT)</a> and give plain text for both the input and output? or should I mask some tokens in the input and give the complete sentence as the label?</p>
<p>Any other suggestions?</p>
","huggingface"
"93144","Minimal working example or tutorial showing how to use Pytorch's nn.TransformerDecoder for batch text generation in training and inference modes?","2021-04-16 16:11:45","93146","7","10099","<pytorch><transformer><sequence-to-sequence><text-generation><huggingface>","<p>I want to solve a sequence-to-sequence text generation task (e.g. question answering, language translation, etc.).</p>
<p>For the purposes of this question, you may assume that I already have the input part already handled. (I already have a tensor of dimensions batch_size x num_input_tokens x input_dim representing the input sequences. Also, all input sequences in my problem are of the same length, so no masking is required on the input side of things).</p>
<p>Now, I want to generate the output sequences using nn.TransformerDecoder. I'm aware of Pytorch's official tutorial <a href=""https://pytorch.org/tutorials/beginner/transformer_tutorial.html"" rel=""noreferrer"">SEQUENCE-TO-SEQUENCE MODELING WITH NN.TRANSFORMER AND TORCHTEXT</a>. Unfortunately, the official tutorial doesn't meet my needs, for the following reasons:</p>
<ul>
<li>nn.TransformerDecoder is not used in the example.</li>
<li>The example is about language modeling, not text generation. There is no forward loop that generates text word by word.</li>
</ul>
<p>I've searched around the web and I've found a few things, but nothing like a simple and minimal working example that directly applies to my problem setting. Concretely, on the output side of things I need the following:</p>
<ul>
<li>I want to generate output sequences in batch. I've found codes on GitHub where people appear to be doing text generation, but they do it for a single sequence at a time, not a batch of multiple sequences.</li>
<li>The output sequences may have different lengths.</li>
<li>I want to train my model with the teacher-forcing strategy and batches of multiple sequences. Given that in training I know the lengths of the sequences in advance, you may assume that I already have my batches padded with zeroes. However, I still need to figure out how to implement the forward function of my model, with a generation loop that uses nn.TransformerDecoder. Basically, I need to figure out how to iterate word-wise over my batch of output sequences, masking out the future words in each step (so that the model doesn't cheat by trivially predicting the next words).</li>
<li>Then, I need a similar forward function for inference mode. I need to figure out how to implement the generation loop to do basically the same as in training mode, except that instead of teacher-forcing I want to implement greedy search (i.e. use the tokens with highest predicted probability at iteration i as the next input for iteration i+1).</li>
</ul>
<p>I already know how to do all this using LSTMs. Below you can see the forward function of a model that I implemented in the past to do exactly what I just said with an LSTM. The same forward function is used for both training and inference, depending on the value of the variable 'mode':</p>
<pre><code>  def forward(
      self,
      image_local_features,
      question_vectors,
      answers=None,
      max_answer_length=None,
      mode='train',
  ):
    if mode == 'train':
      batch_size, max_answer_length = answers.shape
      assert answers is not None
    else:
      batch_size = image_local_features.size(0)
      assert max_answer_length is not None
    
    y = self.embedding_table(self.start_idx).expand(batch_size, -1)
    o = torch.zeros(batch_size, self.hidden_size).to(DEVICE)
    h = self.W_h(question_vectors)
    c = self.W_c(question_vectors)

    if mode == 'train':
      answer_embeddings = self.embedding_table(answers.permute(1,0))
      assert answer_embeddings.shape == (max_answer_length, batch_size, self.embed_size)

    output = []

    for t in range(max_answer_length):
      y_bar = torch.cat((y,o),1)
      assert y_bar.shape == (batch_size, self.embed_size + self.hidden_size)
      assert h.shape == (batch_size, self.hidden_size)
      assert c.shape == (batch_size, self.hidden_size)
      h, c = self.lstm_cell(y_bar, (h, c))
      e = (self.W_attn(image_local_features) * h.unsqueeze(1)).sum(-1)
      att = torch.softmax(e,-1)
      a = (image_local_features * att.unsqueeze(2)).sum(1)
      assert a.shape == (batch_size, self.image_local_feat_size)
      u = torch.cat((a,h),1)
      assert u.shape == (batch_size, self.hidden_size + self.image_local_feat_size)
      v = self.W_u(u)
      o = self.dropout(torch.tanh(v))
      assert o.shape == (batch_size, self.hidden_size)
      output.append(self.W_vocab(o))
      if mode == 'train':
        y = answer_embeddings[t] # teacher-forcing
      else:
        y = self.embedding_table(torch.argmax(output[t], 1)) # greedy search
      assert y.shape == (batch_size, self.embed_size)

    output = torch.stack(output, 1)
    assert output.shape == (batch_size, max_answer_length, self.vocab_size)
    return output
</code></pre>
<p>Another way to phrase my question would be: how can I reimplement what I did with LSTMs using nn.TransformerDecoder instead?</p>
<p>Any minimal working / <em>hello world</em> example that shows how to do batch training and batch inference with nn.TransformerDecoder for text generation will be very appreciated.</p>
<hr />
<p><em>Note</em>: alternatively, if there is a straightforward way of accomplishing the same with an out-of-the-box solution from <a href=""https://huggingface.co/transformers/"" rel=""noreferrer"">hugginface</a>, that would be awesome too.</p>
","huggingface"
"91073","dealing with HuggingFace's model's tokens","2021-03-24 03:19:40","","1","288","<nlp><bert><tokenization><huggingface><bart>","<p>I have a few questions regarding tokenizing word/characters/emojis for different huggingface models.</p>
<p>From my understanding, a model would only perform best during inference if the token of the input sentence are within the tokens that the model‚Äôs tokenizer was trained on.</p>
<p>My questions are:</p>
<ol>
<li><p>is there a way to easily find out if a particular word/emoji is compatible (included during model training) with the model? (in huggingface context)</p>
</li>
<li><p>if this word/emoji is not was not included during model training, what are the best ways to deal with these words/emojis, such that model inference would give best possible output considering the inclusion of these word/emoji as input. (for 2. it would be nice if it could be answered in the context of my huggingface setup below, if possible)</p>
</li>
</ol>
<p>My current setup is as follows:</p>
<pre><code>from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
pre_trained_model = 'facebook/bart-large-mnli'
task = 'zero-shot-classification'
candidate_labels = ['happy', 'sad', 'angry', 'confused']
tokenizer = AutoTokenizer.from_pretrained(pre_trained_model)
model = AutoModelForSequenceClassification.from_pretrained(pre_trained_model)
zero_shot_classifier = pipeline(model=model, tokenizer=tokenizer, task=task)

zero_shot_classifier('today is a good day üòÉ', candidate_labels=candidate_labels)
</code></pre>
<p>Any help is appreciated üòÉ</p>
","huggingface"
"90908","How to do NER predictions with Huggingface BERT transformer","2021-03-20 01:59:04","","1","2445","<machine-learning><tensorflow><transformer><named-entity-recognition><huggingface>","<p>I am trying to do a prediction on a test data set without any labels for an NER problem.</p>
<p>Here is some background. I am doing named entity recognition using tensorflow and Keras. I am using huggingface transformers.</p>
<p>I have two datasets. A train dataset and a test dataset. The training set has labels, the tests does not.
Below you will see what a tokenized sentence looks like, what it's labels look like, and what it looks like after encoding</p>
<pre><code>['The', 'pope', &quot;isn't&quot;, 'really', 'making', 'much', 'of', 'an', 'effort', '.', 'He', &quot;'s&quot;, 'wearing', 'the', 'same', 'clothes', 'as', 'yesterday', '.']
['O', 'B-person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']
[101, 1109, 17460, 2762, 112, 189, 1541, 1543, 1277, 1104, 1126, 3098, 119, 1124, 112, 188, 3351, 1103, 1269, 3459, 1112, 8128, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
</code></pre>
<p>Here is the code on how I tokenized my text and encoded my labels</p>
<pre><code>from transformers import DistilBertTokenizerFast

tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-cased')
train_encodings = tokenizer(train_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)
val_encodings = tokenizer(val_texts, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True)

def encode_tags(tags, encodings):
    labels = [[tag2id[tag] for tag in doc] for doc in tags]
    encoded_labels = []
    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):
        # create an empty array of -100
        doc_enc_labels = np.ones(len(doc_offset),dtype=int) * -100
        arr_offset = np.array(doc_offset)

        # set labels whose first offset position is 0 and the second is not 0
        doc_enc_labels[(arr_offset[:,0] == 0) &amp; (arr_offset[:,1] != 0)] = doc_labels
        encoded_labels.append(doc_enc_labels.tolist())

    return encoded_labels

train_labels = encode_tags(train_tags, train_encodings)
val_labels = encode_tags(val_tags, val_encodings)

</code></pre>
<p>I have gotten my model to train and work. I'm getting pretty goods numbers when validating. Here is how that was done</p>
<pre><code>from transformers import TFDistilBertForTokenClassification, TFTrainer, TFTrainingArguments

training_args = TFTrainingArguments(
    output_dir='./results',
    num_train_epochs=5,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=16,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    evaluation_strategy = &quot;epoch&quot;,
    learning_rate = 2e-5
)

with training_args.strategy.scope():
    model = TFDistilBertForTokenClassification.from_pretrained('distilbert-base-cased', num_labels=len(unique_tags))

trainer = TFTrainer(
    model=model,                         # the instantiated ü§ó Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset,            # evaluation dataset
    compute_metrics=compute_metrics
)

trainer.train()

trainer.evaluate()
</code></pre>
<p>My main issue is that I don't know how to predict with this. I'm not familiar with the library and the documentation has not been helping much.</p>
<p>I can apparently use <code>trainer.predict(*param*)</code>, but I can't figure out what to actually input as the <em>param</em>.</p>
<p>On the other hand, when I do <code>model.predict(param)</code> where the param is the encoded sentence example I show above, I get this result</p>
<pre><code>TFTokenClassifierOutput(loss=None, logits=array([[[-0.3232851 ,  0.12578554, -0.47193137, ...,  0.16509804,
          0.19799986, -0.3560003 ]],

       [[-1.8808482 , -1.07631   , -0.49765658, ..., -0.7443374 ,
         -1.2379731 , -0.5022731 ]],

       [[-1.4291595 , -1.8587289 , -1.5842767 , ..., -1.1863587 ,
         -0.21151644, -0.52205306]],

       ...,

       [[-1.6405941 , -1.2474233 , -1.0701559 , ..., -1.1816512 ,
          0.323739  , -0.45317683]],

       [[-1.6405947 , -1.247423  , -1.0701554 , ..., -1.1816509 ,
          0.3237388 , -0.45317668]],

       [[-1.6405947 , -1.247423  , -1.0701554 , ..., -1.1816509 ,
          0.3237388 , -0.45317668]]], dtype=float32), hidden_states=None, attentions=None)
</code></pre>
<p>I don't know how I'm supposed to take that result and decode it back into labels. What am I supposed to do with the logits array? How am I supposed to predict this?</p>
","huggingface"
"87911","Top-K vs AUC - communicating results and next steps","2021-01-13 11:12:30","","1","118","<python><tensorflow><text-classification><huggingface>","<p>I have a bi-LSTM multi-label text classification model which when training on a highly imbalanced dataset with 1000 possible labels gives a top-k (k=5) categorical accuracy of 86% and a focal loss of 0.33 when trained with about 1m rows of data. When I also used the AUC metric, it gave 96%, but I'm not comfortable reporting this to my client because they will tear it apart looking for near 100% accurate predictions.</p>
<p>Three questions:</p>
<ol>
<li>What is a good way of explaining the AUC to non-tech savvy people?</li>
<li>The top-k score tells me there is room for improvement, however the AUC tells me I can chill out and boast about what I've built. From the model below, and given that I'm using transformers tokenizer to process the inputs, is there anything blindingly obvious that I could be doing better or is worth exploring further? Or should I just accept that the AUC% is very good and leave it at that?</li>
</ol>
<pre><code>input = Input(shape=(args.max_seq_len,))
        vocab_size = tokenizer.vocab_size

        initializer = tf.keras.initializers.HeUniform()

        model = Embedding(vocab_size, 600, input_length=args.max_seq_len, embeddings_initializer=initializer)(input)
        model = Bidirectional(LSTM(100, return_sequences=True, dropout=0.10, kernel_initializer=initializer), merge_mode='concat')(model)

        model = Flatten()(model)
        model = Dense(500, activation='relu',kernel_initializer=initializer)(model)
        model = Dropout(0.1)(model)
        model = BatchNormalization()(model)
        model = Dense(500, activation='relu', kernel_initializer=initializer)(model)
        model = Dropout(0.1)(model)
        model = BatchNormalization()(model)
        model = Dense(500, activation='relu',kernel_initializer=initializer)(model)
        model = Dropout(0.1)(model)
        model = BatchNormalization()(model)
        output = Dense(n_classes, activation='sigmoid')(model)

        model = Model(input, output)

        optimizer = tf.keras.optimizers.Adam(1e-3)
        if args.predict:
            model = load_weights(args, model)
            # Freeze layer weights during prediction mode
            model.trainable = False

        loss = tfa.losses.SigmoidFocalCrossEntropy()

        model.compile(loss=loss, optimizer=optimizer,
                      metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=5)])
</code></pre>
<ol start=""3"">
<li>I read that AUC is great at dealing with imbalanced datasets, but if the model is trained no-differently depending on which metric I use, would you still consider using i.e. MLSMOTE anyway to seek further improvement?</li>
</ol>
","huggingface"
"87898","How to i get word embeddings for out of vocabulary words using a transformer model?","2021-01-13 07:02:51","","3","1313","<nlp><transformer><stanford-nlp><tokenization><huggingface>","<p>When i tried to get word embeddings of a sentence using bio_clinical bert, for a sentence of 8 words i am getting 11 token ids(+start and end) because &quot;embeddings&quot; is an out of vocabulary word/token, that is being split into em,bed,ding,s.</p>
<p>I would like to know if there is any aggregation strategies available that make sense apart from doing a mean of these vectors.</p>
<pre><code>from transformers import AutoTokenizer, AutoModel
# download and load model
tokenizer = AutoTokenizer.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)
model = AutoModel.from_pretrained(&quot;emilyalsentzer/Bio_ClinicalBERT&quot;)

sentences = ['This framework generates embeddings for each input sentence']


#Tokenize sentences
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')


#Compute token embeddings
with torch.no_grad():
    model_output = model(**encoded_input)

print(encoded_input['input_ids'].shape)
</code></pre>
<p>Output :
torch.Size([1, 13])</p>
<pre><code>for token in encoded_input['input_ids'][0]:
  print(tokenizer.decode([token]))
</code></pre>
<p>Output:</p>
<pre><code>[CLS]
this
framework
generates
em
##bed
##ding
##s
for
each
input
sentence
[SEP]
</code></pre>
","huggingface"
"87634","List of Google T5 possible operations","2021-01-07 15:21:18","","0","153","<nlp><transfer-learning><transformer><huggingface>","<p>I am trying to use the huggingface.co pre-trained model of Google T5 (<a href=""https://huggingface.co/t5-base"" rel=""nofollow noreferrer"">https://huggingface.co/t5-base</a>) for a variety of tasks. But I can`t find a list of many tasks it really supports and how to address them. I found <code>summarize: </code> + the text to summarize. I also try to find an overview in the paper (<a href=""https://arxiv.org/abs/1910.10683"" rel=""nofollow noreferrer"">https://arxiv.org/abs/1910.10683</a>) there are for instance examples of question-answering in the appendix but without instructions how to tell T5 to answer a specific question. The huggingface.co documentation did not provide any further information besides the <code>summarize: </code>declaration.</p>
","huggingface"
"87058","Which script can be used to finetune BERT for SQuAD question answering in Hugging Face library?","2020-12-23 12:27:24","87070","0","205","<bert><huggingface><question-answering>","<p>I have gone through lot of blogs which talk about <code>run_squad.py</code> script from Hugging Face, but I could not find it in the latest repo. So, which script has to be used now for fine tuning?</p>
","huggingface"
"86526","BERT for classification model degenerates into all-positive predictions","2020-12-10 18:46:03","","0","168","<nlp><training><bert><huggingface>","<p>As a learning project, I'm training a BERT model with the CoLA dataset to detect sentence acceptability. Unfortunately my model is learning to classify every instance as &quot;acceptable&quot;, and I'm not sure what is going wrong with my code. Can anyone provide any help or insight into why this happens?</p>
<h3>Technical details</h3>
<p>I'm using hugging face's <code>transformers</code> library with PyTorch.</p>
<p>The (stripped version of the) code is as follows. Instrumentation and other details have been left out so the code is more readable.</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')

model = transformers.AutoModelForSequenceClassification.from_pretrained(
    'bert-base-cased',
    num_labels=2,
)

optimizer = transformers.AdamW(
    model.parameters(),
    lr=2e-5,
    eps=1e-8,
)

# The next lines read the CoLA dataset and split it for training and validation
training_dataloader = ...
validation_dataloader = ...

for epoch in range(4):
    train_loss = 0

    for batch in tqdm(train_dataloader):
        model.train()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        model.zero_grad()

        model_output = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            return_dict=True,
        )

        batch_loss = model_output.loss.sum()

        train_loss += batch_loss.item()

        batch_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        # HERE: Measure performance after learning from each batch here with validation_dataloader
</code></pre>
<p>I've checked and the code that reads the dataset seems correct, so it seems that the problem lies either in measuring the performance of the model or in the learning phase.</p>
<p>To measure the performance, I'm running the model over the validation split and then converting the logits into actual classifications as follows:</p>
<pre class=""lang-py prettyprint-override""><code>tp = fn = fp = tn = 0

for batch in validation_dataloader:
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['label'].to(device)

    model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        return_dict=True,
    )

    expected = batch['label']
    predictions = output.logits[:,1] &gt; output.logits[:,0] # Is this correct?

    tp += sum(1 for exp, pred in zip(expected, predictions) if     exp and     pred)
    fn += sum(1 for exp, pred in zip(expected, predictions) if     exp and not pred)
    fp += sum(1 for exp, pred in zip(expected, predictions) if not exp and     pred)
    tn += sum(1 for exp, pred in zip(expected, predictions) if not exp and not pred)
</code></pre>
<p>Is the line <code>predictions = ...</code> correct?</p>
","huggingface"
"86133","Huggingface Library - Multi-document summarization","2020-11-30 18:34:52","","1","272","<automatic-summarization><huggingface><bart>","<p>Can BART, PEGASUS ... etc. API in huggingface library be used to directly perform multi document summarization?</p>
<p>(e.g. here: <a href=""https://huggingface.co/transformers/model_doc/bart.html"" rel=""nofollow noreferrer"">https://huggingface.co/transformers/model_doc/bart.html</a>)</p>
","huggingface"
"84269","How to apply pruning on a BERT model?","2020-10-20 12:20:10","","1","163","<python><tensorflow><bert><huggingface><pruning>","<p>I have trained a BERT model using ktrain (tensorflow wrapper) to recognize emotion on text, it works but it suffers from really slow inference. That makes my model not suitable for a production environment. I have done some research and it seems pruning could help.</p>
<p>Tensorflow provides some options for pruning e.g.  tf.contrib.model_pruning .The problem is that it is not a widely used technique and I can not find a simple enough example that could help me to understand how to use it. Can someone help?</p>
<p><strong>Only answers that include a coding solution will be considered for the bounty.</strong></p>
<p>I provide my working code below for reference.</p>
<pre><code>import pandas as pd
import numpy as np
import preprocessor as p
import emoji
import re
import ktrain
from ktrain import text
from unidecode import unidecode
import nltk

#text preprocessing class
class TextPreprocessing:
    def __init__(self):
        p.set_options(p.OPT.MENTION, p.OPT.URL)
  
    def _punctuation(self,val): 
        val = re.sub(r'[^\w\s]',' ',val)
        val = re.sub('_', ' ',val)
        return val
  
    def _whitespace(self,val):
        return &quot; &quot;.join(val.split())
  
    def _removenumbers(self,val):
        val = re.sub('[0-9]+', '', val)
        return val
  
    def _remove_unicode(self, text):
        text = unidecode(text).encode(&quot;ascii&quot;)
        text = str(text, &quot;ascii&quot;)
        return text  
    
    def _split_to_sentences(self, body_text):
        sentences = re.split(r&quot;(?&lt;!\w\.\w.)(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?)\s&quot;, body_text)
        return sentences
    
    def _clean_text(self,val):
        val = val.lower()
        val = self._removenumbers(val)
        val = p.clean(val)
        val = ' '.join(self._punctuation(emoji.demojize(val)).split())
        val = self._remove_unicode(val)
        val = self._whitespace(val)
        return val
  
    def text_preprocessor(self, body_text):

        body_text_df = pd.DataFrame({&quot;body_text&quot;: body_text},index=[1])

        sentence_split_df = body_text_df.copy()

        sentence_split_df[&quot;body_text&quot;] = sentence_split_df[&quot;body_text&quot;].apply(
            self._split_to_sentences)

        lst_col = &quot;body_text&quot;
        sentence_split_df = pd.DataFrame(
            {
                col: np.repeat(
                    sentence_split_df[col].values, sentence_split_df[lst_col].str.len(
                    )
                )
                for col in sentence_split_df.columns.drop(lst_col)
            }
        ).assign(**{lst_col: np.concatenate(sentence_split_df[lst_col].values)})[
            sentence_split_df.columns
        ]
        
        body_text_df[&quot;body_text&quot;] = body_text_df[&quot;body_text&quot;].apply(self._clean_text)

        final_df = (
            pd.concat([sentence_split_df, body_text_df])
            .reset_index()
            .drop(columns=[&quot;index&quot;])
        )
        
        return final_df[&quot;body_text&quot;]

#instantiate data preprocessing object
text1 = TextPreprocessing()

#import data
data_train = pd.read_csv('data_train_v5.csv', encoding='utf8', engine='python')
data_test = pd.read_csv('data_test_v5.csv', encoding='utf8', engine='python')

#clean the data
data_train['Text'] = data_train['Text'].apply(text1._clean_text)
data_test['Text'] = data_test['Text'].apply(text1._clean_text)

X_train = data_train.Text.tolist()
X_test = data_test.Text.tolist()

y_train = data_train.Emotion.tolist()
y_test = data_test.Emotion.tolist()

data = data_train.append(data_test, ignore_index=True)

class_names = ['joy','sadness','fear','anger','neutral']

encoding = {
    'joy': 0,
    'sadness': 1,
    'fear': 2,
    'anger': 3,
    'neutral': 4
}

# Integer values for each class
y_train = [encoding[x] for x in y_train]
y_test = [encoding[x] for x in y_test]

trn, val, preproc = text.texts_from_array(x_train=X_train, y_train=y_train,
                                                                       x_test=X_test, y_test=y_test,
                                                                       class_names=class_names,
                                                                       preprocess_mode='distilbert',
                                                                       maxlen=350)

model = text.text_classifier('distilbert', train_data=trn, preproc=preproc)

learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)

predictor = ktrain.get_predictor(learner.model, preproc)

#save the model on a file for later use
predictor.save(&quot;models/bert_model&quot;)

message = &quot;This is a happy message&quot;

#cleaning - takes 5ms to run
clean = text1._clean_text(message)

#prediction - takes 325 ms to run
predictor.predict_proba(clean)
<span class=""math-container"">```</span>
</code></pre>
","huggingface"
"81681","Bert for QuestionAnswering input exceeds 512","2020-09-14 12:59:36","81689","4","1063","<bert><transformer><question-answering><huggingface>","<p>I'm training Bert on question answering (in Spanish) and i have a large context, only the context exceeds 512, the total question + context is 10k, i found that longformer is bert like for long document, but there's no pretrained in spanish so, is there any idea get around bert.</p>
<p>What i tried is:</p>
<pre><code>from transformers import BertConfig
config=BertConfig.from_pretrained(BERT_MODEL_PATH)
config.max_length=4000 
config.max_position_embeddings=4000
config.output_hidden_states=True
model = MyBertModel(config)    

</code></pre>
<p>but still gives me an error mismatch</p>
<blockquote>
<p>RuntimeError: Error(s) in loading state_dict for BertModel:
size mismatch for bert.embeddings.position_embeddings.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([4000, 768]).</p>
</blockquote>
","huggingface"
"76527","Overfitting in Huggingface's TFBertForSequenceClassification","2020-06-23 15:52:59","","1","2869","<overfitting><bert><huggingface>","<p>I'm using Huggingface's TFBertForSequenceClassification for multilabel tweets classification. During training the model archives good accuracy, but the validation accuracy is poor. I've tried to solve the overfitting using some dropout but the performance is still poor. The model is as follows:</p>
<pre><code># Get and configure the BERT model
config = BertConfig.from_pretrained(&quot;bert-base-uncased&quot;, hidden_dropout_prob=0.5, num_labels=13)
bert_model = TFBertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;, config=config)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=0.00015, clipnorm=0.01)
loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.CategoricalAccuracy('accuracy')

bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])
bert_model.summary()
</code></pre>
<p>The summary is as follows:</p>
<p><a href=""https://i.sstatic.net/4CQnP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4CQnP.png"" alt=""Bert Model summary"" /></a></p>
<p>When I fit the model, the outcome is:</p>
<pre><code>history = bert_model.fit(train_ds, epochs=30, validation_data = test_ds)
</code></pre>
<p><a href=""https://i.sstatic.net/RZJ5T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RZJ5T.png"" alt="""" /></a> <a href=""https://i.sstatic.net/UVH41.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UVH41.png"" alt=""enter image description here"" /></a></p>
","huggingface"
"68641","Should weight distribution change more when fine-tuning transformers-based classifier?","2020-02-24 20:08:45","","1","284","<pytorch><transformer><huggingface><weight-initialization><histogram>","<p>I'm using pre-trained DistilBERT model from <a href=""https://github.com/huggingface/transformers"" rel=""nofollow noreferrer"">Huggingface</a> with custom classification head, which is almost the same as in the <a href=""https://github.com/huggingface/transformers/blob/fb560dcb075497f61880010245192e7e1fdbeca4/src/transformers/modeling_distilbert.py#L579"" rel=""nofollow noreferrer"">reference implementation</a>: </p>

<pre class=""lang-py prettyprint-override""><code>class PretrainedTransformer(nn.Module):
    def __init__(
        self, target_classes):
        super().__init__()
        base_model_output_shape=768
        self.base_model = DistilBertModel.from_pretrained(""distilbert-base-uncased"")
        self.classifier = nn.Sequential(
            nn.Linear(base_model_output_shape, out_features=base_model_output_shape),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(base_model_output_shape, out_features=target_classes),
        )

        for layer in self.classifier:
            if isinstance(layer, nn.Linear):
                layer.weight.data.normal_(mean=0.0, std=0.02)
                if layer.bias is not None:
                    layer.bias.data.zero_()

    def forward(self, input_, y=None):
        X, length, attention_mask = input_
        base_output = self.base_model(X, attention_mask=attention_mask)[0]
        base_model_last_layer = base_output[:, 0]
        cls = self.classifier(base_model_last_layer)
        return cls
</code></pre>

<p>During training, I use linear LR warmup schedule with max LR=<code>5-e5</code> and <em>cross entropy loss</em>.
In general, the model is able to learn on my dataset and reach high precision/recall metrics.</p>

<p><strong>My question is:</strong></p>

<p>Should weights distributions and biases in <em>classification</em> layers change more during training? It seems like the weights almost do not change at all, even when I do not initialize them as in the code (to mean=<code>0.0</code> and std=<code>0.02</code>). Is this an indication that something is wrong with my model or it's just because the layers I've added are redundant and model does not learn nothing new?</p>

<p>Take look at the image of weight from the tensorboard:
<a href=""https://i.sstatic.net/JRDQ7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/JRDQ7.png"" alt=""weights of classification layers""></a></p>
","huggingface"
"62831","BERT - How Question answering is different than classification","2019-11-07 12:54:55","","0","1178","<pytorch><bert><question-answering><huggingface>","<p>Basically I am trying to understand how question answering works in case of BERT. Code for both classes QuestionAnswering and Classification is pasted below for reference. My understanding is:  </p>

<pre><code>class BertForSequenceClassification(PreTrainedBertModel):
    def __init__(self, config, num_labels=2):
        super(BertForSequenceClassification, self).__init__(config)
        self.num_labels = num_labels
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, num_labels)
        self.apply(self.init_bert_weights)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):
        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            return loss
        else:
            return logits
</code></pre>

<p>In Above code pooled_output is considered useful in line  <code>_, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)</code>  </p>

<p>And in below QnA code encoder layer output (i.e., sequence_output) is considered useful in line: <code>sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)</code></p>

<pre><code>class BertForQuestionAnswering(PreTrainedBertModel):
    def __init__(self, config):
        super(BertForQuestionAnswering, self).__init__(config)
        self.bert = BertModel(config)
        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version
        # self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.qa_outputs = nn.Linear(config.hidden_size, 2)
        self.apply(self.init_bert_weights)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):
        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)
        logits = self.qa_outputs(sequence_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1)
        end_logits = end_logits.squeeze(-1)
</code></pre>

<p>Now my questions are:  </p>

<ol>
<li><p>Why there are 2 logits being returned in sequence_output for Question Answering case</p></li>
<li><p>What is different in encoder layer and pooled layer </p></li>
<li><p>Why is encoder layer (sequence_output) is considered in QnA case and Pooled layer in classification case</p></li>
</ol>
","huggingface"