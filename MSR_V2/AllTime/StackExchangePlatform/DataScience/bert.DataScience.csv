Post Link,Title,CreationDate,AcceptedAnswerId,Score,ViewCount,Tags,Body,TagName
"{
  ""id"": 129869,
  ""title"": ""Sampling multiple masked tokens through Metropolis–Hastings""
}","Sampling multiple masked tokens through Metropolis–Hastings","2024-08-04 16:21:41","","1","17","<algorithms><bert><sampling>","<p>I'm trying to replicate the finding of the the publication <a href=""https://arxiv.org/pdf/2106.02736"" rel=""nofollow noreferrer"">&quot;Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis-Hastings&quot;</a> for obtaining the joint distribution of multiple masked values in a BERT-like model. The central algorithm is the following:</p>
<p><a href=""https://i.sstatic.net/gwZT993I.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/gwZT993I.png"" alt=""The algorithm of interest"" /></a></p>
<p>Given the somewhat complex notation within the manuscript, my current minimal working code looks like that:</p>
<pre class=""lang-py prettyprint-override""><code>from typing import Sequence
import numpy as np
from copy import copy

LENGTH = 5
MASK_TOKEN = 42

def forward(sequence: Sequence[int]) -&gt; np.ndarray:
    # Placeholder: Run the forward pass and get the unormalized logits
    return np.asarray([[0.2, 0.7, 0.1]] * len(sequence))

def energy_function(sequence: Sequence[int]):
    # That should be E_raw as specified in the paper.
    values = []
    for i in range(len(sequence)):
        sequence_new = copy(sequence)
        sequence_new[i] = MASK_TOKEN
        values.append(forward(sequence_new))
        
    # QUESTION 1: HOW TO REDUCE THAT TO A SCALAR?
    return np.asarray(values).sum(axis=0)

x = np.argmax(forward([MASK_TOKEN] * LENGTH), axis=-1)
for e in range(100):
    for t in range(LENGTH):
        e_old = energy_function(x)
        
        # Mask the next token
        x_next = x.copy()
        w_old = x[t]
        x_next[t] = MASK_TOKEN

        # Sample the token
        # QUESTION 2: Is x_next correct here?
        prediction = forward(x_next)
        w_new = np.random.choice(a=prediction.shape[1], size=1, p=prediction[t])
        x_next[t] = w_new

        # Prepare everything for the calculation of the acceptance probability
        # QUESTION 3: ARE THOSE INDICES CORRECT?
        q1 = prediction[w_new]
        q2 = prediction[w_old]
        e_new = energy_function(x_next)

        acceptance_probability = min(1.0, 
            (np.exp(-e_old) * q2) / (np.exp(-e_new) * q1)
        )

        if np.random.uniform(0.0, 1.0) &lt;= acceptance_probability:
            x = x_next
</code></pre>
<p>However, I'm now stuck at some details:</p>
<ol>
<li>(Line 6) For obtaining the acceptance probability, we need E_raw or E_norm to return a scalar. However, we obtain a sum of raw logits or softmax'ed logits. I could not find any strategy in the paper for reducing the value to a single scalar. Where is my mistake in understanding the paper?</li>
<li>(Line 8) Should we really sample from X or from X' instead?</li>
<li>(Line 9) Do you understand that notation that we must use the sampled token ID for indexing the vectors, too?</li>
</ol>
","bert"
"{
  ""id"": 129814,
  ""title"": ""Using multiple text inputs for one output with RoBERTa/DistillBERT""
}","Using multiple text inputs for one output with RoBERTa/DistillBERT","2024-07-28 22:32:05","","0","10","<bert><text-classification><text>","<p>In a current project i want to fine tune a RoBERTa/DistillBERT for text classification.</p>
<p>The model should take two text input features, limited to the length of around 280 characters, and generate a single output label.
However, using the provided base models by Huggingface, they seem to only accept a single text input feature.</p>
<p>I found <a href=""https://arxiv.org/pdf/1908.08345"" rel=""nofollow noreferrer"">this paper</a> and <a href=""https://discuss.huggingface.co/t/multiple-texts-as-inputs-to-transformers-models/2222/6"" rel=""nofollow noreferrer"">this forum post</a>, suggesting to concatenate the inputs and separate them by using a special token.</p>
<p>However, i wonder if there are better approaches, which do not use concatination, but instead keep the input features separated.</p>
","bert"
"{
  ""id"": 129657,
  ""title"": ""Best model for enforcing corporate naming conventions""
}","Best model for enforcing corporate naming conventions","2024-07-10 05:52:42","","0","26","<python><nlp><bert><fuzzy-classification>","<p>I'm working on a project (Python) to enforce the company naming convention of products on product lists provided by clients/suppliers. I'm having a list of company names (Standardised names) and those of external. I'm considering typos too - generating this list using GPT.</p>
<p>Here's are the models I'm considering:<br />
Sequence-to-Sequence (Seq2Seq): LTSM over RNN<br />
Transformer-Based Models: BERT on custom data</p>
<p>Additionally, I'm looking up fuzzy string matching.</p>
<p>Could anyone recommend other approaches, or if I'm missing something?
Greatly appreciated :)</p>
<p>Edit:<br />
This a sample of the dataset I'm dealing with. This is the correct names. I'm creating my own dataset with around 20 incorrect names alongside the correct ones. There are about 20k+ unique names.</p>
<p><a href=""https://i.sstatic.net/mdKt161D.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mdKt161D.png"" alt=""Dataset"" /></a></p>
","bert"
"{
  ""id"": 129640,
  ""title"": ""NLP model for word recovery (analogy to BERT, but letters)""
}","NLP model for word recovery (analogy to BERT, but letters)","2024-07-07 11:03:14","","0","23","<nlp><bert><llm>","<p>I am working on solving the problem of restoring words in text where some letters are missing. For example (restore words where vowels are removed): Hll wrld -&gt; Hello world n ltrntv ssssmnt sggsts -&gt; An alternative assessment suggests</p>
<p>Can you please suggest what pre-trained models I can use to solve this problem or maybe there are some articles that describe similar problems? I know it's similar to BERT, but BERT predicts the word, not the letters.</p>
","bert"
"{
  ""id"": 129580,
  ""title"": ""How can I make my Hugging Face fine-tuned model's config.json file reference a specific revision/commit from the original pretrained model?""
}","How can I make my Hugging Face fine-tuned model's config.json file reference a specific revision/commit from the original pretrained model?","2024-07-01 15:46:05","","0","18","<nlp><bert><huggingface><finetuning>","<p>I uploaded this model: <a href=""https://huggingface.co/pamessina/CXRFE"" rel=""nofollow noreferrer"">https://huggingface.co/pamessina/CXRFE</a>, which is a fine-tuned version of this model: <a href=""https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized</a></p>
<p>Unfortunately, CXR-BERT-specialized has this issue: <a href=""https://github.com/huggingface/transformers/issues/30412"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/issues/30412</a></p>
<p>I fixed the issue with this pull request: <a href=""https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized/discussions/5"" rel=""nofollow noreferrer"">https://huggingface.co/microsoft/BiomedVLP-CXR-BERT-specialized/discussions/5</a></p>
<p>However, when I save my fine-tuned model, the config.json file doesn't point to that specific pull request, pointing instead to the main branch by default, but the main branch of CXR-BERT-specialized has the aforementioned issue. As a consequence, when I try to use my model, it triggers the bug from the main branch of the underlying model, which shouldn't happen it were using the version from my pull request.</p>
<p>I've tried explicitly enforcing the revision I want like this:</p>
<pre><code>model = AutoModel.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized', revision=&quot;6cfc310817fb7d86762d888ced1e3709c57ac578&quot;, trust_remote_code=True)
...
model.save_pretrained(&quot;/home/pamessina/huggingface_models/CXRFE/&quot;)
</code></pre>
<p>But the config file that gets saved doesn't reference the desired revision:</p>
<pre><code>{
  &quot;_name_or_path&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized&quot;,
  &quot;architectures&quot;: [
    &quot;CXRBertModel&quot;
  ],
  &quot;attention_probs_dropout_prob&quot;: 0.25,
  &quot;auto_map&quot;: {
    &quot;AutoConfig&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized--configuration_cxrbert.CXRBertConfig&quot;,
    &quot;AutoModel&quot;: &quot;microsoft/BiomedVLP-CXR-BERT-specialized--modeling_cxrbert.CXRBertModel&quot;
  },
  &quot;classifier_dropout&quot;: null,
  &quot;gradient_checkpointing&quot;: false,
  &quot;hidden_act&quot;: &quot;gelu&quot;,
  &quot;hidden_dropout_prob&quot;: 0.25,
  &quot;hidden_size&quot;: 768,
  &quot;initializer_range&quot;: 0.02,
  &quot;intermediate_size&quot;: 3072,
  &quot;layer_norm_eps&quot;: 1e-12,
  &quot;max_position_embeddings&quot;: 512,
  &quot;model_type&quot;: &quot;cxr-bert&quot;,
  &quot;num_attention_heads&quot;: 12,
  &quot;num_hidden_layers&quot;: 12,
  &quot;pad_token_id&quot;: 0,
  &quot;position_embedding_type&quot;: &quot;absolute&quot;,
  &quot;projection_size&quot;: 128,
  &quot;torch_dtype&quot;: &quot;float32&quot;,
  &quot;transformers_version&quot;: &quot;4.41.2&quot;,
  &quot;type_vocab_size&quot;: 2,
  &quot;use_cache&quot;: true,
  &quot;vocab_size&quot;: 30522
}
</code></pre>
<p>And when I try to use my fine-tuned model from Hugging Face on Google Colab, I get this error:</p>
<p><a href=""https://i.sstatic.net/rUuXJTyk.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/rUuXJTyk.png"" alt=""enter image description here"" /></a></p>
<p>As  you can see, it's invoking the version associated with the commit id &quot;b59c09e51ab2410b24f4be214bbb49043fe63fc2&quot;, when instead it should be using the commit id &quot;6cfc310817fb7d86762d888ced1e3709c57ac578&quot; with my pull request that fixes the bug.</p>
<p>What can I do?</p>
<p>Thanks in advance.</p>
","bert"
"{
  ""id"": 129208,
  ""title"": ""Fine-tuning pretrained model on 2 tasks with 2 labeled dataset""
}","Fine-tuning pretrained model on 2 tasks with 2 labeled dataset","2024-05-27 16:05:43","","0","25","<classification><nlp><bert>","<p>I am having difficulty using BERT for a sentiment analysis task that handles both aspect-based sentiment analysis (ABSA) and comment sentiment analysis. I know that using two separate classification layers on top of the hidden layer can classify these two tasks, but I don't know how to handle labels for these two datasets. One dataset only has sentiment labels (positive, negative, and neutral), while the other includes 10 aspects with 4 sentiment labels (including NONE for aspects that are not present). I am wondering if I can assign a task type to the data and use it to identify which task is being trained on the model. Please let me know your thoughts on this problem. Thank you for your help! Have a good day!!!!!</p>
","bert"
"{
  ""id"": 128908,
  ""title"": ""How can self-attention be used to combine representations from long text?""
}","How can self-attention be used to combine representations from long text?","2024-04-30 18:42:49","","0","14","<python><nlp><bert><attention-mechanism>","<p>The paper &quot;<a href=""https://arxiv.org/abs/1905.05583"" rel=""nofollow noreferrer"">How to Fine-Tune BERT for Text Classification?</a>&quot; discusses using self-attention to combine the representations of a long input text that has been broken into chunks (section 5.3.1).</p>
<blockquote>
<p>The input text is firstly divided into k = L/510 fractions, which is fed into BERT to obtain the representation of the k text fractions. The representation of each fraction is the hidden state of the [CLS] tokens of the last layer. Then we use mean pooling, max pooling and self-attention to combine the representations of all the fractions.</p>
</blockquote>
<p>However, the paper does not go into detail on how this is actually done. Could anyone please explain how this is accomplished or provide examples that go into greater detail? I'm looking to implement this in Python alongside code that uses the Hugging Face transformers package.</p>
","bert"
"{
  ""id"": 128829,
  ""title"": ""Weird behaviour when using RobERTA for text classification""
}","Weird behaviour when using RobERTA for text classification","2024-04-24 06:09:59","","3","51","<classification><nlp><bert><text-classification>","<p>I have a dataset with around 70 classes and the dataset is largely balanced ~150 samples per class. I am finetuning RoBERTA-base for 4 epochs with a <code>{lr =5e-5, wd = 0.01, batch_size=32}</code>, so fairly standard hyperparameters.</p>
<p>Two of the classes in the dataset are &quot;increaseCreditLimit&quot; and &quot;decreaseCreditLimit&quot;, and the data in them is quite obviously different. But when I test the model on very straightforward utterances like &quot;increase my credit limit&quot;, &quot;increase my credit card limit&quot;, etc, the prediction is always &quot;decreaseCreditLimit&quot;.</p>
<p>A finetuned distilBERT gets all of them correct, so I know that there is nothing amiss in the dataset or the preprocessing. Can someone please shed some light on what could be wrong with the finetuning of the RoBERTa model?</p>
<p>If we look at the SHAP values, it seems that the RoBERTA tokenizer breaks the word &quot;increase&quot; into &quot;incre&quot; and &quot;ase&quot;. But the distilBERT tokenizer doesn't do so and the attribution for the word &quot;increase&quot; is very significant.</p>
<p><strong>RoBERTA SHAP values (Output 14 corresponds to the &quot;increaseCreditLimit&quot; class and Output 57 corresponds to &quot;decreaseCreditLimit&quot;</strong></p>
<p><a href=""https://i.sstatic.net/mLhUcVvD.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/mLhUcVvD.png"" alt=""RoBERTA SHAP values (Output 14 corresponds to the &quot;increaseCreditLimit&quot; class and Output 57 corresponds to &quot;decreaseCreditLimit&quot;)"" /></a></p>
<p><strong>DistilBERT SHAP values</strong></p>
<p><a href=""https://i.sstatic.net/3KbxtNQl.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/3KbxtNQl.png"" alt=""DistilBERT SHAP values"" /></a></p>
<p>Train val loss per epoch
<a href=""https://i.sstatic.net/8Emm92TK.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/8Emm92TK.png"" alt=""enter image description here"" /></a></p>
","bert"
"{
  ""id"": 128773,
  ""title"": ""Use text embeddings to map job descriptions to ESCO occupations""
}","Use text embeddings to map job descriptions to ESCO occupations","2024-04-19 15:07:53","","2","41","<transformer><bert><embeddings><llm><semantic-similarity>","<p>I'm trying to build a model to map job descriptions to <a href=""https://esco.ec.europa.eu/en/classification/occupation_main"" rel=""nofollow noreferrer"">ESCO occupations</a> which is a taxonomy for job titles. Every ESCO occupations have a title, a description and some essential skills.
Ideally I would have built a classification model but since I don't have labelled data that's out of the question.</p>
<p>So my idea was to generate text embeddings from every ESCO occupation and then for an input job description, and using cosine similarity, find the most similar ESCO occupation to that job description. I'm using this <a href=""https://huggingface.co/jjzha/esco-xlm-roberta-large"" rel=""nofollow noreferrer"">model</a> to generate the embeddings, which is an XLM-roBERTa which was pre-trained on job market data. I use the mean of the embeddings for every token as the job description's final embedding.
However the results are very bad, it fails to find most relevant ESCO occupations.</p>
<p>Here's how I compute the embeddings:</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&quot;jjzha/esco-xlm-roberta-large&quot;)
model = AutoModel.from_pretrained(&quot;jjzha/esco-xlm-roberta-large&quot;)

sample_job_title = &quot;We are looking for a junior software developer with experience in React and Python.&quot;
encoded_input = tokenizer(sample_job_title, padding=True, truncation=True, return_tensors=&quot;pt&quot;)
with torch.inference_mode():
    output = model(**encoded_input)
embedding = output.last_hidden_state.mean(dim=1)
</code></pre>
<p>From this output I retrieve output.last_hidden_state, which should correspond to the tokens embeddings, and then compute the mean embedding. This returns a pytorch tensor of shape (1, 1024). I have another tensor of shape (3007, 1024), <code>esco_embeddings</code> which corresponds to the embeddings for every 3007 ESCO occupation.
I then compute cosine similarity by doing:</p>
<pre class=""lang-py prettyprint-override""><code>similarities = torch.nn.functional.cosine_similarity(embedding, esco_embeddings, dim=1)
</code></pre>
<p>And find the k most similar ESCO occupations by computing</p>
<pre class=""lang-py prettyprint-override""><code>most_similar = torch.topk(similarities, k)
</code></pre>
<p>I thought that the problem might be in the embeddings itself, with XLM-roBERTa generating embeddings for every token and not one emebdding for the whole text.</p>
<p>Does anyone have an idea why that isn't working, and how it could be fixed? Maybe there's a better approach?</p>
<p>Thanks for the help.</p>
","bert"
"{
  ""id"": 128703,
  ""title"": ""Does Fine Tuning with Custom Label Build Upon the Capability of Zero Shot Classification or Does It Train from Scratch?""
}","Does Fine Tuning with Custom Label Build Upon the Capability of Zero Shot Classification or Does It Train from Scratch?","2024-04-15 09:30:38","","0","41","<nlp><bert><text-classification><huggingface><zero-shot-learning>","<p>The task is to classify email text bodies into exclusive categories like feedback, complaint etc. I have a labelled dataset available having about 350 samples.</p>
<p>I have tried the <code>facebook/bart-large-mnli</code> zero shot classification model where I passed the class names as possible label. It is already giving a decent performance.</p>
<p>Now, if I want to improve by using the existing labelled dataset and some model like <code>distilbert-base-uncased</code>, then will I lose the capability offered by the zero shot model altogether, and the new model will be trained entirely based on the labelled data?</p>
<p>I am afraid to go down the route because the number of labelled samples is so small, I feel it will fail to update the huge models having more weights than number of samples (we know the larger the model, the more samples you need).</p>
<p>So how do you guys address this concern, and how best to use the capabilities of the zero shot model on huggingface, while also using the labelled sample somehow?</p>
<p>I feel if I could just nudge the zero shot model a bit with the training samples, that would be the best possibility, but how to achieve it with huggingface?</p>
","bert"
"{
  ""id"": 128654,
  ""title"": ""Reducing emails token count preprocessing for Large Email Datasets - Feeding LLMs""
}","Reducing emails token count preprocessing for Large Email Datasets - Feeding LLMs","2024-04-11 19:41:27","128660","1","66","<nlp><data-cleaning><preprocessing><bert><llm>","<p>I have a large email dataset in .txt format and want to feed LLMs (like Gemini and ChatGPT) to provide answers based on email content.</p>
<p>The token count for my email data is very high (~1M for 1K emails), exceeding LLM token limits. Even after preprocessing the basic headers, there are still a lot of tokens with low information like email threads on the body of the email.</p>
<p>I'm considering the following models/approaches:</p>
<ol>
<li>Signature removal: This <a href=""https://huggingface.co/spaces/Jean-Baptiste/email_parser"" rel=""nofollow noreferrer"">hugging face bert project</a> does a good job, even if the model is trained in french.</li>
<li>Quoted Text Identification (for text inside the list of emails)</li>
<li>Stop Word Removal</li>
<li>BERT-Based Transformers for Focused Preprocessing</li>
<li>Summarization Transformers</li>
<li>Extractive Summarization</li>
</ol>
<p>The challenge is doing so in mixed language emails (parts in french, english and portuguese). Since this is a very common (emails or any text) is there any hugging face project to address most of these points?</p>
","bert"
"{
  ""id"": 128494,
  ""title"": ""Subsequence classification""
}","Subsequence classification","2024-03-29 13:15:33","","0","7","<nlp><bert>","<p>Given multiple paragraphs, is it possible to classify an entire paragraph while taking into account the surrounding paragraphs?</p>
<pre><code>Paragraph1

Paragraph2

Paragraph3
</code></pre>
<p>I am thinking of average pooling the token embeddings from a specific paragraph. This seems uncommon as I wasn't able to find examples of this online.</p>
<p>It is not quite the same as NER, since we know where the paragraph begins and ends. If using sequence classification on the paragraph, it wouldn't take into account the surrounding text.</p>
","bert"
"{
  ""id"": 128346,
  ""title"": ""Will hypermeters tuned on sampled dataset work for the whole dataset?""
}","Will hypermeters tuned on sampled dataset work for the whole dataset?","2024-03-18 12:53:15","","0","9","<nlp><bert><text-classification><hyperparameter><huggingface>","<p>I'm doing multi-label classification on text data using BERT model. Since the dataset is huge, around  50 thousand rows, I was thinking to use stratify sampling on dataset to reduce it to around 2-4 thousand to hyperparameter tune on.
I'm confused between trading off number of trails with size of sample set. Example: Would training 3000 rows with 5 trials will be better than training 1500 rows with 10 trials?
Moreover, thinking if I should drop epoch from tuning and focus on learning rate and weight decay.</p>
","bert"
"{
  ""id"": 128234,
  ""title"": ""Approach for Multi-class Classification of texts""
}","Approach for Multi-class Classification of texts","2024-03-10 08:46:57","","0","26","<nlp><word-embeddings><transformer><bert><text-classification>","<p>I'm trying to do a project where I have paragraphs and I need to classify them into multiple labels. The dataset is around 40k rows with labels.
I understand there is no one right approach but should I consider typical ML classifiers like embeddings + Logistic regression, xgboost etc.
Or should I directly consider fine tuning transformers like BERT,DistilBERT etc.</p>
<p>My priority is on getting accurate predictions and I have a few weeks to complete this.</p>
","bert"
"{
  ""id"": 128113,
  ""title"": ""How can I use contextual embeddings with BERT for sentiment analysis/classification""
}","How can I use contextual embeddings with BERT for sentiment analysis/classification","2024-03-01 22:34:09","128118","1","162","<nlp><word-embeddings><transformer><bert><embeddings>","<p>I have a BERT model which I want to use for sentiment analysis/classification. E.g. I have some tweets that need to get a POSITIVE,NEGATIVE or NEUTRAL label. I can't understand how contextual embeddings would help in a better model, practically.</p>
<p>I process the tweets and sentences to make them ready to be fed into the tokenizer. After I get every embedding as well as its mask, and feed it into a BERT model. From that BERT model I get some hidden states in return. As I understand it, now I have to also use a linear layer to take that 768 output from BERT and output a possibility for the 3 labels.</p>
<p><strong>How can contextual embeddings help me here?</strong> I get that we can use a combination of those hidden states/layers that we get for every sentence by the BERT model, and that helps us create better embeddings, which technically mean better models. But, after I follow some approach, e.g. summing the last four hidden states, or taking a mean of every token to create a token for each word, how do I proceed now? Do I need another model to take that embedding and output the labels that way (e.g. a linear layer but after the contextual embeddings are created)? Am I thinking of this the right way? Any input would be appreciated.</p>
","bert"
"{
  ""id"": 127010,
  ""title"": ""job title normalizer""
}","job title normalizer","2024-02-23 17:48:15","","0","99","<nlp><recommender-system><word2vec><bert>","<p>is there any way to normalize job titles using ml or nlp?</p>
<p>examples:</p>
<p>raw title:  UX/UI Engineers
normalized title:  Software Engineers</p>
<p>raw title:  UX/UI Designer
normalized title:  Graphic Designers</p>
<p>raw title:  .NET Developer
normalized title:  Web Developers</p>
<p>raw title:  Senior Android Engineer
normalized title:  Software Developers</p>
<p>raw title:  Jr ml engineer
normalized title:  ML Engineer</p>
","bert"
"{
  ""id"": 126861,
  ""title"": ""Is it methodologically correct to use the data to be used for finetuning in the pretrain phase of the BERT model?""
}","Is it methodologically correct to use the data to be used for finetuning in the pretrain phase of the BERT model?","2024-02-14 09:43:26","126873","1","35","<bert><text-classification><finetuning><methodology><pretraining>","<p>Let us assume the training of a BERT model.</p>
<p>An initial pre-train is performed with a large data set A.</p>
<p>Subsequently a finetuning is performed with a dataset B which is part of A, but now with labels of different classes for a text classification problem. This dataset B is split into the corresponding train, validation and test sets to obtain the classifier metrics.</p>
<p>Is this methodologically correct?</p>
<p>In a way, in the pre-train phase, the data used to validate the model has already been &quot;seen&quot;, so in my opinion, the model is not being evaluated on &quot;new&quot; data and is somehow &quot;cheating&quot;.</p>
<p>Elsewhere I have seen that the argument for using this data is that the training target in the pretrain phase and the finetuning phase are different and, more importantly, in the pretrain phase there are no labels.</p>
<p>Could someone confirm if it would be better not to use the finetuning data in the initial pretrain (whether train, evaluation or test)?</p>
","bert"
"{
  ""id"": 126601,
  ""title"": ""Why the standard deviation of the BERT weight initialization is 0.02 by default""
}","Why the standard deviation of the BERT weight initialization is 0.02 by default","2024-01-26 04:54:32","126616","0","79","<transformer><bert><variance><weight-initialization>","<p>The purpose of weight initialization in the neural network is to keep the variance of calculation output in the layers to 1.0, and it depends on the calculations involved in the layers.</p>
<p>Initializing weight <code>W</code> with <a href=""https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"" rel=""nofollow noreferrer"">Xavier initialization</a> for  Matrix Multiplication <code>X@W.T</code> at Self Attention in <a href=""https://arxiv.org/pdf/1706.03762.pdf"" rel=""nofollow noreferrer"">Transformer Architecture</a> will use the standard deviation <span class=""math-container"">$\frac{1}{\sqrt{D}}$</span> to sample values from <span class=""math-container"">$N(\mu=0,\sigma=\frac{1}{\sqrt{D}})$</span> so that the <a href=""https://stats.stackexchange.com/a/52699/105137"">product has variance 1.0</a>, providing the dimensions of X and W are both <code>D</code> and X follows the normal distribution.</p>
<p>The dimension <code>D</code> of Transformer based BERT is <code>768</code>, so <span class=""math-container"">$\sigma$</span> is expected to be <code>0.036</code>. But BertConfig says it is using <code>0.02</code>. Where is <code>0.02</code> coming from?</p>
<ul>
<li><a href=""https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertConfig"" rel=""nofollow noreferrer"">BertConfig</a></li>
</ul>
<blockquote>
<p>initializer_range (float, optional, <strong>defaults to 0.02</strong>) — The standard deviation of the truncated_normal_initializer for initializing all weight matrices.</p>
</blockquote>
","bert"
"{
  ""id"": 126142,
  ""title"": ""The using of golden dataset in Augmented SBERT Training""
}","The using of golden dataset in Augmented SBERT Training","2023-12-21 19:43:34","","0","16","<nlp><training><transformer><bert><autoencoder>","<p>I use the training strategy of <a href=""https://www.sbert.net/examples/training/data_augmentation/README.html"" rel=""nofollow noreferrer"">Augmented SBERT (Domain-Transfer)</a>. In the code example they use the golden-dataset (STSb) for the training evaluator. Here two code snippets of the <a href=""https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/data_augmentation/train_sts_indomain_semantic.py"" rel=""nofollow noreferrer"">example of sentence-transformers</a>:</p>
<p><em>Get data and split them</em></p>
<pre class=""lang-py prettyprint-override""><code>gold_samples = []
dev_samples = []
test_samples = []

with gzip.open(sts_dataset_path, 'rt', encoding='utf8') as fIn:
    reader = csv.DictReader(fIn, delimiter='\t', quoting=csv.QUOTE_NONE)
    for row in reader:
        score = float(row['score']) / 5.0  # Normalize score to range 0 ... 1

        if row['split'] == 'dev':
            dev_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))
        elif row['split'] == 'test':
            test_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))
        else:
            #As we want to get symmetric scores, i.e. CrossEncoder(A,B) = CrossEncoder(B,A), we pass both combinations to the train set
            gold_samples.append(InputExample(texts=[row['sentence1'], row['sentence2']], label=score))
            gold_samples.append(InputExample(texts=[row['sentence2'], row['sentence1']], label=score))
</code></pre>
<p><em>Initialize evaluator and fit model</em></p>
<pre class=""lang-py prettyprint-override""><code>logging.info(&quot;Read STSbenchmark dev dataset&quot;)
evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')

# Configure the training.
warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up
logging.info(&quot;Warmup-steps: {}&quot;.format(warmup_steps))

# Train the bi-encoder model
bi_encoder.fit(train_objectives=[(train_dataloader, train_loss)],
          evaluator=evaluator,
          epochs=num_epochs,
          evaluation_steps=1000,
          warmup_steps=warmup_steps,
          output_path=bi_encoder_path
          )
</code></pre>
<p><strong>First Question: Why is the golden-dataset used for the evaluation, if the model fits on the silver-dataset?</strong></p>
<p>Further, the <code>test_sample</code> from the golden dataset is used for the final analysis:</p>
<pre class=""lang-py prettyprint-override""><code># load the stored augmented-sbert model
bi_encoder = SentenceTransformer(bi_encoder_path)
test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')
test_evaluator(bi_encoder, output_path=bi_encoder_path)
</code></pre>
<p><strong>Second Question: Why is the <code>test_sample</code> based on the golden-dataset? Why is the <code>test_sample</code> not based on the silver dataset?</strong></p>
","bert"
"{
  ""id"": 126140,
  ""title"": ""Interpretation of Evaluation Values of Augmented SBERT Training with EmbeddingSimilarityEvaluator()""
}","Interpretation of Evaluation Values of Augmented SBERT Training with EmbeddingSimilarityEvaluator()","2023-12-21 18:43:11","","1","38","<training><transformer><bert><similarity><domain-adaptation>","<p>I train a BI-Encoder to get an Augmented SBERT and I get a final training result.</p>
<p>How can I interpret the following output of the final training result?</p>
<pre><code>EmbeddingSimilarityEvaluator: Evaluating the model on test dataset:
Cosine-Similarity : Pearson: 0.8115 Spearman: 0.7777      
Manhattan-Distance: Pearson: 0.7318 Spearman: 0.6822      
Euclidean-Distance: Pearson: 0.7332 Spearman: 0.6835      
Dot-Product-Similarity: Pearson: 0.7780 Spearman: 0.7543

0.7777387754875323 # output of test_evaluator(...)
</code></pre>
<p>The output result out of the following code snipped:</p>
<pre class=""lang-py prettyprint-override""><code># load the stored augmented-sbert model
bi_encoder = SentenceTransformer(bi_encoder_path)
test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')
test_evaluator(bi_encoder, output_path=bi_encoder_path)
</code></pre>
<p>Is a high or low Person resp. Spearman better? They give information about the correlation.</p>
","bert"
"{
  ""id"": 125004,
  ""title"": ""Explanation : Simpler models beat BERT base""
}","Explanation : Simpler models beat BERT base","2023-12-11 09:20:38","","0","31","<nlp><bert><text-classification>","<p>I have been trying to train different models for a multi-class classification task of texts. My data set consists of rows of text and its label. The texts are short sentences.
I tried the following models :
after generating embeddings using distilbert-base-uncased i trained :</p>
<ul>
<li>Random Forest Classifier :F1-Score: 0.59</li>
<li>SVM :F1-Score: 0.63</li>
<li>Gradient Boosting Classifier:F1-Score: 0.65</li>
<li>MLPClassifier:F1-Score: 0.68</li>
</ul>
<p>then :</p>
<ul>
<li>-BERT base :F1-Score: 0.40</li>
</ul>
<p>Since BERT is supposed to be the most powerful model for this use case I was surprised with the result, I was trying to find an explanation and hypothesis for that. So has anyone faced similar results before . what do you think may be the reason?</p>
","bert"
"{
  ""id"": 124780,
  ""title"": ""Assign layers and weights in BERT""
}","Assign layers and weights in BERT","2023-11-27 06:42:24","","0","38","<pytorch><transformer><bert><language-model>","<p>I print the weight names and shape of the BERT transformer. Now, I want to assign the printed weight to the layers in the transformers architecture:
<a href=""https://i.sstatic.net/Dtaun.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Dtaun.png"" alt=""enter image description here"" /></a></p>
<p>In the following, I can assign query, key and value:</p>
<p><a href=""https://i.sstatic.net/wfHeW.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wfHeW.png"" alt=""enter image description here"" /></a></p>
<p>But, in the print there are attention.output.dense.weight and attention.output.LayerNorm.weight, where can I find it in the architecture of transformer/attention-head?</p>
<p>Further there is an intermediate.dense.weight and there output.dense.weight and output.LayerNorm.weight. Are they parts of &quot;Add &amp; Norm&quot; after the multi-head-attention blocks?</p>
","bert"
"{
  ""id"": 124726,
  ""title"": ""What happens when I set is_decoder to True in the bert API from huggingface?""
}","What happens when I set is_decoder to True in the bert API from huggingface?","2023-11-23 18:14:29","","0","87","<transformer><bert><huggingface>","<p>Please help me understand the implications of initialising the bert model from huggingface with <code>is_decoder</code> parameter set to <code>True</code></p>
<p>According to the <a href=""https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#bertmodel:%7E:text=The%20model%20can,the%20forward%20pass."" rel=""nofollow noreferrer"">documentation</a>:</p>
<blockquote>
<p>The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of cross-attention is added between the self-attention layers, following the architecture described in Attention is all you need by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.
To behave as an decoder the model needs to be initialized with the is_decoder argument of the configuration set to True; an encoder_hidden_states is expected as an input to the forward pass.</p>
</blockquote>
<p>I know how Bert is just the encoder and there is no causal masking which is the hallmark of a decoder. Can you please me understand how <code>is_decoder</code> changes the implementation of the bert to make it usable as a decoder?</p>
","bert"
"{
  ""id"": 124577,
  ""title"": ""How does Bert masked language modelling task make sense if half the time the next sentence is wrong context in the sequence passed through the encoder""
}","How does Bert masked language modelling task make sense if half the time the next sentence is wrong context in the sequence passed through the encoder","2023-11-14 07:42:17","124578","2","115","<nlp><word-embeddings><transformer><bert>","<p>Bert has two types of tasks that it uses to learn contextual word embeddings:</p>
<ol>
<li>Masked word prediction</li>
<li>Next sentence prediction</li>
</ol>
<p>I have read the <a href=""https://arxiv.org/abs/1810.04805"" rel=""nofollow noreferrer"">paper</a> and even there the training details are a little fuzzy or, dont make sense to me</p>
<p>To quote the paper:</p>
<blockquote>
<p>To generate each training input sequence, we sample two spans of text from the corpus, which we
refer to as “sentences” even though they are typically much longer than single sentences (but can
be shorter also). The first sentence receives the A
embedding and the second receives the B embedding. 50% of the time B is the actual next sentence
that follows A and 50% of the time it is a random
sentence, which is done for the “next sentence prediction” task. They are sampled such that the combined length is ≤ 512 tokens. The LM masking is
applied after WordPiece tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces.</p>
</blockquote>
<p>Question:
If 50% of our training examples are sentence (in Bert sense) followed by another unrelated sentence and we run it as one sequence through the transformer and the objective is to find context-specific embeddings then how does the objective of masked language modelling make sense in these cases? Our second sentence is the wrong context here. Masked language modelling makes sense only if the second sentence is the accurate entailment which is the case only in 50% of the cases</p>
","bert"
"{
  ""id"": 124530,
  ""title"": ""F1 and Exact-Match (EM) Score in Extractive QA NLP""
}","F1 and Exact-Match (EM) Score in Extractive QA NLP","2023-11-10 15:51:39","","0","59","<nlp><bert><huggingface><llm><question-answering>","<p>I have a question as to how the F1 should be calculated in NLP and whether the text normalization is optional or not.</p>
<p>So I have been working on a project where we created a closed-domain extractive QA dataset from scratch, and we are trying to finetune and assess the performance of several LLMs in this new dataset. I came across different definitions of the F1 score, sometimes with text normalization (<a href=""https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#Exact-Match"" rel=""nofollow noreferrer"">like in here</a>) and sometimes not. I have run all my experiments without the normalization step for both the EM and F1 scores. Should I rerun all experiments?</p>
<p>Is the</p>
","bert"
"{
  ""id"": 124273,
  ""title"": ""TFRobertaSequenceClassification for Address Normalization task""
}","TFRobertaSequenceClassification for Address Normalization task","2023-10-26 10:34:07","","0","26","<nlp><bert><normalization><language-model><finetuning>","<p>I have dataset with two column: one with faulty addresses, and other with correct addresses. I want to train a model such that, I can use it later for correcting all the incoming faulty addresses.
I have done tokenization, data splitting task for the same, but I can't make my model to start training.</p>
<p>I get GraphExecution error, which points towrads mismatch of dimension.</p>
<p>Exact error:
<strong>logits and labels must have the same first dimension, got logits shape [16,55] and labels shape [880]</strong></p>
<p>I don't know from where these values are coming from, as my X_Train consists of list with each list being length of <strong>45</strong>, and y_train is a list of length <strong>55.</strong> (All are integer values)</p>
<p>As, I am new to this task, any suggestions, comments, concerns, questions are welcome.</p>
<p>Also, I have tried basic ML approach, but the results were quite poor. Hence, please suggest me on the line of this approach only.</p>
<p>Here are some specifications to my approach:</p>
<pre><code>Tokenization: RobertaTokenizer.from_pretrained(&quot;roberta-base&quot;)
Model: TFRobertaForSequenceClassification.from_pretrained(&quot;roberta-base&quot;)
optimizer: Adam
learning_rate = 1e-5
loss: SparseCategoricalCrossentropy
epochs: 10
batch_size: 16
</code></pre>
<p>I think following are the list of models which we can use:</p>
<p><a href=""https://i.sstatic.net/Shp81.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/Shp81.png"" alt=""enter image description here"" /></a></p>
<p>github link used: <a href=""https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/__init__.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/src/transformers/models/roberta/__init__.py</a></p>
<p>TIA</p>
","bert"
"{
  ""id"": 124235,
  ""title"": ""How can I avoid the irrelevant number of sentences in the result?""
}","How can I avoid the irrelevant number of sentences in the result?","2023-10-23 20:09:42","","0","44","<nlp><bert><semantic-similarity>","<p>The nature of the data I have is not arranged; however, I'm trying to extract the appropriate sentences for each query as a sample for ground truth. Also, the most critical problem is that I use the BERT model, which searches for the top 500 sentences in which the similarity value is greater than the 0.5 threshold. So the result is disorganized. For example, I got for a query</p>
<pre><code>5 for the relevant, 495 for the irrelevant, and 6 for the Total relevant
</code></pre>
<p>How can I avoid the irrelevant number of sentences in the result?</p>
","bert"
"{
  ""id"": 124155,
  ""title"": ""How to use Bertweet model for topic modeling""
}","How to use Bertweet model for topic modeling","2023-10-17 13:28:48","","0","70","<python><pytorch><bert><topic-model><huggingface>","<p>The problem is implementation of Bertweet in a topic-modeling project with understandable output like <a href=""https://maartengr.github.io/BERTopic/api/bertopic.html#bertopic._bertopic.BERTopic.fit_transform"" rel=""nofollow noreferrer"">BERTopic</a>, i want to use it on a relatively large (20k tweets) unlabelled dataset to segment it into topics, number of which is either user-specified or pre-defined by the model.</p>
<p>I've read a <a href=""https://huggingface.co/docs/transformers/model_doc/bertweet"" rel=""nofollow noreferrer"">documentation</a> of Bertweet and it's too short to be cohesive for a someone like me without previous experience with neural networks and transformers.
(For safety here's the whole example code from the link above)</p>
<pre><code>import torch
from transformers import AutoModel, AutoTokenizer

bertweet = AutoModel.from_pretrained(&quot;vinai/bertweet-base&quot;)

# For transformers v4.x+:
tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/bertweet-base&quot;, use_fast=False)

# For transformers v3.x:
# tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/bertweet-base&quot;)

# INPUT TWEET IS ALREADY NORMALIZED!
line = &quot;SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:&quot;

input_ids = torch.tensor([tokenizer.encode(line)])

with torch.no_grad():
    features = bertweet(input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
# from transformers import TFAutoModel
# bertweet = TFAutoModel.from_pretrained(&quot;vinai/bertweet-base&quot;)
</code></pre>
<p>From documentation example the output is:</p>
<pre><code>with torch.no_grad():
    features = bertweet(input_ids)
</code></pre>
<p>which is <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</code>
class, that contains attributes and information i don't understand.</p>
<p>And few not less important questions:</p>
<ol>
<li>Is this suitable for unsupervised learning tasks like
topic-modeling?</li>
<li>How to unpack this class to a format similar to
<strong>document - assigned_topic</strong>?</li>
</ol>
","bert"
"{
  ""id"": 124034,
  ""title"": ""Training model using BERT""
}","Training model using BERT","2023-10-07 16:46:44","124035","1","174","<nlp><data-science-model><bert><finetuning>","<p>I have generated dataset using chat gpt. Dataset has 9000 data recodes. It's 6 class sentiment analysis. classes are 0,1,2,3,4,5
I used 3000 recodes for training, 1200 recods for validation and testing.</p>
<p>This is the class counts</p>
<p>For training:</p>
<pre><code>5: 622 ,3: 614 ,0: 593,4: 571,2: 604,1: 596
</code></pre>
<p>For Validation:</p>
<pre><code>1: 221,5: 193,0: 193,4: 212,3: 182,2: 199
</code></pre>
<p>For Testing:</p>
<pre><code>3: 204,2: 197,0: 214,4: 217,1: 183,5: 185
</code></pre>
<p>I trained this using BERT('bert-base-uncased') with 25 epochs. Learning rate 2e-5</p>
<p>This is the result.</p>
<p><a href=""https://i.sstatic.net/acDCp.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/acDCp.png"" alt=""enter image description here"" /></a></p>
<p>Validation</p>
<pre><code>Validation Accuracy: 0.5666666666666667
Validation Classification Report:
              precision    recall  f1-score   support

           0       0.53      0.69      0.60       193
           1       0.54      0.50      0.52       221
           2       0.52      0.49      0.51       199
           3       0.54      0.50      0.52       182
           4       0.55      0.60      0.57       212
           5       0.77      0.63      0.69       193

    accuracy                           0.57      1200
   macro avg       0.58      0.57      0.57      1200
weighted avg       0.57      0.57      0.57      1200
</code></pre>
<p>Testing</p>
<pre><code>Test Accuracy: 0.5658333333333333
Test Classification Report:
              precision    recall  f1-score   support

           0       0.57      0.68      0.62       214
           1       0.49      0.56      0.52       183
           2       0.58      0.52      0.55       197
           3       0.59      0.49      0.53       204
           4       0.53      0.57      0.55       217
           5       0.68      0.58      0.62       185

    accuracy                           0.57      1200
   macro avg       0.57      0.56      0.57      1200
weighted avg       0.57      0.57      0.57      1200
</code></pre>
<p>I trained this with using different count of data. but graph shape is same with different accuracies. My questions are:</p>
<ol>
<li>how to increase accuracy and what whould be the issue?</li>
<li>Is that a issue with some words in multiple classes (because i some words in many classes in this dataset)?</li>
</ol>
","bert"
"{
  ""id"": 123919,
  ""title"": ""Help understanding working of KeyBERT for keyphrase extraction""
}","Help understanding working of KeyBERT for keyphrase extraction","2023-09-30 07:20:08","","0","98","<deep-learning><nlp><bert><tokenization><document-understanding>","<p>I'm fairly new to reading and understanding research papers, so I wanted to get a second opinion on whether my understanding of KeyBERT was correct. Here is a high level overview of my understanding with an example -</p>
<ul>
<li>example sentence - “I like reading. I enjoy swimming. Reading is fun.”</li>
<li>let stop_words = None, ngram_range = (2, 2) that means bigrams will be considered, let number of candidate words we want to return ( top_n ) =  2</li>
<li>steps -
<ul>
<li>sentences will be split into unique bigrams ( 2 words ) - [ ‘i like’, ‘like reading’, ‘i enjoy’, ‘enjoy swimming’, ‘reading is’, ‘is fun’ ]</li>
<li>this list is passed to BERT for embedding. BERT will encode each element of the list and return a list of vectors v1,...,v6</li>
<li>The document is also passed to BERT for embedding, and a vector v7 representing the document is returned.</li>
<li>if seed_keywords, i.e. words that should be given more importance are given as a parameter, they are also embedded into vectors, and mean, say c1, of those vectors is taken. This vector c1 is then added to document vector v7.</li>
<li>it computes a similarity score between each of the candidate embedding vectors v1,v2,...,v6 with the document embedding vector v7. it uses cosine similarity for this.</li>
<li>it sorts the candidate vectors in descending order by similarity score, and returns the top 2 ( top_n ) vectors</li>
</ul>
</li>
</ul>
<p>Let me know if I'm missing any details. Also, are there any better alternatives for the task of keyphrase extraction for specific domain?</p>
","bert"
"{
  ""id"": 123883,
  ""title"": ""Combining Textual, Categorical and Numerical data for Semantic Search using SentenceTransformers model""
}","Combining Textual, Categorical and Numerical data for Semantic Search using SentenceTransformers model","2023-09-28 00:50:48","","0","67","<nlp><categorical-data><bert><embeddings><search>","<p>I'm building a food semantic search model and I want to use a pre-trained <a href=""https://www.sbert.net/examples/applications/semantic-search/README.html"" rel=""nofollow noreferrer"">SentenceTransformers</a> model with cosine similarity. I'm using <a href=""https://www.kaggle.com/datasets/hugodarwood/epirecipes/data?select=full_format_recipes.json"" rel=""nofollow noreferrer"">Epicurious</a> dataset for the corpus which consists of textual (&quot;title&quot;, &quot;description&quot;, &quot;directions&quot;) as well as categorical (&quot;categories&quot;) and numerical data (&quot;calories&quot;, &quot;fat&quot;, &quot;sodium&quot;).</p>
<p>The model that computes embeddings for the corpus takes <code>String</code> data as an input. So my idea was to combine the categorical and numerical data with textual data in a single <code>String</code>.</p>
<p>Do you think it's a right way to handle categorical data? If yes, what are the ways to concatenate such data (maybe using column names with the values or adding some tags)? Otherwise, what is a better way to handle such data?</p>
","bert"
"{
  ""id"": 123867,
  ""title"": ""How to deal with short text data using NLP models?""
}","How to deal with short text data using NLP models?","2023-09-27 02:41:01","","0","24","<nlp><bert>","<p>Now I want to use my own domain data to train NLP model like BERT. The following is the details of my data:</p>
<ol>
<li>data length distribution: over 70% of my data has the length shorter than 5 and the largest length is 14;</li>
<li>data format: the data is a list of number representing the AS-PATH of BGP announcement.</li>
</ol>
<p>I've tried to use HuggingFace transformer package to define BERT model and train, but the MLM loss is quite high. I think this is the cause of data length.</p>
<p>So I want to know that is there any way to deal with these short data?</p>
","bert"
"{
  ""id"": 123053,
  ""title"": ""Why does everyone use BERT in research instead of LLAMA or GPT or PaLM, etc?""
}","Why does everyone use BERT in research instead of LLAMA or GPT or PaLM, etc?","2023-08-03 01:11:29","123060","9","12125","<nlp><bert><language-model><gpt><research>","<p>It could be that I'm misunderstanding the problems space and the iterations of LLAMA, GPT, and PaLM are all based on BERT like many language models are, but every time I see a new paper in improving language models it takes BERT as a based an adds some kind of fine-tuning or filtering or something. I don't understand why BERT became the default in research circles when all anyone hears about publicly is GPT-2,3,4 or more recently LLAMA-2. I have a feeling it has something to do with BERT being open-source, but that can't be the whole story. This question might not be specific enough, please let me know. Thanks.</p>
","bert"
"{
  ""id"": 123020,
  ""title"": ""Classification errors on 'bert-base-uncased' text classifier""
}","Classification errors on 'bert-base-uncased' text classifier","2023-07-31 11:07:49","","0","57","<nlp><multiclass-classification><bert><text-classification>","<p>Disclaimer : This is a long question, please be patient. Thanks in advance</p>
<p>I am using <em>bert-base-uncased</em> for text-classification. I have 11 classes, and the classification is happening alright for most of the classes. But of these 11 classes there are three classes, say <strong>A, B and C</strong>. Where there are high misclassification errors. I wish to reduce the errors between these classes.</p>
<p>Current State of my model :</p>
<ol>
<li>Model used Hugging Face <em>bert-base-uncased</em>.</li>
<li>Loss function : Weighted Cross Entropy where the weights represent the inverse of the fraction of each class in the data.</li>
<li>The text data related to classes A, B and C are not unbalanced and are roughly comparable to the most populous class</li>
</ol>
<p>My Questions :</p>
<ol>
<li>Can anyone say why is this occuring?</li>
<li>I am thinking of using some-other loss function specifically for these three classes, say soft-F1 from torchmetrics. The idea is that nn.Cross_Entropy() will be used for all classes and apart from that I will use soft-F1 when the true_label belongs to these three classes.</li>
</ol>
<p>Thus the final loss function will be <code>loss = frac * $nn.Cross_entropy() + (1-frac) * torchmetrics.soft_f1(if true_label in [A, B, C])</code>, where <code>frac</code> is an hyper-parameter.</p>
<p>Will this approach work, or should I use something else ?</p>
","bert"
"{
  ""id"": 122825,
  ""title"": ""building embeddings for Phrases from scratch""
}","building embeddings for Phrases from scratch","2023-07-19 08:47:17","","0","73","<word-embeddings><word2vec><bert><terminology><document-term-matrix>","<p>I have a datadet with many phrases which I would like to embed them from scratch. I dont want the cosine of the words in order to get a phrase embedding, this is because the phrases may appear in a different enviroment and I want to embed the two words together, or the tree words together in their own envoroment.</p>
<p>Is this possible?</p>
<p>If yes how exactly?</p>
<p>Thank You in advance.</p>
","bert"
"{
  ""id"": 122783,
  ""title"": ""Word embeddings""
}","Word embeddings","2023-07-17 08:58:13","122796","0","62","<machine-learning><machine-learning-model><word-embeddings><word2vec><bert>","<p>I m looking into word embedding and I would like to ask if I could train words or sentences in two layers. And if I wanted that one layer is more important, how could I calculate it? For example context should be more important than grammar or syntax and positioning should have another weight. How could I express it?</p>
<p>Thank You in advance.</p>
","bert"
"{
  ""id"": 122749,
  ""title"": ""BERT is a supervised learning or semi-supervised learning?""
}","BERT is a supervised learning or semi-supervised learning?","2023-07-14 20:21:12","122750","0","118","<bert>","<p>I use 'bert-base-cased' pre-trained model for encoding a dataset of text that was labeled to labels 0, 1. Then the encoded dataset is trained using BERT model imported from Transformer library. Does it supervised learning or semi-supervised?</p>
","bert"
"{
  ""id"": 122539,
  ""title"": ""Can bert uncased predict text classification on foreign data?""
}","Can bert uncased predict text classification on foreign data?","2023-07-03 21:50:47","122541","0","483","<nlp><dataset><bert>","<p>I am trying to do the fake news/real news classification and used a pre-trained bert uncased model as transfer learning and it gave a solid 81% accuracy. But the problem is while doing sanity checks, I found my dataset has some Korean/Chinese text articles and these are some real news and it gave the trustworthy score(basically probability) as 60-70%. If Bert-uncased is only for the English language, I'm just thinking about how it processes those languages. Does anyone have any insights?</p>
","bert"
"{
  ""id"": 122267,
  ""title"": ""Building BERT tokenizer with custom data""
}","Building BERT tokenizer with custom data","2023-06-20 13:10:57","122268","1","379","<bert>","<p>I'm wondering if there is a way to train our own Bert tokenizer instead of using pre-trained tokenizer provided by huggingface?</p>
","bert"
"{
  ""id"": 122220,
  ""title"": ""what is the difference between NSP and text prediction""
}","what is the difference between NSP and text prediction","2023-06-17 17:02:37","","0","278","<machine-learning><deep-learning><nlp><transformer><bert>","<p>In BERT, NSP (Next Sentence Prediction) is for predicting next sentence based on context and Text prediction task is also for predicting next word or phrases.</p>
<p>So, both are for predicting next sentence or word/ phrase only and both are BERT NLP tasks, then why these two?</p>
","bert"
"{
  ""id"": 121943,
  ""title"": ""I can't get good performance from BERT""
}","I can't get good performance from BERT","2023-06-02 23:09:53","","1","57","<keras><nlp><word-embeddings><bert><recurrent-neural-network>","<p>I trained NLP models. This is a subset (200 instances) of my data set of 10,000 instances:<a href=""https://pastebin.com/FThmWXeE"" rel=""nofollow noreferrer"">This the link of the dataset on pastebin</a></p>
<p>I compare an LSTM model with a glove model and a BERT model. I expected a good performance with BERT. I can't get past 20% accuracy with BERT at all. I wonder what I'm missing in its implementation.</p>
<pre><code>!pip list
#tensorflow 2.12.0
!python --version
#python 3.10.11

import json 
import tensorflow as tf
import numpy as np
from hyperopt import Trials, STATUS_OK, tpe
from sklearn.model_selection import train_test_split
from keras.layers import Input
from sklearn.metrics import accuracy_score
import pandas as pd

# Reading of file
f = open ('sampled_data.json', &quot;r&quot;)
data = json.loads(f.read())
</code></pre>
<h1>Data preprocessing</h1>
<pre><code>X=[x[&quot;title&quot;].lower() for x in data]
y=[x[&quot;categories&quot;][0].lower() for x in data]
X_train,X_test,y_train,y_test= train_test_split(X,y, test_size=0.2, 
                                                random_state=42)
</code></pre>
<h3>target preprocessing. To consider the category unknown if not seen in test set</h3>
<pre><code>cat_to_id={'&lt;UNK&gt;':'0'}
for cat in y_train:
  if cat not in cat_to_id:
    cat_to_id[cat]=len(cat_to_id)

#MAPPING WITH RESPECT TO THE TRAINING SET
id_to_cat={v:k for k,v in cat_to_id.items()}
def preprocess_Y(Y,cat_to_id):
  res=[]
  for ex in Y:
    if ex not in cat_to_id.keys():
      res.append(cat_to_id['&lt;UNK&gt;'])
    else: 
      res.append(cat_to_id[ex])
  return np.array(res)

y_train_id=preprocess_Y(y_train,cat_to_id)
y_test_id=preprocess_Y(y_test,cat_to_id)
y_test_id=y_test_id.astype(float)

# Tokenization of of features
tokenizer=tf.keras.preprocessing.text.Tokenizer(num_words=10000)
tokenizer.fit_on_texts(X_train)

# TEXT TO SEQUENCE
X_train_seq=tokenizer.texts_to_sequences(X_train)
X_test_seq=tokenizer.texts_to_sequences(X_test)

#PADDING pad_sequences function transform in array
max_len=max([len(length) for length in X_train_seq])
X_train_pad= tf.keras.preprocessing.sequence.pad_sequences(X_train_seq,maxlen=max_len, truncating='post')
X_test_pad= tf.keras.preprocessing.sequence.pad_sequences(X_test_seq,maxlen=max_len, truncating='post')


####### RECCURRENT NEURAL NETWORK###############

vocab_size=len(tokenizer.word_index)
Embed_dim=300
dropout=0.2
dense_size=128
num_cat=len(cat_to_id)
batch_size=16
epochs=15

### CREER LE MODELE
model_rnn=tf.keras.models.Sequential()

# Add an embedding layer
model_rnn.add(tf.keras.layers.Embedding(input_dim=vocab_size, 
                                        output_dim=Embed_dim, 
                                        input_length=max_len))

# Add an LSTM layer
model_rnn.add(tf.keras.layers.LSTM(units=128))
model_rnn.add(tf.keras.layers.Dropout(0.4))

# Dense + activation
model_rnn.add(tf.keras.layers.Dense(units=dense_size,activation='relu'))
#Classifieur + activation
model_rnn.add(tf.keras.layers.Dense(units=num_cat,activation='softmax'))
print(model_rnn.summary())

model_rnn.compile(loss= 'sparse_categorical_crossentropy',
                  optimizer='adam',
                  metrics='accuracy')

model_rnn.fit(X_train_pad,y_train_id, batch_size=batch_size, epochs=epochs)

model_rnn.evaluate(X_test_pad, y_test_id)


Epoch 1/15
9/9 [==============================] - 5s 166ms/step - loss: 3.6699 - accuracy: 0.1643
Epoch 2/15
9/9 [==============================] - 1s 128ms/step - loss: 3.3861 - accuracy: 0.2286
Epoch 3/15
9/9 [==============================] - 1s 157ms/step - loss: 3.1313 - accuracy: 0.2357
Epoch 4/15
9/9 [==============================] - 1s 88ms/step - loss: 3.0774 - accuracy: 0.2286
Epoch 5/15
9/9 [==============================] - 1s 127ms/step - loss: 3.0358 - accuracy: 0.2286
Epoch 6/15
9/9 [==============================] - 0s 27ms/step - loss: 2.9461 - accuracy: 0.2286
Epoch 7/15
9/9 [==============================] - 0s 27ms/step - loss: 2.7970 - accuracy: 0.2357
Epoch 8/15
9/9 [==============================] - 1s 75ms/step - loss: 2.5048 - accuracy: 0.2429
Epoch 9/15
9/9 [==============================] - 1s 86ms/step - loss: 2.2543 - accuracy: 0.3357
Epoch 10/15
9/9 [==============================] - 1s 47ms/step - loss: 1.9985 - accuracy: 0.4357
Epoch 11/15
9/9 [==============================] - 0s 39ms/step - loss: 1.7728 - accuracy: 0.4929
Epoch 12/15
9/9 [==============================] - 1s 41ms/step - loss: 1.5552 - accuracy: 0.5929
Epoch 13/15
9/9 [==============================] - 0s 11ms/step - loss: 1.3320 - accuracy: 0.5929
Epoch 14/15
9/9 [==============================] - 0s 11ms/step - loss: 1.1506 - accuracy: 0.6786
Epoch 15/15
9/9 [==============================] - 0s 42ms/step - loss: 0.9498 - accuracy: 0.7714
2/2 [==============================] - 1s 13ms/step - loss: 6.6335 - accuracy: 0.2000



############# MODEL WITH GLOVE###################
embeddings_index = {}
f = open('glove.6B.300d.txt', encoding='utf-8')
for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

# Create embedding matrix
word_index=tokenizer.word_index
num_words = len(word_index) + 1
embedding_dim = 300
embedding_matrix = np.zeros((num_words, embedding_dim))
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector


    num_words=len(tokenizer.word_index)+1
embedding_dim=300
max_len=max([len(length) for length in X_train_seq])
dense_size=128
num_cat=len(cat_to_id)
batch_size=16
epochs=7
num_classes=len(cat_to_id)


# Create the model
model_glove = tf.keras.models.Sequential()
model_glove.add(tf.keras.layers.Embedding(input_dim=num_words,
                                          output_dim=embedding_dim,
                                          input_length=max_len,
                                          weights=[embedding_matrix]
                                       ))

#model_glove.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128)))
model_glove.add(tf.keras.layers.LSTM(units=128))
model_rnn.add(tf.keras.layers.Dropout(0.2))
model_glove.add(tf.keras.layers.Dense(num_classes, activation='softmax'))
    

# Compile the model
model_glove.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model_glove.fit(X_train_pad,y_train_id , epochs=epochs, batch_size=batch_size)
model_glove.evaluate(X_test_pad, y_test_id)
Epoch 1/7
9/9 [==============================] - 4s 169ms/step - loss: 3.5065 - accuracy: 0.1714
Epoch 2/7
9/9 [==============================] - 1s 148ms/step - loss: 2.9357 - accuracy: 0.2357
Epoch 3/7
9/9 [==============================] - 1s 152ms/step - loss: 2.5611 - accuracy: 0.2929
Epoch 4/7
9/9 [==============================] - 1s 108ms/step - loss: 2.1017 - accuracy: 0.4286
Epoch 5/7
9/9 [==============================] - 1s 116ms/step - loss: 1.5988 - accuracy: 0.6071
Epoch 6/7
9/9 [==============================] - 1s 88ms/step - loss: 1.0982 - accuracy: 0.7571
Epoch 7/7
9/9 [==============================] - 1s 67ms/step - loss: 0.7189 - accuracy: 0.8786
2/2 [==============================] - 1s 11ms/step - loss: 3.7847 - accuracy: 0.1833



########### MODEL WITH BERT##################

pip install tensorflow keras transformers
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

#
from tensorflow.keras.preprocessing.sequence import pad_sequences
max_sequence_length=100
# Tokenization and adding special toens
X_train_encoded = [tokenizer.encode(X_train, add_special_tokens=True) for text in X_train]
# Padding
input_ids = pad_sequences(X_train_encoded, maxlen=max_sequence_length, padding='post', truncating='post')
num_classes=len(cat_to_id)
inputs = tf.keras.Input(shape=(max_sequence_length,), dtype=tf.int32)
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from tensorflow.keras.layers import Dense
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Define and compile the model
bert_model = TFBertModel.from_pretrained('bert-base-uncased')
inputs = tf.keras.Input(shape=(max_sequence_length,), dtype=tf.int32)
outputs = bert_model(inputs)[1]
outputs = Dense(num_classes, activation='softmax')(outputs)

model = tf.keras.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x=input_ids, y=y_train_id, epochs=20, batch_size=64)
# For prediction, preprocess the input in the same way
tokenized_inputs_test = [tokenizer.tokenize(text) for text in X_test]
input_ids_test = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_inputs_test]
input_ids_test = pad_sequences(input_ids_test, maxlen=max_sequence_length, padding='post', truncating='post')
# Evaluate the model
loss, accuracy = model.evaluate(x=input_ids_test, y=y_test_id)

Epoch 1/20
3/3 [==============================] - 3s 947ms/step - loss: 3.2514 - accuracy: 0.2062
Epoch 2/20
3/3 [==============================] - 3s 953ms/step - loss: 3.2550 - accuracy: 0.2062
Epoch 3/20
3/3 [==============================] - 3s 950ms/step - loss: 3.2695 - accuracy: 0.2062
Epoch 4/20
3/3 [==============================] - 3s 957ms/step - loss: 3.2598 - accuracy: 0.2062
Epoch 5/20
3/3 [==============================] - 3s 958ms/step - loss: 3.2604 - accuracy: 0.2062
Epoch 6/20
3/3 [==============================] - 3s 953ms/step - loss: 3.2649 - accuracy: 0.2062
Epoch 7/20
3/3 [==============================] - 3s 948ms/step - loss: 3.2507 - accuracy: 0.2062
Epoch 8/20
3/3 [==============================] - 3s 940ms/step - loss: 3.2564 - accuracy: 0.2062
Epoch 9/20
3/3 [==============================] - 3s 932ms/step - loss: 3.2727 - accuracy: 0.2062
Epoch 10/20
3/3 [==============================] - 3s 944ms/step - loss: 3.2611 - accuracy: 0.2062
Epoch 11/20
3/3 [==============================] - 3s 930ms/step - loss: 3.2527 - accuracy: 0.2062
Epoch 12/20
3/3 [==============================] - 3s 923ms/step - loss: 3.2578 - accuracy: 0.2062
Epoch 13/20
3/3 [==============================] - 3s 921ms/step - loss: 3.2626 - accuracy: 0.2062
Epoch 14/20
3/3 [==============================] - 3s 935ms/step - loss: 3.2546 - accuracy: 0.2062
Epoch 15/20
3/3 [==============================] - 3s 922ms/step - loss: 3.2617 - accuracy: 0.2062
Epoch 16/20
3/3 [==============================] - 3s 918ms/step - loss: 3.2577 - accuracy: 0.2062
Epoch 17/20
3/3 [==============================] - 3s 922ms/step - loss: 3.2602 - accuracy: 0.2062
Epoch 18/20
3/3 [==============================] - 3s 921ms/step - loss: 3.2617 - accuracy: 0.2062
Epoch 19/20
3/3 [==============================] - 3s 929ms/step - loss: 3.2513 - accuracy: 0.2062
Epoch 20/20
3/3 [==============================] - 3s 919ms/step - loss: 3.2497 - accuracy: 0.2062
</code></pre>
","bert"
"{
  ""id"": 121454,
  ""title"": ""Otimization of similarity search for multiple embeddings by creating a weighted artificial embedding""
}","Otimization of similarity search for multiple embeddings by creating a weighted artificial embedding","2023-05-11 12:23:34","","1","147","<word-embeddings><bert><embeddings>","<p>I have embeddings of text created with a BERT model. A group of these embeddings should be used to find similar embeddings corresponding to this group. I know that you can use average or max (or concatenation) to combine multiple embeddings, which the can be used to for example calculate a cosine-similarity. But I wonder if there are other ways to do so, to even improve the result?</p>
<p>My Idea is to create an &quot;artificial&quot; embedding with weights to highlight important features. For that I'm calculating the mean for each dimension as well, but in a next step I am weighting the dimensions by their corresponding standard deviation.
To be able to compare this new created artificial embedding, I normalize it to be more similar to the others.</p>
<p>Another idea is to reduce dimensions of the original embeddings to find the most important features (e.g. with PCA) and then calculate for example the mean. Nevertheless it will be difficult to compare the new embedding, as it then has a different size and the cosine-similarity-score is not possible to compute anymore.</p>
<p>What is a good approach to address this problem? Are there any state of the art ways or best practices to do it?</p>
","bert"
"{
  ""id"": 121322,
  ""title"": ""detect/generate possible tokens for the dataset (name,type/category,signatures)""
}","detect/generate possible tokens for the dataset (name,type/category,signatures)","2023-05-04 05:56:11","","0","67","<machine-learning><transformer><bert>","<p>I have a dataset in the following format:</p>
<pre><code>name, type, signature
Eg1 : A, 2, abc123
Eg2 : A, 2, ab3
Eg3 : A, 2, addc1
</code></pre>
<p>If we need to train the following dataset using roberta or any other model how can we do it? Or is there any other way to train this model to detect possible signatures for the name?</p>
","bert"
"{
  ""id"": 121121,
  ""title"": ""Bert model for document sentiment classification""
}","Bert model for document sentiment classification","2023-04-24 16:43:35","121122","0","68","<deep-learning><nlp><transformer><bert><sentiment-analysis>","<p>I am trying to fine-tune a Bert model for sentiment analysis. Instead of one sentence, my inputs are documents (including several sentences) and I am not removing dots. I was wondering if is it okay to use just the embedding of the first token in such cases. If not, what should I do?</p>
","bert"
"{
  ""id"": 121004,
  ""title"": ""Fine-tuned MLM based RoBERTa not improving performance""
}","Fine-tuned MLM based RoBERTa not improving performance","2023-04-18 12:42:45","","1","1239","<transformer><bert><attention-mechanism><language-model><huggingface>","<p>We have lots of domain-specific data (200M+ data points, each document having ~100 to ~500 words) and we wanted to have a domain-specific LM.</p>
<p>We took some sample data points (2M+) &amp; fine-tuned RoBERTa-base (using HF-Transformer) using the Mask Language Modelling (MLM) task.</p>
<p>So far,</p>
<ol>
<li>we did 4-5 epochs (512 sequence length, batch-size=48)</li>
<li>used cosine learning rate scheduler (2-3 cycles/epochs)</li>
<li>We used dynamin masking (masked 15% tokens)</li>
</ol>
<p>Since the RoBERTa model is finetuned on domain-specific data, we do expect this model to perform better than the pre-trained-RoBERTa which is trained on general texts (wiki data, books, etc)</p>
<p>We did perform some tasks like Named Entity Recognition (NER), Text Classification, and Embedding generation to perform cosine similarity tasks. We did this on both finetuned domain-specific RoBERTa and pre-trained-RoBERTa.</p>
<p>Surprisingly, the results are the same (very small difference) for both models. We did try Spacy models too, but the results are the same.</p>
<p>Perplexity scores indicate that finetuned MLM-based RoBERTa has a minimal loss.</p>
<ol>
<li>Can anyone please help us understand why MLM based model is NOT performing better?</li>
<li>Should we go for more data OR more epochs OR both, to see some effect?</li>
<li>are we doing anything wrong here? Let me know if any required details are missing. I will update</li>
</ol>
<p>any suggestions OR any valuable links addressing these concerns would be really helpful</p>
<p>Huggingface discussion page: <a href=""https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913"" rel=""nofollow noreferrer"">https://discuss.huggingface.co/t/fine-tuned-mlm-based-roberta-not-improving-performance/36913</a></p>
","bert"
"{
  ""id"": 120910,
  ""title"": ""Below text-classification model gives accuracy of 0.77 only on one dataset and 0.99 on spam-ham dataset? What should I do to increase with my dataset?""
}","Below text-classification model gives accuracy of 0.77 only on one dataset and 0.99 on spam-ham dataset? What should I do to increase with my dataset?","2023-04-14 09:19:10","","1","89","<nlp><rnn><bert><text-classification><attention-mechanism>","<pre class=""lang-py prettyprint-override""><code>from keras.models import Model
from keras.layers import Input, Dense, Dropout, Embedding, Conv1D, MaxPooling1D, Flatten, Bidirectional, GRU, Concatenate, Lambda, Multiply, Permute, RepeatVector,dot



text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)
encoder_inputs = preprocessor(text_input)
outputs = encoder(encoder_inputs)
#pooled_output = outputs[&quot;pooled_output&quot;]      # [batch_size, 768].

sequence_output = outputs[&quot;sequence_output&quot;]
dropout_layer = Dropout(0.3)(sequence_output)  
# add BiGRU layer with attention mechanism
bigru_output= Bidirectional(GRU(units=64,activation='tanh',return_sequences=True))(dropout_layer)


# Add a CNN layer
conv_layer1 = Conv1D(filters=128, kernel_size=2, activation='relu',padding=&quot;same&quot;)(bigru_output)
conv_layer2 = Conv1D(filters=128, kernel_size=3, activation='relu',padding=&quot;same&quot;)(bigru_output)
conv_layer3 = Conv1D(filters=128, kernel_size=4, activation='relu',padding=&quot;same&quot;)(bigru_output)
# max_pool_layer = MaxPooling1D(pool_size=2)(conv_layer)
conv_layer= tf.keras.layers.Concatenate()([conv_layer1,conv_layer2,conv_layer3])
# Add a dropout layer after the CNN layer
conv_layer = Dropout(0.3)(conv_layer)
# Map each cnn output vector to a unique context vector using a Dense layer
context_vectors = Dense(128, activation='tanh')(conv_layer)

# Define a function to compute attention scores
def compute_attention_score(context_vector, query_vector):
    &quot;&quot;&quot;
    Computes the attention score between a context vector and a query vector.
    &quot;&quot;&quot;
    score = dot([context_vector, query_vector], axes=[1, 1])
    score = Activation('softmax')(score)
    return score

# Compute attention scores for each context vector using a lambda function
attention_scores = Lambda(lambda x: compute_attention_score(x[0], x[1]))([context_vectors, bigru_output])

# Compute the weighted sum of the context vectors using the attention scores
weighted_context_vectors = Lambda(lambda x: dot([x[0], x[1]], axes=[1, 1]))([attention_scores, context_vectors])

# Concatenate the weighted context vectors with the BiGRU output vector
attention_output = Lambda(lambda x: tf.concat([x[0], x[1]], axis=-1))([bigru_output, weighted_context_vectors])


# Add max pooling layer
max_pool_layer = MaxPooling1D(pool_size=2)(attention_output)

# Flatten and add dense layer for final output
flatten_layer = Flatten()(attention_output)
output_layer = Dense(units=1, activation='sigmoid')(flatten_layer)


# define the model
model = Model(name=&quot;BBRCA&quot;,inputs=text_input, outputs=output_layer)
</code></pre>
","bert"
"{
  ""id"": 120680,
  ""title"": ""BERTopic: Is it okay to ignore the first two topics?""
}","BERTopic: Is it okay to ignore the first two topics?","2023-04-03 18:58:28","120785","0","642","<bert><topic-model>","<p>I used BERTopic to generate a topic model over a large dataset of texts. The result is very appealing and the modeled topics are mostly perfectly interpretable for a human, especially compared to other topic modeling approaches.</p>
<p>According to the documentation (e.g. <a href=""https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html"" rel=""nofollow noreferrer"">https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html</a>) the topic with number <strong>-1</strong> refers to outliers and should be ignored. Topic <strong>-1</strong> is in my case indeed a topic consisting of unrelated common words without a possible interpretation.</p>
<p>However, in my topic model, as well the second topic with number <strong>0</strong> is just a mixture of unrelated words and not a &quot;nice topic&quot; in terms of human sense. All the following topics are very nice topics with a clear meaning.</p>
<p><strong>My question:</strong> Is it okay to ignore the first two topics (<strong>-1</strong> &amp; <strong>0</strong>) modeled by BERTopic and only start using the topics from topic <strong>1</strong> on? Or is this problematic and indicating an issue with the model? Is there a parameter that can be changed to change this behavior in order to ensure that only topic <strong>-1</strong> will be an uninterpretable topic?</p>
","bert"
"{
  ""id"": 120642,
  ""title"": ""Doubt in ELMO, BERT, Word2Vec""
}","Doubt in ELMO, BERT, Word2Vec","2023-04-02 09:12:27","","0","229","<machine-learning><nlp><lstm><word-embeddings><bert>","<p>I read an answer on Quora where a NLP Practioner stated that using ELMO and BERT embeddings as input to LSTM or some RNN will defeat the purpose of ELMo and BERT. I am not sure I agree with the above statement.</p>
<p>Normally we pass words to LSTM to obtain context specific represtations and I am aware of this. But, we pass word2vec instead of one-hot because the contextual representation after LSTM processed it will be better. Similarly common sense states that, if we give ELMO or BERT word embeddings to LSTM, It should output more context rich words than word2vec. Aint I right?</p>
<p>I am aware that once the context is obtained we can fine-tune it straight away for some downstream tasks. But why not use it this way in which we pass the context embeddings of ELMo and BERT to an LSTM ?</p>
<p>Doubt #2 :</p>
<p>I saw a post where the author used ELMo Embeddings with average vectors for each word for logistic regression and tree based models. While this worked for them, In general, It doesn't make sense ? because, In Logistic regression, Each parameter is fixed to an input. Like, Theta1*X1. So if X1 is of different word every time, It should ideally be more confusing to the model to fix that parameter compared to TFIDF where we have a fixed index for each word ?</p>
","bert"
"{
  ""id"": 120630,
  ""title"": ""Fine-tune GPT on sketch data (stroke-3)""
}","Fine-tune GPT on sketch data (stroke-3)","2023-04-01 17:44:43","","1","87","<data><preprocessing><bert><huggingface><finetuning>","<p>These past days I have started a personal project where I would like to build a model that, given an uncompleted sketch, it can finish it.
I was planning on using some pretrained models that are available in HuggingFace and fine-tune them with my sketch data for my task. The sketch data I have is in stoke-3 format, like the following example:
<br>
[ <br>
[10, 20, 1], <br>
[20, 30, 1], <br>
[30, 40, 1], <br>
[40, 50, 0], <br>
[50, 60, 1], <br>
[60, 70, 0]<br>
]
<br>
The first value of each triple is the X-coordinate, the second value the Y-coordinate and the last value is a binary value indicating whether the pen is down (1) or up (0).
I was wondering if you guys could give me some instruction/tips about how should I approach this problem? How should I prepare/preprocess the data so I can fit it into the pre-trained models like BERT, GPT, etc. Since it's stroke-3 data and not text or a sequence of numbers, I don't really know how should I treat/process the data.</p>
<p>Thanks a lot! :)</p>
","bert"
"{
  ""id"": 120601,
  ""title"": ""Do transformers (e.g. BERT) have an unlimited input size?""
}","Do transformers (e.g. BERT) have an unlimited input size?","2023-03-31 09:47:49","120602","8","2641","<machine-learning><nlp><transformer><bert><hyperparameter>","<p>There are various sources on the internet that claim that BERT has a fixed input size of 512 tokens (e.g. <a href=""https://datascience.stackexchange.com/q/89684/141432"">this</a>, <a href=""https://stackoverflow.com/q/58636587/9352077"">this</a>, <a href=""https://www.saltdatalabs.com/blog/bert-how-to-handle-long-documents"" rel=""noreferrer"">this</a>, <a href=""https://datascience.stackexchange.com/q/113489/141432"">this</a> ...). This magical number also appears in the BERT paper (<a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">Devlin et al. 2019</a>), the RoBERTa paper (<a href=""https://arxiv.org/pdf/1907.11692.pdf"" rel=""noreferrer"">Liu et al. 2019</a>) and the SpanBERT paper (<a href=""https://www.cs.princeton.edu/%7Edanqic/papers/tacl2020.pdf"" rel=""noreferrer"">Joshi et al. 2020</a>).</p>
<p>The going wisdom has always seemed <em>to me</em> that when NLP transitioned from recurrent models (RNN/LSTM Seq2Seq, Bahdanau ...) to transformers, we traded variable-length input for fixed-length input that required padding for shorter sequences and could not extend beyond 512 tokens (or whatever other magical number you want to assign your model).</p>
<p>However, come to think of it, all the parameters in a transformer (Vaswani et al. 2017) work on a token-by-token basis: the weight matrices in the attention heads and the FFNNs are applied tokenwise, and hence their parameters are independent of the input size. <strong>Am I correct that a transformer (encoder-decoder, BERT, GPT ...) can take in an arbitrary amount of tokens even with fixed parameters, i.e., the amount of parameters it needs to train is independent of the input size?</strong></p>
<p>I understand that memory and/or time will become an issue for large input lengths since attention is O(n²). This is, however, a limitation of our <em>machines</em> and not of our <em>models</em>. Compare this to an LSTM, which can be run on any sequence but compresses its information into a fixed hidden state and hence blurs all information eventually. <em>If</em> the above claim is correct, then I wonder: <strong>What role does input length play during pre-training of a transformer, given infinite time/memory?</strong></p>
<p>Intuitively, the learnt embedding matrix and weights must somehow be different if you were to train with extremely large contexts, and I wonder if this would have a positive or a negative impact. In an LSTM, it has negative impact, but a transformer doesn't have its information bottleneck.</p>
","bert"
"{
  ""id"": 120227,
  ""title"": ""Using BERT to extract a list of words and phrases from documents""
}","Using BERT to extract a list of words and phrases from documents","2023-03-15 16:26:56","","2","402","<nlp><transformer><bert><information-extraction>","<p>I have a list of words and phrases (~3k items). What are my options to extract them from documents (~3M of job descriptions) with NLP? I do not have labeled data.</p>
<p>For example my list of words and phrases look like,</p>
<pre><code>Leadership
Microsoft Office
AWS
.
.
.
Python Programing Language
</code></pre>
<p>The result I am looking for is a matrix(3K x 3M) with binary values inside.</p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th>Doc #</th>
<th>Leadership</th>
<th>Microsoft Office</th>
<th>AWS</th>
<th>...</th>
<th>Python Programing Language</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td></td>
<td>1</td>
</tr>
<tr>
<td>.</td>
<td>.</td>
<td>.</td>
<td>.</td>
<td></td>
<td>.</td>
</tr>
<tr>
<td>3M</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td></td>
<td>1</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><p>Regex -  This is the most straightforward solution comes my mind. However, this solutions is not robust and cannot capture different word/phrase forms. For example, people might write <code>MS Office</code> instead of <code>Microsoft Office</code>. Similarly, people might write <code>Amazon Web Service</code> rather than <code>AWS</code>.</p>
</li>
<li><p>Is there a solution to utilize a Large Language Model such as BERT?</p>
</li>
<li><p>If I create a labeled data using, for example, <a href=""https://docs.aws.amazon.com/sagemaker/latest/dg/sms-label-text.html"" rel=""nofollow noreferrer"">AWS Ground Truth</a>, is there a way to utilize the results to build a model and extract the list of words/phrases?</p>
</li>
</ol>
","bert"
"{
  ""id"": 120215,
  ""title"": ""Does high number of output labels affect the performance of BERT and how to handle the class imbalance issue while doing multi text classification?""
}","Does high number of output labels affect the performance of BERT and how to handle the class imbalance issue while doing multi text classification?","2023-03-15 11:34:02","","1","366","<machine-learning><nlp><transformer><bert><text-classification>","<p>I am using BERT to do multiclass text classification. The number of output classes I have to predict from is: 116 and there is high degree of class imbalance that I see.<br>
We have the following kind of records available for each of the classes:<br>
{'Class A': 975 number of records,<br>
'Class B': 776 number of records,<br>
'Class C': 533 number of records,<br>
'Class D': 412 number of records,<br>
'Class E': 302 number of records,<br>
'Class F': 250 number of records,<br>
'Class G': 207 number of records,<br>
'Class H': 137 number of records,<br>
'Class I': 96 number of records,<br>
'Class J': 51 number of records,<br>
'Class K': 28 number of records,<br>
'Class L': 17 number of records,<br>
'Class M': 7 number of records,<br>
'Class N': 2 number of records}<br></p>
<p>So I have two questions here:<br>
Question1: As we have around 116 output classes to predict from, does that affect the performance of BERT due to the high number of output classes?</p>
<p>Question2: My original data has the similar type of class distribution that I have illustrated above. So how does this affect the performance of BERT and if it affects how do we handle this to get proper output?</p>
<p>Looking forward to get answer from the talented community we have here.</p>
<p>Much thanks in advance.</p>
","bert"
"{
  ""id"": 120209,
  ""title"": ""Is there any concern for a pretrained model to overfitting to a fine-tuning task that has overlapping pretraining and training data?""
}","Is there any concern for a pretrained model to overfitting to a fine-tuning task that has overlapping pretraining and training data?","2023-03-15 08:46:06","","1","448","<nlp><overfitting><bert><pretraining>","<p>Let's say my language model is pretrained on a general text corpus, and I want to use it for some specific downstream task that has it's datasets also included in the general corpus, is there any concern for overfitting or bias?</p>
<p>I can't seem to find much resources that touch on this issue. I read this paper <a href=""https://aclanthology.org/D19-1371.pdf"" rel=""nofollow noreferrer"">SciBERT</a> that shows <em>in-domain pretraining</em> of BERT with vocab and corpus extracted from only <strong>scientific</strong> text would yield better performance on <strong>scientific</strong> tasks. But isn't this just overfitting? I also read a few papers like the <a href=""https://arxiv.org/pdf/1910.10683v3.pdf"" rel=""nofollow noreferrer"">T5</a> paper that claims <em>in-domain pretraining</em> leads to improvement of fine-tuning tasks as if it is a merit to use pretraining data that is similar to finetuning tasks? Is there not a concern for overfitting? Is it not a concern if the pretraining and finetuning objectives are different enough? Or am I misunderstanding the concept of pretraining and overfitting?</p>
<p>Would appreciate if anyone could also provide links to articles that investigate this issue.</p>
","bert"
"{
  ""id"": 120062,
  ""title"": ""Is it valid changing the classification treshold of neural networks for improving the classification performance?""
}","Is it valid changing the classification treshold of neural networks for improving the classification performance?","2023-03-09 01:33:19","","1","92","<machine-learning><classification><bert><text-classification>","<p>I'm dealing with text classification using BERT pre-trained model with a multiclass imbalanced dataset.
When we use a 0.5 default classification threshold we obtain a f1 measure of around 0.7.
But we have noticed that when we decrease the classification threshold we obtain a better performance.</p>
<p>If we use different binary classifiers, one for each class as positive, we have different imbalance rates. And we have notice that the optimal classification threshold decreases as the imbalance rate increases.</p>
<p>Is this an expected behavior?
Besides that. Is it valid to change the classification threshold to the optimal value in order to increase the classifier performance?</p>
<p>Best regards.</p>
","bert"
"{
  ""id"": 119931,
  ""title"": ""SBERT Embeddings from Conversations""
}","SBERT Embeddings from Conversations","2023-03-03 12:09:33","","0","321","<nlp><bert><embeddings>","<p>I have a dataset consisting of text-based conversations between two humans. One conversation has on average 20 turns and can look as follows:</p>
<pre><code>Person 1: Do you like cooking?
Person 2: Yes. I like cooking very much. I got this hobby when I was 12 years sold.
Person 1: Why do you like it?
Person 2: I have no idea. I like cooking by myself. I like to taste delicious food.
...
</code></pre>
<p>With SBERT I can get the embeddings of one turn (e.g., &quot;Hello there, how are you doing?&quot;). Is it also possible to get one embedding with SBERT for several turns or a whole conversation (20 turns)? Are there other models which are capable to do this or are more recent? Afterward, I would like to project the embedding to 2D or 3D space and apply clustering.</p>
","bert"
"{
  ""id"": 119923,
  ""title"": ""How does BERT work for Aspect-Based sentiment analysis?""
}","How does BERT work for Aspect-Based sentiment analysis?","2023-03-02 21:45:31","","1","949","<deep-learning><nlp><bert><sentiment-analysis>","<p>I have recently used a <a href=""https://github.com/ScalaConsultants/Aspect-Based-Sentiment-Analysis"" rel=""nofollow noreferrer"">package</a> to perform Aspect-Based Sentiment Analysis (ABSA) through a BERT model.</p>
<p>Briefly, the model takes two inputs:</p>
<ul>
<li>words that constitute the <em>aspects</em></li>
<li>a sentence on which we want to perform the ABSA</li>
</ul>
<p>The BERT-based model outputs a sentiment list with three integers representing the positive, the neutral and the negative scores.</p>
<p>I would like to understand more about how this kind of model works.</p>
<p>I have seen many posts on Medium that are too general and read some papers that give the basic functioning for grated.</p>
<p>So, if anyone can explain extensively how bert-based ABSA works, it would be appreciated.</p>
<p>Thank you!</p>
","bert"
"{
  ""id"": 118813,
  ""title"": ""What to do if my dataset have only One instance for class in classification?""
}","What to do if my dataset have only One instance for class in classification?","2023-02-26 13:31:29","","1","303","<deep-learning><classification><multiclass-classification><bert>","<p>I am working on a benchmark dataset for text classification. The dataset has about 300 classes, and approximately 50 of these classes have a single instance. In a paper that used fine-tuning BERT, the authors split the dataset into training, validation, and test sets. However, they did not increase the number of instances through oversampling or augmentation. This brings up the topic of what happens to classes with a single instance. In which split should they be included?</p>
","bert"
"{
  ""id"": 118775,
  ""title"": ""How do I improve the accuracy of a BERT-based multilabel text classification model?""
}","How do I improve the accuracy of a BERT-based multilabel text classification model?","2023-02-24 16:41:24","","0","276","<bert><text-classification>","<p>I have a database table with 79,512 rows, each of which describes a category. Each row has a title and a description, and can even have a supercategory. Often, supercategories have categories.</p>
<p>I'm trying to train a model to predict each category's supercategory, to be used in a later model that will categorize other data. However, I never get more than a 20% accuracy on this model. My target is 84% accuracy.</p>
<p>These are multilingual datasets I am working with, so the pretrained model I use is also multi-lingual. However, it is also case-sensitive.</p>
<p>By my count, there are an average 9.86 supercategories per category.</p>
<p>How do I improve the accuracy?</p>
<p>EDIT: I suppose I had better clarify: There is a wide variety of languages here, none of them comprising more than 50% of the whole dataset. Perhaps I should translate the data?</p>
","bert"
"{
  ""id"": 118742,
  ""title"": ""Combining text and image features with different scales""
}","Combining text and image features with different scales","2023-02-23 13:02:04","","0","493","<bert><feature-scaling><vgg16>","<p>I have computed text features using [SBERT][1] and image features using VGG-16. The text features range from -1.58 to 1.58, whereas the image features range between 0 and 521. I would want to concatenate the text and image features and use them to compute cosine similarities. However, as you've probably noticed, the difference in scale would mean that the image features would completely dominate the text ones. </p>
<p>My idea was to use something like sklearn's MinMaxScaler and scale down the image features to the same scale as the SBERT computed features; however, I'm not sure if this is the best solution for my case since other [answers][2] here suggest normalizing both features. In my case, I would say that the text features are <strong>more important</strong> than the image ones.</p>
<p>  [1]: <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a>
  [2]: <a href=""https://datascience.stackexchange.com/questions/39742/creating-a-feature-by-combining-2-features-with-different-units"">Creating a feature by combining 2 features with different units?</a></p>
","bert"
"{
  ""id"": 118697,
  ""title"": ""Sentiment analysis BERT vs Model from scratch""
}","Sentiment analysis BERT vs Model from scratch","2023-02-21 17:06:08","118715","0","248","<machine-learning><bert><sentiment-analysis>","<p>I am working on building a sentiment analyzer, the data I would like to analyze is social media data from twitter, once I have created a the model I want to integrate it into a simply webpage.</p>
<p>I have tried two options:</p>
<ol>
<li><p>Create my own model from scratch, meaning train a word2vec model to perform word embedding, convert my labelled dataset into vectors and train them using Logistic regression, Random forest or SVM.</p>
</li>
<li><p>Fine tune a BERT model using my dataset.</p>
</li>
</ol>
<p>option 1.. Using word2vec and SVM I was able to get the following results:</p>
<pre><code>          precision    recall  f1-score   support

0              0.74      0.67      0.70      1310
1              0.77      0.82      0.79      1716

accuracy                           0.76      3026
macro avg       0.75      0.75      0.75      3026
weighted avg    0.75      0.76      0.75      3026
</code></pre>
<p>option 2.. I fined tuned BERT using the following code <a href=""https://#https://github.com/prateekjoshi565/Fine-Tuning-BERT/blob/master/Fine_Tuning_BERT_for_Spam_Classification.ipynb"" rel=""nofollow noreferrer"">link</a> and was able to achieve the following results after 100 epochs:</p>
<pre><code>          precision    recall  f1-score   support

       0       0.68      0.65      0.66       983
       1       0.74      0.77      0.75      1287

accuracy                           0.71      2270
macro avg      0.71      0.71      0.71      2270
weighted avg   0.71      0.71      0.71      2270
</code></pre>
<p>I used the same dataset for both option 1 and 2, BERT used a smaller subset for validation</p>
<p>What I would like to know:</p>
<p>Is there any advantages in going with option 1.? Does BERT have any disadvantages when it comes to data from social media (data is rather unclean and a lot of slang).</p>
","bert"
"{
  ""id"": 118343,
  ""title"": ""how can I translate Whisper encodings to SBERT embeddings?""
}","how can I translate Whisper encodings to SBERT embeddings?","2023-02-07 02:32:09","118348","0","251","<transformer><bert>","<p>I'm using the Whisper model to recognize speech, and then matching the output text against a list of known questions by generating SBERT embeddings from the text and ranking the known questions by cosine similarity with the SBERT embeddings of the text output from Whisper. It works pretty well, and I'm happy with the level of accuracy.</p>
<p>I'd like to streamline the process a bit, and I understand I can get the encoder embeddings from the whisper model output rather than just the transcribed text.</p>
<p>My question is: What's the best way to fuse these steps together? More generally, is there a good way to translate embeddings from one model vector space to another? What would that task even be called in terms of linear algebra?</p>
","bert"
"{
  ""id"": 118137,
  ""title"": ""why some authors said that BERT cannot be used for text prediction?""
}","why some authors said that BERT cannot be used for text prediction?","2023-01-29 16:38:54","","0","194","<bert>","<p>I was trying to get a grasp about BERT and found this post in DS StackExchange:</p>
<p><a href=""https://datascience.stackexchange.com/questions/46377/"">Can BERT do the next-word-predict task?</a></p>
<p>In broad terms, it says that Bert cannot be used for next-word prediction. I suppose that next-word prediction it could be used, for example, in some sort of autocorrect tools. However, I saw in this blog:</p>
<p><a href=""https://medium.com/mlearning-ai/an-illustration-of-next-word-prediction-with-state-of-the-art-network-architectures-like-bert-gpt-c0af02921f17"" rel=""nofollow noreferrer"">https://medium.com/mlearning-ai/an-illustration-of-next-word-prediction-with-state-of-the-art-network-architectures-like-bert-gpt-c0af02921f17</a></p>
<p>That BERT could be used for text prediction; so, why it is said that it cannot be used for next-word prediction? If somebody could explain the caveats of it and an example to clarify this, it would be really helpful.</p>
<p>Thanks.</p>
","bert"
"{
  ""id"": 117802,
  ""title"": ""BERTopic Visualization""
}","BERTopic Visualization","2023-01-16 19:43:56","","1","447","<visualization><bert><topic-model>","<p>I new to <code>topic modeling</code> and I'm trying to use <code>BERTopic</code> inside of <code>PyCharm</code>. I'm struggling to get any type of visualization to work inside of <code>PyCharm</code> with <code>BERTopic</code>.</p>
<p>Here is my current code:</p>
<pre><code>import pickle
from umap import UMAP
from hdbscan import HDBSCAN
from bertopic import BERTopic
from sent2vec.vectorizer import Vectorizer
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer


# 6000 articles about cryptocurrencies
with open('all_articles', 'rb') as file_in:
    random_articles = pickle.load(file_in)

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
umap_model = UMAP(n_neighbors=3, 
                  n_components=3, 
                  min_dist=0.05)

hdbscan_model = HDBSCAN(min_cluster_size=80, 
                        min_samples=40,
                        gen_min_span_tree=True,
                        prediction_data=True)

vectorizer_model = CountVectorizer(ngram_range=(1, 2), 
                                   stop_words='english')

model = BERTopic(
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    embedding_model=embedding_model,
    vectorizer_model=vectorizer_model,
    top_n_words=5,
    language='english',
    calculate_probabilities=True,
    verbose=True
)
topics, probs = model.fit_transform(random_articles)

# this produces nothing
model.visualize_topics(top_n_topics=5)

</code></pre>
<p>I found this <a href=""https://github.com/MaartenGr/BERTopic/issues/339"" rel=""nofollow noreferrer"">issue</a> on visualization not working within <code>PyCharm</code> with <code>BERTopic</code>.  This issue wasn't useful in solving my issue.</p>
<p>How do I get something to visualize inside of <code>PyCharm</code>?</p>
","bert"
"{
  ""id"": 117634,
  ""title"": ""What does Embeddings Array Represent in BERT's Feature Extraction?""
}","What does Embeddings Array Represent in BERT's Feature Extraction?","2023-01-09 09:08:20","","0","216","<nlp><word-embeddings><transformer><bert>","<p>I am new to academic NLP, and I had been tasked with to use BERT to extract features of a sentence.</p>
<pre><code>text_input = [
  &quot;Hello I'm a single sentence&quot;,
  &quot;And another sentence&quot;,
  &quot;And the very very last one&quot;,
  &quot;My name is Aun&quot;
  ]
</code></pre>
<p>I got embeddings using pipeline from huggingface:</p>
<pre><code>from transformers import pipeline
feature_extraction = pipeline('feature-extraction', model=&quot;distilroberta-base&quot;, tokenizer=&quot;distilroberta-base&quot;)
features = feature_extraction(text_input)
</code></pre>
<p>Embeddings were multi-dimension, which I flattened and then padded to match the array with highest size. Here <code>text_df.head()</code>:</p>
<pre><code>    text_input                  text_embeddings                                 text_em_flat                        text_em_flat_pad
0   Hello I'm a single sentence [[[-0.010155443102121353, 0.07965511828660965,...   [-0.010155443102121353, 0.07965511828660965, 0...   [-0.010155443102121353, 0.07965511828660965, 0...
1   And another sentence        [[[-0.010256338864564896, 0.0948348417878151, ...   [-0.010256338864564896, 0.0948348417878151, -0...   [-0.010256338864564896, 0.0948348417878151, -0...
2   And the very very last one  [[[-0.001137858722358942, 0.09048153460025787,...   [-0.001137858722358942, 0.09048153460025787, -...   [-0.001137858722358942, 0.09048153460025787, -...
3   My name is Aun              [[[-0.0018534815171733499, 0.08652304857969284...   [-0.0018534815171733499, 0.08652304857969284, ...   [-0.0018534815171733499, 0.08652304857969284, ...
</code></pre>
<p>But I don't understand what each value represents in the text_embeddings. I have gone through some explanation, but don't understand if they are token level or segment level or position level embeddings for a stack of all three. Please explain.
Following are the shapes for few instances:</p>
<pre><code>arr_dimen(text_df['text_embeddings'][0]):    [1, 8, 768]
arr_dimen(text_df['text_embeddings'][1]):    [1, 5, 768]
arr_dimen(text_df['text_embeddings'][2]):    [1, 8, 768]
arr_dimen(text_df['text_embeddings'][3]):    [1, 7, 768]
arlen(text_df['text_em_flat'][0]):   6144
arlen(text_df['text_em_flat'][1]):   3840
arlen(text_df['text_em_flat'][2]):   6144
arlen(text_df['text_em_flat'][3]):   5376
</code></pre>
<p>From original paper I understand that BERT divides input in three-layers and then uses them like shown in the figure from original paper. <a href=""https://i.sstatic.net/XPWhh.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/XPWhh.png"" alt=""enter image description here"" /></a></p>
<p>But I want to understand how BERT encodes <code>My name is Aun</code> to an array with shape <code>[1, 7, 768]</code>.</p>
","bert"
"{
  ""id"": 117404,
  ""title"": ""How to Train Q&A model using Bert for multiple comma seperated values in a given data""
}","How to Train Q&A model using Bert for multiple comma seperated values in a given data","2022-12-30 02:31:43","117419","0","91","<bert><spacy><question-answering>","<p>I'm using the entire text book data by scraping the information of each chapter.</p>
<p>How do I highlight the spacy spancat NER or Bert Q&amp;A based models to train multiple comma separated values in the text as important.
For each chapter this behavior is recurring so how do I train the model to detect that it is important and that section is the important part which discusses different topics for each chapter.</p>
<p>Eg:
After scraping the chapter 1:
There is 1 paragraph that describes what topics will be covered in this chapter like x,y,z,a,b,c,d,e.</p>
<p>Similarly in chapter 2,
There is 1 paragraph that describes what topics will be covered in this chapter like f,g,h,i,j,k.</p>
<p>How do I train this model in such a way that if I move to next chapter or even take the next book, It'll recognize these patters as the topics in that chapter or get all important topics discussed in the entire book? SO, it'll be the sum of all such comma separated values in the book.</p>
","bert"
"{
  ""id"": 117373,
  ""title"": ""Should i remove french special characters and apostrophes""
}","Should i remove french special characters and apostrophes","2022-12-29 00:22:09","117384","0","288","<nlp><data-cleaning><preprocessing><bert>","<p>I am working on a french text preprocessing task, in order to prepare the data to train an NLP model. But I do not know if it is better to remove french special characters and apostrophes or keep them. Example:</p>
<pre><code>Malgré que j'ai tellement aimé ce boulot je veut démissionner
</code></pre>
<p>Becomes</p>
<pre><code>Malgre que jai tellement aime ce boulot je veut demissionner
</code></pre>
<p>I have also noticed that most lemmatization libraries for french text are not at all efficient so i was wondering if I could skip this step, and also skip the stopwords removal step.
In general the preprocessing steps will be :</p>
<ol>
<li>Remove URLs and Emails</li>
<li>Demojize Emojis</li>
<li>Transform number into text (6-&gt;six)</li>
<li>Removal of all special characters including french special characters</li>
</ol>
","bert"
"{
  ""id"": 117293,
  ""title"": ""Combining sentence embeddings of two different models (sBERT and mBERT)""
}","Combining sentence embeddings of two different models (sBERT and mBERT)","2022-12-25 02:49:52","117311","0","985","<nlp><bert><huggingface>","<br>
I am working on a chatbot that helps students. <br>
So, I wanted to make use of bert model which has better performance on mathematics, which lead to me to math-bert, but the paper on it said that it was trained only on mathematical corpus, which means it wont have great performance on general sentences (example in image), so is there a method to combine sentence-bert and math-bert?
<br>[![enter image description here][1]][1]
<br>Or, the only way is to train bert model from scratch using corpus used for sentence-bert and math-bert.
","bert"
"{
  ""id"": 117144,
  ""title"": ""How many samples in dataset are required to fine-tune BERT for binary classification?""
}","How many samples in dataset are required to fine-tune BERT for binary classification?","2022-12-18 15:51:18","","2","1754","<nlp><dataset><bert><finetuning>","<p>I'm trying to fine-tune a BERT-based model for a binary classification task (data is in English). The dataset I'm working with is quite small (~500 samples, out of which 80% are currently used for training), and I'm wondering if there is a rule of thumb for the minimal number of samples that is required to produce a decent model. I am able to increase the size of the dataset by labeling manually (though that would only be possible in the case of thousands of samples).</p>
<p>Any ideas? If this is problem-dependent, ideas on how to assess the size of dataset required would be appreciated.</p>
<p>Thanks!</p>
","bert"
"{
  ""id"": 116655,
  ""title"": ""Is there bias in matrix multiplications for self attention""
}","Is there bias in matrix multiplications for self attention","2022-12-02 06:45:40","","0","901","<transformer><bert><attention-mechanism>","<p>When the query matrix Q is computed as <span class=""math-container"">$XW_Q$</span>,  (<span class=""math-container"">$W_Q$</span> is the weight matrix for the queries), is it implemented as a linear layer without bias? I see some blogs saying there is are bias terms as well.</p>
<p>To paraphrase the question in a different way, are there bias terms when Q , K and V are computed using a linear layer in pytorch? If yes, why does it make sense since adding bias makes the transformation a non linear one</p>
","bert"
"{
  ""id"": 116642,
  ""title"": ""What are MLM and NSP models actually used for after they've been trained?""
}","What are MLM and NSP models actually used for after they've been trained?","2022-12-01 15:24:23","116647","-1","350","<machine-learning><deep-learning><nlp><bert><language-model>","<p>I am a Python programmer working with deep learning nets and I have recently built my own language models as well as I have fine-tuned popular models like BERT. MY question is - after these models have been successfully trained, what are they used for? I understand that masked-language models can predict what the masked word is, but what is the point? What is a real-world application of this model? The same question goes for next-sentence prediction models - what is a real-world application?</p>
<p>Thank you.</p>
","bert"
"{
  ""id"": 116518,
  ""title"": ""NER - What advantage does IO Format have over BIO Format""
}","NER - What advantage does IO Format have over BIO Format","2022-11-27 13:23:02","116523","3","458","<machine-learning><nlp><transformer><bert><named-entity-recognition>","<p>In <a href=""https://aclanthology.org/2021.acl-long.248.pdf"" rel=""nofollow noreferrer"">this</a> paper, the authors say that they used IO schema instead of BIO in their dataset, which, if I am not wrong, means they just tag the corresponding Entity Type or &quot;O&quot; in case the word is not a Named Entity. What advantage does this method have? I would imagine that it just takes away valuable information from the model and makes it harder to detect entities that span multiple words</p>
","bert"
"{
  ""id"": 116222,
  ""title"": ""CNN-BERT Text Classification good results on train and val, but bad prediction on testing""
}","CNN-BERT Text Classification good results on train and val, but bad prediction on testing","2022-11-16 14:37:26","","0","288","<neural-network><keras><cnn><bert><text-classification>","<p>I built a Keras model to predict hoax news and true news using the CNN-BERT Text Classification algorithm with Categorical Classification, with label 1 indicating a hoax and 0 indicating true news.</p>
<p>Although the model I created appears to have good training and validation accuracy results, when I make predictions on the test set, it does not appear to be able to predict.</p>
<p>Pretrained BERT model that i used</p>
<pre><code>tfhub_handle_encoder = hub.load('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1')

tfhub_handle_preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')
</code></pre>
<p>Model</p>
<pre><code>from tensorflow.keras import regularizers


def build_CNN_classifier_model():
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output'] # [batch_size, 768].
    net = sequence_output = outputs[&quot;sequence_output&quot;] #[batch_size, seq_length, 768]
      
    
    net = tf.keras.layers.Conv1D(32, (2), activation='relu')(net)
    net = tf.keras.layers.MaxPooling1D(2)(net)
#     net = tf.keras.layers.Dropout(0.1)(net)
    
    net = tf.keras.layers.Conv1D(64, (2), activation='relu')(net)
    net = tf.keras.layers.MaxPooling1D(2)(net)
    net = tf.keras.layers.Dropout(0.2)(net)
    
    net = tf.keras.layers.Conv1D(128, (2), activation='relu')(net)
    net = tf.keras.layers.MaxPooling1D(2)(net)
    net = tf.keras.layers.Dropout(0.2)(net)
    
#     net = tf.keras.layers.GlobalMaxPool1D()(net)
    
    net = tf.keras.layers.Flatten()(net)
    
    net = tf.keras.layers.Dense(256, activation=&quot;relu&quot;)(net)
                                #,kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),
                                #bias_regularizer=regularizers.L2(1e-4),
                                #activity_regularizer=regularizers.L2(1e-5))
                                
    net = tf.keras.layers.Dropout(0.1)(net)
    
    net = tf.keras.layers.Dense(128, activation=&quot;relu&quot;)(net)
                                #,kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),
                                #bias_regularizer=regularizers.L2(1e-4),
                                #activity_regularizer=regularizers.L2(1e-5))
                                
    net = tf.keras.layers.Dropout(0.1)(net)
    
#     net = tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;, name='classifier')(net)
    net = tf.keras.layers.Dense(2, activation=&quot;softmax&quot;, name='classifier')(net)
    
    return tf.keras.Model(text_input, net)
</code></pre>
<p>compile</p>
<pre><code>from official.nlp import optimization  # to create AdamW optmizer

epochs = 10
steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()
num_train_steps = steps_per_epoch * epochs
num_warmup_steps = int(0.1*num_train_steps)

init_lr = 1e-5
optimizer = optimization.create_optimizer(init_lr=init_lr,
                                          num_train_steps=num_train_steps,
                                          num_warmup_steps=num_warmup_steps,
                                          optimizer_type='adamw')

cnn_classifier_model.compile(optimizer=optimizer,
                             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                             metrics=tf.keras.metrics.SparseCategoricalAccuracy('accuracy'))



print(f'Training model with {tfhub_handle_encoder}')
cnn_history = cnn_classifier_model.fit(x=train_ds,
                                       validation_data=val_ds,
                                       epochs=epochs,
                                       class_weight=class_weight
                                      )
</code></pre>
<p>Results</p>
<p><a href=""https://i.sstatic.net/wCLSv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/wCLSv.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/YOuiv.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/YOuiv.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/uYtC7.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uYtC7.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://i.sstatic.net/dPb1M.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/dPb1M.png"" alt=""enter image description here"" /></a></p>
<p>Test Prediction</p>
<p><a href=""https://i.sstatic.net/yRCmO.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/yRCmO.png"" alt=""enter image description here"" /></a></p>
<p>it seems like my test model can't tell which one is hoax news and which one is real news, what's the problem?</p>
","bert"
"{
  ""id"": 116180,
  ""title"": ""Word embedding for Non-NLP words""
}","Word embedding for Non-NLP words","2022-11-15 07:50:33","","0","90","<word-embeddings><bert>","<p>I would like to embed words with a context, but that is not &quot;Natural Language&quot; - but just a list of words about more or less the same topic. Is there a way to use this context for the embedding ? I suppose Bert and other pre-trained models won't work out of the box, as they are trained for Natural Language ?</p>
","bert"
"{
  ""id"": 116101,
  ""title"": ""Ordering training text data by length""
}","Ordering training text data by length","2022-11-12 04:34:21","116111","0","131","<machine-learning><nlp><bert><text-classification><performance>","<p>If I have text data where the length of documents greatly varies and I'd like to use it for training where I use batching, there is a great chance that long strings will be mixed with short strings and the average time to process each batch will increase because of padding within the batches.
I imagine sorting documents naively by length would create a bias of some sort since long documents and short one would tend to be similar to each other.
Are there any methods that have been tried that can help reduce training time in this case without sacrificing model performance?</p>
","bert"
"{
  ""id"": 115717,
  ""title"": ""Do I need to train a tokenizer when training SBERT with MLM?""
}","Do I need to train a tokenizer when training SBERT with MLM?","2022-10-30 08:32:22","","1","395","<bert>","<p>I have trained a SBERT model with MLM on my own corpus which is somewhat domain specific using these guides:</p>
<p><a href=""https://ireneli.eu/2021/03/28/deep-learning-19-training-mlm-on-any-pre-trained-bert-models/"" rel=""nofollow noreferrer"">https://ireneli.eu/2021/03/28/deep-learning-19-training-mlm-on-any-pre-trained-bert-models/</a>
<a href=""https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py"" rel=""nofollow noreferrer"">https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py</a></p>
<p>When I saved a tokenizer with</p>
<pre><code>tokenizer.save_pretrained(output_dir)
</code></pre>
<p>it created a set of files. So I opened a vocab.txt and tried to search for some domain specific words but I could not find them.</p>
<ol>
<li>Do I need to train a tokenizer as well on my corpus?</li>
<li>If so, then do I need to retrain the SBERT model again using MLM?
(which would be really disappointing since I don't have a GPU and had to train in the cloud paying for GPU)</li>
<li>Is the model I have now useless if I use it with the original tokenizer that lacks domain specific words?</li>
</ol>
","bert"
"{
  ""id"": 115353,
  ""title"": ""How does BERT produce CLS token? Internally does it do max-pooling or avarage pooling?""
}","How does BERT produce CLS token? Internally does it do max-pooling or avarage pooling?","2022-10-18 19:00:55","115356","1","1733","<nlp><transformer><bert>","<p>I ran experiment to compare max-pooled word tokens vs CLS token for sentence classification and CLS clearly wins. Trying to understand how BERT generates CLS token embedding if its better than max or avg pooling.</p>
","bert"
"{
  ""id"": 115267,
  ""title"": ""Fine tuning BERT without pre-training it on domain specific corpus""
}","Fine tuning BERT without pre-training it on domain specific corpus","2022-10-16 05:55:45","115269","1","1081","<nlp><bert><search>","<p>I'm building an internal semantic search engine using BERT/SBERT + ElasticSearch 8 where  answers are retrieved based on their cosine similarity with a query.</p>
<p>The documents to be searched are somewhat domain-specific, off the top of my head estimation is that about 10% of the vocabulary is not present in Wiki or Common Crawl datasets on which BERT models were trained. These are basically &quot;made-up&quot; words - niche product and brand names.</p>
<p>So my question is:</p>
<ol>
<li>Should I pre-train a BERT/SBERT model first on my specific corpus to learn the embeddings for these words using MLM?</li>
</ol>
<p>or</p>
<ol start=""2"">
<li>Can I skip pre-training and start fine-tuning a selected model for Q/A using SQUAD, synthetic Q/A based on my corpus and actual logged user queries?</li>
</ol>
<p>My concern is that if I skip #1 then a model would not know the embeddings for some of the &quot;made up&quot; words, replace them with &quot;unknown&quot; token and this might lead to worse search performance.</p>
","bert"
"{
  ""id"": 115231,
  ""title"": ""Good NLP model for computationally cheap predictions that can reasonably approximate language model given large training data set""
}","Good NLP model for computationally cheap predictions that can reasonably approximate language model given large training data set","2022-10-14 15:01:01","115249","1","39","<machine-learning><nlp><bert>","<p>I have a corpus of about one billion sentences, in which I am attempting to resolve NER conflicts (when two terms overlap in a sentence). My initial plan is to have an SME label the correct tag in each of a large number of conflicts, then use those labels to train either an NER model or a binary classification model (like GAN-ALBERT), to identify the correct choice when two NER tags conflict.</p>
<p>The problem is, about 5% of these sentences contain conflicts, and I don't think that I have the computational resources to run BERT or ALBERT prediction on 50 million sentences in a reasonable amount of time. So, my hope is to use the ALBERT model to generate a large number of labels (perhaps one million) for a computationally cheaper model.</p>
<p>So, I'm wondering if there is a model, 10 to 100 times cheaper at prediction than BERT, that could be trained to do a reasonable job of replicating the ALBERT model's performance, given a large amount of training data generated by said model.</p>
","bert"
"{
  ""id"": 115064,
  ""title"": ""\""cross-validation on the training set\"" while development and test set are distinct from the training: does it make sense? semantic mistake?""
}","""cross-validation on the training set"" while development and test set are distinct from the training: does it make sense? semantic mistake?","2022-10-10 10:25:00","","3","67","<cross-validation><bert>","<p>I got stuck on this paragraph from the academic article &quot;Measuring news sentiment&quot;: <a href=""https://www.sciencedirect.com/science/article/pii/S0304407620303535#tbl3"" rel=""nofollow noreferrer"">https://www.sciencedirect.com/science/article/pii/S0304407620303535#tbl3</a></p>
<p>&quot;As is best practice, we split the labeled dataset into <strong>a training set, a development set, and a hold-out test set.</strong> The development and test sets have 100 observations each, leaving 600 observations for the training set. (..) hyper-parameter optimization is done through grid search, <strong>using</strong> <strong>cross-validation on the</strong> <strong>training set</strong> to evaluate model performance for each possible set of hyper-parameters. The optimal model is then evaluated against the development set. Finally, after all models have been developed, we test them all against our hold-out test set for final results.&quot;</p>
<p>To me, what they describe <strong>(the bold parts)</strong> is incoherent. I would like to know if I'm wrong or not.
I understand &quot;cross-validation on the training set&quot; as doing the validation on a subsample of the training set, e.g. doing k-fold cross validation. But if you split the dataset into training, development, and test set, why would you do validation within the training set, as I understand the authors assert (second bold part)? The validation should be done on the development set, shouldn't it? Is that a semantic mistake or are they doing something I don't understand?</p>
","bert"
"{
  ""id"": 114687,
  ""title"": ""Bertopic with embedding: unable to use find_topic""
}","Bertopic with embedding: unable to use find_topic","2022-09-26 00:24:00","","2","518","<python><nlp><training><bert>","<p>I've used BERTopic with success for the following tasks: get topics, visualise (topics, barcharts, documents ...) and DTM (extended to get area plot with considerable success).</p>
<p>However, I am unable to use the find_topics() function</p>
<blockquote>
<p><em>(There are a few others I'm struggling with, which I'll post as new questions so as not to conflate this one).</em></p>
</blockquote>
<p>I get an error message indicating that I'm using embedding (which is true).</p>
<pre class=""lang-py prettyprint-override""><code># Prepare embeddings using default 'sentence embedding'
sentence_model = SentenceTransformer(&quot;all-MiniLM-L6-v2&quot;)
embeddings = sentence_model.encode(docs_bert, show_progress_bar=True)
</code></pre>
<p>Trying to solve that, I have tried to instantiate a new model without embedding</p>
<pre class=""lang-py prettyprint-override""><code>model_ngram_embed2 = BERTopic(embedding_model=embeddings)
</code></pre>
<p>but it then throws an error:</p>
<blockquote>
<p>ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()</p>
</blockquote>
<p>I need to instantiate before I can fit_transform the model to my doc (text corpus), after which I would then be able to find_topics().<br />
How do I go about that? What should be done?</p>
<p>Regarding find_topics(), I've read allowing <em><a href=""https://github.com/MaartenGr/BERTopic/issues/79"" rel=""nofollow noreferrer"">precomputed embeddings in bertopic.find_topics()</a></em> issue<br />
NB: <em>Python 3.8.8 | IPython 7.31.1 | BERTopic 0.11.0</em></p>
","bert"
"{
  ""id"": 114670,
  ""title"": ""One word changes everything NLP""
}","One word changes everything NLP","2022-09-24 20:25:10","114672","0","65","<deep-learning><nlp><transformer><bert>","<p>I have a classification model (BERT) that classifies sentences as either question or normal sentences. But whenever a sentence has &quot;how&quot; word, the model chooses &quot;question&quot; class.
How can I solve this issue? (I have a very big dataset.)</p>
","bert"
"{
  ""id"": 114008,
  ""title"": ""Extend BERT or any transformer model using manual features""
}","Extend BERT or any transformer model using manual features","2022-09-01 08:11:37","","2","918","<transformer><bert><text-classification>","<p>I have been doing a thesis in my citation classifications. I just implemented Bert model for the classification of citations. I have 4 output classes and I give an input sentence and my model returns an output that tells the category of citation. Now my supervisor gave me another task.</p>
<p>You have to search that whether it is possible to extend BERT or any transformer model using manual features. e.g. You are currently giving a sentence as the only input followed by its class. What if you can give a sentence, and some other features as input; as we do in other classifiers?</p>
<p>I need some guidance about this problem. How can I add an extra feature in my Bert model and the feature would be categorical not numerical.</p>
<p>This is my code what I have done for implementation of BERT model
I want to add manual features in my code</p>
<p>I am using Bert Tokenizer</p>
<pre><code>tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
</code></pre>
<p>Then I am making dataset of inputs ids and attention masks of size 256</p>
<pre><code>X_input_ids = np.zeros((len(df), 256))
X_attn_masks = np.zeros((len(df), 256))
</code></pre>
<p>This is my function of tokenization of sentences</p>
<pre><code>def generate_training_data(df, ids, masks, tokenizer):
    for i, text in tqdm(enumerate(df['Citing Sentence'])):
        tokenized_text = tokenizer.encode_plus(
            text,
            max_length=256, 
            truncation=True, 
            padding='max_length', 
            add_special_tokens=True,
            return_tensors='tf'
        )
        ids[i, :] = tokenized_text.input_ids
        masks[i, :] = tokenized_text.attention_mask
    return ids, masks
</code></pre>
<p>Then I am generating input_ids and attention_masks</p>
<pre><code>from tqdm.auto import tqdm
X_input_ids, X_attn_masks = generate_training_data(df, X_input_ids, X_attn_masks, tokenizer)
</code></pre>
<p>Then I made set of size 4 because my output has 4 categories</p>
<ol>
<li>Related work</li>
<li>Comparison</li>
<li>Using the work</li>
<li>Extending the work</li>
</ol>
<p>One-hot encoded target tensor. Here follow-up is my output array array</p>
<pre><code>labels = np.zeros((len(df), 4))
labels[np.arange(len(df)), df['Follow-up'].values] = 1
dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))

def CitationDatasetMapFunction(input_ids, attn_masks, labels):
    return {
        'input_ids': input_ids,
        'attention_mask': attn_masks
    }, labels
</code></pre>
<p>converting to required format for tensorflow dataset</p>
<pre><code>dataset = dataset.map(CitationDatasetMapFunction)
</code></pre>
<p>batch size, drop any left out tensor</p>
<pre><code>dataset = dataset.shuffle(10000).batch(16, drop_remainder=True)
</code></pre>
<p>for each 4 batch of data we will have len(df)//16 samples, take 80% of that for train.</p>
<pre><code>p = 0.8
train_size = int((len(df)/16)*p) 

train_dataset = dataset.take(train_size)
val_dataset = dataset.skip(train_size)
</code></pre>
<p>Summarising the model</p>
<pre><code>from transformers import TFBertModel
model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights

input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')
attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')

bert_embds = model(input_ids, attention_mask=attn_masks)[1] # 0 -&gt; activation layer (3D), 1 -&gt; pooled output layer (2D)
intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)
output_layer = tf.keras.layers.Dense(4, activation='softmax', name='output_layer')(intermediate_layer) # softmax -&gt; calcs probs of classes

citation_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)
citation_model.summary()
</code></pre>
","bert"
"{
  ""id"": 113810,
  ""title"": ""How to use a `lr_scheduler` when you don't known how many training steps to do?""
}","How to use a `lr_scheduler` when you don't known how many training steps to do?","2022-08-24 11:45:27","","0","207","<bert><transformer><learning-rate>","<p>I am trying to fine-tune a BERT model, but instead of doing it a fix number of training step, I want to use a stalling policy and allow it to run until the model stalls for N evaluations. However, I was previously using the <code>transformers.get_linear_schedule_with_warmup</code>, which requires an explicit number of training steps.</p>
<p>Is there any other learning rate scheduler that I should use for this task?</p>
","bert"
"{
  ""id"": 113489,
  ""title"": ""Input length of Sentence BERT""
}","Input length of Sentence BERT","2022-08-13 19:51:57","113498","0","376","<nlp><bert>","<p>Can Sentence Bert embed an entire paragraph instead of only a sentence? For example, a description of a movie.</p>
<p>If so, what if the word counts exceeded the input limit? I remember Bert can only take up to 512 tokens as inputs?</p>
","bert"
"{
  ""id"": 113374,
  ""title"": ""**tokens when tokens is a dictionary""
}","**tokens when tokens is a dictionary","2022-08-10 08:03:41","","0","71","<python><nlp><bert>","<p>Trying to understand the code from <a href=""https://www.analyticsvidhya.com/blog/2021/05/measuring-text-similarity-using-bert/"" rel=""nofollow noreferrer"">https://www.analyticsvidhya.com/blog/2021/05/measuring-text-similarity-using-bert/</a></p>
<p>I am looking at understanding the syntax on these two lines:</p>
<pre><code>token['input_ids'] = torch.stack(token['input_ids'])
token['attention_mask'] = torch.stack(token['attention_mask'])
</code></pre>
<pre><code>output = model(**token)
output.keys()
</code></pre>
<p>What does <code>**tokens</code> do? I can't seem to print it, or debug its value. I get a <code>Syntax Error</code> exception</p>
<p>I am familiar with the role of <code>**arg</code> in function calls, where it changes an expression like (a=1, b=2) to a dictionary like <code>{'a':1, 'b':2}</code>, but what does it do when the expression is already a dictionary? Or am I misunderstanding something here?</p>
","bert"
"{
  ""id"": 113359,
  ""title"": ""why there is no preprocessing step for training BERT?""
}","why there is no preprocessing step for training BERT?","2022-08-09 12:33:30","113366","3","2470","<deep-learning><nlp><bert>","<p>I would like to train a BERT model from scratch. I read the paper as well as a few online material. It seems there is no preprocessing involved. e.g. removing punctuation, stopwords ...</p>
<p>I wonder why is it like that and would that improve them model if I do so ?</p>
","bert"
"{
  ""id"": 113091,
  ""title"": ""Usage of Word2Vec""
}","Usage of Word2Vec","2022-07-29 17:03:09","","2","323","<machine-learning><deep-learning><nlp><lstm><bert>","<p>Sorry for the basic doubt,</p>
<p>I would like to know if I can use my Word2Vec straight for classification without using LSTM. My assumption is it’s not possible because the ordering of the words will not be take into account. Hence it wont perform for classification.</p>
<p>But we use BERT embeddings for classification. But in this case, BERT generates embedding based on context of the sentence. Hence we can use it for classification. Is my understanding right?</p>
<p>BERT achieves what LSTM learns through ordering without sequential processing, It finds an embedding by processing the sentence as a whole. LSTM also tries to represent a sentence using some context but does it through sequential processing. Is my understanding right?</p>
","bert"
"{
  ""id"": 113070,
  ""title"": ""Transformers vs RNN basic doubt""
}","Transformers vs RNN basic doubt","2022-07-29 09:44:23","113075","0","123","<machine-learning><nlp><lstm><bert><transformer>","<p>I have a basic doubt. Kindly clarify this.</p>
<p>My doubt is, When we are using LSTM's, We pass the words sequentially and get some hidden representations.</p>
<p>Now transformers also does the same thing except non sequentially. But I have seen that the output of BERT based models can be used as word embeddings.</p>
<p>Why can't we use the output of LSTM also as a word embedding? I can find sentence similarity and all with LSTM also ?</p>
<p>For eg : If I have a sentence &quot; is it very hot out there&quot;</p>
<p>Now I will apply word2Vec and get dense representations and pass it to my LSTM model. The output of my LSTM can also be used as word embeddings as we do the same with BERT?</p>
<p>My understanding was that LSTM is used to identify dependencies between words and using that learned weights to perform classification/similar tasks.</p>
","bert"
"{
  ""id"": 112998,
  ""title"": ""How can I build and train mode for Arabic word embedding from scratch using BERT and share the model on hugging face?""
}","How can I build and train mode for Arabic word embedding from scratch using BERT and share the model on hugging face?","2022-07-27 10:49:05","","0","155","<python><bert><topic-model>","<p>my project is (building an Arabic word embedding model). I want to build my own model on hugging face like (aubmindlab/AraBERT model) for Arabic language using Bert for word embedding.
How can I start from scratch to collect data and a pre-training model?
please, can anyone explain the steps to me?</p>
","bert"
"{
  ""id"": 112826,
  ""title"": ""Limitations of NLP BERT model for sentiment analysis""
}","Limitations of NLP BERT model for sentiment analysis","2022-07-20 13:06:43","112832","0","480","<machine-learning><nlp><bert><text-classification>","<p>I am reading a <a href=""https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3757135"" rel=""nofollow noreferrer"">paper</a>, where the authors assess online public sentiment in China in response tot the government's policies during Covid-19, using a Chinese BERT model. The author's objective is not only to learn whether a given online post is critical or supportive, but also learning to whom each post was directed at (e.g. CCP, local governments, health ministry, etc). To achieve this, the authors further state in pages 8 through 9, that they,&quot;To train the classifer, we randomly sample approximately 5,000
posts from each dataset (10,541 posts in total), stratified by post creation data. This sample is used for a number of analyses, and we refer to it as the Hand-Annotated Sample.&quot;</p>
<p>My question here is what's the value of using human-annotated posts in combination with a BERT sentiment analysis model?</p>
<p>Specifically, my understanding of BERT as a technique is that it eliminates or at least minimizes the need for pre-labelling a sample of text for sentiment analysis purposes, and it's not clear to me why we still need hand-annotated text by humans even when using BERT.</p>
","bert"
"{
  ""id"": 112556,
  ""title"": ""Why do RNN text generation models treat word prediction as a classification task?""
}","Why do RNN text generation models treat word prediction as a classification task?","2022-07-10 19:17:26","112574","1","216","<rnn><word-embeddings><bert><text-generation>","<p>In many of the sources I have found regarding text generation with word-based RNN models (LSTM or GRU), the model is trained to perform a classification task across the vocabulary (such as with categorical cross-entropy loss) to predict the next word. An example can be found <a href=""https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"" rel=""nofollow noreferrer"">here</a> for starters. Over a large vocabulary, this gets computationally expensive.</p>
<p>To me, it seems much more practical to first get contextual embeddings for each word in the training/testing dataset by using a pre-trained model like BERT. Then the sequential model could predict words using a loss function that measures the distance between predicted and actual embeddings with MSE or cosine similarity (<a href=""https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html"" rel=""nofollow noreferrer"">CosineEmbeddingLoss</a>). A lookup in the embedding space could return the word nearest to each prediction to make the output human-readable.</p>
<p>Is there anything wrong with the outlined approach or is it viable for text generation? The softmax operator and the classification task seem needlessly expensive for large vocabularies. Although BERT cannot be used to directly generate text (<a href=""https://ai.stackexchange.com/questions/9141/can-bert-be-used-for-sentence-generating-tasks"">can bert be used for sentence generating tasks</a>) I see nothing wrong with training a new model using BERT's embeddings or the embeddings of a similar model (see &quot;BERT for feature extraction&quot; <a href=""http://jalammar.github.io/illustrated-bert/"" rel=""nofollow noreferrer"">here</a>).</p>
","bert"
"{
  ""id"": 112438,
  ""title"": ""How to get all 3 labels' sentiment from finbert instead of the most likely label's?""
}","How to get all 3 labels' sentiment from finbert instead of the most likely label's?","2022-07-06 07:23:47","112446","0","1381","<bert><transformer><sentiment-analysis><huggingface>","<p>I'm using bert to do sentiment analysis. I previous used cardiffnlp's twitter-roberta-base-sentiment, <a href=""https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment"" rel=""nofollow noreferrer"">https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment</a>.</p>
<p>It gives the the usage on its page.</p>
<pre><code>from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer
import numpy as np
from scipy.special import softmax
import csv
import urllib.request

# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []
 
 
    for t in text.split(&quot; &quot;):
        t = '@user' if t.startswith('@') and len(t) &gt; 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return &quot; &quot;.join(new_text)

# Tasks:
# emoji, emotion, hate, irony, offensive, sentiment
# stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary

task='sentiment'
MODEL = f&quot;cardiffnlp/twitter-roberta-base-{task}&quot;

tokenizer = AutoTokenizer.from_pretrained(MODEL)

# download label mapping
labels=[]
mapping_link = f&quot;https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt&quot;
with urllib.request.urlopen(mapping_link) as f:
    html = f.read().decode('utf-8').split(&quot;\n&quot;)
    csvreader = csv.reader(html, delimiter='\t')
labels = [row[1] for row in csvreader if len(row) &gt; 1]

# PT
model = AutoModelForSequenceClassification.from_pretrained(MODEL)
model.save_pretrained(MODEL)

text = &quot;Good night 😊&quot;
text = preprocess(text)
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)

# # TF
# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)
# model.save_pretrained(MODEL)

# text = &quot;Good night 😊&quot;
# encoded_input = tokenizer(text, return_tensors='tf')
# output = model(encoded_input)
# scores = output[0][0].numpy()
# scores = softmax(scores)
</code></pre>
<p>It shows sentiments of all three labels, positive, neutral and negative.</p>
<p>However, I'm now trying to use Finbert from ProsusAI to do sentiment analysis <a href=""https://huggingface.co/ProsusAI/finbert"" rel=""nofollow noreferrer"">https://huggingface.co/ProsusAI/finbert</a>. It doesn't give me its usage on its page. So I'm following this tutorial <a href=""https://towardsdatascience.com/effortless-nlp-using-pre-trained-hugging-face-pipelines-with-just-3-lines-of-code-a4788d95754f"" rel=""nofollow noreferrer"">https://towardsdatascience.com/effortless-nlp-using-pre-trained-hugging-face-pipelines-with-just-3-lines-of-code-a4788d95754f</a>.</p>
<p>My code is</p>
<pre><code>from transformers import pipeline
classifier = pipeline('sentiment-analysis', model='ProsusAI/finbert')
classifier('Stocks rallied and the British pound gained.')
</code></pre>
<p>However, the result is <code>[{'label': 'positive', 'score': 0.8983612656593323}]</code>. It only shows the sentiment of the most likely label's (positive). But I need all three labels' sentiment (positive, neutral and negative). How should I use it?</p>
","bert"
"{
  ""id"": 112402,
  ""title"": ""What did Sentence-Bert return here?""
}","What did Sentence-Bert return here?","2022-07-05 04:20:21","112405","0","305","<bert><transformer><information-retrieval><huggingface><information-extraction>","<p>I used sentence bert to embed sentences from this tutorial <a href=""https://www.sbert.net/docs/pretrained_models.html"" rel=""nofollow noreferrer"">https://www.sbert.net/docs/pretrained_models.html</a></p>
<pre><code>from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('all-mpnet-base-v2')
</code></pre>
<p>This is the event triples <code>t</code> I forgot to concat into sentences,</p>
<pre><code>[('U.S. stock index futures', 'points to', 'start'),
 ('U.S. stock index futures', 'points to', 'higher start')]
</code></pre>
<p><code>model.encode(t)</code> returns a 2d array of shape (2,768), with two idential 768-dimension vectors, and its value is different from both <code>model.encode('U.S. stock index futures')</code> and <code>model.encode('U.S. stock index futures points to start')</code>. What could possibly have it returned?</p>
<p>It is the same situation for other models on huggingface such as <a href=""https://huggingface.co/sentence-transformers/stsb-distilbert-base"" rel=""nofollow noreferrer"">https://huggingface.co/sentence-transformers/stsb-distilbert-base</a></p>
","bert"
"{
  ""id"": 112277,
  ""title"": ""How to reduce the size of Bert model(checkpoint/model_state.bin) using pytorch""
}","How to reduce the size of Bert model(checkpoint/model_state.bin) using pytorch","2022-06-30 16:14:40","","0","391","<machine-learning><pytorch><data-science-model><bert>","<p>I used <code>torch.quantization.quantize_dynamic</code> to reduce the model size but it is reducing my prediction Accuracy score.</p>
<p>I'm using that model file inside the Flask and doing some real time predictions, Due to the large size i'm facing issues while predicting. So could anyone please help me on reducing the bert model size using pytorch and guide me on who to do the real time predictions.</p>
","bert"
"{
  ""id"": 112095,
  ""title"": ""Mapping of an unseen Field/word to an existing description (in the input data), given Field and their respective descriptions as input/training data""
}","Mapping of an unseen Field/word to an existing description (in the input data), given Field and their respective descriptions as input/training data","2022-06-24 07:51:35","","0","43","<nlp><bert><semantic-similarity>","<p>I am working on a NLP problem.</p>
<p><strong>Problem Statement</strong></p>
<p>Given the input of fields &amp; Labels and the respective descriptions, the goal is to the map a new unseen field to one of the most appropriate definitions.</p>
<p><strong>Sample Dataset</strong></p>
<div class=""s-table-container"">
<table class=""s-table"">
<thead>
<tr>
<th style=""text-align: left;""><strong>Label</strong></th>
<th style=""text-align: left;""><strong>Descriptions</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style=""text-align: left;"">Release Date</td>
<td style=""text-align: left;"">Date of formal issuance</td>
</tr>
<tr>
<td style=""text-align: left;"">Language</td>
<td style=""text-align: left;"">The language of the dataset</td>
</tr>
</tbody>
</table>
</div>
<p>So, for any new field, I want to map the field to one of the existing definition</p>
<p><strong>Approach</strong></p>
<p>I used a bert model for embedding combined with cosine similarity to compute the similarity between every new field (test unseen word) with the existing fields and take the definition of the most similar existing field.</p>
<p><strong>Challenges</strong></p>
<p>As my dataset is very small, the results are not great.</p>
<p><strong>Question to the Community</strong></p>
<p>Is it possible to generate a synthetic dataset that simulates the existing dataset, in order to have a bigger dataset which is &quot;contextually similar&quot; to my existing dataset. That might help my results to improve.</p>
<p>Would greatly appreciate any assistence.</p>
<p>Best regards,</p>
<p>nitin</p>
","bert"
"{
  ""id"": 112081,
  ""title"": ""Using BERT embeddings as input for transformer architecture""
}","Using BERT embeddings as input for transformer architecture","2022-06-23 18:53:07","","0","331","<deep-learning><nlp><word-embeddings><bert><transformer>","<p>I will use BERT's embedding weights (as discussed <a href=""https://discuss.huggingface.co/t/how-to-get-embedding-matrix-of-bert-in-hugging-face/10261/4"" rel=""nofollow noreferrer"">here</a>) for embedding in embedding layers of the transformer model. But my question is: don't embeddings of BERT already go through the whole encoding layer and got that matrix? Why shouldn't I just remove-freeze the encoding layer and use BERT embedding vectors as input for the decoding layer? And also I will use BERT embeddings in the input of the decoding layer. Why should I not freeze attention layers in decoder layer too? Because embeddings of output text already have attention information?</p>
","bert"
"{
  ""id"": 112072,
  ""title"": ""What are the inputs of encoder and decoder layers of transformer architecture?""
}","What are the inputs of encoder and decoder layers of transformer architecture?","2022-06-23 14:55:58","","1","497","<nlp><word-embeddings><bert><transformer><tokenization>","<p>In the paper (attention is all you need), it says &quot;embeddings&quot; are the input of the encoding layer. As I know embeddings are the numerical representation of words which is (for example) the output of bert model.</p>
<p>In the other hand, In BERT paper says the input of BERT is tokenized sentence <a href=""https://i.sstatic.net/b9IOH.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/b9IOH.png"" alt=""here"" /></a>.</p>
<p>Since encoder part of the transformer is BERT, In transformer architecture(<a href=""https://i.sstatic.net/fpxwR.png"" rel=""nofollow noreferrer"">this</a>) is the input of the encoder &quot;tokenized sentence&quot; or &quot;embedded sentence&quot;?</p>
<p>In short: what is the input of the encoder and decoder layer in transformers? Please provide an example.</p>
<p>Thanks</p>
","bert"
"{
  ""id"": 112063,
  ""title"": ""Parameters for training a sentence-similarity model using Bert?""
}","Parameters for training a sentence-similarity model using Bert?","2022-06-23 09:19:20","","0","56","<nlp><bert><finetuning><semantic-similarity>","<p>I have a list of sentences :</p>
<pre><code>sentences = [&quot;Missing Plate&quot;, &quot;Plate not found&quot;]
</code></pre>
<p>I am trying to find the most similar sentences in the list by using Transformers model with <a href=""https://huggingface.co/sentence-transformers/paraphrase-MiniLM-L6-v2"" rel=""nofollow noreferrer"">Huggingface embedding</a>. I am able to find the similar sentences but the model is still not able to identify the difference between :</p>
<pre><code>&quot;Message ID exists&quot;  
&quot;Message ID doesn't exist&quot;
</code></pre>
<p>[Note: I am trying to find the similarity by using the <a href=""https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.cos_sim"" rel=""nofollow noreferrer"">Cosine similarity</a> from pytorch]</p>
<p>Can you suggest me ways to hyperparameter tune my model so that the model can weigh in more on the negative words and consider them opposite?</p>
<p>I found the <a href=""https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/trainer#transformers.TrainingArguments"" rel=""nofollow noreferrer"">list of parameters that can be tuned</a> but not sure what the best parameters would be</p>
<p>Thanks!</p>
","bert"
"{
  ""id"": 111948,
  ""title"": ""Model to implement Question Answering System over structured data""
}","Model to implement Question Answering System over structured data","2022-06-19 13:03:11","","1","435","<machine-learning><nlp><bert><transformer><question-answering>","<p>I need to write a program(like a chatbot) that retrieves an answer from a CSV datafile based on a question user asks. So for example if the CSV stores list of products and its specifications in 5-10 columns, then if a user asks a question about specification Y for product X the program should return the correct answer based on CSV. I need to use NLP as the user can write synonyms of a particular word or ask a question a bit differently from the keywords in the dataset.</p>
<p>I think I am supposed to use BERT model using HuggingFace Transformer, but I'm not sure how to use NLP as this is over structured data. Additionally, I don't have a list of questions generated already.</p>
<p>Does anyone suggest how I should do this.</p>
<p>Also some of the specifications are values like prices. I was wondering if there is a way for the program to return the average or sum of two or more products if the user asks that question.</p>
","bert"
"{
  ""id"": 111891,
  ""title"": ""BertTokenizer Loading Problem""
}","BertTokenizer Loading Problem","2022-06-16 17:53:57","111901","1","2070","<deep-learning><nlp><pytorch><bert>","<p>I loaded this BertTokenizer previously, but now it is showing, I have to make sure I don't have a local directory. In my kaggle kernel, I don't have this local directory.
How to solve it?</p>
<pre><code>class Config:
    DEVICE = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
    LR = 2e-5
    TRAIN_BATCH_SIZE = 16
    TEST_BATCH_SIZE = 8
    EPOCHS = 10
    N_FOLD = 5
    TOKENIZER = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)
    CLASSES = 3
    MAX_LEN = 200
    TRAIN_CSV = &quot;.csv&quot;
    TEST_CSV = &quot;test.csv&quot;
    API = &quot;#&quot;
    PROJECT_NAME = &quot;bert-base2&quot;
    MODEL_NAME = &quot;bert-large-uncased&quot;
</code></pre>
<pre><code>OSError: Can't load tokenizer for 'bert-large-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-large-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer.
</code></pre>
","bert"
"{
  ""id"": 111735,
  ""title"": ""Can I use Sentence-Bert to embed event triples?""
}","Can I use Sentence-Bert to embed event triples?","2022-06-12 07:26:13","111766","1","163","<nlp><bert><information-retrieval><information-extraction>","<p>I extracted event triples from sentences using OpenIE. Can I concatenate the components in the event triple to make it a sentence and use Sentence-Bert to embed it?
It seems no one has done this way before so I am questioning my idea.</p>
<p>I'm using news headlines to predict next day's stock movement. For example, there are two news headlines, the first is <em>&quot;U.S. stock index futures points to higher start&quot;</em>, I used openIE to extract it and there are two event triples, [('U.S. stock index futures', 'points to', 'start'), ('U.S. stock index futures', 'points to', 'higher start')]. (There are repetition in the openIE extracted event triples and I don't know how to avoid it.) Since it contains events I'm interested in (stock index), I will embed these two events and take their mean as the the embedding.</p>
<p>The second headline is <em>&quot;STOCKS NEWS US- Economic and earnings diary for Jan 4&quot;</em>, it contains no events as it is only contain nouns. So I will embed it as 0 vector in this case.</p>
","bert"
"{
  ""id"": 111515,
  ""title"": ""Using BERT instead of word2vec to extract most similar words to a given word""
}","Using BERT instead of word2vec to extract most similar words to a given word","2022-06-02 19:59:44","","0","1013","<nlp><word2vec><bert>","<p>I am fairly new to BERT, and I am willing to test two approaches to get &quot;the most similar words&quot; to a given word to use in Snorkel labeling functions for weak supervision.</p>
<p>Fist approach was to use word2vec with pre-trained word embedding of &quot;word2vec-google-news-300&quot; to find the most similar words</p>
<pre><code>@labeling_function()
def lf_find_good_synonyms(x):
  good_synonyms = word_vectors.most_similar(&quot;good&quot;, topn=25) ##Similar words are extracted here
  good_list = syn_list(good_synonyms) ##syn_list just returns the stemmed similar word
  return POSITIVE if any(word in x.stemmed for word in good_list) else ABSTAIN
</code></pre>
<p>The function basically looks for the word &quot;good&quot; or any of it's similar words in a sentence (the sentences have been previously stemmed so are the words as the function syn_list returns the stem of each similar word) if found, the function will simply label the sentence as POSITIVE.</p>
<p>The issue here is that my word vectors are based on word2vec, which does not respect context. I was wondering if I could use BERT instead, and since I am looking for similar words to a certain 'word', how do I include context?</p>
<p>Will using BERT improve the performance much as labeling functions are allowed to be lousy?</p>
","bert"
"{
  ""id"": 111260,
  ""title"": ""NLP - support comments analysis""
}","NLP - support comments analysis","2022-05-24 14:39:39","","0","35","<python><nlp><bert>","<p>I am new to NLP and looking for some direction since after all my reading I haven't found a definite approach and the subject matter is vast. The project is to focus on specific fields of support comments using NLP and Python. The goal is that from the comments I want to verify that the comment is in fact a well made comment for that field. Some requirements is the context of entered text is relevant to the field it has been entered in, it is informative based on the specific field, it is not just a few unhelpful words entered, it can detect that words/sentences have similar semantic meaning (e.g problem is, issue encountered etc).</p>
<p>I first looked into TF-IDF but it doesn't consider context. I have been reading into other deep learning models such as BERT as it includes context. I have read though that BERT was designed more for sentence prediction and missing words rather than for comparing semantic meaning of multiple sentences. USE would be better for comparing if sentences are similar although all the comments will have a general theme and I doubt they will be similar enough to design a system around. Topic modelling sounded like what i'm looking for but I think it's more for general topics you find in newspapers and books rather than comment sections. For the text summarisation option maybe there is not enough text in each comment to give a helpful summary. I think I would prefer unlabelled data if possible so as to minimise manual input but if it's neccessary then we could evaluate it. Is there something in fine tuning a pretrained model such as BERT to analyse all the existing comments for a specific field and somehow extract from that if a new comment is informative and relevant by comparing it to the existing ones?</p>
<p>Example field would be &quot;Problem description&quot;. I am looking for direction on how to classify the content of that field as relevant and informative.</p>
<p>Any direction is appreciated or if you know of existing related material, papers, videos or tutorials.</p>
<p>Thanks</p>
","bert"
"{
  ""id"": 110865,
  ""title"": ""BERT base uncased required gpu ram""
}","BERT base uncased required gpu ram","2022-05-11 15:28:37","110879","1","2157","<bert><transformer><gpu>","<p>I'm working on an NLP task, using BERT, and I have a little doubt about GPU memory.</p>
<p>I already made a model (using DistilBERT) since I had out-of-memory problems with tensorflow on a RTX3090 (24gb gpu's ram, but ~20.5gb usable) with BERT base model.</p>
<p>To make it working, I limited my data to 1.1 milion of sentences in training set (truncating sentences at 128 words), and like 300k in validation, but using an high batch size (256).</p>
<p>Now I have the possibility to retrain the model on a Nvidia A100 (with 40gb gpu's ram), so it's time to use BERT base, and not the distilled version.</p>
<p>My question is, if I reduce the batch size (e.g. from 256 to 64), will I have some possibilities to increase the size of my training data (e.g. from 1.1 to 2-3 milions), the lenght of sentences (e.g. from 128 to 256, or 198) and use the bert base (which has a lot of trainable params more than distilled version) on the 40gb of the A100, or it's probably that I will get an OOM error?</p>
<p>I ask this because I haven't unlimited tries on this cluster, since I'm not alone using it (plus I have to prepare data differently in each case, and it has a quite high size), so I would have an estimation on what could happen.</p>
","bert"
"{
  ""id"": 110454,
  ""title"": ""How do i generate text from ids in Torchtext's sentencepiece_numericalizer?""
}","How do i generate text from ids in Torchtext's sentencepiece_numericalizer?","2022-04-28 14:05:11","110477","0","246","<python><nlp><pytorch><bert><transformer>","<p>The torchtext <code>sentencepiece_numericalizer()</code> outputs a generator with indices SentencePiece model corresponding to token in the input sentence. From the generator, I can get the ids.</p>
<p>My question is how do I get the text back after training?</p>
<p>For example</p>
<pre><code>&gt;&gt;&gt; sp_id_generator = sentencepiece_numericalizer(sp_model)
&gt;&gt;&gt; list_a = [&quot;sentencepiece encode as pieces&quot;, &quot;examples to   try!&quot;]
&gt;&gt;&gt; list(sp_id_generator(list_a))
    [[9858, 9249, 1629, 1305, 1809, 53, 842],
     [2347, 13, 9, 150, 37]]
</code></pre>
<p>How do I convert <code>list_a</code> back t(i.e <code>&quot;sentencepiece encode as pieces&quot;, &quot;examples to try!&quot;</code>)?</p>
","bert"
"{
  ""id"": 110310,
  ""title"": ""Should I pretrain my BERT model on specific dataset if it has only one class of labels?""
}","Should I pretrain my BERT model on specific dataset if it has only one class of labels?","2022-04-24 12:48:40","","0","112","<machine-learning><deep-learning><nlp><bert><transformer>","<p>I want to use BERT model for sentences similarity measuring task. I know that BERT models were trained with natural language inference architecture with dataset with labels neutral, entailment, contradiction.</p>
<p>My data to which I want to apply BERT for sentences similarity task has very specific terms and jargon, so I want to pretrain model on it before. But in that data there are only cases of entailment labels (about 20k rows). Is it a good idea to pretrain model on that data? How could I handle my problem the best way?</p>
<p>Thanks in advance</p>
","bert"
"{
  ""id"": 110119,
  ""title"": ""Why shouldn't we mask [CLS] and [SEP] in preparing inputs for a MLM?""
}","Why shouldn't we mask [CLS] and [SEP] in preparing inputs for a MLM?","2022-04-18 03:28:43","","2","803","<nlp><bert><masking>","<p>I know that MLM is trained for predicting the index of MASK token in the vocabulary list, and I also know that [CLS] stands for the beginning of the sentence and [SEP] telling the model the end of the sentence or another sentence will come soon, but I still can't find the reason for unmasking the [CLS] and [SEP].</p>
<p>Here is the situation that I imagine:
We have a sentence pair like s1/s2, we input its input_ids into the model as the form like &quot;101 xxx 102 yyy 102&quot; and then I think we can ask model predict the token at the middle of the 'xxx' and 'yyy'(namely the first 102), so we can mask the token as 103 which means the MASK token.</p>
<p>I think the imagination is reasonable, could anyone give me a key?</p>
","bert"
"{
  ""id"": 110034,
  ""title"": ""When would you use word2vec over BERT?""
}","When would you use word2vec over BERT?","2022-04-15 05:16:08","","0","317","<word2vec><bert>","<p>I am very new to Machine Learning and I have recently been exposed to word2vec and BERT.</p>
<p>From what I know, word2vec provides a vector representation of words, but is limited to its dictionary definition. This would mean the algorithm may output the unwanted definition of a word with multiple meanings.</p>
<p>BERT on the other hand, is able to use context clues in the sentence to describe the true meaning of the word.</p>
<p>To me, it sounds like BERT would always be the better choice when it comes to identifying the definition of a word.</p>
<p>Could someone explain when word2vec would be more useful?</p>
","bert"
"{
  ""id"": 109953,
  ""title"": ""How does bert produce variable output shape?""
}","How does bert produce variable output shape?","2022-04-13 03:51:15","","1","420","<machine-learning><pytorch><bert>","<p>Suppose if I provide a list of sentences:</p>
<pre><code>['I like python',
 'I am learning python', # longest sentence of length 4 tokens
 'Python is simple']
</code></pre>
<p>Bert will produce an output of (3 * 4+2 * 768).<br />
Because there were 3 sentences, 4 max tokens, 768 hidden states.</p>
<p>Suppose if I provide another list of sentences:</p>
<pre><code>['I like python',
 'I am learning python',
 'Python is simple',
 'Python is fun to learn' # 5 tokens
]
</code></pre>
<p>The new embedding output would be (4 * 5+2 * 768).</p>
<p>I understand that dim[0] becomes 4 because there is now 4 sentences instead. This is achieved by increasing the rows of the tensor(batch size) during tensor computation.</p>
<p>I also understand that dim[1] becomes 5+2 because the max number of token is number 5 and there is [CLS] and [SEP] tokens at the start and end.<br />
I also understand that there is a <strong>padding mechanism</strong> that accepts up to a <strong>max_position_embeddings=512</strong> for bert model.</p>
<hr>
<p>What I want to ask is:</p>
<ul>
<li>during computation, does bert pad all the values after 5th element with zeros and process with computation using a input of (4 * 512) (4 sentences, 512 max tokens).</li>
<li>then after computation from the output of (4 * 512 * 768), the tensor is trimmed to output: (4 * 5+2 * 768).</li>
<li>if the above assumptions is true, isn't it a huge waste of resources, since majority of the 512 tokens are not attention-required.</li>
<li>I read about the <strong>attention_mask</strong> matrix that tells the model which are the tokens needed for computation, but I don't understand how does <strong>attention_mask</strong> achieve this; when the architecture of the model is initialised with N dimensional inputs, how does attention_mask help during computation to ignore/avoid the computation of the attention-masked elements?</li>
<li>which part of the bert model explicitly restrict the output to (4 * 5+2 * 768)?</li>
</ul>
","bert"
"{
  ""id"": 109904,
  ""title"": ""Can I use Bert on data subsets and get a compatible representation for the whole dataset?""
}","Can I use Bert on data subsets and get a compatible representation for the whole dataset?","2022-04-11 20:46:42","","1","15","<nlp><bert>","<p>I need to build an embedding for a massive amount of phrases. I want to use BERT (through the library <a href=""https://www.sbert.net/"" rel=""nofollow noreferrer"">https://www.sbert.net/</a>).</p>
<p>Can I build a partial representation of the data, say encoding 1000 sentences and then another 1000 and join the matrices at the end? If I generate the embeddings by parts (of the whole dataset), will I get a compatible vector representation between the different results? Or, on the contrary, should I build the representation with the whole dataset at the same time?</p>
<p>My final goal is to cluster and analyze the sentence vectors of the whole dataset.</p>
<p>I would appreciate any suggestions on what to read or how to better approach this question. Thanks!!!</p>
","bert"
"{
  ""id"": 109790,
  ""title"": ""Comparison between applications of vanilla transformer and BERT""
}","Comparison between applications of vanilla transformer and BERT","2022-04-08 03:14:40","109796","1","229","<bert><transformer>","<p>I try to identify applications of vanilla transformer in nlp, as well as those in BERT. But I don't seem to find good summaries for either of them. Thus my questions are:</p>
<ol>
<li>what are the applications of transformer and bert respectively?</li>
<li>in (1), why in some application vanilla transformer is used over BERT? (or vice versa?) What're the reasons?</li>
</ol>
<p>TIA.</p>
","bert"
"{
  ""id"": 109658,
  ""title"": ""Hugging face Model Output 'last_hidden_state'""
}","Hugging face Model Output 'last_hidden_state'","2022-04-04 10:59:53","","0","1601","<pytorch><bert><huggingface><bart>","<p>I am using the Huggingface BERTModel, The model gives <strong>Seq2SeqModelOutput</strong> as output. The output contains the past hidden states and the last hidden state.
These are my questions</p>
<ol>
<li>What is the use of the hidden states?</li>
<li>How do I pass my hidden states to my output layer?</li>
<li>What I actually want is the output tokens, from the model how do I get the prediction tokens?</li>
</ol>
","bert"
"{
  ""id"": 109391,
  ""title"": ""Which is the difference between the two Greek BERT models?""
}","Which is the difference between the two Greek BERT models?","2022-03-26 17:29:30","109393","0","237","<nlp><pytorch><bert>","<p>I want to use the Greek BERT which can be found here <a href=""https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1"" rel=""nofollow noreferrer"">https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1</a></p>
<p>However I am confused about which model should I use and which are the differences.</p>
<p>The tokenizer is the same</p>
<pre><code>tokenizer = AutoTokenizer.from_pretrained('nlpaueb/bert-base-greek-uncased-v1')
</code></pre>
<p>but we have two models</p>
<pre><code>model = AutoModel.from_pretrained(&quot;nlpaueb/bert-base-greek-uncased-v1&quot;)
model = AutoModelWithLMHead.from_pretrained(&quot;nlpaueb/bert-base-greek-uncased-v1&quot;)
</code></pre>
<p>Which one should I use?</p>
","bert"
"{
  ""id"": 109344,
  ""title"": ""Is it okay to fine-tuning bert with large context for sequence classification?""
}","Is it okay to fine-tuning bert with large context for sequence classification?","2022-03-25 07:53:49","","1","894","<bert><finetuning>","<p>I want to create sequence classification bert model. The input of model will be 2 sentence. But i want to fine tuning the model with large context data which consists of multiple sentences(which number of tokens could be exceed 512). Is it okay if the size of the training data and the size of the actual input data are different?</p>
<p>Thanks</p>
","bert"
"{
  ""id"": 109105,
  ""title"": ""Can pre-trained transformers (I.e., BERT) handle numerical/spatial data""
}","Can pre-trained transformers (I.e., BERT) handle numerical/spatial data","2022-03-16 13:02:47","","1","126","<python><nlp><bert><geospatial><search>","<p>I’m curious to know if pre-trained transformers could handle search queries that include numerical data or make references to spatial relationships.</p>
<p>Take an example dataset of a list of restaurants, each with a distance relative to the city centre. Would transformers be able to handle:</p>
<p><em>”Which restaurant is closest to city centre?”</em></p>
<p><em>”Which restaurant is 2km from the city centre?”</em></p>
<p>Curious to know if anyone has any opinions on this or has seen any articles/examples covering this aspect of searching.</p>
<p>Thanks!</p>
","bert"
"{
  ""id"": 108740,
  ""title"": ""BERT - The purpose of summing token embedding, positional embedding and segment embedding""
}","BERT - The purpose of summing token embedding, positional embedding and segment embedding","2022-03-04 02:42:07","108752","3","2719","<nlp><bert>","<p>I read the implementation of BERT inputs processing (image below). My question is why the author chose to sum up three types of embedding (token embedding, positional embedding and segment embedding)?
<a href=""https://i.sstatic.net/NdeWj.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/NdeWj.png"" alt=""enter image description here"" /></a></p>
","bert"
"{
  ""id"": 108699,
  ""title"": ""AraBERT Overfitting for sentiment analysis""
}","AraBERT Overfitting for sentiment analysis","2022-03-02 21:14:04","","0","153","<machine-learning><python><deep-learning><bert><sentiment-analysis>","<p>I Am newbie to Machine Learning in general. I am currently trying to follow a tutorial on sentiment analysis using BERT and Transformers.</p>
<p><a href=""https://i.sstatic.net/m6t5o.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/m6t5o.png"" alt=""enter image description here"" /></a></p>
<p>I do not know how i can Read the results to know the overfitting case. Can any one help me to read this and explain to me how to know it afterwards.</p>
","bert"
"{
  ""id"": 108595,
  ""title"": ""Using KerasClassifier for training neural network""
}","Using KerasClassifier for training neural network","2022-02-27 20:39:43","","0","1783","<machine-learning><keras><nlp><bert><transformer>","<p>I created a simple neural network for binary spam/ham text classification using pretrained BERT transformer. The current pure-keras implementation works fine. I wanted however to plot certain metrics of the trained model, in particular the ROC curve. According to <a href=""https://www.dlology.com/blog/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier/"" rel=""nofollow noreferrer"">this blog post</a> I understood that this is only be possible using <code>KerasClassifier()</code> from the <code>keras.wrappers.scikit-learn</code> package which is now deprecated and has been replaced by the <code>scikeras</code> package.</p>
<p>Thus I created a <code>build_keras_nn()</code> function to build my custom BERT-based neural network. I then passed this custom function to <code>KerasClassifier()</code> as shown <a href=""https://www.adriangb.com/scikeras/stable/quickstart.html"" rel=""nofollow noreferrer"">in the documentation</a>, and fitted the model using train data.</p>
<p>At this point I got the following error message:</p>
<pre><code>ValueError: Expected 2D array, got 1D array instead: Reshape your data either using array.reshape(-1, 1) 
if your data has a single feature or array.reshape(1, -1) if it contains a single sample.
</code></pre>
<p>Alright then, I thought a simple array reshape will work but I then ran into the following error:</p>
<pre><code>ValueError: could not convert string to float: 'awful bio obviously hatchet job press release totally 
biased bad grammar'
</code></pre>
<p>So for some reason the <code>KerasClassifier</code> implementation doesn't allow me to directly input text, even though my preprocessing steps are included within the custom function <code>build_keras_nn()</code>.</p>
<p>A full reproducible code is below:</p>
<pre><code>import tensorflow_hub as hub 
import tensorflow_text as text
import tensorflow as tf 
from tensorflow.keras.layers import Input, Dropout, Dense
from tensorflow.keras.metrics import BinaryAccuracy, AUC

bert_encoder_url = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;
bert_preprocessor_url = &quot;https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3&quot;

bert_preprocessor_model = hub.KerasLayer(bert_preprocessor_url)
bert_encoder_model = hub.KerasLayer(bert_encoder_url)

df_ = pd.read_json(spam_ham_json)   # spam_ham_json: data in JSON file as a string

X_train_, X_test_, y_train_, y_test_ = train_test_split(df_['comment_text'], df_['label'])

def build_keras_nn():

    text_input = Input(shape=(), dtype=tf.string, name=&quot;text&quot;)
    preprocessed_text = bert_preprocessor_model(text_input)
    bert_output = bert_encoder_model(preprocessed_text)
    dropout = Dropout(0.1, name='dropout')(bert_output['pooled_output'])
    classification_output = Dense(1, activation='sigmoid', name='classification_output')(dropout)

    model = tf.keras.Model(inputs=[text_input], outputs=[classification_output])

    metrics_list = [AUC(name='auc'), BinaryAccuracy(name='accuracy')]
    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = metrics_list)
    return model 

# Following two rows show the pure-keras implementation: this one works.
# model = build_keras_nn()
# history = model.fit(X_train_, y_train_, epochs=5);

# Now let's see the KerasClassifier
model = KerasClassifier(build_fn=build_keras_nn)
# history = model.fit(X_train_, y_train_, epochs=5);     # &lt;-- Value Error 1
# history = model.fit(np.array(X_train_).reshape(-1,1), np.array(y_train_).reshape(-1,1), epochs=5);     # &lt;-- Value Error 2

</code></pre>
<p>Data is in json format:</p>
<pre><code>'{&quot;comment_text&quot;:{&quot;0&quot;:&quot;problem wanted say problem trying redirect event schedule pakistan NUMBERTAG NUMBERTAG pakistan mother fucker boy want married sister ohhhh love sister boob hmmmmm yummyy&quot;,&quot;1&quot;:&quot;get life fucking loser question ask ask katie goulet picture&quot;,&quot;2&quot;:&quot;cum drinker hey wat nigga thought u could ban took long cuz wa busy az hell recently ill keep cumming back take word cumdrinker&quot;,&quot;3&quot;:&quot;liar liar pant fire seriously looked contribution tennis portal page tennis page ha descussion ever please lie NUMBERTAG NUMBERTAG NUMBERTAG NUMBERTAG&quot;,&quot;4&quot;:&quot;stop writing p nothing discus given lack bsinc education diplomacy&quot;,&quot;5&quot;:&quot;wa fucking page one edit page&quot;,&quot;6&quot;:&quot;question mad gay&quot;,&quot;7&quot;:&quot;warning page nerd please leave one stay girl though pleeeeeeeeeeeeeaaaaaaaaaaaaaaaaaaaaaassssssssssssssssseeeeeeeeeeeee NUMBERTAG oneoneoneoneoneoneoneoneoneoneoenone&quot;,&quot;8&quot;:&quot;full shit&quot;,&quot;9&quot;:&quot;go fuck conrad black cheated thousand people pension anyone defends hm asshole apologist evil&quot;,&quot;10&quot;:&quot;list office bearer national union student australia wp userfy userfied page located&quot;,&quot;11&quot;:&quot;talk history scottish national party claim spying hi sentence someone belief npov claim mean someone belief npov claim&quot;,&quot;12&quot;:&quot;section meant vice review btw magazine website writer name attached also like richardwilson NUMBERTAG even know question ninjarobotpirate wa responding happy criticise answer \\u2026 \\u2026 btw NUMBERTAG far know none editor either albanian croatian maybe airplane vision quite good think take care&quot;,&quot;13&quot;:&quot;next time subtweet&quot;,&quot;14&quot;:&quot;physicsyo yo yo dog&quot;,&quot;15&quot;:&quot;self censorship tv show might might notable tv pre empted breaking news notable happens time&quot;,&quot;16&quot;:&quot;article contains information soursed huddersfield aa street street&quot;,&quot;17&quot;:&quot;utc onto something centrifugal force experienced mass exhibiting inertia result tiny little bullet hitting side ride merry go round rueda puthoff haisch described zero point field electronic lorenz equation coupling inertial frame reference give mass inertial reluctance rather resistance enable describe change velocity direction compare ac v dc tesla v edison NUMBERTAG NUMBERTAG NUMBERTAG june NUMBERTAG&quot;,&quot;18&quot;:&quot;meant wa meant state either unblock create new account rendering block useless simple&quot;,&quot;19&quot;:&quot;NUMBERTAG utc hi NUMBERTAG must mistakenly thought ian wa original member b c always viewed band definitive axeman NUMBERTAG yeah almost bought akai headrush looper year ago notorious role cab one guitarist recording settled bos loop station instead rather headrush boomerang due two reliability price issue respectively check hovercraft southpacific auburn lull kind hallucinitory guitar looping thought cab new lineup wa incredible saw NUMBERTAG skipped classic lineup NUMBERTAG compare two performance wise best NUMBERTAG NUMBERTAG NUMBERTAG may&quot;},&quot;label&quot;:{&quot;0&quot;:1,&quot;1&quot;:1,&quot;2&quot;:1,&quot;3&quot;:1,&quot;4&quot;:1,&quot;5&quot;:1,&quot;6&quot;:1,&quot;7&quot;:1,&quot;8&quot;:1,&quot;9&quot;:1,&quot;10&quot;:0,&quot;11&quot;:0,&quot;12&quot;:0,&quot;13&quot;:0,&quot;14&quot;:0,&quot;15&quot;:0,&quot;16&quot;:0,&quot;17&quot;:0,&quot;18&quot;:0,&quot;19&quot;:0}}'
</code></pre>
","bert"
"{
  ""id"": 108353,
  ""title"": ""how to use bert-tiny using transformers?""
}","how to use bert-tiny using transformers?","2022-02-20 16:02:17","108354","3","2121","<bert>","<p>how can I use BERT-tiny .. I tried to load <code>bert-base-uncased</code> by this line</p>
<pre><code>transformers.AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)
</code></pre>
<p>but how can I use BERT-tiny, please?</p>
","bert"
"{
  ""id"": 108178,
  ""title"": ""How to prepare texts to BERT/RoBERTa models?""
}","How to prepare texts to BERT/RoBERTa models?","2022-02-15 12:15:41","","1","1184","<deep-learning><nlp><bert><transformer><huggingface>","<p>I have an artificial corpus I've built (not a real language) where each document is composed of multiple sentences which again aren't really natural language sentences.</p>
<p>I want to train a language model out of this corpus (to use it later for downstream tasks like classification or clustering with sentence BERT)</p>
<p><strong>How to tokenize the documents?</strong></p>
<p>Do I need to tokenize the input</p>
<p>like this:
<code>&lt;s&gt;sentence1&lt;/s&gt;&lt;s&gt;sentence2&lt;/s&gt;</code></p>
<p>or <code>&lt;s&gt;the whole document&lt;/s&gt;</code></p>
<p><strong>How to train?</strong></p>
<p>Do I need to train an MLM or an NSP or both?</p>
","bert"
"{
  ""id"": 107981,
  ""title"": ""BERT each Word Embedding in Keras""
}","BERT each Word Embedding in Keras","2022-02-08 15:22:02","","1","198","<nlp><bert>","<p>How to use BERT to extract the embeddings of every word in a sentence.
Suppose I pass my corpus of sentences with different lengths to a BERT model , I want to be able to extract the embeddings of each word in every sentence.</p>
<p>And what is the best way to utilize every word embeddings? should I calculate their average?</p>
<p>when I follow the basic usage in this <a href=""https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2"" rel=""nofollow noreferrer"">BERT model</a>, the sequence length is always 128 regardless of the actual sentence length, I know I can change the 128 but it will be the same for all sentences. Is there a way to get an embedding for every word and when the sentence length is less than 128 then remaining embeddings appear as zeros, for example, if my sentence is 7 words, I want bert to return an embeddings of (128,128) but the useful ones are the first 7 and the remaining are zeros.</p>
","bert"
"{
  ""id"": 107883,
  ""title"": ""Pretrained German BERT""
}","Pretrained German BERT","2022-02-05 20:15:34","","0","1037","<nlp><bert>","<p>I'm looking for a (well) pretrained BERT Model in German to be adapted in a Keras/TF framework. Ideally with a minimal example on how to fine-tune the model on specific tasks, i.e. text classification!</p>
<p>Can anyone point me to some (open source) resources?</p>
","bert"
"{
  ""id"": 107725,
  ""title"": ""What are the differences between bert embedding and flair embedding""
}","What are the differences between bert embedding and flair embedding","2022-02-01 11:24:58","107748","0","1523","<deep-learning><nlp><word-embeddings><bert>","<p>I read about <code>BERT</code> embedding model and <code>FLAIR</code> embedding model, and I'm not sure I can tell what are the differences between them ?</p>
<ul>
<li><code>BERT</code> use <code>transformers</code> and <code>FLAIR</code> use <code>BLSTM</code></li>
<li>With <code>BERT</code>, we the feed words into the <code>BERT</code> architecture, and with <code>FLAIR</code> we feed characters into <code>FLAIR</code> architecture.</li>
</ul>
<ol>
<li>What are the strengths of <code>BERT</code> embedeeing ?</li>
<li>What are the strengths of <code>FLAIR</code> embedeeing ?</li>
<li>In which cases would we prefer to use one model rather than another ?</li>
</ol>
","bert"
"{
  ""id"": 107713,
  ""title"": ""Which will be best deep learning model for topic classification using NLP""
}","Which will be best deep learning model for topic classification using NLP","2022-02-01 07:29:18","","1","53","<machine-learning><deep-learning><nlp><multiclass-classification><bert>","<p>I have a dataset consisting of two columns [Text, topic_labels].
Topic_labels are of 6 categories for ex: [plants,animals,birds,insects etc]</p>
<p>I would like to build deep learning-based models in order to be able to classify topic_labels.
so far I have implemented both supervised[SVM, Logistic] &amp; unsupervised [topic-LDA, Guided-LDA] approaches in a traditional way by applying both Word2Vec &amp; TF-IDF  but I wanted to implement state-of-the-art deep learning classification techniques for the text data?</p>
<p>Suggest me the best deep learning model for text topic classification.</p>
","bert"
"{
  ""id"": 107672,
  ""title"": ""Getting the keywords of text classification prediction in real time""
}","Getting the keywords of text classification prediction in real time","2022-01-31 09:50:55","","1","106","<machine-learning><deep-learning><bert><text-classification>","<p>I am using a BERT based text classification model to classify sentences/documents. My use case is like I need to highlight the words that are responsible for classification of a particular class. This has to be done in real time with minimum amount of time. I have tried using LIME for getting the words responsible for predictions but it is very slow. Kindly suggest any techniques to have that in real time or any model architecture that can predict class along with its words responsible for prediction of that particular class.</p>
","bert"
"{
  ""id"": 107521,
  ""title"": ""What is distillation in Neural Network?""
}","What is distillation in Neural Network?","2022-01-27 14:11:00","","-1","211","<deep-learning><neural-network><bert>","<p>Distillation seems to be a general technique to reduce the size of NLP/NN models. Can anyone help me to understand intuition and how does it work?</p>
","bert"
"{
  ""id"": 107412,
  ""title"": ""Where to start with ChatBots?""
}","Where to start with ChatBots?","2022-01-24 11:20:40","","1","36","<python><nlp><lstm><bert><chatbot>","<p>I want to start my journey into ChatBots and how I can create them. I have read some articles regarding the type of chatbots. Basically there are 2 types of chatbots, one is a <strong>rule based</strong> and the other is a <strong>NLP/ML based</strong>. I am more interested in the latter and want some kind of started guide. I have read that these kind of chatbots usually use some kind of attention models (as they give state of the art results) like <strong>BERT</strong>. I have no experience in attention models. I have started with LSTM models and it's variations, as attention models have LSTM at their core I think.</p>
<p>What I want from the stalwarts in this field is some <strong>advice/starter guide (be it articles/blogs/videos) on how to get started or am I going in the right direction or not.</strong> It would really help a novice like me!</p>
<p>Thank you!</p>
","bert"
"{
  ""id"": 107212,
  ""title"": ""Get sentence embeddings of transformer-based models""
}","Get sentence embeddings of transformer-based models","2022-01-19 00:12:26","","0","1671","<nlp><bert><transformer><embeddings><huggingface>","<p>I want to get sentence embeddings of transformer-based models (Bert, Roberta, Albert, Electra...).</p>
<p>I plan on doing mean pooling on the hidden states of the second last layer just as what bert-as-service did.</p>
<p>So my questions is that when I do mean pooling, should I include the embeddings related to [PAD] tokens or [CLS] token or [SEP] token?</p>
<p>For example, my sequence is 300 tokens, and are padded into 512 tokens.</p>
<p>The output size is 512 (tokens) * 768 (embeddings).</p>
<p>So should I average the embeddings of first 300 tokens or the embeddings of whole 512 tokens?</p>
<p>Why the embeddings of the last 212 tokens are non-zero?</p>
","bert"
"{
  ""id"": 107190,
  ""title"": ""does ValueError: 'rat' is not in list means not exist in tokenizer""
}","does ValueError: 'rat' is not in list means not exist in tokenizer","2022-01-18 12:52:43","107248","1","110","<nlp><word-embeddings><bert><tokenization>","<p>Does this error means that the word doesn't exist in the tokenizer</p>
<pre><code>return sent.split(&quot; &quot;).index(word)
ValueError: 'rat' is not in list
</code></pre>
<p>the code sequences like</p>
<pre><code>def sentences():
   for sent in sentences:
       token = tokenizer.tokenize(sent)
       for i in token :
           idx = get_word_idx(sent,i)
def get_word_idx(sent: str, word: str):
    return sent.split(&quot; &quot;).index(word)
</code></pre>
<p>sentences split returns <code>['long', 'restaurant', 'table', 'with', 'rattan', 'rounded', 'back', 'chairs']</code>
which <code>rattan</code> here is the problem as i think</p>
","bert"
"{
  ""id"": 107076,
  ""title"": ""How can i get the vector of word using BERT?""
}","How can i get the vector of word using BERT?","2022-01-14 14:17:14","","1","2027","<nlp><word-embeddings><bert><representation>","<p>I need to get word-vectors using BERT and got this function that i think it should be the one i need</p>
<pre><code>def get_bert_embed_matrix(sentences):
    device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)
    model_config = transformers.AutoConfig.from_pretrained('bert-base-uncased', output_hidden_states=True)
    model = transformers.AutoModel.from_pretrained('bert-base-uncased', config=model_config)
    tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')  
   for i in sentences:
        tokenized_text = tokenizer.tokenize(i)
        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)        
        tokens_tensor = torch.tensor([indexed_tokens])
        model.eval()
        outputs = model(tokens_tensor)
        hidden_states = outputs[2]
        word_embed_6 = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)
    return word_embed_6
</code></pre>
<p>Does the method return vectors for sub-word or word ?</p>
","bert"
"{
  ""id"": 106956,
  ""title"": ""Difference between Doc2Vec and BERT""
}","Difference between Doc2Vec and BERT","2022-01-11 18:20:49","106975","0","2758","<machine-learning><nlp><bert><transformer><doc2vec>","<p>I am trying to understand the difference between Doc2Vec and BERT. I do understand that doc2vec uses a paragraph ID which also serves as a paragraph vector. I am not sure though if that paragraph ID serves in better able to understand the context in that vector?</p>
<p>Moreover, BERT definitely understands the context and attributes different vectors for words such as &quot;Bank&quot;. for instance,</p>
<ol>
<li>I robbed a bank</li>
<li>I was sitting by the bank of a river.</li>
</ol>
<p>BERT would allocate different vectors for the word BANK here. Trying to understand if doc2vec also gets this context since the paragraph id would be different here (for doc2vec). Can anyone please help with this?</p>
","bert"
"{
  ""id"": 106898,
  ""title"": ""How to categorise customer complaint using NLP""
}","How to categorise customer complaint using NLP","2022-01-10 06:24:11","107069","0","776","<machine-learning><classification><nlp><tensorflow><bert>","<p>I have a dataset of community complaints and I would like to build a NLP model on those descriptions and tag a category (can be referred for an inspection or Not ie &quot;Not referred) to each of them. Boolean answer ( Yes or No) would suffice my requirement.</p>
<p>For example: Our customer service department process complaints that are received via phone or email with &quot;referred&quot; or &quot;not referred&quot; status. Right now they are checking descriptions to classify them manually as &quot;referred&quot; or &quot;not referred&quot;. My ultimate goal is to automate the whole and build a Machine Learning Model which gives a binary output &quot;Yes&quot; or No&quot; based on descriptions.  So that, they dont need to check manually and process those complaints. That ML model should categorise the future complaints into two buckets &quot;Referred&quot; &quot;Not Referred&quot; The classification of the issues they have received into buckets will help the department to provide customized solutions to the customers in each group.</p>
<p>Is there a way in NLP to build and train a model to automate this process? I have been reading stuffs about NLP for the past couple of days and it looks like NLP has a lot of good features to get a head start in addressing this issue. Could someone please guide me with the way I should use NLP to address this issue?</p>
<p>Based on recent research and recommendation, i read few article on this task. Below screenshot from one of them;
<a href=""https://i.sstatic.net/HZQzG.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/HZQzG.png"" alt=""BERT"" /></a></p>
<p>In that <code>print</code> section he has passed one index row to get sentiment output. Can I get similar output for multiple rows if i dont initialise <code>iloc[0]</code>?</p>
<p>That is what I am after. We receive a bunch of messages daily from the community assisting Line and want to classify them into two buckets – of interest, not of interest.</p>
","bert"
"{
  ""id"": 106664,
  ""title"": ""Comparing the cosine similarities of the same word representations, from two separate models (vector spaces)""
}","Comparing the cosine similarities of the same word representations, from two separate models (vector spaces)","2022-01-03 17:29:13","","2","39","<nlp><word2vec><bert>","<p>I am comparing the cosine similarities of word representations derived from a BERT model and also from a static Word2Vec model.</p>
<p>I understand that the vector spaces of the two models are inherently different due to the dimensionality of BERT (768) and Word2Vec (300). Essentially I am trying to find a way to compare the two cosine similarity measurements between the same words but from two different models.</p>
<p>I also have a set of user-determined similarity scores between the words, e.g., 'vaccinate' - 'inoculate' = 8.99. I was thinking of using this as a scaling factor for the two similarities so each cosine similarity from the vector space would then be scaled by the same amount.</p>
<p>I essentially want to quantitatively compare the cosine similarity scores between two models' representations for the same words. Any help would be appreciated.</p>
","bert"
"{
  ""id"": 106609,
  ""title"": ""One class classifier for fraud call detection ( in Hindi language) using BERT""
}","One class classifier for fraud call detection ( in Hindi language) using BERT","2022-01-01 15:40:23","","0","31","<nlp><data-science-model><bert>","<p>I have created a dataset of text files that are nothing but transcripts of fraud call recordings. I want to implement one class classifier as I have only fraud call audio and transcripts but not a single normal(not fraud) call recording. I want to use BERT's multilanguage model but I don't know intermediate steps or resources for such a project.  Can anyone help me with the approach? thank you.</p>
","bert"
"{
  ""id"": 106463,
  ""title"": ""How to feed a Knowledge Base into Language Models?""
}","How to feed a Knowledge Base into Language Models?","2021-12-27 17:35:51","106506","2","591","<machine-learning><deep-learning><nlp><bert><knowledge-graph>","<p>I’m a CS undergrad trying to make my way into NLP Research. For some time, I have been wanting to incorporate &quot;everyday commonsense reasoning&quot; within the existing state-of-the-art Language Models; i.e. to make their generated output more reasonable and in coherence with our practical world. Although there do exist some commonsense knowledge bases like ConceptNet (2018), ATOMIC (2019), OpenMind CommonSense (MIT), Cyc (1984), etc., they exist in form of knowledge graphs, ontology, and taxonomies.</p>
<p>My question is, how can I go about leveraging the power of these knowledge bases into current transformer language models like BERT and GPT-2? How can we fine-tune these models (or maybe train new ones from scratch) using these knowledge bases, such that they retain their language modeling capabilities but also get enhanced through a new commonsense understanding of our physical world?</p>
<p>If any better possibilities exist other than fine-tuning, I'm open to ideas.</p>
","bert"
"{
  ""id"": 106341,
  ""title"": ""How to improve language model ex: BERT on unseen text in training?""
}","How to improve language model ex: BERT on unseen text in training?","2021-12-22 10:50:35","106342","2","291","<classification><nlp><text-mining><bert><huggingface>","<p>I am using pre-trained language model for binary classification. I fine-tune the model by training on data my downstream task. The results are good almost 98% F-measure.</p>
<p>However, when I remove a specific similar sentence from the training data and add it to my test data, the classifier fails to predict the class of that sentence. For example, the sentiment analysis task</p>
<blockquote>
<p>&quot;I love the movie more specifically the acting was great&quot;</p>
</blockquote>
<p>I removed from training all sentences containing the words <strong>&quot; more specifically&quot;</strong> and surprisingly in the test set they were all misclassified, so the precision decreased by a huge amount.</p>
<p>Any ideas on how can I further fine-tune/improve my model to work better on unseen text in training to avoid the problem I described above? (of course without feeding the model on sentences containing the words <strong>&quot;more specifically&quot;</strong>)</p>
<p>Note: I observed the same performance regardless of the language model in use (BERT, RoBERTa etc).</p>
","bert"
"{
  ""id"": 106214,
  ""title"": ""How to precompute one sequence in a sequence-pair task when using BERT?""
}","How to precompute one sequence in a sequence-pair task when using BERT?","2021-12-17 10:22:30","106217","1","158","<deep-learning><nlp><bert><tokenization>","<p>BERT uses separator tokens ([SEP]) to input two sequences for a sequence-pair task. If I understand the BERT architecture correctly, attention is applied to all inputs thus coupling the two sequences right from the start.</p>
<p>Now, consider a sequence-pair task in which one of the sequences is constant and known from the start. E.g. Answering multiple unknown questions about a known context. To me it seems that there could be a computational advantage if one would precompute (part of) the model with the context only. However, if my assumption is correct that the two sequences are coupled from the start, precomputation is infeasible.</p>
<p>Therefore my question is:
<strong>How to precompute one sequence in a sequence-pair task while still using (pre-trained) BERT?</strong> Can we combine BERT with some other type of architecture to achieve this? And does it even make sense to do it in terms of speed and accuracy?</p>
","bert"
"{
  ""id"": 106189,
  ""title"": ""Twitter Sentiment Analysis: problem in predicting""
}","Twitter Sentiment Analysis: problem in predicting","2021-12-16 14:17:19","","1","55","<bert><sentiment-analysis>","<p>I am going to do Sentiment Analysis over some tweets. The goal is to find out which post is with and which one is against a specific topic(which tweet is saying this product is good and which on is saying that is not good).
I have about 6000 tweets for each Positive, Negative and Neutral. I have tested some models like Naive Bayes, NN, Decision Tree and Random Forest but I saw no good results.
When I refer to confusion matrix, I see that many Positive and Negative are predicted <strong>interchangeably</strong>. Also, when I try to add some layers (for example in NN), it is going to over-fit.
I use these models, but almost all the results are the same:</p>
<pre><code>[(TF-IDF)] + [(Naive Bayes), (Decision Tree), (Random Forest)]

[(BERT), (Distilbert)] + [(Fully connected NN)] 
</code></pre>
","bert"
"{
  ""id"": 104909,
  ""title"": ""Text classification length""
}","Text classification length","2021-12-09 02:29:07","","0","324","<classification><nlp><bert><text-classification><text>","<p>I have a set of text examples I need to learn as class A, and they are of varying lengths, say 10 sentences to 1 sentence long.  I have to parse a document to find those strings of text that match one of the example texts as an example of class A.</p>
<p>What is the right/best/common way to go about handling the length of text variation of the examples vs how much text you use to compare to in a document?  That is, do I just first go through the document 10 sentences at a time (windowing) and running a prediction on each group of 10 (since some training examples were 10 sentences long), and then do it again for 9 sentence long groups, etc., down to 1?</p>
<p>Or is there something already handles arbitrary text length?</p>
","bert"
"{
  ""id"": 104902,
  ""title"": ""Is binary classification the right choice in this case?""
}","Is binary classification the right choice in this case?","2021-12-08 17:12:53","104943","1","180","<classification><nlp><bert><text-classification><binary-classification>","<p>I am somewhat new to text classification and I have some questions if you folks can help:</p>
<p>I have some text I need to be able to classify as belonging to a single class or not (usually 1-10 sentences long each).  For the examples of the class, I have around 500 examples, but the non-class case can really be any text at all of which I have hundreds of documents with tens of thousands of sentences (which are not the class).  What I have to do is be able to classify each of the sentences in each document as belonging to the class or not.  The vast majority won't belong.</p>
<ol>
<li><p>I'm using a BERT based Binary Classifier (simpletransformer) to identify the text similar to (or exactly) the 500 class examples, does this seem reasonable/possible?</p>
</li>
<li><p>How should I deal with the class imbalance of 500 to 10000's?  I tried oversampling the minority class (my target), but it seems to overfit when I do that.</p>
</li>
<li><p>What is the usual way of handling this particular use case?  The 1-class anomaly detection doesn't seem to fit here, from what I can tell.  Is there a similar NLP style training that works for this case? Or something else?</p>
</li>
</ol>
<p>Would it make sense to just do a semantic similarity comparison of some sort?  That is, just take the class examples, and for each sentence in a document, test to see how similar it is to each class example and if the text is &quot;close enough&quot; to any of the class examples, then it's a &quot;hit&quot;?  this would seem slow...  Is there a standard/good library for semantic comparison?</p>
","bert"
"{
  ""id"": 104824,
  ""title"": ""How to add custom embeddings for bert""
}","How to add custom embeddings for bert","2021-12-06 11:44:53","","2","435","<bert>","<p>Pretrained Bert input has token embeddings, segment embeddings, position embeddings. But I would like to add some custom embeddings along with them and feed them to pretrained bert. How can I implement this in pytorch? Is it possible?</p>
","bert"
"{
  ""id"": 104536,
  ""title"": ""BERT vs GPT architectural, conceptual and implemetational differences""
}","BERT vs GPT architectural, conceptual and implemetational differences","2021-11-26 21:22:18","","10","8948","<machine-learning><nlp><bert><transformer><gpt>","<p>In the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT paper</a>, I learnt that BERT is encoder-only model, that is it involves only transformer encoder blocks.</p>
<p>In the <a href=""https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf"" rel=""noreferrer"">GPT paper</a>, I learnt that GPT is decoder-only model, that is it involves only transformer decoder blocks.</p>
<p>I was guessing whats the difference. I know following difference between encoder and decoder blocks: GPT Decoder looks only at previously generated tokens and learns from them and not in right side tokens. BERT Encoder gives attention to tokens on both sides.</p>
<p>But I have following doubts:</p>
<p><strong>Q1.</strong> GPT2,3 focuses on new/one/zero short learning. Cant we build new/one/zero short learning model with encoder-only architecture like BERT?</p>
<p><strong>Q2.</strong> Huggingface <code>Gpt2Model</code> contains <code>forward()</code> method. I guess, feeding single data instance to this method is like doing one shot learning?</p>
<p><strong>Q3.</strong> I have implemented neural network model which utilizes output from <code>BertModel</code> from hugging face. Can I simply swap <code>BertModel</code> class with <code>GPT2Model</code> with some class and will it. The return value of <a href=""https://huggingface.co/transformers/model_doc/gpt2.html#transformers.GPT2Model.forward"" rel=""noreferrer""><code>Gpt2Model.forward</code></a> does indeed contain <code>last_hidden_state</code> similar to <a href=""https://huggingface.co/transformers/model_doc/bert.html#transformers.BertModel.forward"" rel=""noreferrer""><code>BertModel.forward</code></a>. So, I guess swapping out <code>BertModel</code> with <code>Gpt2Model</code> will indeed work, right?</p>
<p><strong>Q4.</strong> Apart from being decoder-only and encoder-only, auto-regressive vs non-auto-regressive and whether or not accepting tokens generated so far as input, what high level architectural / conceptual differences GPT and BERT have?</p>
","bert"
"{
  ""id"": 104493,
  ""title"": ""How to learn common sense constants? Look in body for detail""
}","How to learn common sense constants? Look in body for detail","2021-11-25 18:26:41","","2","31","<nlp><machine-learning-model><bert>","<p>If I wanted to learn constants for example week -&gt; 7 days, chicken -&gt; 2 legs, day -&gt; 24, 1km -&gt; 1000 meters hours, and so on, would it be possible to extract this information from a BERT model trained on the right dataset like Wikipedia words? If not, what model would I have to use?</p>
","bert"
"{
  ""id"": 104143,
  ""title"": ""Optimal batch size and number of epoch for BERT""
}","Optimal batch size and number of epoch for BERT","2021-11-14 21:24:58","104218","1","11569","<keras><tensorflow><bert><hyperparameter-tuning><hyperparameter>","<p>I use this tutorial <a href=""https://www.tensorflow.org/text/tutorials/classify_text_with_bert"" rel=""nofollow noreferrer"">https://www.tensorflow.org/text/tutorials/classify_text_with_bert</a> and get different accuracy depend on epoch numbers and batch sizes. What's optimal parameters?</p>
","bert"
"{
  ""id"": 103936,
  ""title"": ""How to Fine Tune a BERT model for sentiment analysis to get the best f1 score""
}","How to Fine Tune a BERT model for sentiment analysis to get the best f1 score","2021-11-08 12:42:25","","1","374","<loss-function><bert><transformer><sentiment-analysis><huggingface>","<p>I am building a multi-class sentiment analysis BERT model that's optimized to give the best f1 score.</p>
<p>More specifically, I train each epoch by optimizing binary cross entropy per class, taking the mean, and then run back propagation to optimize the parameters. At the end of each epoch, I then compute the soft-f1 score on the validation set, and then save that model only if the score has improved. The predicted class is chosen by taking an argmax, so there is no threshold.</p>
<p>There are a few observations here, some of which are troubling:</p>
<ol>
<li><p>If I simply use log-loss as the criteria for choosing the best model, then the model will typically only train for 3-4 epochs before the losses start to degrade. This is the standard approach, and gives descent f1 scores across the training/validation and test sets.</p>
</li>
<li><p>If I instead use the soft-f1 criteria as specified above, the model will typically train for <em>15</em> epochs (where I use a stopping condition of 5 epochs if there is no improvement). The f1 scores are significantly better on the training set, and marginally better on the validation and test sets. I'm guessing that the improvement on the training set is simply due to overfitting, as the model has run substantially longer. But then there are also improvements in the validation and test sets too. The log-loss for this second model is (naturally) worse than model #1.</p>
</li>
</ol>
<p>My question, which is the best model? Is it correct to use the f1 score to choose the best model while training, or am I just overfitting without realizing? Thanks!</p>
","bert"
"{
  ""id"": 103904,
  ""title"": ""Why should I understand AI architectures?""
}","Why should I understand AI architectures?","2021-11-07 13:20:46","","10","2703","<machine-learning><deep-learning><cnn><machine-learning-model><bert>","<p>Why should I understand what is happening deep down in some AI architecture?</p>
<p>For example LSTM-BERT- Partial Conv... Architectures like this. Why should I understand what is going on while I can find any model on the Internet or any implementations on the Internet?</p>
","bert"
"{
  ""id"": 103895,
  ""title"": ""Ways to cluster word senses with word embeddings""
}","Ways to cluster word senses with word embeddings","2021-11-07 07:01:45","","2","184","<nlp><word-embeddings><word2vec><bert>","<p>I'm trying to <em><strong>semantically cluster polysemous words</strong></em> or word with different meanings in a corpus for my class study and I want to do it by <em>word embeddings</em> but I have no Idea how to reach to the clustered target that I want. (a similar target that I'm looking for is posted below as an image)</p>
<p><strong>What I have</strong>: <em>a corpus</em></p>
<p><strong>What I want</strong>: <em>clustering of K frequent words with other most related semantically similar words in a corpus</em> considering that these words have multiple senses.</p>
<blockquote>
<p>For example: suppose word <em>cell</em> is repeated 5000 times in a corpus, here are some sentences:  &quot;There are many organelles in a biological cell&quot; , &quot;He went to prison cell&quot; and<br />
&quot;we are running out of cell phones&quot;, in every sentence we are receiving <em>contextually a different meaning from cell</em>, respectively, <em>blood cell</em>, <em>prison</em> and <em>mobile/phone</em>.</p>
</blockquote>
<p>So I want to cluster each word [for example <em>cell</em> in here] with their <em>semantically similar words</em>. (<em>sometime similar words are synonyms</em>)</p>
<p><strong>What I've done</strong>:</p>
<ul>
<li>preprocessing corpus for finding K frequent words.</li>
<li>As each meaning of word is contextualized to the correspondent sentence, I thought we could compare BERT vectors of those sentences with other sentences, but the problem is Bert compare vector to vector and different senses are dependent on their sentence but I don't know how I should correctly locate semantically similar words in sentences that are compared to the first sentence.</li>
</ul>
<p>I searched for related papers, there was a <em>WordNet</em> that seems to be similar but it is not constructed with word embedding methods.</p>
<blockquote>
<p>Although there are word embeddings like <code>GloVe</code>, <code>FastText</code>, <code>Word2Vec</code> that can bring us similar words but the context would be ignored and I didn't find anywhere that they can work <em>semantically</em>!</p>
</blockquote>
<p>And at last there was an <code>ELMo</code>, ELMo word representations take the entire input sentence into equation for calculating the word embeddings. Hence, the term <em>&quot;cell&quot;</em> would have different ELMo vectors under different contexts but it is still categorizing the words by their context and not putting Words that are semantically similar in a category, this way I have no Idea how to cluster different meanings by their semantically similar words.</p>
<p>Plus I've checked the papers and <strong>WSD</strong> is not the thing I want, maybe <code>Word Sense Induction</code> clustering` seems to be more accurate but still not exact.</p>
<p>Here is a photo of WordNet with word <strong>Search</strong>, kindda similar to the thing I want. (you can see each cluster of word <em>search</em> is connected semantically to their similar group)</p>
<p><a href=""https://i.sstatic.net/IRsry.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/IRsry.jpg"" alt=""WordNet"" /></a></p>
<hr />
<p>I'm asking here to get more ideas or maybe my intuitions are all wrong.</p>
<p>Thanks for your time.</p>
<p>Any information would be helpful and appreciated.</p>
","bert"
"{
  ""id"": 103791,
  ""title"": ""Error when trying to predict BERT model and obtaining classification report""
}","Error when trying to predict BERT model and obtaining classification report","2021-11-04 01:58:57","","2","227","<python><multiclass-classification><multilabel-classification><bert><text-classification>","<p>I am following this <a href=""https://towardsdatascience.com/multi-label-multi-class-text-classification-with-bert-transformer-and-keras-c6355eccb63a"" rel=""nofollow noreferrer"">tutorial</a> about multi-label, multi-class classification using BERT. I am trying to get the predicted classification for <code>y_pred</code>, but when running it I got an error.</p>
<pre><code># Ready test data
test_y_kriteria = to_categorical(data_test['Kriteria'])
test_y_sentiment = to_categorical(data_test['Sentiment'])

test_x = tokenizer(
    text=data_test['Review Text'].to_list(),
    add_special_tokens=True,
    max_length=max_length,
    truncation=True,
    padding=True, 
    return_tensors='tf',
    return_token_type_ids = False,
    return_attention_mask = False,
    verbose = True)
# Run evaluation
model_eval = model.evaluate(
    x={'input_ids': test_x['input_ids']},
    y={'kriteria': test_y_kriteria, 'sentiment': test_y_sentiment}

#Predict
pred = model.predict(test_x)
</code></pre>
<p>The error is <code>ValueError: Unsupported value type BatchEncoding returned by IteratorSpec._serialize</code>. I tried changing <code>test_x</code> into a list</p>
<pre><code># Ready test data
test_y_kriteria = to_categorical(data_test['Kriteria'])
test_y_sentiment = to_categorical(data_test['Sentiment'])


tokenized_test_x = [
    tokenizer.tokenize(
    text=txt,
    add_special_tokens=True,
    max_length=max_length,
    truncation=True,
    padding=True, 
    return_tensors='tf',
    return_token_type_ids = False,
    return_attention_mask = False,
    verbose = True)
    for txt in data_test['Review Text'].to_list()]

# Run evaluation
model_eval = model.evaluate(
    x={'input_ids': test_x['input_ids']},
    y={'kriteria': test_y_kriteria, 'sentiment': test_y_sentiment}
)

#Predict
pred = model.predict(tokenized_test_x)
</code></pre>
<p>and I got this error instead <code>ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).</code>. I tried to look for solution for this, and most solutions suggested to use <code>asarray()</code>, so I ran <code>arr_token = np.asarray(tokenized_test_x, dtype=object)</code> and tried to predict using <code>arr_token</code> but it still returned the same error. How do I solve this?</p>
","bert"
"{
  ""id"": 103628,
  ""title"": ""How do I work with noisy real world text data for text classification?""
}","How do I work with noisy real world text data for text classification?","2021-10-30 04:18:11","","0","32","<machine-learning><nlp><bert><text>","<p>I have a topic classification model built upon Bert, when I deploy my model people input strings of a random nature like :</p>
<p>&quot;aaaaaa&quot; &quot;aaa bbb&quot; &quot;ab ab ab&quot;</p>
<p>and so on.</p>
<p>My model predicts a false positive for these types of inputs 10% of the time. How do I make my machine learning model robust to these inputs?</p>
","bert"
"{
  ""id"": 103583,
  ""title"": ""how to improve my imbalanced data NLP model?""
}","how to improve my imbalanced data NLP model?","2021-10-28 14:23:18","","0","329","<nlp><bert><language-model><allennlp>","<p>I want to classify a patient's health as a prediction probability and get the top 10 most ill patients in a hospital. I have patient's condition notes, medical notes, diagnoses notes, and lab notes for each day.</p>
<p>Current approach -</p>
<ol>
<li>vectorize all the notes using spacy's scispacy model and sum all the vectors grouped by patient id and day. (200 columns)</li>
<li>find the unit vectors of the above vectors. (200 columns)</li>
<li>use a moving average function on the vectors grouped by patient id and day.(200 columns)</li>
<li>find the unit vectors of the above moving average vectors (200 columns)</li>
<li>combine all the above columns and use them as independent features.</li>
<li>use a lgbm classifier.</li>
</ol>
<p>The data is imbalanced and the current AUC-ROC is around .78.</p>
<p>What else can I do to improve my AUC-ROC?
Can I use bert for this problem? how should I use it?</p>
<p>I'm currently using a moving average as a patient's health deteriorates over time.</p>
<p>Any suggestion/answer/feedback?</p>
","bert"
"{
  ""id"": 103279,
  ""title"": ""Combining models trained on a multilingual multi-source corpus""
}","Combining models trained on a multilingual multi-source corpus","2021-10-19 09:47:40","","1","39","<bert><transformer>","<p>Consider the following training corpora:</p>
<ul>
<li><p><em>dataset1</em>: composed of French instances</p>
</li>
<li><p><em>dataset2</em>: <em>dataset1</em> + Arabic instances</p>
</li>
<li><p><em>test_dataset</em> (for both scenarios): composed of French instances</p>
</li>
</ul>
<p>(the same annotation guidelines were used for both languages).</p>
<p>After analyzing the results of our preliminary experimental setup, we chose BERT as our baseline system.</p>
<p>Considering the different languages involved, we experimented with different models capable of handling them: FlauBERT and CamemBERT (for French), AraBERT (for Arabic) as well as BERT multilingual. Generally, for both languages, the results obtained by BERT multilingual are lower than those obtained by the language specific models.</p>
<p>Is it theoretically possible to <em>merge</em> multiple models together into one model, effectively combining all the data learnt so far? For example, combining CamemBERT trained only on the French part of <em>dataset2</em> and AraBERT trained only on the Arabic part?</p>
","bert"
"{
  ""id"": 103208,
  ""title"": ""why is the BERT NSP task useful for sentence classification tasks?""
}","why is the BERT NSP task useful for sentence classification tasks?","2021-10-17 05:19:12","103277","1","591","<bert>","<p>BERT pre-trains the special <code>[CLS]</code> token on the NSP task - for every pair <code>A-B</code> predicting whether sentence B follows sentence A in the corpus or not.
When fine-tuning BERT for sentence classification (e.g. spam or not), it is recommended to use a degenerate pair <code>A-null</code> and use the <code>[CLS]</code> token output for our task.</p>
<p>How is that making sense? in the pre-training stage, BERT never saw such pairs, how come it will eat them just fine and &quot;know&quot; that instead of extracting the relation between A and B it is to extract the meaning of sentence A as there is no sentence B?</p>
<p>Is there another practice of fine-tuning the model with <code>A-spam</code> and <code>A-notspam</code> for every sentence A, and seeing which pair gets the better NSP score? or is that totally equivalent to fine tuning with <code>A-null</code>?</p>
<p>related to <a href=""https://datascience.stackexchange.com/questions/77044/bert-transformer-why-bert-transformer-uses-cls-token-for-classification-inst"">Bert-Transformer : Why Bert transformer uses [CLS] token for classification instead of average over all tokens?</a></p>
","bert"
"{
  ""id"": 103157,
  ""title"": ""Using Sentence-Bert with other features in scikit-learn""
}","Using Sentence-Bert with other features in scikit-learn","2021-10-15 01:44:44","","1","225","<machine-learning><nlp><tensorflow><bert>","<p>I have a dataset where one feature is text and 4 more features. Sentence-Bert vectorizer transforms text data into tensors. I can use these sparse matrices directly with a machine learning classifier. Can I replace the text column with tensors? Also how can I train the model. Below is the code I used to transform the text into vectors.</p>
<pre><code>model = SentenceTransformer('sentence-transformers/LaBSE')
sentence_embeddings = model.encode(X_train['tweet'], convert_to_tensor=True, show_progress_bar=True)
sentence_embeddings1 = model.encode(X_test['tweet'], convert_to_tensor=True, show_progress_bar=True)
</code></pre>
","bert"
"{
  ""id"": 103016,
  ""title"": ""NER prections with distilbert transformer model""
}","NER prections with distilbert transformer model","2021-10-11 02:45:37","","1","339","<bert><transformer><text-classification><named-entity-recognition>","<p>I am trying to extract 'agreement date' label from a corpus of legal contracts. In the train dataset, I used pytorch-transformer model to train.</p>
<pre><code>model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))
</code></pre>
<p>Here label_list is the IOB format which gives ['B-Date', 'I-Date', 'O'] and model_checkpoint is &quot;distilbert-base-uncased&quot;
I train the dataset after defining TrainingArguements, datacollator and matrics computaitons from predictions</p>
<pre><code>model_output_dir = 'C:/Python/model_output_dir'

args = TrainingArguments(
    output_dir = model_output_dir,
    evaluation_strategy = &quot;epoch&quot;,
    logging_strategy=&quot;epoch&quot;,
    save_strategy=&quot;epoch&quot;,
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,    
    run_name = model_checkpoint,
    metric_for_best_model=&quot;f1&quot;,
    load_best_model_at_end = True,
    weight_decay=0.01,
    )

from transformers import DataCollatorForTokenClassification
data_collator = DataCollatorForTokenClassification(tokenizer)
from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)]

    # Define the metric parameters
    overall_precision = precision_score(true_labels, true_predictions, zero_division=1)
    overall_recall = recall_score(true_labels, true_predictions, zero_division=1)
    overall_f1 = f1_score(true_labels, true_predictions, zero_division=1)
    overall_accuracy = accuracy_score(true_labels, true_predictions)
    
    # Return a dictionary with the calculated metrics
    return {
        &quot;precision&quot;: overall_precision,
        &quot;recall&quot;: overall_recall,
        &quot;f1&quot;: overall_f1,
        &quot;accuracy&quot;: overall_accuracy,}

trainer = Trainer(
                model= model,
                args = args,
                train_dataset=tokenized_datasets[&quot;train&quot;],
                eval_dataset=tokenized_datasets[&quot;test&quot;],
                data_collator=data_collator,
                tokenizer=tokenizer,
                compute_metrics=compute_metrics,
                )
</code></pre>
<p>My input training data after preprocessing look like this
<a href=""https://i.sstatic.net/tPxRT.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tPxRT.jpg"" alt=""enter image description here"" /></a></p>
<p>My test data has agreement text only in tokenized form. When I predict I get the big tensor like below
<a href=""https://i.sstatic.net/qBySx.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/qBySx.jpg"" alt=""enter image description here"" /></a></p>
<p>How I get the required date label from this output ?</p>
","bert"
"{
  ""id"": 102728,
  ""title"": ""BERT Text Classification Model gives error""
}","BERT Text Classification Model gives error","2021-10-02 21:43:06","","0","481","<tensorflow><bert><text-classification>","<p>I am fine-tuning a BERT Model for text classification with Tensorflow. Here is my code for building the model:</p>
<pre><code># Building the model
def create_model():
  input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                      name=&quot;input_word_ids&quot;),
  input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                  name=&quot;input_mask&quot;),
  input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,
                                  name=&quot;input_type_ids&quot;)
 
  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])

  drop = tf.keras.layers.Dropout(0.4)(pooled_output)
  output = tf.keras.layers.Dense(1, activation=&quot;sigmoid&quot;, name=&quot;output&quot;)(drop)
  
  model = tf.keras.Model(
      inputs={
          'input_word_ids': input_word_ids,
          'input_mask': input_mask,
          'input_type_ids': input_type_ids
          },
          outputs=output)
  return model
</code></pre>
<p>I used this BERT Layer:</p>
<pre><code>bert_layer = hub.KerasLayer(&quot;https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4&quot;, trainable=True)
</code></pre>
<p>When creating the model: <code>model = create_model()</code>, it gives the following error:</p>
<pre><code>ValueError: Could not find matching function to call loaded from the SavedModel. Got:
  Positional arguments (3 total):
     [(&lt;tf.Tensor 'inputs:0' shape=(None, 512) dtype=int32&gt;,), (&lt;tf.Tensor 'inputs_1:0' shape= 
(None, 512) dtype=int32&gt;,), &lt;tf.Tensor 'inputs_2:0' shape=(None, 512) dtype=int32&gt;]
     False
     None
  Keyword arguments: {}

Expected these arguments to match one of the following 4 option(s):

Option 1:
  Positional arguments (3 total):
     {'input_word_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_word_ids'), 
'input_type_ids': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_type_ids'), 
'input_mask': TensorSpec(shape=(None, None), dtype=tf.int32, name='input_mask')}
     False
     None
  Keyword arguments: {}
</code></pre>
<p>I would appreciate it if you have any solution in mind.</p>
","bert"
"{
  ""id"": 102607,
  ""title"": ""Is it possible to fine-tuning BERT by training it on multiple datasets? (Each dataset having it's own purpose)""
}","Is it possible to fine-tuning BERT by training it on multiple datasets? (Each dataset having it's own purpose)","2021-09-29 11:54:47","","1","816","<nlp><bert><transformer><transfer-learning><finetuning>","<p>BERT can be fine-tuned on a dataset for a specific task. Is it possible to fine-tune it on all these datasets for different tasks and then be utilized for these tasks instead of fine-tuning a BERT model specific to each task?</p>
","bert"
"{
  ""id"": 102531,
  ""title"": ""How long does it take to fine-tune XLNet?""
}","How long does it take to fine-tune XLNet?","2021-09-27 13:53:30","","0","188","<nlp><bert><finetuning><pretraining>","<p>XLNet takes a lot more time than BERT during <strong>pre-training</strong>. This results in XLNet performing better than BERT in over 20 NLP tasks. How long does XLNet take for <strong>fine-tuning</strong> (let's assume this is running on Google Colab)?</p>
<p>(Let's assume a text summarization task with around 4000 examples)</p>
","bert"
"{
  ""id"": 102514,
  ""title"": ""Is it possible to fine-tune a (Spanish RoBERTa) model for a different task?""
}","Is it possible to fine-tune a (Spanish RoBERTa) model for a different task?","2021-09-26 21:02:21","","1","370","<bert><sentiment-analysis><text-classification><huggingface>","<p>I'm doing sentiment analysis of Spanish tweets.</p>
<p>After reviewing some of the recent literature, I've seen that there's been a most recent effort to train a <a href=""https://huggingface.co/BSC-TeMU/roberta-large-bne"" rel=""nofollow noreferrer"">RoBERTa model</a> exclusively on Spanish text. It seems to perform better than the current state-of-the-art model for Spanish language modeling so far, <a href=""https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased"" rel=""nofollow noreferrer"">BETO</a>.</p>
<p>The initial BETO model has been trained with the Whole Word Masking technique for a variety of tasks, but <strong>does not include text classification</strong>.</p>
<p>However, I've come across a <a href=""https://huggingface.co/finiteautomata/beto-sentiment-analysis#beto-sentiment-analysis"" rel=""nofollow noreferrer"">model</a> that used the BETO model to create a model that is able to do text classification - more precisely, sentiment analysis.</p>
<p>It seems therefore possible to have a baseline model that cannot perform a certain task and fine-tune (or is it train?) for a different task.</p>
<p><strong>QUESTION:</strong><br>
Would it, therefore, be possible to take the afore-mentioned RoBERTa model - which in its initial form cannot perform text classification - and fine-tune it to a different task (sentiment analysis in tweets)? <br>
If so, how would I go about this? Can someone name any helpful resources?</p>
<p>Note: I have a labelled dataset with Spanish Tweets that I could use for fine-tuning.</p>
","bert"
"{
  ""id"": 102398,
  ""title"": ""Does BERT need supervised data only when fine-tuning?""
}","Does BERT need supervised data only when fine-tuning?","2021-09-23 14:40:49","102426","3","2313","<machine-learning><nlp><unsupervised-learning><supervised-learning><bert>","<p>I've read many articles and papers mentioning how unsupervised training is conducted while <strong>pre-training</strong> a BERT model. I would like to know if it is possible to fine-tune a BERT model in <strong>an unsupervised manner</strong> or does it <strong>always have to be supervised</strong>?</p>
","bert"
"{
  ""id"": 102214,
  ""title"": ""How to get sentiment score for a word in a given dataset""
}","How to get sentiment score for a word in a given dataset","2021-09-18 15:19:36","","2","732","<nlp><dataset><bert><sentiment-analysis>","<p>I have a sentiment analysis dataset that is labeled in three categories: positive, negative, and neutral. I also have a list of words (mostly nouns), for which I want to calculate the sentiment value, to understand &quot;how&quot; (positively or negatively) these entities were talked about in the dataset. I have read some online resources like blogs and thought about a couple of approaches for calculating the sentiment score for a particular word X.</p>
<ol>
<li><p>Calculate how many data instances (sentences) which have the word X in those, have &quot;positive&quot; labels, have &quot;negative&quot; labels, and &quot;neutral&quot; labels. Then, calculate the weighted average sentiment for that word.</p>
</li>
<li><p>Take a generic untrained BERT architecture, and then train it using the dataset. Then, pass each word from the list to that trained model to get the sentiment scores for the word.</p>
</li>
</ol>
<p>Does any of these approaches make sense? If so, can you suggest some related works that I can look at?
If these approaches don't make sense, could you please advise how I can calculate the sentiment score for a word, in a given dataset?</p>
","bert"
"{
  ""id"": 102084,
  ""title"": ""For text classification, would a BoW or Word Embeddings based model ever be better than a Language Model?""
}","For text classification, would a BoW or Word Embeddings based model ever be better than a Language Model?","2021-09-14 18:47:58","","1","215","<nlp><word-embeddings><bert><text-classification><language-model>","<p>I've done a bit of research, with <a href=""https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"" rel=""nofollow noreferrer"">this</a> being the best as far as objectively measuring quality, but wanted to ask from a theoretical perspective if BoW-based models (e.g. using TF-IDF) or word embeddings-based models (e.g. Word2Vec) would ever be a better choice than a language model (e.g. BERT) for a text classification problem?</p>
<p>The specific problem I'm working on is binary classification of short 2-8 word snippets such as &quot;Air bubble in ampoule&quot; into categories &quot;requires response&quot; or &quot;does not require response&quot;, but I'm more interested in the general question above.</p>
","bert"
"{
  ""id"": 101880,
  ""title"": ""How BERT model differentiate words with different meanings?""
}","How BERT model differentiate words with different meanings?","2021-09-07 19:35:59","","1","43","<nlp><bert>","<p>How BERT model differentiate between words with different meanings e.g. #Transformers like name of the movie or name of a library by @huggingface?</p>
","bert"
"{
  ""id"": 101713,
  ""title"": ""Annotating NER dataset""
}","Annotating NER dataset","2021-09-02 09:21:20","101766","1","431","<bert><named-entity-recognition><annotation>","<p>I am working on annotating a dataset for the purpose of named entity recognition.</p>
<p>In principle, I have seen that for multi-phrase (not single word) elements, annotations work like this (see this example below):</p>
<ol>
<li>Romania (<code>B-CNT</code>)</li>
<li>United States of America (<code>B-CNT C-CNT C-CNT C-CNT</code>)</li>
</ol>
<p>where <code>B-CNT</code> stands for &quot;beginning-country&quot; and <code>C-CNT</code> represents &quot;continuing-country&quot;.</p>
<p>The problem that I face is that I have a case in which (not related to countries) where I need to annotate like <code>B-W GAP_WORD C-W C-W</code>.</p>
<p>How should I proceed with the annotation in this case?</p>
<p>If I do annotate like in the schema above, should I expect a <code>BERT</code>-alike entity recognition system to learn and detect that a phrase can be like <code>B-W GAP_WORD C-W C-W</code>, or do I need that &quot;C-W&quot; (continuation word) to be exactly after the B-W (beginning word)?</p>
<p>Which solution is correct of the following 2:</p>
<ol>
<li><code>B-W GAP_WORD C-W C-W</code></li>
<li><code>B-W GAP_WORD B-W C-W</code></li>
</ol>
<p>And then, in case 2, find a way to make the connection between the B-Ws (actually corresponding to the same entity)?</p>
","bert"
"{
  ""id"": 100598,
  ""title"": ""Is it possible to add new vocabulary to BERT's tokenizer when fine-tuning?""
}","Is it possible to add new vocabulary to BERT's tokenizer when fine-tuning?","2021-08-29 07:33:44","","2","3697","<nlp><word-embeddings><bert><finetuning>","<p>I want to fine-tune BERT by training it on a domain dataset of my own. The domain is specific and includes many terms that probably weren't included in the original dataset BERT was trained on. I know I have to use BERT's tokenizer as the model was originally trained on its embeddings.
To my understanding words unknown to the tokenizer will be masked with [UNKNOWN]. What if some of these words are common in my dataset? Does it make sense to add new IDs for them? is it possible without interferring with the network's parameters and the existing embeddings? If so, how is it done?</p>
","bert"
"{
  ""id"": 100322,
  ""title"": ""How to improve the evaluation score for highly imbalanced dataset?""
}","How to improve the evaluation score for highly imbalanced dataset?","2021-08-20 13:07:46","","0","223","<machine-learning><deep-learning><nlp><bert>","<p>I have trained my BERT model(bert-base-cased) to detect toxic comments. I used the Toxic Comment Classification Challenge dataset from the Kaggle. My accuracy is 98% and the AUROC for various sub-classes is above 90%. However, my Precision, Recall, and F1 score is less. The scores are shown in the image <a href=""https://i.sstatic.net/YpNAP.png"" rel=""nofollow noreferrer"" title=""Scores"">Evaluation Scores</a>.
The dataset is highly imbalanced. The ratio of clean comments is way higher than the toxic comments. Any suggestions to improve the evaluation scores?</p>
<p>Here's the final score</p>
<pre><code>           precision  recall  f1-score   support
</code></pre>
<p>micro avg       0.61      0.85      0.71      1743  <br/>
macro avg       0.56      0.69      0.61      1743<br />
weighted avg    0.64      0.85      0.72      1743  <br/>
samples avg     0.08      0.09      0.08      1743</p>
","bert"
"{
  ""id"": 100317,
  ""title"": ""Can I average the BERT embeddings of multiple instances of the same word to get one vector representation of the word?""
}","Can I average the BERT embeddings of multiple instances of the same word to get one vector representation of the word?","2021-08-20 11:45:07","","0","571","<word-embeddings><bert><corpus>","<p>In the project I'm working on right now I would like to get one embedding for every unique lemma in a corpus. Could I get this by averaging the embeddings of every instance of a lemma?</p>
<p>For example, say that there were 500 tokens of the lemma &quot;walk&quot; - regardless of conjugation - could I then add/average/concatenate these 500 embeddings together to get one embedding accurately representing all of them?</p>
<p>If this would work, which operation should I use on the embeddings to get the best result?</p>
","bert"
"{
  ""id"": 100179,
  ""title"": ""Remedy for small batch size?""
}","Remedy for small batch size?","2021-08-17 07:56:46","100183","0","92","<machine-learning><deep-learning><bert>","<p>I am trying to reproduce results of other people's research, but we cannot afford to do it with the same batch size as theirs, due to limited computing resources.</p>
<p>The method they use is a simple sequence classification using BERT.
They do it with batch size 48, learning rate 4e-5, optimization Adam, and epoch 1.</p>
<p>Their result is f2 score of 0.65 and we are nowhere near there despite doing everything the same except the batch size (we do it with batch size 16).</p>
<p>What are some ways to compensate for small batch size? I would greatly appreciate any suggestions.</p>
","bert"
"{
  ""id"": 99933,
  ""title"": ""How to do Bert Finetuning of failure cases?""
}","How to do Bert Finetuning of failure cases?","2021-08-11 07:54:06","","1","44","<nlp><bert>","<p>I have a large dataset(public available) of text that is labelled. However the test distribution (actual production setting of company) while similar is not from the same source and thus tends to fail on some cases. It is not possible to label our test distribution due to manpower issues. How do I incrementally train my Bert Model to better handle failure cases  without overfitting? (i.e. we only label failure cases due to complains from clients which are not many.)</p>
","bert"
"{
  ""id"": 99796,
  ""title"": ""HuggingFace Transformers is giving loss: nan - accuracy: 0.0000e+00""
}","HuggingFace Transformers is giving loss: nan - accuracy: 0.0000e+00","2021-08-06 19:23:11","","2","2756","<nlp><bert><huggingface><loss>","<p>I am a HuggingFace Newbie and I am fine-tuning a BERT model (<code>distilbert-base-cased</code>) using the Transformers library but the training loss is not going down, instead I am getting <code>loss: nan - accuracy: 0.0000e+00</code>.</p>
<p>My code is largely per the boiler plate on the [HuggingFace course][1]:-</p>
<pre><code>model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3)

opt = Adam(learning_rate=lr_scheduler)

model.compile(optimizer=opt, loss=loss, metrics=['accuracy'])

model.fit(
    encoded_train.data,
    np.array(y_train),
    validation_data=(encoded_val.data, np.array(y_val)),
    batch_size=8,
    epochs=3
)
</code></pre>
<p>Where my loss function is:-</p>
<p><code>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</code></p>
<p>The learning rate is calculated like so:-</p>
<pre><code>lr_scheduler = PolynomialDecay(
    initial_learning_rate=5e-5,
    end_learning_rate=0.,
    decay_steps=num_train_steps
    )
</code></pre>
<p>The number of training steps is computed thus:-</p>
<pre><code>batch_size = 8
num_epochs = 3

num_train_steps = (len(encoded_train['input_ids']) // batch_size) * num_epochs
</code></pre>
<p>So far then all very much like the boiler plate code.</p>
<p>My data looks like this:-</p>
<pre><code>{'input_ids': &lt;tf.Tensor: shape=(1040, 512), dtype=int32, numpy=
array([[  101,   155,  1942, ...,     0,     0,     0],
       [  101, 27900,  7641, ...,     0,     0,     0],
       [  101,   155,  1942, ...,     0,     0,     0],
       ...,
       [  101,   109,  7414, ...,     0,     0,     0],
       [  101,  2809,  1141, ...,     0,     0,     0],
       [  101,  1448,  1111, ...,     0,     0,     0]], dtype=int32)&gt;, 'attention_mask': &lt;tf.Tensor: shape=(1040, 512), dtype=int32, numpy=
array([[1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       ...,
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0],
       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)&gt;}
</code></pre>
<p>And like this:-</p>
<pre><code>10     2
147    1
342    1
999    3
811    3
Name: sentiment, dtype: int64
</code></pre>
<p>I have studied the forums and made the most obvious checks:-</p>
<p>Here I check if there are any NaN in the data:-</p>
<pre><code>print(&quot;Any NaN in y_train? &quot;,np.isnan(np.array(y_train)).any())

print(&quot;Any NaN in y_val? &quot;,np.isnan(np.array(y_val)).any())
</code></pre>
<p>Which gives:-</p>
<pre><code>Any NaN in y_train?  False
Any NaN in y_val?  False
</code></pre>
<p>I also tried:-</p>
<pre><code>print(&quot;Any NaN in encoded_train['input_ids']? &quot;,np.isnan(np.array(encoded_train['input_ids'])).any())
print(&quot;Any NaN 'encoded_train['attention_mask']'? &quot;,np.isnan(np.array(encoded_train['attention_mask'])).any())
</code></pre>
<p>but only got:-</p>
<pre><code>Any NaN in encoded_train['input_ids']?  False
Any NaN 'encoded_train['attention_mask']'?  False
</code></pre>
<p>I am struggling to know where to go next with this code.</p>
<p>The full error trace looks like this, you can see the accuracy and loss on each epoch and this model is obviously not training at all:-</p>
<pre><code>Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']
- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_59']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Epoch 1/3
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
130/130 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0019WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).
WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.
130/130 [==============================] - 63s 452ms/step - loss: nan - accuracy: 0.0019 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 2/3
130/130 [==============================] - 57s 438ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
Epoch 3/3
130/130 [==============================] - 57s 441ms/step - loss: nan - accuracy: 0.0000e+00 - val_loss: nan - val_accuracy: 0.0000e+00
&lt;tensorflow.python.keras.callbacks.History at 0x7f304f714fd0&gt;
</code></pre>
<p>I would be happy to post more details if anyone is able to tell me what it would be useful to see.</p>
","bert"
"{
  ""id"": 99755,
  ""title"": ""An issue for sub-word tokenization preprocessing transformer""
}","An issue for sub-word tokenization preprocessing transformer","2021-08-05 22:47:38","99765","0","121","<python><nlp><bert><transformer><python-3.x>","<p>I'm stacked with executing <strong>the sub-word tokenization preprocessing</strong> to use transformer.</p>
<p>According to the tutorial on <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">the article</a>, I have executed the sample code.</p>
<p>However, one function was not defined properly and no hint to fix it on the article.</p>
<p>If you have any ideas to fix the code, could you help me?</p>
<h2>Error</h2>
<pre><code>     22  return X, y
     23 
---&gt; 24 X_train, y_train = preprocess(train_samples)
     25 X_val, y_val = preprocess(val_samples)

     12 def preprocess(samples):
     13  tag_index = {tag: i for i, tag in enumerate(schema)}
---&gt; 14  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
     15  max_len = max(map(len, tokenized_samples))
     16  X = np.zeros((len(samples), max_len), dtype=np.int32)

TypeError: 'module' object is not callable
</code></pre>
<h2>Code</h2>
<p>This code is from <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">the article</a> to build a model for named entity recognition using transformer.</p>
<pre class=""lang-py prettyprint-override""><code>import numpy as np
import tqdm
 
def tokenize_sample(sample):
  seq = [
         (subtoken, tag)
         for token, tag in sample
         for subtoken in tokenizer(token)['input_ids'][1:-1]
         ]
  return [(3, 'O')] + seq + [(4, 'O')]

def preprocess(samples):
  tag_index = {tag: i for i, tag in enumerate(schema)}
  tokenized_samples = list(tqdm(map(tokenize_sample, samples)))
  max_len = max(map(len, tokenized_samples))
  X = np.zeros((len(samples), max_len), dtype=np.int32)
  y = np.zeros((len(samples), max_len), dtype=np.int32)
  for i, sentence in enumerate(tokenized_samples):
    for j, (subtoken_id, tag) in enumerate(sentence):
      X[i, j] = subtoken_id
      y[i,j] = tag_index[tag]
  return X, y

X_train, y_train = preprocess(train_samples)
X_val, y_val = preprocess(val_samples)
</code></pre>
<h2>What I tried</h2>
<p>I checked that the function, tokenize_sample is executable with the below code.</p>
<p>However, I'm not sure how to insert it to the original code.</p>
<pre><code>for sample in samples:
  print(tokenize_sample(sample))
</code></pre>
","bert"
"{
  ""id"": 99723,
  ""title"": ""Transformers (BERT) vs LSTM on Sentiment Analysis/NER - dataset sizes comparison""
}","Transformers (BERT) vs LSTM on Sentiment Analysis/NER - dataset sizes comparison","2021-08-05 08:12:35","99727","2","1738","<dataset><lstm><bert>","<p>I am aware (continuously learning) of the advantages of Transformers over LSTMs.</p>
<p>At the same time, I was wondering from the viewpoint of <strong>size of the data needed, contrast of those two techniques</strong>, supposing I want to train for a downstream task, (classification or NER for instance), in which case would I need more data to achieve a specific result (although I am fully aware we never know in advance for a task how much data we need).</p>
<p>Presuming a result of N% (supposing that threshold is achievable for both LSTM and BERT), which architecture (LSTM or BERT) would require a bigger dataset (regardless of the size, I am aware dataset size is task-dependent and subject to change) to reach that point.</p>
<p>Does BERT need a bigger dataset to achieve &quot;good results&quot; (an empirical observation would help me) or a, say, bidirectional LSTM?</p>
","bert"
"{
  ""id"": 99688,
  ""title"": ""Error to load a pre-trained BERT model""
}","Error to load a pre-trained BERT model","2021-08-04 13:39:27","99736","0","566","<python><nlp><bert><transformer>","<h2>Background</h2>
<p>I'm reading <a href=""https://blog.codecentric.de/en/2020/12/ner-with-little-data-transformers-to-the-rescue/"" rel=""nofollow noreferrer"">this article</a> about a natural language task, named entity recognition and trying to load a pre-trained BERT model on Google colaboratory.</p>
<p><strong>How can I fix an error to load a pre-trained BERT model?</strong></p>
<h2>Code</h2>
<pre class=""lang-py prettyprint-override""><code>from transformers import AutoConfig, TFAutoModelForTokenClassification
MODEL_NAME = 'bert-base-german-cased' 
config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))
model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)
model.summary()
</code></pre>
<h2>Error</h2>
<p>I can understand that schema is not defined before the line, but I cannot find a clew on the article to fix it.</p>
<pre><code>      1 from transformers import AutoConfig, TFAutoModelForTokenClassification
      2 MODEL_NAME = 'bert-base-german-cased'
----&gt; 3 config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))
      4 model = TFAutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)
      5 model.summary()

NameError: name 'schema' is not defined
</code></pre>
<h2>What I tried</h2>
<p>I checked <a href=""https://blog.codecentric.de/en/2020/11/take-control-of-named-entity-recognition-with-you-own-keras-model/"" rel=""nofollow noreferrer"">previous blogpost</a> following the advice from a comment, and found one description.</p>
<p>However, I'm not sure where to insert it to the original code.</p>
<pre><code>For simplicity, we’ll truncate the sentences to a maximum length and pad shorter input sequences. But first, let us determine the set of all tags in the data and add an extra tag for the padding:

#code
schema = ['_'] + sorted({tag for sentence in samples for _, tag in sentence})
</code></pre>
<p>Is it correct understanding?</p>
<pre><code>def load_data(filename: str):
   with open(filename, 'r') as file:
     lines = [line[:-1].split() for line in file]
     samples, start = [], 0
     for end, parts in enumerate(lines):
       if not parts:
         sample = [(token, tag.split('-')[-1]) for token, tag in lines[start:end]]
         samples.append(sample)
         start = end + 1
     if start &lt; end:
       samples.append(lines[start:end])
     
     return samples

samples = load_data('data/01_raw/bag.conll')
train_samples = load_data('data/01_raw/bag.conll')
val_samples = load_data('data/01_raw/bgh.conll')
all_samples = train_samples + val_samples

schema = ['_'] + sorted({tag for sentence in samples for _, tag in sentence})
</code></pre>
<p>I checked the output.</p>
<pre><code>print(schema)
#result
['_', 'AN', 'EUN', 'GRT', 'GS', 'INN', 'LD', 'LDS', 'LIT', 'MRK', 'O', 'ORG', 'PER', 'RR', 'RS', 'ST', 'STR', 'UN', 'VO', 'VS', 'VT']
</code></pre>
","bert"
"{
  ""id"": 99609,
  ""title"": ""How to interepret BERT Attention""
}","How to interepret BERT Attention","2021-08-02 14:27:57","99612","0","44","<bert>","<p>Can we tell BERT extracts local features?
For example consider the sentence &quot;This is my first sentence. This is my second sentence&quot;.
Now How Bert extracts the features.
attention is computed for each sentence or as whole?</p>
","bert"
"{
  ""id"": 99605,
  ""title"": ""Bert to extract local features""
}","Bert to extract local features","2021-08-02 13:46:28","99607","0","2905","<bert>","<p>Bert is pre-trained model which can be fine-tuned for the text classification. How to extract local features using BERT</p>
","bert"
"{
  ""id"": 98151,
  ""title"": ""Combining heterogeneous numerical and text features""
}","Combining heterogeneous numerical and text features","2021-07-21 13:05:14","","3","1454","<neural-network><regression><decision-trees><bert><embeddings>","<p>We want to solve a regression problem of the form &quot;given two objects <span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span>, predict their score (think about it as a similarity) <span class=""math-container"">$w(x,y)$</span>&quot;. We have 2 types of features:</p>
<ul>
<li>For each object, we have about 1000 numerical features, mainly of the following types: 1) &quot;Historical score info&quot;, e.g. historical means <span class=""math-container"">$w(x,\cdot)$</span> up to the point we use the feature; 2) 0/1 features meaning whether object <span class=""math-container"">$x$</span> has a particular attribute, etc.</li>
<li>For each object, we have a text which describes the object (description is not reliable, but still useful).</li>
</ul>
<p>Clearly, when predicting a score for a pair <span class=""math-container"">$(x,y)$</span>, we can use features for both <span class=""math-container"">$x$</span> and <span class=""math-container"">$y$</span>.</p>
<p>We are currently using the following setup (I omit validation/testing):</p>
<ul>
<li>For texts, we compute their BERT embeddings and then produce a feature based on the similarity between the embedding vectors (e.g. cosine similarity between them).</li>
<li>We split the dataset into fine-tuning and training datasets. The fine-tuning dataset may be empty meaning no fine-tuning.</li>
<li>Using the fine-tuning dataset, we fine-tune BERT embeddings.</li>
<li>Using the training dataset, we train decision trees to predict the scores.</li>
</ul>
<p>We compare the following approaches:</p>
<ul>
<li>Without BERT features.</li>
<li>Using BERT features, but without fine-tuning. There is some reasonable improvement in prediction accuracy.</li>
<li>Using BERT features, with fine-tuning. The improvement is very small (but the prediction using only BERT features improved, of course).</li>
</ul>
<p><strong>Question:</strong> Is there something simple I'm missing in this approach? E.g. maybe there are better ways to use texts? Other ways to use embeddings? Better approaches compared with decision trees?</p>
<p>I tried to do multiple things, without any success. The approaches which I expected to provide improvements are the following:</p>
<ul>
<li><p>Fine-tune embeddings to predict difference between <span class=""math-container"">$w(x,y)$</span> and mean <span class=""math-container"">$w(x, \cdot)$</span>. The motivation is that we already have a feature &quot;mean <span class=""math-container"">$w(x,\cdot)$</span>&quot;, which is a baseline for an object <span class=""math-container"">$x$</span>, and we are interested in the deviation from this mean.</p>
</li>
<li><p>Use NN instead of decision trees. Namely, I use few dense layers to turn embedding vectors into features, like this:</p>
<pre><code> nn.Sequential(
      nn.Linear(768 * 2, 1000),
      nn.BatchNorm1d(1000),
      nn.ReLU(),
      nn.Linear(1000, 500),
      nn.BatchNorm1d(500),
      nn.ReLU(),
      nn.Linear(500, 100),
      nn.BatchNorm1d(100),
      nn.ReLU(),
      nn.Linear(100, 10),
      nn.BatchNorm1d(10),
      nn.ReLU(),
  )
</code></pre>
<p>After that, I combine these new <span class=""math-container"">$10$</span> features with <span class=""math-container"">$2000$</span> features I already have, and use similar architecture on top of them:</p>
<pre><code>  nn.Sequential(
      nn.Linear(10 + n_features, 1000),
      nn.BatchNorm1d(1000),
      nn.ReLU(),
      nn.Linear(1000, 500),
      nn.BatchNorm1d(500),
      nn.ReLU(),
      nn.Linear(500, 100),
      nn.BatchNorm1d(100),
      nn.ReLU(),
      nn.Linear(100, 1),
  )
</code></pre>
</li>
</ul>
<p>But as a result, my prediction is much worse compared with decision trees. Are there better architectures suited for my case?</p>
","bert"
"{
  ""id"": 97617,
  ""title"": ""BERT Optimization for Production""
}","BERT Optimization for Production","2021-07-08 13:18:05","97652","1","111","<nlp><bert><transformer><semantic-similarity>","<p>I'm  using BERT to transform text into 768 dim vector, It's multilingual :</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') 
</code></pre>
<p>Now i want to put the model into production but the embedding time is too much and i want to reduce and optimize the model <strong>to reduce the embedding time</strong> What are the libraries that enable me to do this ?</p>
","bert"
"{
  ""id"": 97529,
  ""title"": ""Document ranking on a web scraped dataset without any labelled data""
}","Document ranking on a web scraped dataset without any labelled data","2021-07-06 15:11:05","","1","139","<nlp><text-mining><bert><information-retrieval><similar-documents>","<p>I want to create a document ranking model which returns similar rows in the dataset for a sample query. The text in this corpus is standard english but without any labels (ie no query-related documents structure). Is it possible to use a pretrained model trained on a large corpus (like bert or word2vec) and use it directly on the scraped dataset without any evaluation and get decent results? If not this, is training a model on the MS macro dataset and applying it on this corpus worth exploring?</p>
","bert"
"{
  ""id"": 97310,
  ""title"": ""What is the difference between BERT and Roberta""
}","What is the difference between BERT and Roberta","2021-07-01 11:02:12","97340","11","21066","<bert><transformer>","<p>I want to understand the difference between BERT and Roberta. I saw the article below.</p>
<p><a href=""https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8"" rel=""noreferrer"">https://towardsdatascience.com/bert-roberta-distilbert-xlnet-which-one-to-use-3d5ab82ba5f8</a></p>
<p>It mentions that Roberta was trained on 10x more data but I don't understand the dynamic masking part. It says masked tokens change between epochs.
Shouldn't this flatten the learning curve?</p>
","bert"
"{
  ""id"": 97098,
  ""title"": ""NLP-Problem, language model BERT?""
}","NLP-Problem, language model BERT?","2021-06-25 13:57:19","","1","46","<machine-learning><deep-learning><nlp><bert><language-model>","<p>Right now I am in the process of deciding on my masters thesis topic. Right now I and my professor are thinking about the possibility of having a large dataset of product requirements given in a natural language. My task would be to develop a domain-specific language (DSL) for these requirements and afterwards to develop a neural network that gets the dataset containing these requirements as an input and transforms each requirement into a requirement in this DSL that I have to develop beforehand.</p>
<p>In theory a topic which I personally find very interesting and would love to learn more about, the only problem is, my current knowledge in this area is very limited so I am trying to read up on it right now.</p>
<p>An example for such a DSL would be presented in this Paper: <a href=""https://www.researchgate.net/publication/281372036_A_Textual_Domain_Specific_Language_for_Requirement_Modelling"" rel=""nofollow noreferrer"">A Textual Domain Specific Language for Requirement Modelling</a>
(It's not exactly the same but a good showcase of the DSL might be derived from it).</p>
<p>For the people that don't want to read the paper there is an example given like this:</p>
<p><code>Natural Language Req: &quot;The Lateral and Vertical 'ERC' and'WRD' labels shall be displayed in a green color&quot;</code></p>
<p>The neural network should then take this requirement as an input and generate the following requirements:</p>
<p>Req1: <strong>The</strong> Lateral_ERC_Label <strong>shall be displayed in</strong> green</p>
<p>Req2: <strong>The</strong> Lateral_WRD_Label <strong>shall be displayed in</strong> green</p>
<p>Req3: <strong>The</strong> Vertical_ERC_Label <strong>shall be displayed in</strong> green</p>
<p>Req4: <strong>The</strong> Vertical_WRD_Label <strong>shall be displayed in</strong> green</p>
<p>As we can see the DSL, in this case, has the keywords &quot;The&quot; and &quot;shall be displayed in&quot; (Bolded) that stay the same for each generated requirement, and in between are the specifications for the requirement.</p>
<p>Now the question is, how is this achievable? Or rather, is this achievable at all? My professor said I should look into BERT for this task but my research has led me to the conclusion that BERT is not really suitable for text generation.</p>
<p>On the other hand, I am not really sure if this above-mentioned problem can be classified as text generation. It looks like a combination problem of text extraction (extracting the necessary keywords) and text generation.</p>
<p>But I am not really sure if I am even looking and researching in the right direction right now. So I would really appreciate it if somebody would be able to help me out here and giving me a push in the right direction.</p>
","bert"
"{
  ""id"": 96904,
  ""title"": ""How to write a generator to fine-tune transformer based models (Tensorflow)""
}","How to write a generator to fine-tune transformer based models (Tensorflow)","2021-06-21 06:00:20","","0","382","<keras><tensorflow><bert><transformer>","<p>I have been trying to write a generator for DistillBertFast model</p>
<pre><code>## Generator
def _generator(text=train_texts, label=Y_oh_train, batch_size=1):
# label = tf.ragged.constant(label)
while True:
    for i in range(0,len(text),batch_size):
        yield dict(tokenizer(text[i:i+batch_size], truncation=True, padding=True, return_tensors='tf')), label[i:i+batch_size]

## tf Dataset
train_dataset = tf.data.Dataset.from_generator(_generator, output_types=({'input_ids':tf.int32,
                                                                      'attention_mask':tf.int32}, tf.float32))


## model compile

    loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=True)
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),
    loss=loss_fn,
    metrics=[tf.keras.metrics.categorical_accuracy])

## sample data
train_texts = ['This gif kills me Death is literally gushing towards you and you really gon do a whole 3point turn', 'LOVE TEST Raw Real JaDine', 'We would like to wish everyone a very Happy New Year and all the best in 2018']

Y_oh_train=array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0.],
   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0.],
   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
    0.]])
</code></pre>
<p>But when I try to fit the model it gives error:</p>
<pre><code>    ValueError                                Traceback (most recent call last)
&lt;ipython-input-195-05df82e86e2e&gt; in &lt;module&gt;()
      4     loss=loss_fn,
      5     metrics=[tf.keras.metrics.categorical_accuracy])
----&gt; 6 model.fit(t)

9 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1181                 _r=1):
   1182               callbacks.on_train_batch_begin(step)
-&gt; 1183               tmp_logs = self.train_function(iterator)
   1184               if data_handler.should_sync:
   1185                 context.async_wait()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--&gt; 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    931       # This is the first call of __call__, so we have to initialize.
    932       initializers = []
--&gt; 933       self._initialize(args, kwds, add_initializers_to=initializers)
    934     finally:
    935       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    762     self._concrete_stateful_fn = (
    763         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--&gt; 764             *args, **kwds))
    765 
    766     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   3048       args, kwargs = None, None
   3049     with self._lock:
-&gt; 3050       graph_function, _ = self._maybe_define_function(args, kwargs)
   3051     return graph_function
   3052 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3442 
   3443           self._function_cache.missed.add(call_context_key)
-&gt; 3444           graph_function = self._create_graph_function(args, kwargs)
   3445           self._function_cache.primary[cache_key] = graph_function
   3446 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3287             arg_names=arg_names,
   3288             override_flat_arg_shapes=override_flat_arg_shapes,
-&gt; 3289             capture_by_value=self._capture_by_value),
   3290         self._function_attributes,
   3291         function_spec=self.function_spec,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    997         _, original_func = tf_decorator.unwrap(python_func)
    998 
--&gt; 999       func_outputs = python_func(*func_args, **func_kwargs)
   1000 
   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    670         # the function a weak reference to itself to avoid a reference cycle.
    671         with OptionalXlaContext(compile_with_xla):
--&gt; 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    673         return out
    674 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    984           except Exception as e:  # pylint:disable=broad-except
    985             if hasattr(e, &quot;ag_error_metadata&quot;):
--&gt; 986               raise e.ag_error_metadata.to_exception(e)
    987             else:
    988               raise

ValueError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:855 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py:800 call  *
        distilbert_output = self.distilbert(
    /usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py:415 call  *
        embedding_output = self.embeddings(
    /usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.py:122 call  *
        final_embeddings = self.LayerNorm(inputs=final_embeddings)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1030 __call__  **
        outputs = call_fn(inputs, *args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/normalization.py:1218 call
        ndims = len(input_shape)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_shape.py:855 __len__
        raise ValueError(&quot;Cannot take the length of shape with unknown rank.&quot;)

ValueError: Cannot take the length of shape with unknown rank.
</code></pre>
<p>I have been trying to find a work around, I can't put in a fixed tensor shape in generator because I can't control the shape of output from generator, it'd be based on the max length on each call, I can't load all the data at once, since the data is too huge to be loaded in memory</p>
","bert"
"{
  ""id"": 96632,
  ""title"": ""Influence of label names on the classfierier perfromance""
}","Influence of label names on the classfierier perfromance","2021-06-14 20:24:33","96635","0","24","<python><nlp><word-embeddings><bert>","<p>I am building a text classifier, the labels in my training data are not just short names like &quot;Dog&quot; or &quot;Cat&quot;, they are more of lengthy sentences that range from 2 words to around 20 words.</p>
<p>Does the length of the label/class name affect the performance of the classifier? in other words, should I try to shorten the names?</p>
","bert"
"{
  ""id"": 96595,
  ""title"": ""Ways to build Abstractive summarisation and what are it's challenges""
}","Ways to build Abstractive summarisation and what are it's challenges","2021-06-14 07:30:21","","0","21","<deep-learning><nlp><bert><transformer>","<p>What are state of art techniques to build Abstractive summarisation on some paragraphs or articles and what kind of hurdles or challenges are there to approach this problem?</p>
","bert"
"{
  ""id"": 96471,
  ""title"": ""How can i extract words from a single concatenated word?""
}","How can i extract words from a single concatenated word?","2021-06-10 10:34:49","96472","1","252","<nlp><text-mining><word-embeddings><bert><spacy>","<p>I'm stuck on this problem and would love some input.</p>
<p>I have mulitple words such as <em>getExtention</em>, <em>getPath</em>, <em>someWord</em> or <em>someword</em> and i want to separate each concatinated words into its own words such as:</p>
<p>getExtention ---&gt; [get][Extention].</p>
<p>someword --&gt; [some][word].</p>
<p>The concatenated words can also be in all small letters.</p>
<p>Do you guys have any ideas how I could achieve that?</p>
","bert"
"{
  ""id"": 96463,
  ""title"": ""Using BERT for search engine with an Elastic Database""
}","Using BERT for search engine with an Elastic Database","2021-06-10 06:02:15","","0","566","<bert><grid-search><elastic-net>","<p>I want to make Documents search engine where the user will type a query and top n relevant documents should be shown.
I want to use BERT for the searching and the first question is can i use it with an Elastic Database ?
Seconed question is which task should i use for the pretrained model 1) Question-Answer 2) Binary Classification as 1 relevant 2 not relevant ?</p>
","bert"
"{
  ""id"": 96345,
  ""title"": ""why multiple attention heads learn differently""
}","why multiple attention heads learn differently","2021-06-06 18:46:19","","1","932","<bert><transformer><attention-mechanism>","<p>In transformer architecture multi head attention blocks are used. While visualizing their output it can be seen that every layer has learnt different relations of words. e.g., layer 5 has learnt that &quot;It&quot; is more related to &quot;animal&quot;.</p>
<p><a href=""https://i.sstatic.net/uTExM.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/uTExM.png"" alt=""sample attention layer 5"" /></a></p>
<p>Question here is, when all attention layers are running in parallel, what is different fed to different layer so that they learn different things?<br />
Note: this answer is not clear - <a href=""https://datascience.stackexchange.com/questions/42985/why-and-how-bert-can-learn-different-attentions-for-each-head"">why-and-how-bert-can-learn-different-attentions-for-each-head</a></p>
","bert"
"{
  ""id"": 96282,
  ""title"": ""Named Entity Recognition with BIO Tagging""
}","Named Entity Recognition with BIO Tagging","2021-06-04 22:16:50","","3","1734","<bert><named-entity-recognition>","<p>I'm trying to implement NER using BIO annotation. For example</p>
<pre><code>&quot;I went to the United States&quot;  
[O, O, O, B, I, I]
</code></pre>
<p>where B and I denote the beginning and 'I' the following of the entity.</p>
<p>However, when I use a vanilla BERT to do classification(whether it belongs it 'B', 'I', 'O') at each position of the sequence, I encounter cases where 'O' is followed by an 'I'. There are no cases in the data that exhibit ('O', 'I') pattern since there's always a 'B' or 'I' in front. Obviously, there's nothing to enforce the model to exclude such a pattern but I would like to somehow incorporate it into the model(like a transitioning probability from 'O' to 'I' being 0 or something).</p>
<p>I took a look at conditional random fields on top of BERT that is trying to do something very similar but the prediction somehow still contained these 'O' 'I' patterns.</p>
","bert"
"{
  ""id"": 95103,
  ""title"": ""Combine BertForSequenceClassificaion with Additional Features""
}","Combine BertForSequenceClassificaion with Additional Features","2021-05-31 16:28:01","","0","324","<pytorch><bert><text-classification>","<p>I'm using BertForSequenceClassification + Pytorch Lightning-Flash for a text classification task.
I want to add additional features besides the text (e.g. categorical features). From what I understand, I need to override BertForSequenceClassification &quot;forward&quot; method and change the final classification layer (at least) to include the CLS vector + features vector. However, I didn't understand how I adapt the data loading procedure to this task - the text part is represented as input ids, and the rest supposed to be represented differently. Is there a simple way to combine text+features for Bert classification task?
Thank you!</p>
","bert"
"{
  ""id"": 94987,
  ""title"": ""An algorithm to extract the purpose of a document""
}","An algorithm to extract the purpose of a document","2021-05-27 17:33:50","","0","35","<nlp><lstm><bert><transformer><attention-mechanism>","<p>I want to build an algorithm to extract the <strong>purpose of the document</strong> (scientific papers for example) by extracting the sentences that state the purpose. I don't have many annotated data so I might use a semi-supervised learning algorithm. I was thinking of training a Q/A algorithm (using Bert for example..) but I want something to be specific to the purpose task..</p>
<p>Any idea or keywords that might help ?
Thanks!</p>
","bert"
"{
  ""id"": 94036,
  ""title"": ""BERT Self-Attention layer""
}","BERT Self-Attention layer","2021-05-05 18:11:53","94044","0","347","<nlp><bert>","<p>I am trying to use the <strong>first</strong> individual <a href=""https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert/modeling_bert.py#L213"" rel=""nofollow noreferrer"">BertSelfAttention</a> layer for the BERT-base model, but the model I am loading from <code>torch.hub</code> seems to be different then the one used in hugginface <code>transformers.models.bert.modeling_bert</code>:</p>
<pre><code>import torch, transformers

tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)
torch_model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')

inputs = tokenizer.encode_plus(&quot;Hello&quot;, &quot;World&quot;, return_tensors='pt')

output_embedding = torch_model.embeddings(inputs['input_ids'], inputs['token_type_ids'])

output_self_attention = torch_model.encoder.layer[0].attention.self(output_embedding)[0]

# compare output with using the huggingface model directly
bert_self_attn = transformers.models.bert.modeling_bert.BertSelfAttention(torch_model.config) 

# transfer all parameters
bert_self_attn.load_state_dict(torch_model.encoder.layer[0].attention.self.state_dict())
# &lt;All keys matched successfully&gt;

output_self_attention2 = bert_self_attn(output_embedding)[0]
output_self_attention != output_self_attention2 # tensors are not equal?
</code></pre>
<p>Why is <code>output_self_attention2</code> different from <code>output_self_attention</code>? I thought they would give the same output given the same input.</p>
","bert"
"{
  ""id"": 94021,
  ""title"": ""Imbalance classes in Named Entity Recognition""
}","Imbalance classes in Named Entity Recognition","2021-05-05 13:47:40","","2","175","<machine-learning><class-imbalance><bert><named-entity-recognition>","<p>I am currently working on a NER problem which attempts to extract 2 entities - place-of-interest(POI) and street from an address string in the Indonesian language.</p>
<p>I used IndoBert (available <a href=""https://huggingface.co/sarahlintang/IndoBERT"" rel=""nofollow noreferrer"">here</a>) and attached FC layers onto the BERT model, utilising cross-entropy loss to predict the class each word belongs to. However a typical sample sentence label in my dataset looks like this <code>[1, 4, 5, 5, 1, 1, 2, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</code></p>
<p>Where 2-5 represents the POI and street class, 0 are the pad tokens and 1 codes for O tags(BIO tagging). However the model ends up constantly predicting majority class for all words.I then used ignore index and weights in my loss function which looks like this:</p>
<pre><code>WEIGHTS = torch.Tensor([0.05, 0.2, 1, 1, 1, 1])
def loss(predicted, target):
    predicted = torch.rot90(predicted, -1, (1,2))
    criterion = nn.CrossEntropyLoss(weight = WEIGHTS,ignore_index=0, reduction='mean')
    return criterion(predicted, target)
</code></pre>
<p>However while this solves the problem above. The model also does not learn and the losses simply fluctuates around the same level. I am therefore wondering if there is a way to <strong>adjust for this class imbalance</strong> or <strong>stop the model from predicting the classes for [PAD] tokens</strong>.</p>
<hr />
<p>This is the code for my model:</p>
<pre><code>class aem(nn.Module):
    def __init__(self, no_class):
        super().__init__()
        self.bert = AutoModel.from_pretrained(&quot;sarahlintang/IndoBERT&quot;)
        self.drop1 = nn.Dropout(p=0.1)
        self.l1 = nn.Linear(self.bert.config.hidden_size, no_class)
        self.out = nn.GELU()     
    
    def forward(self, inputs, attn):
        hidden = self.bert(inputs, token_type_ids=None, attention_mask=attn, return_dict = True)
        L1out = self.out(self.l1(self.drop1(hidden[0])))
        return L1out
</code></pre>
","bert"
"{
  ""id"": 93931,
  ""title"": ""BERT embedding layer""
}","BERT embedding layer","2021-05-03 20:29:42","93935","3","6705","<nlp><bert>","<p>I am trying to figure how the embedding layer works for the pretrained BERT-base model. I am using pytorch and trying to dissect the following model:</p>
<pre><code>import torch
model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-uncased')
model.embeddings
</code></pre>
<p>This BERT model has 199 different named parameters, of which the first <strong>5</strong> belong to the embedding layer (the first layer)</p>
<pre><code>==== Embedding Layer ====

embeddings.word_embeddings.weight                       (30522, 768)
embeddings.position_embeddings.weight                     (512, 768)
embeddings.token_type_embeddings.weight                     (2, 768)
embeddings.LayerNorm.weight                                   (768,)
embeddings.LayerNorm.bias                                     (768,)
</code></pre>
<p>As I understand, the model accepts input in the shape of <code>[Batch, Indices]</code> where <code>Batch</code> is of arbitrary size (usually 32, 64 or whatever) and <code>Indices</code> are the corresponding indices for each word in the tokenized input sentence. <code>Indices</code> has a max length of <strong>512</strong>. One input sample might look like this:</p>
<pre><code>[[101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 102]]
</code></pre>
<p>This contains only 1 batch and is the tokenized form of the sentence &quot;The quick brown fox jumps over the lazy dog&quot;.</p>
<p>The first <code>word_embeddings</code> weight will translate each number in <code>Indices</code> to a vector spanned in <code>768</code> dimensions (the embedding dimension).</p>
<p>Now, the <code>position_embeddings</code> weight is used to encode the position of each word in the input sentence. Here I am confused about why this parameter is being learnt? Looking at an <a href=""https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/embedding/position.py#L11-#L21"" rel=""nofollow noreferrer"">alternative implementation</a> of the BERT model, the positional embedding is a static transformation. This also seems to be the conventional way of doing the positional encoding in a transformer model. Looking at the <em>alternative implementation</em> it uses the sine and cosine function to encode interleaved pairs in the input. I tried comparing <code>model.embeddings.position_embeddings.weight</code> and <a href=""https://github.com/codertimo/BERT-pytorch/blob/master/bert_pytorch/model/embedding/position.py#L21"" rel=""nofollow noreferrer""><code>pe</code></a>, but I cannot see any similarity. The in the last sentence under under A.2 Pre-training Procedure (page 13) the <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">paper</a> states</p>
<blockquote>
<p>Then, we train the rest
10% of the steps of sequence of 512 to learn the
positional embeddings.</p>
</blockquote>
<p><em><strong>Why is the positional embedding weight being learnt and not predefined?</strong></em></p>
<p>The next layer after the positional embedding is the <code>token_type_embeddings</code>. Here I am confused about how the segment label is inferred by the model. If I understand this correctly each input sentence is delimited by the <code>[SEP]</code> token. In the example above there is only 1 <code>[SEP]</code> token and the segment label must be <code>0</code> for that sentence. But there could be a maximum of 2 segment labels. If so, will the 2 segments be handled separately or are they processed in parallel all the same as one &quot;array&quot;? <em><strong>How does the model handle multiple sentence segments?</strong></em></p>
<p>finally the output from theese 3 embeddings are added togheter and passed through layernorm which I understand. But, <em><strong>are the weights in these embedding layers adjusted when fine-tuning the model to a downstream task?</strong></em></p>
","bert"
"{
  ""id"": 93638,
  ""title"": ""Is positional encoding (in transformers) an estimation of the relative positions of words in the training corpus texts?""
}","Is positional encoding (in transformers) an estimation of the relative positions of words in the training corpus texts?","2021-04-27 19:23:31","93655","1","338","<bert><transformer>","<p>Is this some kind of estimation of the relative positions of words in the training texts? are they creating some kind of statistical &quot;distribution&quot; of words? is &quot;cat&quot; usually 2 or 3 words away from &quot;milk&quot; in English language? things have to have a meaning, havent they?  Is BERT just adding some aditional dimensions to the vector space to include info on the relative positions of words?</p>
","bert"
"{
  ""id"": 93447,
  ""title"": ""Combining textual and numeric features into pre-trained Transformer BERT""
}","Combining textual and numeric features into pre-trained Transformer BERT","2021-04-23 11:07:41","93477","2","2437","<nlp><pytorch><bert><finetuning>","<p>I have a dataset with 3 columns:</p>
<ol>
<li>Text</li>
<li>Meta-data (intending to extract features from it, then use those i.e., numerical features)</li>
<li>Target label</li>
</ol>
<p><strong>Question 1: How can I use a pre-trained BERT instance on more than the text?</strong></p>
<p>One theoretical <a href=""https://datascience.stackexchange.com/questions/54888/how-can-i-add-custom-numerical-features-for-training-to-bert-fine-tuning?rq=1"">solution</a> suggests having BERT fed the text and another neural network with the numerical features fed into this one, then aggregating their output, into another neural network.</p>
<ul>
<li>Is that the most efficient approach?</li>
</ul>
<p><strong>Question 2: How can you connect neural networks?</strong></p>
<ul>
<li><p>You get the output from each, but then what?</p>
</li>
<li><p>You get classification output from BERT, you get classification
output from MLP based on numerical features.</p>
</li>
<li><p>You concatenate these and feed them to another MLP, and you get the
final prediction? Wouldn't that last prediction be less robust ?</p>
</li>
<li><p>In other words, does the last MLP encapsulate the other 2 networks?</p>
</li>
<li><p>If so, what happens if BERT predicts on 90%, but the first MLP just
50%, will we get a lesser outcome?</p>
</li>
</ul>
<p><strong>Question 3: Any tips on how to implement this in pytorch?</strong></p>
","bert"
"{
  ""id"": 93436,
  ""title"": ""Social media text analysis""
}","Social media text analysis","2021-04-23 05:11:33","","2","47","<text-mining><bert>","<p>I'm currently analyzing Korean social media text. The below are the steps of the analysis.</p>
<ol>
<li>Collect/crawling text data from social media (e.g. Twitter, Facebook), which are related to specific topics.</li>
<li>Analyze the data by using BERT. This includes text classification and sentiment analysis.</li>
</ol>
<p>However, I've faced some complicated problems.</p>
<p>In step 1, I searched with some relevant keywords to the topic, but indeed, the collected data are not related to the topic. Some of them are related to the topic but some of them are not. In this case, <strong>how can I get these topic-related data?</strong> Should I annotate the data and training a model to classify the relevant/non-relevant data? Is there any unsupervised approach such as topic modeling? Is there any method that lets me <strong>collect</strong> topic-related data?</p>
<p>In step 2, I actually got documents, sentences, or paragraphs rather than just one sentence. Therefore the <strong>topics and sentiments are mixed</strong> in the documents and I'm really struggling to annotate them. I tried to separate the documents into smaller ones but there are no thresholds to distinguish them since they are social media text. Even if I could, this makes another problem which is the context of the document is disappeared and this kills the topic or sentiments.</p>
<p>Here's the summary:</p>
<ol>
<li>Collecting steps: Collect data related to a specific topic
<ul>
<li>Problems: The collected data are actually not related to the topic</li>
</ul>
</li>
<li>Analyzing steps: Analyze (classification) the data by using BERT
<ul>
<li>The labels are mixed in the collected data. How can I split them into some meaningful pieces (big enough to maintain the context of the documents, small enough to analyze them easily)</li>
</ul>
</li>
</ol>
<p>Thank you.</p>
","bert"
"{
  ""id"": 93419,
  ""title"": ""Embedding from Transformer-based model from paragraph or documnet (like Doc2Vec)""
}","Embedding from Transformer-based model from paragraph or documnet (like Doc2Vec)","2021-04-22 18:47:47","","1","1119","<nlp><bert><transformer><embeddings><doc2vec>","<p>I have a set of data that contains the different lengths of sequences. On average the sequence length is 600. The dataset is like this:</p>
<pre><code>S1 = ['Walk','Eat','Going school','Eat','Watching movie','Walk'......,'Sleep']
S2 = ['Eat','Eat','Going school','Walk','Walk','Watching movie'.......,'Eat']
.........................................
.........................................
S50 = ['Walk','Going school','Eat','Eat','Watching movie','Sleep',.......,'Walk']
</code></pre>
<p>The number of unique actions in the dataset are fixed. That means some sentences may not contain all of the actions.</p>
<p>By using Doc2Vec (Gensim library particularly), I was able to extract embedding for each of the sequences and used that for later task (i.e., clustering or similarity measure)</p>
<p>As transformer is the state-of-the-art method for NLP task. I am thinking if Transformer-based model can be used for similar task. While searching for this technique I came across the <a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">sentence-Transformer</a>. But it uses a pretrained BERT model (which is probably for language but my case is not related to language) to encode the sentences. Is there any way I can get embedding from my dataset using Transformer-based model?</p>
","bert"
"{
  ""id"": 93414,
  ""title"": ""BERT MLM overfitting""
}","BERT MLM overfitting","2021-04-22 16:28:40","","1","489","<nlp><loss-function><optimization><overfitting><bert>","<p>We are training the BERT model on masked language modeling task for the Russian Language. Our dataset consists of 60 mln texts with (128 tokens for each text) from online social networks, predominantly in the Russian language. We have not performed any text preprocessing. We use AdamW optimizer and <a href=""https://huggingface.co/DeepPavlov/rubert-base-cased"" rel=""nofollow noreferrer"">RuBERT</a> as a pre-trained model. The training parameters are the following:</p>
<pre><code>Learning rate: 1**e-5
Warmup: 30 000
Seq. len: 128
</code></pre>
<p>The charts below show the cross-entropy(CE) loss function for test evaluation as iterations go on. One iteration contains 100 batches, One epoch contains ~ 3500 iterations. The blue chart shows loss function when we mask only Cyrillic tokens (Cyrillic BERT). The orange chart shows loss function when any token (including punctuation, URLS, numerals, English letters) can be masked and it is the only difference among the two charts (Default BERT).</p>
<p><a href=""https://i.sstatic.net/bE3Na.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bE3Na.png"" alt=""perplexity"" /></a></p>
<p>My questions are the following:</p>
<ol>
<li>Why &quot;Cyrillic BERT&quot; overfits dramatically after the ~2200 iteration while the &quot;Default BERT&quot; oscillates near 1.67 at the same time? What Bert config parameters may change it?</li>
<li>Why The &quot;Default BERT&quot; has no improvements does not improve after the ~3000 iterations? How can I improve it? (already trying to change hidden_dropout_prob to 0.2 from 0.1)</li>
<li>Does &quot;Cyrillic BERT&quot; shows worse loss function because it is easier to predict punctuation and several English names?</li>
</ol>
","bert"
"{
  ""id"": 93224,
  ""title"": ""Word2vec outperforming BERT, possible?""
}","Word2vec outperforming BERT, possible?","2021-04-18 16:45:11","93280","2","1113","<keras><tensorflow><nlp><word2vec><bert>","<p>I'm trying to solve a multilabel classification (dataset is tweet text) using a combination of BERT and CNN. As a benchmark, I'd compare it to other word embeddings, one of which is Word2vec. After numerous tries, it seems that Word2vec-CNN keeps outperforming BERT-CNN by a slight bit, here's a result from my last try:</p>
<pre><code>Word2vec-CNN
precision (macro): 0.89  
recall (macro): 0.87  
f1-score (macro): 0.88
accuracy (test set): 0.81
hamming loss: 0.062

BERT-CNN
precision (macro): 0.86  
recall (macro): 0.88  
f1-score (macro): 0.87
accuracy (test set): 0.74
hamming loss: 0.073
</code></pre>
<p>Question is:</p>
<ol>
<li>Could it be possible that Word2vec (or any static word embeddings) outperforms BERT (or any contextual word embeddings)? If so, what is the rationale? If there's any research paper on this it would be really helpful.</li>
<li>If not, what could possibly be the cause?</li>
</ol>
<p>FWIW: Model is trained using TensorFlow-Keras (I kind of suspect this is SOMEHOW caused by how TF-Keras calculates its metrics but I still haven't figured out why and, if any, a solution), and both embeddings are pretrained (BERT model was trained on a bigger corpus, around 200:1).</p>
","bert"
"{
  ""id"": 93192,
  ""title"": ""BERT is running out of memory in forward pass for my dictionary""
}","BERT is running out of memory in forward pass for my dictionary","2021-04-17 22:40:09","","1","308","<word-embeddings><bert>","<p>Running code from <a href=""https://datascience.stackexchange.com/a/93068/106276"">this answer</a>, my BERT is running out for my 4k words dictionary.
I don't need to do anything with these words yet, just make embeddings for my data. So, using this exactly:</p>
<pre><code>from transformers import BertModel, BertTokenizer

model = BertModel.from_pretrained('bert-base-uncased')
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

encoded_inputs = tokenizer(labels, padding = True, truncation = True, return_tensors = 'pt')

ids = encoded_inputs['input_ids']
mask = encoded_inputs['attention_mask']

output = model(ids, mask)
lab_embeddings = output.last_hidden_state.tolist()
</code></pre>
<p>gives me memory leakage. How can I manage this with batching since I don't have labels for classification or something like that?</p>
","bert"
"{
  ""id"": 93062,
  ""title"": ""What is the common practice for NLP or text mining for non-English?""
}","What is the common practice for NLP or text mining for non-English?","2021-04-14 18:00:57","","0","44","<nlp><text-mining><bert><pretraining>","<p>A lot of natural language processing tools are pre-trained with corpus in English. What if ones need to analyze, say, <strong>Dutch text</strong>? The blogs I find online are mostly saying <strong>traslating text into English</strong> as pro-processing. Is this the common practice? If not, then what? Also, does <strong>how similar a language is to English</strong> have an impact on the model performance?</p>
<p>For some also widely speaking languages (e.g French, Spanish), do people construct corpus in their own language and train models on it? Forgive my ignorance because I'm not able to read papers in many languages.</p>
","bert"
"{
  ""id"": 93034,
  ""title"": ""How to JUST represent words as embeddings by pretrained BERT?""
}","How to JUST represent words as embeddings by pretrained BERT?","2021-04-13 22:03:42","","0","5000","<nlp><unsupervised-learning><word-embeddings><bert><representation>","<p>I don't have enough data (i.e. I don't have enough texts) --- have only around 4k words in my dictionary. I need to compare given words, then I need to representate it as <em>embedding</em>.</p>
<p>After the representation of words I want to clusterize it, find similar vectors (i.e. words). Maybe even then make a <strong>classification</strong> to a given classes (<strong>classification</strong> there unsupervised --- since I don't have labeled data to train on).</p>
<p>I know that almost any task can be solved &quot;inside&quot; <strong>BERT</strong>, i.e. using <em>fine-tuning</em> in final layer.</p>
<p>Since all described above, <strong>I have two QUESTIONS</strong>; answers/hints/anything really appreciated since i'm stuck on that:</p>
<ol>
<li>How to just extract embeddings from BERT using some dictionary of words and use word representations for futher work?</li>
<li>Can we solve inside BERT using fine-tuning the next problem: a). Load dictionary of words into BERT b). Load given classes (words representing each class. E.g. &quot;fashion&quot;, &quot;nature&quot;). c) Make an unsupervised classification task?</li>
</ol>
","bert"
"{
  ""id"": 92269,
  ""title"": ""BERT Masked Language Model question""
}","BERT Masked Language Model question","2021-03-28 15:04:26","92384","0","85","<bert><transformer>","<p>I have been reading about BERT from the internet, and from what I understand the point of masked language modelling for BERT pretraining is so that BERT will learn to guess a &quot;masked&quot; word from the context given. The loss function will be the lowest for output embeddings which are closest to the original masked word embedding. Wouldn't it be that using this loss funtion does not guarantee that BERT will output word embeddings with context and instead could just output an embedding closest to the original masked word embedding without the relevant context?</p>
<p>Thanks.</p>
","bert"
"{
  ""id"": 92224,
  ""title"": ""How to deal with class imbalance problem in natural language processing?""
}","How to deal with class imbalance problem in natural language processing?","2021-03-27 03:49:05","92233","1","471","<classification><class-imbalance><nlp><bert>","<p>I am doing a NLP binary classification task, using Bert + softmax layer on top of it. The network uses cross-entropy loss.</p>
<p>When the ratio of positive class to negative class is 1:1 or 1:2, the model performs well on correctly classifying both classes(accuracy for each class is around 0.92).</p>
<p>When the ratio is 1:3 to 1:10, the model performs poorly as expected. When the ratio is 1:10, the model has a 0.98 accuracy on correctly classifying negative class instances, but only has a 0.80 accuracy on correctly classifying positive class instances.</p>
<p>The behavior is as expected as the model turns to classify most/all instances toward negative class since the ratio of positive class to negative class is 1:10.</p>
<p>I just want to ask what's the recommended way for handling this kind of class imbalance problem in natural language processing specifically?</p>
<p>I saw someone suggests to change loss function, or perform up/down sampling, but most of them are targetting computer vision class imbalance problem.</p>
","bert"
"{
  ""id"": 91141,
  ""title"": ""Is it good practice to use SMOTE when you have a data set that has imbalanced classes when using BERT model for text classification?""
}","Is it good practice to use SMOTE when you have a data set that has imbalanced classes when using BERT model for text classification?","2021-03-25 16:32:19","91163","0","2087","<bert><smote><oversampling>","<p>I had a question related to SMOTE. If you have a data set that is imbalanced, is it correct to use SMOTE when you are using BERT? I believe I read somewhere that you do not need to do this since BERT take this into account, but I'm unable to find the article where I read that. Either from your own research or experience, would you say that oversampling using SMOTE (or some other algorithm) is useful when classifying using a BERT model? Or would it be redundant/unnecessary?</p>
","bert"
"{
  ""id"": 91073,
  ""title"": ""dealing with HuggingFace's model's tokens""
}","dealing with HuggingFace's model's tokens","2021-03-24 03:19:40","","1","288","<nlp><bert><tokenization><huggingface><bart>","<p>I have a few questions regarding tokenizing word/characters/emojis for different huggingface models.</p>
<p>From my understanding, a model would only perform best during inference if the token of the input sentence are within the tokens that the model’s tokenizer was trained on.</p>
<p>My questions are:</p>
<ol>
<li><p>is there a way to easily find out if a particular word/emoji is compatible (included during model training) with the model? (in huggingface context)</p>
</li>
<li><p>if this word/emoji is not was not included during model training, what are the best ways to deal with these words/emojis, such that model inference would give best possible output considering the inclusion of these word/emoji as input. (for 2. it would be nice if it could be answered in the context of my huggingface setup below, if possible)</p>
</li>
</ol>
<p>My current setup is as follows:</p>
<pre><code>from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
pre_trained_model = 'facebook/bart-large-mnli'
task = 'zero-shot-classification'
candidate_labels = ['happy', 'sad', 'angry', 'confused']
tokenizer = AutoTokenizer.from_pretrained(pre_trained_model)
model = AutoModelForSequenceClassification.from_pretrained(pre_trained_model)
zero_shot_classifier = pipeline(model=model, tokenizer=tokenizer, task=task)

zero_shot_classifier('today is a good day 😃', candidate_labels=candidate_labels)
</code></pre>
<p>Any help is appreciated 😃</p>
","bert"
"{
  ""id"": 90234,
  ""title"": ""What if My Word is not in Bert model vocabulary?""
}","What if My Word is not in Bert model vocabulary?","2021-03-04 08:29:17","90236","2","5240","<keras><bert>","<p>I am doing NER using Bert Model. I have encountered some words in my datasets which is not a part of bert vocabulary and i am getting the same error while converting words to ids. Can someone help me in this?</p>
<p>Below is the code i am using for bert.</p>
<pre><code>df = pd.read_csv(&quot;drive/My Drive/PA_AG_123records.csv&quot;,sep=&quot;,&quot;,encoding=&quot;latin1&quot;).fillna(method='ffill')

!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py

import tensorflow_hub as hub
import tokenization
module_url = 'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2'
bert_layer = hub.KerasLayer(module_url, trainable=True)

vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)

tokens_list=['hrct',
 'heall',
 'government',
 'of',
 'hem',
 'snehal',
 'sarjerao',
 'nawale',
 '12',
 '12',
 '9999',
 'female',
 'mobile',
 'no',
 '1155812345',
 '3333',
 '3333',
 '3333',
 '41st',
 '3iteir',
 'fillow']

max_len =25
text = tokens_list[:max_len-2]
input_sequence = [&quot;[CLS]&quot;] + text + [&quot;[SEP]&quot;]
print(&quot;After adding  flasges -[CLS] and [SEP]: &quot;)
print(input_sequence)


tokens = tokenizer.convert_tokens_to_ids(input_sequence )
print(&quot;tokens to id &quot;)
print(tokens)
<span class=""math-container"">```</span>
</code></pre>
","bert"
"{
  ""id"": 90161,
  ""title"": ""Is LSTM or pretrained BERTForMasked LM usable for predicting changed word in a sentence using a small dataset? (2000 samples)""
}","Is LSTM or pretrained BERTForMasked LM usable for predicting changed word in a sentence using a small dataset? (2000 samples)","2021-03-02 18:48:01","","0","39","<python><lstm><nlp><bert>","<p>I have a small (2000 samples) dataset of newspaper headlines and their humorous conterparts where only one word is changed to sound silly, for example:</p>
<p><strong>Original headline</strong>: Police &lt;officer&gt; arrested for abuse of authority</p>
<p><strong>Humorous headline</strong>: Police &lt;dog&gt; arrested for abuse of authority</p>
<p>I want to train a model to predict changed sentence by the original. I am planning to implement two models for this task: one for binary tagging of input sequences (whether a word in a sentence needs to be changed) and one for predicting sentences with changed words.</p>
<p><strong>Example of Model 1 input</strong>: Police officer arrested for abuse of authority</p>
<p><strong>Example of Model 1 output</strong>: &lt;no-change&gt; &lt;change&gt; &lt;no-change&gt; &lt;no-change&gt; &lt;no-change&gt; &lt;no-change&gt; </p>
<p><strong>Example of Model 2 input</strong>: Police &lt;...&gt; arrested for abuse of authority</p>
<p><strong>Example of Model 2 output</strong>: Police dog arrested for abuse of authority</p>
<p>I am going to use a RNN/LSTM model for sequence tagging. As for the changed word prediction task, I am thinking of either using LSTM (concatenation of two parallel LSTM layers - one to run forwards on left context of word and the other to run backwards on right context) or fine-tuning <a href=""https://huggingface.co/transformers/model_doc/bert.html#bertformaskedlm"" rel=""nofollow noreferrer"">BERTForMaskedLM from huggingface/transformers</a>.</p>
<p>The question is whether it would be appropraite considering the small number of data, or should I switch to some other models?</p>
","bert"
"{
  ""id"": 89915,
  ""title"": ""Where can I find documentation or paper mentioning pre-trained distilbert-base-nli-mean-tokens model?""
}","Where can I find documentation or paper mentioning pre-trained distilbert-base-nli-mean-tokens model?","2021-02-25 14:42:20","","0","1110","<nlp><word-embeddings><bert><transformer><embeddings>","<p>I am trying to find more information about pre-trained model <code>distilbert-base-nli-mean-tokens</code>. Can someone please point me to it's paper or documentation? Is it based on <a href=""https://arxiv.org/abs/1910.01108"" rel=""nofollow noreferrer"">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> paper? This is published in March 2020. I am looking for links between this paper and Sentence-BERT (<a href=""https://www.sbert.net/index.html"" rel=""nofollow noreferrer"">sentence-transformers</a>). Original <a href=""https://arxiv.org/pdf/1908.10084.pdf"" rel=""nofollow noreferrer"">sentence-bert</a> paper is published in Aug 2019. I wanted to try pre-trained model using S-BERT model and hence tried <code>distilbert-base-nli-mean-tokens</code> <a href=""https://www.sbert.net/examples/training/sts/README.html"" rel=""nofollow noreferrer"">model</a>. After implementation I found out that it's much faster than other pre-trained models available on sentence-transformer <a href=""https://www.sbert.net/examples/training/sts/README.html"" rel=""nofollow noreferrer"">website</a>. While studying it's paper I realised original paper do not mention this pre-trained model.</p>
<p>I found <a href=""https://arxiv.org/pdf/2004.09813.pdf"" rel=""nofollow noreferrer"">Making Monolingual Sentence Embeddings Multilingual using
Knowledge Distillation</a> this paper published by same author which do mention <code>DistilmBERT</code> but not <code>DistilBert</code> Can someone please help me solve this mystery?</p>
","bert"
"{
  ""id"": 89819,
  ""title"": ""Normalized 2D tensor values are not in range 0-1""
}","Normalized 2D tensor values are not in range 0-1","2021-02-23 18:39:55","89822","1","399","<python><tensorflow><pytorch><normalization><bert>","<p>Below function takes in 2D tensor and normalizes it using broadcasting .The issue is except all values to be in range 0-1 but the result has values outside this range . How to get all values in 2D tensor in range 0-1</p>
<pre><code>def torch_normalize(tensor_list):
    means = tensor_list.mean(dim=1, keepdim=True)
    stds = tensor_list.std(dim=1, keepdim=True)
    normalized_data = (tensor_list - means) / stds
    return normalized_data

INPUT
tensor_list=tensor([[-5.6839, -7.5829, -7.2277, -6.5066, -8.4702, -7.9844, -5.6841,  1.8570,
          1.6170, -3.7592, -4.4140, -0.4981,  0.2501,  5.8463,  1.8897, -1.3968,
         -5.5402, -2.4561, -5.6819]])


Normalized result 

tensor([[-0.5981, -1.0615, -0.9748, -0.7988, -1.2780, -1.1594, -0.5981,  1.2420,
          1.1835, -0.1284, -0.2882,  0.6673,  0.8499,  2.2155,  1.2500,  0.4480,
         -0.5630,  0.1896, -0.5976]])
</code></pre>
","bert"
"{
  ""id"": 89729,
  ""title"": ""About Natural Question (NQ) benchmark in NLP""
}","About Natural Question (NQ) benchmark in NLP","2021-02-22 02:29:35","","1","34","<nlp><bert><search-engine>","<p>I recently learned that there is a benchmark called NQ.</p>
<p><a href=""https://ai.google.com/research/NaturalQuestions/visualization"" rel=""nofollow noreferrer"">https://ai.google.com/research/NaturalQuestions/visualization</a></p>
<p>Unlike other QA benchmarks which relevant document is povided with query, it has to find information from millions of corpus by itself.</p>
<p>For example, if question is &quot;when are hops added to the brewing process?&quot; other QA benchmark also provide only 1 document about brewing. While NQ provide whole wikipedia text and model has to find most relevant document and answer.</p>
<p>When I tried all the example in the NQ with google search engine it gave me the answer every time. Then</p>
<ol>
<li>How does google search engine is managing those questions so well(Since NLP such as BERT is pretty computationally expensive I think it is not likely google is running whole model for every search)</li>
<li>If google is doing it in other ways than neural network.(Or mixing it with other method) Do we need to bother with method such as REALM(<a href=""https://arxiv.org/pdf/2002.08909.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/2002.08909.pdf</a>) which seems to be computationally heavy?</li>
</ol>
","bert"
"{
  ""id"": 89684,
  ""title"": ""XLNET how to deal with text with more than 512 tokens?""
}","XLNET how to deal with text with more than 512 tokens?","2021-02-21 02:18:22","89702","0","3655","<bert><text-classification>","<p>From what I searched online, XLNET model is pre-trained with 512 tokens, and <a href=""https://github.com/zihangdai/xlnet/issues/80"" rel=""nofollow noreferrer"">https://github.com/zihangdai/xlnet/issues/80</a> , I didn't find too much useful information on that either.
How does XLnet outperform BERT on long text when the max_sequence_length hyperparameter is less than 1024 tokens ?</p>
","bert"
"{
  ""id"": 89357,
  ""title"": ""How to use BERT in seq2seq model?""
}","How to use BERT in seq2seq model?","2021-02-14 11:40:02","89375","1","2629","<neural-network><bert><transformer><transfer-learning><sequence-to-sequence>","<p>I would like to use pretrained BERT as encoder of transformer model. The decoder has the same vocabulary as encoder and I am going to use shared embeddings. But I need <code>&lt;SOS&gt;</code>, <code>&lt;EOS&gt;</code> tokens which are not trained with BERT. How should I get them ? Can I use <code>&lt;CLS&gt;</code> token as <code>&lt;SOS&gt;</code> and <code>&lt;SEP&gt;</code> as <code>&lt;EOS&gt;</code> ? Or I have to create these two embeddings as trainable Variables and concatenate them to the decoder input / labels ?</p>
","bert"
"{
  ""id"": 89221,
  ""title"": ""Medical NER for French language""
}","Medical NER for French language","2021-02-11 03:10:55","","3","191","<word-embeddings><nlp><bert><named-entity-recognition><spacy>","<p>I'm currently exploring the options to extract medical NER specifically for French language. I tried <code>SpaCy</code>'s general French NER but it wasn't helpful to the cause (mainly because of the domain-specific requirements). I assume we cannot use <code>Med7</code> or other English-language specific NER's for this purpose. I'd like to know the options and suggestions on how to proceed. I'd also like to know if <code>BioBERT</code> could come handy for this purpose, particularly by combining it with <code>camemBERT</code> or any other French language models.</p>
<p>If there're no readymade options available, I'm planning to translate French to English and then run the NER. I hope there's some potential for this approach.</p>
","bert"
"{
  ""id"": 89188,
  ""title"": ""Extracting layer output from Classification model of SimpleTransformer""
}","Extracting layer output from Classification model of SimpleTransformer","2021-02-10 07:15:33","","1","27","<python><bert><text-classification>","<p>I have fine tuned a bert base model for text classification task. Now, I want to extract hidden layer output so as to combine this output with other features to train a random forest model. Problem lies in the fact that I don't know how to extract hidden layer output. It would be really great if someone can help me out in this regard.</p>
<pre><code>from simpletransformers.classification import ClassificationModel

model_xlm = ClassificationModel('bert', 'bert-base-uncased')
model_xlm.train_model(df_train)
</code></pre>
","bert"
"{
  ""id"": 89076,
  ""title"": ""Would there be any reason to pretrain BERT on specific texts?""
}","Would there be any reason to pretrain BERT on specific texts?","2021-02-07 20:21:47","89078","1","551","<bert><transfer-learning><language-model><pretraining>","<p>So the official BERT English model is trained on Wikipedia and BookCurpos <a href=""https://en.wikipedia.org/wiki/BERT_(language_model)"" rel=""nofollow noreferrer"">(source)</a>.</p>
<p>Now, for example, let's say I want to use BERT for Movies tag recommendation. Is there any reason for me to pretrain a new BERT model from scratch on movie-related dataset?</p>
<p>Can my model become more accurate since I trained it on movie-related texts rather than general texts? Is there an example of such usage?</p>
<p>To be clear, the question is on the importance of <strong>context</strong> (not size) of the dataset.</p>
","bert"
"{
  ""id"": 89014,
  ""title"": ""Are all 110 million parameter in bert are trainable""
}","Are all 110 million parameter in bert are trainable","2021-02-06 05:38:05","89018","1","853","<deep-learning><neural-network><nlp><bert><transformer>","<p>I am trying to understand are all these 110 million parameters trainable of bert uncased model. Is there any non trainable parameters in this image below?</p>
<p>By trainable I understand they are initialized with random weight and during pretraining these weights are backpropagated and updated.</p>
<p><a href=""https://i.sstatic.net/tMIhL.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/tMIhL.png"" alt=""Bert 110 Million Parameters"" /></a></p>
","bert"
"{
  ""id"": 88977,
  ""title"": ""Backpropagation of a transformer""
}","Backpropagation of a transformer","2021-02-05 13:22:11","88978","3","5928","<deep-learning><neural-network><nlp><bert><transformer>","<p>when a transformer model is trained there is linear layer in the end of decoder which i understand is a fully connected neural network. During training of a transformer model when a loss is obtained it will backpropagate to adjust the weights.</p>
<p>My question is how deep the backpropagation is?</p>
<ul>
<li>does it happen only till linear layer weights(fully connected neural net) ?</li>
<li>OR does it extend to all the decoder layer weight matrices(Q,K,V) and Feed forward layers weights?</li>
<li>OR does it extend to the even the encoder + decoder weights ?</li>
</ul>
<p>Please help me with the answer.</p>
","bert"
"{
  ""id"": 88417,
  ""title"": ""what's the motivation behind BERT masking 2 words in a sentence?""
}","what's the motivation behind BERT masking 2 words in a sentence?","2021-01-24 20:03:49","88423","1","462","<nlp><bert><language-model>","<p><a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">bert</a> and the more recent <a href=""https://arxiv.org/pdf/1910.10683.pdf"" rel=""nofollow noreferrer"">t5</a> ablation study, agree that</p>
<blockquote>
<p>using a <strong>denoising</strong> objective always results in better downstream task performance compared to a language model</p>
</blockquote>
<p>where denoising == masked-lm == cloze.</p>
<p>I understand why learning to represent a word according to its bidirectional surroundings makes sense. However, I fail to understand why is it beneficial to learn to mask 2 words in the same sentence, e.g. <code>The animal crossed the road</code> =&gt; <code>The [mask] crossed the [mask]</code>. Why does it make sense to learn to represent <code>animal</code> without the context of <code>road</code>?</p>
<p>Note: I understand that the masking probability is 15% which corresponds to 1/7 words, which makes it pretty rare for 2 words in the same sentence to be masked, but why would it <strong>ever</strong> be beneficial, even with low probability?</p>
<p>Note2: please ignore the masking procedure sometimes replacing mask with a random/same word instead of <code>[mask]</code>, T5 investigates this choice in considerable length and I suspect that it's just an empirical finding :)</p>
","bert"
"{
  ""id"": 88348,
  ""title"": ""Bert and SVM classification""
}","Bert and SVM classification","2021-01-22 16:17:48","","1","1574","<machine-learning><classification><svm><bert>","<p>I'm trying to understand the concepts in the title and how they fit into the task of binary classification. According to my understanding so far, you can encode text using various feature-extraction methods such a bag of words. You can then use something like liblinear to obtain a SVM LibLinear model that is able to classify your data. On the other hand, you can build a model by concatenating Bert with a Dense layer. You can then fine tune this model and again, you obtain a classifier. Where would you use either one of them and why?</p>
","bert"
"{
  ""id"": 87662,
  ""title"": ""Why does BERT embedding increase the number of tokens?""
}","Why does BERT embedding increase the number of tokens?","2021-01-08 07:31:36","87663","0","344","<nlp><pytorch><bert>","<p>I am new to DataScience and trying to implement BERT embedding for one of my problems. But I am having one doubt here.
I am trying to embed the following sentence with BERT - &quot;Twinkle twinkle little star&quot;.
BERT tokenizer generates the following tokens - ['twin', '##kle', 'twin', '##kle', 'little', 'star']</p>
<p>But the final embedded tensor is having a dimension of [1,8,1024]</p>
<p>Why is the number of tokens 8 instead of 6? For any text, I am observing that number of tokens in the final embedding is getting increased by 2. Can anyone please help me to understand this?</p>
<p>I am giving the code snippet here -</p>
<pre><code>from transformers import BertTokenizer, BertForSequenceClassification, BertModel

PRE_TRAINED_MODEL_PATH = 'BERT\wwm_cased_L-24_H-1024_A-16'
tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_PATH)
model = BertModel.from_pretrained(PRE_TRAINED_MODEL_PATH)

encoded_input = tokenizer(texts, return_tensors='pt', padding=True)
emb = model(**encoded_input)
</code></pre>
","bert"
"{
  ""id"": 87637,
  ""title"": ""What is the difference between GPT blocks and BERT blocks""
}","What is the difference between GPT blocks and BERT blocks","2021-01-07 15:43:05","87679","3","2977","<bert><transformer><gpt>","<p>Nowadays many applications only use the Encoder and Decoder part of the Transformer respectively. I am having trouble understanding the difference though.</p>
<p>If GPT uses Decoder only and BERT uses Encoder only does this mean that the only difference between the two is basically in the masking part?</p>
<p>The cross attention layer in the Decoder is omitted since there is no Encoder within GPT right?</p>
","bert"
"{
  ""id"": 87606,
  ""title"": ""What is the number of neurons for the input layer of the BERT?""
}","What is the number of neurons for the input layer of the BERT?","2021-01-06 20:42:18","87612","1","730","<bert><transformer>","<p>I think it is the vocab size. However I am not sure and I appreciate your help.</p>
","bert"
"{
  ""id"": 87579,
  ""title"": ""Detecting grammatical errors with BERT""
}","Detecting grammatical errors with BERT","2021-01-06 09:48:55","","3","1391","<nlp><bert><grammar-inference>","<p>We fine-tuned BERT (<code>bert-base-uncased</code>) model with <code>CoLA</code> dataset for sentence classification task. The dataset is a mix of sentences with and without grammatical errors. The retrained model is then used to identify sentences with or without errors. Are there any other approaches we could make use of using BERT, other than building a classifier?</p>
","bert"
"{
  ""id"": 87571,
  ""title"": ""Is a BiLSTM layer required if we use BERT?""
}","Is a BiLSTM layer required if we use BERT?","2021-01-06 07:05:20","87572","5","1894","<lstm><bert><named-entity-recognition>","<p>I am new to Deep learning based NLP and I have a doubt -
I am trying to build a NER model and I found some journals where people are relying on BERT-BiLSTM-CRF model for it. As far as I know BERT is a language model that scans the contexts in both the directions and embeds words according to the context. Now my question is - if context is captured during word embedding with BERT, why do we need another layer of BiLSTM?</p>
","bert"
"{
  ""id"": 87389,
  ""title"": ""Inference order in BERT masking task""
}","Inference order in BERT masking task","2020-12-31 20:33:17","87390","1","150","<neural-network><nlp><bert><transformer><language-model>","<p>In BERT, multiple words in a single sentence can be masked at once. Does the model infer all of those words at once or iterate over them in either left to right or some other order?</p>
<p>For example:</p>
<blockquote>
<p>The dog walked in the park.</p>
</blockquote>
<p>Can be masked as</p>
<blockquote>
<p>The [mask] walked in [mask] park.</p>
</blockquote>
<p>In what order (if any) are these tokens predicted?</p>
<p>If you have any further reading on the topic, I'd appreciate it as well.</p>
","bert"
"{
  ""id"": 87382,
  ""title"": ""How do I handle class imbalance for text data when using pretrained models like BERT?""
}","How do I handle class imbalance for text data when using pretrained models like BERT?","2020-12-31 14:09:09","","1","561","<class-imbalance><multiclass-classification><bert><transformer><smote>","<p>I have a skewed dataset consisting of samples of the form:</p>
<pre><code>Category 1 10000
Category 2  2000
Category 3   400
Category 4   300 
Category 5   100
</code></pre>
<p>The dataset consists of text with data labeled into one of the five categories. I am trying to use the pretrained models like BERT for the classification task but the model fails to identify the
categories 3-5 .I have tried to apply class weights in the loss criterion however it doesn't help much although it gives better performance as compared to simple fine tuning of the pretrained models. I have came to know about SMOTE and other methods in order to handle the class imbalance issues . But since most of the transformer models expect the inputs as text which are later tokenized by their respective tokenizers I am not able to do any kind of oversampling . If there is a workaround for this thing I would be interested to know about it.</p>
","bert"
"{
  ""id"": 87244,
  ""title"": ""how to run bert's pretrained model word embeddings faster?""
}","how to run bert's pretrained model word embeddings faster?","2020-12-28 10:31:24","87347","5","2371","<nlp><pandas><word-embeddings><bert>","<p>I'm trying to get word embeddings for clinical data using microsoft/pubmedbert.
I have 3.6 million text rows. Converting texts to vectors for 10k rows takes around 30 minutes. So for 3.6 million rows, it would take around - 180 hours(8days approx).</p>
<blockquote>
<p>Is there any method where I can speed up the process?</p>
</blockquote>
<p>My code -</p>
<pre><code>from transformers import AutoTokenizer
from transformers import pipeline
model_name = &quot;microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext&quot;
tokenizer = AutoTokenizer.from_pretrained(model_name)
classifier = pipeline('feature-extraction',model=model_name, tokenizer=tokenizer)

def lambda_func(row):
    tokens = tokenizer(row['notetext'])
    if len(tokens['input_ids'])&gt;512:
        tokens = re.split(r'\b', row['notetext'])
        tokens= [t for t in tokens if len(t) &gt; 0 ]
        row['notetext'] = ''.join(tokens[:512])
    row['vectors'] = classifier(row['notetext'])[0][0]        
    return row

def process(progress_notes):     
    progress_notes = progress_notes.apply(lambda_func, axis=1)
    return progress_notes

progress_notes = process(progress_notes)
vectors_2d = np.reshape(progress_notes['vectors'].to_list(), (vectors_length, vectors_breadth))
vectors_df = pd.DataFrame(vectors_2d)
</code></pre>
<p>My progress_notes dataframe looks like -</p>
<pre><code>progress_notes = pd.DataFrame({'id':[1,2,3],'progressnotetype':['Nursing Note', 'Nursing Note', 'Administration Note'], 'notetext': ['Patient\'s skin is grossly intact with exception of skin tear to r inner elbow and r lateral lower leg','Patient with history of Afib with RVR. Patient is incontinent of bowel and bladder.','Give 2 tablet by mouth every 4 hours as needed for Mild to moderate Pain Not to exceed 3 grams in 24 hours']})
</code></pre>
<p>Note - 1) I'm running the code on aws ec2 instance r5.8x large(32 CPUs) - I tried using multiprocessing but the code goes into a deadlock because bert takes all my cpu cores.</p>
","bert"
"{
  ""id"": 87186,
  ""title"": ""BERT :dropout(): argument 'input' (position 1) must be Tensor, not str""
}","BERT :dropout(): argument 'input' (position 1) must be Tensor, not str","2020-12-27 01:41:45","93498","0","4077","<python><deep-learning><nlp><pytorch><bert>","<p>I am new to NLP and would like to build a BERT model for sentiment analysis so I am following <a href=""https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/"" rel=""nofollow noreferrer"">this</a> tutorial.</p>
<p>However, I am getting the error below:</p>
<pre><code>F.softmax(model(input_ids, attention_mask), dim = 1)
</code></pre>
<p>When I would like to execute this cell I get the error:</p>
<pre><code> dropout(): argument 'input' (position 1) must be Tensor, not str
</code></pre>
","bert"
"{
  ""id"": 87058,
  ""title"": ""Which script can be used to finetune BERT for SQuAD question answering in Hugging Face library?""
}","Which script can be used to finetune BERT for SQuAD question answering in Hugging Face library?","2020-12-23 12:27:24","87070","0","205","<bert><huggingface><question-answering>","<p>I have gone through lot of blogs which talk about <code>run_squad.py</code> script from Hugging Face, but I could not find it in the latest repo. So, which script has to be used now for fine tuning?</p>
","bert"
"{
  ""id"": 86572,
  ""title"": ""BERT uses WordPiece, RoBERTa uses BPE""
}","BERT uses WordPiece, RoBERTa uses BPE","2020-12-11 19:10:22","86573","0","2335","<bert><transfer-learning><transformer><language-model><tokenization>","<p>In the original <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer""><strong>BERT</strong></a> paper, section <em>'A.2 Pre-training Procedure'</em>, it is mentioned:</p>
<blockquote>
<p>The LM masking is applied after <strong>WordPiece</strong> tokenization with a uniform masking rate of 15%, and no special consideration given to partial word pieces.</p>
</blockquote>
<p>And in the <a href=""https://arxiv.org/pdf/1907.11692.pdf"" rel=""nofollow noreferrer""><strong>RoBERTa</strong></a> paper, section <em>'4.4 Text Encoding'</em> it is mentioned:</p>
<blockquote>
<p>The original BERT implementation (Devlin et al., 2019) uses a
character-level <strong>BPE</strong> vocabulary of size 30K, which is learned after
preprocessing the input with heuristic tokenization rules.</p>
</blockquote>
<p>I appreciate if someone can clarify why in the <a href=""https://arxiv.org/pdf/1907.11692.pdf"" rel=""nofollow noreferrer"">RoBERTa</a> paper it is said that <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">BERT</a> uses BPE?</p>
","bert"
"{
  ""id"": 86548,
  ""title"": ""Trained BERT models perform unpredictably on test set""
}","Trained BERT models perform unpredictably on test set","2020-12-11 10:23:51","86557","4","418","<nlp><bert><transformer>","<p>We are training a BERT model (using the Huggingface library) for a sequence labeling task with six labels: five labels indicate that a token belongs to a class that is interesting to us, and one label indicates that the token does not belong to any class.</p>
<p>Generally speaking, this works well: loss decreases with each epoch, and we get good enough results. However, if we compute precision, recall and f-score after each epoch on a test set, we see that they oscillate quite a bit. We train for 1,000 epochs. After 100 epochs performance seems to have plateaued. During the last 900 epochs, precision jumps constantly to seemingly random values between 0.677 and 0.709; recall between 0.729 and 0.798. The model does not seem to stabilize.
To mitigate the problem, we already tried the following:</p>
<ul>
<li>We increase the size of our test data set.</li>
<li>We experimented with different learning rates and batch sizes.</li>
<li>We used different transformer models from the Huggingface library, e.g. RoBERTa, GPT-2 etc.
Nothing of this has helped.</li>
</ul>
<p>Does anyone have any recommendations on what we could do here? How can we pick the “best model”? Currently, we pick the one that performs best on the test set, but we are unsure about this approach.</p>
","bert"
"{
  ""id"": 86526,
  ""title"": ""BERT for classification model degenerates into all-positive predictions""
}","BERT for classification model degenerates into all-positive predictions","2020-12-10 18:46:03","","0","168","<nlp><training><bert><huggingface>","<p>As a learning project, I'm training a BERT model with the CoLA dataset to detect sentence acceptability. Unfortunately my model is learning to classify every instance as &quot;acceptable&quot;, and I'm not sure what is going wrong with my code. Can anyone provide any help or insight into why this happens?</p>
<h3>Technical details</h3>
<p>I'm using hugging face's <code>transformers</code> library with PyTorch.</p>
<p>The (stripped version of the) code is as follows. Instrumentation and other details have been left out so the code is more readable.</p>
<pre class=""lang-py prettyprint-override""><code>tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-cased')

model = transformers.AutoModelForSequenceClassification.from_pretrained(
    'bert-base-cased',
    num_labels=2,
)

optimizer = transformers.AdamW(
    model.parameters(),
    lr=2e-5,
    eps=1e-8,
)

# The next lines read the CoLA dataset and split it for training and validation
training_dataloader = ...
validation_dataloader = ...

for epoch in range(4):
    train_loss = 0

    for batch in tqdm(train_dataloader):
        model.train()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        model.zero_grad()

        model_output = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            return_dict=True,
        )

        batch_loss = model_output.loss.sum()

        train_loss += batch_loss.item()

        batch_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        # HERE: Measure performance after learning from each batch here with validation_dataloader
</code></pre>
<p>I've checked and the code that reads the dataset seems correct, so it seems that the problem lies either in measuring the performance of the model or in the learning phase.</p>
<p>To measure the performance, I'm running the model over the validation split and then converting the logits into actual classifications as follows:</p>
<pre class=""lang-py prettyprint-override""><code>tp = fn = fp = tn = 0

for batch in validation_dataloader:
    input_ids = batch['input_ids'].to(device)
    attention_mask = batch['attention_mask'].to(device)
    labels = batch['label'].to(device)

    model(
        input_ids=input_ids,
        attention_mask=attention_mask,
        labels=labels,
        return_dict=True,
    )

    expected = batch['label']
    predictions = output.logits[:,1] &gt; output.logits[:,0] # Is this correct?

    tp += sum(1 for exp, pred in zip(expected, predictions) if     exp and     pred)
    fn += sum(1 for exp, pred in zip(expected, predictions) if     exp and not pred)
    fp += sum(1 for exp, pred in zip(expected, predictions) if not exp and     pred)
    tn += sum(1 for exp, pred in zip(expected, predictions) if not exp and not pred)
</code></pre>
<p>Is the line <code>predictions = ...</code> correct?</p>
","bert"
"{
  ""id"": 86104,
  ""title"": ""What is the difference between BERT architecture and vanilla Transformer architecture""
}","What is the difference between BERT architecture and vanilla Transformer architecture","2020-11-30 03:34:44","86108","3","2386","<nlp><bert><transformer><encoder>","<p>I'm doing some research for the summarization task and found out BERT is derived from the Transformer model. In every blog about BERT that I have read, they focus on explaining what is a bidirectional encoder, So, I think this is what made BERT different from the vanilla Transformer model. But as far as I know, the Transformer reads the entire sequence of words at once, therefore it is considered bidirectional too. Can someone point out what I'm missing?</p>
","bert"
"{
  ""id"": 85973,
  ""title"": ""BERT minimal batch size""
}","BERT minimal batch size","2020-11-26 10:40:38","86027","1","3042","<bert><hyperparameter>","<p>Is there a minimum batch size for training/re-fining a BERT model on custom data?</p>
<p>Could you name any cases where a mini batch size between 1-8 would make sense?</p>
<p>Would a batch size of 1 make sense at all?</p>
","bert"
"{
  ""id"": 85972,
  ""title"": ""BERT data cleaning""
}","BERT data cleaning","2020-11-26 10:35:07","","0","207","<nlp><preprocessing><bert><transformer>","<p>I am wondering which data cleaning steps should be performed if you want to re-fine a BERT model on custom text data.</p>
<p>Which steps should be performed?</p>
<p>Does it make sense to perform a stemming or lemmatization if it has not been applied to the initial training of the BERT Base/Large model?</p>
","bert"
"{
  ""id"": 85772,
  ""title"": ""Using BERT for the first time, what are the two columns of my test_results.tsv?""
}","Using BERT for the first time, what are the two columns of my test_results.tsv?","2020-11-22 05:10:21","","0","105","<classification><sentiment-analysis><bert>","<p>I followed the steps to feed in both dev, test, train.tsv to the model, trained it, then tried to classify test data, and I only have 1 feature, and the classification is binary, 1 or 0. I assumed my test_results.tsv would have just 1 column, representing the estimated classification. Any advice? Here is a picture of my output.  I thought it would be a column of 1's and 0's showing me its estimated classification.</p>
<p><a href=""https://i.sstatic.net/kvMgN.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/kvMgN.png"" alt=""enter image description here"" /></a></p>
","bert"
"{
  ""id"": 85566,
  ""title"": ""How pre-trained BERT model generates word embeddings for out of vocabulary words?""
}","How pre-trained BERT model generates word embeddings for out of vocabulary words?","2020-11-17 19:34:13","85570","5","12585","<nlp><word-embeddings><bert><oov>","<p>Currently, I am reading <a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>. I want to understand how pre-trained BERT generates word embeddings for out of vocabulary words? Models like <a href=""https://arxiv.org/abs/1802.05365"" rel=""noreferrer"">ELMo</a> process inputs at character-level and can generate word embeddings for out of vocabulary words. Can BERT do something similar?</p>
","bert"
"{
  ""id"": 85510,
  ""title"": ""From where does BERT get the tokens it predicts?""
}","From where does BERT get the tokens it predicts?","2020-11-16 19:00:50","85524","2","475","<nlp><bert><language-model><tokenization>","<p>When BERT is used for masked language modeling, it masks a token and then tries to predict it.</p>
<p>What are the candidate tokens BERT can choose from? Does it just predict an integer (like a regression problem) and then use that token? Or does it do a softmax over all possible word tokens? For the latter, isn't there just an enormous amount of possible tokens? I have a hard time imaging BERT treats it like a classification problem where # classes = # all possible word tokens.</p>
<p>From where does BERT get the token it predicts?</p>
","bert"
"{
  ""id"": 84692,
  ""title"": ""Can I fine-tune the BERT on a dissimilar/unrelated task?""
}","Can I fine-tune the BERT on a dissimilar/unrelated task?","2020-10-30 07:20:30","84695","1","466","<bert><transformer><language-model><tokenization>","<p>In the original BERT paper, section 3 (arXiv:1810.04805) it is mentioned:</p>
<p>&quot;During pre-training, the model is trained on unlabeled data over <strong>different</strong> pre-training tasks.&quot;</p>
<p>I am not sure if I correctly understood the meaning of the word <strong>&quot;different&quot;</strong> here. different means a different <strong>dataset</strong> or a different <strong>prediction task</strong>?</p>
<p>For example if we pre-train the BERT on a &quot;sentence-classification-task&quot; with a big dataset. Then, should I fine-tune it again on the <strong>same</strong> &quot;sentence-classification-task&quot; task on a smaller and task-specific data-set or I can use the trained model for some other tasks such as &quot;sentence-tagging&quot;?</p>
","bert"
"{
  ""id"": 84404,
  ""title"": ""NLP Bert model to to calculate text similarity, same sentence but not close similarity""
}","NLP Bert model to to calculate text similarity, same sentence but not close similarity","2020-10-23 09:23:07","","1","222","<nlp><bert>","<p>Dear expert here:</p>
<p>I have a simple program to calculate text similarity. The program is copied from internet. Initially, I have a list of sentences or stored in db and fetched from db, then</p>
<ol>
<li>I make the list vectorizered and convered into bert model's vector, then using cosine to calculate the similarity.</li>
<li>2nd time, I instantiate a new vectorizer and vectorizer only one sentence and then compare against the first time bert model's vector, the result is different from 1st one.</li>
<li>3rd time, I using original vectorizer and vectorizered same sentence again, I notice the result is still different.</li>
</ol>
<p>Conclusion，one has to vectorizer sentence one by one, not a list. Why???</p>
<pre><code>from scipy import spatial
from sent2vec.vectorizer import Vectorizer

sentences = [
    &quot;This is an awesome book to learn NLP.&quot;,
    &quot;This is an awesome book to learn NLP.&quot;,
    &quot;This is a great book to learn NLP&quot;,
    &quot;This is a great book to learn NLP bert model&quot;,
    &quot;DistilBERT is an amazing NLP model.&quot;,
    &quot;We can interchangeably use embedding, encoding, or vectorizing.&quot;,
    &quot;How about a chinese dinner? this shall be close to 1 or close to 0, I suppose it is close to 1, i.e. the distance is very big&quot;
]

vectorizer = Vectorizer()
vectorizer.bert(sentences)
vectors_bert = vectorizer.vectors

dist_1 = spatial.distance.cosine(vectors_bert[0], vectors_bert[1])
dist_2 = spatial.distance.cosine(vectors_bert[0], vectors_bert[2])
dist_3 = spatial.distance.cosine(vectors_bert[0], vectors_bert[3])
dist_4 = spatial.distance.cosine(vectors_bert[0], vectors_bert[4])
dist_5 = spatial.distance.cosine(vectors_bert[0], vectors_bert[5])
dist_6 = spatial.distance.cosine(vectors_bert[0], vectors_bert[6])
print('dist_1: {0}, dist_2: {1}, dist_3: {2}, dist_4: {3}, dist_5: {4}, dist_6: {5}'.format(dist_1, dist_2, dist_3, dist_4, dist_5, dist_6))
   

test2 = []
test2.append(sentences[0])
vectorizer2 = Vectorizer()
vectorizer2.bert(test2)
vectors_bert2 = vectorizer2.vectors

dist_same_string_different_vectorizer = spatial.distance.cosine(vectors_bert[0], vectors_bert2[0])
print(&quot;dist same string different vectorizer is {0}&quot;.format(dist_same_string_different_vectorizer))

test3 = []
test3.append(sentences[0])
vectorizer.bert(test3)
vectors_bert3 = vectorizer.vectors
print(&quot;length of vector_bert3 is {0}&quot;.format(len(vectors_bert3)))
dist_same_string_same_vectorizer_different_time = spatial.distance.cosine(vectors_bert[0], vectors_bert3[0])
print(&quot;dist same string same vectorizer different time is {0}&quot;.format(dist_same_string_same_vectorizer_different_time))
</code></pre>
<p>/2/result</p>
<pre><code>dist_1: 0.0, dist_2: 0.039356231689453125, dist_3: 0.06737476587295532, dist_4: 0.046457767486572266, dist_5: 0.12459474802017212, dist_6: 0.1990041732788086
dist same string different vectorizer is 0.21568220853805542
length of vector_bert3 is 1
dist same string same vectorizer different time is 0.21568220853805542

Process finished with exit code 0
</code></pre>
","bert"
"{
  ""id"": 84269,
  ""title"": ""How to apply pruning on a BERT model?""
}","How to apply pruning on a BERT model?","2020-10-20 12:20:10","","1","163","<python><tensorflow><bert><huggingface><pruning>","<p>I have trained a BERT model using ktrain (tensorflow wrapper) to recognize emotion on text, it works but it suffers from really slow inference. That makes my model not suitable for a production environment. I have done some research and it seems pruning could help.</p>
<p>Tensorflow provides some options for pruning e.g.  tf.contrib.model_pruning .The problem is that it is not a widely used technique and I can not find a simple enough example that could help me to understand how to use it. Can someone help?</p>
<p><strong>Only answers that include a coding solution will be considered for the bounty.</strong></p>
<p>I provide my working code below for reference.</p>
<pre><code>import pandas as pd
import numpy as np
import preprocessor as p
import emoji
import re
import ktrain
from ktrain import text
from unidecode import unidecode
import nltk

#text preprocessing class
class TextPreprocessing:
    def __init__(self):
        p.set_options(p.OPT.MENTION, p.OPT.URL)
  
    def _punctuation(self,val): 
        val = re.sub(r'[^\w\s]',' ',val)
        val = re.sub('_', ' ',val)
        return val
  
    def _whitespace(self,val):
        return &quot; &quot;.join(val.split())
  
    def _removenumbers(self,val):
        val = re.sub('[0-9]+', '', val)
        return val
  
    def _remove_unicode(self, text):
        text = unidecode(text).encode(&quot;ascii&quot;)
        text = str(text, &quot;ascii&quot;)
        return text  
    
    def _split_to_sentences(self, body_text):
        sentences = re.split(r&quot;(?&lt;!\w\.\w.)(?&lt;![A-Z][a-z]\.)(?&lt;=\.|\?)\s&quot;, body_text)
        return sentences
    
    def _clean_text(self,val):
        val = val.lower()
        val = self._removenumbers(val)
        val = p.clean(val)
        val = ' '.join(self._punctuation(emoji.demojize(val)).split())
        val = self._remove_unicode(val)
        val = self._whitespace(val)
        return val
  
    def text_preprocessor(self, body_text):

        body_text_df = pd.DataFrame({&quot;body_text&quot;: body_text},index=[1])

        sentence_split_df = body_text_df.copy()

        sentence_split_df[&quot;body_text&quot;] = sentence_split_df[&quot;body_text&quot;].apply(
            self._split_to_sentences)

        lst_col = &quot;body_text&quot;
        sentence_split_df = pd.DataFrame(
            {
                col: np.repeat(
                    sentence_split_df[col].values, sentence_split_df[lst_col].str.len(
                    )
                )
                for col in sentence_split_df.columns.drop(lst_col)
            }
        ).assign(**{lst_col: np.concatenate(sentence_split_df[lst_col].values)})[
            sentence_split_df.columns
        ]
        
        body_text_df[&quot;body_text&quot;] = body_text_df[&quot;body_text&quot;].apply(self._clean_text)

        final_df = (
            pd.concat([sentence_split_df, body_text_df])
            .reset_index()
            .drop(columns=[&quot;index&quot;])
        )
        
        return final_df[&quot;body_text&quot;]

#instantiate data preprocessing object
text1 = TextPreprocessing()

#import data
data_train = pd.read_csv('data_train_v5.csv', encoding='utf8', engine='python')
data_test = pd.read_csv('data_test_v5.csv', encoding='utf8', engine='python')

#clean the data
data_train['Text'] = data_train['Text'].apply(text1._clean_text)
data_test['Text'] = data_test['Text'].apply(text1._clean_text)

X_train = data_train.Text.tolist()
X_test = data_test.Text.tolist()

y_train = data_train.Emotion.tolist()
y_test = data_test.Emotion.tolist()

data = data_train.append(data_test, ignore_index=True)

class_names = ['joy','sadness','fear','anger','neutral']

encoding = {
    'joy': 0,
    'sadness': 1,
    'fear': 2,
    'anger': 3,
    'neutral': 4
}

# Integer values for each class
y_train = [encoding[x] for x in y_train]
y_test = [encoding[x] for x in y_test]

trn, val, preproc = text.texts_from_array(x_train=X_train, y_train=y_train,
                                                                       x_test=X_test, y_test=y_test,
                                                                       class_names=class_names,
                                                                       preprocess_mode='distilbert',
                                                                       maxlen=350)

model = text.text_classifier('distilbert', train_data=trn, preproc=preproc)

learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=6)

predictor = ktrain.get_predictor(learner.model, preproc)

#save the model on a file for later use
predictor.save(&quot;models/bert_model&quot;)

message = &quot;This is a happy message&quot;

#cleaning - takes 5ms to run
clean = text1._clean_text(message)

#prediction - takes 325 ms to run
predictor.predict_proba(clean)
<span class=""math-container"">```</span>
</code></pre>
","bert"
"{
  ""id"": 84038,
  ""title"": ""Using pretrained LSTM and Bert Models in CPU Only Environment - How to speed up Predictions?""
}","Using pretrained LSTM and Bert Models in CPU Only Environment - How to speed up Predictions?","2020-10-15 09:42:56","84055","0","1703","<python><tensorflow><nlp><lstm><bert>","<p>I have trained two text classification models using GPU on Azure. The models are the following</p>
<ol>
<li>Bert (ktrain)</li>
<li>Lstm Word2Vec (tensorflow)</li>
</ol>
<p>Exaples of the code can be found here: <a href=""https://github.com/lukasgarbas/nlp-text-emotion"" rel=""nofollow noreferrer"">nlp</a></p>
<p>I saved the models into files (.h5) for later use. The files are big e.g. 27,613kb for the lstm and 1.2 gb for bert.</p>
<p>I loaded the models and in a computer where only CPU is available. They both work fine but the <code>model.predict(text)</code> function is super slow predicting the class of the text e.g. on average 1 tweet sized message per second.</p>
<p>Adding GPU on the computer is not an option. I wonder if there is another way to make it run faster? e.g. train the models in a different way (without compromising accuracy) or save the model in a different file format?</p>
","bert"
"{
  ""id"": 82908,
  ""title"": ""If i use use BERT embeddings for if cosine(sent1,sent2) > 0.9, then is it fair to assume s1 and s2 are similar""
}","If i use use BERT embeddings for if cosine(sent1,sent2) > 0.9, then is it fair to assume s1 and s2 are similar","2020-10-12 13:16:08","","2","996","<nlp><bert><cosine-distance><semantic-similarity>","<p>According to BERT author Jacob Devlin: I'm not sure what these vectors are, since BERT does not generate meaningful sentence vectors. It seems that this is doing average pooling over the word tokens to get a sentence vector, but we never suggested that this will generate meaningful sentence representations. And even if they are decent representations when fed into a DNN trained for a downstream task, it doesn't mean that they will be meaningful in terms of cosine distance. (Since cosine distance is a linear space where all dimensions are weighted equally).</p>
","bert"
"{
  ""id"": 82720,
  ""title"": ""Is it possible to predict sentiment of unlabelled dataset using BERT?""
}","Is it possible to predict sentiment of unlabelled dataset using BERT?","2020-10-08 06:13:25","82731","0","867","<prediction><sentiment-analysis><bert>","<p>I have a large unlabeled dataset and I want to predict sentiment for each document in this dataset. I want to know, is it possible that I can use BERT for sentiment analysis of unlabeled data? I have seen so many tutorials and read the blog posts but I couldn't find one. All shows the use of BERT on datasets that are already labeled such as the IMDB review dataset or Yelp review.</p>
","bert"
"{
  ""id"": 82180,
  ""title"": ""Generative chatbots with BERT pretrained vectors""
}","Generative chatbots with BERT pretrained vectors","2020-09-24 17:07:18","82210","-3","181","<nlp><bert><embeddings>","<p>Most places seem to train generative chatbots with one hot encoded vectors. See <a href=""https://towardsdatascience.com/generative-chatbots-using-the-seq2seq-model-d411c8738ab5"" rel=""nofollow noreferrer"">here</a> for example, and even the official tutorial on <a href=""https://pytorch.org/tutorials/beginner/chatbot_tutorial.html"" rel=""nofollow noreferrer"">pytorch</a>.<br />
But using one hot encoded vectors are undoubtedly the worst performing method. No tutorial seems to provide this using BERT vectors.</p>
<ul>
<li>Why hasn't chatbots been built with BERT vectors?</li>
<li>Are BERT vectors are not meant to be used this way?</li>
</ul>
","bert"
"{
  ""id"": 81792,
  ""title"": ""What is the typical accuracy of masked language models during BERT pretraining?""
}","What is the typical accuracy of masked language models during BERT pretraining?","2020-09-16 09:12:52","","2","1127","<nlp><bert>","<p>I was reading the BERT paper but I didn't find any tables concerning the performance of the masked language models during pretraining. Does anyone know the accuracy of BERT's masked language model?</p>
","bert"
"{
  ""id"": 81681,
  ""title"": ""Bert for QuestionAnswering input exceeds 512""
}","Bert for QuestionAnswering input exceeds 512","2020-09-14 12:59:36","81689","4","1063","<bert><transformer><question-answering><huggingface>","<p>I'm training Bert on question answering (in Spanish) and i have a large context, only the context exceeds 512, the total question + context is 10k, i found that longformer is bert like for long document, but there's no pretrained in spanish so, is there any idea get around bert.</p>
<p>What i tried is:</p>
<pre><code>from transformers import BertConfig
config=BertConfig.from_pretrained(BERT_MODEL_PATH)
config.max_length=4000 
config.max_position_embeddings=4000
config.output_hidden_states=True
model = MyBertModel(config)    

</code></pre>
<p>but still gives me an error mismatch</p>
<blockquote>
<p>RuntimeError: Error(s) in loading state_dict for BertModel:
size mismatch for bert.embeddings.position_embeddings.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([4000, 768]).</p>
</blockquote>
","bert"
"{
  ""id"": 81595,
  ""title"": ""Does BERT has any advantage over GPT3?""
}","Does BERT has any advantage over GPT3?","2020-09-12 04:37:50","","11","11917","<nlp><bert><gpt>","<p>I have read a couple of documents that explain in detail about the greater edge that GPT-3(Generative Pre-trained Transformer-3) has over BERT(Bidirectional Encoder Representation from Transformers). So am curious to know whether BERT scores better than GPT-3 in any particular area of NLP?</p>
<p>It's quite interesting to note that OpenAI's GPT-3 is not open-sourced whereas tech behemoth Google's BERT is open-sourced. I felt OpenAI's stance and the hefty price tag for GPT-3 api is in stark contrast to its mission statement(OpenAI’s mission is to ensure that artificial general intelligence (AGI)—by which we mean highly autonomous systems that outperform humans at most economically valuable work—benefits all of humanity).</p>
<p><a href=""https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/"" rel=""noreferrer"">https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/</a>
<a href=""https://thenextweb.com/neural/2020/07/23/openais-new-gpt-3-language-explained-in-under-3-minutes-syndication/"" rel=""noreferrer"">https://thenextweb.com/neural/2020/07/23/openais-new-gpt-3-language-explained-in-under-3-minutes-syndication/</a>
<a href=""https://medium.com/towards-artificial-intelligence/gpt-3-from-openai-is-here-and-its-a-monster-f0ab164ea2f8"" rel=""noreferrer"">https://medium.com/towards-artificial-intelligence/gpt-3-from-openai-is-here-and-its-a-monster-f0ab164ea2f8</a></p>
","bert"
"{
  ""id"": 81508,
  ""title"": ""Question about BERT embeddings with high cosine similarity""
}","Question about BERT embeddings with high cosine similarity","2020-09-10 15:13:03","85325","2","341","<nlp><bert><transformer><cosine-distance>","<p>Under what circumstances would BERT assign two occurrences of the same word similar embeddings? If those occurrences are contained within similar syntactic relations with their co-occurrents?</p>
","bert"
"{
  ""id"": 81248,
  ""title"": ""Does finetuning BERT involving updating all of the parameters or just the final classification layer?""
}","Does finetuning BERT involving updating all of the parameters or just the final classification layer?","2020-09-04 20:54:25","","2","2013","<nlp><bert><transformer><finetuning><pretraining>","<p>Currently learning and reading about transformer models, I get that during the pretraining stage the BERT model is trained on a large corpus via MLM and NSP.  But during finetuning, for example trying to classify sentiment based on another text, are all of the BERT parameters (110M+ parameters + final classification layer) updated or just only final classification layers?  Couldn't find a concrete answer to this in the resources I've been looking at.</p>
<p>Thank you in advance.</p>
","bert"
"{
  ""id"": 81089,
  ""title"": ""Getting sentence embeddings with sentence_transformers""
}","Getting sentence embeddings with sentence_transformers","2020-09-01 15:23:32","81138","0","3450","<machine-learning><python><deep-learning><nlp><bert>","<p>I have a text column in my data frame which contains paragraph(s) having multiple and variable sentences in each instance/example/row of the dataframe. Then, I created the sentence tokens of that paragraph using <code>sent_tokenizer</code> of nltk and put it into another column.</p>
<p>So my data frame looks like this:</p>
<pre><code>index       text                                              class

0           [&quot;Hello i live in berlin&quot;, 'I'm xxx']                                                          1
1           [&quot;My name is xx&quot;, &quot;I have a cat&quot;, &quot;Love is life&quot;]                                              0
</code></pre>
<p>now when I'm using:</p>
<pre><code>from sentence_transformers import SentenceTransformer
model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')
sentences = df['text']
sentences = sentences.tolist()
embeddings = model.encode(sentences)
</code></pre>
<p>I'm getting:</p>
<pre><code>TypeError: expected string or bytes-like object
</code></pre>
<p>The <strong>encode</strong>  method is not taking a list of list of sentences as an argument.</p>
","bert"
"{
  ""id"": 80965,
  ""title"": ""Loss first decreases and then increases""
}","Loss first decreases and then increases","2020-08-29 09:30:52","","2","216","<nlp><bert><transformer>","<p>I am using pre-trained <code>xlnet-base-cased</code> model and training it further on real vs fake news detection dataset. I noticed a trend in accuracy for first epoch. Accuracy increases till some point (approximately half) of first epoch and then decreases. Loss also first decreases and then increases rapidly (it is not Nan). What can be reason of this trend ?<br />
I noticed same trend when I tried to run it with <code>roberta-base</code>. But any such trend was not noticed when I trained with <code>distilbert</code>, accuracy goes on increasing in this case.</p>
<p>Here is graph for accuracy VS steps:
<a href=""https://i.sstatic.net/bIY8E.jpg"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/bIY8E.jpg"" alt=""enter image description here"" /></a></p>
<p>[EDIT]
File containing running output of the model:
<a href=""https://drive.google.com/file/d/1r5AWftyHTLf5sqtgWnQm_4lqB84UrJex/view?usp=sharing"" rel=""nofollow noreferrer"">https://drive.google.com/file/d/1r5AWftyHTLf5sqtgWnQm_4lqB84UrJex/view?usp=sharing</a></p>
","bert"
"{
  ""id"": 80817,
  ""title"": ""What GPU size do I need to fine tune BERT base cased?""
}","What GPU size do I need to fine tune BERT base cased?","2020-08-26 13:48:40","","6","10777","<machine-learning><nlp><word-embeddings><bert><gpu>","<p>I want to fine tune BERT Multilingual but I'm not aware about the GPU requirements to train BERT Multilingual. I have GTX 1050ti 4GB on my local machine. I want to know what size of GPU is needed and what type of GPU is needed to train BERT. I have access to server resources. Could anyone tell me what size of GPU should I request for on server.</p>
","bert"
"{
  ""id"": 80782,
  ""title"": ""Can we use sentence transformers to embed sentences without labels?""
}","Can we use sentence transformers to embed sentences without labels?","2020-08-25 14:39:45","80784","1","934","<nlp><word-embeddings><bert>","<p>I was trying to use this project :</p>
<p><a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a></p>
<p>for embedding non english sentences, the language is not a human speaking language, its machine language (x86)</p>
<p>but the problem is i cannot find a simple example where it shows how can i embed sentences using a custom dataset without any labels or similarity values of the sentences.</p>
<p>basically i have an array of sentences lists without any labels for sentences or similarity values for them, and i want to embed them into vectors in a way that it preserves the semantic of the sentence the best way possible, so far i have used word2vec and doc2vec using gensim library so i wanted to try this method to see if its any better?</p>
","bert"
"{
  ""id"": 80660,
  ""title"": ""Splitting into multiple heads -- multihead self attention""
}","Splitting into multiple heads -- multihead self attention","2020-08-22 16:19:20","","1","2151","<tensorflow><bert><transformer><attention-mechanism>","<p>So, I have a doubt in <strong>Attention is all you need</strong>:</p>
<p>The implementation of transformers on tensorflow's official documentation <a href=""https://www.tensorflow.org/tutorials/text/transformer#multi-head_attention"" rel=""nofollow noreferrer"">says</a>:</p>
<blockquote>
<p>Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads.</p>
</blockquote>
<p>However, The paper mentions:</p>
<blockquote>
<p>Instead of performing a single attention function with dmodel-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values h times with different, learned
linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional
output values.</p>
</blockquote>
<p>There is no mention of <em>splitting</em> Q,K and V to obtain heads. Instead, the paper says that they are passed through 'h' different dense layers to convert d-model dimensional vectors to 'h' different dk,dk and dv dimensional vectors respectively. So basically the pseudocode, from what I understood, should look something like this:</p>
<pre><code>Q,K &amp; V are d-model dimensional vectors.

for i in range(h):
   Qi = Dense(dk)(Q)
   Ki = Dense(dk)(K)
   Vi = Dense(dv)(V)
   Ai = Attention(Qi, Ki, Vi)

A0, A1, A2, ..., Ah are then concatenated.
</code></pre>
<p>Is this right? or am I missing something here?</p>
","bert"
"{
  ""id"": 80595,
  ""title"": ""How should I use BERT embeddings for clustering (as opposed to fine-tuning BERT model for a supervised task)""
}","How should I use BERT embeddings for clustering (as opposed to fine-tuning BERT model for a supervised task)","2020-08-21 02:00:07","80607","8","7780","<machine-learning><deep-learning><nlp><word-embeddings><bert>","<p>First of all, I want to say that I am asking this question because I am interested in using BERT embeddings as document features to do clustering. I am using Transformers from the Hugging Face library. I was thinking of averaging all of the Word Piece embeddings for each document so that each document has a unique vector. I would then use those vectors for clustering. Please feel free to comment if you think this is not a good idea, or if I am missing something or not understanding something.</p>
<p>The issue that I see with this is that you are only using the first N tokens which is specified by <code>max_length</code> in Hugging Face library. What if the first N tokens are not the best representation for that document? Wouldn't it be better to randomly choose N tokens, or better yet randomly choose N tokens 10 times?</p>
<p>Furthermore, I realize that using the WordPiece tokenizer is a replacement for lemmatization so the standard NLP pre-processing is supposed to be simpler. However, since we are already only using the first N tokens, and if we are not getting rid of stop words then useless stop words will be in the first N tokens. As far as I have seen, in the examples for Hugging Face, no one really does more preprocessing before the tokenization.</p>
<p>[See example below of the tokenized (from Hugging Face), first 64 tokens of a document]</p>
<p>Therefore, I am asking a few questions here  (feel free to answer only one or provide references to papers or resources that I can read):</p>
<ol>
<li>Why are the first N tokens chosen, instead of at random? 1a) is there anything out there that randomly chooses N tokens perhaps multiple times?</li>
<li>Similar to question 1, is there any better way to choose tokens? Perhaps using TF-IDF on the tokens to at least rule out certain useless tokens?</li>
<li>Do people generally use more preprocessing before using the Word Piece tokenizer?</li>
<li>To what extent does the choice of <code>max_length</code> affect performance?</li>
<li>Why is there a limit of 512 max length in Hugging Face library? Why not just use the length of the longest document?</li>
<li>Is it a good idea to average the WordPiece embeddings to get a matrix (if you want to do clustering)?</li>
<li>Is it a good idea to use BERT embeddings to get features for documents that can be clustered in order to find similar groups of documents? Or is there some other way that is better?</li>
</ol>
<p>original:
<code>'Trump tries to smooth things over with GOP insiders. Hollywood, Florida (CNN) Donald Trump\'s new delegate guru told Republican Party insiders at a posh resort here on Thursday that the billionaire front-runner is recalibrating the part &quot;that he\'s been playing&quot; and is ready</code></p>
<p>tokenized:</p>
<pre><code>['[CLS]',
 'trump',
 'tries',
 'to',
 'smooth',
 'things',
 'over',
 'with',
 'go',
 '##p',
 'insider',
 '##s',
 '.',
 'hollywood',
 ',',
 'florida',
 '(',
 'cnn',
 ')',
 'donald',
 'trump',
 &quot;'&quot;,
 's',
 'new',
 'delegate',
 'guru',
 'told',
 'republican',
 'party',
 'insider',
 '##s',
 'at',
 'a',
 'po',
 '##sh',
 'resort',
 'here',
 'on',
 'thursday',
 'that',
 'the',
 'billionaire',
 'front',
 '-',
 'runner',
 'is',
 'rec',
 '##ali',
 '##bra',
 '##ting',
 'the',
 'part',
 '&quot;',
 'that',
 'he',
 &quot;'&quot;,
 's',
 'been',
 'playing',
 '&quot;',
 'and',
 'is',
 'ready',
 '[SEP]']
</code></pre>
","bert"
"{
  ""id"": 80543,
  ""title"": ""NLP SBert (Bert) for answer comparison STS""
}","NLP SBert (Bert) for answer comparison STS","2020-08-20 00:19:19","","2","540","<nlp><bert>","<p>I've been researching a good way to automate short answer evaluation. Essentially a teacher gives a test with some questions like:</p>
<p>Question: why did columbus sail westward to find asia?</p>
<p>Answer: so he could find a new trade route to Asia through the ocean. Three goals of the Spanish in the Americas were the desire to attain great amounts of riches, to establish claims on as much land as possible, and to colonize as much as possible.</p>
<p>With that we have the correct answer and would like to compare that with the students answer and produce a score based on similarity. I know this isn't a reliable replacement for human grading, but for the sake of the example.</p>
<p>I've come across this paper and codebase:
<a href=""https://arxiv.org/pdf/1908.10084.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1908.10084.pdf</a></p>
<p><a href=""https://github.com/UKPLab/sentence-transformers"" rel=""nofollow noreferrer"">https://github.com/UKPLab/sentence-transformers</a></p>
<p>It seems like the ideal method for solving this problem, but most examples are based on scoring/ranking of semantic search. I question whether I'm on the right path, given that I'm just comparing two answers and not a cluster. Anyone with more experience, possibly can provide some guidance?</p>
","bert"
"{
  ""id"": 80370,
  ""title"": ""Using BERT for co-reference resolving, what's the loss function?""
}","Using BERT for co-reference resolving, what's the loss function?","2020-08-16 17:43:04","","0","143","<nlp><bert>","<p>I'm working my way around using BERT for co-reference resolving. I'm following this highly-cited paper BERT for Coreference Resolution: Baselines and Analysis (<a href=""https://arxiv.org/pdf/1908.09091.pdf"" rel=""nofollow noreferrer"">https://arxiv.org/pdf/1908.09091.pdf</a>). I have following questions, the details can't be found easily from the paper, hope you guys help me out.</p>
<p>What’s the input? is it antecedents + parapraph?
What’s the output? clusters &lt;mention, antecedent&gt; ?
More importantly <strong>What’s the loss function?</strong></p>
<p>For comparison, in another highly-cited paper by [Clark .et al] using Reinforcement Learning, it's very clear about what reward function is. <a href=""https://cs.stanford.edu/people/kevclark/resources/clark-manning-emnlp2016-deep.pdf"" rel=""nofollow noreferrer"">https://cs.stanford.edu/people/kevclark/resources/clark-manning-emnlp2016-deep.pdf</a></p>
","bert"
"{
  ""id"": 80090,
  ""title"": ""Are there any objections to using the same (unlabelled) data for pre-training of a BERT-Based model and the downstream task?""
}","Are there any objections to using the same (unlabelled) data for pre-training of a BERT-Based model and the downstream task?","2020-08-11 04:16:09","","3","103","<nlp><bert><pretraining>","<p>I'm looking to train an Electra model using unlabelled data in a specific field. Are there any objections to using the same data for unsupervised learning and then using the same data downstream for the supervised learning task?</p>
","bert"
"{
  ""id"": 79883,
  ""title"": ""Problem of continuous training - Supervised learning""
}","Problem of continuous training - Supervised learning","2020-08-06 10:07:55","","1","241","<nlp><supervised-learning><bert><text-classification><data-augmentation>","<p>I am sure this is a most common problem, but would like to know by experts on how to tackle it. Note that, I mostly deal with textual data (NLP problems).<br />
When a supervised learning model is created, say a text classifier, and it works well on seen data then we deploy the model in production (you can think of a chatbot also).</p>
<p><strong>But in real time, when new type of data comes where the prediction fails, we find that a new word or new pattern is breaking the model. So we go ahead and retrain the model with new encountered data.</strong> This is where the continuous learning problem starts.</p>
<p>Can ML/NLP veterans please suggest some alternates to solve this labor work? Following approaches have been tried and the problems also listed:</p>
<ul>
<li>We simply can't train with new data infinitely. As production systems should be self healing. We cant put the cost of a human admin constantly monitoring the project. Also, it is practically not possible to get huge domain data during model training phase.</li>
<li>Use of advanced embeddings, and SoTA models like BERT.   (Problem: The accuracy of these models is too hard to control)</li>
<li>Synthetic data generation/data augmentatoin.  (Problem : Does not work well in case of NLP problems. Refer: <a href=""https://datascience.stackexchange.com/questions/77916/training-with-less-data"">training-with-less-data</a> )</li>
<li>Unsupervised classification  (Problem: Does not work well on closed domain problems, as most unsupervised models are either statistical which give a fair value of accuracy but not decent , or are trained on public domain data)</li>
<li>Reinforcement learning. (Problem: Real world NLP data is not labeled unlike a self driven car where the feedback is instant)</li>
</ul>
","bert"
"{
  ""id"": 79813,
  ""title"": ""Loading a Model with weights and optimizers without creating an instance in PyTorch""
}","Loading a Model with weights and optimizers without creating an instance in PyTorch","2020-08-05 09:33:06","","0","313","<nlp><pytorch><bert>","<p>I recently downloaded <a href=""https://camembert-model.fr/"" rel=""nofollow noreferrer"">Camembert Model</a> to fine-tune it for my purpose.</p>
<p>Upon unzipping the file the contents are:
<a href=""https://i.sstatic.net/DiKVi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/DiKVi.png"" alt=""enter image description here"" /></a></p>
<p>Upon loading the <code>model.pt</code> file using pytorch:</p>
<pre><code>import torch
model = torch.load(model_saved_at)
</code></pre>
<p>I saw that <code>model</code> was in OrderedDict format containing the following keys:</p>
<pre><code>args
model
optimizer_history
extra_state
last_optimizer_state
</code></pre>
<p>As the name suggests most of them are <code>OrderedKeys</code> themselves with the exception of <code>args</code> which belongs to a class <code>argsparse.Namespace</code>. Using <code>vars()</code> we can see <code>args</code> only contains some hyperparameters and values which are to be passed from the command-line.</p>
<p><code>model[&quot;model&quot;]</code> contains the weights which I want to load and use as my base model.
A small part of it is as shown below:</p>
<pre><code>for ans in model[&quot;model&quot;].keys():
    try:
        print(ans, &quot;\t&quot; ,model[&quot;model&quot;][ans].size())
    except:
        print(ans, type(ans))
</code></pre>
<pre><code>decoder.sentence_encoder.embed_tokens.weight     torch.Size([32005, 768])
decoder.sentence_encoder.embed_positions.weight      torch.Size([514, 768])
decoder.sentence_encoder.layers.0.self_attn.in_proj_weight   torch.Size([2304, 768])
decoder.sentence_encoder.layers.0.self_attn.in_proj_bias     torch.Size([2304])
decoder.sentence_encoder.layers.0.self_attn.out_proj.weight      torch.Size([768, 768])
decoder.sentence_encoder.layers.0.self_attn.out_proj.bias    torch.Size([768])
decoder.sentence_encoder.layers.0.self_attn_layer_norm.weight    torch.Size([768])
decoder.sentence_encoder.layers.0.self_attn_layer_norm.bias      torch.Size([768])
decoder.sentence_encoder.layers.0.fc1.weight     torch.Size([3072, 768])
decoder.sentence_encoder.layers.0.fc1.bias   torch.Size([3072])
decoder.sentence_encoder.layers.0.fc2.weight     torch.Size([768, 3072])
decoder.sentence_encoder.layers.0.fc2.bias   torch.Size([768])
</code></pre>
<p>However, I cannot use <code>load_state_dict()</code> since I have no instance of this class. How am I suppose to load the weights and optimization parameters without creating an instance? I thought of using <code>sentence.bpe.model</code> but they are for tokenization purposes.</p>
","bert"
"{
  ""id"": 79772,
  ""title"": ""Can we use BERT for only word embedding and then use SVM/RNN to do intent classification?""
}","Can we use BERT for only word embedding and then use SVM/RNN to do intent classification?","2020-08-04 13:08:47","","4","8628","<nlp><rnn><svm><word-embeddings><bert>","<p>According to this article, &quot;<a href=""http://www.diva-portal.org/smash/get/diva2:1349006/FULLTEXT01.pdf"" rel=""nofollow noreferrer"">Systems used for intent classification contain the following <strong>two components:
Word embedding, and a classifier</strong>.</a>&quot; This article also evaluated <strong>BERT+SVM</strong> and <strong>Word2Vec+SVM</strong>.</p>
<p>I'm trying to do the opposite, comparing two different <strong>classifiers</strong> (RNN and SVM) using BERT's word embedding.</p>
<p>Most Python codes that I found use BERT for the whole intent classification problem which made me confused. <a href=""https://www.kdnuggets.com/2020/02/intent-recognition-bert-keras-tensorflow.html"" rel=""nofollow noreferrer"">Example</a></p>
<p>I only want to map the words into vectors with BERT and feed the result into a classifier (SVM/RNN).
Does BERT support word embedding and text classification at the same time? Does someone has an explanation? Is what I'm trying to test feasible with Python?</p>
<p><em>I have a dataframe that has two columns: intent and questions. It's a small dataset.</em></p>
<p>Thank you!</p>
","bert"
"{
  ""id"": 78556,
  ""title"": ""For NLP, is GPT-3 better than RoBERTa?""
}","For NLP, is GPT-3 better than RoBERTa?","2020-07-30 17:51:12","78562","0","2626","<nlp><bert><transformer><gpt>","<p>I am learning deep learning and I want to get into NLP. I have done LSTM, and now I am learning about vectorisation and transformers. Can you please tell me, which algorithm is more effective and accurate?</p>
","bert"
"{
  ""id"": 78074,
  ""title"": ""Imbalanced Dataset (Transformers): How to Decide on Class Weights?""
}","Imbalanced Dataset (Transformers): How to Decide on Class Weights?","2020-07-21 11:38:25","","2","6058","<class-imbalance><bert><transfer-learning><imbalance>","<p>I'm using <code>SimpleTranformers</code> to train and evaluate a model.</p>
<p>Since the dataset I am using is severely imbalanced, it is recommended that I assign weights to each label. An example of assigning weights for <code>SimpleTranformers</code> is given <a href=""https://simpletransformers.ai/docs/classification-models/#setting-class-weights"" rel=""nofollow noreferrer"">here</a>.</p>
<p>My question, however, is: How <em><strong>exactly</strong></em> do I choose what's the appropriate weight for each class? Is there a specific methodology, e.g., a formula that uses the ratio of the labels?</p>
<p>Follow-up question: Are the weights used for the same dataset &quot;universal&quot;? I.e., if I use a totally different model, can I use the same weights or should I assign different weights depending on the model.</p>
<p><strong>p.s.1.</strong> If it makes any difference, I'm using <code>roBERTa</code>.</p>
<p><strong>p.s.2.</strong> There is a similar question <a href=""https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras"">here</a>, however, I believe that my question is not a duplicate because a) the attached question is about Keras where my question is about Transformers, and b) I'm also asking about general recommendations of how weight values are decided where the attached question is not.</p>
","bert"
"{
  ""id"": 77257,
  ""title"": ""BERT reasoning capabilities""
}","BERT reasoning capabilities","2020-07-06 18:00:56","","0","73","<bert><transformer>","<p>I'm working on a Twitter classification task and while analyzing the errors I found quite a few strange predictions. I'm searching for a tool (preferably open-source) similar to <a href=""https://towardsdatascience.com/how-does-bert-reason-54feb363211"" rel=""nofollow noreferrer"">https://towardsdatascience.com/how-does-bert-reason-54feb363211</a> that is able to compute the highest positive/negative attribution given to the words (the reason why I'm not able to use the approach presented in the aforementioned article is the price). In this way, I hope that I'll be able to better understand (and possibly correct) these misclassifications. I tried looking at the attention heads but I don't feel that I'm able to fully understand and draw conclusions based on this information.</p>
<p>Any help would be greatly appreciated!</p>
","bert"
"{
  ""id"": 77206,
  ""title"": ""Does BERT pretrain only on masked tokens?""
}","Does BERT pretrain only on masked tokens?","2020-07-06 03:05:28","77281","-1","103","<bert>","<p>I was a bit confused on the details of the Masked Language Model in BERT pretraining. Does the model only predict the masked tokens for the purposes of pretraining or does it predict it for all tokens?</p>
","bert"
"{
  ""id"": 77044,
  ""title"": ""Bert-Transformer : Why Bert transformer uses [CLS] token for classification instead of average over all tokens?""
}","Bert-Transformer : Why Bert transformer uses [CLS] token for classification instead of average over all tokens?","2020-07-02 21:25:51","","8","6323","<machine-learning><deep-learning><tensorflow><bert><transformer>","<p>I am doing experiments on bert architecture and found out that most of the fine-tuning task takes the final hidden layer as text representation and later they pass it to other models for the further downstream task.</p>
<p>Bert's last layer looks like this :</p>
<p><a href=""https://i.sstatic.net/m0jrg.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/m0jrg.png"" alt=""enter image description here"" /></a></p>
<p>Where we take the [CLS] token of each sentence :</p>
<p><a href=""https://i.sstatic.net/1OklZ.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/1OklZ.png"" alt=""enter image description here"" /></a></p>
<p><a href=""https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"" rel=""noreferrer"">Image source</a></p>
<p>I went through many discussion on this <a href=""https://github.com/huggingface/transformers/issues/1950"" rel=""noreferrer"">huggingface issue</a>,  <a href=""https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-its-encoding-output-is-important"">datascience forum question</a>,  <a href=""https://github.com/google-research/bert/issues/196"" rel=""noreferrer"">github issue</a> Most of the data scientist gives this explanation :</p>
<blockquote>
<p>BERT is bidirectional, the [CLS] is encoded including all
representative information of all tokens through the multi-layer
encoding procedure. The representation of [CLS] is individual in
different sentences.</p>
</blockquote>
<p>My question is, Why the author ignored the other information ( each token's vector ) and taking the average, max_pool or other methods to make use of all information rather than using [CLS] token for classification?</p>
<p>How does this [CLS] token help compare to the average of all token vectors?</p>
","bert"
"{
  ""id"": 76913,
  ""title"": ""What are the merges and vocab files used for in BERT-based models?""
}","What are the merges and vocab files used for in BERT-based models?","2020-06-30 20:40:39","76925","1","610","<neural-network><nlp><bert>","<p>The title says it all. I see plenty online about how to initialize RoBERTa with a merges and vocab file, but what is the point of these files? What exactly are they used for?</p>
","bert"
"{
  ""id"": 76872,
  ""title"": ""Next sentence prediction in RoBERTa""
}","Next sentence prediction in RoBERTa","2020-06-29 20:55:34","","2","2865","<nlp><bert><transformer>","<p>I'm trying to wrap my head around the way next sentence prediction works in RoBERTa. Based on their paper, in section 4.2, I understand that in the original BERT they used a pair of text segments which may contain multiple sentences and the task is to predict whether the second segment is the direct successor of the first one. RoBERTa's authors proceed to examine 3 more types of predictions - the first one is basically the same as BERT, only using two sentences insted of two segments, and you still predict whether the second sentence is the direct successor of the first one. But I can't understand what the goal is in the other 2. I will cite their explanation below:</p>
<p>• FULL-SENTENCES: Each input is packed with full sentences sampled contiguously from one or more documents, such that the total length is at most 512 tokens. Inputs may cross document boundaries. When we reach the end of one document, we begin sampling sentences from the next document and add an extra separator token between documents. We remove the NSP loss.</p>
<p>• DOC-SENTENCES: Inputs are constructed similarly to FULL-SENTENCES, except that they
may not cross document boundaries. Inputs sampled near the end of a document may be
shorter than 512 tokens, so we dynamically increase the batch size in these cases to achieve a similar number of total tokens as FULL-SENTENCES. We remove the NSP loss.</p>
<p>So from what I understand in these two training strategies they already sample consecutive sentences, or at least consecutive sentences from neighbouring documents,  and I can't see what they are trying to predict - it can't be whether they're consecutive text blocks, because to me it seems that all of their training examples have already been sampled contiguously, thus making such a task redundant. It would be of enormous help if someone were to shed some light on the issue, thanks in advance!</p>
","bert"
"{
  ""id"": 76527,
  ""title"": ""Overfitting in Huggingface's TFBertForSequenceClassification""
}","Overfitting in Huggingface's TFBertForSequenceClassification","2020-06-23 15:52:59","","1","2869","<overfitting><bert><huggingface>","<p>I'm using Huggingface's TFBertForSequenceClassification for multilabel tweets classification. During training the model archives good accuracy, but the validation accuracy is poor. I've tried to solve the overfitting using some dropout but the performance is still poor. The model is as follows:</p>
<pre><code># Get and configure the BERT model
config = BertConfig.from_pretrained(&quot;bert-base-uncased&quot;, hidden_dropout_prob=0.5, num_labels=13)
bert_model = TFBertForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;, config=config)

optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=0.00015, clipnorm=0.01)
loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.CategoricalAccuracy('accuracy')

bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])
bert_model.summary()
</code></pre>
<p>The summary is as follows:</p>
<p><a href=""https://i.sstatic.net/4CQnP.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/4CQnP.png"" alt=""Bert Model summary"" /></a></p>
<p>When I fit the model, the outcome is:</p>
<pre><code>history = bert_model.fit(train_ds, epochs=30, validation_data = test_ds)
</code></pre>
<p><a href=""https://i.sstatic.net/RZJ5T.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/RZJ5T.png"" alt="""" /></a> <a href=""https://i.sstatic.net/UVH41.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/UVH41.png"" alt=""enter image description here"" /></a></p>
","bert"
"{
  ""id"": 75631,
  ""title"": ""What is syntax V and S standing for nominal subject?""
}","What is syntax V and S standing for nominal subject?","2020-06-07 19:46:03","75638","0","29","<deep-learning><nlp><bert>","<p>I was reading the recent paper <a href=""https://www.aclweb.org/anthology/P19-1580.pdf"" rel=""nofollow noreferrer"">https://www.aclweb.org/anthology/P19-1580.pdf</a> and noticed that in section 5.2, the syntactic relation is studied in terms of the ""direction between two tokens"". In table 1, the result is further shown with direction like <span class=""math-container"">$$v \to s, s \to v$$</span> for nsubj(nominal subject). </p>

<p><a href=""https://i.sstatic.net/zmQLR.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/zmQLR.png"" alt=""enter image description here""></a></p>

<p>However, what does V and S refer to here and where I can find more materials about this ?  </p>
","bert"
"{
  ""id"": 75343,
  ""title"": ""What are the simplest methods for the label noise problem?""
}","What are the simplest methods for the label noise problem?","2020-06-03 02:16:33","88116","0","51","<machine-learning><neural-network><deep-learning><bert><transformer>","<p>If I have enough low quality data from unsupervised methods or rule-based methods.</p>

<p>I read from <a href=""https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise"" rel=""nofollow noreferrer"">https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise</a> ,but these methods are a little complex for me.</p>

<p>In detail, I deal with a multi-label classification task. First I crawl web page such as wiki and use regex-based rule to mark the label. The model input is the wiki title and the model output is the rule-matched labels from wiki content. My task is to predict the labels for the wiki title.</p>

<p>Do you think <strong>removing the wrong data predicted by trained model</strong> is a simple but effective method?</p>
","bert"
"{
  ""id"": 75201,
  ""title"": ""TensorFlow1.15, multi-GPU-1-machine, how to set batch_size?""
}","TensorFlow1.15, multi-GPU-1-machine, how to set batch_size?","2020-06-01 05:23:51","88118","1","632","<deep-learning><tensorflow><bert><transformer>","<p>The input function code:</p>

<pre><code>    def input_fn(params):
        """"""The actual input function.""""""
        batch_size = FLAGS.train_batch_size

        name_to_features = {
            ""input_ids"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""input_mask"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""segment_ids"":
                tf.FixedLenFeature([max_seq_length], tf.int64),
            ""masked_lm_positions"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
            ""masked_lm_ids"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.int64),
            ""masked_lm_weights"":
                tf.FixedLenFeature([max_predictions_per_seq], tf.float32),
            ""next_sentence_labels"":
                tf.FixedLenFeature([1], tf.int64),
        }

        # For training, we want a lot of parallel reading and shuffling.
        # For eval, we want no shuffling and parallel reading doesn't matter.
        if is_training:
            d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))
            d = d.repeat()
            d = d.shuffle(buffer_size=len(input_files))

            # `cycle_length` is the number of parallel files that get read.
            cycle_length = min(num_cpu_threads, len(input_files))

            # `sloppy` mode means that the interleaving is not exact. This adds
            # even more randomness to the training pipeline.
            d = d.apply(
                tf.contrib.data.parallel_interleave(
                    tf.data.TFRecordDataset,
                    sloppy=is_training,
                    cycle_length=cycle_length))
            d = d.shuffle(buffer_size=100)
        else:
            d = tf.data.TFRecordDataset(input_files)
            # Since we evaluate for a fixed number of steps we don't want to encounter
            # out-of-range exceptions.
            d = d.repeat()

        # We must `drop_remainder` on training because the TPU requires fixed
        # size dimensions. For eval, we assume we are evaluating on the CPU or GPU
        # and we *don't* want to drop the remainder, otherwise we wont cover
        # every sample.
        d = d.apply(
            tf.contrib.data.map_and_batch(
                lambda record: _decode_record(record, name_to_features),
                batch_size=batch_size,
                num_parallel_batches=num_cpu_threads,
                drop_remainder=True))
        d = d.prefetch(10)
        return d

</code></pre>

<p>The mirrow strategy code:</p>

<pre><code>    distribution = tf.contrib.distribute.MirroredStrategy(
        devices=[""device:GPU:%d"" % i for i in range(FLAGS.n_gpus)],
        # num_gpus=4,
        cross_tower_ops=tf.distribute.HierarchicalCopyAllReduce())
    run_config = RunConfig(
        train_distribute=distribution,
        # eval_distribute=dist_strategy,
        log_step_count_steps=log_every_n_steps,
        model_dir=FLAGS.output_dir,
        save_checkpoints_steps=FLAGS.save_checkpoints_steps)

    model_fn = model_fn_builder(
        bert_config=bert_config,
        init_checkpoint=FLAGS.init_checkpoint,
        learning_rate=FLAGS.learning_rate,
        num_train_steps=FLAGS.num_train_steps,
        num_warmup_steps=FLAGS.num_warmup_steps,
        use_tpu=FLAGS.use_tpu,
        use_one_hot_embeddings=FLAGS.use_tpu)

    # If TPU is not available, this will fall back to normal Estimator on CPU
    # or GPU.
    estimator = Estimator(
        model_fn=model_fn,
        params={},
        config=run_config)
</code></pre>

<p>The problem is that if I have 4 GPU. Each GPU could run 8 batchsize. I set <code>batch_size = 8</code> not 32. <code>batch_size = 32</code> will OOM.</p>

<p>Am I right? Will the data be distributed to 4 GPU with different batches?</p>
","bert"
"{
  ""id"": 75108,
  ""title"": ""German Chatbot or conversational AI""
}","German Chatbot or conversational AI","2020-05-30 10:55:41","","3","859","<dataset><nlp><bert><transformer>","<p>I want to build a chatbot mostly BERT(Transformer) based in the German Language. But I do not find any German chatbot data set!</p>

<p>So does it make sense to use google translator API to translate the English dataset to German and then train the model on it?</p>

<p>Any idea where I can find German datasets or solve this issue?</p>
","bert"
"{
  ""id"": 74918,
  ""title"": ""How to use fine tuning of BERT when i have unlabelled dataset of text documents?""
}","How to use fine tuning of BERT when i have unlabelled dataset of text documents?","2020-05-27 03:24:06","","2","2904","<deep-learning><nlp><transfer-learning><bert>","<p>I have gained a basic understanding of using BERT for various NLP/text mining tasks. When it comes to fine-tuning of BERT, I always see that fine-tuning is performed using some classification tasks. So, how should I refine the word/sentence embeddings vector given by the BERT model in the case when I have a set completely unlabelled set of documents? I'm aware that the BERT model is originally trained on unlabelled data, so there must be some way.</p>
","bert"
"{
  ""id"": 74598,
  ""title"": ""Using BERT for input embeddings in a seq2seq model""
}","Using BERT for input embeddings in a seq2seq model","2020-05-21 14:27:11","","1","319","<bert>","<p>I'm currently trying to implement a paper that describes using BERT to embed inputs into a seq2seq model. </p>

<p>""For word vectors, we use the deep
contextualized word vectors from ELMo (Peters
et al., 2018) or BERT (Devlin et al., 2018). The
answer tag follows the BIO2
tagging scheme."" <a href=""https://arxiv.org/pdf/1909.06356.pdf"" rel=""nofollow noreferrer"">source</a></p>

<p>The issue I'm running into is the first input into the decoder. Traditionally this is the start token, but BERT's start token is both a sentence representation &amp; not consistent. Depending on what is being embedded the start token representation will change. e.g. embed(""101"") will have a different representation of ""101"" than embed(""101 2000 2004 1102"").</p>

<p>This creates issues for the model during inference as there is no ground truth to get a start token from. Using just the embed(""101"") as the start token causes the model to predict the same thing every time when otherwise it would not. It seems like there is a good number of people who use BERT to embed their inputs but I don't see anyone addressing this particular issue. I have considered the possibility of using BERT exclusivity as the encoder of a model and taking the [cls] token from that embedding as the first input to the decoder. However, this contradicts the paper listed above.  </p>
","bert"
"{
  ""id"": 74115,
  ""title"": ""Is BERT a language model?""
}","Is BERT a language model?","2020-05-13 12:22:22","74119","9","3853","<nlp><bert><transformer><language-model>","<p>Is BERT a language model in the sense of a function that gets a sentence and returns a probability?
I know its main usage is sentence embedding, but can it also provide this functionality?</p>
","bert"
"{
  ""id"": 73761,
  ""title"": ""Implementation of BERT using Tensorflow vs PyTorch""
}","Implementation of BERT using Tensorflow vs PyTorch","2020-05-07 18:57:06","73764","1","3503","<tensorflow><pytorch><bert>","<p><strong>BERT</strong> is an NLP model developed by <em>Google</em>. The original BERT model is built by the TensorFlow team, there is also a version of BERT which is built using PyTorch. What is the main difference between these two models?</p>
","bert"
"{
  ""id"": 73408,
  ""title"": ""What information does output of [SEP] token captures in BERT?""
}","What information does output of [SEP] token captures in BERT?","2020-05-02 12:46:52","73775","2","2605","<nlp><word-embeddings><bert>","<p>After reading around on the web I came to understand that the output representation of the special token [CLS] captures the representation of a sentence (am I correct?).</p>

<p>My primary question is what information does the output embedding of [SEP] token (T_SEP) captures?</p>

<p>My other doubt is if I input a bunch of sentences into BERT separated by [SEP] does the output embedding of [CLS] contain information about all the sentences?</p>
","bert"
"{
  ""id"": 73260,
  ""title"": ""How to convert subword PPL to word level PPL?""
}","How to convert subword PPL to word level PPL?","2020-04-29 20:09:43","","1","26","<nlp><pytorch><bert><language-model>","<p>I'm using this formula to covert subword perpexity to word perplexity:
<code>PPL_word = exp(log(PPL_subword) * num_subwords / num_words)</code>
The question is do I need to include the [SEP] and [CLS] tokens when counting subwords?</p>
","bert"
"{
  ""id"": 73189,
  ""title"": ""Does BERT use GLoVE?""
}","Does BERT use GLoVE?","2020-04-28 21:23:47","73276","7","5281","<nlp><bert><transformer><attention-mechanism>","<p>From all the docs I read, people push this way and that way on how BERT uses or generates embedding. I GET that there is a key and a query and a value and those are all generated.</p>

<p>What I don't know is if the original embedding - the original thing you put into BERT - could or should be a vector. People wax poetic about how BERT or ALBERT can't be used for word to word comparisons, but nobody says explicitly what bert is consuming. Is it a vector? If so is it just a one-hot vector? Why is it not a GLoVE vector? (ignore the positional encoding discussion for now please)</p>
","bert"
"{
  ""id"": 73151,
  ""title"": ""How to identify topic transition in consecutive sentences using Python?""
}","How to identify topic transition in consecutive sentences using Python?","2020-04-28 12:42:59","73161","0","380","<python><data-mining><similarity><bert><topic-model>","<p>I'm new to data mining. I want to detect topic transition among consecutive sentences. For instance, I have a paragraph (this could be a collection of dozens of sentences, sometimes without transitional words) as follows:</p>
<blockquote>
<p>As I really like Mickey Mouse, I was hopping to go to Florida. But my
dad took me to Nevada. Obviously, Mickey Mouse was not there. But, I
attended a camp with other children. And, I really enjoyed and learnt a lot from my
camp.</p>
</blockquote>
<p>Here, I want to automatically split this into following sub-paraphs:</p>
<blockquote>
<ol>
<li><p>As I really like Mickey Mouse, I was hopping to go to Florida. But my
dad took me to Nevada. Obviously, Mickey Mouse was not there.</p>
</li>
<li><p>But, I attended a camp with other children. And, I really enjoyed and learnt a lot from my
camp.</p>
</li>
</ol>
</blockquote>
<p>As far as I know, this is not the sentence similarity measurement. What technique should be used here? Any example using python or tensorflow models would be greatly appreciated.</p>
","bert"
"{
  ""id"": 72565,
  ""title"": ""System Requirement to train BERT model""
}","System Requirement to train BERT model","2020-04-19 05:51:39","","1","923","<nlp><recommender-system><bert>","<ol>
<li><p>How much Hardware is required to train it well?(My current PC specs: 8GB RAM, i5 2 core Processor, Standard GPU (No work going on GPU))</p></li>
<li><p>I have a dataset of approx 1lakh records.Is it is necessary to train BERT model on such large records or it will give better results on less records as well?</p></li>
<li><p>Is BERT capable to classify unidentified text which is not present in train data but in test data?</p></li>
</ol>
","bert"
"{
  ""id"": 72533,
  ""title"": ""Training PCA on BERT word embedding: entire training dataset or each document?""
}","Training PCA on BERT word embedding: entire training dataset or each document?","2020-04-18 10:30:40","","1","607","<python><scikit-learn><pca><bert>","<p>I want to reduce the dimensionality of the BERT word embedding to, let's say, 50 dimensions. I am trying with PCA. I will use that for the document classification task. </p>

<p>Now for training  PCA, should I train on the entire dataset by using all the word vectors from the entire data set at once that is:</p>

<pre><code>pca.fit_transform([all_the_word_vectors_of_the_dataset])
</code></pre>

<p>or word vectors per document, that is:</p>

<pre><code>for document in train_dataset:
    pca.fit_transform([word_vectors_of_current_document])
</code></pre>
","bert"
"{
  ""id"": 72420,
  ""title"": ""Can BERT be used for predicting words?""
}","Can BERT be used for predicting words?","2020-04-16 09:03:52","72449","1","900","<neural-network><deep-learning><bert><transformer><attention-mechanism>","<p>I have a question regarding the pre-training section (in particular, the Masked Language Model).</p>

<p>In the example <em>Let's stick to improvisation in this skit</em>, by masking the word <em>improvisation</em>, after applying BERT (followed by the FFNN and Softmax), by looking at the probabilities of each of the possible English words,  we are able to correctly predict that the masked word was <em>improvisation</em>.</p>

<p>Is it possible to actually play with this by using my own examples? I was wondering if I can input a sentence in a different language (from the multilingual model) and have a sorted list of the most probable words that were masked in the original sentence. If it's possible, what needs to be tweaked?</p>

<p>Any help would be greatly appreciated.</p>
","bert"
"{
  ""id"": 71952,
  ""title"": ""BERT classifier with Ktrain API is unable to predict new data""
}","BERT classifier with Ktrain API is unable to predict new data","2020-04-08 11:32:29","","1","718","<deep-learning><keras><tensorflow><nlp><bert>","<p>I have trained a classifier for sentiment analysis using BERT architecture.
I am able to train the classifier and I am getting a validation accuracy of 87%. But whenever I feed in test data, or some simple sentence like ""What an amazing movie"", ""I would love that book"", etc, the model predicts the class for one and says list index out of range for other.
I tried to find if there is a bug in my code as well.</p>

<pre><code>(x_train, y_train), (x_test, y_test), preproc = text.texts_from_csv(TWITTER,
                                                                       preprocess_mode='bert',                                                                    
                                                                       text_column = 'text', label_columns = ['target'])

model = text.text_classifier('bert', (x_train, y_train), preproc=preproc)
learner = ktrain.get_learner(model,train_data=(x_train, y_train), val_data=(x_test, y_test), batch_size=6)

learner.fit_onecycle(2e-5, 1)

predictor = ktrain.get_predictor(learner.model,preproc)

predictor.predict(['I am very happy to meet you!'])

</code></pre>

<p>Error that I am getting</p>

<pre><code>IndexError                                Traceback (most recent call last)
&lt;ipython-input-28-82b8da552189&gt; in &lt;module&gt;()
----&gt; 1 predictor.predict(['I am very happy to meet you!'])

1 frames
/usr/local/lib/python3.6/dist-packages/ktrain/text/predictor.py in &lt;listcomp&gt;(.0)
     56                 preds = np.squeeze(preds)
     57                 if len(preds.shape) == 0: preds = np.expand_dims(preds, -1)
---&gt; 58         result =  preds if return_proba or multilabel or not self.c else [self.c[np.argmax(pred)] for pred in preds]
     59         if multilabel and not return_proba:
     60             result =  [list(zip(self.c, r)) for r in result]

IndexError: list index out of range
</code></pre>

<p>My problem is that if I have any bugs in my code, then I should get this error every time I execute the predict method. Say I have 100 new test points, I am getting this error only for 30 to 40 test points and the remaining are classified properly. I tested this theory by feeding in tweets one tweet at a time. But I do not understand why is this happening.</p>
","bert"
"{
  ""id"": 71777,
  ""title"": ""How can I tokenize a text file with BERT or something similar?""
}","How can I tokenize a text file with BERT or something similar?","2020-04-05 14:48:44","","1","229","<python><nlp><nltk><bert>","<p>I want to use the <a href=""https://www.dropbox.com/s/7ewzdrbelpmrnxu/rumdetect2017.zip?dl=0&amp;file_subpath=%2Frumor_detection_acl2017"" rel=""nofollow noreferrer"">twitter datasets</a> in a project and the tweet contents look something like this:</p>

<pre><code>tweet_ID         tweet_text

12324124         some text here that has been twitted bla bla bla
35325323         some other text, trump, usa , merica ,etc.
56743563         bla bla text whatever tweet bla bla
</code></pre>

<p>Now I would like to end-up with a file that contains tweet_IDs and some vector encodings. I was reading about BERT, ROBERTA, etc. Is there a way to simply generate these encodings without writing a huge amount of boilerplate code? </p>
","bert"
"{
  ""id"": 70253,
  ""title"": ""Difference between using BERT as a 'feature extractor' and fine tuning BERT with its layers fixed""
}","Difference between using BERT as a 'feature extractor' and fine tuning BERT with its layers fixed","2020-03-26 10:11:35","","1","2595","<deep-learning><nlp><bert><finetuning>","<p>I understand that there are two ways of leveraging BERT for some NLP classification task: </p>

<ol>
<li>BERT might perform ‘feature extraction’ and its output is input further to another (classification) model </li>
<li>The other way is fine-tuning BERT on some text classification task by adding an output layer or layers to pretrained BERT and retraining the whole (with varying number of BERT layers fixed </li>
</ol>

<p>However, if in the second case, we fix all the layers and add ALL the layers from the classification model will be added, 1st and 2nd approaches are effectively the same, am I right? </p>
","bert"
"{
  ""id"": 69749,
  ""title"": ""Remove subwords from BERT output""
}","Remove subwords from BERT output","2020-03-15 19:44:58","","2","275","<python><keras><nlp><lstm><bert>","<p>I'm trying to build a multilingual WSD system with BERT on top as the embedding layer.
In order to have better performances, after BERT finishes its job (and performs Transfer Learning), I need to remove the subwords from its output. Is there a way to do so? <br>
I've tried to detach the model from the network's architecture, doing something like this... but I need to do this as a custom layer and I'm not 100% sure that this is even right</p>

<pre><code>class Bert:
    def __init__(self):
        input_word_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=""input_word_ids"")
        input_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=""input_mask"")
        segment_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=""segment_ids"")
        print(""dopwnloading BERT..."")
        bert = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1"", trainable=False, name=""BERT"")
        print(""BERT downloaded"")
        pooled_output, sequence_output = bert([input_word_ids, input_mask, segment_ids])
        self.model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])
        self.model.summary()


    def predict(self, input_word_ids, input_mask, segment_ids, positional_ids,needed_padding, train_mode:bool = False):
        print(""Starting BERT prediction..."")
        pool_embs, all_embs = self.model.predict(
            {'input_word_ids': input_word_ids, 'input_mask': input_mask, 'segment_ids': segment_ids},
            verbose=1,
            batch_size=64
        )
        del pool_embs
        to_return = []
        print(""Conversion\nSoftware version 2.0..."")
        for i in tqdm(range(len(positional_ids))):
            indexes_to_extrapolate = np.concatenate((positional_ids[i],needed_padding[i]))
            indexes_to_extrapolate = indexes_to_extrapolate[:63] if len(indexes_to_extrapolate) &gt; 64 else indexes_to_extrapolate
            new_version = tf.gather(all_embs[i], tf.constant(indexes_to_extrapolate))
            if train_mode and new_version.shape[0] &lt; 64:
                #Means that, originally, there has to be a padding!
                #And, if there is, it can surely be found in the first position of the needed_padding!
                how_much_iteration = 64 - new_version.shape[0]
                if how_much_iteration &gt; 0:
                    for iteratore in range(how_much_iteration):
                        tmp_padding_for_iteration = needed_padding[i][0]
                        new_version = tf.concat([new_version, tf.constant(all_embs[i][tmp_padding_for_iteration], shape=(1,768))], 0)
            with open(""registro_shape.txt"",""a"") as registro:
                registro.write(""Shape --&gt; "" +str(new_version.shape)+""\n"")
            if new_version.shape[0] &gt; 64:
                print(""wth"")
            to_return.append(new_version)
        return tf.stack(to_return)
</code></pre>

<p><strong>EDIT</strong>: I'll try to contextualize the case with more information regarding the architecture of the network.
In particular, this is the architecture of the network that I'm trying to build for the WSD task. Note that the network should perform a multitask learning task:</p>

<ol>
<li>Bert</li>
<li>BiLSTM</li>
<li>Attention Layer</li>
<li>3 outputs layer</li>
</ol>

<p>self.tokenizatore = FullTokenizer(bert_path,do_lower_case=False)</p>

<p>input_word_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32,name=""input_word_ids"")</p>

<p>input_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32,name=""input_mask"")</p>

<p>segment_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32,name=""segment_ids"")</p>

<pre><code>print(""dopwnloading BERT..."")
bert = hub.KerasLayer(""https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/1"", trainable=False)
print(""BERT downloaded"")
pooled_output, sequence_output = bert([input_word_ids, input_mask, segment_ids])
LSTM = tf.keras.layers.Bidirectional(
    tf.keras.layers.LSTM(
        units=hidden_size,
        dropout=dropout,
        recurrent_dropout=recurrent_dropout,
        return_sequences=True,
        return_state=True
    )
)(sequence_output)
LSTM = self.produce_attention_layer(LSTM)
LSTM = tf.keras.layers.Dropout(0.5)(LSTM)

babelnet_output = tf.keras.layers.Dense(outputs_size[0], activation=""softmax"", name=""babelnet"")(LSTM)
domain_output = tf.keras.layers.Dense(outputs_size[1], activation=""softmax"", name=""domain"")(LSTM)
lexicon_output = tf.keras.layers.Dense(outputs_size[2], activation=""softmax"", name=""lexicon"")(LSTM)



def produce_attention_layer(self, LSTM):
    """"""
    Produces an Attention Layer like the one mentioned in the Raganato et al. Neural Sequence Learning Models for Word Sense Disambiguation,
    chapter 3.2
    :param lstm: The LSTM that will be used in the task
    :return: The LSTM that was previously given in input with the enhancement of the Attention Layer
    """"""
    hidden_states = tf.keras.layers.Concatenate()([LSTM[1],LSTM[3]])
    ripetitore = tf.keras.layers.RepeatVector(tf.keras.backend.shape(LSTM[0])[1])(hidden_states)
    u = tf.keras.layers.Dense(1, activation=""tanh"")(ripetitore)
    attivazione = tf.keras.layers.Activation('softmax')(u)  # We are using a custom softmax(axis = 1) loaded in this notebook
    dotor = tf.keras.layers.Multiply()([LSTM[0],attivazione])

    return dotor
</code></pre>
","bert"
"{
  ""id"": 69737,
  ""title"": ""Fastest way for 1 vs all lookup on embeddings""
}","Fastest way for 1 vs all lookup on embeddings","2020-03-15 15:29:51","69786","2","2822","<machine-learning><bert><embeddings><cosine-distance><similar-documents>","<p>I have a dataset with about 1 000 000 texts where I have computed their sentence embeddings with a language model and stored them in a numpy array. </p>

<p>I wish to compare a new unseen text to all the 1 000 000 pre-computed embeddings and perform cosine similarity to retrieve the most semantic similar document in the corpus. </p>

<p>What is the most efficient to perform this 1-vs-all comparison?</p>

<p>I would thankful for any pointers and feedback! </p>
","bert"
"{
  ""id"": 69640,
  ""title"": ""What should be the labels for subword tokens in BERT for NER task?""
}","What should be the labels for subword tokens in BERT for NER task?","2020-03-13 13:32:57","75225","9","3346","<bert><named-entity-recognition><labels>","<p>For any NER task, we need a sequence of words and their corresponding labels.
To extract features for these words from BERT, they need to be tokenized into subwords.</p>
<p>For example, the word <code>'infrequent'</code> (with label B-count) will be tokenized into <code>['in', '##fr', '##e', '##quent']</code>. How will its label be represented?</p>
<p>According to the BERT <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""nofollow noreferrer"">paper</a>:</p>
<blockquote>
<p>We use the representation of the first sub-token as the input to the token-level classifier over the NER label set.</p>
</blockquote>
<p>So I assume, for the subwords <code>['in', '##fr', '##e', '##quent']</code> , the label for the first subword will either be this <code>['B-count', 'B-count', 'B-count', 'B-count']</code> where we propagate the word label to all the subwords. Or should it be <code>['B-count', 'X', 'X', 'X']</code> where we leave the original label on the first token of the word, then use the label “X” for subwords of that word.</p>
<p>Any help will be appreciated.</p>
","bert"
"{
  ""id"": 69209,
  ""title"": ""Generating synonyms or similar words from multiples word embeddings""
}","Generating synonyms or similar words from multiples word embeddings","2020-03-05 12:14:52","","5","11452","<nlp><bert>","<p>I am looking for a way to generate synonyms, using word embeddings. From one word, and from multiple words. Such as the two example below:</p>

<p>""word"" -> Word embedding -> generate synonym of ""word""</p>

<p>""word"", ""synonym of word .. ""->  -> Word embeddings -> generate a synonym of both word</p>

<p>I am very new at this. What do you think I should use ?</p>

<p>I also want to use a tools that, in further work, would take into account context for word embedding generation, such as:</p>

<p>""sentence in including a word"" -> Word embedding of word -> generate synonym of ""word"" in that context</p>

<p>I think I will start to do it with BERT... How should I start ? or which alternative should I use ?</p>

<p>Thanks for your help !</p>
","bert"
"{
  ""id"": 69044,
  ""title"": ""How to generate a sentence with exactly N words?""
}","How to generate a sentence with exactly N words?","2020-03-03 09:30:27","","3","161","<nlp><bert><ai><text-generation><gpt>","<p>Thanks to GPT2 pretrained model now it is possible to generate meaningful sequence of words with or without prefix. However a sentence should end with a proper endings (.,!,?). I am just wondering how to generate a sentence (with proper ending) of length N? </p>

<p>One possible approach is post-processing, that is process many sequences and choose the ones the serve the purpose! However, it could be a really daunting task to use in any pipeline. </p>

<p>Is there any suggestion, perhaps a secondary algorithm, to tune the hyper-parameter such that it produces sentence of desired length with higher probabilities. </p>
","bert"
"{
  ""id"": 68822,
  ""title"": ""BERT in production""
}","BERT in production","2020-02-27 17:16:12","","3","564","<apache-spark><apache-hadoop><bert>","<p>I've created a BERT model. What are the ways to do the deployment of this model? Is it possible to use it with Spark, Hadoop or Docker?</p>
","bert"
"{
  ""id"": 68594,
  ""title"": ""Can I fine-tune BERT, ELMO or XLnet for Seq2Seq neural machine translation?""
}","Can I fine-tune BERT, ELMO or XLnet for Seq2Seq neural machine translation?","2020-02-24 08:40:38","","2","2365","<machine-learning><deep-learning><bert><sequence-to-sequence><machine-translation>","<p>I'm working on neural machine translator that translates English sentences to American sign language sentences(e.g below). I've a quite small dataset - around 1000 sentence pairs. I'm wondering if it is possible to fine-tune BERT, ELMO or XLnet for Seq2seq encoder/decoder machine translation.</p>

<p>English: He sells food.</p>

<p>American sign language: Food he sells</p>
","bert"
"{
  ""id"": 68496,
  ""title"": ""Is it possible feed BERT to seq2seq encoder/decoder NMT (for low resource language)?""
}","Is it possible feed BERT to seq2seq encoder/decoder NMT (for low resource language)?","2020-02-22 01:54:34","","0","1876","<machine-learning><deep-learning><bert><sequence-to-sequence><machine-translation>","<p>I'm working on NMT model which the input and the target sentences are from the same language (but the grammar differs). I'm planning to pre-train and use BERT since I'm working on small dataset and low/under resource language. so is it possible to feed BERT to the seq2Seq encoder/decoder? </p>
","bert"
"{
  ""id"": 68373,
  ""title"": ""BertPunc (punctuation restoration with BERT)""
}","BertPunc (punctuation restoration with BERT)","2020-02-19 23:06:57","","5","1375","<nlp><bert>","<p>I've found the <a href=""https://github.com/nkrnrnk/BertPunc"" rel=""nofollow noreferrer"">script</a> for punctuation restoration. And I have one question about this method.</p>
<p>I will briefly explain the logic of the author. One of four tokens is assigned for each word: Other (0), PERIOD (1), COMMA (2), QUESTION (3). Further, all words are converted to BERT tokens. Here is an example:</p>
<pre><code>  2045  0
  2003  0
  2200  0
  2210  0
  3983  0
  2301  0
  2974  0
  1999  0
  2068  2
</code></pre>
<p>Next, we do padding. We set a segment (e.g. eight words) and for each word we take two words before a token and four words after a token. Also, we add a padding token after each word. For the very first word, there are no words before. Therefore, a word are taken from the end. Similarly, for the last word, there are no words after and therefore a word are taken from the beginning.
Here is an example of it.</p>
<pre><code>[1999, 2068, 2045, 0, 2003, 2200, 2210, 3983] 0
[2068, 2045, 2003, 0, 2200, 2210, 3983, 2301] 0
[2045, 2003, 2200, 0, 2210, 3983, 2301, 2974] 0
[2003, 2200, 2210, 0, 3983, 2301, 2974, 1999] 0
[2200, 2210, 3983, 0, 2301, 2974, 1999, 2068] 0
[2210, 3983, 2301, 0, 2974, 1999, 2068, 2045] 0
[3983, 2301, 2974, 0, 1999, 2068, 2045, 2003] 0
[2301, 2974, 1999, 0, 2068, 2045, 2003, 2200] 0
[2974, 1999, 2068, 0, 2045, 2003, 2200, 2210] 2
</code></pre>
<p>The first column contains tokens, and the second column contains punctuation symbols. In first column, &quot;0&quot; corresponds to a padding. Next we do TensorDataset, and than DataLoader. In the second column, '0' corresponds to &quot;other&quot;, and '2' corresponds to a &quot;period&quot;. Finaly we train a model:</p>
<pre><code>for inputs, labels in data_loader_train:
       inputs, labels = inputs.cuda(), labels.cuda()
       output = model(inputs)
</code></pre>
<p>The algorithm works well, but I do not understand the following.
What's the point of putting padding in the middle? Maybe we can do punctuation restoration with BERT in a more simple way?</p>
","bert"
"{
  ""id"": 68264,
  ""title"": ""Multimodal end-to-end deep learning""
}","Multimodal end-to-end deep learning","2020-02-18 02:48:21","","1","37","<cnn><image-classification><text><bert>","<p>I'm thinking of working on a project that involves multiple models of data and wanted to share my thoughts to get some feedback. Think of problem of sentiment classification where the input contains both text and input (maybe Amazon reviews where some users share images of the products). I was thinking of designing a classifier in this way:</p>

<ol>
<li>Assuming each review is a datapoint, we represent that datapoint with a tensor that contains the concatenated values representing the image and the text preprocessed appropriately.</li>
<li>We then input this data into a deep learning model where the tensor is correctly separated and the image part is fed into a CNN and the text part is fed to a pre-trained BERT model.</li>
<li>The outputs of both the CNN and BERT are then concatenated and fed to a linear classifier with a couple of layers.</li>
<li>The output of this linear classifier is the prediction of the model from which we derive the loss and the entire network is trained end-to-end.</li>
</ol>

<p>I'm looking for thoughts and feedback on this task and approach. Specifically,</p>

<ol>
<li>Does this approach make sense and do you think its worth trying or is there a better approach that you can point me to?</li>
<li>One issue I can think for some reviews there might be multiple images. How do I merge these multiple images to a single review to form a datapoint?</li>
<li>Do you know of any datasets/tasks that conforms to this modality?</li>
<li>Do you know of any previous work that has tackled this problem?</li>
</ol>

<p>To point 4, I did some preliminary research and only came across a very limited number of papers:</p>

<ul>
<li><a href=""http://ceur-ws.org/Vol-2125/paper_219.pdf"" rel=""nofollow noreferrer"">Combining textual and visual representations for multimodal author profiling Notebook for PAN at CLEF 2018</a></li>
<li><a href=""https://arxiv.org/pdf/1907.06370.pdf"" rel=""nofollow noreferrer"">Multimodal deep networks for text and image-based document classification</a></li>
</ul>

<p>Thanks.</p>
","bert"
"{
  ""id"": 68082,
  ""title"": ""Semantic text similarity using BERT""
}","Semantic text similarity using BERT","2020-02-14 12:01:07","68085","2","3204","<python><nlp><bert>","<p>Given two sentences, I want to quantify the degree of similarity between the two text-based on Semantic similarity.
Semantic Textual Similarity (STS) assesses the degree to which two sentences are semantically equivalent to each other.
 say my input is of order:</p>

<pre><code>index    line1                       line2
0        the cat ate the mouse      the mouse was eaten by the cat
1        the dog chased the cat     the alligator is fat 
2        the king ate the cake      the cake was ingested by the king
</code></pre>

<p>after application of the algorithm , the output needs to be </p>

<pre><code>index    line1                       line2                           lbl
0        the cat ate the mouse      the mouse was eaten by the cat    1
1        the dog chased the cat     the alligator is fat              0
2        the king ate the cake      the cake was ingested by the king 1
</code></pre>

<p>Here lbl= 1 means the sentences are semantically similar and lbl=0 means it isn't.
How would i implement this in python ?
I read the documentation of <a href=""https://github.com/hanxiao/bert-as-service#book-tutorial"" rel=""nofollow noreferrer"">bert-as-a-service</a> but since i am an absolute noob in this regard I couldn't understand it properly.</p>
","bert"
"{
  ""id"": 67914,
  ""title"": ""What are the elements in a BERT word embedding?""
}","What are the elements in a BERT word embedding?","2020-02-11 19:10:10","67968","3","3548","<word-embeddings><nlp><bert><language-model>","<p>As far as I understand, BERT is a word embedding that can be fine-tuned or used directly. </p>

<p>With older word embeddings (word2vec, Glove), each word was only represented once in the embedding (one vector per word). This was a problem because it did not take homonyms into account. As far as I understand, BERT tackles this problem by taking context into condsideration. </p>

<p>What does this mean for the word embedding itself? Is there still one vector for each word token? If so, how is context taken into consideration? If no, what is the format of the embedding?</p>
","bert"
"{
  ""id"": 67614,
  ""title"": ""BERT word embedings for finding word definition""
}","BERT word embedings for finding word definition","2020-02-05 22:30:01","","4","479","<nlp><word-embeddings><bert>","<p>Can BERT, GPT or other contextualised embedings be used for finding word definitions? What would be the most effective and not complicated approach for tackling a sample task as described below.</p>

<p>Map the meaning of the word 'bank' in the sentence ""I was walking along the bank of the river"" with one of the definitions listed in the WortNet database (or other word-sense lookup table).</p>
","bert"
"{
  ""id"": 66786,
  ""title"": ""What is a 'hidden state' in BERT output?""
}","What is a 'hidden state' in BERT output?","2020-01-20 22:00:44","66788","3","14117","<nlp><rnn><bert>","<p>I'm trying to understand the workings and output of BERT, and I'm wondering how/why each layer of BERT has a 'hidden state'.</p>

<p>I understand what RNN's have a 'hidden state' that gets passed to each time step, which is a representation of previous inputs. But I've read that BERT isn't a RNN - it's a CNN with attention.</p>

<p>But you can output the hidden state for each layer of a BERT model.  How is it that BERT has hidden states if it's not a RNN?</p>
","bert"
"{
  ""id"": 66394,
  ""title"": ""How does BERT and GPT-2 encoding deal with token such as <|startoftext|>, <s>""
}","How does BERT and GPT-2 encoding deal with token such as <|startoftext|>, <s>","2020-01-13 10:07:32","66399","3","5437","<nlp><pytorch><bert><gpt>","<p>As I understand, GPT-2 and BERT are using Byte-Pair Encoding which is a subword encoding. Since lots of start/end token is used such as &lt;|startoftext|> and , as I image the encoder should encode the token as one single piece.</p>

<p>However, when I use pytorch <code>BertTokenizer</code> it seems the encoder also separate token into pieces. Is this correct behaviour?</p>

<pre><code>from pytorch_pretrained_bert import BertTokenizer, cached_path
tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False) 
tokenizer.tokenize('&lt;s&gt; This is a sentence &lt;|endoftext|&gt;')
</code></pre>

<p>The results are:</p>

<pre><code>['&lt;',
 's',
 '&gt;',
 'This',
 'is',
 'a',
 'sentence',
 '&lt;',
 '|',
 'end',
 '##oft',
 '##ex',
 '##t',
 '|',
 '&gt;']
</code></pre>
","bert"
"{
  ""id"": 66207,
  ""title"": ""What is purpose of the [CLS] token and why is its encoding output important?""
}","What is purpose of the [CLS] token and why is its encoding output important?","2020-01-09 17:20:10","66209","72","100277","<nlp><sentiment-analysis><bert><language-model><text-classification>","<p>I am reading <a href=""http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"" rel=""noreferrer"">this article on how to use BERT</a> by Jay Alammar and I understand things up until:</p>

<blockquote>
  <p>For sentence classification, we’re only only interested in BERT’s output for the [CLS] token, so we select that slice of the cube and discard everything else.</p>
</blockquote>

<p>I have read <a href=""https://datascience.stackexchange.com/questions/46312/what-is-the-vector-value-of-cls-sep-tokens-in-bert"">this topic</a>, but still have some questions:</p>

<p>Isn't the [CLS] token at the very beginning of each sentence? Why is that ""we are only interested in BERT's output for the [CLS] token""? Can anyone help me get my head around this? Thanks!</p>
","bert"
"{
  ""id"": 66205,
  ""title"": ""Predicting Missing Word in Text""
}","Predicting Missing Word in Text","2020-01-09 17:07:08","","1","682","<nlp><bert>","<p>I know about BERT and other solutions when you masking some words and try to predict them.
But let say I have a text:</p>

<blockquote>
  <p>Transformer have taken the of Natural Processing
  by storm, transforming the field by leaps and bounds. New,
  bigger, and better models to crop up almost every , 
  benchmarks in performance across a wide variety of tasks.</p>
</blockquote>

<p>And I cannot in advance say to BERT where masking is. I am looking for an algorithm which can understand where missing words are and after that predict them.</p>
","bert"
"{
  ""id"": 66009,
  ""title"": ""Multilingual Bert sentence vector captures language used more than meaning - working as interned?""
}","Multilingual Bert sentence vector captures language used more than meaning - working as interned?","2020-01-07 07:29:00","66019","3","507","<deep-learning><nlp><pytorch><bert>","<p>Playing around with BERT, I downloaded the Huggingface Multilingual Bert and entered three sentences, saving their sentence vectors (the embedding of <code>[CLS]</code>), then translated them via Google Translate, passed them through the model and saved their sentence vectors.</p>

<p>I then compared the results using cosine similarity.</p>

<p>I was surprised to see that each sentence vector was pretty far from the one generated from the sentence translated from it (0.15-0.27 cosine distance) while different sentences from the same language were quite close indeed (0.02-0.04 cosine distance).</p>

<p>So instead of having sentences of similar meaning (but different languages) grouped together (in 768 dimensional space ;) ), dissimilar sentences of the same language are closer. </p>

<p>To my understanding the whole point of Multilingual Bert is inter-language transfer learning - for example training a model (say, and FC net) on representations in one language and having that model be readily used in other languages.  </p>

<p>How can that work if sentences (of different languages) of the exact meaning are mapped to be more apart than dissimilar sentences of the same language?  </p>

<p>My code:</p>

<pre><code>import torch

import transformers
from transformers import AutoModel,AutoTokenizer

bert_name=""bert-base-multilingual-cased""
tokenizer = AutoTokenizer.from_pretrained(bert_name)
MBERT = AutoModel.from_pretrained(bert_name)

#Some silly sentences
eng1='A cat jumped from the trees and startled the tourists'
e=tokenizer.encode(eng1, add_special_tokens=True)
ans_eng1=MBERT(torch.tensor([e]))

eng2='A small snake whispered secrets to large cats'
t=tokenizer.tokenize(eng2)
e=tokenizer.encode(eng2, add_special_tokens=True)
ans_eng2=MBERT(torch.tensor([e]))

eng3='A tiger sprinted from the bushes and frightened the guests'
e=tokenizer.encode(eng3, add_special_tokens=True)
ans_eng3=MBERT(torch.tensor([e]))

# Translated to Hebrew with Google Translate
heb1='חתול קפץ מהעץ והבהיל את התיירים'
e=tokenizer.encode(heb1, add_special_tokens=True)
ans_heb1=MBERT(torch.tensor([e]))

heb2='נחש קטן לחש סודות לחתולים גדולים'
e=tokenizer.encode(heb2, add_special_tokens=True)
ans_heb2=MBERT(torch.tensor([e]))

heb3='נמר רץ מהשיחים והפחיד את האורחים'
e=tokenizer.encode(heb3, add_special_tokens=True)
ans_heb3=MBERT(torch.tensor([e]))


from scipy import spatial
import numpy as np

# Compare Sentence Embeddings

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_heb1[1].data.numpy())

print ('Eng1-Heb1 - Translated sentences',result)


result = spatial.distance.cosine(ans_eng2[1].data.numpy(), ans_heb2[1].data.numpy())

print ('Eng2-Heb2 - Translated sentences',result)

result = spatial.distance.cosine(ans_eng3[1].data.numpy(), ans_heb3[1].data.numpy())

print ('Eng3-Heb3 - Translated sentences',result)

print (""\n---\n"")

result = spatial.distance.cosine(ans_heb1[1].data.numpy(), ans_heb2[1].data.numpy())

print ('Heb1-Heb2 - Different sentences',result)

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_eng2[1].data.numpy())

print ('Heb1-Heb3 - Similiar sentences',result)

print (""\n---\n"")

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_eng2[1].data.numpy())

print ('Eng1-Eng2 - Different sentences',result)

result = spatial.distance.cosine(ans_eng1[1].data.numpy(), ans_eng3[1].data.numpy())

print ('Eng1-Eng3 - Similiar sentences',result)

#Output:
""""""
Eng1-Heb1 - Translated sentences 0.2074061632156372
Eng2-Heb2 - Translated sentences 0.15557605028152466
Eng3-Heb3 - Translated sentences 0.275478720664978

---

Heb1-Heb2 - Different sentences 0.044616520404815674
Heb1-Heb3 - Similar sentences 0.027982771396636963

---

Eng1-Eng2 - Different sentences 0.027982771396636963
Eng1-Eng3 - Similar sentences 0.024596810340881348
""""""
</code></pre>

<p>P.S.</p>

<p>At least the Heb1 was closer to Heb3 than to Heb2.
This was also observed for the English equivalents, but less so. </p>

<p>N.B.
Originally asked on Stack Overflow, <a href=""https://stackoverflow.com/questions/59619760/multilingual-bert-sentence-vector-captures-language-used-more-than-meaning-wor"">here</a></p>
","bert"
"{
  ""id"": 65671,
  ""title"": ""Why do BERT classification do worse with longer sequence length?""
}","Why do BERT classification do worse with longer sequence length?","2019-12-31 18:08:12","108967","6","1008","<deep-learning><bert><transformer><hyperparameter-tuning><hyperparameter>","<p>I've been experimenting using transformer networks like BERT for some simple classification tasks. My tasks are binary assignment, the datasets are relatively balanced, and the corpus are abstracts from <a href=""https://www.ncbi.nlm.nih.gov/pubmed/"" rel=""nofollow noreferrer"">PUBMED</a>. The median number of tokens from pre-processing is about 350 but I'm finding a strange result as I vary the sequence length. While using too few tokens hampers BERT in a predictable way, BERT doesn't do better with more tokens. It looks like the optimal number of tokens is about 128 and consistently performs worse as I give it more of the abstract.</p>

<p>What could be causing this, and how can I investigate it further?<a href=""https://i.sstatic.net/9b1Vi.png"" rel=""nofollow noreferrer""><img src=""https://i.sstatic.net/9b1Vi.png"" alt=""enter image description here""></a></p>
","bert"
"{
  ""id"": 65620,
  ""title"": ""User profiling based on multiple posts""
}","User profiling based on multiple posts","2019-12-30 10:08:29","","5","86","<machine-learning><deep-learning><nlp><bert>","<p>I currently have collected a dataset of different social media posts for each user with labels assigned to each user. I tried to use LSTM, and BERT for the text classification problem, So for each post, I try to predict the label(for example age). This is not sufficient because you need all of the information contained in the sum of posts to determine the user's age for example.
My first thought was to concatenate all of the posts for a single user but since I am currently using BERT which has a maximum sequence length of 512 it wouldn't work. My second idea was to use a text summary and combine them in one vector and hope it doesn't pass the maximum length limit.</p>

<p>Do you have any suggestions for a possible solution? I would assume this problem has been dealt with in the scientific literature and I would be thankful if anyone could point me in the right direction.</p>
","bert"
"{
  ""id"": 65595,
  ""title"": ""fine tune BERT in a small GPU""
}","fine tune BERT in a small GPU","2019-12-29 21:39:53","","1","367","<gpu><bert><finetuning>","<p>I want to to fine tune the BERT base model but the only accelerated HW I have access to is a couple of <a href=""https://www.nvidia.com/object/product-quadro-600-us.html"" rel=""nofollow noreferrer"">Quadro 600</a> GPUs which only pack 1GB RAM and 96 CUDA cores each.</p>

<p>My question: is it even possible to fine tune BERT over this HW? If so, would it be worth with so few CUDA cores available? How can I know whether GPU memory will fit all the data and parameters?</p>
","bert"
"{
  ""id"": 65241,
  ""title"": ""Why is the decoder not a part of BERT architecture?""
}","Why is the decoder not a part of BERT architecture?","2019-12-21 17:09:07","65242","29","26177","<nlp><bert><machine-translation><attention-mechanism>","<p>I can't see how BERT makes predictions without using a decoder unit, which was a part of all models before it including transformers and standard RNNs. How are output predictions made in the BERT architecture without using a decoder? How does it do away with decoders completely?</p>
<p>To put the question another way: what decoder can I use, along with BERT, to generate output text? If BERT only encodes, what library/tool can I use to decode from the embeddings?</p>
","bert"
"{
  ""id"": 65235,
  ""title"": ""Measuring quality of answers from QnA systems""
}","Measuring quality of answers from QnA systems","2019-12-21 15:21:19","","1","99","<bert><transformer><search-engine><question-answering>","<p>I am having a question answering system which is using Seq2Seq kind of architecture. Actually it is a transformer architecture. When a question is asked it gives startposition and endposition of answer along with their logits.<br>
The answer is formed by choosing the best logits span and final probability is calculated by summing the start and end logits.  </p>

<p>Now the problem is, I have multiple answer and many times the good answer is at 2nd or 3rd place (after sorting on the result of sum of start and end probability).  Is there any metric in search engine science using which I can rank the best answers?  </p>

<p>Followings have been tried:</p>

<ul>
<li>cosine similarity between question words and answers  -  This works many times but  fails when question semantic meaning is complex</li>
<li>TFIDF - gives good score but fails when there is synonym in answers rather than matching word.</li>
<li>gensim semantic similarity - fails badly.</li>
<li>BLUE score and new BERTF1Score also tried</li>
</ul>

<p>Few terms I heard of but I doubt if these work, like Mean Reciprocal Rank which I think gives search quality rather than answer quality and also the correct response is required to calculate MRR (Please correct if I am wrong). Or the PageRank which is not valid in my case as the answer semantic meaning is preferred in QnA rather than the document popularity.  </p>

<p>Kindly suggest other metrics which search engines generally use to rank the answers.</p>
","bert"
"{
  ""id"": 64684,
  ""title"": ""Can BERT/ELMo be used (or retrained) to generate a text in both directions?""
}","Can BERT/ELMo be used (or retrained) to generate a text in both directions?","2019-12-12 09:39:29","","0","877","<nlp><bert><text-generation>","<p>Text generation is perhaps one of the fun things to do with old NGram or new BERT/ELMo models. I am wondering can BERT be used to generate text from the end of a sentence, or better in both directions. That is instead of giving some starting words, we give some ending words. </p>
","bert"
"{
  ""id"": 64583,
  ""title"": ""What are the good parameter ranges for BERT hyperparameters while finetuning it on a very small dataset?""
}","What are the good parameter ranges for BERT hyperparameters while finetuning it on a very small dataset?","2019-12-10 18:31:43","","15","18746","<deep-learning><bert><finetuning>","<p>I need to finetune BERT model (from the huggingface repository) on a sentence classification task. However, my dataset is really small.I have 12K sentences and only 10% of them are from positive classes. Does anyone here have any experience on finetuning bert in small datasets? Do you have any suggestions for learning rate, batch size, epoch number warmup steps etc.?</p>
","bert"
"{
  ""id"": 64326,
  ""title"": ""How can I feed BERT to neural machine translation?""
}","How can I feed BERT to neural machine translation?","2019-12-06 11:50:33","64599","2","3685","<keras><tensorflow><sequence-to-sequence><bert><machine-translation>","<p>I am trying to feed the <code>input</code> and <code>target</code> sentences to an NMT model, I am trying to use BERT here, But I don't have any idea how to give it to my model.
before that, I was using one-hot encoding and I got issues there and I want to use BERT.</p>

<p>Also, I have to note that, I am new in TensorFlow and deep learning. So please share your opinion with me about the use of BERT in NMT.</p>

<p>my goal is only to use BERT in my model for translation purpose</p>

<p>my model definition:</p>

<pre class=""lang-py prettyprint-override""><code>def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):
    model = Sequential()
    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))
    model.add(LSTM(units))
    model.add(RepeatVector(out_timesteps))
    model.add(LSTM(units, return_sequences=True))
    model.add(Dense(out_vocab, activation='softmax'))
    return model
</code></pre>
","bert"
"{
  ""id"": 64323,
  ""title"": ""How to load the pre-trained BERT model from local/colab directory?""
}","How to load the pre-trained BERT model from local/colab directory?","2019-12-06 10:59:04","64513","5","28704","<nlp><bert>","<p>Hi i downloaded the BERT pretrained model (<a href=""https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip"" rel=""noreferrer"">https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip</a>) from here and saved to a directory in gogole colab and in local .</p>

<p>when i try to load the model in colab im getting ""We assumed '/content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/config.json"" . tried to laod the model in local machine and getting same error .</p>

<p>this is how i loaded the model:
from transformers import BertForMaskedLM
BertNSP=BertForMaskedLM.from_pretrained('/content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/')</p>

<p>is this the correct way of loading model from the directory when i have downloaded the pretrained model ?
Im getting error "" '/content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/config.json' ""
the downloaded model had these naming conventions where file name start with bert_ but the BertForMaskedLM class is expecting the file name to be config.json .</p>

<p>bert_config.json
bert_model.ckpt.data-00000-of-00001
bert_model.ckpt.index vocab.txt
bert_model.ckpt.meta</p>

<p>FULL ERROR:
Model name '/content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/' was not found in model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). We assumed '/content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/config.json' was a path or url to a configuration file named config.json or a directory containing such a file but couldn't find any such file at this path or url.</p>

<p>when i renamed the above 4 files by removing bert from all 4 file names , i get this error even though the ""model.ckpt.index"" files exist </p>

<p>ERROR:
""OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index'] found in directory /content/drive/My Drive/bert_training/uncased_L-12_H-768_A-12/ or from_tf set to False""</p>
","bert"
"{
  ""id"": 64254,
  ""title"": ""Weight matrices in transformers""
}","Weight matrices in transformers","2019-12-05 10:34:50","","2","535","<sequence-to-sequence><bert><machine-translation><transformer><attention-mechanism>","<p>I am trying to understand the transformer architecture.</p>

<p>I am aware that the encoder/decoder contains multiple stacked self attention layers. Further each layer contains multiple heads. For example take 8 heads.</p>

<p>Now for a particular layer we will have 8 <strong>different</strong> sets of (Wq, Wk, Wv), the weight matrices used to calculate the query, key and value.</p>

<p>Now what I want to know is whether these weight matrices are <strong>shared</strong> between the different layers i.e are the (Wq, Wk, Wv) matrices of head#1 in layer 1 same for head#1 of layers 2, 3, ....?</p>

<p>And if they are shared, doesn't it affect in parallelization?</p>
","bert"
"{
  ""id"": 63787,
  ""title"": ""where to store embeddings for similarity search?""
}","where to store embeddings for similarity search?","2019-11-26 10:30:11","","5","7510","<word-embeddings><databases><bert><semantic-similarity>","<p>I've asked on stackoverflow already <a href=""https://stackoverflow.com/questions/59027867/what-is-a-good-way-to-store-nlp-embeddings-nparrays-plus-information"">(here)</a>, but I figured that the approach of storing embeddings in an ordinary postgres-Database might be flawed from the very beginning. I will shortly etch out the application again:</p>
<ul>
<li>text corpora (few hundred thousand documents, containing a few paragraphs)</li>
<li>embeddings create with BERT (for each paragraph)</li>
<li>Application: similarity search (retrieve similar paragraphs and reference to the document)</li>
</ul>
<p>I've seen tutorials about creating embeddings with BERT etc. and it all works. The Crux I have is how to manage having a few million embeddings and searching for similar ones. Where to store them, plus the additonal information (raw text related to the embeddings and document which contains the text).<br />
So the question is:<br />
How does one store a few million embeddings (768-Dimensional numpy arrays) in an efficient and searchable way without using cloud-environments (data privacy reasons)?<br />
Is Tensorflow Records the right answer?<br />
Is it in the end a relational database?<br />
Is it something different? It's my first NLP task and I might simply not know the obvious answer. However, searching on stackexchange and google didn't provide a solution.</p>
","bert"
"{
  ""id"": 63746,
  ""title"": ""NLP Transformers: How to get a fixed sentences embedding vectors size?""
}","NLP Transformers: How to get a fixed sentences embedding vectors size?","2019-11-25 15:13:00","","4","1250","<machine-learning><deep-learning><nlp><word-embeddings><bert>","<p>I'm loading a language model from torch hub (<a href=""https://camembert-model.fr/#about"" rel=""nofollow noreferrer"">CamemBERT</a> a French RoBERTa-based model) and using it do embed some sentences:  </p>

<pre class=""lang-py prettyprint-override""><code>import torch
camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')
camembert.eval()  # disable dropout (or leave in train mode to finetune)


def embed(sentence):
   tokens = camembert.encode(sentence)
   # Extract all layer's features (layer 0 is the embedding layer)
   all_layers = camembert.extract_features(tokens, return_all_hiddens=True)
   embeddings = all_layers[0]
   return embeddings

# Here we see that the shape of the embedding vector depends on the number of tokens in the sentence

u = embed(""Bonjour, ça va ?"")
u.shape # torch.Size([1, 7, 768])
v = embed(""Salut, comment vas-tu ?"")
v.shape # torch.Size([1, 9, 768])

</code></pre>

<p>Imagine now, I want to calculate the <code>cosine distance</code> between the vectors (tensors in our case) <code>u</code> and <code>v</code> : </p>

<pre class=""lang-py prettyprint-override""><code>cos = torch.nn.CosineSimilarity(dim=0)
cos(u, v) #will throw an error since the shape of `u` is different from the shape of `v``
</code></pre>

<p>I'm asking what is the best method to use in order to always get the <strong>same embedding shape</strong> for a sentence regardless the count of tokens?</p>

<p>=> The first solution I'm thinking of is calculating the <code>mean on axis=1</code> (mean embedding of tokens in the sentence) since axis=0 and axis=2 have always the same size :</p>

<pre class=""lang-py prettyprint-override""><code>cos = torch.nn.CosineSimilarity(dim=1) #dim becomes 1 now

u = u.mean(axis=1)
v = v.mean(axis=1)

cos(u, v).detach().numpy().item() # works now and gives 0.7269
</code></pre>

<p>But, I'm afraid that I'm hurting the embedding when calculating the mean! </p>

<p>=> The second solution is to pad shorter sentences out, that means:  </p>

<ul>
<li>giving a list of sentences to embed at a time (instead of embedding sentence by sentence)</li>
<li>look up for the sentence with the longest tokens and embed it, get its shape <code>S</code></li>
<li>for the rest of sentences embed then pad zero to get the same shape <code>S</code> (the sentence has 0 in the rest of dimensions)</li>
</ul>

<p>What are your thoughts? What technique would you use and why? </p>
","bert"
"{
  ""id"": 63459,
  ""title"": ""How to obtain the word vectors optimally""
}","How to obtain the word vectors optimally","2019-11-20 10:59:05","","0","429","<word-embeddings><bert><spacy>","<p>I have a list of strings as shown</p>

<pre><code>sent_list = [""Carrefour is in France"", ""Apple pie is delicious"", ""Amazon has just delivered"", ...]
</code></pre>

<p>My code to get word embeddings below</p>

<pre><code>import spacy

nlp = spacy.load(""en_trf_bertbaseuncased_lg"")

for sent in sent_list:
    print(nlp(sent).vector)
</code></pre>

<p>This takes considerable time when the list is of large size (>10000). I tried disabling <code>sentencizer</code> within the <code>nlp pipe</code> but with not much improvement. How can this be optimized for shorter run time?</p>
","bert"
"{
  ""id"": 63217,
  ""title"": ""Similarity of words using BERTMODEL""
}","Similarity of words using BERTMODEL","2019-11-15 17:06:44","","3","6878","<nlp><word-embeddings><similarity><bert>","<p>I want to find the similarity of words using the BERT model within the NER task. I have my own dataset so, I don't want to use the pre-trained model.
I do the following:</p>

<pre><code>from transformers import BertModel
hidden_reps, cls_head = BertModel(token_ids , attention_mask = attn_mask , token_type_ids = seg_ids)
</code></pre>

<p>where</p>

<pre><code>token_ids with Shape : [1, 4, 47]
attn_mask with Shape : [1, 4, 47]
seg_ids with Shape : [1, 4, 47]
</code></pre>

<p>but I have an error :</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-74-5fa632122cc7&gt; in &lt;module&gt;()
      1 from transformers import BertModel
----&gt; 2 hidden_reps, cls_head = BertModel(token_ids , attention_mask = attn_mask , token_type_ids = seg_ids)

TypeError: __init__() got an unexpected keyword argument 'attention_mask'
</code></pre>

<p>How can I fix this error??</p>

<p>and how can I find the word embedding and the similarity of a given word using this model? </p>

<p>I could not find any tutorials or similar codes for this task.</p>
","bert"
"{
  ""id"": 63140,
  ""title"": ""BERT for non-textual sequence data""
}","BERT for non-textual sequence data","2019-11-14 08:55:22","","2","724","<neural-network><categorical-data><embeddings><bert><transformer>","<p>I'm working on a deep learning solution for classifying sequence data that isn't raw text but rather entities (which have already been extracted from the text). I am currently using word2vec-style embeddings to feed the entities to a CNN, but I was wondering if a Transformer (à la BERT) would be a better alternative &amp; provide a better way of capturing the semantics of the entities involved. I can't seem to find any articles (let alone libraries) to apply sth like BERT to non-textual sequence data. Does anybody know any papers about this angle? I've thought about training a BERT model from scratch and treating the entities as if they were text. The issue with that though is that apparently BERT is slow when dealing with long sequences (sentences). In my data I often have sequences that have a length of 1000+  so I'm worried BERT won't cut it. Any help, insights or references are very much appreciated! Thanks</p>
","bert"
"{
  ""id"": 63124,
  ""title"": ""BERT Model Evaluation Measure in terms of Syntax Correctness and Semantic Coherence""
}","BERT Model Evaluation Measure in terms of Syntax Correctness and Semantic Coherence","2019-11-14 03:29:42","","2","360","<nlp><bert><language-model>","<p>For example I have an original sentence. The word barking corresponds to the word that is missing.</p>

<pre><code>Original Sentence : The dog is barking.
Incomplete Sentence : The dog is ___________.
</code></pre>

<p>For example, using the BERT model, it predicts the word crying instead of 
the word barking. How will I measure the accuracy of the BERT Model in terms of how syntactically correct and semantically coherent the predicted word is?</p>

<p>(For an instance, there are a lot of incomplete sentences, and the task is to evaluate BERT accuracy based on these incomplete sentences.)</p>

<p>In other words, how will I measure the distance in terms of semantics in terms of model between the two words <code>barking</code> and <code>crying</code>.</p>

<p>Please help.</p>
","bert"
"{
  ""id"": 62862,
  ""title"": ""Preprocessing for Text Classification in Transformer Models (BERT variants)""
}","Preprocessing for Text Classification in Transformer Models (BERT variants)","2019-11-08 06:28:48","","14","8598","<python><nlp><preprocessing><bert><transformer>","<p>This might be silly to ask, but I am wondering if one should carry out the conventional text preprocessing steps for training one of the transformer models?</p>
<p>I remember for training a Word2Vec or Glove, we needed to perform an extensive text cleaning like: tokenize, remove stopwords, remove punctuations, stemming or lemmatization and more. However, during last few days I have had a quick jump into transformer models (fascinated btw), and what I have noticed that most of these models have a built-in tokenizer (cool), but none of the demos, examples, or tutorials are performing any of the these text preprocessing steps. You may take <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""noreferrer"">fast-bert</a> for instance, there are no text preprocessing involved for the demos (maybe it is just a demo), but at <a href=""https://github.com/kaushaltrivedi/fast-bert#5-model-inference"" rel=""noreferrer"">inference</a> the whole sentences are passed without any cleaning:</p>
<pre><code>texts = ['I really love the Netflix original movies',
         'this movie is not worth watching']
predictions = learner.predict_batch(texts)
</code></pre>
<p>The same is true for the <a href=""https://github.com/huggingface/transformers"" rel=""noreferrer"">original transformer</a> by HuggingFace. Or many tutorials that I have looked at (take <a href=""https://github.com/kaushaltrivedi/bert-toxic-comments-multilabel/blob/master/toxic-bert-multilabel-classification.ipynb"" rel=""noreferrer"">this</a> or <a href=""https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d"" rel=""noreferrer"">another one</a>). I can imagine that depending on the task this might not be required, e.g. next work prediction or machine translation and more. More importantly I think this is part of the contextual-based approach that these models offer (that is the innovation so to say) that are meant to keep most of the text and we may obtain a minimum but still good representation of the each token (out of vocabulary word). Borrowed from <a href=""https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d"" rel=""noreferrer"">medium article</a> by HuggingFace:</p>
<blockquote>
<p><strong>Tokenisation</strong>
BERT-Base, uncased uses a vocabulary of 30,522 words. The
processes of tokenisation involves splitting the input text into list
of tokens that are available in the vocabulary. In order to deal with
the words not available in the vocabulary, BERT uses a technique
called BPE based WordPiece tokenisation. In this approach an out of
vocabulary word is progressively split into subwords and the word is
then represented by a group of subwords. Since the subwords are part
of the vocabulary, we have learned representations an context for
these subwords and the context of the word is simply the combination
of the context of the subwords.</p>
</blockquote>
<p>But does that hold true for tasks like multi-label text classification? In my use case the text is full of not useful stopwords, punctuation, characters and abbreviations and it is multi-label text classification as mentioned earlier. And in fact the prediction accuracy is not good (after a few rounds of training using <a href=""https://github.com/kaushaltrivedi/fast-bert"" rel=""noreferrer"">fast-bert</a>). What do I miss here?</p>
","bert"
"{
  ""id"": 62831,
  ""title"": ""BERT - How Question answering is different than classification""
}","BERT - How Question answering is different than classification","2019-11-07 12:54:55","","0","1178","<pytorch><bert><question-answering><huggingface>","<p>Basically I am trying to understand how question answering works in case of BERT. Code for both classes QuestionAnswering and Classification is pasted below for reference. My understanding is:  </p>

<pre><code>class BertForSequenceClassification(PreTrainedBertModel):
    def __init__(self, config, num_labels=2):
        super(BertForSequenceClassification, self).__init__(config)
        self.num_labels = num_labels
        self.bert = BertModel(config)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(config.hidden_size, num_labels)
        self.apply(self.init_bert_weights)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):
        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        if labels is not None:
            loss_fct = CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
            return loss
        else:
            return logits
</code></pre>

<p>In Above code pooled_output is considered useful in line  <code>_, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)</code>  </p>

<p>And in below QnA code encoder layer output (i.e., sequence_output) is considered useful in line: <code>sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)</code></p>

<pre><code>class BertForQuestionAnswering(PreTrainedBertModel):
    def __init__(self, config):
        super(BertForQuestionAnswering, self).__init__(config)
        self.bert = BertModel(config)
        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version
        # self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.qa_outputs = nn.Linear(config.hidden_size, 2)
        self.apply(self.init_bert_weights)

    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):
        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)
        logits = self.qa_outputs(sequence_output)
        start_logits, end_logits = logits.split(1, dim=-1)
        start_logits = start_logits.squeeze(-1)
        end_logits = end_logits.squeeze(-1)
</code></pre>

<p>Now my questions are:  </p>

<ol>
<li><p>Why there are 2 logits being returned in sequence_output for Question Answering case</p></li>
<li><p>What is different in encoder layer and pooled layer </p></li>
<li><p>Why is encoder layer (sequence_output) is considered in QnA case and Pooled layer in classification case</p></li>
</ol>
","bert"
"{
  ""id"": 62788,
  ""title"": ""TLDR Bot - Sentence Tagging w/ BERT""
}","TLDR Bot - Sentence Tagging w/ BERT","2019-11-06 19:23:05","","0","303","<nlp><lstm><bert>","<p>Currently making a bot that condenses news articles. I'm tagging sentences as important or not important using a simple BERT classifier. The results were... not great. I'm really interested in how I can improve the results using LSTM.</p>

<p>I'm now batching 5 sentences together, calculating their BERT encodings, and then using 2 LSTM Layers, one backwards one forwards, to predict if the sentence is important.</p>

<p>Unfortunately I'm now calculating 5 times the number of embeddings, and if it doesn't work, I can't seem to figure out how to feed a variable number of things into BERT using Tensorflow, to see if I can tweak some results.</p>

<p>Are there other methods to add surrounding sentences to this context?</p>
","bert"
"{
  ""id"": 62730,
  ""title"": ""Best structure for a LSTM Bert sentence classifier""
}","Best structure for a LSTM Bert sentence classifier","2019-11-05 18:56:41","","0","306","<nlp><bert>","<p>I'm interested in classifying sentences using BERT. Finetuning on a single sentence had very poor results. I'd like to add a forward and backward LSTM layer to try to improve results. I'm having trouble figuring out the best structure for an efficient implementation.</p>

<p>My first idea is to implement it as normal, but use a tf while loop to get all the BERT embeddings, and then feed them into the output. This seems really inefficient because I'll be calculating way more embeddings than I need.</p>

<p>The next would be to pre calculate all BERT embeddings and then feed those as input to my LSTM and classifier layer. Using this I think it's clear how to train the LSTM and classifier model, but how would I train the BERT Model as well? Is this a common method of constructing / training models? Any references?</p>

<p>Finally the last idea was to use the 2 sentence BERT embedding. It appears it would be limited to a 3 sentence window - previous sentence, current, and next. So scaling up might not work if the model still does not perform that well. Another issue is that it's unclear, at least to me, if the pooled output of a sentence</p>

<pre><code>[cls] sentenceA [sep] sentenceB [sep]
</code></pre>

<p>gives a representation of sentenceA, of both of them, or of sentenceB 'given' sentenceA. The pooled output is the representation of [cls] token, but in the paper / github there was no reference to if the [sep] representation encodes anything at all.</p>

<p>Any help, insights, or similar work would be appreciated!</p>
","bert"
"{
  ""id"": 62658,
  ""title"": ""How to get sentence embedding using BERT?""
}","How to get sentence embedding using BERT?","2019-11-04 15:22:32","","39","78505","<tensorflow><nlp><pytorch><bert>","<p>How to get sentence embedding using BERT?</p>
<pre class=""lang-py prettyprint-override""><code>from transformers import BertTokenizer
tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')
sentence='I really enjoyed this movie a lot.'
#1.Tokenize the sequence:
tokens=tokenizer.tokenize(sentence)
print(tokens)
print(type(tokens))
</code></pre>
<h1>2. Add [CLS] and [SEP] tokens:</h1>
<pre class=""lang-py prettyprint-override""><code>tokens = ['[CLS]'] + tokens + ['[SEP]']
print(&quot; Tokens are \n {} &quot;.format(tokens))
</code></pre>
<h1>3. Padding the input:</h1>
<pre class=""lang-py prettyprint-override""><code>T=15
padded_tokens=tokens +['[PAD]' for _ in range(T-len(tokens))]
print(&quot;Padded tokens are \n {} &quot;.format(padded_tokens))
attn_mask=[ 1 if token != '[PAD]' else 0 for token in padded_tokens  ]
print(&quot;Attention Mask are \n {} &quot;.format(attn_mask))
</code></pre>
<h1>4. Maintain a list of segment tokens:</h1>
<pre class=""lang-py prettyprint-override""><code>seg_ids=[0 for _ in range(len(padded_tokens))]
print(&quot;Segment Tokens are \n {}&quot;.format(seg_ids))
</code></pre>
<h1>5. Obtaining indices of the tokens in BERT’s vocabulary:</h1>
<pre class=""lang-py prettyprint-override""><code>sent_ids=tokenizer.convert_tokens_to_ids(padded_tokens)
print(&quot;senetence idexes \n {} &quot;.format(sent_ids))
token_ids = torch.tensor(sent_ids).unsqueeze(0) 
attn_mask = torch.tensor(attn_mask).unsqueeze(0) 
seg_ids   = torch.tensor(seg_ids).unsqueeze(0)
</code></pre>
<h1>Feed them to BERT</h1>
<pre class=""lang-py prettyprint-override""><code>hidden_reps, cls_head = bert_model(token_ids, attention_mask = attn_mask,token_type_ids = seg_ids)
print(type(hidden_reps))
print(hidden_reps.shape ) #hidden states of each token in inout sequence 
print(cls_head.shape ) #hidden states of each [cls]

output:
hidden_reps size 
torch.Size([1, 15, 768])

cls_head size
torch.Size([1, 768])
</code></pre>
<p>Which vector represents the sentence embedding here? Is it <code>hidden_reps</code>  or <code>cls_head</code> ?</p>
<p>Is there any other way to get sentence embedding from BERT in order to perform similarity check with other sentences?</p>
","bert"
"{
  ""id"": 62037,
  ""title"": ""Can BERT embeddings be used to reproduce the original content of the text?""
}","Can BERT embeddings be used to reproduce the original content of the text?","2019-10-21 15:38:14","","2","229","<deep-learning><nlp><machine-translation><bert><neural-style-transfer>","<p>From what I understand, BERT provides contextualized embeddings that are not deterministic the way Word2Vec embeddings (i.e. the word ""Queen"" doesn't always produce the same vector, it'll be different depending on the context)</p>

<p>Is there a way to ""reverse"" these contextualized embeddings to produce an output related to the original content of the text? For instance, how would I do machine translation, or style transfer?</p>
","bert"
"{
  ""id"": 61825,
  ""title"": ""Why is word prediction an obsession in Natural Language Processing?""
}","Why is word prediction an obsession in Natural Language Processing?","2019-10-16 14:52:38","61893","8","803","<nlp><bert>","<p>I have heard how great <a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">BERT</a> is at masked word prediction, i.e. predicting a missing word from a sentence.</p>

<p>In a <a href=""https://medium.com/saarthi-ai/bert-how-to-build-state-of-the-art-language-models-59dddfa9ac5d"" rel=""noreferrer"">Medium post about BERT</a>, it says:</p>

<blockquote>
  <p>The basic task of a language model is to predict words in a blank, or it predicts the probability that a word will occur in that particular context. Let’s take another example:</p>
  
  <p>“FC Barcelona is a _____ club”</p>
</blockquote>

<p>Indeed, I recently heard about <a href=""https://arxiv.org/abs/1907.10529"" rel=""noreferrer"">SpanBERT</a>, which is ""designed to better represent and predict spans of text"".</p>

<p>What I do not understand is: <strong><em>why?</em></strong></p>

<ol>
<li>I cannot think of any common reason that a human would need to do this task, let alone why it would need to be automated.</li>
<li>This does not even seem to be a task where it is particularly easy to evaluate the success of a model. For example, </li>
</ol>

<blockquote>
  <p>My ___ is cold</p>
</blockquote>

<p>This could reasonably be a number of possible words. How can BERT be expected to get this right, and how can humans or another algorithm be expected to evaluate whether ""soup"" is a better answer than ""coffee""?</p>

<p>Clearly there are a lot of smart people who think that this is important, so I accept my lack of understanding is likely based on my own ignorance. Is it that this task itself is not important, but it's a proxy for ability at other tasks?</p>

<p>What am I missing?</p>
","bert"
"{
  ""id"": 61447,
  ""title"": ""BERT training on two tasks: what is the order of tasks?""
}","BERT training on two tasks: what is the order of tasks?","2019-10-08 17:05:50","62857","0","792","<bert>","<p>I read that BERT has been trained on two tasks: Masked Language Modeling and Next Sentence Prediction. I want to gain clarity how exactly it was done. </p>

<p>Was it initially trained on  Masked Language Modeling (where we predict masked token) and later on Sentence Prediction (where we predict isnext or not)? It seems like these two tasks would require different architecture. Does BERT has some common architecture for these two tasks with interchanging layer for individual tasks? Still question of ordering remains. </p>
","bert"
"{
  ""id"": 58691,
  ""title"": ""Smallest Possible Dataset for Text Classification using BERT""
}","Smallest Possible Dataset for Text Classification using BERT","2019-09-04 19:16:57","","0","944","<nlp><transfer-learning><bert>","<p>What are your experiences for appropriate dataset sizes for usual text classification tasks using a finetuned BERT such as sentiment analysis?</p>

<p>~100 examples</p>

<p>~1000 examples</p>

<p>...</p>

<p>~10000000 examples</p>

<p>What are your experiences?</p>
","bert"
"{
  ""id"": 57819,
  ""title"": ""Structuring a LSTM Layer""
}","Structuring a LSTM Layer","2019-08-19 17:53:22","","3","60","<machine-learning><lstm><bert>","<p>I'm trying to improve an NER Bert sequence tagger using LSTM layers in TensorFlow. I'm a bit unclear on the interface and how a LSTM layer should be set up.</p>

<p>Currently, I'm taking in 3-5 sentences and outputting a layer of shape 
<code>(?, 5, max_token_length,token_vector_output)</code>.
I'd like to take into consideration previous sentence output for the next sequence I'm tagging.</p>

<p>Reshaping the above to <code>(?, 5, max_token_length*token_vector_output)</code> we can then apply a tf.keras.layers.LSTM layer, but this will yield a tensor of shape <code>(?, unit_size)</code> This appears to kill all sequence data off. Is this the correct way to feed my data to a LSTM layer?</p>

<p><strong>As an update, I think I have found a solution - let me know if this isn't a correct way to structure the network.</strong></p>

<p>The LSTM will output a tensor <code>(?, unit_size)</code> regularly - but this is the current state of the LSTM layer. Using the <code>return_sequences=True</code> argument, it will instead return a tensor of shape <code>(?,sequence_length, unit_size)</code>, which is the history of the internal states.</p>

<p>In my case, I will be structuring the input as <code>(?, 5 * max_token_length, token_vector_output)</code> to the LSTM layer, returning a <code>(?, 5 * max_token_length, unit_size)</code> tensor. Taking the last <code>max_token_length</code> on axis 1, I'll get the states during the sentence I'm tagging. My logits will a weighted sum of this state tensor and the regular sequential Bert Output of that sentence, and applying a softmax will give the final labels!</p>
","bert"
"{
  ""id"": 56651,
  ""title"": ""BERT : text classification and feature extractionn""
}","BERT : text classification and feature extractionn","2019-07-31 02:38:31","","1","155","<nlp><bert>","<p>I have tried multi-label text classification with BERT. </p>

<p>Here is the sample input: <strong>$15.00 hour, customer service, open to industries</strong></p>

<p>One of the labels is Billing_rate and prediction score looks quite good.</p>

<p>Now my question is if I want to extract <strong>$15.00 hour</strong> basically feature value out of BERT. Can you please suggest what are my next step options? </p>
","bert"
"{
  ""id"": 54906,
  ""title"": ""Predicting word from a set of words""
}","Predicting word from a set of words","2019-07-02 11:27:51","","2","399","<neural-network><nlp><word2vec><word-embeddings><bert>","<p>My task is to predict relevant words based on a short description of an idea. for example ""SQL is a domain-specific language used in programming and designed for managing data held in a relational database"" should produce words like ""mysql"", ""Oracle"", ""Sybase"", ""Microsoft SQL Server"" etc...</p>

<p>My thinking is to treat the initial text as a set of words (after lemmatization and stop words removal) and predict words that should be in that set. I can then take all of my texts (of which I have a lot), remove a single word and learn to predict it.</p>

<p>My initial thought was to use word2vec and find words similar to my set. But this doesn't work very well, as I don't want similar words but words that go together in many sentences, which is sort of the task that word2vec is training on to create its embedding...</p>

<p>How would you model this problem? I don't think RNNs are relevant here because I want to use a set of words - they do not have any order, I'm not trying to predict the <em>next</em> word in any way. However, the size of the set could vary I think...</p>

<p>What kind of NN architecture would you use for this sort of problem?</p>

<p>UPDATE:</p>

<p>To further explain the purpose of the predictor and why word2vec doesn't work for me:</p>

<p>I did train a word2vec model on my data (which is very domain specific and I have lots of data on it). Now my problem is that it produces words that are similar to the ones I input to it... and not ones that could be in the context.</p>

<p>e.g: I input ""computer manufacturer"" to get most similar words and i get:</p>

<p>(u'equipment_manufacturer', 0.7463390827178955),
 (u'manufacture', 0.7410058975219727),
 (u'manufacturer_distributor', 0.7196526527404785),
 (u'distributor', 0.6950951814651489),
 (u'desktop_computer', 0.6632821559906006)</p>

<p>These are terms that have similar meaning...</p>

<p>But I'm not looking for similar meaning. I would be more interested in getting ""Dell"" for example as the output.
Another example is to associate ""Apple_inc"" with informatics, OS, hardware.. - not similar words, but related ones...</p>

<p>So my thinking now is to build a word2vec CBOW model, but not to use just the embeddings that are created but actually use the network to predict new words from context... </p>
","bert"
"{
  ""id"": 54888,
  ""title"": ""How can I add custom numerical features for training to BERT fine tuning?""
}","How can I add custom numerical features for training to BERT fine tuning?","2019-07-02 06:41:19","","7","7179","<deep-learning><tensorflow><bert>","<p>I have currently fine tuned the BERT model on some custom data and I want to conduct some more experiments to increase the accuracy. </p>

<p>My original dataset consists of a pair of sentences (like MRPC dataset). I want to increase the accuracy of classification by adding some numerical features (which I will separately calculate). I wanted to know if I could train bert using this after I have already fine tuned it ?</p>

<p>I have read about some solutions people have proposed in the past like :
<em>'Extracting word embeddings (extract_features.py on Bert GITHUB) and combining that with my custom data to feed a single layer CNN network.'</em> I dont want to lose out on the accuracy the BERT network is providing me by only extracting features from the pretrained model. </p>

<p>So is there any way I can create a kind of hybrid model which first fine tunes BERT and then I add my features and it feeds into another model for an improved classification ?</p>

<p>P.S</p>

<p>As I am new to tensorflow and Deep learning, please let me know if there is something fundamentally wrong in my understanding. Thank you for your help</p>
","bert"
"{
  ""id"": 54412,
  ""title"": ""How to add a CNN layer on top of BERT?""
}","How to add a CNN layer on top of BERT?","2019-06-24 21:26:21","","5","9086","<nlp><cnn><pytorch><transfer-learning><bert>","<p>I am just playing with <strong>bert (Bidirectional Encoder Representation from Transformer)</strong> <br>
<a href=""https://arxiv.org/abs/1810.04805"" rel=""noreferrer"">Research Paper</a></p>

<p>Suppose I want to add any other model or layers like Convolutional Neural Network layers (CNN), Non Linear (NL) layers on top of BERT model. <strong>How can I do this?</strong></p>

<p><strong>I am not able to figure out where should I change in code of BERT.</strong> I am using the pytorch implementation of <a href=""https://github.com/huggingface/pytorch-pretrained-BERT"" rel=""noreferrer"">bert</a>
from huggingface.</p>

<p><em>This is what I want to do:</em>
<a href=""https://i.sstatic.net/LGIlP.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/LGIlP.png"" alt=""enter image description here""></a>
<a href=""https://i.sstatic.net/ckzSB.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/ckzSB.png"" alt=""enter image description here""></a></p>

<p><strong>Please show steps to implement this using sudo code</strong> which will help me in implemention of cnn on top of BERT.</p>
","bert"
"{
  ""id"": 54232,
  ""title"": ""BERT vs Word2VEC: Is bert disambiguating the meaning of the word vector?""
}","BERT vs Word2VEC: Is bert disambiguating the meaning of the word vector?","2019-06-21 16:25:16","","27","20156","<word2vec><word-embeddings><bert>","<p><strong>Word2vec</strong>:
Word2vec provides a vector for each token/word and those vectors encode the meaning of the word. Although those vectors are not human interpretable, the meaning of the vectors are understandable/interpretable by comparing with other vectors (for example, the vector of <code>dog</code> will be most similar to the vector of <code>cat</code>), and various interesting equations (for example <code>king-men+women=queen</code>, which proves how well those vectors hold the semantic of words).</p>

<p><strong>The problem</strong> with word2vec is that each word has only one vector but in the real world each word has different meaning depending on the context and sometimes the meaning can be totally different (for example, <code>bank as a financial institute</code> vs <code>bank of the river</code>).</p>

<p><strong>Bert:</strong>
One important difference between Bert/ELMO (dynamic word embedding) and Word2vec is that these models consider the context and for each token, there is a vector.</p>

<p><strong>Now the question is</strong>, do vectors from Bert hold the behaviors of word2Vec and solve the meaning disambiguation problem (as this is a contextual word embedding)? </p>

<p><strong>Experiments</strong>
To get the vectors from google's pre-trained model, I used <a href=""https://pypi.org/project/bert-embedding/"" rel=""noreferrer"">bert-embedding-1.0.1</a> library. 
I first tried to see whether it hold the <strong>similarity property</strong>. To test, I took the first paragraphs from wikipedia page of Dog, Cat, and Bank (financial institute). The similar word for <code>dog</code> is: 
    ('dog',1.0)
    ('wolf', 0.7254540324211121)
    ('domestic', 0.6261438727378845)
    ('cat', 0.6036421656608582)
    ('canis', 0.5722522139549255)
    ('mammal', 0.5652133226394653)
Here, the first element is token and second is the similarity.</p>

<p>Now for <strong>disambiguation test</strong>: 
Along with Dog, Cat and Bank (financtial institute), I added a paragraph of <a href=""https://en.wikipedia.org/wiki/Bank_(geography)"" rel=""noreferrer"">River bank</a> from wikipedia. This is to check that bert can differentiate between two different types of <code>Bank</code>. Here the hope is, the vector of token bank (of river) will be close to vector of <code>river</code> or <code>water</code> but far away from <code>bank(financial institute)</code>, <code>credit</code>, <code>financial</code> etc. Here is the result: 
The second element is the sentence to show the context. </p>

<pre><code>('bank', 'in geography , the word bank generally refers to the land alongside a body of water . different structures are referred to as', 1.0)
('bank', 'a bank is a financial institution that accepts deposits from the public and creates credit .', 0.7796692848205566)
('bank', 'in limnology , a stream bank or river bank is the terrain alongside the bed of a river , creek , or', 0.7275459170341492)
('bank', 'in limnology , a stream bank or river bank is the terrain alongside the bed of a river , creek , or', 0.7121304273605347)
('bank', 'the bank consists of the sides of the channel , between which the flow is confined .', 0.6965076327323914)
('banks', 'markets to their importance in the financial stability of a country , banks are highly regulated in most countries .', 0.6590269804000854)
('banking', 'most nations have institutionalized a system known as fractional reserve banking under which banks hold liquid assets equal to only a', 0.6490173935890198)
('banks', 'most nations have institutionalized a system known as fractional reserve banking under which banks hold liquid assets equal to only a', 0.6224181652069092)
('financial', 'a bank is a financial institution that accepts deposits from the public and creates credit .', 0.614281952381134)
('banks', 'stream banks are of particular interest in fluvial geography , which studies the processes associated with rivers and streams and the deposits', 0.6096583604812622)
('structures', 'in geography , the word bank generally refers to the land alongside a body of water . different structures are referred to as', 0.5771245360374451)
('financial', 'markets to their importance in the financial stability of a country , banks are highly regulated in most countries .', 0.5701562166213989)
('reserve', 'most nations have institutionalized a system known as fractional reserve banking under which banks hold liquid assets equal to only a', 0.5462549328804016)
('institution', 'a bank is a financial institution that accepts deposits from the public and creates credit .', 0.537483811378479)
('land', 'in geography , the word bank generally refers to the land alongside a body of water . different structures are referred to as', 0.5331911444664001)
('of', 'in geography , the word bank generally refers to the land alongside a body of water . different structures are referred to as', 0.527492105960846)
('water', 'in geography , the word bank generally refers to the land alongside a body of water . different structures are referred to as', 0.5234918594360352)
('banks', 'bankfull discharge is a discharge great enough to fill the channel and overtop the banks .', 0.5213838815689087)
('lending', 'lending activities can be performed either directly or indirectly through due capital .', 0.5207482576370239)
('deposits', 'a bank is a financial institution that accepts deposits from the public and creates credit .', 0.5131596922874451)
('stream', 'in limnology , a stream bank or river bank is the terrain alongside the bed of a river , creek , or', 0.5108630061149597)
('bankfull', 'bankfull discharge is a discharge great enough to fill the channel and overtop the banks .', 0.5102289915084839)
('river', 'in limnology , a stream bank or river bank is the terrain alongside the bed of a river , creek , or', 0.5099104046821594)
</code></pre>

<p>Here, the result of the most similar vectors of bank (as a river bank, the token is taken from the context of the first row and that is why the similarity score is 1.0. So, the second one is the closest vector). From the result, it can be seen that the first most close token's meaning and context is very different. Even the token <code>river</code>, <code>water and</code>stream` has lower similarity. </p>

<p>So, it seems that the vectors do not really disambiguate the meaning. 
Why is that?
Isn't the contextual token vector supposed to disambiguate the meaning of a word?</p>
","bert"
"{
  ""id"": 54008,
  ""title"": ""Calculating cosine similarity between 3D arrays using Python""
}","Calculating cosine similarity between 3D arrays using Python","2019-06-18 10:36:27","","5","7171","<python><cosine-distance><bert><matrix>","<p>I have two matrices with multiple columns and three rows each. I calculated the cosine similarity (sklearn) but it gives the result as a matrix. How can I obtain one single value?
The matrices are the embeddings of two words each, obtained from BERT.</p>
","bert"
"{
  ""id"": 53875,
  ""title"": ""What is whole word masking in the recent BERT model?""
}","What is whole word masking in the recent BERT model?","2019-06-15 23:13:57","53910","12","7755","<nlp><language-model><bert>","<p>I was checking <a href=""https://github.com/google-research/bert"" rel=""noreferrer"">BERT GitHub page</a> and noticed that there are new models built from a new training technique called ""whole word masking"". Here is a snippet describing it:</p>

<blockquote>
  <p>In the original pre-processing code, we randomly select WordPiece tokens to mask. For example:</p>
</blockquote>

<pre><code>Input Text: the man jumped up , put his basket on phil ##am ##mon ' s head

Original Masked Input: [MASK] man [MASK] up , put his [MASK] on phil [MASK] ##mon ' s head
</code></pre>

<blockquote>
  <p>The new technique is called Whole Word Masking. In this case, we always mask all of the the tokens corresponding to a word at once. The overall masking rate remains the same.</p>
</blockquote>

<pre><code>Whole Word Masked Input: the man [MASK] up , put his basket on [MASK] [MASK] [MASK] ' s head
</code></pre>

<p>I can't understand ""<em>we always mask all of the the tokens corresponding to a word at once</em>"". ""jumped"", ""phil"", ""##am"", and ""##mon"" are masked and I am not sure how these tokens are related.</p>
","bert"
"{
  ""id"": 53772,
  ""title"": ""Get long answers from BERT""
}","Get long answers from BERT","2019-06-14 08:27:56","","1","503","<nlp><bert>","<p>We are using Google BERT for question and answering. We are using vanialla bert-base-uncased as well as squad trained checkpoints.   </p>

<p>The answers from BERT are very short and crisp. For example, if we ask describe a chatbot, then it will simply return, takes input from user and replies ...  Though the answer actually is complete paragraph.  </p>

<ul>
<li>Will training BERT on long paragraphs can solve this problem? If yes, any idea from where we can get such a QnA dataset, as it is not possible to create a huge QnA dataset manually.</li>
<li>Is there any other tweak which can be done at some BERT layer, so that it starts understanding a long answer?</li>
<li>Or is there any other framework or system already have solved this kind of problem, by maybe integrating with some other neural network technique, or by using a pipeline of multiple components?</li>
</ul>
","bert"
"{
  ""id"": 53555,
  ""title"": ""How to classify neutral sentiments using BERT""
}","How to classify neutral sentiments using BERT","2019-06-11 05:27:34","","2","1017","<sentiment-analysis><bert>","<p>We can do text classification as positive and negative as mentioned in below notebook. But is there any way to classify neutral sentiment also?</p>

<p><a href=""https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb"" rel=""nofollow noreferrer"">https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb</a></p>

<p>Actually, I want to know like what kind of changes do we need to make in the above notebook so that it can classify neutral sentiments also besides positive and negative.</p>

<p>Thanks in advance.</p>
","bert"
"{
  ""id"": 53462,
  ""title"": ""Incorrect Text Classification, But Accurate Model. Do I Perform Manual Text Classification For A Data Set?""
}","Incorrect Text Classification, But Accurate Model. Do I Perform Manual Text Classification For A Data Set?","2019-06-08 20:46:24","","1","188","<machine-learning><classification><nlp><data><bert>","<p>I'm currently using Google's BERT pre-trained sentiment analysis model that is trained on an IMDb pos/neg review dataset. I'm using this model to predict whether tweets are positive (bullish) or negative (bearish). While the model is accurate when plugging in my own test data (F1 Score ~86%), the classification itself is not accurate. Tweets that are undoubtedly positive/bullish, and not classified as so. Perhaps this is because the language in the investment world is different than a movie review - which uses universally recognized positive/negative words and/or sentences.</p>

<p>The same is true when I take my tweet dataset and use Vader SentimentIntensityAnalyser to parse pos/neg tweets into separate folders.</p>

<p>So my question is... since the language that is used for telling whether a stock is bullish/bearish is uniquely different from that of an Amazon review, or movie review, would it be optimal for me to manually classify my dataset into positive (bullish) and negative (bearish) datasets?  </p>
","bert"
"{
  ""id"": 53270,
  ""title"": ""BERT: it is possible to use it for topic modeling?""
}","BERT: it is possible to use it for topic modeling?","2019-06-05 17:07:41","","7","7520","<topic-model><lda><bert>","<p>I'm struggling to understand which are the full capabilities of BERT: it is possible to make topic modeling of text, like the one we can achieve with LDA?</p>
","bert"
"{
  ""id"": 52723,
  ""title"": ""Bert: fine-tuning the entire pre-trained model end-to-end vs using contextual token vector""
}","Bert: fine-tuning the entire pre-trained model end-to-end vs using contextual token vector","2019-05-27 17:01:51","","3","1013","<word-embeddings><bert>","<p>In the official <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">github page</a> of BERT, it mentions that: </p>

<blockquote>
  <p>In certain cases, rather than fine-tuning the entire pre-trained model
  end-to-end, it can be beneficial to obtained pre-trained contextual 
      embeddings, which are fixed contextual representations of each input 
      token generated from the hidden layers of the pre-trained model. This 
      should also mitigate most of the out-of-memory issues.</p>
</blockquote>

<p>I am wondering in which cases, using only token vectors, will be more beneficial (other than out of memory issue)? </p>
","bert"
"{
  ""id"": 52719,
  ""title"": ""meaning of fine-tuning in nlp task""
}","meaning of fine-tuning in nlp task","2019-05-27 15:48:21","","7","2860","<nlp><word2vec><word-embeddings><transfer-learning><bert>","<p>There are two types of transfer learning model. One is
feature extraction, where the weights of the pre-trained model are not changed while training on the actual task and other is the weights of the pre-trained model can be changed.</p>

<p>According to those categorizations, static word vector like word2vec is a feature extraction model, where each vector encodes the meaning of the word.  </p>

<p>The meaning of the word changes context. For example, ""Bank of the river""  vs ""Bank as a financial institute"". These word2vec vectors do not differentiate between these meaning. </p>

<p>Current models like Bert consider the context. Bert is a language representation model. That means, it internally can represent word by contextual word vectors.</p>

<p>By default, Bert is a fine-tuning model. This is where my imagination about fine-tuning started to fall apart. 
Let's say, on top of Bert model we created some task-specific layer. Now, if we fine tune, by definition, the weights of the lower level (language representation layer) will change at least a bit that means, the vector of the word will also change (if we compare before and after fine tune). That means the meaning of the word change a bit because of the new task.
If my above interpretation is correct, I can not comprehend this phenomenon for example, the word vectors of task sentiment analysis are different that the word vectors (of the same word) of task question answering. Can anybody help me?    </p>

<p>Please correct me if anything above is wrong. Thanks  </p>
","bert"
"{
  ""id"": 51785,
  ""title"": ""what is the first input to the decoder in a transformer model?""
}","what is the first input to the decoder in a transformer model?","2019-05-11 08:36:07","51798","12","15292","<nlp><sequence><bert><transformer>","<p><a href=""https://i.sstatic.net/SPNEP.png"" rel=""noreferrer""><img src=""https://i.sstatic.net/SPNEP.png"" alt=""from https://jalammar.github.io/illustrated-transformer/""></a></p>

<p>The image is from url: <a href=""https://jalammar.github.io/illustrated-transformer/"" rel=""noreferrer"">Jay Alammar on transformers</a></p>

<p>K_encdec and V_encdec are calculated in a matrix multiplication with the encoder outputs and sent to the encoder-decoder attention layer of each decoder layer in the decoder.<br>
The previous output is the input to the decoder from step 2 but what is the input to the decoder in step 1?  Just the K_encdec and V_encdec or is it necessary to prompt the decoder by inputting the vectorized output (from the encoder) for the first word?  </p>
","bert"
"{
  ""id"": 51697,
  ""title"": ""bert-as-service maximum sequence length""
}","bert-as-service maximum sequence length","2019-05-09 18:06:36","51713","2","2379","<nlp><sequence><bert>","<p>I installed bert-as-service (<a href=""https://github.com/hanxiao/bert-as-service"" rel=""nofollow noreferrer"">bert-as-service github repo</a>) and tried encoding some sentences in Japanese on the <code>multi_cased_L-12_H-768_A-12</code> model.  It seems to work as I am getting vectors of length 768 per word but <code>np.shape()</code> shows this for each sentence:</p>

<pre><code>np.shape(vec_j[0]): (25, 768)
np.shape(vec_j[1]): (25, 768)
np.shape(vec_j[2]): (25, 768)
np.shape(vec_j[3]): (25, 768)
np.shape(vec_j[4]): (25, 768)
type: &lt;class 'numpy.ndarray'&gt;
</code></pre>

<p>My sentences are short so there is quite a bit of padding with 0's.  Still, I am unsure why this model seems to have a maximum sequence length of 25 rather than the 512 mentioned here:  <a href=""https://github.com/google-research/bert/blob/master/README.md"" rel=""nofollow noreferrer"">Bert documentation section on tokenization</a></p>

<blockquote>
  <p>""Truncate to the maximum sequence length. (You can use up to 512, but
  you probably want to use shorter if possible for memory and speed
  reasons.)""</p>
</blockquote>
","bert"
"{
  ""id"": 51522,
  ""title"": ""What is the use of [SEP] in paper BERT?""
}","What is the use of [SEP] in paper BERT?","2019-05-07 04:53:18","","18","25003","<machine-learning><nlp><transformer><bert>","<p>I know that [CLS] means the start of a sentence and [SEP] makes BERT know the second sentence has begun. </p>

<p>However, I have a question.</p>

<p>If I have 2 sentences, which are s1 and s2, and our fine-tuning task is the same. </p>

<p>In one way, I add special tokens and the input looks like [CLS]+s1+[SEP] + s2 + [SEP]. </p>

<p>In another, I make the input look like [CLS] + s1 + s2 + [SEP]. </p>

<p>When I input them to BERT respectively, what is the difference between them? </p>

<ul>
<li><p>Will the s1 in second one integrate more information from s2 than the s1 in first one does?</p></li>
<li><p>Will the token embeddings change a lot between the 2 methods?</p></li>
</ul>

<p>Thanks for any help!</p>
","bert"
"{
  ""id"": 51279,
  ""title"": ""Which is the \""most properly working\"" Bert-Ner repository""
}","Which is the ""most properly working"" Bert-Ner repository","2019-05-02 13:59:15","","0","381","<deep-learning><pytorch><bert>","<p>I am trying to find a repository in Github to get a Pytorch-reimplementation of the Bert model for NER task. So far, I found the following repos:</p>

<ul>
<li><a href=""https://github.com/kamalkraj/BERT-NER"" rel=""nofollow noreferrer"">https://github.com/kamalkraj/BERT-NER</a></li>
<li><a href=""https://github.com/Louis-udm/NER_BERT_CRF"" rel=""nofollow noreferrer"">https://github.com/Louis-udm/NER_BERT_CRF</a></li>
<li><a href=""https://github.com/sberbank-ai/ner-bert"" rel=""nofollow noreferrer"">https://github.com/sberbank-ai/ner-bert</a></li>
</ul>

<p>They are not exactly the same models (for example some use CRF as additionally), but for me the correctness is more important. When I check the issues, I saw that people were complaining about either mistakes in the evaluation or training procedures. So, my question, has anyone tried any of these repositories for Ner tasks before? If you know any other options, I'm open to hear. </p>
","bert"
"{
  ""id"": 49522,
  ""title"": ""What is GELU activation?""
}","What is GELU activation?","2019-04-18 08:06:24","49535","43","32348","<activation-function><bert><mathematics>","<p>I was going through <a href=""https://arxiv.org/pdf/1810.04805.pdf"" rel=""noreferrer"">BERT paper</a> which uses <a href=""https://arxiv.org/pdf/1606.08415.pdf"" rel=""noreferrer"">GELU (Gaussian Error Linear Unit)</a> which states equation as 
<span class=""math-container"">$$ GELU(x) = xP(X ≤ x) = xΦ(x).$$</span> which in turn is approximated to <span class=""math-container"">$$0.5x(1 + tanh[\sqrt{
2/π}(x + 0.044715x^3)])$$</span></p>

<p>Could you simplify the equation and explain how it has been approximated.</p>
","bert"
"{
  ""id"": 49460,
  ""title"": ""BERT has a non deterministic behaviour""
}","BERT has a non deterministic behaviour","2019-04-17 07:57:15","","1","556","<neural-network><nlp><feature-extraction><bert>","<p>I am using the BERT implementation in <a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a> for feature extracting and I have noticed a weird behaviour which I was not expecting: if I execute the program twice on the same text, I get different results. I need to know if this is normal and why this happens in order to treat this fact in one or another way. Why is the reason for this? Aren't neural networks deterministic algorithms?</p>
","bert"
"{
  ""id"": 48100,
  ""title"": ""How is WordPiece tokenization helpful to effectively deal with rare words problem in NLP?""
}","How is WordPiece tokenization helpful to effectively deal with rare words problem in NLP?","2019-03-27 16:54:59","","7","9153","<nlp><word-embeddings><bert>","<p>I have seen that NLP models such as <a href=""https://github.com/google-research/bert"" rel=""noreferrer"">BERT</a> utilize WordPiece for tokenization. In WordPiece, we split the tokens like playing to play and ##ing. It is mentioned that it covers a wider spectrum of Out-Of-Vocabulary (OOV) words. Can someone please help me explain how WordPiece tokenization is actually done, and how it handles effectively helps to rare/OOV words?</p>
","bert"
"{
  ""id"": 47687,
  ""title"": ""Paragraph Generator using BERT or GPT""
}","Paragraph Generator using BERT or GPT","2019-03-20 16:22:00","","2","224","<nlp><bert><gpt>","<p>I am trying to generate similar sentences, called paragraph generation. For example, what is the name of the eldest brother of ram? - For these paragraphs can be - who is the oldest brother of ram? , Who is the oldest blood brother of ram? , I want to know the eldest sibling of ram.  etc etc  </p>

<p>BERT and OpenAI GPT are one of the most powerful NLP systems, as per my knowledge, at least for now. However, I heard that BERT is based on MLM so it cant be used for sentence generation, but can be used for word prediction.  </p>

<p>Can OpenAI GPT be used for this purpose? If so, please give some pointer.<br>
If any other name is in mind which is the State of Art please suggest.</p>
","bert"
"{
  ""id"": 47406,
  ""title"": ""Incrementally Train BERT with minimum QnA records - to get improved results""
}","Incrementally Train BERT with minimum QnA records - to get improved results","2019-03-16 10:30:21","","2","1886","<machine-learning><training><transformer><bert>","<p>We are using Google BERT for Question and Answering. We have fine tuned BERT with SQUAD QnA release train data set (<a href=""https://github.com/google-research/bert"" rel=""nofollow noreferrer"">https://github.com/google-research/bert</a> , <a href=""https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json"" rel=""nofollow noreferrer"">https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json</a>)   </p>

<p>It generated new checkpoints and BERT is giving good answers for most of questions we asked on our text documents. However, there are some questions which it is answering wrong, so we are trying to further fine tune with our Question and known answer on our text document. We further trained based on last generated checkpoint and got new checkpoint.   </p>

<p><strong>With new checkpoint when we are asking the same question</strong>, the answer did not got corrected! Previously BERT was giving wrong answer with 99% confidence and now also <strong>giving same wrong answer with 95% confidence</strong>.  </p>

<p>Can someone suggest, if they have same or similar experience, and suggest please.<br>
Following are questions in BERT github Issues, and are unanswered for quite some time:  </p>

<ul>
<li>BERT accuracy reduced after providing custom training..The answer is
also not correct : <a href=""https://github.com/google-research/bert/issues/492"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/issues/492</a></li>
<li>Unable to incrementally train BERT with custom training: <a href=""https://github.com/google-research/bert/issues/482"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/issues/482</a></li>
<li>Little training has no impact:    <a href=""https://github.com/google-research/bert/issues/481"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/issues/481</a>  </li>
<li>Custom Domain Training: <a href=""https://github.com/google-research/bert/issues/498"" rel=""nofollow noreferrer"">https://github.com/google-research/bert/issues/498</a></li>
</ul>
","bert"
"{
  ""id"": 47397,
  ""title"": ""How Transformer is Bidirectional - Machine Learning""
}","How Transformer is Bidirectional - Machine Learning","2019-03-16 07:12:36","","3","1721","<machine-learning><transformer><bert>","<p>Asking question in datascience forum, as this forum seems well suited for data science related questions: <a href=""https://stackoverflow.com/questions/55158554/how-transformer-is-bidirectional-machine-learning/55158766?noredirect=1#comment97066160_55158766"">https://stackoverflow.com/questions/55158554/how-transformer-is-bidirectional-machine-learning/55158766?noredirect=1#comment97066160_55158766</a></p>

<p>I am coming from Google BERT context (Bidirectional Encoder representations from Transformers). I have gone through architecture and codes. People say this is <strong>bidirectional by nature</strong>. To make it unidirectional attention some mask is to be applied.</p>

<p>Basically a transformer takes key, values and queries as input; uses encoder decoder architecture; and applies attention to these keys, queries and values. What I understood is we need to pass tokens explicitly rather than transformer understanding this by nature.</p>

<p>Can someone please explain <strong>what makes transformer bidirectional by nature</strong></p>

<p>Answer received so far:<br>
1. People confirmed that Transformer has Bidirectional nature, rather than an external code making it bidirectional.<br>
2. 
My doubt: We are passing Q K V embeddings to transformer, to which it applies N layers of self attention using ScaledDotMatrix attention. Same thing can be done by unidirection approach as well. May I know what part I am missing in my understanding. If someone can point to code where it is getting bidirectional, it would be a great help.</p>
","bert"
"{
  ""id"": 46679,
  ""title"": ""Bert Fine Tuning with additional features""
}","Bert Fine Tuning with additional features","2019-03-05 02:57:48","48436","9","5412","<nlp><bert>","<p>I want to use Bert for an nlp task. But I also have additional features that I would like to include. </p>

<p>From what I have seen, with fine tuning, one only changes the labels and retrains the classification layer.</p>

<p>Is there a way to used pre-trained Bert models and include additional features?</p>
","bert"
"{
  ""id"": 46377,
  ""title"": ""Can BERT do the next-word-predict task?""
}","Can BERT do the next-word-predict task?","2019-02-28 08:37:42","46382","21","11856","<neural-network><deep-learning><attention-mechanism><transformer><bert>","<p>As BERT is bidirectional (uses bi-directional transformer), is it possible to use it for the next-word-predict task? If yes, what needs to be tweaked?</p>
","bert"